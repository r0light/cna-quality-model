# Cloud architectural principles by Pahl et al. 2018

Pahl, C.; Jamshidi, P. & Zimmermann, O.
Architectural Principles for Cloud Software 
ACM Transactions on Internet Technology, Association for Computing Machinery (ACM), 2018, 18, 1-23

*   **Service-orientation**. Service-orientation is a grouping of principles, cf. Table I, defining the SOA style, including layering, modularity and loose coupling, which is a relevant organisational principle of the cloud [Jamshidi et al. 2015] as already discussed in Section 7. The SOA principles are adopted from [Zimmermann 2009].
    Layering  
    Motivation: Service characteristics such as interface granularity and lifecycle vary, e.g., between a technical logging service and a business process.  
    Solution: We organize the SOA into 3++ architectural layers.  
    Effect: More indirections, however, require a communications infrastructure.  
    Modularity
    Motivation: Integrating monolithic applications such as traditional enterprise packages is hard.
    Solution:  Refactor into services, expose service interfaces only and encapsulate implementation details.
    Effect: Services have to be located and invoked in a coordinated manner and service invocations should be free of undesired side effects (state management).
    Loose Coupling
    Motivation: Once applications have been modularized, dependencies between services occur.
    Solution: Couple services loosely (across several dimensions), e.g., a messaging system decouples in time, location, and language dimensions.
    Effect: Messaging with single endpoint. The receiver is stateless, so conversational sessions require a correlation logic and asynchronous communication complicates systems management.
*   **Virtualization**. The virtualization of resources in the cloud is an essential contributor to elastically manage and provide these resources as services [Antonopoulos and Gillam 2010]. The virtualization principle is defined in Table II. We can distinguish three forms: Firstly, infrastructure virtualization separates physical infrastructures to create dedicated (virtual) resources. Secondly, platform virtualization through containerisation, which abstracts away the platform (technology stack), achieving flexible virtualized application management. Finally, application virtualization to separate applications from the underlying platform and infrastructure so that applications may run parallel to other applications while moving across other platforms.
    Motivation: Cloud computing with sharing of pooled resources only pays off at scale. Elasticity is required to manage changing demands and environmental conditions and deploy loads flexibly.  
    Solution: Infrastructure virtualization is software that separates physical infrastructures to create various dedicated (virtual) resources. Infrastructure virtualization software in the form of hypervisors and elastic infrastructures [Fehling et al. 2014] makes it possible to run multiple operating systems and multiple applications on the same server at the same time. It enables businesses to reduce IT costs while increasing the efficiency, utilization and flexibility of their existing computer hardware. Platform virtualization is achieved through containerisation, which abstracts away the platform (technology stack) rather than the hardware. Containers build on infrastructure virtualization techniques for process isolation, but enable full-stack lightweight and portable application containers (e.g., LXC or Docker) to be assembled on top of platform components [He et al. 2012; Pahl 2015]. Application virtualization separate applications from an underlying platform or infrastructure. This enables applications to run parallel to others while moving across other platforms.  
    Effect: Cloud virtualization provides self-service capability, elasticity, automated management, scalability and pay-as you go service that is not inherent in virtualization alone. Infrastructure virtualization requires process isolation to manage security concerns if resources are shared. Platform virtualization requires a container support to achieve portability at platform level.  
*   **Adaptation**. A set of model dimensions helps to frame the adaptivity problem [De Lemos et al. 2013]. In order to contextualise the adaptation cloud principle in its wider adaptivity context, we align the general modelling dimensions to the cloud.  
    -   Goals – Goals as system objectives: evolution, flexibility, multiplicity, duration, dependency. In the cloud, multiple and interdependent goals (e.g., cost and performance) can evolve and are often expressed in flexible terms (from informal to formal, certain to uncertain). They can vary in duration.
    -   Cause for adaptation – Change captures causes of adaptation: source, type, frequency, anticipation. In the cloud, resources provide information through monitoring, but subject to uncertainty. These can be categorized by resource and quality type. The frequency varies. Thus, anticipation using prediction techniques is essential.
    -   System reaction to change – Mechanisms to implement adaptation: type, autonomy, scope, duration, timeliness, triggering. In the cloud, different actions are possible, for instance scaling. These can be autonomous or manual and can cover different resource types. Some latency from when the change is triggered to when the change exists (change needs to go through several layers), the duration is typically shorttermed with a need to react swiftly (timeliness), triggered through feedback loop.
    -   Impact of Adaptation on System – define adaptation impact: criticality, predictability, overhead, resilience. In the cloud, non-operational aspects apply, covering how mission/safety-critical an adaptation is or the significance of the adaptation in the context of possible failure, and whether adaptation impact is durable/persistent.
A challenge is mapping requirements to the underlying architecture. We focus on the
operational aspects later on: cause for change and reaction to adaptation. Their application to the cloud is reflected in the principle definition in Table III.
Motivation: Systems should self-adapt with minimal human involvement. Systems must be able to cope with variable resources, system errors, and changing user characteristics, while maintaining the goals and properties envisioned by the developers and users.
Solution: Adaptation is a process, in which an adaptive system adapts its behaviour based on information acquired about its user(s) and environment [Cheng et al. 2009].
Effect: Drivers of adaptivity are often requirements to maintain quality-of-service at the user end to stay within the (non-functional) limits possibly stated in a service-level agreement.
*   **Uncertainty**. To deal with uncertainty in cloud architectures, we need to measure at different layers and map between the different tiers in the cloud. Upper levels represent application-level qualities, e.g., performance for different storage configurations. Lower levels are loads of infrastructure resources that run the service (processor and memory loads). Furthermore, there is a mapping of the infrastructure loads into a cost model – which can of course be a major driver of adaptation decisions. Mappings of metrics between layers, e.g., predicting service-level response times from infrastructure measurements [Zhang et al. 2014], adds a degree of uncertainty. Multi-cloud deployments with different monitoring mechanisms and the interference of the network in the calculation of performance metrics is equally a contributor of uncertainty. Ideally, system qualities can be reliably measured. However, the cloud adds a high degree of uncertainty. This uncertainty can be captured in uncertainty levels [Sawyer et al. 2010], from general confidence about the shape of the future, with some key variables not having precise values to the impossibility to frame possible scenarios. Different sources of uncertainty can be identified [Jamshidi et al. 2014]:
    -   Uncertainty in adaptation and itsmodels. Adaptation thresholds rely on knowledge of system behaviour and how resources are managed. The accuracy of policies remains
subjective, making them prone to uncertainty. Unpredictable changes in environment or application demand may require policies to be continuously re-evaluated.     
    -   Uncertainty in the dynamic provisioning environment. Acquiring or releasing resources in the cloud is not instantaneous. During this time (minutes for VMs) the application is vulnerable to workload increases, causing uncertainty.
    -   Uncertainty in monitoring data. The controller needs to continuously monitor application states as well as of resources in which applications are deployed in order to timely react to load variations. Monitoring involves data collected by measurementspecific probes or sensors, which are not immune to measurement deviations (sensory noise). This sensory noise is another source of uncertainty.
    -   Uncertainty in change enactment. Even the same change can take different times depending on uncertainty in underlying resources. This is reflected in the virtualization principle in Table IV.
Motivation: Uncertainty emerges from various sources in cloud systems – as uncertainty from different interpretations and decisions in the adaptation definition process or as uncertainty arising from possible different, distributed monitoring systems resulting in partially unreliable and
incomplete data.
Solution: Models also reflect how we deal with uncertainty in dynamic systems.
Effect: A system’s current non-functional properties need to be aligned with non-functional requirements. Due to the layering, mapping and managing these across layers, but also within one layer, is challenging. Service Level Agreements (SLAs) become somewhat fuzzy.
