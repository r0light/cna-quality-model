% Encoding: UTF-8

@InProceedings{Anchuri2014,
  author    = {Anchuri, Pranay and Sumbaly, Roshan and Shah, Sam},
  booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
  title     = {Hotspot Detection in a Service-Oriented Architecture},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {1749–1758},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '14},
  abstract  = {Large-scale websites are predominantly built as a service-oriented architecture. Here,
services are specialized for a certain task, run on multiple machines, and communicate
with each other to serve a user's request. Reducing latency and improving the cost
to serve is quite important, but optimizing this service call graph is particularly
challenging due to the volume of data and the graph's non-uniform and dynamic nature.In
this paper, we present a framework to detect hotspots in a service-oriented architecture.
The framework is general, in that it can handle arbitrary objective functions. We
show that finding the optimal set of hotspots for a metric, such as latency, is NP-complete
and propose a greedy algorithm by relaxing some constraints. We use a pattern mining
algorithm to rank hotspots based on the impact and consistency. Experiments on real
world service call graphs from LinkedIn, the largest online professional social network,
show that our algorithm consistently outperforms baseline methods.},
  doi       = {10.1145/2661829.2661991},
  isbn      = {9781450325981},
  keywords  = {monitoring, call graph, service-oriented architecture, hotspots},
  location  = {Shanghai, China},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2661829.2661991},
}

@InProceedings{Miranda2014,
  author    = {Miranda, Breno and Bertolino, Antonia},
  booktitle = {Proceedings of the 9th International Workshop on Automation of Software Test},
  title     = {Social Coverage for Customized Test Adequacy and Selection Criteria},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {22–28},
  publisher = {Association for Computing Machinery},
  series    = {AST 2014},
  abstract  = {Test coverage information can be very useful for guiding testers in enhancing their
test suites to exercise possible uncovered entities and in deciding when to stop testing.
However, for complex applications that are reused in different contexts and for emerging
paradigms (e.g., component-based development, service-oriented architecture, and cloud
computing), traditional coverage metrics may no longer provide meaningful information
to help testers on these tasks. Various proposals are advocating to leverage information
that come from the testing community in a collaborative testing approach. In this
work we introduce a coverage metric, the Social Coverage, that customizes coverage
information in a given context based on coverage data collected from similar users.
To evaluate the potential of our proposed approach, we instantiated the social coverage
metric in the context of a real world service oriented application. In this exploratory
study, we were able to predict the entities that would be of interest for a given
user with an average precision of 97% and average recall of 75%. Our results suggest
that, in similar environments, social coverage can provide a better support to testers
than traditional coverage.},
  doi       = {10.1145/2593501.2593505},
  isbn      = {9781450328586},
  keywords  = {User Similarity, Coverage Testing, Service-Oriented Application, Relative Coverage},
  location  = {Hyderabad, India},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2593501.2593505},
}

@Article{Castelluccia2014,
  author     = {Castelluccia, Daniela and Boffoli, Nicola},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Service-Oriented Product Lines: A Systematic Mapping Study},
  year       = {2014},
  issn       = {0163-5948},
  month      = mar,
  number     = {2},
  pages      = {1–6},
  volume     = {39},
  abstract   = {Software product line engineering and service-oriented architectures both enable organizations
to capitalize on reuse of existing software assets and capabilities and improve competitive
advantage in terms of development savings, product flexibility, time-to-market. Both
approaches accommodate variation of assets, including services, by changing the software
being reused or composing services according a new orchestration. Therefore, variability
management in Service-oriented Product Lines (SoPL) is one of the main challenges
today. In order to highlight the emerging evidence-based results from the research
community, we apply the well-defined method of systematic mapping in order to populate
a classification scheme for the SoPL field of interest. The analysis of results throws
light on the current open issues. Moreover, different facets of the scheme can be
combined to answer more specific research questions. The report reveals the need for
more empirical research able to provide new metrics measuring efficiency and efficacy
of the proposed models, new methods and tools supporting variability management in
SoPL, especially during maintenance and verification and validation. The mapping study
about SoPL opens further investigations by means of a complete systematic review to
select and validate the most efficient solutions to variability management in SoPL.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2579281.2579294},
  issue_date = {March 2014},
  keywords   = {empirical study, mapping study, service-oriented computing, product line development, service-oriented architecture, software product line, variability management},
  numpages   = {6},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2579281.2579294},
}

@InProceedings{Oliveira2016,
  author    = {Oliveira, Joyce Aline and Junior, Jose J.L.D.},
  booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
  title     = {A Three-Dimensional View of Reuse in Service Oriented Architecture},
  year      = {2016},
  address   = {Porto Alegre, BRA},
  pages     = {409–416},
  publisher = {Brazilian Computer Society},
  series    = {SBSI 2016},
  abstract  = {The reuse in Service Oriented Architecture (SOA) has been used strategically in organizations
to reduce development costs and increase the quality of applications. This article
reports a qualitative research realized with experts in order to identify goals, barriers,
facilitators, strategies, metrics and benefits associated with reuse in SOA. The results
were summarized in three dimensions (management, architecture, operation) and represented
by a conceptual model that can serve as a preliminary roadmap to manage the reuse
in SOA.},
  isbn      = {9788576693178},
  keywords  = {qualitative research, SOA reuse, Services Oriented Architecture},
  location  = {Florianopolis, Santa Catarina, Brazil},
  numpages  = {8},
}

@InProceedings{Ke2017,
  author    = {Ke, Weimao},
  booktitle = {Proceedings of the 2017 International Conference on Management Engineering, Software Engineering and Service Sciences},
  title     = {Distributed Search Efficiency and Robustness in Service Oriented Multi-Agent Networks},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {9–18},
  publisher = {Association for Computing Machinery},
  series    = {ICMSS '17},
  abstract  = {We study decentralized searches in a large service-oriented agent network and investigate
the influences of multiple factors on search efficiency. In this study we focus on
overall system robustness and examine search performance in unstable environments
where individual agents may fail or a system-wide attack may occur. Experimental results
show that searches continue to be efficient when a large number of service agents
become unavailable. Surprisingly, overall system performance in terms of a search
path length metric improves with an increasing number of unavailable agents. Service
unavailability also has an impact on the load balance of service agents. We plan to
conduct further research to verify observed patterns and to understand related implications
on system architecture design.},
  doi       = {10.1145/3034950.3034975},
  isbn      = {9781450348348},
  keywords  = {distributed search, information network, robustness, service agents},
  location  = {Wuhan, China},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3034950.3034975},
}

@InProceedings{Lehmann2017,
  author    = {Lehmann, Martin and Sandnes, Frode Eika},
  booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
  title     = {A Framework for Evaluating Continuous Microservice Delivery Strategies},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICC '17},
  abstract  = {The emergence of service-oriented computing, and in particular microservice architecture,
has introduced a new layer of complexity to the already challenging task of continuously
delivering changes to the end users. Cloud computing has turned scalable hardware
into a commodity, but also imposes some requirements on the software development process.
Yet, the literature mainly focuses on quantifiable metrics such as number of manual
steps and lines of code required to make a change. The industry, on the other hand,
appears to focus more on qualitative metrics such as increasing the productivity of
their developers. These are common goals, but must be measured using different approaches.
Therefore, based on interviews of industry stakeholders a framework for evaluating
and comparing approaches to continuous microservice delivery is proposed. We show
that it is possible to efficiently evaluate and compare strategies for continuously
delivering microservices.},
  articleno = {64},
  doi       = {10.1145/3018896.3018961},
  isbn      = {9781450347747},
  keywords  = {microservices, microservice architectures, deployment strategy, cloud computing, evaluation framework, continuous deployment},
  location  = {Cambridge, United Kingdom},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3018896.3018961},
}

@InProceedings{Oliveira2018,
  author    = {Oliveira, Joyce Aline and Vargas, Matheus and Rodrigues, Roni},
  booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
  title     = {SOA Reuse: Systematic Literature Review Updating and Research Directions},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SBSI'18},
  abstract  = {Service Oriented Architecture (SOA) reuse has been used strategically in organizations
to reduce development costs and increase the quality of applications. This article
analyzes a systematic literature review in order to identify concepts, goals, strategies,
and metrics of SOA reuse. The results show that the main goal of SOA reuse is to decrease
development costs. The factor that most negatively influences SOA reuse is the existence
of legacy systems. The strategy used most to potentialize SOA reuse is business process
management. Metrics proposed by studies to measure SOA reuse are related to modularity
and adaptability indicators. The study is relevant because it increases the body of
knowledge of the area. Additionally, a set of gaps to be addressed by researchers
and reuse practitioners was identified.},
  articleno = {71},
  doi       = {10.1145/3229345.3229419},
  isbn      = {9781450365598},
  keywords  = {Service Oriented Architecture, systematic literature review, SOA reuse},
  location  = {Caxias do Sul, Brazil},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3229345.3229419},
}

@InProceedings{Rudorfer2018,
  author    = {Rudorfer, Martin and Pannen, Tessa J. and Kr\"{u}ger, J\"{o}rg},
  booktitle = {Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control},
  title     = {A Case Study on Granularity of Industrial Vision Services},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ISCSIC '18},
  abstract  = {Software engineering paradigms such as service-oriented architectures are increasingly
often applied in the field of factory automation. Functions like robot motion planning
or object recognition are provided by cloud services. A crucial architectural aspect
is the granularity, i.e. the scope and size of individual services. In our case study,
we examine a service-based object recognition application for a robotic assembly use
case. We implement three different granularity levels, measure their communication
and computation times and discuss further architectural features. The fine-granular
approach encapsulates individual image processing operations as services, which have
high reusability but impose large communication overheads. The medium granularity
approach is object-wise and offers best reuse efficiency and cohesion. The coarse
solution offers the best performance.},
  articleno = {59},
  doi       = {10.1145/3284557.3284713},
  isbn      = {9781450366281},
  keywords  = {Object Recognition, Granularity, Service-Oriented Architecture},
  location  = {Stockholm, Sweden},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3284557.3284713},
}

@InProceedings{Ilin2017,
  author    = {Ilin, I. and Levina, A. and Abran, A. and Iliashenko, O.},
  booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
  title     = {Measurement of Enterprise Architecture (EA) from an IT Perspective: Research Gaps and Measurement Avenues},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {232–243},
  publisher = {Association for Computing Machinery},
  series    = {IWSM Mensura '17},
  abstract  = {Reorganizational projects in general and software-related projects in particular,
are often implemented with a focus only on the reorganized components within an organizational
management system, not taking into account relationships with the other components
of an enterprise architecture (EA). This paper first looks at the current state of
EA measurement to identify weaknesses and gaps in aligning and measuring EA components,
EA structures and EA interrelationships from an IT perspective. It then identifies
from related works available innovative measurement concepts that could contribute
for aligning, measuring and monitoring software-related projects within an EA strategy.
This includes measurement avenues within a Balanced Scorecard (BSC), contributions
of functional size measurement to the BSC, and measurement of software structures
and functionality within a service-oriented architecture (SOA).},
  doi       = {10.1145/3143434.3143457},
  isbn      = {9781450348539},
  keywords  = {balanced scorecard (BSC), function points (FP), enterprise architecture (EA), enterprise architecture measurement, service-oriented architecture (SOA), functional size measurement (FSM)},
  location  = {Gothenburg, Sweden},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3143434.3143457},
}

@InProceedings{Akiki2014,
  author    = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  title     = {Integrating Adaptive User Interface Capabilities in Enterprise Applications},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {712–723},
  publisher = {Association for Computing Machinery},
  series    = {ICSE 2014},
  abstract  = {Many existing enterprise applications are at a mature stage in their development
and are unable to easily benefit from the usability gains offered by adaptive user
interfaces (UIs). Therefore, a method is needed for integrating adaptive UI capabilities
into these systems without incurring a high cost or significantly disrupting the way
they function. This paper presents a method for integrating adaptive UI behavior in
enterprise applications based on CEDAR, a model-driven, service-oriented, and tool-supported
architecture for devising adaptive enterprise application UIs. The proposed integration
method is evaluated with a case study, which includes establishing and applying technical
metrics to measure several of the method’s properties using the open-source enterprise
application OFBiz as a test-case. The generality and flexibility of the integration
method are also evaluated based on an interview and discussions with practitioners
about their real-life projects.},
  doi       = {10.1145/2568225.2568230},
  isbn      = {9781450327565},
  keywords  = {Adaptive user interfaces, integration, software architectures, model-driven engineering, software metrics, enterprise systems},
  location  = {Hyderabad, India},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2568225.2568230},
}

@InProceedings{Ashrafi2017,
  author    = {Ashrafi, Tasnia H. and Arefin, Sayed E. and Das, Kowshik D. J. and Hossain, Md. A. and Chakrabarty, Amitabha},
  booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
  title     = {FOG Based Distributed IoT Infrastructure},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICC '17},
  abstract  = {The Internet of Things(IoT) can be defined as a network connectivity bridge between
people, systems and physical world. With the increasing number of IoT devices and
networks, dealing with enormous number of data efficiently is becoming more and more
challenging for the present infrastructure which is a very big matter of concern.
In this paper, we depicted the current infrastructure and proposed another model of
IoT infrastructure to surpass the difficulties of the existing infrastructure, which
will be a coordinated effort of Fog computing amalgamation with Machine-to-Machine(M2M)
intelligent communication protocol followed by incorporation of Service Oriented Architecture(SOA)
and finally integration of Agent based SOA. This model will have the capacity to exchange
data by breaking down dependably and methodically with low latency, less bandwidth,
heterogeneity in less measure of time maintaining the Quality of Service(QoS) precisely.},
  articleno = {124},
  doi       = {10.1145/3018896.3036365},
  isbn      = {9781450347747},
  keywords  = {fog computing, M2M communication, heterogeneous devices, agent based SOA, internet of things (IoT), quality of service (QoS), service oriented architecture(SOA)},
  location  = {Cambridge, United Kingdom},
  numpages  = {13},
  url       = {https://doi.org/10.1145/3018896.3036365},
}

@InProceedings{FilelisPapadopoulos2017,
  author    = {Filelis-Papadopoulos, C. K. and Gravvanis, G. A. and Morrison, J. P.},
  booktitle = {Proceedings of the 1st International Workshop on Next Generation of Cloud Architectures},
  title     = {CloudLightning Simulation and Evaluation Roadmap},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {CloudNG:17},
  abstract  = {The CloudLightning (CL) system, designed in the frame of the CloudLightning project,
is a service-oriented architecture for the emerging large scale heterogeneous cloud.
It facilitates a clear distinction between service-lifecyle management and resource-lifecycle
management. This separation of concerns is used to make resource management issues
tractable at scale and to enable functionality that is currently not naturally covered
by the cloud paradigm. In particular, the CL project seeks to maximize computational
efficiency of the cloud in a number of specific ways; by exploiting prebuilt HPC environments,
by dynamically building HPC instances, by improving server utilization, by reducing
power consumption and by improving service delivery. Given the scale and complexity
of this project, its utility can presently only be measured through simulation. This
paper outlines the parameters, constraints and limitation being considered as part
of the design and construction of that simulation environment.},
  articleno = {2},
  doi       = {10.1145/3068126.3068128},
  isbn      = {9781450349369},
  keywords  = {CloudLightning, Self - Organisation, Self - Management, Evaluation},
  location  = {Belgrade, Serbia},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3068126.3068128},
}

@InProceedings{Bogner2018,
  author    = {Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred},
  booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
  title     = {Limiting Technical Debt with Maintainability Assurance: An Industry Survey on Used Techniques and Differences with Service- and Microservice-Based Systems},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {125–133},
  publisher = {Association for Computing Machinery},
  series    = {TechDebt '18},
  abstract  = {Maintainability assurance techniques are used to control this quality attribute and
limit the accumulation of potentially unknown technical debt. Since the industry state
of practice and especially the handling of Service- and Microservice-Based Systems
in this regard are not well covered in scientific literature, we created a survey
to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related
treatment of systems based on service-orientation, and c) influences on developer
satisfaction w.r.t. maintainability. 60 software professionals responded to our online
questionnaire. The results indicate that using explicit and systematic techniques
has benefits for maintainability. The more sophisticated the applied methods the more
satisfied participants were with the maintainability of their software while no link
to a hindrance in productivity could be established. Other important findings were
the absence of architecture-level evolvability control mechanisms as well as a significant
neglect of service-oriented particularities for quality assurance. The results suggest
that industry has to improve its quality control in these regards to avoid problems
with long-living service-based software systems.},
  doi       = {10.1145/3194164.3194166},
  isbn      = {9781450357135},
  keywords  = {microservice-based systems, maintainability, industry, survey, software quality control, service-based systems},
  location  = {Gothenburg, Sweden},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3194164.3194166},
}

@InProceedings{Abid2017,
  author    = {Abid, Ahmed and Messai, Nizar and Rouached, Mohsen and Abid, Mohamed and Devogele, Thomas},
  booktitle = {Proceedings of the Symposium on Applied Computing},
  title     = {Semantic Similarity Based Web Services Composition Framework},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {1319–1325},
  publisher = {Association for Computing Machinery},
  series    = {SAC '17},
  abstract  = {Computing similarities between Web services is a main concern in Service Oriented
Architecture as it allows to decide which services are likely to be matched into a
composite workflow, or in other cases, which services can be substituted in order
to ensure continuous service availability. With the high maturity achieved by the
standards, tools and frameworks in the Semantic Web domain, measuring Web services
similarities relies more than ever on semantic descriptions of services as well as
on semantic relationships these descriptions may hold. In this paper we present a
Framework for Web services composition based on computing semantic similarity between
Web services. We particularly focus on Services Matching engine which uses the considered
similarity measure first to classify Web services into classes of functionally similar
Web services and then to propose a composite sequence of services that matches a requested
goal. In both tasks, the presented framework appeals for best known techniques of
similarity computing and data and knowledge extraction, respectively.},
  doi       = {10.1145/3019612.3019805},
  isbn      = {9781450344869},
  keywords  = {matching, semantic similarity, web services, discovery and composition},
  location  = {Marrakech, Morocco},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3019612.3019805},
}

@InProceedings{Miranda2014a,
  author    = {Miranda, Breno},
  booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
  title     = {A Proposal for Revisiting Coverage Testing Metrics},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {899–902},
  publisher = {Association for Computing Machinery},
  series    = {ASE '14},
  abstract  = {Test coverage information can be very useful for guiding testers in enhancing their
test suites to exercise possible uncovered entities and in deciding when to stop testing.
Since the concept of test criterion was born, several contributions have been made
by both academia and industry in the definition and adaptation of adequacy criteria
aiming at ensuring the discovery of more failures. Numerous contributions have also
been done in the development of coverage tools. However, for complex applications
that are reused in different contexts and for emerging paradigms (e.g., component-based
development, service-oriented architecture, and cloud computing), traditional coverage
metrics may no longer provide meaningful information to help testers on these tasks.
Inspired by the idea of relative coverage this research focuses on the introduction
of meaningful coverage metrics to cope with the challenges imposed by the current
programming paradigms as well as on the definition of a theoretical framework for
the development of relative coverage metrics.},
  doi       = {10.1145/2642937.2653471},
  isbn      = {9781450330138},
  keywords  = {traditional coverage, coverage testing, relative coverage},
  location  = {Vasteras, Sweden},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2642937.2653471},
}

@InProceedings{Suresh2017,
  author    = {Suresh, Lalith and Bodik, Peter and Menache, Ishai and Canini, Marco and Ciucu, Florin},
  booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
  title     = {Distributed Resource Management across Process Boundaries},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {611–623},
  publisher = {Association for Computing Machinery},
  series    = {SoCC '17},
  abstract  = {Multi-tenant distributed systems composed of small services, such as Service-oriented
Architectures (SOAs) and Micro-services, raise new challenges in attaining high performance
and efficient resource utilization. In these systems, a request execution spans tens
to thousands of processes, and the execution paths and resource demands on different
services are generally not known when a request first enters the system. In this paper,
we highlight the fundamental challenges of regulating load and scheduling in SOAs
while meeting end-to-end performance objectives on metrics of concern to both tenants
and operators. We design Wisp, a framework for building SOAs that transparently adapts
rate limiters and request schedulers system-wide according to operator policies to
satisfy end-to-end goals while responding to changing system conditions. In evaluations
against production as well as synthetic workloads, Wisp successfully enforces a range
of end-to-end performance objectives, such as reducing average latencies, meeting
deadlines, providing fairness and isolation, and avoiding system overload.},
  doi       = {10.1145/3127479.3132020},
  isbn      = {9781450350280},
  keywords  = {resource management, service-oriented architectures, microservices, scheduling, rate limiting},
  location  = {Santa Clara, California},
  numpages  = {13},
  url       = {https://doi.org/10.1145/3127479.3132020},
}

@Article{Wu2018,
  author     = {Wu, Yumei and Fang, Yuanyuan and Liu, Bin and Zhao, Zehui},
  journal    = {Personal Ubiquitous Comput.},
  title      = {A Novel Service Deployment Approach Based on Resilience Metrics for Service-Oriented System},
  year       = {2018},
  issn       = {1617-4909},
  month      = oct,
  number     = {5–6},
  pages      = {1099–1107},
  volume     = {22},
  abstract   = {Service-Oriented Architecture (SOA) has been widely used in IT areas and is expected
to bring a lot of benefits. However, the SOA system developers have to address new
challenging issues such as computational resource failure before such benefits can
be realized. This paper develops a graph-theoretic model for the SOA system and proposes
metrics that quantify the resilience of such system under resource failures. It explores
two service deployment strategies to optimize resilience by taking not only communication
costs among services but also the computation costs of services into consideration.
Among them, two types of undirected graphs are developed to model the relationships
between services, including Service Dependence Graph (SDG) and Service Concurrence
Graph (SCG). Then, these two graphs are integrated into Service Relationship Graph
(SRG) and adopt the k-cut optimization theory to complete the service deployment.
Finally, this paper verifies the effectiveness of the above methods in improving the
resilience of the system through a series of experiments, which indicate that our
methods perform better than the previous methods in improving resilience of the SOA
system.},
  address    = {Berlin, Heidelberg},
  issue_date = {October 2018},
  keywords   = {Resilience, SOA, Service relationship graph, Service deployment},
  numpages   = {9},
  publisher  = {Springer-Verlag},
}

@InProceedings{Ouni2015,
  author    = {Ouni, Ali and Gaikovina Kula, Raula and Kessentini, Marouane and Inoue, Katsuro},
  booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
  title     = {Web Service Antipatterns Detection Using Genetic Programming},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {1351–1358},
  publisher = {Association for Computing Machinery},
  series    = {GECCO '15},
  abstract  = {Service-Oriented Architecture (SOA) is an emerging paradigm that has radically changed
the way software applications are architected, designed and implemented. SOA allows
developers to structure their systems as a set of ready-made, reusable and compostable
services. The leading technology used today for implementing SOA is Web Services.
Indeed, like all software, Web services are prone to change constantly to add new
user requirements or to adapt to environment changes. Poorly planned changes may risk
introducing antipatterns into the system. Consequently, this may ultimately leads
to a degradation of software quality, evident by poor quality of service (QoS). In
this paper, we introduce an automated approach to detect Web service antipatterns
using genetic programming. Our approach consists of using knowledge from real-world
examples of Web service antipatterns to generate detection rules based on combinations
of metrics and threshold values. We evaluate our approach on a benchmark of 310 Web
services and a variety of five types of Web service antipatterns. The statistical
analysis of the obtained results provides evidence that our approach is efficient
to detect most of the existing antipatterns with a score of 85% of precision and 87%
of recall.},
  doi       = {10.1145/2739480.2754724},
  isbn      = {9781450334723},
  keywords  = {web services, search-based software engineering, antipatterns},
  location  = {Madrid, Spain},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2739480.2754724},
}

@InProceedings{Mendes2019,
  author    = {Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel},
  booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
  title     = {Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SBSI'19},
  abstract  = {In the last decade the (big) data-driven science paradigm became a wide-spread reality.
However, this approach has some limitations such as a performance dependency on the
quality of the data and the lack of reproducibility of the results. In order to enable
this reproducibility, many tools such as Workflow Management Systems were developed
to formalize process pipelines and capture execution traces. However, interoperating
data generated by these solutions became a problem, since most systems adopted proprietary
data models. To support interoperability across heterogeneous provenance data, we
propose a Service Oriented Architecture with a polystore storage design in which provenance
is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible
for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover,
we propose a query layer that provides location and access transparency to users.
Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios.
Firstly, we illustrate how two research groups can compare their processes and results.
Secondly, we show how our architecture can be used as a queriable provenance repository.
We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology.
Finally, we show our solution usability and extensibility appeal by comparing it to
similar approaches.},
  articleno = {49},
  doi       = {10.1145/3330204.3330259},
  isbn      = {9781450372374},
  keywords  = {polystore, Workflows interoperability, heterogeneous provenance data integration},
  location  = {Aracaju, Brazil},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3330204.3330259},
}

@InProceedings{Mathas2018,
  author    = {Mathas, Christos M. and Segou, Olga E. and Xylouris, Georgios and Christinakis, Dimitris and Kourtis, Michail-Alexandros and Vassilakis, Costas and Kourtis, Anastasios},
  booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
  title     = {Evaluation of Apache Spot's Machine Learning Capabilities in an SDN/NFV Enabled Environment},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ARES 2018},
  abstract  = {Software Defined Networking (SDN) and Network Function Virtualisation (NFV) are transforming
modern networks towards a service-oriented architecture. At the same time, the cybersecurity
industry is rapidly adopting Machine Learning (ML) algorithms to improve detection
and mitigation of complex attacks. Traditional intrusion detection systems perform
signature-based detection, based on well-known malicious traffic patterns that signify
potential attacks. The main drawback of this method is that attack patterns need to
be known in advance and signatures must be preconfigured. Hence, typical systems fail
to detect a zero-day attack or an attack with unknown signature. This work considers
the use of machine learning for advanced anomaly detection, and specifically deploys
the Apache Spot ML framework on an SDN/NFV-enabled testbed running cybersecurity services
as Virtual Network Functions (VNFs). VNFs are used to capture traffic for ingestion
by the ML algorithm and apply mitigation measures in case of a detected anomaly. Apache
Spot utilises Latent Dirichlet Allocation to identify anomalous traffic patterns in
Netflow, DNS and proxy data. The overall performance of Apache Spot is evaluated by
deploying Denial of Service (Slowloris, BoNeSi) and a Data Exfiltration attack (iodine).},
  articleno = {52},
  doi       = {10.1145/3230833.3233278},
  isbn      = {9781450364485},
  keywords  = {Apache Spot, Software Defined Networking, Network Function Virtualisation, Latent Dirichlet Allocation, Penetration Testing, Machine Learning, SHIELD Project},
  location  = {Hamburg, Germany},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3230833.3233278},
}

@Article{Poess2014,
  author     = {Poess, Meikel and Rabl, Tilmann and Jacobsen, Hans-Arno and Caufield, Brian},
  journal    = {Proc. VLDB Endow.},
  title      = {TPC-DI: The First Industry Benchmark for Data Integration},
  year       = {2014},
  issn       = {2150-8097},
  month      = aug,
  number     = {13},
  pages      = {1367–1378},
  volume     = {7},
  abstract   = {Historically, the process of synchronizing a decision support system with data from
operational systems has been referred to as Extract, Transform, Load (ETL) and the
tools supporting such process have been referred to as ETL tools. Recently, ETL was
replaced by the more comprehensive acronym, data integration (DI). DI describes the
process of extracting and combining data from a variety of data source formats, transforming
that data into a unified data model representation and loading it into a data store.
This is done in the context of a variety of scenarios, such as data acquisition for
business intelligence, analytics and data warehousing, but also synchronization of
data between operational applications, data migrations and conversions, master data
management, enterprise data sharing and delivery of data services in a service-oriented
architecture context, amongst others. With these scenarios relying on up-to-date information
it is critical to implement a highly performing, scalable and easy to maintain data
integration system. This is especially important as the complexity, variety and volume
of data is constantly increasing and performance of data integration systems is becoming
very critical. Despite the significance of having a highly performing DI system, there
has been no industry standard for measuring and comparing their performance. The TPC,
acknowledging this void, has released TPC-DI, an innovative benchmark for data integration.
This paper motivates the reasons behind its development, describes its main characteristics
including workload, run rules, metric, and explains key decisions.},
  doi        = {10.14778/2733004.2733009},
  issue_date = {August 2014},
  numpages   = {12},
  publisher  = {VLDB Endowment},
  url        = {https://doi.org/10.14778/2733004.2733009},
}

@InProceedings{Hassam2014,
  author    = {Hassam, Mickael and Kara, Nadjia and Belqasmi, Fatna and Glitho, Roch},
  booktitle = {Proceedings of the 12th ACM International Symposium on Mobility Management and Wireless Access},
  title     = {Virtualized Infrastructure for Video Game Applications in Cloud Environments},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {109–114},
  publisher = {Association for Computing Machinery},
  series    = {MobiWac '14},
  abstract  = {Mobile video games are fast-growing and fast-evolving. Cloud computing's paradigm
can bring several benefits to mobile video games, like cost reduction through an efficient
usage of resources, or an easier and faster on-demand deployment of new applications.
This paper focuses on the architecture aspects of mobile cloud-based video gaming
and proposes a new service oriented and virtualized paradigm. The idea is to provide
reusable game engines sub modules like the rendering or physics engines as cloud computing
services. We call these sub modules substrates. Offered by different substrates providers
as services, they can be dynamically discovered, used and composed. There are several
motivations for substrates virtualization, including the rapid introduction of new
video game applications and cost efficiency through resource sharing. This paper also
describes the implementation of a prototype and the measurements performed to validate
some aspects of our paradigm. As a preliminary validation of this solution, we analyze
the effects of different parameters like virtualization or inner latency on the QoS.
The performance analysis shows that the overhead introduced by substrate virtualization
is acceptable, and reveals how the low-latency connectivity between substrates that
compose a video game application and the limitation of the amount of these substrates
are crucial to achieve a satisfactory level of QoS.},
  doi       = {10.1145/2642668.2642679},
  isbn      = {9781450330268},
  keywords  = {infrastructure and platform as services, mobile video game applications, virtualization, cloud computing, substrate},
  location  = {Montreal, QC, Canada},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2642668.2642679},
}

@InProceedings{Beier2015,
  author    = {Beier, Maximilian and Jansen, Christoph and Mayer, Geert and Penzel, Thomas and Rodenbeck, Andrea and Siewert, Ren\'{e} and Wu, Jie and Krefting, Dagmar},
  booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Multicenter Data Sharing for Collaboration in Sleep Medicine},
  year      = {2015},
  pages     = {880–889},
  publisher = {IEEE Press},
  series    = {CCGRID '15},
  abstract  = {Clinical Sleep Research is an inherent multidisciplinary field, as many health issues
may affect a person's sleep conditions and sleep disorders may cause several health
problems. Many patients with chronic sleep disorders suffer from different further
medical conditions - called multimorbidity. Due to the high variety of the reasons
and the courses of sleep disorders, individual cases are difficult to compare. Therefore
there is a high demand for sleep researchers to collaborate with each other to reach
necessary participant numbers and multidisciplinary expertise. To date, inter-institutional
sleep research is poorly supported by IT systems. In particular the heterogeneity
and the quality variations within the acquired biosignal data - caused by different
biosignal recorders or different measurement procedures - are impeding common biosignal
data processing. In this manuscript we introduce a virtual research platform supporting
inter-institutional data sharing and processing. The infrastructure is based on XNAT
- a free and open-source neuroimaging research platform - a loosely coupled service
oriented architecture and scalable virtualization in the backend. The system is capable
of local pseudonymization of biosignal data, mapping to a standardized set of parameters
and automatic quality assessment. Terms and quality measures are derived from the
"Manual for the Scoring of Sleep and Associated Events" of the American Academy of
Sleep Medicine, the de-facto standard for diagnostic biosignal analysis in sleep medicine.},
  doi       = {10.1109/CCGrid.2015.148},
  isbn      = {9781479980062},
  keywords  = {REST, biosignal, XNAT, cloud, sleep, polysomnography, OpenStack},
  location  = {Shenzhen, China},
  numpages  = {10},
  url       = {https://doi.org/10.1109/CCGrid.2015.148},
}

@Article{Shi2020,
  author     = {Shi, Min and Tang, Yufei and Zhu, Xingquan and Liu, Jianxun},
  journal    = {ACM Trans. Web},
  title      = {Topic-Aware Web Service Representation Learning},
  year       = {2020},
  issn       = {1559-1131},
  month      = apr,
  number     = {2},
  volume     = {14},
  abstract   = {The advent of Service-Oriented Architecture (SOA) has brought a fundamental shift
in the way in which distributed applications are implemented. An overwhelming number
of Web-based services (e.g., APIs and Mashups) have leveraged this shift and furthered
development. Applications designed with SOA principles are typically characterized
by frequent dependencies with one another in the form of heterogeneous networks, i.e.,
annotation relations between tags and services, and composition relations between
Mashups and APIs. Although prior work has shown the utility gained by exploring these
networks, their analysis is still in its infancy. This article develops an approach
to learning representations of the Web service network, which seeks to embed Web services
in low-dimensional continuous vectors with preserved information of the network structure,
functional tags, and service descriptions, such that services with similar functional
properties and network structures are mapped together in the learned latent space.
We first propose a topic generative model for constructing two topic distribution
networks (Mashup-Topic and API-Topic) from the service content. Then, we present an
efficient optimization process to derive low-dimensional vector representations of
Web services from a tri-layer bipartite network with the Mashup-Topic and API-Topic
networks on two ends and the Mashup-API composition network in the middle. Experiments
on real-word datasets have verified that our approach is effective to learn robust
low-rank service representations, i.e., 25% F1-measure gain over the state-of-the-art
in Web service recommendation task.},
  address    = {New York, NY, USA},
  articleno  = {9},
  doi        = {10.1145/3386041},
  issue_date = {April 2020},
  keywords   = {network embedding, service representation, Web services, probabilistic topic model, Mashups},
  numpages   = {23},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3386041},
}

@InProceedings{Tummalapalli2020,
  author    = {Tummalapalli, Sahithi and Kumar, Lov and Murthy, N. L. Bhanu},
  booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
  title     = {Prediction of Web Service Anti-Patterns Using Aggregate Software Metrics and Machine Learning Techniques},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ISEC 2020},
  abstract  = {Service-Oriented Architecture(SOA) can be characterized as an approximately coupled
engineering intended to meet the business needs of an association/organization. Service-Based
Systems (SBSs) are inclined to continually change to enjoy new client necessities
and adjust the execution settings, similar to some other huge and complex frameworks.
These changes may lead to the evolution of designs/products with poor Quality of Service
(QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns.
Anti-patterns makes the evolution and maintenance of the software systems hard and
complex. Early identification of modules, classes, or source code regions where anti-patterns
are more likely to occur can help in amending and maneuvering testing efforts leading
to the improvement of software quality. In this work, we investigate the application
of three sampling techniques, three feature selection techniques, and sixteen different
classification techniques to develop the models for web service anti-pattern detection.
We report the results of an empirical study by evaluating the approach proposed, on
a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of
five types of web-service anti-patterns. Experimental results demonstrated that SMOTE
is the best performing data sampling techniques. The experimental results also reveal
that the model developed by considering Uncorrelated Significant Predictors(SUCP)
as the input obtained better performance compared to the model developed by other
metrics. Experimental results also show that the Least Square Support Vector Machine
with Linear(LSLIN) function has outperformed all other classifier techniques.},
  articleno = {8},
  doi       = {10.1145/3385032.3385042},
  isbn      = {9781450375948},
  keywords  = {Anti-pattern, Classifiers, Service-Based Systems(SBS), Aggregation measures, WSDL, Feature Selection, Class imbalance distribution, Machine Learning, Web-Services, Source Code Metrics},
  location  = {Jabalpur, India},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3385032.3385042},
}

@InProceedings{Camargo2016,
  author    = {de Camargo, Andr\'{e} and Salvadori, Ivan and Mello, Ronaldo dos Santos and Siqueira, Frank},
  booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
  title     = {An Architecture to Automate Performance Tests on Microservices},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {422–429},
  publisher = {Association for Computing Machinery},
  series    = {iiWAS '16},
  abstract  = {The microservices architecture provides a new approach to develop applications. As
opposed to monolithic applications, in which the application comprises a single software
artifact, an application based on the microservices architecture is composed by a
set of services, each one designed to perform a single and well-defined task. These
services allow the development team to decouple several parts of the application using
different frameworks, languages and hardware for each part of the system. One of the
drawbacks for adopting the microservices architecture to develop applications is testability.
In a single application test boundaries can be more easily established and tend to
be more stable as the application evolves, while with microservices we can have a
set of hundreds of services that operate together and are prone to change more rapidly.
Each one of these services needs to be tested and updated as the service changes.
In addition, the different characteristics of these services such as languages, frameworks
or the used infrastructure have to be considered in the testing phase. Performance
tests are applied to assure that a particular software complies with a set of non-functional
requirements such as throughput and response time. These metrics are important to
ensure that business constraints are respected and to help finding performance bottlenecks.
In this paper, we present a new approach to allow the performance tests to be executed
in an automated way, with each microservice providing a test specification that is
used to perform tests. Along with the architecture, we also provide a framework that
implements some key concepts of this architecture. This framework is available as
an open source project1.},
  doi       = {10.1145/3011141.3011179},
  isbn      = {9781450348072},
  keywords  = {test automation, performance test, microservices},
  location  = {Singapore, Singapore},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3011141.3011179},
}

@InProceedings{Brito2021,
  author    = {Brito, Miguel and Cunha, J\'{a}come and Saraiva, Jo\~{a}o},
  booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
  title     = {Identification of Microservices from Monolithic Applications through Topic Modelling},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {1409–1418},
  publisher = {Association for Computing Machinery},
  series    = {SAC '21},
  abstract  = {Microservices emerged as one of the most popular architectural patterns in the recent
years given the increased need to scale, grow and flexibilize software projects accompanied
by the growth in cloud computing and DevOps. Many software applications are being
submitted to a process of migration from its monolithic architecture to a more modular,
scalable and flexible architecture of microservices. This process is slow and, depending
on the project's complexity, it may take months or even years to complete.This paper
proposes a new approach on microservice identification by resorting to topic modelling
in order to identify services according to domain terms. This approach in combination
with clustering techniques produces a set of services based on the original software.
The proposed methodology is implemented as an open-source tool for exploration of
monolithic architectures and identification of microservices. A quantitative analysis
using the state of the art metrics on independence of functionality and modularity
of services was conducted on 200 open-source projects collected from GitHub. Cohesion
at message and domain level metrics' showed medians of roughly 0.6. Interfaces per
service exhibited a median of 1.5 with a compact interquartile range. Structural and
conceptual modularity revealed medians of 0.2 and 0.4 respectively.Our first results
are positive demonstrating beneficial identification of services due to overall metrics'
results.},
  doi       = {10.1145/3412841.3442016},
  isbn      = {9781450381048},
  location  = {Virtual Event, Republic of Korea},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3412841.3442016},
}

@InProceedings{Ma2020,
  author    = {Ma, Meng and Xu, Jingmin and Wang, Yuan and Chen, Pengfei and Zhang, Zonghua and Wang, Ping},
  booktitle = {Proceedings of The Web Conference 2020},
  title     = {AutoMAP: Diagnose Your Microservice-Based Web Applications Automatically},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {246–258},
  publisher = {Association for Computing Machinery},
  series    = {WWW '20},
  abstract  = {The high complexity and dynamics of the microservice architecture make its application
diagnosis extremely challenging. Static troubleshooting approaches may fail to obtain
reliable model applies for frequently changing situations. Even if we know the calling
dependency of services, we lack a more dynamic diagnosis mechanism due to the existence
of indirect fault propagation. Besides, algorithm based on single metric usually fail
to identify the root cause of anomaly, as single type of metric is not enough to characterize
the anomalies occur in diverse services. In view of this, we design a novel tool,
named AutoMAP, which enables dynamic generation of service correlations and automated
diagnosis leveraging multiple types of metrics. In AutoMAP, we propose the concept
of anomaly behavior graph to describe the correlations between services associated
with different types of metrics. Two binary operations, as well as a similarity function
on behavior graph are defined to help AutoMAP choose appropriate diagnosis metric
in any particular scenario. Following the behavior graph, we design a heuristic investigation
algorithm by using forward, self, and backward random walk, with an objective to identify
the root cause services. To demonstrate the strengths of AutoMAP, we develop a prototype
and evaluate it in both simulated environment and real-work enterprise cloud system.
Experimental results clearly indicate that AutoMAP achieves over 90% precision, which
significantly outperforms other selected baseline methods. AutoMAP can be quickly
deployed in a variety of microservice-based systems without any system knowledge.
It also supports introduction of various expert knowledge to improve accuracy.},
  doi       = {10.1145/3366423.3380111},
  isbn      = {9781450370233},
  keywords  = {anomaly diagnosis, web application, Microservice architecture, root cause, cloud computing},
  location  = {Taipei, Taiwan},
  numpages  = {13},
  url       = {https://doi.org/10.1145/3366423.3380111},
}

@InProceedings{Lopez2017,
  author    = {L\'{o}pez, Manuel Ram\'{\i}rez and Spillner, Josef},
  booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
  title     = {Towards Quantifiable Boundaries for Elastic Horizontal Scaling of Microservices},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {35–40},
  publisher = {Association for Computing Machinery},
  series    = {UCC '17 Companion},
  abstract  = {One of the most useful features of a microservices architecture is its versatility
to scale horizontally. However, not all services scale in or out uniformly. The performance
of an application composed of microservices depends largely on a suitable combination
of replica count and resource capacity. In practice, this implies limitations to the
efficiency of autoscalers which often overscale based on an isolated consideration
of single service metrics. Consequently, application providers pay more than necessary
despite zero gain in overall performance. Solving this issue requires an application-specific
determination of scaling limits due to the general infeasibility of an application-agnostic
solution. In this paper, we study microservices scalability, the auto-scaling of containers
as microservice implementations and the relation between the number of replicas and
the resulting application task performance. We contribute a replica count determination
solution with a mathematical approach. Furthermore, we offer a calibration software
tool which places scalability boundaries into declarative composition descriptions
of applications ready to be consumed by cloud platforms.},
  doi       = {10.1145/3147234.3148111},
  isbn      = {9781450351959},
  keywords  = {scalability, replication, optimization, microservices},
  location  = {Austin, Texas, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3147234.3148111},
}

@InProceedings{Anwar2015,
  author    = {Anwar, Ali and Sailer, Anca and Kochut, Andrzej and Butt, Ali R.},
  booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
  title     = {Anatomy of Cloud Monitoring and Metering: A Case Study and Open Problems},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {APSys '15},
  abstract  = {Microservices based architecture has recently gained traction among the cloud service
providers in quest for a more scalable and reliable modular architecture. In parallel
with this architectural choice, cloud providers are also facing the market demand
for fine grained usage based prices. Both the management of the microservices complex
dependencies, as well as the fine grained metering require the providers to track
and log detailed monitoring data from their deployed cloud setups. Hence, on one hand,
the providers need to record all such performance changes and events, while on the
other hand, they are concerned with the additional cost associated with the resources
required to store and process this ever increasing amount of collected data.In this
paper, we analyze the design of the monitoring subsystem provided by open source cloud
solutions, such as OpenStack. Specifically, we analyze how the monitoring data is
collected by OpenStack and assess the characteristics of the data it collects, aiming
to pinpoint the limitations of the current approach and suggest alternate solutions.
Our preliminary evaluation of the proposed solutions reveals that it is possible to
reduce the monitored data size by up to 80% and missed anomaly detection rate from
3% to as low as 0.05% to 0.1%.},
  articleno = {6},
  doi       = {10.1145/2797022.2797039},
  isbn      = {9781450335546},
  location  = {Tokyo, Japan},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2797022.2797039},
}

@InProceedings{FreitasApolinario2020,
  author    = {de Freitas Apolin\'{a}rio, Daniel Rodrigo and de Fran\c{c}a, Breno Bernard Nicolau},
  booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
  title     = {Towards a Method for Monitoring the Coupling Evolution of Microservice-Based Architectures},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {71–80},
  publisher = {Association for Computing Machinery},
  series    = {SBCARS '20},
  abstract  = {The microservice architecture is claimed to satisfy ongoing software development demands,
such as resilience, flexibility, and velocity. However, developing applications based
on microservices also brings some drawbacks, such as the increased software operational
complexity. Recent studies have also pointed out the lack of methods to prevent problems
related to the maintainability of these solutions. Disregarding established design
principles during the software evolution may lead to the so-called architectural erosion,
which can end up in a condition of unfeasible maintenance. As microservices can be
considered a new architecture style, there are few initiatives to monitoring the evolution
of software microservice-based architectures. In this paper, we introduce the SYMBIOTE
method for monitoring the coupling evolution of microservice-based systems. More specifically,
this method collects coupling metrics during runtime (staging or production environments)
and monitors them throughout software evolution. The longitudinal analysis of the
collected measures allows detecting an upward trend in coupling metrics that could
be signs of architectural erosion. To develop the proposed method, we performed an
experimental analysis of the coupling metrics behavior using artificially-generated
data.},
  doi       = {10.1145/3425269.3425273},
  isbn      = {9781450387545},
  keywords  = {maintainability, coupling metrics, software evolution, microservices},
  location  = {Natal, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3425269.3425273},
}

@InProceedings{Mellouk2016,
  author    = {Mellouk, Abdelhamid},
  booktitle = {Proceedings of the Seventh Symposium on Information and Communication Technology},
  title     = {New Provider Services for Convergence Technologies Based on Quality of Experience and Quality of Service Paradigms},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {1},
  publisher = {Association for Computing Machinery},
  series    = {SoICT '16},
  abstract  = {Based on a convergence of network technologies, the Next Generation Network (NGN)
is being deployed to carry high quality video and voice data. In fact, the convergence
of network technologies has been driven by the converging needs of end-users. The
perceived end-to-end quality is becoming one of the main goals required by users that
must be guaranteed by the network operators and the Internet Service Providers, through
manufacturer equipment. This is referred to as the notion of Quality of Experience
(QoE) and is becoming commonly used to represent user perception. The QoE is not a
technical metric, but rather a concept consisting of all elements of a user's perception
of the network services. In this talk, we focus on the idea of how to integrate the
QoE into a control- command chain in order to construct an adaptive network system.
More precisely, in the context of Content-Oriented Networks that is used to redesign
the current Internet architecture to accommodate content-oriented applications and
services, the talk aim to describe an end-to-end QoE model applied to a Content Distribution
Network architecture and see relationships between Quality of service and Quality
of Experience.},
  doi       = {10.1145/3011077.3011078},
  isbn      = {9781450348157},
  location  = {Ho Chi Minh City, Vietnam},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3011077.3011078},
}

@InProceedings{Cardarelli2019,
  author    = {Cardarelli, Mario and Iovino, Ludovico and Di Francesco, Paolo and Di Salle, Amleto and Malavolta, Ivano and Lago, Patricia},
  booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
  title     = {An Extensible Data-Driven Approach for Evaluating the Quality of Microservice Architectures},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {1225–1234},
  publisher = {Association for Computing Machinery},
  series    = {SAC '19},
  abstract  = {Microservice architecture (MSA) is defined as an architectural style where the software
system is developed as a suite of small services, each running in its own process
and communicating with lightweight mechanisms. The benefits of MSA are many, ranging
from an increase in development productivity, to better business-IT alignment, agility,
scalability, and technology flexibility. The high degree of microservices distribution
and decoupling is, however, imposing a number of relevant challenges from an architectural
perspective. In this context, measuring, controlling, and keeping a satisfactory level
of quality of the system architecture is of paramount importance.In this paper we
propose an approach for the specification, aggregation, and evaluation of software
quality attributes for the architecture of microservice-based systems. The proposed
approach allows developers to (i) produce architecture models of the system, either
manually or automatically via recovering techniques, (ii) contribute to an ecosystem
of well-specified and automatically-computable software quality attributes for MSAs,
and (iii) continuously measure and evaluate the architecture of their systems by (re-)using
the software quality attributes defined in the ecosystem. The approach is implemented
by using Model-Driven Engineering techniques.The current implementation of the approach
has been validated by assessing the maintainability of a third-party, publicly available
benchmark system.},
  doi       = {10.1145/3297280.3297400},
  isbn      = {9781450359337},
  keywords  = {microservices, architecture recovery, software quality, model-driven},
  location  = {Limassol, Cyprus},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3297280.3297400},
}

@InProceedings{Brilhante2017,
  author    = {Brilhante, Jonathan and Costa, Rostand and Maritan, Tiago},
  booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
  title     = {Asynchronous Queue Based Approach for Building Reactive Microservices},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {373–380},
  publisher = {Association for Computing Machinery},
  series    = {WebMedia '17},
  abstract  = {To achieve scalability and flexibility in larger applications a new approach arises,
named by Microservices (MS). However MS architectures are at their inception and are
even more a concept than a fully mature design pattern. One of the hardest topics
in this approach is how to properly migrate or develop a single microservice, in terms
of scope, efficiency and dependability. In this sense, this work proposes a new architectural
model based on high-level architecture pattern of reactive programming to the internal
structure of a new microservice. The new model of microservices are internally coordinated
by asynchronous queues, which allowed to preserve compatibility with most monolithic
components and provide an encapsulation process to enable its continuity. A comparative
study between the standard approach and the proposed architecture was carried out
to measure the eventual performance improvement of the new strategy.},
  doi       = {10.1145/3126858.3126873},
  isbn      = {9781450350969},
  keywords  = {asynchronous queues, reactive approach, micro services, refactoring},
  location  = {Gramado, RS, Brazil},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3126858.3126873},
}

@InProceedings{Aniche2016,
  author    = {Aniche, Maur\'{\i}cio and Gerosa, Marco Aur\'{e}lio and Treude, Christoph},
  booktitle = {Proceedings of the 30th Brazilian Symposium on Software Engineering},
  title     = {Developers' Perceptions on Object-Oriented Design and Architectural Roles},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {63–72},
  publisher = {Association for Computing Machinery},
  series    = {SBES '16},
  abstract  = {Software developers commonly rely on well-known software architecture patterns, such
as MVC, to build their applications. In many of these patterns, classes play specific
roles in the system, such as Controllers or Entities, which means that each of these
classes has specific characteristics in terms of object-oriented class design and
implementation. Indeed, as we have shown in a previous study, architectural roles
are different from each other in terms of code metrics. In this paper, we present
a study in a software development company in which we captured developers' perceptions
on object-oriented design aspects of the architectural roles in their system and whether
these perceptions match the source code metric analysis. We found that their developers
do not have a common perception of how their architectural roles behave in terms of
object-oriented design aspects, and that their perceptions also do not match the results
of the source code metric analysis. This phenomenon also does not seem to be related
to developers' experience. We find these results alarming, and thus, we suggest software
development teams to invest in education and knowledge sharing about how their system's
architectural roles behave.},
  doi       = {10.1145/2973839.2973846},
  isbn      = {9781450342018},
  keywords  = {object-oriented design, code metrics, software architecture},
  location  = {Maring\'{a}, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2973839.2973846},
}

@InProceedings{Dasgupta2021,
  author    = {Dasgupta, Gargi B.},
  booktitle = {14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
  title     = {AI and Its Applications in the Cloud Strategy},
  year      = {2021},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ISEC 2021},
  abstract  = {The fourth industrial revolution identifies cloud computing, data, and artificial
intelligence (AI) as opportunity clusters with double digit growth in the next couple
of years. As part of the cloud and digital transformation, the role of AI is crucial
in enabling that transformation as well as creating the new breed of applications
on top. AI mechanisms can help accelerate the modernization of applications, their
management, and the testing on cloud architectures. I will focus on two sub-problems:
1) Refactoring of massive monolith applications using AI techniques. This problem
statement is particularly relevant in understanding legacy un-optimized code and transforming
them to be more cloud-ready. Microservices are indeed becoming the de-facto design
choice for software architecture. It involves partitioning the software components
into finer modules such that the development can happen independently [2]. It also
provides natural benefits when deployed on the cloud since resources can be allocated
dynamically to necessary components based on demand. We are exploring how AI can help
accelerate the transformation of existing applications to microservices. 2) Detecting
faults in application behavior at runtime from operational data. This problem statement
is particularly relevant in understanding how to manage this new architecture of multiple
microservices across the cloud stack [1], [3]. Operational data artifacts span across
logs, metrics, tickets, and traces. Looking at signals across the artifacts and across
the stack presents a challenging data correlation problem. AI mechanisms can help
accelerate problem determination in these complex environments. I will also share
my thoughts on how fundamental breakthroughs in AI Research will be needed as we address
some of the core problems of cloud computing.},
  articleno = {2},
  doi       = {10.1145/3452383.3452385},
  isbn      = {9781450390460},
  keywords  = {modernization, AI Ops, code refactoring, hybrid cloud, log anomalies},
  location  = {Bhubaneswar, Odisha, India},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3452383.3452385},
}

@InProceedings{Khazaei2017,
  author    = {Khazaei, Hamzeh and Ravichandiran, Rajsimman and Park, Byungchul and Bannazadeh, Hadi and Tizghadam, Ali and Leon-Garcia, Alberto},
  booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
  title     = {Elascale: Autoscaling and Monitoring as a Service},
  year      = {2017},
  address   = {USA},
  pages     = {234–240},
  publisher = {IBM Corp.},
  series    = {CASCON '17},
  abstract  = {Auto-scalability has become an evident feature for cloud software systems including
but not limited to big data and IoT applications. Cloud application providers now
are in full control over their applications' microservices and macroservices; virtual
machines and containers can be provisioned or deprovisioned on demand at run-time.
Elascale strives to adjust both micro/macro resources with respect to workload and
changes in the internal state of the whole application stack. Elascale leverages Elasticsearch
stack for collection, analysis and storage of performance metrics. Elascale then uses
its default scaling engine to elastically adapt the managed application. Extendibility
is guaranteed through provider, schema, plug-in and policy elements in the Elascale
by which flexible scalability algorithms, including both reactive and proactive techniques,
can be designed and implemented for various technologies, infrastructures and software
stacks. In this paper, we present the architecture and initial implementation of Elascale;
an instance will be leveraged to add auto-scalability to a generic IoT application.
Due to zero dependency to the target software system, Elascale can be leveraged to
provide auto-scalability and monitoring as-a-service for any type of cloud software
system.},
  keywords  = {macroservices, elasticsearch, monitoring, microservices, auto-scalability, cloud application, scalability as a service, containers, docker},
  location  = {Markham, Ontario, Canada},
  numpages  = {7},
}

@InProceedings{Pan2021,
  author    = {Pan, Yicheng and Ma, Meng and Jiang, Xinrui and Wang, Ping},
  booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  title     = {Faster, Deeper, Easier: Crowdsourcing Diagnosis of Microservice Kernel Failure from User Space},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {646–657},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2021},
  abstract  = {With the widespread use of cloud-native architecture, increasing web applications
(apps) choose to build on microservices. Simultaneously, troubleshooting becomes full
of challenges owing to the high dynamics and complexity of anomaly propagation. Existing
diagnostic methods rely heavily on monitoring metrics collected from the kernel side
of microservice systems. Without a comprehensive monitoring infrastructure, application
owners and even cloud operators cannot resort to these kernel-space solutions. This
paper summarizes several insights on operating a top commercial cloud platform. Then,
for the first time, we put forward the idea of user-space diagnosis for microservice
kernel failures. To this end, we develop a crowdsourcing solution - DyCause, to resolve
the asymmetric diagnostic information problem. DyCause deploys on the application
side in a distributed manner. Through lightweight API log sharing, apps collect the
operational status of kernel services collaboratively and initiate diagnosis on demand.
Deploying DyCause is fast and lightweight as we do not have any architectural and
functional requirements for the kernel. To reveal more accurate correlations from
asymmetric diagnostic information, we design a novel statistical algorithm that can
efficiently discover the time-varying causalities between services. This algorithm
also helps us build the temporal order of the anomaly propagation. Therefore, by using
DyCause, we can obtain more in-depth and interpretable diagnostic clues with limited
indicators. We apply and evaluate DyCause on both a simulated test-bed and a real-world
cloud system. Experimental results verify that DyCause running in the user-space outperforms
several state-of-the-art algorithms running in the kernel on accuracy. Besides, DyCause
shows superior advantages in terms of algorithmic efficiency and data sensitivity.
Simply put, DyCause produces a significantly better result than other baselines when
analyzing much fewer or sparser metrics. To conclude, DyCause is faster to act, deeper
in analysis, and easier to deploy.},
  doi       = {10.1145/3460319.3464805},
  isbn      = {9781450384599},
  keywords  = {granger causal intervals, root cause analysis, microservice system, dynamic service dependency},
  location  = {Virtual, Denmark},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3460319.3464805},
}

@InProceedings{Mercier2020,
  author    = {Mercier, Julien and Whissell-Turner, Kathleen and Paradis, Ariane and Avaca, Ivan},
  booktitle = {9th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
  title     = {Good Vibrations: Tuning a Systems Dynamics Model of Affect and Cognition in Learning to the Appropriate Frequency Bands of Fine-Grained Temporal Sequences of Data: Frequency Bands of Affect and Cognition},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {194–202},
  publisher = {Association for Computing Machinery},
  series    = {DSAI 2020},
  abstract  = {Process-oriented studies of cooperative learning from an educational neuroscience
perspective has not been firmly quantified experimentally. Within a modeling approach
aimed at the development of a systems dynamics model of affect and cognition, the
goal of this exploratory study is to identify typical timescales of variation for
continuous metrics of affect (Frontal Alpha Asymmetry (FAA): valence) and cognition
(Cognitive Load (CL); Index of Cognitive Engagement (ICE); Frontal Midline Theta (FMT):
attention). These metrics were obtained from 72 participants paired in dyads (player
and watcher) from whom electroencephalography (EEG) was recorded for 2 hours while
one participant was playing a serious game to learn Physics, and the other one was
watching passively. The results show rather slow cyclical variation for every metric
tested, accompanied in certain cases by short bursts of faster variations. This result
converges with [Newell 1990] cognitive architecture assuming that psychophysiological
measures capture activity at higher levels such as operation tasks and operations.
Theoretical, methodological and applied implications are discussed. Also, the need
for further fine-grained analyses of the context and other atypical analyses are expressed.},
  doi       = {10.1145/3439231.3440604},
  isbn      = {9781450389372},
  keywords  = {educational neuroscience, online measures of learning, game-based learning},
  location  = {Online, Portugal},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3439231.3440604},
}

@InProceedings{Shan2019,
  author    = {Shan, Huasong and Chen, Yuan and Liu, Haifeng and Zhang, Yunpeng and Xiao, Xiao and He, Xiaofeng and Li, Min and Ding, Wei},
  booktitle = {The World Wide Web Conference},
  title     = {??-Diagnosis: Unsupervised and Real-Time Diagnosis of Small- Window Long-Tail Latency in Large-Scale Microservice Platforms},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {3215–3222},
  publisher = {Association for Computing Machinery},
  series    = {WWW '19},
  abstract  = {Microservice architectures and container technologies are broadly adopted by giant
internet companies to support their web services, which typically have a strict service-level
objective (SLO), tail latency, rather than average latency. However, diagnosing SLO
violations, e.g., long tail latency problem, is non-trivial for large-scale web applications
in shared microservice platforms due to million-level operational data and complex
operational environments. We identify a new type of tail latency problem for web services,
small-window long-tail latency (SWLT), which is typically aggregated during a small
statistical window (e.g., 1-minute or 1-second). We observe SWLT usually occurs in
a small number of containers in microservice clusters and sharply shifts among different
containers at different time points. To diagnose root-causes of SWLT, we propose an
unsupervised and low-cost diagnosis algorithm-?-Diagnosis, using two-sample test algorithm
and ?-statistics for measuring similarity of time series to identify root-cause metrics
from millions of metrics. We implement and deploy a real-time diagnosis system in
our real-production microservice platforms. The evaluation using real web application
datasets demonstrates that ?-Diagnosis can identify all the actual root-causes at
runtime and significantly reduce the candidate problem space, outperforming other
time-series distance based root-cause analysis algorithms.},
  doi       = {10.1145/3308558.3313653},
  isbn      = {9781450366748},
  keywords  = {Root-cause analysis, time series similarity, tail latency},
  location  = {San Francisco, CA, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3308558.3313653},
}

@InProceedings{Mlotshwa2020,
  author    = {Mlotshwa, Likhwa Lothar and Makura, Sheunesu M. and Karie, Nickson M. and Kebande, Victor R.},
  booktitle = {Proceedings of the 2nd International Conference on Intelligent and Innovative Computing Applications},
  title     = {Opportunistic Security Architecture for Osmotic Computing Paradigm in Dynamic IoT-Edge's Resource Diffusion},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICONIC '20},
  abstract  = {Increased heterogeneity of physical resources has had positive and negative effects
in Internet of Things (IoT) through the existence of edge computing. As a result,
there has been a need for effective dynamic management of IoT, cloud and edge resources,
in order to address the existence of low-level constraints during resource migration.
Nevertheless, the explosion of IoT devices and data has allowed orchestration of microservices
to adopt an opportunistic approach to how applications and services are deployed in
the edge in IoT platform. A notable approach has been osmotic computing that allows
resources from a federated cloud to be able to diffuse from an ecosystem of higher
solute (network properties and entities) concentration to solvent (applications, layered
interfaces and services). We posit that, while computing resources and applications
are able to move from the federated environment, to the cloud deployable models, to
the edge, then to IoT ecosystem, there is a higher chance of susceptibility of threats
and attacks that may be directed to the emerging edge applications/data due to dynamic
emergent configurations. This paper proposes a 5-layer opportunistic architecture
that adds security metrics across different levels of osmotic computing paradigm.
The proposed 5-layer security architecture addresses the need for autonomously securing
resources-edge computation, edge storage and emerging edge configurations as the computing
resources move to a higher solute in heterogenous edge and cloud datacenters across
IoT devices. This has been achieved by proposing security metrics that address the
prevailing challenge with a degree of certainty.},
  articleno = {9},
  doi       = {10.1145/3415088.3415097},
  isbn      = {9781450375580},
  keywords  = {edge, security architecture, opportunistic, osmotic, IoT},
  location  = {Plaine Magnien, Mauritius},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3415088.3415097},
}

@InProceedings{MarceloDornbusch2016,
  author    = {L., Marcelo Dornbusch and Rauta, Leonardo R.P. and Silva, Paulo H. and Silva, Rodrigo C. and Irigoite, Adriano M. and Wangham, Michelle S.},
  booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
  title     = {Remote and Continuous Monitoring of Electrical Quantities Using Web of Things and Cloud Computing},
  year      = {2016},
  address   = {Porto Alegre, BRA},
  pages     = {393–400},
  publisher = {Brazilian Computer Society},
  series    = {SBSI 2016},
  abstract  = {The remote monitoring and control of machines are essential in industrial environments.
Emerging communication technologies such as Web of Things and Machine to Machine Communication
can meet this demand for automation. This paper aims to introduce a solution, called
Smart Meter, for continuous and remote monitoring of electrical quantities, in smart
industrial environments with three-phase systems. The proposed solution uses a resource-oriented
architecture and makes use of a Smart Gateway for communication, RESTful web services
and cloud computing. The solution was integrated with a real case study and evaluated
by software testing. The results obtained demonstrate the feasibility of the solution,
and the correctness of measurements persisted in the cloud.},
  isbn      = {9788576693178},
  keywords  = {Cloud Computing, M2M, Remote Monitoring of Electrical Quantities, Web of Things},
  location  = {Florianopolis, Santa Catarina, Brazil},
  numpages  = {8},
}

@InProceedings{Toffetti2015,
  author    = {Toffetti, Giovanni and Brunner, Sandro and Bl\"{o}chlinger, Martin and Dudouet, Florian and Edmonds, Andrew},
  booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
  title     = {An Architecture for Self-Managing Microservices},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {19–24},
  publisher = {Association for Computing Machinery},
  series    = {AIMC '15},
  abstract  = {Running applications in the cloud efficiently requires much more than deploying software
in virtual machines. Cloud applications have to be continuously managed: 1) to adjust
their resources to the incoming load and 2) to face transient failures replicating
and restarting components to provide resiliency on unreliable infrastructure. Continuous
management monitors application and infrastructural metrics to provide automated and
responsive reactions to failures (health management) and changing environmental conditions
(auto-scaling) minimizing human intervention.In the current practice, management functionalities
are provided as infrastructural or third party services. In both cases they are external
to the application deployment. We claim that this approach has intrinsic limits, namely
that separating management functionalities from the application prevents them from
naturally scaling with the application and requires additional management code and
human intervention. Moreover, using infrastructure provider services for management
functionalities results in vendor lock-in effectively preventing cloud applications
to adapt and run on the most effective cloud for the job.In this position paper we
propose a novel architecture that enables scalable and resilient self-management of
microservices applications on cloud.},
  doi       = {10.1145/2747470.2747474},
  isbn      = {9781450334761},
  location  = {Bordeaux, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2747470.2747474},
}

@Article{Brondolin2020,
  author     = {Brondolin, Rolando and Santambrogio, Marco D.},
  journal    = {ACM Trans. Archit. Code Optim.},
  title      = {A Black-Box Monitoring Approach to Measure Microservices Runtime Performance},
  year       = {2020},
  issn       = {1544-3566},
  month      = nov,
  number     = {4},
  volume     = {17},
  abstract   = {Microservices changed cloud computing by moving the applications’ complexity from
one monolithic executable to thousands of network interactions between small components.
Given the increasing deployment sizes, the architectural exploitation challenges,
and the impact on data-centers’ power consumption, we need to efficiently track this
complexity. Within this article, we propose a black-box monitoring approach to track
microservices at scale, focusing on architectural metrics, power consumption, application
performance, and network performance. The proposed approach is transparent w.r.t.
the monitored applications, generates less overhead w.r.t. black-box approaches available
in the state-of-the-art, and provides fine-grain accurate metrics.},
  address    = {New York, NY, USA},
  articleno  = {34},
  doi        = {10.1145/3418899},
  issue_date = {December 2020},
  keywords   = {network performance monitoring, cloud computing, performance monitoring, docker, power attribution, kubernetes, Microservices},
  numpages   = {26},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3418899},
}

@InProceedings{Boukoros2017,
  author    = {Boukoros, Spyros and Katzenbeisser, Stefan},
  booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
  title     = {Measuring Privacy in High Dimensional Microdata Collections},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ARES '17},
  abstract  = {Microdata is collected by companies in order to enhance their quality of service as
well as the accuracy of their recommendation systems. These data often become publicly
available after they have been sanitized. Recent reidentification attacks on publicly
available, sanitized datasets illustrate the privacy risks involved in microdata collections.
Currently, users have to trust the provider that their data will be safe in case data
is published or if a privacy breach occurs. In this work, we empower users by developing
a novel, user-centric tool for privacy measurement and a new lightweight privacy metric.
The goal of our tool is to estimate users' privacy level prior to sharing their data
with a provider. Hence, users can consciously decide whether to contribute their data.
Our tool estimates an individuals' privacy level based on published popularity statistics
regarding the items in the provider's database, and the users' microdata. In this
work, we describe the architecture of our tool as well as a novel privacy metric,
which is necessary for our setting where we do not have access to the provider's database.
Our tool is user friendly, relying on smart visual results that raise privacy awareness.
We evaluate our tool using three real world datasets, collected from major providers.
We demonstrate strong correlations between the average anonymity set per user and
the privacy score obtained by our metric. Our results illustrate that our tool which
uses minimal information from the provider, estimates users' privacy levels comparably
well, as if it had access to the actual database.},
  articleno = {15},
  doi       = {10.1145/3098954.3098977},
  isbn      = {9781450352574},
  keywords  = {microdata, user empowerment, privacy metrics, privacy},
  location  = {Reggio Calabria, Italy},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3098954.3098977},
}

@InProceedings{Kogias2020,
  author    = {Kogias, Marios and Bugnion, Edouard},
  booktitle = {4th Asia-Pacific Workshop on Networking},
  title     = {Tail-Tolerance as a Systems Principle Not a Metric},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {16–22},
  publisher = {Association for Computing Machinery},
  series    = {APNet '20},
  abstract  = {Tail-latency tolerance (or just simply tail-tolerance) is the ability for a system
to deliver a response with low-latency nearly all the time. It it typically expressed
as a system metric (e.g., the 99th or 99.99th percentile latency) or as a service-level
objective (e.g., the maximum throughput so that the tail latency is below a desired
threshold). We advocate instead that modern datacenter systems should incorporate
tail-tolerance as a core systems design principle and not a metric to be observed,
and that tail-tolerant systems can be built out of large and complex applications
whose individual components may suffer from latency deviations. This is analogous
to fault-tolerance, where a fault-tolerant system can be built out of unreliable components.
The general solution is for the system to control the applied load and keep it under
the threshold that violates the latency SLO. We propose to augment RPC semantics with
an architectural layer that measures the observed tail latency and probabilistically
rejects RPC requests maintaining throughput under the threshold that violates the
SLO. Our design is application-independent, and does not make any assumptions about
the request service time distribution. We implemented a proof of concept for such
a tail-tolerant layer using programmable switches, called SVEN. We demonstrate that
the approach is suitable even for microsecond-scale RPCs with variable service times.
Moreover, our approach does not induce measurable overheads, and can maintain the
maximum achieved throughput very close to the load level that would violate the SLO
without SVEN.},
  doi       = {10.1145/3411029.3411032},
  isbn      = {9781450388764},
  location  = {Seoul, Republic of Korea},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3411029.3411032},
}

@InProceedings{Khan2018,
  author    = {Khan, Junaid Ahmed and Westphal, Cedric and Garcia-Luna-Aceves, J. J. and Ghamri-Doudane, Yacine},
  booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
  title     = {NICE: Network-Oriented Information-Centric Centrality for Efficiency in Cache Management},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {31–42},
  publisher = {Association for Computing Machinery},
  series    = {ICN '18},
  abstract  = {All Information-Centric Networking (ICN) architectures proposed to date aim at connecting
users to content directly, rather than connecting clients to servers. Surprisingly,
however, although content caching is an integral of any information-Centric Network,
limited work has been reported on information-centric management of caches in the
context of an ICN. Indeed, approaches to cache management in networks of caches have
focused on network connectivity rather than proximity to content.We introduce the
Network-oriented Information-centric Centrality for Efficiency (NICE) as a new metric
for cache management in information-centric networks. We propose a method to compute
information-centric centrality that scales with the number of caches in a network
rather than the number of content objects, which is many orders of magnitude larger.
Furthermore, it can be pre-processed offline and ahead of time. We apply the NICE
metric to a content replacement policy in caches, and show that a content replacement
based on NICE exhibits better performances than LRU and other policies based on topology-oriented
definitions of centrality.},
  doi       = {10.1145/3267955.3267965},
  isbn      = {9781450359597},
  keywords  = {content offloading, ICN, graph centrality, cache management},
  location  = {Boston, Massachusetts},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3267955.3267965},
}

@InProceedings{Heinl2019,
  author    = {Heinl, Michael P. and Giehl, Alexander and Wiedermann, Norbert and Plaga, Sven and Kargl, Frank},
  booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
  title     = {MERCAT: A Metric for the Evaluation and Reconsideration of Certificate Authority Trustworthiness},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {1–15},
  publisher = {Association for Computing Machinery},
  series    = {CCSW'19},
  abstract  = {Public key infrastructures (PKIs) build the foundation for secure communication of
a vast majority of cloud services. In the recent past, there has been a series of
security incidents leading to increasing concern regarding the trust model currently
employed by PKIs. One of the key criticisms is the architecture's implicit assumption
that certificate authorities (CAs) are trustworthy a priori.This work proposes a holistic
metric to compensate this assumption by a differentiating assessment of a CA's individual
trustworthiness based on objective criteria. The metric utilizes a wide range of technical
and non-technical factors derived from existing policies, technical guidelines, and
research. It consists of self-contained submetrics allowing the simple extension of
the existing set of criteria. The focus is thereby on aspects which can be assessed
by employing practically applicable methods of independent data collection.The metric
is meant to help organizations, individuals, and service providers deciding which
CAs to trust or distrust. For this, the modularized submetrics are clustered into
coherent submetric groups covering a CA's different properties and responsibilities.
By applying individually chosen weightings to these submetric groups, the metric's
outcomes can be adapted to tailored protection requirements according to an exemplifying
attacker model.},
  doi       = {10.1145/3338466.3358917},
  isbn      = {9781450368261},
  keywords  = {metric, digital certificate, pki, trustworthiness assessment, cloud security, ca, x.509},
  location  = {London, United Kingdom},
  numpages  = {15},
  url       = {https://doi.org/10.1145/3338466.3358917},
}

@InProceedings{Kim2018,
  author    = {Kim, Yubin and Callan, Jamie},
  booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
  title     = {Measuring the Effectiveness of Selective Search Index Partitions without Supervision},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {91–98},
  publisher = {Association for Computing Machinery},
  series    = {ICTIR '18},
  abstract  = {Selective search architectures partition a document collection into topic-oriented
index shards, usually using algorithms that have random components. Different mappings
of documents into index shards (shard maps) produce different search accuracy and
consistency, however identifying which shard maps will deliver the highest average
effectiveness is an open problem. This paper presents a new metric, Area Under Recall
Curve (AUReC), to evaluate and compare shard maps. AUReC is the first such metric
that is independent of resource selection and shard cut-off estimation. It does not
require an end-to-end evaluation or manual gold-standard judgements. Experiments show
that its predictions are highly-correlated with evaluating end-to-end systems of various
configurations, while being easier to implement and computationally inexpensive.},
  doi       = {10.1145/3234944.3234952},
  isbn      = {9781450356565},
  keywords  = {clustering, cluster-based retrieval, evaluation, distributed search, selective search},
  location  = {Tianjin, China},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3234944.3234952},
}

@Article{Tomusk2015,
  author     = {Tomusk, Erik and Dubach, Christophe and O’boyle, Michael},
  journal    = {ACM Trans. Archit. Code Optim.},
  title      = {Four Metrics to Evaluate Heterogeneous Multicores},
  year       = {2015},
  issn       = {1544-3566},
  month      = nov,
  number     = {4},
  volume     = {12},
  abstract   = {Semiconductor device scaling has made single-ISA heterogeneous processors a reality.
Heterogeneous processors contain a number of different CPU cores that all implement
the same Instruction Set Architecture (ISA). This enables greater flexibility and
specialization, as runtime constraints and workload characteristics can influence
which core a given workload is run on. A major roadblock to the further development
of heterogeneous processors is the lack of appropriate evaluation metrics. Existing
metrics can be used to evaluate individual cores, but to evaluate a heterogeneous
processor, the cores must be considered as a collective. Without appropriate metrics,
it is impossible to establish design goals for processors, and it is difficult to
accurately compare two different heterogeneous processors.We present four new metrics
to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity,
gap overhead, set overhead, and generality. The metrics consider sets rather than
individual cores. We use examples to demonstrate each metric, and show that the metrics
can be used to quantify intuitions about heterogeneous cores.},
  address    = {New York, NY, USA},
  articleno  = {37},
  doi        = {10.1145/2829950},
  issue_date = {January 2016},
  keywords   = {gap overhead, effective speed, single-ISA, generality, Localized nonuniformity, set overhead},
  numpages   = {25},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2829950},
}

@InProceedings{Luebke2015,
  author    = {L\"{u}bke, Daniel},
  booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
  title     = {Using Metric Time-Lines for Identifying Architecture Shortcomings in Process Execution Architectures},
  year      = {2015},
  pages     = {55–58},
  publisher = {IEEE Press},
  series    = {SAM '15},
  abstract  = {Process Execution with Service Orchestrations is an emerging architectural style for
developing business software systems. However, few special metrics for guiding software
architecture decisions have been proposed and no existing business process metrics
have been evaluated for their suitability. By following static code metrics over time,
architects can gain a better understanding, how processes and the whole system evolve
and whether the metrics evolve as expected. This allows architects to recogniize when
to intervene in the development and make architecture adjustments or refactorings.
This paper presents an explatory study that uses time-lines of static process size
metrics for constant feedback to software architects that deal with process-oriented
architectures.},
  location  = {Florence, Italy},
  numpages  = {4},
}

@InProceedings{Boiarov2019,
  author    = {Boiarov, Andrei and Tyantov, Eduard},
  booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  title     = {Large Scale Landmark Recognition via Deep Metric Learning},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {169–178},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '19},
  abstract  = {This paper presents a novel approach for landmark recognition in images that we've
successfully deployed at Mail.ru. This method enables us to recognize famous places,
buildings, monuments, and other landmarks in user photos. The main challenge lies
in the fact that it's very complicated to give a precise definition of what is and
what is not a landmark. Some buildings, statues and natural objects are landmarks;
others are not. There's also no database with a fairly large number of landmarks to
train a recognition model. A key feature of using landmark recognition in a production
environment is that the number of photos containing landmarks is extremely small.
This is why the model should have a very low false positive rate as well as high recognition
accuracy. We propose a metric learning-based approach that successfully deals with
existing challenges and efficiently handles a large number of landmarks. Our method
uses a deep neural network and requires a single pass inference that makes it fast
to use in production. We also describe an algorithm for cleaning landmarks database
which is essential for training a metric learning model. We provide an in-depth description
of basic components of our method like neural network architecture, the learning strategy,
and the features of our metric learning approach. We show the results of proposed
solutions in tests that emulate the distribution of photos with and without landmarks
from a user collection. We compare our method with others during these tests. The
described system has been deployed as a part of a photo recognition solution at Cloud
Mail.ru, which is the photo sharing and storage service at Mail.ru Group.},
  doi       = {10.1145/3357384.3357956},
  isbn      = {9781450369763},
  keywords  = {metric learning, landmark recognition, deep learning},
  location  = {Beijing, China},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3357384.3357956},
}

@InProceedings{Schmidts2018,
  author    = {Schmidts, Oliver and Kraft, Bodo and Schreiber, Marc and Z\"{u}ndorf, Albert},
  booktitle = {Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice},
  title     = {Continuously Evaluated Research Projects in Collaborative Decoupled Environments},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {2–9},
  publisher = {Association for Computing Machinery},
  series    = {SER&amp;IP '18},
  abstract  = {Often, research results from collaboration projects are not transferred into productive
environments even though approaches are proven to work in demonstration prototypes.
These demonstration prototypes are usually too fragile and error-prone to be transferred
easily into productive environments. A lot of additional work is required.Inspired
by the idea of an incremental delivery process, we introduce an architecture pattern,
which combines the approach of Metrics Driven Research Collaboration with microservices
for the ease of integration. It enables keeping track of project goals over the course
of the collaboration while every party may focus on their expert skills: researchers
may focus on complex algorithms, practitioners may focus on their business goals.Through
the simplified integration (intermediate) research results can be introduced into
a productive environment which enables getting an early user feedback and allows for
the early evaluation of different approaches. The practitioners' business model benefits
throughout the full project duration.},
  doi       = {10.1145/3195546.3195549},
  isbn      = {9781450357449},
  keywords  = {lean software development, software architecture, metrics, research best practices, research collaboration management},
  location  = {Gothenburg, Sweden},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3195546.3195549},
}

@InProceedings{Durelli2017,
  author    = {Durelli, Rafael S. and Viana, Matheus C. and de S. Landi, Andr\'{e} and Durelli, Vinicius H. S. and Delamaro, Marcio E. and de Camargo, Valter V.},
  booktitle = {Proceedings of the 31st Brazilian Symposium on Software Engineering},
  title     = {Improving the Structure of KDM Instances via Refactorings: An Experimental Study Using KDM-RE},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {174–183},
  publisher = {Association for Computing Machinery},
  series    = {SBES'17},
  abstract  = {Architecture-Driven Modernization (ADM) is an initiative of the Object Management
Group (OMG) whose main purpose is to provide standard metamodels for software modernization
activities. The most important metamodel is the Knowledge Discovery Metamodel (KDM),
which represents software artifacts in a language-agnostic fashion. A fundamental
step in software modernization is refactoring. However, there is a lack of tools that
address how refactoring can be applied in conjunction with ADM. We developed a tool,
called KDM-RE, that supports refactorings in KDM instances through: (i) a set of wizards
that aid the software modernization engineer during refactoring activities; (ii) a
change propagation module that keeps the internal metamodels synchronized; and (iii)
the selection and application of refactorings available in its repository. This paper
evaluates the application of refactorings to KDM instances in an experiment involving
seven systems implemented in Java. We compared the pre-refactoring versions of these
systems with the refactored ones using the Quality Model for Object-Oriented Design
(QMOOD) metric set. The results from this evaluation suggest that KDM-RE provides
advantages to software modernization engineers refactoring systems represented as
KDMs.},
  doi       = {10.1145/3131151.3131153},
  isbn      = {9781450353267},
  keywords  = {Knowledge-Discovery Metamodel, Model-Driven Development, Refactoring, Architecture-Driven Modernization},
  location  = {Fortaleza, CE, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3131151.3131153},
}

@Article{Arcuri2019,
  author     = {Arcuri, Andrea},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  title      = {RESTful API Automated Test Case Generation with EvoMaster},
  year       = {2019},
  issn       = {1049-331X},
  month      = jan,
  number     = {1},
  volume     = {28},
  abstract   = {RESTful APIs are widespread in industry, especially in enterprise applications developed
with a microservice architecture. A RESTful web service will provide data via an API
over the network using HTTP, possibly interacting with databases and other web services.
Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP
requests/responses to a remote server. Many approaches in the literature do black-box
testing, because often the tested API is a remote service whose code is not available.
In this article, we consider testing from the point of view of the developers, who
have full access to the code that they are writing. Therefore, we propose a fully
automated white-box testing approach, where test cases are automatically generated
using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding
metrics. However, REST is not a protocol but rather a set of guidelines on how to
design resources accessed over HTTP endpoints. For example, there are guidelines on
how related resources should be structured with hierarchical URIs and how the different
HTTP verbs should be used to represent well-defined actions on those resources. Test-case
generation for RESTful APIs that only rely on white-box information of the source
code might not be able to identify how to create prerequisite resources needed before
being able to test some of the REST endpoints. Smart sampling techniques that exploit
the knowledge of best practices in RESTful API design are needed to generate tests
with predefined structures to speed up the search. We implemented our technique in
a tool called EvoMaster, which is open source. Experiments on five open-source, yet
non-trivial, RESTful services show that our novel technique automatically found 80
real bugs in those applications. However, obtained code coverage is lower than the
one achieved by the manually written test suites already existing in those services.
Research directions on how to further improve such an approach are therefore discussed,
such as the handling of SQL databases.},
  address    = {New York, NY, USA},
  articleno  = {3},
  doi        = {10.1145/3293455},
  issue_date = {February 2019},
  keywords   = {Software engineering, testing, REST, web service},
  numpages   = {37},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3293455},
}

@Article{Yu2019,
  author     = {Yu, Tuo and Nahrstedt, Klara},
  journal    = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  title      = {ShoesHacker: Indoor Corridor Map and User Location Leakage through Force Sensors in Smart Shoes},
  year       = {2019},
  month      = sep,
  number     = {3},
  volume     = {3},
  abstract   = {The past few years have witnessed the rise of smart shoes, the wearable devices that
measure foot force or track foot motion. However, people are not aware of the possible
privacy leakage from in-shoe force sensors. In this paper, we explore the possibility
of locating an indoor victim based on the force signals leaked from smart shoes. We
present ShoesHacker, an attack scheme that reconstructs the corridor map of the building
that the victim walks in based on force data only. The corridor map enables the attacker
to recognize the building, and thus locate the victim on a global map. To handle the
lack of training data, we design the stair landing detection algorithm, based on which
we extract training data when victims are walking in stairwells. We estimate the trajectory
of each walk, and propose the path merging algorithm to merge the trajectories. Moreover,
we design a metric to quantify the similarity between corridor maps, which makes building
recognition possible. Our experimental results show that, the building recognition
accuracy reaches 77.5% in a 40-building dataset, and the victim can be located with
an average error lower than 6 m, which reveals the danger of privacy leakage through
smart shoes. CCS Concepts: • Information systems~Mobile information processing systems;
Location based services; • Human-centered computing~Mobile devices; Ubiquitous and
mobile computing systems and tools; • Security and privacy~Domain-specific security
and privacy architectures.},
  address    = {New York, NY, USA},
  articleno  = {120},
  doi        = {10.1145/3351278},
  issue_date = {September 2019},
  keywords   = {smart shoes, force sensors, Corridor map reconstruction},
  numpages   = {29},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3351278},
}

@InProceedings{Staar2018,
  author    = {Staar, Peter W J and Dolfi, Michele and Auer, Christoph and Bekas, Costas},
  booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
  title     = {Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {774–782},
  publisher = {Association for Computing Machinery},
  series    = {KDD '18},
  abstract  = {Over the past few decades, the amount of scientific articles and technical literature
has increased exponentially in size. Consequently, there is a great need for systems
that can ingest these documents at scale and make the contained knowledge discoverable.
Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images)
as well as the presentation of the data (e.g. complex tables) make the extraction
of qualitative and quantitive data extremely challenging. In this paper, we present
a modular, cloud-based platform to ingest documents at scale. This platform, called
the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse
and annotate documents (i.e. collect ground-truth), train machine-learning classification
algorithms and ultimately convert any type of PDF or bitmap-documents to a structured
content representation format. We will show that each of the modules is scalable due
to an asynchronous microservice architecture and can therefore handle massive amounts
of documents. Furthermore, we will show that our capability to gather groundtruth
is accelerated by machine-learning algorithms by at least one order of magnitude.
This allows us to both gather large amounts of ground-truth in very little time and
obtain very good precision/recall metrics in the range of 99% with regard to content
conversion to structured output. The CCS platform is currently deployed on IBM internal
infrastructure and serving more than 250 active users for knowledge-engineering project
engagements.},
  doi       = {10.1145/3219819.3219834},
  isbn      = {9781450355520},
  keywords  = {ibm research, machine learning, deep learning, artificial intelligence, ibm, cloud architecture, table processing, knowledge ingestion, cloud computing, ai, asynchronous architecture, pdf, document conversion},
  location  = {London, United Kingdom},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3219819.3219834},
}

@InProceedings{CalcinaCcori2017,
  author    = {Calcina-Ccori, Pablo and Costa, Laisa and Fedrecheski, Geovane and Esquiagola, John and Zuffo, Marcelo and da Silva, Fl\'{a}vio Corr\^{e}a},
  booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
  title     = {Agile Servient Integration with the Swarm: Automatic Code Generation for Nodes in the Internet of Things},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICFNDS '17},
  abstract  = {Swarm vision, consists in an organic ecosystem of heterogeneous devices that communicate
and collaborate to achieve complex results. In previous work, we have proposed an
architecture to implement this vision based on web technologies. In this paper, we
have proposed a framework that makes the creation of Swarm-ready servients (devices
that acts both as server and client) easier, by generating a ready-to-run project
from a high-level description of the service. The project generated contains all dependencies
and libraries needed to integrate an IoT device into the Swarm, thus saving development
and configuration time. We compared the development effort of creating a servient
by hand and by using our framework, having the number of lines of code as a metric.
Our results show a reduction of 500% in the development effort to connect a device
to the Swarm. The next steps include a semantic high-level description for participating
services and support for resource-constrained devices.},
  articleno = {30},
  doi       = {10.1145/3102304.3102334},
  isbn      = {9781450348447},
  keywords  = {Swarm, automatic code generation, Servient, Internet of Things},
  location  = {Cambridge, United Kingdom},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3102304.3102334},
}

@InProceedings{Liu2018,
  author    = {Liu, Jiaming and Xian, Cuiling},
  booktitle = {Proceedings of the 2018 International Conference on Information Hiding and Image Processing},
  title     = {Extracting Information of Urban Land Surface with High Resolution Remote Sensing Data},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {62–67},
  publisher = {Association for Computing Machinery},
  series    = {IHIP 2018},
  abstract  = {With the advantages of high spatial resolution and definition, and rich information,
land classification and land utilization of urban surface can be reached by using
the high resolution remote sensing data. Based on the high resolution remote sensing
image data Worldview-1 and Worldview-2 as the main data source, using the methods
of stereo images, object-oriented land use classification technique and architecture
shadow, this study (1) extracts 5-meter resolution DEM data of Shilipu district in
Wuhan and the average relative error compared with the measured DEM data of ground
points is 3.71% with relatively high precision, (2) obtains the land use information
of this area with an accuracy rate of 90%, and (3) achieve the data of building height
in this area with the relative error of less than 20%. The results of this paper show
that the high speed and precision can meet the 3d modeling elevation precision and
plane precision of digital urban buildings when using high-resolution remote sensing
images to extract basic geographic information in small urban areas, which will play
an important role in the research of urban surface information extraction in the future.},
  doi       = {10.1145/3292425.3292428},
  isbn      = {9781450365468},
  keywords  = {High resolution, urban surface, remote sensing},
  location  = {Manchester, United Kingdom},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3292425.3292428},
}

@Article{Sahoo2020,
  author     = {Sahoo, Kshira Sagar and Puthal, Deepak},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {SDN-Assisted DDoS Defense Framework for the Internet of Multimedia Things},
  year       = {2020},
  issn       = {1551-6857},
  month      = dec,
  number     = {3s},
  volume     = {16},
  abstract   = {The Internet of Things is visualized as a fundamental networking model that bridges
the gap between the cyber and real-world entity. Uniting the real-world object with
virtualization technology is opening further opportunities for innovation in nearly
every individual’s life. Moreover, the usage of smart heterogeneous multimedia devices
is growing extensively. These multimedia devices that communicate among each other
through the Internet form a unique paradigm called the Internet of Multimedia Things
(IoMT). As the volume of the collected data in multimedia application increases, the
security, reliability of communications, and overall quality of service need to be
maintained. Primarily, distributed denial of service attacks unveil the pervasiveness
of vulnerabilities in IoMT systems. However, the Software Defined Network (SDN) is
a new network architecture that has the central visibility of the entire network,
which helps to detect any attack effectively. In this regard, the combination of SDN
and IoMT, termed SD-IoMT, has the immense ability to improve the network management
and security capabilities of the IoT system. This article proposes an SDN-assisted
two-phase detection framework, namely SD-IoMT-Protector, in which the first phase
utilizes the entropy technique as the detection metric to verify and alert about the
malicious traffic. The second phase has trained with an optimized machine learning
technique for classifying different attacks. The outcomes of the experimental results
signify the usefulness and effectiveness of the proposed framework for addressing
distributed denial of service issues of the SD-IoMT system.},
  address    = {New York, NY, USA},
  articleno  = {98},
  doi        = {10.1145/3394956},
  issue_date = {January 2021},
  keywords   = {machine learning, IoMT, entropy, Control plane, security, SDN},
  numpages   = {18},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3394956},
}

@InProceedings{Scrocca2020,
  author    = {Scrocca, Mario and Tommasini, Riccardo and Margara, Alessandro and Valle, Emanuele Della and Sakr, Sherif},
  booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
  title     = {The Kaiju Project: Enabling Event-Driven Observability},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {85–96},
  publisher = {Association for Computing Machinery},
  series    = {DEBS '20},
  abstract  = {Microservices architectures are getting momentum. Even small and medium-size companies
are migrating towards cloud-based distributed solutions supported by lightweight virtualization
techniques, containers, and orchestration systems. In this context, understanding
the system behavior at runtime is critical to promptly react to errors. Unfortunately,
traditional monitoring techniques are not adequate for such complex and dynamic environments.
Therefore, a new challenge, namely observability, emerged from precise industrial
needs: expose and make sense of the system behavior at runtime. In this paper, we
investigate observability as a research problem. We discuss the benefits of events
as a unified abstraction for metrics, logs, and trace data, and the advantages of
employing event stream processing techniques and tools in this context. We show that
an event-based approach enables understanding the system behavior in near real-time
more effectively than state-of-the-art solutions in the field. We implement our model
in the Kaiju system and we validate it against a realistic deployment supported by
a software company.},
  doi       = {10.1145/3401025.3401740},
  isbn      = {9781450380287},
  keywords  = {event stream processing, orchestration systems, observability, event-based systems},
  location  = {Montreal, Quebec, Canada},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3401025.3401740},
}

@InProceedings{Buevich2014,
  author    = {Buevich, Maxim and Schnitzer, Dan and Escalada, Tristan and Jacquiau-Chamski, Arthur and Rowe, Anthony},
  booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
  title     = {Fine-Grained Remote Monitoring, Control and Pre-Paid Electrical Service in Rural Microgrids},
  year      = {2014},
  pages     = {1–12},
  publisher = {IEEE Press},
  series    = {IPSN '14},
  abstract  = {In this paper, we present the architecture, design and experiences from a wirelessly
managed microgrid deployment in rural Les Anglais, Haiti. The system consists of a
three-tiered architecture with a cloud-based monitoring and control service, a local
embedded gateway infrastructure and a mesh network of wireless smart meters deployed
at 52 buildings. Each smart meter device has an 802.15.4 radio that enables remote
monitoring and control of electrical service. The meters communicate over a scalable
multi-hop TDMA network back to a central gateway that manages load within the system.
The gateway also provides an 802.11 interface for an on-site operator and a cellular
modem connection to a cloud-backend that manages and stores billing and usage data.
The cloud backend allows occupants in each home to pre-pay for electricity at a particular
peak power limit using a text messaging service. The system activates each meter within
seconds and locally enforces power limits with provisioning for theft detection. We
believe that this fine-grained micro-payment model can enable sustainable power in
otherwise unfeasible areas.This paper provides a chronology of our deployment and
installation strategy that involved GPS-based site mapping along with various network
conditioning actions required as the network evolved. Finally, we summarize key lessons
learned and hypothesis about additional hardware that could be used to ease the tracing
of faults like short circuits and downed lines within microgrids.},
  isbn      = {9781479931460},
  keywords  = {wireless local area networks, sensor networks, microgrid},
  location  = {Berlin, Germany},
  numpages  = {12},
}

@InProceedings{Nagendra2019,
  author    = {Nagendra, Vasudevan and Yegneswaran, Vinod and Porras, Phillip and Das, Samir R},
  booktitle = {Proceedings of the 35th Annual Computer Security Applications Conference},
  title     = {Coordinated Dataflow Protection for Ultra-High Bandwidth Science Networks},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {568–583},
  publisher = {Association for Computing Machinery},
  series    = {ACSAC '19},
  abstract  = {The Science DMZ (SDMZ) is a special purpose network architecture proposed by ESnet
(Energy Sciences Network) to facilitate distributed science experimentation on terabyte-
(or petabyte-) scale data, exchanged over ultra-high bandwidth WAN links. Critical
security challenges faced by these networks include: (i) network monitoring at high
bandwidths, (ii) reconciling site-specific policies with project-level policies for
conflict-free policy enforcement, (iii) dealing with geographically-distributed datasets
with varying levels of sensitivity, and (iv) dynamically enforcing appropriate security
rules. To address these challenges, we develop a fine-grained dataflow-based security
enforcement system, called CoordiNetZ (CNZ), that provides coordinated situational
awareness, i.e., the use of context-aware tagging for policy enforcement using the
dynamic contextual information derived from hosts and network elements. We also developed
tag and IP-based security microservices that incur minimal overheads in enforcing
security to data flows exchanged across geographically-distributed SDMZ sites. We
evaluate our prototype implementation across two geographically distributed SDMZ sites
with SDN-based case studies, and present performance measurements that respectively
highlight the utility of our framework and demonstrate efficient implementation of
security policies across distributed SDMZ networks.},
  doi       = {10.1145/3359789.3359843},
  isbn      = {9781450376280},
  keywords  = {usability and human-centric aspects of security, distributed systems security, software-defined programmable security, NFV, big data security, network security, SDN},
  location  = {San Juan, Puerto Rico, USA},
  numpages  = {16},
  url       = {https://doi.org/10.1145/3359789.3359843},
}

@Article{Bouraoui2017,
  author     = {Bouraoui, Hasna and Jerad, Chadlia and Chattopadhyay, Anupam and Hadj-Alouane, Nejib Ben},
  journal    = {ACM Trans. Embed. Comput. Syst.},
  title      = {Hardware Architectures for Embedded Speaker Recognition Applications: A Survey},
  year       = {2017},
  issn       = {1539-9087},
  month      = apr,
  number     = {3},
  volume     = {16},
  abstract   = {Authentication technologies based on biometrics, such as speaker recognition, are
attracting more and more interest thanks to the elevated level of security offered
by these technologies. Despite offering many advantages, such as remote use and low
vulnerability, speaker recognition applications are constrained by the heavy computational
effort and the hard real-time constraints. When such applications are run on an embedded
platform, the problem becomes more challenging, as additional constraints inherent
to this specific domain are added. In the literature, different hardware architectures
were used/designed for implementing a process with a focus on a given particular metric.
In this article, we give a survey of the state-of-the-art works on implementations
of embedded speaker recognition applications. Our aim is to provide an overview of
the different approaches dealing with acceleration techniques oriented towards speaker
and speech recognition applications and attempt to identify the past, current, and
future research trends in the area. Indeed, on the one hand, many flexible solutions
were implemented, using either General Purpose Processors or Digital Signal Processors.
In general, these types of solutions suffer from low area and energy efficiency. On
the other hand, high-performance solutions were implemented on Application Specific
Integrated Circuits or Field Programmable Gate Arrays but at the expense of flexibility.
Based on the available results, we compare the application requirements vis-\`{a}-vis
the performance achieved by the systems. This leads to the projection of new research
trends that can be undertaken in the future.},
  address    = {New York, NY, USA},
  articleno  = {78},
  doi        = {10.1145/2975161},
  issue_date = {July 2017},
  keywords   = {Embedded hardware, acceleration, classification algorithms and implementations, speaker recognition},
  numpages   = {28},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2975161},
}

@InProceedings{Torquato2021,
  author    = {Torquato, Matheus and Maciel, Paulo and Vieira, Marco},
  booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
  title     = {Analysis of VM Migration Scheduling as Moving Target Defense against Insider Attacks},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {194–202},
  publisher = {Association for Computing Machinery},
  series    = {SAC '21},
  abstract  = {As cybersecurity threats evolve, cloud computing defenses must adapt to face new challenges.
Unfortunately, due to resource sharing, cloud computing platforms open the door for
insider attacks, which consist of malicious actions from cloud authorized users (e.g.,
clients of an Infrastructure-as-a-Service (IaaS) cloud) targeting the co-hosted users
or the underlying provider environment. Virtual machine (VM) migration is a Moving
Target Defense (MTD) technique to mitigate insider attacks effects, as it provides
VMs positioning manageability. However, there is a clear demand for studies quantifying
the security benefits of VM migration-based MTD considering different system architecture
configurations. This paper tries to fill such a gap by presenting a Stochastic Reward
Net model for the security evaluation of a VM migration-based MTD. The security metric
of interest is the probability of attack success. We consider multiple architectures,
ranging from one physical machine pool (without MTD) up to four physical machine pools.
The evaluation also considers the unavailability due to VM migration. The key contributions
are i) a set of results highlighting the probability of insider attacks success over
time in different architectures and VM migration schedules, and ii) suggestions for
selecting VMs as candidates for MTD deployment based on the tolerance levels of the
attack success probability. The results are validated against simulation results to
confirm the accuracy of the model.},
  doi       = {10.1145/3412841.3441899},
  isbn      = {9781450381048},
  keywords  = {VM migration, moving target defense, availability, stochastic petri nets, migration-based dynamic platform},
  location  = {Virtual Event, Republic of Korea},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3412841.3441899},
}

@InProceedings{Kim2018a,
  author    = {Kim, Heejin and Jeon, Seil and Raza, Syed M. and Lee, Joohyun and Choo, Hyunseung},
  booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
  title     = {Service-Aware Split Point Selection for User-Centric Mobility Enhancement in SDN},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {IMCOM '18},
  abstract  = {IP mobility anchor works as the redirection/split point of the packet destined to
the mobile terminal (MT), as well as IP address/prefix assignment and mobility binding
management in the legacy mobility management protocols. In software-defined networking
(SDN), the split point can be managed by the SDN controller or controller application,
as the control of the network is separated from the forwarding entities. The demand
of user QoE is ever increasing and they always want to get the best service continuity
served in mobility management. Differentiated split point selection per service type
could be one of the effective measures to enhance user QoE in a mobility management
environment. In this paper, we propose a service-aware split point selection mechanism
for user-centric mobility management enhancement in SDN. Specifically, we propose
the mobility control architecture, which can classify service flow type and determine
advantageous split point depending on a service flow type. We analyze the performance
of the proposed split point selection mechanism compared to target mechanisms. We
also measure the performance metrics on an ONOS-based SDN testbed to identify the
superiority of the proposed mechanism.},
  articleno = {95},
  doi       = {10.1145/3164541.3164576},
  isbn      = {9781450363853},
  keywords  = {mobility management, split point selection, software-defined networking},
  location  = {Langkawi, Malaysia},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3164541.3164576},
}

@Article{Izadpanah2019,
  author     = {Izadpanah, Ramin and Allan, Benjamin A. and Dechev, Damian and Brandt, Jim},
  journal    = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
  title      = {Production Application Performance Data Streaming for System Monitoring},
  year       = {2019},
  issn       = {2376-3639},
  month      = apr,
  number     = {2},
  volume     = {4},
  abstract   = {In this article, we present an approach to streaming collection of application performance
data. Practical application performance tuning and troubleshooting in production high-performance
computing (HPC) environments requires an understanding of how applications interact
with the platform, including (but not limited to) parallel programming libraries such
as Message Passing Interface (MPI). Several profiling and tracing tools exist that
collect heavy runtime data traces either in memory (released only at application exit)
or on a file system (imposing an I/O load that may interfere with the performance
being measured). Although these approaches are beneficial in development stages and
post-run analysis, a systemwide and low-overhead method is required to monitor deployed
applications continuously. This method must be able to collect information at both
the application and system levels to yield a complete performance picture.In our approach,
an application profiler collects application event counters. A sampler uses an efficient
inter-process communication method to periodically extract the application counters
and stream them into an infrastructure for performance data collection. We implement
a tool-set based on our approach and integrate it with the Lightweight Distributed
Metric Service (LDMS) system, a monitoring system used on large-scale computational
platforms. LDMS provides the infrastructure to create and gather streams of performance
data in a low overhead manner. We demonstrate our approach using applications implemented
with MPI, as it is one of the most common standards for the development of large-scale
scientific applications.We utilize our tool-set to study the impact of our approach
on an open source HPC application, Nalu. Our tool-set enables us to efficiently identify
patterns in the behavior of the application without source-level knowledge. We leverage
LDMS to collect system-level performance data and explore the correlation between
the system and application events. Also, we demonstrate how our tool-set can help
detect anomalies with a low latency. We run tests on two different architectures:
a system enabled with Intel Xeon Phi and another system equipped with Intel Xeon processor.
Our overhead study shows our method imposes at most 0.5% CPU usage overhead on the
application in realistic deployment scenarios.},
  address    = {New York, NY, USA},
  articleno  = {8},
  doi        = {10.1145/3319498},
  issue_date = {June 2019},
  keywords   = {application profiling, Application and system monitoring, performance data streaming},
  numpages   = {25},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3319498},
}

@InProceedings{Fayed2021,
  author    = {Fayed, Marwan and Bauer, Lorenz and Giotsas, Vasileios and Kerola, Sami and Majkowski, Marek and Odintsov, Pavel and Sitnicki, Jakub and Chung, Taejoong and Levin, Dave and Mislove, Alan and Wood, Christopher A. and Sullivan, Nick},
  booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
  title     = {The Ties That Un-Bind: Decoupling IP from Web Services and Sockets for Robust Addressing Agility at CDN-Scale},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {433–446},
  publisher = {Association for Computing Machinery},
  series    = {SIGCOMM '21},
  abstract  = {The couplings between IP addresses, names of content or services, and socket interfaces,
are too tight. This impedes system manageability, growth, and overall provisioning.
In turn, large-scale content providers are forced to use staggering numbers of addresses,
ultimately leading to address exhaustion (IPv4) and inefficiency (IPv6).In this paper,
we revisit IP bindings, entirely. We attempt to evolve addressing conventions by decoupling
IP in DNS and from network sockets. Alongside technologies such as SNI and ECMP, a
new architecture emerges that ``unbinds'' IP from services and servers, thereby returning
IP's role to merely that of reachability. The architecture is under evaluation at
a major CDN in multiple datacenters. We show that addresses can be generated randomly
emph{per-query}, for 20M+ domains and services, from as few as ~4K addresses, 256
addresses, and even emph{one} IP address. We explain why this approach is transparent
to routing, L4/L7 load-balancers, distributed caching, and all surrounding systems
-- and is emph{highly desirable}. Our experience suggests that many network-oriented
systems and services (e.g., route leak mitigation, denial of service, measurement)
could be improved, and new ones designed, if built with addressing agility.},
  doi       = {10.1145/3452296.3472922},
  isbn      = {9781450383837},
  keywords  = {programmable sockets, provisioning, addressing, content distribution},
  location  = {Virtual Event, USA},
  numpages  = {14},
  url       = {https://doi.org/10.1145/3452296.3472922},
}

@InProceedings{Nikravesh2015,
  author    = {Nikravesh, Ali Yadavar and Ajila, Samuel A. and Lung, Chung-Horng},
  booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
  title     = {Towards an Autonomic Auto-Scaling Prediction System for Cloud Resource Provisioning},
  year      = {2015},
  pages     = {35–45},
  publisher = {IEEE Press},
  series    = {SEAMS '15},
  abstract  = {This paper investigates the accuracy of predictive auto-scaling systems in the Infrastructure
as a Service (IaaS) layer of cloud computing. The hypothesis in this research is that
prediction accuracy of auto-scaling systems can be increased by choosing an appropriate
time-series prediction algorithm based on the performance pattern over time. To prove
this hypothesis, an experiment has been conducted to compare the accuracy of time-series
prediction algorithms for different performance patterns. In the experiment, workload
was considered as the performance metric, and Support Vector Machine (SVM) and Neural
Networks (NN) were utilized as time-series prediction techniques. In addition, we
used Amazon EC2 as the experimental infrastructure and TPC-W as the benchmark to generate
different workload patterns. The results of the experiment show that prediction accuracy
of SVM and NN depends on the incoming workload pattern of the system under study.
Specifically, the results show that SVM has better prediction accuracy in the environments
with periodic and growing workload patterns, while NN outperforms SVM in forecasting
unpredicted workload pattern. Based on these experimental results, this paper proposes
an architecture for a self-adaptive prediction suite using an autonomic system approach.
This suite can choose the most suitable prediction technique based on the performance
pattern, which leads to more accurate prediction results.},
  keywords  = {workload pattern, neural networks, resource provisioning, support vector machine, cloud computing, auto-scaling, autonomic},
  location  = {Florence, Italy},
  numpages  = {11},
}

@InProceedings{Dudouet2015,
  author    = {Dudouet, Florian and Edmonds, Andrew and Erne, Michael},
  booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
  title     = {Reliable Cloud-Applications: An Implementation through Service Orchestration},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {1–6},
  publisher = {Association for Computing Machinery},
  series    = {AIMC '15},
  abstract  = {As cloud-deployed applications became more and more mainstream, continuously more
complex services started to be deployed; indeed where initially monolithic applications
were simply ported to the cloud, applications are now more and more often composed
of micro-services. This improves the flexibility of an application but also makes
it more complex due to the sheer number of services comprising it.As deployment and
runtime management becomes more complex, orchestration software are becoming necessary
to completely manage the lifecycle of cloud applications. One crucial problem remaining
is how these applications can be made reliable in the cloud, a naturally unreliable
environment.In this paper we propose concepts and architectures which were implemented
in our orchestration software to guarantee reliability. Our initial implementation
also relies on Monasca, a well-known monitoring software for Open-Stack, to gather
proper metric and execute threshold-based actions. This allows us to show how service
reliability can be ensured using orchestration and how a proper incident-management
software feeding decisions to the orchestration engine ensures high-availability of
all components of managed applications.},
  doi       = {10.1145/2747470.2747471},
  isbn      = {9781450334761},
  keywords  = {incident management, orchestration, cloud computing, reliability},
  location  = {Bordeaux, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2747470.2747471},
}

@InProceedings{Albataineh2019,
  author    = {Albataineh, Abdallah and Al-Qassas, Raad S. and Qasaimeh, Malik},
  booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
  title     = {A New Architecture for Voice Interconnection Using Packet Switched Network},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {DATA '19},
  abstract  = {Interconnecting voice service providers require a mutual trust between communicating
entities, which are built either using bilateral agreements or intermediary service
provider. To achieve such relationship between Anonymous Service Providers we should
have an automated mechanism. In this paper, we propose a conceptual architecture that
can build such relationship between communicating Anonymous Service Providers. By
applying this architecture, we argue that we can increase efficiency, security, and
performance of service provider's networks. The impact of internet speed on the interconnection
network is measured using key metrics including ACD, ASR, PDD, NER, and MOS.},
  articleno = {13},
  doi       = {10.1145/3368691.3368704},
  isbn      = {9781450372848},
  keywords  = {voice network architecture, SS7, ACD, PDD, ASR, SIP},
  location  = {Dubai, United Arab Emirates},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3368691.3368704},
}

@InProceedings{Lin2014,
  author    = {Lin, Chih-Lu and Chen, Ying-Liang and Kao, Hung-Yu},
  booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
  title     = {Question Difficulty Evaluation by Knowledge Gap Analysis in Question Answer Communities},
  year      = {2014},
  pages     = {336–339},
  publisher = {IEEE Press},
  series    = {ASONAM '14},
  abstract  = {The Community Question Answer (CQA) service is a typical forum of Web 2.0 that shares
knowledge among people. There are thousands of questions that are posted and solved
every day. Because of the various users of the CQA service, question search and ranking
are the most important topics of research in the CQA portal. In this study, we addressed
the problem of identifying questions as being hard or easy by means of a probability
model. In addition, we observed the phenomenon called knowledge gap that is related
to the habit of users and used a knowledge gap diagram to illustrate how much of a
knowledge gap exists in different categories. To this end, we proposed an approach
called the knowledge-gap-based difficulty rank (KG-DRank) algorithm, which combines
the user-user network and the architecture of the CQA service to find hard questions.
We used f-measure, AUC, MAP, NDCG, precision@Top5 and concordance analysis to evaluate
the experimental results. Our results show that our approach leads to better performance
than other baseline approaches across all evaluation metrics.},
  isbn      = {9781479958764},
  keywords  = {social network, CQA portal, link analysis, knowledge gap, expert finding, difficulty},
  location  = {Beijing, China},
  numpages  = {4},
}

@InProceedings{Galteri2019,
  author    = {Galteri, Leonardo and Seidenari, Lorenzo and Bertini, Marco and Uricchio, Tiberio and Del Bimbo, Alberto},
  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
  title     = {Fast Video Quality Enhancement Using GANs},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {1065–1067},
  publisher = {Association for Computing Machinery},
  series    = {MM '19},
  abstract  = {Video compression algorithms result in a reduction of image quality, because of their
lossy approach to reduce the required bandwidth. This affects commercial streaming
services such as Netflix, or Amazon Prime Video, but affects also video conferencing
and video surveillance systems. In all these cases it is possible to improve the video
quality, both for human view and for automatic video analysis, without changing the
compression pipeline, through a post-processing that eliminates the visual artifacts
created by the compression algorithms. Generative Adversarial Networks have obtained
extremely high quality results in image enhancement tasks; however, to obtain such
results large generators are usually employed, resulting in high computational costs
and processing time. In this work we present an architecture that can be used to reduce
the computational cost and that has been implemented on mobile devices. A possible
application is to improve video conferencing, or live streaming. In these cases there
is no original uncompressed video stream available. Therefore, we report results using
no-reference video quality metric showing high naturalness and quality even for efficient
networks.},
  doi       = {10.1145/3343031.3350592},
  isbn      = {9781450368896},
  keywords  = {gans, video quality enhancement, real-time enhancement, video streaming, video compression},
  location  = {Nice, France},
  numpages  = {3},
  url       = {https://doi.org/10.1145/3343031.3350592},
}

@InProceedings{Savola2017,
  author    = {Savola, Reijo and Abie, Habtamu and Kanstr\'{e}n, Teemu},
  booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
  title     = {Session Details: Fourth International Workshop on Measurability of Security in Software Architectures (MeSSa 2017)},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ECSA '17},
  abstract  = {Cybersecurity incidents are increasing, and at the same time, our society depends
more and more on cyber-physical systems. Systematic approaches to measure cybersecurity
are needed in order to support efficient construction and maintenance of secure software
systems. Security measurement of software architectures is needed to produce sufficient
evidence of security level as early as in the design phase. Design-time security measuring
should support "security by design" approach. Moreover, software architectures have
to support runtime security measurement to obtain up-to-date security information
from an online software system, service or product. Security metrics and measurements
are exploited in situational awareness monitoring and self-adaptive security solutions.
The area of security metrics and security assurance metrics research is evolving,
but still lacks widely accepted metrics definitions and applicable measuring techniques.
Strong collaboration between security experts, software architects and system developers
is needed to address this. MeSSa2017 workshop addresses these and other related topics
to increase the importance of the overall picture, requiring sets of design patterns,
measurements, metrics, best practices, and means to integrate this cost-effectively
in the overall design and operational profiles.The outcome of the workshop will be
an increased shared understanding of challenges and opportunities in systematic approaches
to measure cybersecurity, which are needed in order to support efficient construction
and maintenance of secure software systems.},
  doi       = {10.1145/3258045},
  isbn      = {9781450352178},
  location  = {Canterbury, United Kingdom},
  url       = {https://doi.org/10.1145/3258045},
}

@InProceedings{Gafurov2018,
  author    = {Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  title     = {Achieving Test Automation with Testers without Coding Skills: An Industrial Report},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {749–756},
  publisher = {Association for Computing Machinery},
  series    = {ASE 2018},
  abstract  = {We present a process driven test automation solution which enables delegating (part
of) automation tasks from test automation engineer (expensive resource) to test analyst
(non-developer, less expensive). In our approach, a test automation engineer implements
test steps (or actions) which are executed automatically. Such automated test steps
represent user actions in the system under test and specified by a natural language
which is understandable by a non-technical person. Then, a test analyst with a domain
knowledge organizes automated steps combined with test input to create an automated
test case. It should be emphasized that the test analyst does not need to possess
programming skills to create, modify or execute automated test cases. We refine benchmark
test automation architecture to be better suitable for an effective separation and
sharing of responsibilities between the test automation engineer (with coding skills)
and test analyst (with a domain knowledge). In addition, we propose a metric to empirically
estimate cooperation between test automation engineer and test analyst's works. The
proposed automation solution has been defined based on our experience in the development
and maintenance of Helsenorg, the national electronic health services in Norway which
has had over one million of visits per month past year, and we still use it to automate
the execution of regression tests.},
  doi       = {10.1145/3238147.3240463},
  isbn      = {9781450359375},
  keywords  = {Test automation, keyword-driven test automation, process-driven test automation, Helsenorge, DSL for test automation},
  location  = {Montpellier, France},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3238147.3240463},
}

@InProceedings{Stevanetic2014,
  author    = {Stevanetic, Srdjan and Zdun, Uwe},
  booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
  title     = {Exploring the Relationships between the Understandability of Components in Architectural Component Models and Component Level Metrics},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {EASE '14},
  abstract  = {Architectural component models represent high level designs and are frequently used
as a central view of architectural descriptions of software systems. The components
in those models represent important high level organization units that group other
components and classes in object-oriented design views. Hence, understandability of
components and their interactions plays a key role in supporting the architectural
understanding of a software system. In this paper we present a study we carried out
to examine the relationships between the effort required to understand a component,
measured through the time that participants spent on studying a component, and component
level metrics that describe component's size, complexity and coupling in terms of
the number of classes in a component and the classes' relationships. The participants
were 49 master students, and they had to fully understand the components' functionalities
in order to answer 4 true/false questions for each of the 7 components in the architecture
of the Soomla Android store system. Correlation, collinearity and multivariate regression
analysis were performed. The results of the analysis show a statistically significant
correlation between three of the metrics, number of classes, number of incoming dependencies,
and number of internal dependencies, on one side, and the effort required to understand
a component, on the other side. In a multivariate regression analysis we obtained
3 reasonably well-fitting models that can be used to estimate the effort required
to understand a component. In our future work we plan to study more components and
investigate more metrics and their relationships to the understandability of components
and architectural component models.},
  articleno = {32},
  doi       = {10.1145/2601248.2601264},
  isbn      = {9781450324762},
  keywords  = {software metrics, understandability, architectural component models, empirical evaluation},
  location  = {London, England, United Kingdom},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2601248.2601264},
}

@InProceedings{Munoz2017,
  author    = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
  booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
  title     = {Green Software Development and Research with the HADAS Toolkit},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {205–211},
  publisher = {Association for Computing Machinery},
  series    = {ECSA '17},
  abstract  = {Energy is a critical resource, and designing a sustainable software architecture is
a non-trivial task. Developers require energy metrics that support sustainable software
architectures reflecting quality attributes such as security, reliability, performance,
etc., identifying what are the concerns that impact more in the energy consumption.
A variability model of different designs and implementations of an energy model should
exist for this task, as well as a service that stores and compares the experimentation
results of energy and time consumption of each concern, finding out what is the most
eco-efficient solution. The experimental measurements are performed by energy experts
and researchers that share the energy model and metrics in a collaborative repository.
HADAS confronts these tasks modelling and reasoning with the variability of energy
consuming concerns for different energy contexts, connecting HADAS variability model
with its energy efficiency collaborative repository, establishing a Software Product
Line (SPL) service. Our main goal is to help developers to perform sustainability
analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit
prototype is implemented based on a Clafer model and Choco solver, and it has been
tested with several case studies.},
  doi       = {10.1145/3129790.3129818},
  isbn      = {9781450352178},
  keywords  = {software product line, optimisation, variability, clafer, CVL, repository, energy efficiency, metrics},
  location  = {Canterbury, United Kingdom},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3129790.3129818},
}

@InProceedings{VanLanduyt2015,
  author    = {Van Landuyt, Dimitri and Joosen, Wouter},
  booktitle = {Proceedings of the Fifth International Workshop on Twin Peaks of Requirements and Architecture},
  title     = {On the Role of Early Architectural Assumptions in Quality Attribute Scenarios: A Qualitative and Quantitative Study},
  year      = {2015},
  pages     = {9–15},
  publisher = {IEEE Press},
  series    = {TwinPeaks '15},
  abstract  = {Architectural assumptions are fundamentally different from architectural decisions
because they can not be traced directly to requirements, nor to domain, technical
or environmental constraints; they represent conditions under which the designed solution
is expected to be valid. Early architectural assumptions are similar in nature, with
the key difference that they are not made during architectural design but during requirement
elicitation, not by the software architect (a solution-oriented stakeholder), but
by the requirements engineer (a problem-oriented stakeholder). They represent initial
assumptions about the system's architecture, and allow the requirements engineer to
be more precise in documenting the requirements of the system.The role of early architectural
assumptions in the current practice of quality attribute scenario elicitation and
related development activities in the transition to architecture is unknown and under-investigated.
In this paper, we present the results of an exploratory study that focuses on the
role and nature of these assumptions in the early development stages. We studied a
reasonably large set of quality attribute scenarios for a realistic industrial case,
a smart metering system. Our study (i) confirms that quality attribute scenario elicitation
in practice does rely heavily on early architectural assumptions, and (ii) shows that
they do influence the perceived quality of the requirements body as a whole, in some
cases positively, in other cases negatively.These findings provide empirical arguments
in favor of making such assumptions explicit already during the requirements elicitation
activities. Especially in the context of iterative software development methodologies
such as the Twin Peaks model, a well-defined and -documented set of assumptions could
smoothen the transition between successive development iterations.},
  location  = {Florence, Italy},
  numpages  = {7},
}

@InProceedings{Stevanetic2015,
  author    = {Stevanetic, Srdjan and Zdun, Uwe},
  booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
  title     = {Software Metrics for Measuring the Understandability of Architectural Structures: A Systematic Mapping Study},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {EASE '15},
  abstract  = {The main idea of software architecture is to concentrate on the "big picture" of a
software system. In the context of object-oriented software systems higher-level architectural
structures or views above the level of classes are frequently used to capture the
"big picture" of the system. One of the critical aspects of these higher-level views
is understandability, as one of their main purposes is to enable designers to abstract
away fine-grained details. In this article we present a systematic mapping study on
software metrics related to the understandability concepts of such higher-level software
structures with regard to their relations to the system implementation. In our systematic
mapping study, we started from 3951 studies obtained using an electronic search in
the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our
inclusion/exclusion criteria as well as the snowballing technique we selected 268
studies for in-depth study. From those, we selected 25 studies that contain relevant
metrics. We classify the identified studies and metrics with regard to the measured
artefacts, attributes, quality characteristics, and representation model used for
the metrics definitions. Additionally, we present the assessment of the maturity level
of the identified studies. Overall, there is a lack of maturity in the studies. We
discuss possible techniques how to mitigate the identified problems. From the academic
point of view we believe that our study is a good starting point for future studies
aiming at improving the existing works. From a practitioner's point of view, the results
of our study can be used as a catalogue and an indication of the maturity of the existing
research results.},
  articleno = {21},
  doi       = {10.1145/2745802.2745822},
  isbn      = {9781450333504},
  location  = {Nanjing, China},
  numpages  = {14},
  url       = {https://doi.org/10.1145/2745802.2745822},
}

@InProceedings{Hu2016,
  author    = {Hu, Hao and Hong, Xingchen and Terstriep, Jeff and Liu, Yan Y. and Finn, Michael P. and Rush, Johnathan and Wendel, Jeffrey and Wang, Shaowen},
  booktitle = {Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},
  title     = {TopoLens: Building a CyberGIS Community Data Service for Enhancing the Usability of High-Resolution National Topographic Datasets},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {XSEDE16},
  abstract  = {Geospatial data, often embedded with geographic references, are important to many
application and science domains, and represent a major type of big data. The increased
volume and diversity of geospatial data have caused serious usability issues for researchers
in various scientific domains, which call for innovative cyberGIS solutions. To address
these issues, this paper describes a cyberGIS community data service framework to
facilitate geospatial big data access, processing, and sharing based on a hybrid supercomputer
architecture. Through the collaboration between the CyberGIS Center at the University
of Illinois at Urbana-Champaign (UIUC) and the U.S. Geological Survey (USGS), a community
data service for accessing, customizing, and sharing digital elevation model (DEM)
and its derived datasets from the 10-meter national elevation dataset, namely TopoLens,
is created to demonstrate the workflow integration of geospatial big data sources,
computation, analysis needed for customizing the original dataset for end user needs,
and a friendly online user environment. TopoLens provides online access to precomputed
and on-demand computed high-resolution elevation data by exploiting the ROGER supercomputer.
The usability of this prototype service has been acknowledged in community evaluation.},
  articleno = {39},
  doi       = {10.1145/2949550.2949652},
  isbn      = {9781450347556},
  keywords  = {web-based gateway environment, CyberGIS, microservices, geospatial big data, elevation data, data sharing},
  location  = {Miami, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2949550.2949652},
}

@Article{Wu2020,
  author     = {Wu, Hao and Liu, Weizhi and Lin, Huanxin and Wang, Cho-Li},
  journal    = {ACM Trans. Archit. Code Optim.},
  title      = {A Model-Based Software Solution for Simultaneous Multiple Kernels on GPUs},
  year       = {2020},
  issn       = {1544-3566},
  month      = mar,
  number     = {1},
  volume     = {17},
  abstract   = {As a critical computing resource in multiuser systems such as supercomputers, data
centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking
is an intuitive solution to underutilization in GPGPU computing. Recently proposed
solutions of multitasking GPUs can be classified into two categories: (1) spatially
partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of
compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels
simultaneously within a CU. Compared to SPS, SMK can improve resource utilization
even further due to the interleaving of instructions from kernels with low dynamic
resource contentions.However, it is hard to implement SMK on current GPU architecture,
because (1) techniques for applying SMK on top of GPU hardware scheduling policy are
scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences
of concurrently executed kernels. In this article, we propose a lightweight and effective
performance model to evaluate the complex interferences of SMK. Based on the probability
of independent events, our performance model is built from a totally new angle and
contains limited parameters. Then, we propose a metric, symbiotic factor, which can
evaluate an SMK scheme so that kernels with complementary resource utilization can
corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing
and kernel stretching techniques and integrate them to apply SMK on GPUs instead of
simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based
concurrent kernel execution whose kernel launching order brings fast execution time,
the results of corunning kernel pairs show 11%, 18%, and 12% speedup on AMD R9 290X,
RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the
results show 29%, 18%, and 51% speedup on AMD R9 290X, RX 480, and Vega 64, respectively,
on average.},
  address    = {New York, NY, USA},
  articleno  = {7},
  doi        = {10.1145/3377138},
  issue_date = {March 2020},
  keywords   = {concurrent kernel execution, GPGPU},
  numpages   = {26},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3377138},
}

@InProceedings{Mohammad2018,
  author    = {Mohammad, Hafeezul Rahman and Xu, Keyang and Callan, Jamie and Culpepper, J. Shane},
  booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
  title     = {Dynamic Shard Cutoff Prediction for Selective Search},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {85–94},
  publisher = {Association for Computing Machinery},
  series    = {SIGIR '18},
  abstract  = {Selective search architectures use resource selection algorithms such as Rank-S or
Taily to rank index shards and determine how many to search for a given query. Most
prior research evaluated solutions by their ability to improve efficiency without
significantly reducing early-precision metrics such as P@5 and NDCG@10. This paper
recasts selective search as an early stage of a multi-stage retrieval architecture,
which makes recall-oriented metrics more appropriate. A new algorithm is presented
that predicts the number of shards that must be searched for a given query in order
to meet recall-oriented goals. Decoupling shard ranking from deciding how many shards
to search clarifies efficiency vs. effectiveness trade-offs, and enables them to be
optimized independently. Experiments on two corpora demonstrate the value of this
approach.},
  doi       = {10.1145/3209978.3210005},
  isbn      = {9781450356572},
  keywords  = {resource selection, distributed search, selective search},
  location  = {Ann Arbor, MI, USA},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3209978.3210005},
}

@InProceedings{Mangla2018,
  author    = {Mangla, Tarun and Zegura, Ellen and Ammar, Mostafa and Halepovic, Emir and Hwang, Kyung-Wook and Jana, Rittwik and Platania, Marco},
  booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
  title     = {VideoNOC: Assessing Video QoE for Network Operators Using Passive Measurements},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {101–112},
  publisher = {Association for Computing Machinery},
  series    = {MMSys '18},
  abstract  = {Video streaming traffic is rapidly growing in mobile networks. Mobile Network Operators
(MNOs) are expected to keep up with this growing demand, while maintaining a high
video Quality of Experience (QoE). This makes it critical for MNOs to have a solid
understanding of users' video QoE with a goal to help with network planning, provisioning
and traffic management. However, designing a system to measure video QoE has several
challenges: i) large scale of video traffic data and diversity of video streaming
services, ii) cross-layer constraints due to complex cellular network architecture,
and iii) extracting QoE metrics from network traffic. In this paper, we present VideoNOC,
a prototype of a flexible and scalable platform to infer objective video QoE metrics
(e.g., bitrate, rebuffering) for MNOs. We describe the design and architecture of
VideoNOC, and outline the methodology to generate a novel data source for fine-grained
video QoE monitoring. We then demonstrate some of the use cases of such a monitoring
system. VideoNOC reveals video demand across the entire network, provides valuable
insights on a number of design choices by content providers (e.g., OS-dependent performance,
video player parameters like buffer size, range of encoding bitrates, etc.) and helps
analyze the impact of network conditions on video QoE (e.g., mobility and high demand).},
  doi       = {10.1145/3204949.3204956},
  isbn      = {9781450351928},
  keywords  = {video streaming, passive measurement, cellular network, QoE},
  location  = {Amsterdam, Netherlands},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3204949.3204956},
}

@InProceedings{Seraoui2017,
  author    = {Seraoui, Youssef and Belmekki, Mostafa and Bellafkih, Mostafa and Raouyane, Brahim},
  booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
  title     = {ETOM Mapping onto NFV Framework: IMS Use Case},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICCWCS'17},
  abstract  = {Telecom professionals have a strong interest in the proposition and adaptation of
innovate network management models and frameworks to help mobile network operators
(MNOs) to improve their business processes and get more agile in the telecoms industry
that evolves with great speed. The model being established by the TeleManagement Forum
(TM Forum) is the Enhanced Telecom Operations MAP (eTOM) business process framework
on which we rely in this work to propose a mapping of the eTOM model onto the network
functions virtualization (NFV) framework with the projection of this function mapping
onto the IP Multimedia Subsystem (IMS) use case. This mapping covers essentially four
main components playing important rules in the MNO's business processes, including
customers, services, infrastructure resources, and also service providers. The main
goal, thereby, is to design a combined architecture in a virtualized environment for
dynamic delivery of services with quality of service (QoS) and improved resource performance
so as to meet the purposes of the 5G network in terms of a proposed, virtual telecom
environment managed and orchestrated by the conjunction of the aforementioned paradigms.
Indeed, we conducted simulations to evaluate part of this function mapping in an IMS
setting for static service chain provisioning. Thus, results showed possible provisioning
of services in this context in measuring SIP related key performance indicators and
performance metrics. Results showed the feasibility of our approach. In addition,
resource performance improved obviously in the NFV context in accordance with eTOM
business processes.},
  articleno = {45},
  doi       = {10.1145/3167486.3167534},
  isbn      = {9781450353069},
  keywords  = {New Generation Operations Systems and Software (NGOSS), service chain provisioning, network functions virtualization (NFV), IP Multimedia Subsystem (IMS), resource performance, Enhanced Telecom Operations Map (eTOM), five generation (5G), Business process, quality of service (QoS)},
  location  = {Larache, Morocco},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3167486.3167534},
}

@InProceedings{Saurez2018,
  author    = {Saurez, Enrique and Balasubramanian, Bharath and Schlichting, Richard and Tschaen, Brendan and Huang, Zhe and Narayanan, Shankaranarayanan Puzhavakath and Ramachandran, Umakishore},
  booktitle = {Proceedings of the 3rd Workshop on Middleware for Edge Clouds &amp; Cloudlets},
  title     = {METRIC: A Middleware for Entry Transactional Database Clustering at the Edge},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {2–7},
  publisher = {Association for Computing Machinery},
  series    = {MECC'18},
  abstract  = {A geo-distributed database for edge architectures spanning thousands of sites needs
to assure efficient local updates while replicating sufficient state across sites
to enable global management and support mobility, failover etc. To address this requirement,
a new paradigm for database clustering that achieves a better balance than existing
solutions between performance and strength of semantics called entry transactionality
is introduced. Inspired by entry consistency in shared memory systems, entry transactionality
guarantees that only a client that owns a range of keys in the database has a sequentially
consistent value of the keys and can perform local and, hence, efficient transactions
across these keys. Important use cases enabled by entry transactionality such as federated
controllers and state management for edge applications are identified. The semantics
of entry transactionality incorporating the complex failure modes in geo-distributed
services are defined, and the difficult challenges in realizing these semantics are
outlined. Then, a novel Middleware for Entry Transactional Clustering (METRIC) that
combines existing SQL databases with an underlying geo-distributed entry consistent
store to realize entry transactionality is described. This paper provides initial
findings from an on-going effort.},
  doi       = {10.1145/3286685.3286686},
  isbn      = {9781450361170},
  location  = {Rennes, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3286685.3286686},
}

@InProceedings{Wang2017,
  author    = {Wang, Zicong and Chen, Xiaowen and Li, Chen and Guo, Yang},
  booktitle = {Proceedings of the Eleventh IEEE/ACM International Symposium on Networks-on-Chip},
  title     = {Fairness-Oriented and Location-Aware NUCA for Many-Core SoC},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {NOCS '17},
  abstract  = {Non-uniform cache architecture (NUCA) is often employed to organize the last level
cache (LLC) by Networks-on-Chip (NoC). However, along with the scaling up for network
size of Systems-on-Chip (SoC), two trends gradually begin to emerge. First, the network
latency is becoming the major source of the cache access latency. Second, the communication
distance and latency gap between different cores is increasing. Such gap can seriously
cause the network latency imbalance problem, aggravate the degree of non-uniform for
cache access latencies, and then worsen the system performance.In this paper, we propose
a novel NUCA-based scheme, named fairness-oriented and location-aware NUCA (FL-NUCA),
to alleviate the network latency imbalance problem and achieve more uniform cache
access. We strive to equalize network latencies which are measured by three metrics:
average latency (AL), latency standard deviation (LSD), and maximum latency (ML).
In FL-NUCA, the memory-to-LLC mapping and links are both non-uniform distributed to
better fit the network topology and traffics, thereby equalizing network latencies
from two aspects, i.e., non-contention latencies and contention latencies, respectively.
The experimental results show that FL-NUCA can effectively improve the fairness of
network latencies. Compared with the traditional static NUCA (S-NUCA), in simulation
with synthetic traffics, the average improvements for AL, LSD, and ML are 20.9%, 36.3%,
and 35.0%, respectively. In simulation with PARSEC benchmarks, the average improvements
for AL, LSD, and ML are 6.3%, 3.6%, and 11.2%, respectively.},
  articleno = {13},
  doi       = {10.1145/3130218.3130225},
  isbn      = {9781450349840},
  keywords  = {Networks-on-chip, non-uniform cache architecture, memory mapping},
  location  = {Seoul, Republic of Korea},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3130218.3130225},
}

@InProceedings{Berba2019,
  author    = {Berba, Elizalde M. and Palaoag, Thelma D.},
  booktitle = {Proceedings of the 8th International Conference on Informatics, Environment, Energy and Applications},
  title     = {Improving Customer Satisfaction on Internet Services in L-NU Using Virtualized AAA Network Architecture},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {178–183},
  publisher = {Association for Computing Machinery},
  series    = {IEEA '19},
  abstract  = {This study mainly aims to improve the satisfaction level on internet services in Lyceum-Northwestern
University (L-NU). From a traditional network architecture, the researchers made use
of a virtualized Authentication, Authorization and Accounting (AAA) network architecture
to improve the internet services provided to the students of L-NU. For the methodology
of this study, the researchers had to make use of a network lifecycle called Prepare,
Plan, Design, Implement, Operate, and Optimize (PPDIOO) and there was also a need
to combine both quantitative and qualitative research approach. To make this happen,
the researchers had to fulfill the following objectives: a) determine the current
network setup of L-NU, b) measure the current satisfaction level of the users, c)
design, develop and implement a virtualized AAA network architecture, d) measure the
satisfaction level of the users who have used the AAA network setup, and e) compare
the measured satisfaction level from the users who used the internet facilities using
the traditional network architecture and satisfaction level from users who have used
the internet facilities after the implementation of AAA network architecture. As a
result, it was found out that the implementation of the new network architecture has
significantly improved the internet service level of L-NU which is reflected by a
higher customer satisfaction rating. Therefore, the researchers conclude that it is
most essential that AAA network architecture be implemented to enterprise type of
network setup such as but not limited to education institutions in managing their
internet services. Consequently, this kind of network architecture lead to a more
effective and more efficient way of managing network resources of an institution or
an organization while further improving the satisfaction level. In order to optimize
the AAA network architecture and gain more implementation advantages, virtualization
technology was used to contain and run numerous operating system instances such as
four physical servers into one single physical server which favors to saving resources
such as energy, space, money and of which also leads to simplified administration.},
  doi       = {10.1145/3323716.3323729},
  isbn      = {9781450361040},
  keywords  = {authorization, AAA, authentication, hypervisor, accounting, customer satisfaction, PPDIOO, virtualization},
  location  = {Osaka, Japan},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3323716.3323729},
}

@InProceedings{Dong2014,
  author    = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
  booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
  title     = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {365–366},
  publisher = {Association for Computing Machinery},
  series    = {SIGCOMM '14},
  abstract  = {After more than two decades of evolution, TCP and its end host based modifications
can still suffer from severely degraded performance under real-world challenging network
conditions. The reason, as we observe, is due to TCP family's fundamental architectural
deficiency, which hardwires packet-level events to control responses and ignores emprical
performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented
Congestion Control (PCC), a new congestion control architecture in which each sender
controls its sending strategy based on empirically observed performance metrics. We
show through preliminary experimental results that PCC achieves consistently high
performance under various challenging network conditions.},
  doi       = {10.1145/2619239.2631456},
  isbn      = {9781450328364},
  keywords  = {congestion control},
  location  = {Chicago, Illinois, USA},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2619239.2631456},
}

@Article{Dong2014a,
  author     = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
  journal    = {SIGCOMM Comput. Commun. Rev.},
  title      = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
  year       = {2014},
  issn       = {0146-4833},
  month      = aug,
  number     = {4},
  pages      = {365–366},
  volume     = {44},
  abstract   = {After more than two decades of evolution, TCP and its end host based modifications
can still suffer from severely degraded performance under real-world challenging network
conditions. The reason, as we observe, is due to TCP family's fundamental architectural
deficiency, which hardwires packet-level events to control responses and ignores emprical
performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented
Congestion Control (PCC), a new congestion control architecture in which each sender
controls its sending strategy based on empirically observed performance metrics. We
show through preliminary experimental results that PCC achieves consistently high
performance under various challenging network conditions.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2740070.2631456},
  issue_date = {October 2014},
  keywords   = {congestion control},
  numpages   = {2},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2740070.2631456},
}

@InProceedings{Ewing2014,
  author    = {Ewing, John M. and Menasc\'{e}, Daniel A.},
  booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
  title     = {A Meta-Controller Method for Improving Run-Time Self-Architecting in SOA Systems},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {173–184},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '14},
  abstract  = {This paper builds on SASSY, a system for automatically generating SOA software architectures
that optimize a given utility function of multiple QoS metrics. In SASSY, SOA software
systems are automatically re-architected when services fail or degrade. Optimizing
both architecture and service provider selection presents a pair of nested NP-hard
problems. Here we adapt hill-climbing, beam search, simulated annealing, and evolutionary
programming to both architecture optimization and service provider selection. Each
of these techniques has several parameters that influence their efficiency. We introduce
in this paper a meta-controller that automates the run-time selection of heuristic
search techniques and their parameters. We examine two different meta-controller implementations
that each use online learning. The first implementation identifies the best heuristic
search combination from various prepared combinations. The second implementation analyzes
the current self-architecting problem (e.g. changes in performance metrics, service
degradations/failures) and looks for similar, previously encountered re-architecting
problems to find an effective heuristic search combination for the current problem.
A large set of experiments demonstrates the effectiveness of the first meta-controller
implementation and indicates opportunities for improving the second meta-controller
implementation.},
  doi       = {10.1145/2568088.2568098},
  isbn      = {9781450327336},
  keywords  = {heuristic search, meta-controlled qos optimization, autonomic computing, soa, combinatorial search techniques, metaheuristics, automated run-time software architecting},
  location  = {Dublin, Ireland},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2568088.2568098},
}

@InProceedings{Kuang2015,
  author    = {Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
  booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Modeling Cross-Architecture Co-Tenancy Performance Interference},
  year      = {2015},
  pages     = {231–240},
  publisher = {IEEE Press},
  series    = {CCGRID '15},
  abstract  = {Cloud computing has become a dominant computing paradigm to provide elastic, affordable
computing resources to end users. Due to the increased computing power of modern machines
powered by multi/many-core computing, data centers often co-locate multiple virtual
machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing
and competition. Applications or VMs co-locating in one physical machine can interfere
with each other despite of the promise of performance isolation through virtualization.
Modeling and predicting co-run interference therefore becomes critical for data center
job scheduling and QoS (Quality of Service) assurance. Co-run interference can be
categorized into two metrics, sensitivity and pressure, where the former denotes how
an application's performance is affected by its co-run applications, and the latter
measures how it impacts the performance of its co-run applications. This paper shows
that sensitivity and pressure are both application- and architecture-dependent. Further,
we propose a regression model that predicts an application's sensitivity and pressure
across architectures with high accuracy. This regression model enables a data center
scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate
with another VMs/applications.},
  doi       = {10.1109/CCGrid.2015.152},
  isbn      = {9781479980062},
  location  = {Shenzhen, China},
  numpages  = {10},
  url       = {https://doi.org/10.1109/CCGrid.2015.152},
}

@InProceedings{Sharma2018,
  author    = {Sharma, Puneet and Raghuramu, Arun and Lee, David and Saxena, Vinay and Chuah, Chen-Nee},
  booktitle = {Proceedings of the 17th ACM Workshop on Hot Topics in Networks},
  title     = {We Don't Need No Licensing Server},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {162–168},
  publisher = {Association for Computing Machinery},
  series    = {HotNets '18},
  abstract  = {Cloudification of edge to core infrastructure has led to new and rich application
and service deployment and operational models. These ecosystems have complex relationships
between the application vendors, infrastructure operators and application users. Traditional
licensing and compliance enforcement methods such as those based on in person audits
and dynamic issuing of license keys inhibit the resource provisioning and consumption
flexibility offered by cloudified services due to scalability and management overheads.
In this work, we argue the need for a trusted framework for application usage rights
compliance. This new architecture named "Metered Boot" provides a way to realize trusted,
capacity/usage based rights compliance for service deployments that allows decoupling
of usage rights governed by application vendors from the resource provisioning by
the infrastructure provider. We have built a Metered Boot prototype for a particular
usecase of NFV usage rights compliance.},
  doi       = {10.1145/3286062.3286086},
  isbn      = {9781450361200},
  location  = {Redmond, WA, USA},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3286062.3286086},
}

@InProceedings{Aniche2019,
  author    = {Aniche, Maur\'{\i}cio and Yoder, Joseph W. and Kon, Fabio},
  booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
  title     = {Current Challenges in Practical Object-Oriented Software Design},
  year      = {2019},
  pages     = {113–116},
  publisher = {IEEE Press},
  series    = {ICSE-NIER '19},
  abstract  = {According to the extensive 50-year-old body of knowledge in object-oriented programming
and design, good software designs are, among other characteristics, lowly coupled,
highly cohesive, extensible, comprehensible, and not fragile. However, with the increased
complexity and heterogeneity of contemporary software, this might not be enough.This
paper discusses the practical challenges of object-oriented design in modern software
development. We focus on three main challenges: (1) how technologies, frameworks,
and architectures pressure developers to make design decisions that they would not
take in an ideal scenario, (2) the complexity of current real-world problems require
developers to devise not only a single, but several models for the same problem that
live and interact together, and (3) how existing quality assessment techniques for
object-oriented design should go beyond high-level metrics.Finally, we propose an
agenda for future research that should be tackled by both scientists and practitioners
soon. This paper is a call for arms for more reality-oriented research on the object-oriented
software design field.},
  doi       = {10.1109/ICSE-NIER.2019.00037},
  keywords  = {domain modeling, software architecture, class design, object-oriented programming, software engineering, object-oriented design, software design},
  location  = {Montreal, Quebec, Canada},
  numpages  = {4},
  url       = {https://doi.org/10.1109/ICSE-NIER.2019.00037},
}

@InProceedings{Alshehri2018,
  author    = {Alshehri, Asma and Benson, James and Patwa, Farhan and Sandhu, Ravi},
  booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
  title     = {Access Control Model for Virtual Objects (Shadows) Communication for AWS Internet of Things},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {175–185},
  publisher = {Association for Computing Machinery},
  series    = {CODASPY '18},
  abstract  = {The concept of Internet of Things (IoT) has received considerable attention and development
in recent years. There have been significant studies on access control models for
IoT in academia, while companies have already deployed several cloud-enabled IoT platforms.
However, there is no consensus on a formal access control model for cloud-enabled
IoT. The access-control oriented (ACO) architecture was recently proposed for cloud-enabled
IoT, with virtual objects (VOs) and cloud services in the middle layers. Building
upon ACO, operational and administrative access control models have been published
for virtual object communication in cloud-enabled IoT illustrated by a use case of
sensing speeding cars as a running example.In this paper, we study AWS IoT as a major
commercial cloud-IoT platform and investigate its suitability for implementing the
afore-mentioned academic models of ACO and VO communication control. While AWS IoT
has a notion of digital shadows closely analogous to VOs, it lacks explicit capability
for VO communication and thereby for VO communication control. Thus there is a significant
mismatch between AWS IoT and these academic models. The principal contribution of
this paper is to reconcile this mismatch by showing how to use the mechanisms of AWS
IoT to effectively implement VO communication models. To this end, we develop an access
control model for virtual objects (shadows) communication in AWS IoT called AWS-IoT-ACMVO.
We develop a proof-of-concept implementation of the speeding cars use case in AWS
IoT under guidance of this model, and provide selected performance measurements. We
conclude with a discussion of possible alternate implementations of this use case
in AWS IoT.},
  doi       = {10.1145/3176258.3176328},
  isbn      = {9781450356329},
  keywords  = {devices, abac, security, iot architecture, acl, aws iot, internet of things (iot), access control, virtual objects, rbac},
  location  = {Tempe, AZ, USA},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3176258.3176328},
}

@InProceedings{Gao2021,
  author    = {Gao, Chengsi and Li, Bing and Wang, Ying and Chen, Weiwei and Zhang, Lei},
  booktitle = {Proceedings of the 2021 on Great Lakes Symposium on VLSI},
  title     = {Tenet: A Neural Network Model Extraction Attack in Multi-Core Architecture},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {21–26},
  publisher = {Association for Computing Machinery},
  series    = {GLSVLSI '21},
  abstract  = {As neural networks (NNs) are being widely deployed in many cloud-oriented systems
for safety-critical tasks, the privacy and security of NNs become significant concerns
to users in the cloud platform that shares the computation infrastructure such as
memory resource. In this work, we observed that the memory timing channel in the shared
memory of cloud multi-core architecture poses the risk of network model information
leakage. Based on the observation, we propose a learning-based method to steal the
model architecture of the NNs by exploiting the memory timing channel without any
high-level privilege or physical access. We first trained an end-to-end measurement
network offline to learn the relation between memory timing information and NNs model
architecture. Then, we performed an online attack and reconstructed the target model
using the prediction from the measurement network. We evaluated the proposed attack
method on a multi-core architecture simulator. The experimental results show that
our learning-based attack method can reconstruct the target model with high accuracy
and improve the adversarial attack success rate by 42.4%.},
  doi       = {10.1145/3453688.3461512},
  isbn      = {9781450383936},
  keywords  = {deep learning security, multi-core, machine learning, memory timing channel},
  location  = {Virtual Event, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3453688.3461512},
}

@InProceedings{Stohr2014,
  author    = {Stohr, Denny and Wilk, Stefan and Effelsberg, Wolfgang},
  booktitle = {Proceedings of the First International Workshop on Internet-Scale Multimedia Management},
  title     = {Monitoring of User Generated Video Broadcasting Services},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {39–42},
  publisher = {Association for Computing Machinery},
  series    = {WISMM '14},
  abstract  = {Mobile video broadcasting services offer users the opportunity to instantly share
content from their mobile handhelds to a large audience over the Internet. However,
existing data caps in cellular network contracts and limitations in their upload capabilities
restrict the adoption of mobile video broadcasting services. Additionally, the quality
of those video streams is often reduced by the lack of skills of recording users and
the technical limitations of the video capturing devices. Our research focuses on
large-scale events that attract dozens of users to record video in parallel. In many
cases, available network infrastructure is not capable to upload all video streams
in parallel. To make decisions on how to appropriately transmit those video streams,
a suitable monitoring of the video generation process is required. For this scenario,
a measurement framework is proposed that allows Internet-scale mobile broadcasting
services to deliver samples in an optimized way. Our framework architecture analyzes
three zones for effectively monitoring user-generated video. Besides classical Quality
of Service metrics on the network state, video quality indicators and additional auxiliary
sensor information is gathered. Aim of this framework is an efficient coordination
of devices and their uploads based on the currently observed system state.},
  doi       = {10.1145/2661714.2661726},
  isbn      = {9781450331579},
  keywords  = {cellular networks, mobile, mix, video broadcast, network monitoring, video composition, measurement},
  location  = {Orlando, Florida, USA},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2661714.2661726},
}

@Article{Miao2016,
  author     = {Miao, Wang and Min, Geyong and Wu, Yulei and Wang, Haozhe and Hu, Jia},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {Performance Modelling and Analysis of Software-Defined Networking under Bursty Multimedia Traffic},
  year       = {2016},
  issn       = {1551-6857},
  month      = sep,
  number     = {5s},
  volume     = {12},
  abstract   = {Software-Defined Networking (SDN) is an emerging architecture for the next-generation
Internet, providing unprecedented network programmability to handle the explosive
growth of big data driven by the popularisation of smart mobile devices and the pervasiveness
of content-rich multimedia applications. In order to quantitatively investigate the
performance characteristics of SDN networks, several research efforts from both simulation
experiments and analytical modelling have been reported in the current literature.
Among those studies, analytical modelling has demonstrated its superiority in terms
of cost-effectiveness in the evaluation of large-scale networks. However, for analytical
tractability and simplification, existing analytical models are derived based on the
unrealistic assumptions that the network traffic follows the Poisson process, which
is suitable to model nonbursty text data, and the data plane of SDN is modelled by
one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies
have shown that, due to the features of heavy volume and high velocity, the multimedia
big data generated by real-world multimedia applications reveals the bursty and correlated
nature in the network transmission. With the aim of capturing such features of realistic
traffic patterns and obtaining a comprehensive and deeper understanding of the performance
behaviour of SDN networks, this article presents a new analytical model to investigate
the performance of SDN in the presence of the bursty and correlated arrivals modelled
by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance
metrics in terms of the average latency and average network throughput of the SDN
networks are derived based on the developed analytical model. To consider a realistic
multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted
to model the SDN data plane. To address the challenging problem of obtaining the key
performance metrics, for example, queue-length distribution of a PQ system with a
given service capacity, a versatile methodology extending the Empty Buffer Approximation
(EBA) method is proposed to facilitate the decomposition of such a PQ system to two
SSSQ systems. The validity of the proposed model is demonstrated through extensive
simulation experiments. To illustrate its application, the developed model is then
utilised to study the strategy of the network configuration and resource allocation
in SDN networks.},
  address    = {New York, NY, USA},
  articleno  = {77},
  doi        = {10.1145/2983637},
  issue_date = {December 2016},
  keywords   = {performance modelling and analysis, Software-defined networking, multimedia big data, queueing decomposition, resource allocation},
  numpages   = {19},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2983637},
}

@InProceedings{Maikantis2020,
  author    = {Maikantis, Theodoros and Tsintzira, Angeliki-Agathi and Ampatzoglou, Apostolos and Arvanitou, Elvira-Maria and Chatzigeorgiou, Alexander and Stamelos, Ioannis and Bibi, Stamatia and Deligiannis, Ignatios},
  booktitle = {24th Pan-Hellenic Conference on Informatics},
  title     = {Software Architecture Reconstruction via a Genetic Algorithm: Applying the Move Class Refactoring},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {135–139},
  publisher = {Association for Computing Machinery},
  series    = {PCI 2020},
  abstract  = {Modularity is one of the four key principles of software design and architecture.
According to this principle, software should be organized into modules that are tightly
linked internally (high cohesion), whereas at the same time as independent from other
modules as possible (low coupling). However, in practice, this principle is violated
due to poor architecting design decisions, lack of time, or coding shortcuts, leading
to a phenomenon termed as architectural technical debt (ATD). To alleviate this problem
(lack of architectural modularity), the most common solution is the application of
a software refactoring, namely Move Class—i.e., moving classes (the core artifact
in object-oriented systems) from one module to another. To identify Move Class refactoring
opportunities, we employ a search-based optimization process, relying on optimization
metrics, through which optimal moves are derived. Given the extensive search space
required for applying a brute-force search strategy, in this paper, we propose the
use of a genetic algorithm that re-arranges existing software classes into existing
or new modules (software packages in Java, or folders in C++). To validate the usefulness
of the proposed refactorings, we performed an industrial case study on three projects
(from the Aviation, Healthcare, and Manufacturing application domains). The results
of the study indicate that the proposed architecture reconstruction is able to improve
modularity, improving both coupling and cohesion. The obtained results can be useful
to practitioners through an open source tool; whereas at the same point, they open
interesting future work directions.},
  doi       = {10.1145/3437120.3437292},
  isbn      = {9781450388979},
  location  = {Athens, Greece},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3437120.3437292},
}

@InProceedings{Guendogan2020,
  author    = {G\"{u}ndo\u{g}an, Cenk and Ams\"{u}ss, Christian and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
  booktitle = {Proceedings of the 7th ACM Conference on Information-Centric Networking},
  title     = {Toward a RESTful Information-Centric Web of Things: A Deeper Look at Data Orientation in CoAP},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {77–88},
  publisher = {Association for Computing Machinery},
  series    = {ICN '20},
  abstract  = {The information-centric networking (ICN) paradigm offers replication of autonomously
verifiable content throughout a network, in which content is bound to names instead
of hosts. This has proven beneficial in particular for the constrained IoT. Several
approaches, the most prominent of which being Named Data Networking, propose access
to named content directly on the network layer. Independently, the IETF CoAP protocol
group started to develop mechanisms that support autonomous content processing and
in-network storage.In this paper, we explore the emerging CoAP protocol building blocks
and how they contribute to an information-centric network architecture for a data-oriented
RESTful Web of Things. We discuss design options and measure characteristic performances
of different network configurations, which deploy CoAP proxies and OSCORE content
object security, and compare with NDN. Our findings indicate an almost continuous
design space ranging from plain CoAP at the one end to NDN on the other. On both ends---ICN
and CoAP---we identify protocol features and aspects whose mutual transfer potentially
improves design and operation of the other.},
  doi       = {10.1145/3405656.3418718},
  isbn      = {9781450380409},
  keywords  = {Internet of Things, ICN, protocol evaluation, content object security, CoAP Proxy, OSCORE},
  location  = {Virtual Event, Canada},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3405656.3418718},
}

@Article{ZeinalipourYazti2017,
  author     = {Zeinalipour-Yazti, Demetrios and Laoudias, Christos},
  journal    = {SIGSPATIAL Special},
  title      = {The Anatomy of the Anyplace Indoor Navigation Service},
  year       = {2017},
  month      = oct,
  number     = {2},
  pages      = {3–10},
  volume     = {9},
  abstract   = {The pervasiveness of smartphones is leading to the uptake of a new class of Internet-based
Indoor Navigation (IIN) services, which might soon diminish the need of Satellite-based
localization technologies in urban environments. These services rely on geo-location
databases that store spatial models along with wireless, light and magnetic signals
used to localize users and provide better power efficiency and wider coverage than
predominant approaches. In this article we overview Anyplace, an open, modular, extensible
and scalable navigation architecture that exploits crowdsourced Wi-Fi data to develop
a novel navigation service that won several international research awards for its
utility and accuracy (i.e., less than 2 meters). Our MIT-licenced open-source software
stack has to this date been used by thaousands of researchers and practitioners around
the globe, with the public Anyplace service reaching over 100,000 real user interactions.},
  address    = {New York, NY, USA},
  doi        = {10.1145/3151123.3151125},
  issue_date = {July 2017},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3151123.3151125},
}

@InProceedings{Lumba2020,
  author    = {Lumba, Ester and Waworuntu, Alexander},
  booktitle = {Proceedings of the 2020 2nd Asia Pacific Information Technology Conference},
  title     = {Application of Lecturer Performance Report in Indonesia with Model View Controller (MVC) Architecture},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {23–28},
  publisher = {Association for Computing Machinery},
  series    = {APIT 2020},
  abstract  = {Lecturers in Indonesia have a fundamental obligation to conduct Tri Dharma activities
consisting of teaching, research and community service. Most higher education institutions
use Tri Dharma as a measure of lecturer's performance. In addition, lecturer activity
data related to Tri Dharma is needed by the head of study program and department related
to research, publication and community service to be stored which will be used as
a source of data during the accreditation process. This paper discusses the application
development of lecturer performance reports using the Model View Controller (MVC)
architecture with Java programming language. The result is a desktop-based application
that will be used by the head of the study program and the lecturers.},
  doi       = {10.1145/3379310.3379320},
  isbn      = {9781450376853},
  keywords  = {desktop-based application, application development, MVC architecture, Indonesia higher-education},
  location  = {Bali Island, Indonesia},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3379310.3379320},
}

@InProceedings{Alder2019,
  author    = {Alder, Fritz and Asokan, N. and Kurnikov, Arseny and Paverd, Andrew and Steiner, Michael},
  booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
  title     = {S-FaaS: Trustworthy and Accountable Function-as-a-Service Using Intel SGX},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {185–199},
  publisher = {Association for Computing Machinery},
  series    = {CCSW'19},
  abstract  = {Function-as-a-Service (FaaS) is a recent and popular cloud computing paradigm in which
the function provider specifies a function to be run and is billed only for the computational
resources used by that function. Compared to other cloud paradigms, FaaS requires
significantly more fine-grained measurement of functions' compute time and memory
usage. Since functions are short and stateless, small ephemeral entities (e.g. individuals
or underutilized data centers) can become FaaS service providers. However, this exacerbates
the already substantial challenges of 1) ensuring integrity of computation, 2) minimizing
information revealed to the service provider, and 3) accurately measuring computational
resource usage.To address these challenges, we introduce S-FaaS, the first architecture
and implementation of FaaS to provide strong security and accountability guarantees
using Intel SGX. To match the dynamic event-driven nature of FaaS, we introduce a
new key distribution enclave and a novel transitive attestation protocol. A core contribution
of S-FaaS is our set of reusable resource measurement mechanisms that securely measure
compute time and memory usage inside an enclave. We have integrated S-FaaS into the
OpenWhisk FaaS framework and provide this as open source software.},
  doi       = {10.1145/3338466.3358916},
  isbn      = {9781450368261},
  keywords  = {intel sgx, function-as-a-service, resource measurement},
  location  = {London, United Kingdom},
  numpages  = {15},
  url       = {https://doi.org/10.1145/3338466.3358916},
}

@InProceedings{Lachmann2016,
  author    = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
  booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
  title     = {Fine-Grained Test Case Prioritization for Integration Testing of Delta-Oriented Software Product Lines},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {1–10},
  publisher = {Association for Computing Machinery},
  series    = {FOSD 2016},
  abstract  = {Software product line (SPL) testing is a challenging task, due to the huge number
of variants sharing common functionalities to be taken into account for efficient
testing. By adopting the concept of regression testing, incremental SPL testing strategies
cope with this challenge by exploiting the reuse potential of test artifacts between
subsequent variants under test. In previous work, we proposed delta-oriented test
case prioritization for incremental SPL integration testing, where differences between
architecture test model variants allow for reasoning about the order of reusable test
cases to be executed. However, the prioritization left two issues open, namely (1)
changes to component behavior are ignored, which may also influence component interactions
and, (2) the weighting and ordering of similar test cases result in an unintended
clustering of test cases. In this paper, we extend the test case prioritization technique
by (1) incorporating changes to component behavior allowing for a more fine-grained
analysis and (2) defining a dissimilarity measure to avoid clustered test case orders.
We prototyped our test case prioritization technique and evaluated its applicability
and effectiveness by means of a case study from the automotive domain showing positive
results.},
  doi       = {10.1145/3001867.3001868},
  isbn      = {9781450346474},
  keywords  = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
  location  = {Amsterdam, Netherlands},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3001867.3001868},
}

@InProceedings{Hang2019,
  author    = {Hang, Zijun and Shi, Yang and Wen, Mei and Quan, Wei and Zhang, Chunyuan},
  booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
  title     = {SWAP: A Sliding Window Algorithm for in-Network Packet Measurement},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {84–89},
  publisher = {Association for Computing Machinery},
  series    = {HP3C '19},
  abstract  = {Network traffic measurement is a fundamental part of many network applications, such
as DDOS detection, capacity planning, and quality-of-service improvement. To achieve
this, we need to count the number of packets passed during a past time interval. Traditionally,
switches sample the packets and send them to the CPU for analysis. It is unavoidable
that the sampling will sacrifice the measuring accuracy. Nowadays, programmable switches
can keep the counters in the data plane. However, they still rely on the CPU to drain
and clear the records periodically, which brings in too much communication latency.
To overcome these disadvantages, we propose a metering mechanism under the RMT architectural
model called SWAP. SWAP is carefully designed to count the number of packets during
an interval accurately with little hardware resource usage. We prototype it using
P4 and simulation results show SWAP achieves high efficiency and moderate accuracy
at line speed.},
  doi       = {10.1145/3318265.3318280},
  isbn      = {9781450366380},
  keywords  = {P4, programmable switches, network algorithm, software-defined networks},
  location  = {Xi'an, China},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3318265.3318280},
}

@InProceedings{ElMrabet2019,
  author    = {El Mrabet, Zakaria and Ezzari, Mehdi and Elghazi, Hassan and El Majd, Badr Abou},
  booktitle = {Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security},
  title     = {Deep Learning-Based Intrusion Detection System for Advanced Metering Infrastructure},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {NISS19},
  abstract  = {Smart grid is an alternative solution of the conventional power grid which harnesses
the power of the information technology to save the energy and meet todays' environment
requirements. Due to the inherent vulnerabilities in the information technology, the
smart grid is exposed to wide variety of threats that could be translated into cyber-attacks.
In this paper, we develop a deep learning-based intrusion detection system to defend
against cyber-attacks in the advanced metering infrastructure network. The proposed
machine learning approach is trained and tested extensively on an empirical industrial
dataset which is composed of several attack' categories including the scanning, buffer
overflow, and denial of service attacks. Then, an experimental comparison in terms
of detection accuracy is conducted to evaluate the performance of the proposed approach
with Na\"{\i}ve Bayes, Support Vector Machine, and Random Forest. The obtained results
suggest that the proposed approaches produce optimal results comparing to the other
algorithms. Finally, we propose a network architecture to deploy the proposed anomaly-based
intrusion detection system across the Advanced metering infrastructure network. In
addition, we propose a network security architecture composed of two types of Intrusion
detection system types, Host and Network based, deployed across the Advanced Metering
Infrastructure network to inspect the traffic and detect the malicious one at all
the levels.},
  articleno = {58},
  doi       = {10.1145/3320326.3320391},
  isbn      = {9781450366458},
  keywords  = {Deep learning, Intrusion detection system, Advanced Metering Infrastructure, cross entropy loss, detection accuracy},
  location  = {Rabat, Morocco},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3320326.3320391},
}

@InProceedings{Peldszus2016,
  author    = {Peldszus, Sven and Kulcs\'{a}r, G\'{e}za and Lochau, Malte and Schulze, Sandro},
  booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
  title     = {Continuous Detection of Design Flaws in Evolving Object-Oriented Programs Using Incremental Multi-Pattern Matching},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {578–589},
  publisher = {Association for Computing Machinery},
  series    = {ASE 2016},
  abstract  = {Design flaws in object-oriented programs may seriously corrupt code quality thus
increasing the risk for introducing subtle errors during software maintenance and
evolution. Most recent approaches identify design flaws in an ad-hoc manner, either
focusing on software metrics, locally restricted code smells, or on coarse-grained
architectural anti-patterns. In this paper, we utilize an abstract program model capturing
high-level object-oriented code entities, further augmented with qualitative and quantitative
design-related information such as coupling/cohesion. Based on this model, we propose
a comprehensive methodology for specifying object-oriented design flaws by means of
compound rules integrating code metrics, code smells and anti-patterns in a modular
way. This approach allows for efficient, automated design-flaw detection through incremental
multi-pattern matching, by facilitating systematic information reuse among multiple
detection rules as well as between subsequent detection runs on continuously evolving
programs. Our tool implementation comprises well-known anti-patterns for Java programs.
The results of our experimental evaluation show high detection precision, scalability
to real-size programs, as well as a remarkable gain in efficiency due to information
reuse.},
  doi       = {10.1145/2970276.2970338},
  isbn      = {9781450338455},
  keywords  = {object-oriented software architecture, design-flaw detection, continuous software evolution},
  location  = {Singapore, Singapore},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2970276.2970338},
}

@InProceedings{Pradhan2021,
  author    = {Pradhan, Ayush and Joy, Eldhose and Jawagal, Harsha and Prasad Jayaraman, Sundar},
  booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
  title     = {A Framework for Leveraging Contextual Information in Automated Domain Specific Comprehension},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {263–270},
  publisher = {Association for Computing Machinery},
  series    = {ISEEIE 2021},
  abstract  = {When it comes to information, Enterprises today are seen as a black hole, a mass of
it goes in but gets difficult to extract the practical knowledge out of it. An automated
system that has the ability to consume this large mass of information and provide
specific, knowledgeable, domain-oriented responses back, will go a long way in unlocking
the value of this large-scale unstructured information. In a bid to enrich the answering
system's accuracy in Machine Reading Comprehension (MRC), we propose a domain-specific
Question Answers (QuAns) framework that specifically aims to auto-generate questions
from a domain-based document using an improvised Sequence to Sequence (Seq2Seq) technique
equipped with Attention and Copy mechanism. The generated questions are conditioned
on a set of candidate answers, derived using a combination of heuristic-driven and
graph-based techniques. Further, it also leverages the contextual information by pooling
strategy to build an automated response system using a deep custom fine-tuned Bidirectional
Encoder Representations from Transformers (BERT) framework and retrieving the top-k
contexts for a user query. The evaluation of the QuAns architecture is performed in
combination with human supervision as at times, the automated metrics like BLEU, Exact
Match (EM), F1 score, etc. fail to gauge the diverse semantic and structural aspects
of a generated response. Primarily, the proffered ensemble technique has leveraged
the augmented domain knowledge to enrich the answering response efficacy and improving
the EM and F1 score by 14.86% and 12.76% respectively over Vanilla BERT architecture.
To enhance the user experience, the conversational system is equipped with Natural
Language Generation (NLG) to present a human-readable response. Our architectural
pipeline aims to provide a one-stop solution for the organizations in processing huge
volumes of multidisciplinary data by significantly reducing the human introspection
and the overhead cost.},
  doi       = {10.1145/3459104.3459148},
  isbn      = {9781450389839},
  location  = {Seoul, Republic of Korea},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3459104.3459148},
}

@Article{Luong2019,
  author     = {Luong, Doanh Kim and Ali, Muhammad and Benamrane, Fouad and Ammar, Ibrahim and Hu, Yim-Fun},
  journal    = {SIGMETRICS Perform. Eval. Rev.},
  title      = {Seamless Handover for Video Streaming over an SDN-Based Aeronautical Communications Network},
  year       = {2019},
  issn       = {0163-5999},
  month      = jan,
  number     = {3},
  pages      = {98–99},
  volume     = {46},
  abstract   = {There have been increasing interests in applying Software Defined Networking (SDN)
to aeronautical communications primarily for air traffic management purposes. From
the service passenger communications' point of view, a major goal is to improve passengers'
perception of quality of experience on the infotainment services being provided for
them. Due to the high speed of aircrafts and the use of multiple radio technologies
during different flight phases and across different areas, vertical handovers between
these different radio technologies are envisaged. This poses a challenge to maintain
the quality of service during such handovers, especially for high bandwidth applications
such as video streaming. This paper proposes an SDN-based aeronautical communications
architecture consisting of both satellite and terrestrial-based radio technology.
In addition, an experimental implementation of the Locator ID Separation Protocol
(LISP) protocol with built-in multi-homing capability over the SDN-based architecture
was proposed to handle vertical handovers between the satellite and other radio technologies
onboard the aircraft. By using both objective and subjective Quality of Experience
(QoE) metrics, the simulation experiments show the benefit of combining LISP with
SDN to improve the video streaming quality during the handover in the aeronautical
communication environment.},
  address    = {New York, NY, USA},
  doi        = {10.1145/3308897.3308943},
  issue_date = {December 2018},
  keywords   = {sdn, multi-homing, vertical handovers, aeronautical communications, lisp mobility},
  numpages   = {2},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3308897.3308943},
}

@InProceedings{Karedla2015,
  author    = {Karedla, Rama},
  booktitle = {Applicative 2015},
  title     = {Programming for the Intel Xeon Processor},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {Applicative 2015},
  abstract  = {Software programmers tend to focus on the software layer leaving performance on the
table by not taking advantage of the underlying hardware. This talk will help the
programmer take advantage of the underlying Intel Xeon server architecture to write
more efficient programs. We broadly cover topics such as time measurement, memory
ordering, making efficient use of the multi level caches, NUMA aware programming and
the use of the many compute cores available in the Xeon architecture via multi-threading.We
hope to show the benefit to both, latency and throughput oriented applications. The
talk will also address using the new AVX vector registers to achieve higher performance,
and briefly touch upon the recently announced Transactional Synchronization Extensions
(TSX) features. Examples of application profiling will demonstrate the benefit of
optimizing for performance in parallel with code development.},
  doi       = {10.1145/2742580.2742810},
  isbn      = {9781450335270},
  location  = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2742580.2742810},
}

@InProceedings{Lazarescu2014,
  author    = {Lazarescu, Mihai T. and Cohen, Albert and Guatto, Adrien and L\^{e}, Nhat Minn and Lavagno, Luciano and Pop, Antoniu and Prieto, Manuel and Terechko, Andrei and Sutii, Alexandru},
  booktitle = {Proceedings of the 17th International Workshop on Software and Compilers for Embedded Systems},
  title     = {Energy-Aware Parallelization Flow and Toolset for C Code},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {79–88},
  publisher = {Association for Computing Machinery},
  series    = {SCOPES '14},
  abstract  = {Multicore architectures are increasingly used in embedded systems to achieve higher
throughput with lower energy consumption. This trend accentuates the need to convert
existing sequential code to effectively exploit the resources of these architectures.
We present a parallelization flow and toolset for legacy C code that includes a performance
estimation tool, a parallelization tool, and a streaming-oriented parallelization
framework. These are part of the work-in-progress EU FP7 PHARAON project that aims
to develop a complete set of techniques and tools to guide and assist software development
for heterogeneous parallel architectures. We demonstrate the effectiveness of the
use of the toolset in an experiment where we measure the parallelization quality and
time for inexperienced users, and the parallelization flow and performance results
for the parallelization of a practical example of a stereo vision application.},
  doi       = {10.1145/2609248.2609264},
  isbn      = {9781450329415},
  keywords  = {execution profiling, program parallelization, energy estimation, data dependency analysis},
  location  = {Sankt Goar, Germany},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2609248.2609264},
}

@Article{Ohkawa2014,
  author     = {Ohkawa, Takeshi and Uetake, Daichi and Yokota, Takashi and Ootsu, Kanemitsu and Baba, Takanobu},
  journal    = {SIGARCH Comput. Archit. News},
  title      = {Reconfigurable and Hardwired ORB Engine on FPGA by Java-to-HDL Synthesizer for Realtime Application},
  year       = {2014},
  issn       = {0163-5964},
  month      = jun,
  number     = {5},
  pages      = {77–82},
  volume     = {41},
  abstract   = {A platform for networked FPGA system design, which is named "ORB Engine", is proposed
to add more controllability and design productivity on FPGA-based systems composed
of software and hardwired IPs. A developer can define an object-oriented interface
for the circuit IP in FPGA, and implement the control sequence part using Java. The
circuit IP in FPGA can be handled through object-oriented interface from variety of
programing languages like C++, Java, Python, Ruby and so on. Application specific
and high-efficiency circuit for ORB (Object Request Broker) protocol processing is
synthesized from easy-handling Java code using JavaRock Java-to-HDL synthesizer within
the de-facto standard CORBA (Common Object Request Broker Architecture). The measurement
result shows a very low latency as low as 200us of UDP/IP packet in/out and exhibits
a fluctuation free delay performance, which is desirable for real-time applications.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2641361.2641374},
  issue_date = {December 2013},
  numpages   = {6},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2641361.2641374},
}

@Article{Tiwari2014,
  author     = {Tiwari, Umesh and Kumar, Santosh},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {In-out Interaction Complexity Metrics for Component-Based Software},
  year       = {2014},
  issn       = {0163-5948},
  month      = sep,
  number     = {5},
  pages      = {1–4},
  volume     = {39},
  abstract   = {In the current state of software engineering, component-based software development
is one of the most alluring paradigms for developing large and complex software products.
In this software engineering methodology pre-engineered, pre-tested, context-based,
adaptable, deployable software components are assembled according to a predefined
architecture. Rather than developing a system from scratch, component-based software
development emphasizes the integration of these components according to the user's
requirements and specifications. In component-based software, the components interact
to access and provide services and functionality to each other. Currently, the emphasis
of industry and researchers is on developing impressive and efficient metrics and
measurement tools to analyze the interaction complexity among these components. To
represent the request and the response of services among components, we have used
outgoing edges and incoming edges respectively. In this paper we have defined these
interactions as In-Interactions and Out-Interactions. The metrics proposed in this
paper are solely based on the interactions among the components. In this work some
simple methods and metrics for computing the complexity of composable components are
suggested. The metrics discussed in this paper include the computation of interaction
complexities as Total-Interactions of a component, Total- Interactions of component-based
software, Interaction-Ratio of a component, Interaction-Ratio of component-based software,
Average- Interaction among components and Interaction-Percentage of components.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2659118.2659135},
  issue_date = {September 2014},
  keywords   = {component-based software development, adaptable, in-interactions, out-interactions, context-based, metrics, pre-engineered},
  numpages   = {4},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2659118.2659135},
}

@InProceedings{Monfared2020,
  author    = {Monfared, Saleh Khalaj and Hajihassani, Omid and Kiarostami, Mohammad Sina and Zanjani, Soroush Meghdadi and Rahmati, Dara and Gorgin, Saeid},
  booktitle = {49th International Conference on Parallel Processing - ICPP : Workshops},
  title     = {BSRNG: A High Throughput Parallel BitSliced Approach for Random Number Generators},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICPP Workshops '20},
  abstract  = {In this work, a high throughput method for generating high-quality Pseudo-Random
Numbers using the bitslicing technique is proposed. In such a technique, instead of
the conventional row-major data representation, column-major data representation is
employed, which allows the bitslicing implementation to take full advantage of all
the available datapath of the hardware platform. By employing this data representation
as building blocks of algorithms, we showcase the capability and scalability of our
proposed method in various PRNG methods in the category of block and stream ciphers.
The LFSR-based (Linear Feedback Shift Register) nature of the PRNG in our implementation
perfectly suits the GPU’s many-core structure due to its register oriented architecture.
In the proposed SIMD vectorized GPU implementation, each GPU thread can generate several
32 pseudo-random bits in each LFSR clock cycle. We then compare our implementation
with some of the most significant PRNGs that display a satisfactory performance throughput
and randomness criteria. The proposed implementation successfully passes the NIST
test for statistical randomness and bit-wise correlation criteria. For computer-based
PRNG and the optical solutions in terms of performance and performance per cost, this
technique is efficient while maintaining an acceptable randomness measure. Our highest
performance among all of the implemented CPRNGs with the proposed method is achieved
by the MICKEY 2.0 algorithm, which shows 40% improvement over state of the art NVIDIA’s
proprietary high-performance PRNG, cuRAND library, achieving 2.72 Tb/s of throughput
on the affordable NVIDIA GTX 2080 Ti.},
  articleno = {12},
  doi       = {10.1145/3409390.3409402},
  isbn      = {9781450388689},
  keywords  = {Cryptography, Stream cipher, Bitslicing, cuRAND, PRNG, High-performance, CUDA},
  location  = {Edmonton, AB, Canada},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3409390.3409402},
}

@Article{Floris2018,
  author     = {Floris, Alessandro and Ahmad, Arslan and Atzori, Luigi},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {QoE-Aware OTT-ISP Collaboration in Service Management: Architecture and Approaches},
  year       = {2018},
  issn       = {1551-6857},
  month      = apr,
  number     = {2s},
  volume     = {14},
  abstract   = {It is a matter of fact that quality of experience (QoE) has become one of the key
factors determining whether a new multimedia service will be successfully accepted
by the final users. Accordingly, several QoE models have been developed with the aim
of capturing the perception of the user by considering as many influencing factors
as possible. However, when it comes to adopting these models in the management of
the services and networks, it frequently happens that no single provider has access
to all of the tools to either measure all influencing factors parameters or control
over the delivered quality. In particular, it often happens to the over-the-top (OTT)
and Internet service providers (ISPs), which act with complementary roles in the service
delivery over the Internet. On the basis of this consideration, in this article we
first highlight the importance of a possible OTT-ISP collaboration for a joint service
management in terms of technical and economic aspects. Then we propose a general reference
architecture for a possible collaboration and information exchange among them. Finally,
we define three different approaches, namely joint venture, customer lifetime value
based, and QoE fairness based. The first aims to maximize the revenue by providing
better QoE to customers paying more. The second aims to maximize the profit by providing
better QoE to the most profitable customers (MPCs). The third aims to maximize QoE
fairness among all customers. Finally, we conduct simulations to compare the three
approaches in terms of QoE provided to the users, profit generated for the providers,
and QoE fairness.},
  address    = {New York, NY, USA},
  articleno  = {36},
  doi        = {10.1145/3183517},
  issue_date = {May 2018},
  keywords   = {OTT, quality of experience, Internet service providers, ISP, Over The Top service providers, QoE management, OTT-ISP collaboration, QoE},
  numpages   = {24},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3183517},
}

@InProceedings{Rrushi2015,
  author    = {Rrushi, Julian L. and Farhangi, Hassan and Nikolic, Radina and Howey, Clay and Carmichael, Kelly and Palizban, Ali},
  booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
  title     = {By-Design Vulnerabilities in the ANSI C12.22 Protocol Specification},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {2231–2236},
  publisher = {Association for Computing Machinery},
  series    = {SAC '15},
  abstract  = {The ANSI C12.22 is a standard that specifies interfaces to data communication networks
in the smart grid. In this paper we discuss several vulnerabilities by design that
we discovered in the ANSI C12.22 protocol specification during an analysis of the
overall protocol architecture. The consequences of an exploitation of those vulnerabilities
consist of denial of service conditions and disruptions to ANSI C12.22 nodes and relays.
We developed attack code to experiment with exploitations of most of the vulnerabilities
that we discuss in this paper. Our research testbed consisted of meters that we emulated
via the Trilliant TstBench software. The emulated meters were running on virtual Windows
machines on a virtual network. In the paper, we provide details of the vulnerabilities
by design that we identified, and thus propose a series of revisions of the ANSI C12.22
protocol specification with the objective of mitigating those vulnerabilities.},
  doi       = {10.1145/2695664.2695835},
  isbn      = {9781450331968},
  keywords  = {smartgrid security, vulnerabilities, ANSI C12.22 protocol},
  location  = {Salamanca, Spain},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2695664.2695835},
}

@InProceedings{Silva2015,
  author    = {da Silva, Madalena P. and Dantas, Mario A.R. and Gon\c{c}alves, Alexandre L. and Pinto, Alex R.},
  booktitle = {Proceedings of the 11th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
  title     = {A Managing QoE Approach for Provisioning User Experience Aware Services Using SDN},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {51–58},
  publisher = {Association for Computing Machinery},
  series    = {Q2SWinet '15},
  abstract  = {Provision and delivery of services with quality is a classic research problem, however
the computational resources available in the network infrastructure of providers are,
usually, managed with conventional Quality of Service (QoS) parameters. This paper
presents an approach of Quality of Experience (QoE) management for providing services
aware of the user experience. QoE modeling and architecture are proposed, with a semantic
engine able to learn the user's experience during the use of a service, detecting
violations of QoS metrics and providing information, allowing the controller to perform
actions in the elements of the Software Defined Networking. The experimental results
demonstrate that the proposal is feasible and functional and that the time spent between
QoE detection and adaptation of policies in network resources do not influence the
quality perceived by the user.},
  doi       = {10.1145/2815317.2815321},
  isbn      = {9781450337571},
  keywords  = {software-defined network, semantic engine, quality of service, quality of experience},
  location  = {Cancun, Mexico},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2815317.2815321},
}

@InProceedings{Odema2018,
  author    = {Odema, Mohanad and Adly, Ihab and El-Baz, Ahmed and Amin, Hani},
  booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
  title     = {A RESTful Architecture for Portable Remote Online Experimentation Services},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {102–105},
  publisher = {Association for Computing Machinery},
  series    = {ICSIE '18},
  abstract  = {In this paper, an architecture is proposed to deliver portable remote online experimentation
services. This can benefit the educational and academic sectors in terms of providing
remote online accessibility to real experiment setups. Thus, the users can be relieved
from geographical and time dependence for the experiment to be conducted. Nowadays,
almost all web services leverage the efficiency and prevalence of the REST (Representational
State Transfer) architecture. Hence, this proposed remote online service has been
implemented in compliance with the RESTful architectural style.Web-based experiments
require compatibility with any of the users' portable devices and accessibility at
any time. A RESTful architecture can fulfill these requirements. In addition, different
experiments can be made available online based on this architecture while sharing
the same infrastructure. A case study has been selected to obtain measurements of
different force components existing inside wind tunnels. The complete implementation
of this system is provided starting from the embedded controller retrieving sensor
measurements to the web server development and user interface design.},
  doi       = {10.1145/3220267.3220280},
  isbn      = {9781450364690},
  keywords  = {Online testing and experimentation, RESTful architecture, Remote testing facilities},
  location  = {Cairo, Egypt},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3220267.3220280},
}

@InProceedings{Couto2019,
  author    = {Couto, Christian Marlon Souza and Terra, Ricardo},
  booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
  title     = {A Quality-Oriented Approach to Recommend Move Method Refactorings},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {315},
  publisher = {Association for Computing Machinery},
  series    = {SBQS'19},
  abstract  = {Refactoring processes are common in large software systems, especially when developers
neglect architectural erosion process for long periods. Even though there are many
refactoring approaches, very few consider the refactoring impact on the software quality.Given
this scenario, we propose a refactoring approach to software systems oriented to software
quality metrics. Based on the QMOOD (Quality Model for Object Oriented Design), the
main idea is to move methods between classes in order to maximize the values of the
quality metrics. Using a formal notation, we describe the problem as follows. Given
a software system S, our approach recommends a sequence of refactorings R1, R2,...,
Rn that result in system versions S1, S2,..., Sn, where quality(Si+1) &gt; quality(Si).We
performed three types of evaluation to verify the usefulness of our implemented tool,
called QMove. First, we applied our approach on 13 open-source systems that we modified
by randomly moving a subset of its methods to other classes, then checking if our
approach would recommend the moved methods to return to their original place, and
we achieve 84% recall, on average. Second, we compared QMove against two state-of-art
refactoring tools (JMove and JDeodorant) on the 13 previously evaluated systems, and
QMove showed better recall value (84%) than the other two (30% and 29%, respectively).
Third, we conducted the same comparison among QMove, JMove, and JDeodorant applied
in two proprietary systems where experts evaluated the quality of the recommendations.
QMove obtained eight positively evaluated recommendations from the experts, against
two and none of JMove and JDeodorant, respectively.},
  doi       = {10.1145/3364641.3364680},
  isbn      = {9781450372824},
  keywords  = {refactoring, software architecture, quality metrics},
  location  = {Fortaleza, Brazil},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3364641.3364680},
}

@InProceedings{Sar2019,
  author    = {van der Sar, Jerom and Donkervliet, Jesse and Iosup, Alexandru},
  booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
  title     = {Yardstick: A Benchmark for Minecraft-like Services},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {243–253},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '19},
  abstract  = {Online gaming applications entertain hundreds of millions of daily active players
and often feature vastly complex architecture. Among online games, Minecraft-like
games simulate unique (e.g., modifiable) environments, are virally popular, and are
increasingly provided as a service. However, the performance of Minecraft-like services,
and in particular their scalability, is not well understood. Moreover, currently no
benchmark exists for Minecraft-like games. Addressing this knowledge gap, in this
work we design and use the Yardstick benchmark to analyze the performance of Minecraft-like
services. Yardstick is based on an operational model that captures salient characteristics
of Minecraft-like services. As input workload, Yardstick captures important features,
such as the most-popular maps used within the Minecraft community. Yardstick captures
system- and application-level metrics, and derives from them service-level metrics
such as frequency of game-updates under scalable workload. We implement Yardstick,
and, through real-world experiments in our clusters, we explore the performance and
scalability of popular Minecraft-like servers, including the official vanilla server,
and the community-developed servers Spigot and Glowstone. Our findings indicate the
scalability limits of these servers, that Minecraft-like services are poorly parallelized,
and that Glowstone is the least viable option among those tested.},
  doi       = {10.1145/3297663.3310307},
  isbn      = {9781450362399},
  keywords  = {yardstick, distributed systems, as a service, online gaming, minecraft, benchmark},
  location  = {Mumbai, India},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3297663.3310307},
}

@InProceedings{Zodik2016,
  author    = {Zodik, Gabi},
  booktitle = {Proceedings of the 9th India Software Engineering Conference},
  title     = {Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {11–12},
  publisher = {Association for Computing Machinery},
  series    = {ISEC '16},
  abstract  = {The second wave of change presented by the age of mobility, wearables, and IoT focuses
on how organizations and enterprises, from a wide variety of commercial areas and
industries, will use and leverage the new technologies available. Businesses and industries
that don't change with the times will simply cease to exist.Applications need to be
powered by cognitive and contextual technologies to support real-time proactive decisions.
These decisions will be based on the mobile context of a specific user or group of
users, incorporating location, time of day, current user task, and more. Driven by
the huge amounts of data produced by mobile and wearables devices, and influenced
by privacy concerns, the next wave in computing will need to exploit data and computing
at the edge of the network. Future mobile apps will have to be cognitive to 'understand'
user intentions based on all the available interactions and unstructured data.Mobile
applications are becoming increasingly ubiquitous, going beyond what end users can
easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business
(B2B) apps, only about 30% of the development efforts appear in the interface of the
mobile app. For example, areas such as the collaborative nature of the software or
the shortened development cycle and time-to-market are not apparent to end users.
The other 70% of the effort invested is dedicated to integrating the applications
with back-office systems and developing those aspects of the application that operate
behind the scenes.An important, yet often complex, part of the solution and mobile
app takes place far from the public eye-in the back-office environment. It is there
that various aspects of customer relationship management must be addressed: tracking
usage data, pushing out messaging as needed, distributing apps to employees within
the enterprise, and handling the wide variety of operational and management tasks-often
involving the collection and monitoring of data from sensors and wearable devices.
All this must be carried out while addressing security concerns that range from verifying
user identities, to data protection, to blocking attempted breaches of the organization,
and activation of malicious code. Of course, these tasks must be augmented by a systematic
approach and vigilant maintenance of user privacy.The first wave of the mobile revolution
focused on development platforms, run-time platforms, deployment, activation, and
management tools for multi-platform environments, including comprehensive mobile device
management (MDM). To realize the full potential of this revolution, we must capitalize
on information about the context within which mobile devices are used. With both employees
and customers, this context could be a simple piece of information such as the user
location or time of use, the hour of the day, or the day of the week. The context
could also be represented by more complex data, such as the amount of time used, type
of activity performed, or user preferences. Further insight could include the relationship
history with the user and the user's behavior as part of that relationship, as well
as a long list of variables to be considered in various scenarios. Today, with the
new wave of wearables, the definition of context is being further extended to include
environmental factors such as temperature, weather, or pollution, as well as personal
factors such as heart rate, movement, or even clothing worn.In both B2E and B2C situations,
a context-dependent approach, based on the appropriate context for each specific user,
offers a superior tool for working with both employees and clients alike. This mode
of operation does not start and end with the individual user. Rather, it takes into
account the people surrounding the user, the events taking place nearby, appliances
or equipment activated, the user's daily schedule, as well as other, more general
information, such as the environment and weather.Developing enterprise-wide, context-dependent,
mobile solutions is still a complex challenge. A system of real added-value services
must be developed, as well as a comprehensive architecture. These four-tier architectures
comprise end-user devices like wearables and smartphones, connected to systems of
engagement (SoEs), and systems of record (SoRs). All this is needed to enable data
analytics and collection in the context where it is created. The data collected will
allow further interaction with employees or customers, analytics, and follow-up actions
based on the results of that analysis. We also need to ensure end-to-end (E2E) security
across these four tiers, and to keep the data and application contexts in sync. These
are just some of the challenges being addressed by IBM Research.As an example, these
technologies could be deployed in the retail space, especially in brick-and-mortar
stores. Identifying a customer entering a store, detecting her location among the
aisles, and cross-referencing that data with the customer's transaction history, could
lead to special offers tailor-made for that specific customer or suggestions relevant
to her purchasing process. This technology enables real-world implementation of metrics,
analytics, and other tools familiar to us from the online realm. We can now measure
visits to physical stores in the same way we measure web page hits: analyze time spent
in the store, the areas visited by the customer, and the results of those visits.
In this way, we can also identify shoppers wandering around the store and understand
when they are having trouble finding the product they want to purchase. We can also
gain insight into the standard traffic patterns of shoppers and how they navigate
a store's floors and departments. We might even consider redesigning the store layout
to take advantage of this insight to enhance sales.In healthcare, the context can
refer to insight extracted from data received from sensors on the patient, from either
his mobile device or wearable technology, and information about the patient's environment
and location at that moment in time. This data can help determine if any assistance
is required. For example, if a patient is discharged from the hospital for continued
at-home care, doctors can continue to remotely monitor his condition via a system
of sensors and analytic tools that interpret the sensor readings.This approach can
also be applied to the area of safety. Scientists at IBM Research are developing a
platform that collects and analyzes data from wearable technology to protect the safety
of employees working in construction, heavy industry, manufacturing, or out in the
field. This solution can serve as a real-time warning system by analyzing information
gathered from wearable sensors embedded in personal protective equipment, such as
smart safety helmets and protective vests, and in the workers' individual smartphones.
These sensors can continuously monitor a worker's pulse rate, movements, body temperature,
and hydration level, as well as environmental factors such as noise level, and other
parameters. The system can provide immediate alerts to the worker about any dangers
in the work environment to prevent possible injury. It can also be used to prevent
accidents before they happen or detect accidents once they occur. For example, with
sophisticated algorithms, we can detect if a worker falls based on a sudden difference
in elevations detected by an accelerometer, and then send an alert to notify her peers
and supervisor or call for help. Monitoring can also help ensure safety in areas where
continuous exposure to heat or dangerous materials must be limited based on regulated
time periods.Mobile technologies can also help manage events with massive numbers
of participants, such as professional soccer games, music festivals, and even large-scale
public demonstrations, by sending alerts concerning long and growing lines or specific
high-traffic areas. These technologies can be used to detect accidents typical of
large-scale gatherings, send warnings about overcrowding, and alert the event organizers.
In the same way, they can alleviate parking problems or guide public transportation
operators- all via analysis and predictive analytics.IBM Research - Haifa is currently
involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers
have a special expertise in time- and location-based intelligent applications, including
visual maps that display activity contexts and predictive analytics systems for mobile
data and users. In another area, IBM researchers in Haifa are developing new cognitive
services driven from the unique data available on mobile and wearable devices. Looking
to the future, the IBM Research team is further advancing the integration of wearable
technology, augmented reality systems, and biometric tools for mobile user identity
validation.Managing contextual data and analyzing the interaction between the different
kinds of data presents fascinating challenges for the development of next-generation
programming. For example, we need to rethink when and where data processing and computations
should occur: Is it best to leave them at the user-device level, or perhaps they should
be moved to the back-office systems, servers, and/or the cloud infrastructures with
which the user device is connected? New-age applications are becoming more and more
distributed. They operate on a wide range of devices, such as wearable technologies,
use a variety of sensors, and depend on cloud-based systems.As a result, a new distributed
programming paradigm is emerging to meet the needs of these use-cases and real-time
scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and
data in business systems, and must be able to shift computation from the cloud to
the edge, based on context in close to real-time. By processing data at the edge of
the network, close to where the interactions and processing are happening, we can
help reduce latency and offer new opportunities for improved privacy and security.Despite
all these interactions, data collection, and the analytic insights based upon them-we
cannot forget the issues of privacy. Without a proper and reliable solution that offers
more control over what personal data is shared and how it is used, people will refrain
from sharing information. Such sharing is necessary for developing and understanding
the context in which people are carrying out various actions, and to offer them tools
and services to enhance their actions.In the not-so-distant future, we anticipate
the appearance of ad-hoc networks for wearable technology systems that will interact
with one another to further expand the scope and value of available context-dependent
data.},
  doi       = {10.1145/2856636.2876471},
  isbn      = {9781450340182},
  location  = {Goa, India},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2856636.2876471},
}

@InProceedings{Mo2018,
  author    = {Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  title     = {Experiences Applying Automated Architecture Analysis Tool Suites},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {779–789},
  publisher = {Association for Computing Machinery},
  series    = {ASE 2018},
  abstract  = {In this paper, we report our experiences of applying three complementary automated
software architecture analysis techniques, supported by a tool suite, called DV8,
to 8 industrial projects within a large company. DV8 includes two state-of-the-art
architecture-level maintainability metrics—Decoupling Level and Propagation Cost,
an architecture flaw detection tool, and an architecture root detection tool. We collected
development process data from the project teams as input to these tools, reported
the results back to the practitioners, and followed up with telephone conferences
and interviews. Our experiences revealed that the metrics scores, quantitative debt
analysis, and architecture flaw visualization can effectively bridge the gap between
management and development, help them decide if, when, and where to refactor. In particular,
the metrics scores, compared against industrial benchmarks, faithfully reflected the
practitioners’ intuitions about the maintainability of their projects, and enabled
them to better understand the maintainability relative to other projects internal
to their company, and to other industrial products. The automatically detected architecture
flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify
the “hotspots" within the systems that are responsible for high maintenance costs.
Except for the two smallest projects for which both architecture metrics indicated
high maintainability, all other projects are planning or have already begun refactorings
to address the problems detected by our analyses. We are working on further automating
the tool chain, and transforming the analysis suite into deployable services accessible
by all projects within the company.},
  doi       = {10.1145/3238147.3240467},
  isbn      = {9781450359375},
  keywords  = {Software Quality, Software Maintenance, Software Architecture},
  location  = {Montpellier, France},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3238147.3240467},
}

@InProceedings{Meryem2018,
  author    = {Meryem, Amar and Samira, Douzi and Bouabid, El Ouahidi},
  booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
  title     = {Enhancing Cloud Security Using Advanced MapReduce K-Means on Log Files},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {63–67},
  publisher = {Association for Computing Machinery},
  series    = {ICSIM2018},
  abstract  = {Many customers ranked cloud security as a major challenge that threaten their work
and reduces their trust on cloud service's provider. Hence, a significant improvement
is required to establish better adaptations of security measures that suit recent
technologies and especially distributed architectures.Considering the meaningful recorded
data in cloud generated log files, making analysis on them, mines insightful value
about hacker's activities. It identifies malicious user behaviors and predicts new
suspected events. Not only that, but centralizing log files, prevents insiders from
causing damage to system. In this paper, we proposed to take away sensitive log files
into a single server provider and combining both MapReduce programming and k-means
on the same algorithm to cluster observed events into classes having similar features.
To label unknown user behaviors and predict new suspected activities this approach
considers cosine distances and deviation metrics.},
  doi       = {10.1145/3178461.3178462},
  isbn      = {9781450354387},
  keywords  = {K-means, Deviation metric, MapReduce, Cloud Security, log files},
  location  = {Casablanca, Morocco},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3178461.3178462},
}

@InProceedings{Forget2015,
  author    = {Forget, Alain and Chiasson, Sonia and Biddle, Robert},
  booktitle = {Proceedings of the 2015 New Security Paradigms Workshop},
  title     = {Choose Your Own Authentication},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {1–15},
  publisher = {Association for Computing Machinery},
  series    = {NSPW '15},
  abstract  = {To solve the long-standing problems users have in creating and remembering text passwords,
a wide variety of alternative authentication schemes have been proposed. Some of these
schemes outperform others by various metrics in various contexts. However, none unilaterally
outperform all others, and so text passwords persist as the main scheme applications
depend upon. In this paper, we challenge the long-standing assumption that only one
authentication scheme can be offered by an application service. We propose Choose
Your Own Authentication (CYOA): a novel authentication architecture that enables users
to choose a scheme amongst several available alternatives. CYOA would enable users
to select whichever scheme best suits their preferences, abilities, and usage context.
Existing text password systems could easily be replaced. Furthermore, the three-party
architecture would enable delegating the management of authentication systems to trusted-third
parties. The architecture allows rapid deployment and testing of novel authentication
technologies. Our two-week usability study suggests that participants were willing
to leverage alternative schemes. Participants were confident that CYOA could keep
their financial information secure.},
  doi       = {10.1145/2841113.2841114},
  isbn      = {9781450337540},
  keywords  = {user study, usable security, survey, Authentication},
  location  = {Twente, Netherlands},
  numpages  = {15},
  url       = {https://doi.org/10.1145/2841113.2841114},
}

@InProceedings{Diamantopoulos2020,
  author    = {Diamantopoulos, Nikos and Wong, Jeffrey and Mattos, David Issa and Gerostathopoulos, Ilias and Wardrop, Matthew and Mao, Tobias and McFarland, Colin},
  booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
  title     = {Engineering for a Science-Centric Experimentation Platform},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {191–200},
  publisher = {Association for Computing Machinery},
  series    = {ICSE-SEIP '20},
  abstract  = {Netflix is an internet entertainment service that routinely employs experimentation
to guide strategy around product innovations. As Netflix grew, it had the opportunity
to explore increasingly specialized improvements to its service, which generated demand
for deeper analyses supported by richer metrics and powered by more diverse statistical
methodologies. To facilitate this, and more fully harness the skill sets of both engineering
and data science, Netflix engineers created a science-centric experimentation platform
that leverages the expertise of scientists from a wide range of backgrounds working
on data science tasks by allowing them to make direct code contributions in the languages
used by them (Python and R). Moreover, the same code that runs in production is able
to be run locally, making it straightforward to explore and graduate both metrics
and causal inference methodologies directly into production services.In this paper,
we provide two main contributions. Firstly, we report on the architecture of this
platform, with a special emphasis on its novel aspects: how it supports science-centric
end-to-end workflows without compromising engineering requirements. Secondly, we describe
its approach to causal inference, which leverages the potential outcomes conceptual
framework to provide a unified abstraction layer for arbitrary statistical models
and methodologies.},
  doi       = {10.1145/3377813.3381349},
  isbn      = {9781450371230},
  keywords  = {causal inference, A/B testing, experimentation, software architecture, science-centric},
  location  = {Seoul, South Korea},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3377813.3381349},
}

@InProceedings{Komisarek2021,
  author    = {Komisarek, Miko\l{}aj and Pawlicki, Marek and Kowalski, Miko\l{}aj and Marzecki, Adrian and Kozik, Rafa\l{} and Choraundefined, Micha\l{}},
  booktitle = {The 16th International Conference on Availability, Reliability and Security},
  title     = {Network Intrusion Detection in the Wild - the Orange Use Case in the SIMARGL Project},
  year      = {2021},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ARES 2021},
  abstract  = {There is a profuse abundance of network security incidents around the world every
day. Increasingly, services and data stored on servers fall victim to sophisticated
techniques that cause all sorts of damage. Hackers invent new ways to bypass security
measures and modify the existing viruses in order to deceive defense systems. Therefore,
in response to these illegal procedures, new ways to defend against them are being
developed. In this paper, a method for anomaly detection based on machine learning
technique is presented and a near real-time processing system architecture is proposed.
The main contribution is a test-run of ML algorithms on real-world data coming from
a world-class telecom operator. This work investigates the effectiveness of detecting
malicious behaviour in network packets using several machine learning techniques.
The results achieved are expressed with a set of metrics. For better clarity on the
classifier performance, 10-fold cross-validation was used.},
  articleno = {65},
  doi       = {10.1145/3465481.3470091},
  isbn      = {9781450390514},
  keywords  = {network intrusion detection, machine learning},
  location  = {Vienna, Austria},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3465481.3470091},
}

@InProceedings{Fernandes2018,
  author    = {Fernandes, Rodrigo and Sim\~{a}o, Jos\'{e} and Veiga, Lu\'{\i}s},
  booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
  title     = {EcoVMbroker: Energy-Aware Scheduling for Multi-Layer Datacenters},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {403–410},
  publisher = {Association for Computing Machinery},
  series    = {SAC '18},
  abstract  = {The cloud relies on efficient algorithms to find resources for jobs by fulfilling
the job's requirements and at the same time optimise an objective function. Utility
is a measure of the client satisfaction that can be seen as an objective function
maximised by schedulers based on the agreed service level agreement (SLA). We propose
EcoVM-Broker which can reduce energy consumption by using dynamic voltage frequency
scaling (DVFS) and applying reductions of utility, different for classes of users
and across ranges of resource allocations. Using efficient data structures and a hierarchical
architecture, we created a scalable solution for the fast growing heterogeneous cloud.
EcoVMBroker proved that we can delegate work in a hierarchical datacenter, make decisions
based on summaries of resource usage collected from several nodes and still be efficient.},
  doi       = {10.1145/3167132.3167178},
  isbn      = {9781450351911},
  keywords  = {DVFS, virtual machice scheduling, partial utility, energy efficiency},
  location  = {Pau, France},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3167132.3167178},
}

@InProceedings{Erb2018,
  author    = {Erb, Benjamin and Mei\ss{}ner, Dominik and Kargl, Frank and Steer, Benjamin A. and Cuadrado, Felix and Margan, Domagoj and Pietzuch, Peter},
  booktitle = {Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)},
  title     = {Graphtides: A Framework for Evaluating Stream-Based Graph Processing Platforms},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {GRADES-NDA '18},
  abstract  = {Stream-based graph systems continuously ingest graph-changing events via an established
input stream, performing the required computation on the corresponding graph. While
there are various benchmarking and evaluation approaches for traditional, batch-oriented
graph processing systems, there are no common procedures for evaluating stream-based
graph systems. We, therefore, present GraphTides, a generic framework which includes
the definition of an appropriate system model, an exploration of the parameter space,
suitable workloads, and computations required for evaluating such systems. Furthermore,
we propose a methodology and provide an architecture for running experimental evaluations.
With our framework, we hope to systematically support system development, performance
measurements, engineering, and comparisons of stream-based graph systems.},
  articleno = {3},
  doi       = {10.1145/3210259.3210262},
  isbn      = {9781450356954},
  keywords  = {graph processing, graph analytics, evolving graphs, evaluation, temporal graphs, stream-based graphs, measurements},
  location  = {Houston, Texas},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3210259.3210262},
}

@InProceedings{A.Panayiotou2021,
  author    = {A. Panayiotou, Nikolaos and P. Stavrou, Vasileios and E. Stergiou, Konstantinos},
  booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
  title     = {Applying the Industry 4.0 in a Smart Gas Grid: The Greek Gas Distribution Network Case},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {180–184},
  publisher = {Association for Computing Machinery},
  series    = {ISEEIE 2021},
  abstract  = {The aim of this paper is to design and implement a series of actions regarding the
operation of DEDA S.A. (Natural Gas Distribution Networks), based on principles of
Industry 4.0. The Natural Gas Distribution sector is one of the most critical and
innovative areas where Industry 4.0 can be applied, being part of critical infrastructure
management. At first, company's business process architecture was developed, with
the aim to export DEDA's business process and functional specifications related to
the required information systems. Subsequently, company's communication network is
implemented alongside the company's gas network, in coordination with the company's
control room.In addition, modernization of metering system is taking place in order
to exchange information between smart meters and the control room. A number of Information
Systems, such as the pipeline surveillance system and the Business Intelligence system
will also be installed in order to ensure communication at different levels using
Cloud technologies. The implementation is expected to improve DEDA's organization,
increasing customers' service level. As a result, there will be an expected increase
in the operational efficiency of DEDA's network through the use of advanced technologies,
in cooperation with business process modelling techniques. The effort should be continued
in this direction in order to achieve even greater improvement in business processes,
information systems and pipeline automation.},
  doi       = {10.1145/3459104.3459136},
  isbn      = {9781450389839},
  location  = {Seoul, Republic of Korea},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3459104.3459136},
}

@InProceedings{Boss2016,
  author    = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
  booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
  title     = {Setting up Architectural SW Health Builds in a New Product Line Generation},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ECSAW '16},
  abstract  = {Setting up a new product line generation in a mature domain, typically does not start
from scratch but takes into consideration the architecture and assets of the former
product line generation. Being able to accommodate legacy and 3rd party code is one
of the major product line qualities to be met. On the other side, product line qualities
like reusability, maintainability and alterability, i.e. being able to cope up with
a large amount of variability, with configurability and fast integratability are major
drivers.While setting up a new product line generation and thus a new corresponding
architecture, we this time focused on architectural software (SW) health and tracking
of architectural metrics from the very beginning. Taking the definition of "architecture
being a set of design decisions" [18] literally, we attempt to implement an architectural
check for every design decision taken. Architectural design decisions in our understanding
do not only - and even not mainly - deal with the definition of components and their
interaction but with patterns and rules or anti-patterns. The rules and anti-patterns,
"what not to do" or more often also "what not to do <u>any more</u>", is even more
important in setting up a new product line generation because developers are not only
used to the old style of developing and the old architecture, but also still have
to develop assets for both generations.In this article we describe selected architectural
checks that we have implemented, the layered architecture check and the check for
usage of obsolete services. Additionally we discuss selected architectural metrics:
the coupling coefficient metrics and the instability metrics. In the summary and outlook
we describe our experiences and still open topics in setting up architectural SW health
checks for a large-scale product line.The real-world examples are taken from the domain
of Engine Control Unit development at Robert Bosch GmbH.},
  articleno = {16},
  doi       = {10.1145/2993412.3003392},
  isbn      = {9781450347815},
  keywords  = {architectural technical debt, software architecture, product line development, technical debt, software erosion, architectural checks, embedded software},
  location  = {Copenhagen, Denmark},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2993412.3003392},
}

@InProceedings{Mohammadi2019,
  author    = {Mohammadi, Farnaz and Panou, Angeliki and Ntantogian, Christoforos and Karapistoli, Eirini and Panaousis, Emmanouil and Xenakis, Christos},
  booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
  title     = {CUREX: SeCUre and PRivate HEalth Data EXchange},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {263–268},
  publisher = {Association for Computing Machinery},
  series    = {WI '19 Companion},
  abstract  = {The Health sector's increasing dependence on digital information and communication
infrastructures renders it vulnerable to privacy and cybersecurity threats, especially
as the theft of health data has become lucrative for cyber criminals. CUREX comprehensively
addresses the protection of the confidentiality and integrity of health data by producing
a novel, flexible and scalable situational awareness-oriented platform. It allows
a healthcare provider to assess cybersecurity and privacy risks that are exposed to
and suggest optimal strategies for addressing these risks with safeguards tailored
to each business case and application. CUREX is fully GDPR compliant by design. At
its core, a decentralised architecture enhanced by a private blockchain infrastructure
ensures the integrity of the data and –most importantly- the patient safety. Crucially,
CUREX expands beyond technical measures and improves cyber hygiene through training
and awareness activities for healthcare personnel. Its validation focuses on highly
challenging cases of health data exchange, spanning patient cross-border mobility,
remote healthcare, and data exchange for research.},
  doi       = {10.1145/3358695.3361753},
  isbn      = {9781450369886},
  keywords  = {Risk assessment, Blockchain, eHealth, Cybersecurity, Cyber hygiene},
  location  = {Thessaloniki, Greece},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3358695.3361753},
}

@InProceedings{Eckel2021,
  author    = {Eckel, Michael and Riemann, Tim},
  booktitle = {The 16th International Conference on Availability, Reliability and Security},
  title     = {Userspace Software Integrity Measurement},
  year      = {2021},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ARES 2021},
  abstract  = {Todays computing systems are more interconnected and sophisticated than ever before.
Especially in healthcare 4.0, services and infrastructures rely on cyber-physical
systemss (CPSess) and Internet of Things (IoT) devices. This adds to the complexity
of these highly connected systems and their manageability. Even worse, the variety
of emerging cyber attacks is becoming more severe and sophisticated, making healthcare
one of the most important sectors with major security risks. The development of appropriate
countermeasures constitutes one of the most complex and difficult challenges in cyber
security research. Research areas include, among others, anomaly detection, network
security, multi-layer event detection, cyber resiliency, and integrity protection.
Securing the integrity of software running on a device is a desirable protection goal
in the context of systems security. With a Trusted Platform Module (TPM), measured
boot, and remote attestation there exist technologies to ensure that a system has
booted up correctly and runs only authentic software. The Linux Integrity Measurement
Architecture (IMA) extends these principles into the operating systems (OSes), measuring
native binaries before they are loaded. However, interpreted language files, such
as Java classes and Python scripts, are not considered executables and are not measured
as such. Contemporary OSess ship with many of these and it is vital to consider them
as security-critical as native binaries. In this paper, we introduce Userspace Software
Integrity Measurement (USIM) for the Linux OSes. Userspace Software Integrity Measurement
(USIM) enables interpreters to measure, log, and irrevocably anchor critical events
in the TPM. We develop a software library in C which provides TPM-based measurement
functionality as well as the USIM service, which provides concurrent access handling
to the TPM based event logging. Further, we develop and implement a concept to realize
highly frequent event logging on the slow TPM. We integrate this library into the
Java Virtual Machine (JVM) to measure Java classes and show that it can be easily
integrated into other interpreters. With performance measurements we demonstrate that
our contribution is feasible and that overhead is negligible.},
  articleno = {138},
  doi       = {10.1145/3465481.3470018},
  isbn      = {9781450390514},
  keywords  = {integrity verification, Trusted Computing, Systems security},
  location  = {Vienna, Austria},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3465481.3470018},
}

@InProceedings{Petz2020,
  author    = {Petz, Adam},
  booktitle = {Proceedings of the 7th Symposium on Hot Topics in the Science of Security},
  title     = {An Infrastructure for Faithful Execution of Remote Attestation Protocols},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {HotSoS '20},
  abstract  = {Experience shows that even with a well-intentioned user at the keyboard, a motivated
attacker can compromise a computer system at a layer below or adjacent to the shallow
forms of authentication that are now accepted as commonplace[3]. Therefore, rather
than asking "Can we trust the person behind the keyboard", a still better question
might be: "Can we trust the computer system underneath?". An emerging technology for
gaining trust in a remote computing system is remote attestation. Remote attestation
is the activity of making a claim about properties of a target by supplying evidence
to an appraiser over a network[2]. Although many existing approaches to remote attestation
wisely adopt a layered architecture-where the bottom layers measure layers above-the
dependencies between components remain static and measurement orderings fixed. For
modern computing environments with diverse topologies, we can no longer fix a target
architecture any more than we can fix a protocol to measure that architecture.Copland
[1] is a domain-specific language and formal framework that provides a vocabulary
for specifying the goals of layered attestation protocols. It also provides a reference
semantics that characterizes system measurement events and evidence handling; a foundation
for comparing protocol alternatives. The aim of this work is to refine the Copland
semantics to a more fine-grained notion of attestation manager execution-a high-privilege
thread of control responsible for invoking attestation services and bundling evidence
results. This refinement consists of two cooperating components called the Copland
Compiler and the Attestation Virtual Machine (AVM). The Copland Compiler translates
a Copland protocol description into a sequence of primitive attestation instructions
to be executed in the AVM. When considered in combination with advances in virtualization,
trusted hardware, and high-assurance system software components-like compilers, file-systems,
and OS kernels-a formally verified remote attestation infrastructure creates exciting
opportunities for building system-level security arguments.},
  articleno = {17},
  doi       = {10.1145/3384217.3386393},
  isbn      = {9781450375610},
  location  = {Lawrence, Kansas},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3384217.3386393},
}

@Article{Ji2018,
  author     = {Ji, Kecheng and Ling, Ming and Shi, Longxing and Pan, Jianping},
  journal    = {ACM Trans. Embed. Comput. Syst.},
  title      = {An Analytical Cache Performance Evaluation Framework for Embedded Out-of-Order Processors Using Software Characteristics},
  year       = {2018},
  issn       = {1539-9087},
  month      = aug,
  number     = {4},
  volume     = {17},
  abstract   = {Utilizing analytical models to evaluate proposals or provide guidance in high-level
architecture decisions is been becoming more and more attractive. A certain number
of methods have emerged regarding cache behaviors and quantified insights in the last
decade, such as the stack distance theory and the memory level parallelism (MLP) estimations.
However, prior research normally oversimplified the factors that need to be considered
in out-of-order processors, such as the effects triggered by reordered memory instructions,
and multiple dependences among memory instructions, along with the merged accesses
in the same MSHR entry. These ignored influences actually result in low and unstable
precisions of recent analytical models.By quantifying the aforementioned effects,
this article proposes a cache performance evaluation framework equipped with three
analytical models, which can more accurately predict cache misses, MLPs, and the average
cache miss service time, respectively. Similar to prior studies, these analytical
models are all fed with profiled software characteristics in which case the architecture
evaluation process can be accelerated significantly when compared with cycle-accurate
simulations.We evaluate the accuracy of proposed models compared with gem5 cycle-accurate
simulations with 16 benchmarks chosen from Mobybench Suite 2.0, Mibench 1.0, and Mediabench
II. The average root mean square errors for predicting cache misses, MLPs, and the
average cache miss service time are around 4%, 5%, and 8%, respectively. Meanwhile,
the average error of predicting the stall time due to cache misses by our framework
is as low as 8%. The whole cache performance estimation can be sped by about 15 times
versus gem5 cycle-accurate simulations and 4 times when compared with recent studies.
Furthermore, we have shown and studied the insights between different performance
metrics and the reorder buffer sizes by using our models. As an application case of
the framework, we also demonstrate how to use our framework combined with McPAT to
find out Pareto optimal configurations for cache design space explorations.},
  address    = {New York, NY, USA},
  articleno  = {79},
  doi        = {10.1145/3233182},
  issue_date = {August 2018},
  keywords   = {Analytical models, cache misses, cache miss service time, software characteristics, memory level parallelism},
  numpages   = {25},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3233182},
}

@InProceedings{Araldo2020,
  author    = {Araldo, Andrea and Stefano, Alessandro Di and Stefano, Antonella Di},
  booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
  title     = {Resource Allocation for Edge Computing with Multiple Tenant Configurations},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {1190–1199},
  publisher = {Association for Computing Machinery},
  series    = {SAC '20},
  abstract  = {Edge Computing (EC) consists in deploying computational resources, e.g., memory, CPUs,
at the Edge of the network, e.g., base stations, access points, and run there a part
of the computation currently running on the Cloud. This approach promises to reduce
latency, inter-domain traffic and enhance user experience. Since resources at the
Edge are scarce, resource allocation is crucial for EC. While most of the studies
assume users interact directly with the Edge submitting a sequence of tasks, we instead
consider that users will interact with different Service Providers (SPs), as they
currently do in the Web. We therefore consider the case of a Network Operator (NO)
that owns the resources at the Edge and must decide how much resource to allocate
to the different tenants (SPs).We propose MORA, a polynomial time strategy which allows
the NO to maximize its utility, which can be inter-domain traffic savings, improved
users' QoE or other metrics of interest. The core of MORA is that (i) it exploits
service elasticity, i.e., the fact that services can adapt to the resources allocated
by the NO and rely on a remote Cloud for the excess of computation, (ii) it is suitable
for micro-services architecture, which decomposes a single service in a set of components,
which MORA places in the different computational nodes of the Edge and (iii) it copes
with multi-dimensional resources, e.g., memory and CPUs. After analyzing the properties
of the algorithm, we show numerically that it performs close to the optimum. To guarantee
reproducibility, the numerical evaluation is performed on publicly available traces
from Google and Alibaba clusters and in synthetic scenarios and our code is open source.},
  doi       = {10.1145/3341105.3374026},
  isbn      = {9781450368667},
  keywords  = {container systems, edge computing, network optimization, resource allocation, cloud computing},
  location  = {Brno, Czech Republic},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3341105.3374026},
}

@Article{Xu2019,
  author     = {Xu, Zhen and Sun, Chengjie and Long, Yinong and Liu, Bingquan and Wang, Baoxun and Wang, Mingjiang and Zhang, Min and Wang, Xiaolong},
  journal    = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  title      = {Dynamic Working Memory for Context-Aware Response Generation},
  year       = {2019},
  issn       = {2329-9290},
  month      = sep,
  number     = {9},
  pages      = {1419–1431},
  volume     = {27},
  abstract   = {In human-to-human conversations, the context generally provides several backgrounds
and strategic points for the following response. Therefore, many response generation
approaches have explored the methodologies to incorporate the context into the encoder–decoder
architecture, to generate context-aware responses that are remarkably relevant and
cohesive to the given context. However, most approaches pay less attention to semantic
interactions implicitly existing within contextual utterances, which are of great
importance to capture semantic clues of the given dialog context, indeed. This paper
proposes a dynamic working memory mechanism to model long-term semantic hints in the
conversation context, by performing semantic interactions between utterances and updating
context representation dynamically. Then, the outputs of the dynamic working memory
are employed to provide helpful clues for the encoder–decoder architecture to generate
responses to the given dialog. We have evaluated the proposed approach on Twitter
Customer Service Corpus and OpenSubtitles Corpus, with several automatic evaluation
metrics and the human evaluation, and the empirical results show the effectiveness
of the proposed method.},
  doi        = {10.1109/TASLP.2019.2915922},
  issue_date = {September 2019},
  numpages   = {13},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TASLP.2019.2915922},
}

@InProceedings{Marques2020,
  author    = {Marques, Jonatas and Levchenko, Kirill and Gaspary, Luciano},
  booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
  title     = {IntSight: Diagnosing SLO Violations with in-Band Network Telemetry},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {421–434},
  publisher = {Association for Computing Machinery},
  series    = {CoNEXT '20},
  abstract  = {Performance requirements for many of today's high-perfor-mance networks are expressed
as service-level objectives (SLOs), i.e., precise guarantees, typically on latency
and bandwidth, that a user can expect from the network. For network operators, monitoring
their own SLO compliance, and quickly diagnosing any violations, is a critical element
for effective operations. Unfortunately, existing network architectures are not engineered
for this purpose; there is no mechanism, for example, for the operator to monitor
the 95th per-centile latency experienced by a customer. Data plane programmability
has made per-packet measurements possible but brings the challenge of keeping the
monitoring overhead low and practical. In this paper, we present IntSight, a system
for highly accurate and fine-grained detection and diagnosis of SLO violations. The
main contribution of IntSight is, building upon in-band telemetry, introducing path-wise
computation of network metrics and selective generation of reports. We show the effectiveness
of IntSight by way of two use cases. Our evaluation using real networks also shows
that IntSight generates up to two orders of magnitude less monitoring traffic than
state-of-the-art approaches. Furthermore, its processing and memory requirements are
low and therefore compatible with currently existing programmable platforms.},
  doi       = {10.1145/3386367.3431306},
  isbn      = {9781450379489},
  location  = {Barcelona, Spain},
  numpages  = {14},
  url       = {https://doi.org/10.1145/3386367.3431306},
}

@InProceedings{Mirakhorli2015,
  author    = {Mirakhorli, Mehdi and Cleland-Huang, Jane},
  booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
  title     = {Modifications, Tweaks, and Bug Fixes in Architectural Tactics},
  year      = {2015},
  pages     = {377–380},
  publisher = {IEEE Press},
  series    = {MSR '15},
  abstract  = {Architectural qualities such as reliability, performance, and security, are often
realized in a software system through the adoption of tactical design decisions such
as the decision to use redundant processes, a heartbeat monitor, or a specific authentication
mechanism. Such decisions are critical for delivering a system that meets its quality
requirements. Despite the stability of high-level decisions, our analysis has shown
that tactic-related classes tend to be modified more frequently than other classes
and are therefore stronger predictors of change than traditional Object-Oriented coupling
and cohesion metrics. In this paper we present the results from this initial study,
including an analysis of why tactic-related classes are changed, and a discussion
of the implications of these findings for maintaining architectural quality over the
lifetime of a software system.},
  isbn      = {9780769555942},
  keywords  = {modifications, tactics, bugs, architectural decisions, metrics},
  location  = {Florence, Italy},
  numpages  = {4},
}

@Article{Patrignani2015,
  author     = {Patrignani, Marco and Agten, Pieter and Strackx, Raoul and Jacobs, Bart and Clarke, Dave and Piessens, Frank},
  journal    = {ACM Trans. Program. Lang. Syst.},
  title      = {Secure Compilation to Protected Module Architectures},
  year       = {2015},
  issn       = {0164-0925},
  month      = apr,
  number     = {2},
  volume     = {37},
  abstract   = {A fully abstract compiler prevents security features of the source language from being
bypassed by an attacker operating at the target language level. Unfortunately, developing
fully abstract compilers is very complex, and it is even more so when the target language
is an untyped assembly language. To provide a fully abstract compiler that targets
untyped assembly, it has been suggested to extend the target language with a protected
module architecture—an assembly-level isolation mechanism which can be found in next-generation
processors. This article provides a fully abstract compilation scheme whose source
language is an object-oriented, high-level language and whose target language is such
an extended assembly language. The source language enjoys features such as dynamic
memory allocation and exceptions. Secure compilation of first-order method references,
cross-package inheritance, and inner classes is also presented. Moreover, this article
contains the formal proof of full abstraction of the compilation scheme. Measurements
of the overhead introduced by the compilation scheme indicate that it is negligible.},
  address    = {New York, NY, USA},
  articleno  = {6},
  doi        = {10.1145/2699503},
  issue_date = {April 2015},
  keywords   = {Fully abstract compilation, protected module architecture},
  numpages   = {50},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2699503},
}

@InProceedings{Kofler2014,
  author    = {Kofler, Klaus and Davis, Gregory and Gesing, Sandra},
  booktitle = {Proceedings of the 2014 Symposium on Agent Directed Simulation},
  title     = {SAMPO: An Agent-Based Mosquito Point Model in OpenCL},
  year      = {2014},
  address   = {San Diego, CA, USA},
  publisher = {Society for Computer Simulation International},
  series    = {ADS '14},
  abstract  = {Agent-based modeling and simulations are applied for problems where population-level
patterns arise from the interaction of many autonomous individuals. These problems
are compute-intensive and excellent candidates for the use of parallel algorithms
and architectures. As a cross-platform software development framework for parallel
architectures, OpenCL appears as an ideal tool to implement such algorithms. However,
OpenCL does not natively support object-oriented development, which most of the toolkits
and frameworks used to build agent-based models require.The present work describes
an OpenCL implementation of an existing agent-based model, simulating populations
of the Anopheles gambiae mosquito, one of the most important vectors of malaria in
Africa. Discussed are the methods and techniques used to overcome the design challenges,
which arise when transitioning from an object-oriented program to an efficient OpenCL
implementation. In particular, the parallelism inside the program has been maximized,
dynamic divergent branching was reduced, and the number of data transfers between
the OpenCL host and device has been minimized as far as possible.Even though our implementation
was designed for this specific use case, the approach can be generalized to other
contexts, as most agent-based point models would benefit from the same basic design
decisions that we took for our implementation. Comparisons between the object-oriented
and the OpenCL implementation illustrate that using an OpenCL approach offers two
important performance benefits: an overall simulation time speedup of up to 576 with
no measurable loss of accuracy, and better scalability as the agent-population size
increases. The tradeoffs necessary to achieve these performance benefits and the implications
for future agent-based software development frameworks are discussed.},
  articleno = {5},
  keywords  = {agent-based modelling, GPGPU, OpenCL},
  location  = {Tampa, Florida},
  numpages  = {10},
}

@InProceedings{Xu2014,
  author    = {Xu, Yi and Helal, Sumi},
  booktitle = {Proceedings of the 17th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
  title     = {Application Caching for Cloud-Sensor Systems},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {303–306},
  publisher = {Association for Computing Machinery},
  series    = {MSWiM '14},
  abstract  = {Driven by critical and pressing smart city applications, accessing massive numbers
of sensors by cloud-hosted services is becoming an emerging and inevitable situation.
Na\"{\i}vely connecting massive numbers of sensors to the cloud raises major scalability
and energy challenges. An architecture embodying distributed optimization is needed
to manage the scale and to allow limited energy sensors to last longer in such a dynamic
and high-velocity big data system. We developed a multi-tier architecture which we
call Cloud, Edge and Beneath (CEB). Based on CEB, we propose an Application Fragment
Caching Algorithm (AFCA) which selectively caches application fragments from the cloud
to lower layers of CEB to improve cloud scalability. Through experiments, we show
and measure the effect of AFCA on cloud scalability.},
  doi       = {10.1145/2641798.2641814},
  isbn      = {9781450330305},
  keywords  = {cloud-sensor systems, cloud computing, application caching},
  location  = {Montreal, QC, Canada},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2641798.2641814},
}

@InProceedings{Edoh2016,
  author    = {Edoh, Thierry Oscar C. and Atchome, Athanase and Alahassa, Bidossessi R.U. and Pawar, Pravin},
  booktitle = {Proceedings of the 2016 ACM Workshop on Multimedia for Personal Health and Health Care},
  title     = {Evaluation of a Multi-Tier Heterogeneous Sensor Network for Patient Monitoring: The Case of Benin},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {23–29},
  publisher = {Association for Computing Machinery},
  series    = {MMHealth '16},
  abstract  = {In this paper we propose and evaluate a wireless sensor network (WSN) system in order
to improve an existing patient-monitoring and surveillance system at the cardiologic
intensive care unit (CICU) of a large university clinic (Centre Hospitalier Universitaire
Hubert Koutoukou Maga - CHU-HKM) in Cotonou city of Benin (a West-African country).
We have designed a multi-tier architecture and simulated a heterogeneous, autonomous,
and energy efficient wireless sensor network system to overcome issues faced by existing
patient monitoring system in CICU such as manual collection and processing of data.
The improvement of the patient monitoring system has the objectives of providing affordable
and better health care service provision as well as autonomous and automatic collection
and processing of patient's bio-signals and environmental data. The proposed Wireless
Sensor Network consists of wireless heterogeneous nodes which sense patient bio-signals,
measure environmental parameters in the hospitalization rooms such as ambient temperature,
quality of air and send collected data to a gateway (central node) for processing
and storage. The conducted simulation experiments show that the proposed sensor network
architecture which uses ZigBee wireless standard and protocol highly improves the
patience monitoring and surveillance experience at CICU. It promotes collection and
autonomous processing of patient physiological data and room ambient temperature data.
Incorporating such system in CICU will be highly beneficial for taking a correct decision
during treatment. Beyond the accuracy and quality of the collected medical data, proposed
WSN is also designed to reduce the energy consumption within the sensor network system.},
  doi       = {10.1145/2985766.2985772},
  isbn      = {9781450345187},
  keywords  = {wireless sensors network, cooperative sensors, patient monitorin, zigbee standards, intensive care unit, cardiology},
  location  = {Amsterdam, The Netherlands},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2985766.2985772},
}

@InProceedings{Abderrahim2016,
  author    = {Abderrahim, Wiem and Choukair, Zied},
  booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
  title     = {PaaS Dependability Integration Architecture Based on Cloud Brokering},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {484–487},
  publisher = {Association for Computing Machinery},
  series    = {SAC '16},
  abstract  = {Cloud computing has revolutionized the way IT is provisioned nowadays since it exposes
computing capabilities as rental resources to consumers. The emergence of cloud computing
services hasn't though prevented outages in these environments even among high profile
ranked cloud providers. Tremendous efforts concentrated on fault management measures
have been applied for these environments. But they have been focused mainly on the
IaaS service model and have been operated on the cloud provider side alone. In this
context, this paper proposes an architecture for cloud brokering that implements dependability
properties in an end to end way involving different cloud actors and all over the
cloud service models SaaS, PaaS and IaaS.},
  doi       = {10.1145/2851613.2851874},
  isbn      = {9781450337397},
  keywords  = {fault tolerance, cloud provider, PaaS, IaaS, SaaS, fault management, cloud broker, fault forecasting, dependability},
  location  = {Pisa, Italy},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2851613.2851874},
}

@Article{Araldo2018,
  author     = {Araldo, Andrea and Dan, Gyorgy and Rossi, Dario},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {Caching Encrypted Content Via Stochastic Cache Partitioning},
  year       = {2018},
  issn       = {1063-6692},
  month      = feb,
  number     = {1},
  pages      = {548–561},
  volume     = {26},
  abstract   = {In-network caching is an appealing solution to cope with the increasing bandwidth
demand of video, audio, and data transfer over the Internet. Nonetheless, in order
to protect consumer privacy and their own business, content providers CPs increasingly
deliver encrypted content, thereby preventing Internet service providers ISPs from
employing traditional caching strategies, which require the knowledge of the objects
being transmitted. To overcome this emerging tussle between security and efficiency,
in this paper we propose an architecture in which the ISP partitions the cache space
into slices, assigns each slice to a different CP, and lets the CPs remotely manage
their slices. This architecture enables transparent caching of encrypted content and
can be deployed in the very edge of the ISP’s network i.e., base stations and femtocells,
while allowing CPs to maintain exclusive control over their content. We propose an
algorithm, called SDCP, for partitioning the cache storage into slices so as to maximize
the bandwidth savings provided by the cache. A distinctive feature of our algorithm
is that ISPs only need to measure the aggregated miss rates of each CP, but they need
not know the individual objects that are requested. We prove that the SDCP algorithm
converges to a partitioning that is close to the optimal, and we bound its optimality
gap. We use simulations to evaluate SDCP’s convergence rate under stationary and nonstationary
content popularity. Finally, we show that SDCP significantly outperforms traditional
reactive caching techniques, considering both CPs with perfect and with imperfect
knowledge of their content popularity.},
  doi        = {10.1109/TNET.2018.2793892},
  issue_date = {February 2018},
  numpages   = {14},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2018.2793892},
}

@InProceedings{Zou2018,
  author    = {Zou, Luyao and Rui, Xuhua and Nguyen, Tuan Anh and Min, Dugki and Choi, Eunmi and Thang, Tran Duc and Son, Nguyen Nhu},
  booktitle = {Proceedings of the 2018 International Conference on Information Science and System},
  title     = {A Scalable Network Area Storage with Virtualization: Modelling and Evaluation Using Stochastic Reward Nets},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {225–233},
  publisher = {Association for Computing Machinery},
  series    = {ICISS '18},
  abstract  = {Modelling and analysis of storage system in data centers for availability prediction
is of paramount importance. Many studies in literature proposed different architectures
and techniques to enhance availability of the storage system. In this paper, we proposed
to incorporate virtualization techniques on a network area storage. We used stochastic
reward nets to model the system's architecture and operational scenarios. Furthermore,
we investigated various measures of interests including steady state availability,
downtime and downtime cost, and sensitivity of the system availability with respect
to impacting parameters. The analysis results show that the proposed storage system
with virtualization can obtain an acceptable level of service availability. Furthermore,
the sensitivity analysis also points out complicated dependences of service availability
upon system parameters. This paper presents a preliminary study to help guide the
development of a scalable network area storage with virtualization in practice.},
  doi       = {10.1145/3209914.3209918},
  isbn      = {9781450364218},
  keywords  = {Stochastic Reward Nets, Availability, Reliability, Network Attached Storage},
  location  = {Jeju, Republic of Korea},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3209914.3209918},
}

@InProceedings{Mo2019,
  author    = {Mo, Fan and Shahin Shamsabadi, Ali and Katevas, Kleomenis and Cavallaro, Andrea and Haddadi, Hamed},
  booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
  title     = {Poster: Towards Characterizing and Limiting Information Exposure in DNN Layers},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {2653–2655},
  publisher = {Association for Computing Machinery},
  series    = {CCS '19},
  abstract  = {Pre-trained Deep Neural Network (DNN) models are increasingly used in smartphones
and other user devices to enable prediction services, leading to potential disclosures
of (sensitive) information from training data captured inside these models. Based
on the concept of generalization error, we propose a framework to measure the amount
of sensitive information memorized in each layer of a DNN. Our results show that,
when considered individually, the last layers encode a larger amount of information
from the training data compared to the first layers. We find that the same DNN architecture
trained with different datasets has similar exposure per layer. We evaluate an architecture
to protect the most sensitive layers within an on-device Trusted Execution Environment
(TEE) against potential white-box membership inference attacks without the significant
computational overhead.},
  doi       = {10.1145/3319535.3363279},
  isbn      = {9781450367479},
  keywords  = {trusted execution environment, privacy, deep learning, training data, sensitive information exposure},
  location  = {London, United Kingdom},
  numpages  = {3},
  url       = {https://doi.org/10.1145/3319535.3363279},
}

@InProceedings{Wagle2014,
  author    = {Wagle, Shyam S.},
  booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
  title     = {SLA Assured Brokering (SAB) and CSP Certification in Cloud Computing},
  year      = {2014},
  address   = {USA},
  pages     = {1016–1017},
  publisher = {IEEE Computer Society},
  series    = {UCC '14},
  abstract  = {Due to lack of information of the cloud service providers (CSPs), customers can not
easily choose services according to their requirement and due to vendor lock-in and
lack of interoperability standards among cloud service providers, customers cannot
switch the providers once services are subscribed from CSPs. Recently proposed third
party architecture which is called cloud broker can access inter-cloud and provides
services to the customers according to their requirement but providing SLA based cloud
services as per their requirement is still missing in current researches. In our work,
we propose the SLA assured brokering framework which matches the requirements of the
customer with SLA offered by CSPs using similarity matching algorithm and willingness
to pay capacity for the services. It also measures the services offered by CSPs for
certifying and ranking the CSPs.},
  doi       = {10.1109/UCC.2014.167},
  isbn      = {9781479978816},
  keywords  = {SLA, Cloud Brokering, Certifying, Similarity Matching},
  numpages  = {2},
  url       = {https://doi.org/10.1109/UCC.2014.167},
}

@InProceedings{Sirin2016,
  author    = {Sirin, Utku and Appuswamy, Raja and Ailamaki, Anastasia},
  booktitle = {Proceedings of the 12th International Workshop on Data Management on New Hardware},
  title     = {OLTP on a Server-Grade ARM: Power, Throughput and Latency Comparison},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {DaMoN '16},
  abstract  = {Although scaling out of low-power cores is an alternative to power-hungry Intel Xeon
processors for reducing the power overheads, they have proven inadequate for complex,
non-parallelizable workloads. On the other hand, by the introduction of the 64-bit
ARMv8 architecture, traditionally low power ARM processors have become powerful enough
to run computationally intensive server-class applications.In this study, we compare
a high-performance Intel x86 processor with a commercial implementation of the ARM
Cortex-A57. We measure the power used, throughput delivered and latency quantified
when running OLTP workloads. Our results show that the ARM processor consumes 3 to
15 times less power than the x86, while penalizing OLTP throughput by a much lower
factor (1.7 to 3). As a result, the significant power savings deliver up to 9 times
higher energy efficiency. The x86's heavily optimized power-hungry micro-architectural
structures contribute to throughput only marginally. As a result, the x86 wastes power
when utilization is low, while lightweight ARM processor consumes only as much power
as it is utilized, achieving energy proportionality. On the other hand, ARM's quantified
latency can be up to 11x higher than x86 towards to the tail of latency distribution,
making x86 more suitable for certain type of service-level agreements.},
  articleno = {10},
  doi       = {10.1145/2933349.2933359},
  isbn      = {9781450343190},
  location  = {San Francisco, California},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2933349.2933359},
}

@InProceedings{Dalgkitsis2018,
  author    = {Dalgkitsis, Anestis and Louta, Malamati and Karetsos, George T.},
  booktitle = {Proceedings of the 22nd Pan-Hellenic Conference on Informatics},
  title     = {Traffic Forecasting in Cellular Networks Using the LSTM RNN},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {28–33},
  publisher = {Association for Computing Machinery},
  series    = {PCI '18},
  abstract  = {In this work we design and implement a Neural Network that can identify recurrent
patterns in various metrics which can be then used for cellular network traffic forecasting.
Based on a custom architecture and memory, this Neural Network can handle prediction
tasks faster and more accurately in real life scenarios. This approach offers a solution
for service providers to enhance cellular network performance, by utilizing effectively
the available resources. In order to provide a robust conclusion about the performance
and precision of the proposed Neural Network, multiple predictions were made using
the same data-set and the results were compared against other similar algorithms from
the literature.},
  doi       = {10.1145/3291533.3291540},
  isbn      = {9781450366106},
  keywords  = {long-short term memory, cellular networks, traffic forecasting, recurrent neural networks},
  location  = {Athens, Greece},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3291533.3291540},
}

@InProceedings{Pang2018,
  author    = {Pang, Haitian and Zhang, Cong and Wang, Fangxin and Hu, Han and Wang, Zhi and Liu, Jiangchuan and Sun, Lifeng},
  booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
  title     = {Optimizing Personalized Interaction Experience in Crowd-Interactive Livecast: A Cloud-Edge Approach},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {1217–1225},
  publisher = {Association for Computing Machinery},
  series    = {MM '18},
  abstract  = {Enabling users to interact with broadcasters and audience, the crowd-interactive livecast
greatly improves viewer's quality of experience (QoE) and attracts millions of daily
active users recently. In addition to striking the balance between resource utilization
and viewers' QoE met in the traditional video streaming service, this novel service
needs to take supererogatory efforts to improve the interaction QoE, which reflects
the viewer interaction experience. To tackle this issue, we conduct measurement studies
over a large-scale dataset crawled from a representative livecast service provider.
We observe that the individual's interaction pattern is quite heterogeneous: only
10% viewers proactively participate in the interaction, and the rest viewers usually
watch passively. Incorporating the insight into the emerging cloud-edge architecture,
we propose a framework PIECE, which optimizes the Personalized Interaction Experience
with Cloud-Edge architecture (PIECE) for intelligent user access control and livecast
distribution. In particular, we first devise a novel deep neural network based algorithm
to predict users' interaction intensity using the historical viewer pattern. We then
design an algorithm to maximize the individual's QoE, by strategically matching viewer
sessions and transcoding-delivery paths over cloud-edge infrastructure. Finally, we
use trace-driven experiments to verify the effectiveness of PIECE. Our results show
that our prediction algorithm outperforms the state-of-the-art algorithms with a much
smaller mean absolute error (40% reduction). Furthermore, in comparison with the cloud-based
video delivery strategy, the proposed framework can simultaneously improve the average
viewers QoE (26% improvement) and interaction QoE (21% improvement), while maintaining
a high streaming bitrate.},
  doi       = {10.1145/3240508.3240642},
  isbn      = {9781450356657},
  keywords  = {cloud-edge, interactive live streaming, viewer interaction},
  location  = {Seoul, Republic of Korea},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3240508.3240642},
}

@InProceedings{Chang2016,
  author    = {Chang, Wanli and Roy, Debayan and Zhang, Licong and Chakraborty, Samarjit},
  booktitle = {Proceedings of the 35th International Conference on Computer-Aided Design},
  title     = {Model-Based Design of Resource-Efficient Automotive Control Software},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICCAD '16},
  abstract  = {Automotive platforms today run hundreds of millions of lines of software code implementing
a large number of different control applications spanning across safety-critical functionality
to driver assistance and comfort-related functions. While such control software today
is largely designed following model-based approaches, the underlying models do not
take into account the details of the implementation platforms, on which the software
would eventually run. Following the state-of-the-art in control theory, the focus
in such design is restricted to ensuring the stability of the designed controllers
and meeting control performance objectives, such as settling time or peak overshoot.
However, automotive platforms are highly cost-sensitive and the issue of designing
"resource-efficient" controllers has largely been ignored so far and is addressed
using very ad hoc techniques. In this paper, we will illustrate how, following traditional
embedded systems design oriented thinking, computation, communication and memory issues
can be incorporated in the controller design stage, thereby resulting in control software
not only satisfying the usual control performance metrics but also making efficient
utilization of the resources on distributed automotive architectures.},
  articleno = {34},
  doi       = {10.1145/2966986.2980075},
  isbn      = {9781450344661},
  location  = {Austin, Texas},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2966986.2980075},
}

@Article{Aeijoe2015,
  author     = {\"{A}ij\"{o}, Tomi and J\"{a}\"{a}skel\"{a}inen, Pekka and Elomaa, Tapio and Kultala, Heikki and Takala, Jarmo},
  journal    = {ACM Trans. Archit. Code Optim.},
  title      = {Integer Linear Programming-Based Scheduling for Transport Triggered Architectures},
  year       = {2015},
  issn       = {1544-3566},
  month      = dec,
  number     = {4},
  volume     = {12},
  abstract   = {Static multi-issue machines, such as traditional Very Long Instructional Word (VLIW)
architectures, move complexity from the hardware to the compiler. This is motivated
by the ability to support high degrees of instruction-level parallelism without requiring
complicated scheduling logic in the processor hardware. The simpler-control hardware
results in reduced area and power consumption, but leads to a challenge of engineering
a compiler with good code-generation quality.Transport triggered architectures (TTA),
and other so-called exposed datapath architectures, take the compiler-oriented philosophy
even further by pushing more details of the datapath under software control. The main
benefit of this is the reduced register file pressure, with a drawback of adding even
more complexity to the compiler side.In this article, we propose an Integer Linear
Programming (ILP)-based instruction scheduling model for TTAs. The model describes
the architecture characteristics, the particular processor resource constraints, and
the operation dependencies of the scheduled program. The model is validated and measured
by compiling application kernels to various TTAs with a different number of datapath
components and connectivity. In the best case, the cycle count is reduced to 52% when
compared to a heuristic scheduler. In addition to producing shorter schedules, the
number of register accesses in the compiled programs is generally notably less than
those with the heuristic scheduler; in the best case, the ILP scheduler reduced the
number of register file reads to 33% of the heuristic results and register file writes
to 18%. On the other hand, as expected, the ILP-based scheduler uses distinctly more
time to produce a schedule than the heuristic scheduler, but the compilation time
is within tolerable limits for production-code generation.},
  address    = {New York, NY, USA},
  articleno  = {59},
  doi        = {10.1145/2845082},
  issue_date = {January 2016},
  keywords   = {Code generation, transport triggered architectures, instruction-level parallelism, integer linear programming, exposed datapath},
  numpages   = {22},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2845082},
}

@InProceedings{Barros2020,
  author    = {Barros, Daniel D. R. and Horita, Fl\'{a}vio and Fantinato, Denis G.},
  booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
  title     = {Data Mining Tool to Discover DevOps Trends from Public Repositories: Predicting Release Candidates with Gthbmining.Rc},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {658–663},
  publisher = {Association for Computing Machinery},
  series    = {SBES '20},
  abstract  = {Public repositories have been performing an essential role in bringing software and
services to technical communities and general users. Most of the cases, public repositories
have a DevOps tool, with a live and historical database behind it, to support delivering
and all steps this software or service should adopt before going to production. This
paper introduces gthbmining, a data mining set of tools to discover DevOps trends
from public repositories, and presents the module gthbmining.rc. Considering the premise
of a GitHub public repository, the main contribution here is predicting release candidates,
an important label a software release has. The methodology, architecture, components
and interfaces are explained, as well as potential users. The results show a reliable
and flexible tool, as classifiers metrics and graphics are provided, along with the
possibility to add new data mining algorithms in the open source module presented.
Related works are also supplied, and a conclusion shows the outcomes gthbmining.rc
can provide.},
  doi       = {10.1145/3422392.3422501},
  isbn      = {9781450387538},
  keywords  = {Data Mining, GitHub Mining Tool, DevOps, Release Candidate},
  location  = {Natal, Brazil},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3422392.3422501},
}

@InProceedings{Yoon2019,
  author    = {Yoon, Changhoon and Kim, Kwanwoo and Kim, Yongdae and Shin, Seungwon and Son, Sooel},
  booktitle = {The World Wide Web Conference},
  title     = {Doppelg\"{a}Ngers on the Dark Web: A Large-Scale Assessment on Phishing Hidden Web Services},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {2225–2235},
  publisher = {Association for Computing Machinery},
  series    = {WWW '19},
  abstract  = {Anonymous network services on the World Wide Web have emerged as a new web architecture,
called the Dark Web. The Dark Web has been notorious for harboring cybercriminals
abusing anonymity. At the same time, the Dark Web has been a last resort for people
who seek freedom of the press as well as avoid censorship. This anonymous nature allows
website operators to conceal their identity and thereby leads users to have difficulties
in determining the authenticity of websites. Phishers abuse this perplexing authenticity
to lure victims; however, only a little is known about the prevalence of phishing
attacks on the Dark Web. We conducted an in-depth measurement study to demystify the
prevalent phishing websites on the Dark Web. We analyzed the text content of 28,928
HTTP Tor hidden services hosting 21 million dark webpages and confirmed 901 phishing
domains. We also discovered a trend on the Dark Web in which service providers perceive
dark web domains as their service brands. This trend exacerbates the risk of phishing
for their service users who remember only a partial Tor hidden service address. Our
work facilitates a better understanding of the phishing risks on the Dark Web and
encourages further research on establishing an authentic and reliable service on the
Dark Web.},
  doi       = {10.1145/3308558.3313551},
  isbn      = {9781450366748},
  location  = {San Francisco, CA, USA},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3308558.3313551},
}

@InProceedings{Coffman2018,
  author    = {Coffman, Austin R. and Bu\v{s}i\'{c}, Ana and Barooah, Prabir},
  booktitle = {Proceedings of the 5th Conference on Systems for Built Environments},
  title     = {Virtual Energy Storage from TCLs Using QoS Preserving Local Randomized Control},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {93–102},
  publisher = {Association for Computing Machinery},
  series    = {BuildSys '18},
  abstract  = {We propose a control architecture for distributed coordination of a collection of
on/off TCLs (thermostatically controlled loads), such as residential air conditioners,
to provide the same service to the power grid as a large battery. This involves a
collection of loads to coordinate their on/off decisions so that the aggregate power
consumption profile tracks a grid-supplied reference. A key constraint is to maintain
each consumer's quality of service (QoS). Recent works have proposed randomization
at the loads. Thermostats at the loads are replaced by a randomized controller, and
the grid broadcasts a scalar to all loads, which tunes the probability of turning
on or off at each load depending on its state. In this paper we propose a modification
of a previous design by Meyn and Bu\v{s}i\'{c}. The previous design by Meyn and Bu\v{s}i\'{c} ensures
that the indoor temperature remains within a pre-specified bound, but other QoS metrics,
especially the frequency of turning on and off was not limited. The controller we
propose can be tuned to reduce the cycling rate of a TCL to any desired degree. The
proposed design is compared against the design by Meyn and Bu\v{s}i\'{c} and another well
cited design in the literature on control of TCL populations, by Mathieu et al. We
show through simulations that the proposed controller is able to reduce the cycling
of individual ACs compared to the previous designs with little loss in tracking of
the grid-supplied reference signal.},
  doi       = {10.1145/3276774.3276777},
  isbn      = {9781450359511},
  keywords  = {randomized control, virtual energy storage, demand response, distributed control},
  location  = {Shenzen, China},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3276774.3276777},
}

@Article{Wang2019,
  author     = {Wang, Ji and Bao, Weidong and Zheng, Lei and Zhu, Xiaomin and Yu, Philip S.},
  journal    = {ACM Trans. Storage},
  title      = {An Attention-Augmented Deep Architecture for Hard Drive Status Monitoring in Large-Scale Storage Systems},
  year       = {2019},
  issn       = {1553-3077},
  month      = aug,
  number     = {3},
  volume     = {15},
  abstract   = {Data centers equipped with large-scale storage systems are critical infrastructures
in the era of big data. The enormous amount of hard drives in storage systems magnify
the failure probability, which may cause tremendous loss for both data service users
and providers. Despite a set of reactive fault-tolerant measures such as RAID, it
is still a tough issue to enhance the reliability of large-scale storage systems.
Proactive prediction is an effective method to avoid possible hard-drive failures
in advance. A series of models based on the SMART statistics have been proposed to
predict impending hard-drive failures. Nonetheless, there remain some serious yet
unsolved challenges like the lack of explainability of prediction results. To address
these issues, we carefully analyze a dataset collected from a real-world large-scale
storage system and then design an attention-augmented deep architecture for hard-drive
health status assessment and failure prediction. The deep architecture, composed of
a feature integration layer, a temporal dependency extraction layer, an attention
layer, and a classification layer, cannot only monitor the status of hard drives but
also assist in failure cause diagnoses. The experiments based on real-world datasets
show that the proposed deep architecture is able to assess the hard-drive status and
predict the impending failures accurately. In addition, the experimental results demonstrate
that the attention-augmented deep architecture can reveal the degradation progression
of hard drives automatically and assist administrators in tracing the cause of hard
drive failures.},
  address    = {New York, NY, USA},
  articleno  = {21},
  doi        = {10.1145/3340290},
  issue_date = {August 2019},
  keywords   = {SMART, recurrent neural network, attention mechanism, Hard drive failure, deep neural network},
  numpages   = {26},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3340290},
}

@InProceedings{Papalambrou2014,
  author    = {Papalambrou, Andreas and Stefanidis, Kyriakos and Gialelis, John and Serpanos, Dimitrios},
  booktitle = {Proceedings of the 9th Workshop on Embedded Systems Security},
  title     = {Detection, Traceback and Filtering of Denial of Service Attacks in Networked Embedded Systems},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {WESS '14},
  abstract  = {This work presents a composite scheme for detection, traceback and filtering of distributed
denial of service (DDoS) attacks in networked embedded systems. A method based on
algorithmic analysis of various node and network parameters is used to detect attacks
while a packet marking method is used to mitigate the effects of the attack by filtering
the incoming traffic that is part of this attack and trace back to the origin of the
attack. The combination of the detection and mitigation methods provide an increased
level of security in comparison to approaches based on a single method. Furthermore,
the scheme is developed in a way to comply with the novel SHIELD secure architecture
being developed, which aims at providing interoperability with other secure components
as well as metrics to quantify their security properties.},
  articleno = {5},
  doi       = {10.1145/2668322.2668327},
  isbn      = {9781450329323},
  keywords  = {embedded systems security, denial of service attacks},
  location  = {New Delhi, India},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2668322.2668327},
}

@Article{Hu2017,
  author     = {Hu, Han and Wen, Yonggang and Chua, Tat-Seng and Li, Xuelong},
  journal    = {ACM Trans. Intell. Syst. Technol.},
  title      = {Cost-Optimized Microblog Distribution over Geo-Distributed Data Centers: Insights from Cross-Media Analysis},
  year       = {2017},
  issn       = {2157-6904},
  month      = apr,
  number     = {3},
  volume     = {8},
  abstract   = {The unprecedent growth of microblog services poses significant challenges on network
traffic and service latency to the underlay infrastructure (i.e., geo-distributed
data centers). Furthermore, the dynamic evolution in microblog status generates a
huge workload on data consistence maintenance. In this article, motivated by insights
of cross-media analysis-based propagation patterns, we propose a novel cache strategy
for microblog service systems to reduce the inter-data center traffic and consistence
maintenance cost, while achieving low service latency. Specifically, we first present
a microblog classification method, which utilizes the external knowledge from correlated
domains, to categorize microblogs. Then we conduct a large-scale measurement on a
representative online social network system to study the category-based propagation
diversity on region and time scales. These insights illustrate social common habits
on creating and consuming microblogs and further motivate our architecture design.
Finally, we formulate the content cache problem as a constrained optimization problem.
By jointly using the Lyapunov optimization framework and simplex gradient method,
we find the optimal online control strategy. Extensive trace-driven experiments further
demonstrate that our algorithm reduces the system cost by 24.5% against traditional
approaches with the same service latency.},
  address    = {New York, NY, USA},
  articleno  = {40},
  doi        = {10.1145/3014431},
  issue_date = {April 2017},
  keywords   = {cross-media analysis, performance optimization, Social media analytics, data center},
  numpages   = {18},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3014431},
}

@InProceedings{Xin2021,
  author    = {Xin, An},
  booktitle = {Proceedings of the 2021 International Conference on Control and Intelligent Robotics},
  title     = {Research on Multi-Sensor Fusion Perception Method of Vehicle-Infrastructure Collaboration for Smart Automobiles},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {164–175},
  publisher = {Association for Computing Machinery},
  series    = {ICCIR 2021},
  abstract  = {Traffic congestion should be solved, it reduce traffic safety potential risks, and
improve people's travel efficiency. The paper based on intelligent car's own operation
characteristics (Smart car perception is generally only 300 ~ 500 meters, there is
a perceived problem of super-visual distance and vision blind zone), In order to effectively
improve the safety operation level of smart cars, and combined with intelligent non-smart
cars on the road may encounter cars a smart car trip perception highway outside the
scope of frequently asked questions, such as over-site sensations, the whole scene
and area's perception, the blind area, the emergency corner, tunnel, bridge, other
highways travel common scenes and so on. This paper is based on new infrastructure
transformations or newly build's research and practical results such as road traffic
intelligence infrastructure, it deployed the current road traffic to intelligently
infrastructure, especially the technical difficulties existing in the process of common
perceptual equipment (smart cameras, radar) are synonymous with multiple sensor information
fusion perceptions, it proposed a multi-sensor fusion algorithm based on error variance,
and designed a multi-object multi-sensor data processing system architecture. This
paper also proposes traffic operation scheduling architecture based on the game theory
of car road synergies on the basis of multi-sensor data fusion. Finally, these architectures
were analyzed using computer simulation techniques. The results show that the traffic
operation schedule for multi-sensor fusion algorithm based on error variance and game
theory based on the study proposed this study can be more obvious. The safety and
efficiency of the road traffic environment of smart vehicles. Optimization, smart
vehicles equipped with smart vehicles are also more than 25% higher than traditional
common vehicles in terms of vehicle safety. All in all, this study proposed to synergistic
multi-sensor convergence method for smart cars, that based on smart car a smart car
perception, compared to non-smart roads after intelligent infrastructure construction
and transformation of road traffic intelligent transportation systems, Higher efficiency
and more intelligent can better solve the common problems in road traffic environment,
providing people with safer, efficient and high-quality traffic travel services.},
  doi       = {10.1145/3473714.3473742},
  isbn      = {9781450390231},
  keywords  = {vehicle-infrastructure collaboration, perceptual method research, smart vehicle, multi-sensor fusion},
  location  = {Guangzhou, China},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3473714.3473742},
}

@InProceedings{Khan2015,
  author    = {Khan, Yasir Imtiaz and Al-shaer, Ehab and Rauf, Usman},
  booktitle = {Proceedings of the 2015 Workshop on Automated Decision Making for Active Cyber Defense},
  title     = {Cyber Resilience-by-Construction: Modeling, Measuring &amp; Verifying},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {9–14},
  publisher = {Association for Computing Machinery},
  series    = {SafeConfig '15},
  abstract  = {The need of cyber security is increasing as cyber attacks are escalating day by day.
Cyber attacks are now so many and sophisticated that many will unavoidably get through.
Therefore, there is an immense need to employ resilient architectures to defend known
or unknown threats. Engineer- ing resilient system/infrastructure is a challenging
task, that implies how to measure the resilience and how to obtain sufficient resilience
necessary to maintain its service delivery under diverse situations. This paper has
two fold objective, the first is to propose a formal approach to measure cyber resilience
from different aspects (i.e., attacks, failures) and at different levels (i.e., pro-active,
resistive and reactive). To achieve the first objective, we propose a formal frame-
work named as: Cyber Resilience Engineering Framework (CREF). The second objective
is to build a resilient system by construction. The idea is to build a formal model
of a cyber system, which is initially not resilient with respect to attacks. Then
by systematic refinements of the formal model and by its model checking, we attain
resiliency. We exemplify our technique through the case study of simple cyber security
device (i.e., network firewall).},
  doi       = {10.1145/2809826.2809836},
  isbn      = {9781450338219},
  keywords  = {model checking, algebraic petri nets, firewall, cyber resilience},
  location  = {Denver, Colorado, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2809826.2809836},
}

@InProceedings{Lautner2017,
  author    = {Lautner, Douglas and Hua, Xiayu and Debates, Scott and Song, Miao and Shah, Jagat and Ren, Shangping},
  booktitle = {Proceedings of the Symposium on Applied Computing},
  title     = {BaaS (Bluetooth-as-a-Sensor): Conception, Design and Implementation on Mobile Platforms},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {550–556},
  publisher = {Association for Computing Machinery},
  series    = {SAC '17},
  abstract  = {As network connectivity becomes more capable, mobile devices are evolving into sensory
data accumulators. Bluetooth (BT) components, which are widely used for communication
purposes, also have the potential to become contextual sensors by constantly listening
to information broadcast by nearby Bluetooth Low Energy (BLE) beacons. Compared to
traditional Micro-Electro-Mechanical (MEMs) based contextual sensors, Bluetooth-as-a-Sensor
(BaaS) provides a wider sensing spectrum and more comprehensive environmental information.
However, current implementations of BT are optimized as a data transmitter, therefore
deploying BaaS on a traditional mobile platform would cause an unacceptably high current
drain and hence a significant reduction in battery life. Our objective is to conquer
the current drain problem associated with having continuous wireless BT sensing. We
provide a novel BaaS-based architecture which utilizes an energy-efficient sensor
fusion core (SFC) to execute heavy-duty and long-standing tasks. We also present an
optimized duty cycle algorithm that minimizes the duty cycle while guaranteeing an
application's QoS requirements. Both BaaS architecture and algorithm are implemented
and deployed on a Moto X platform and then applied to an indoor location service for
consumer use validation. The performance of the BaaS-based architecture is evaluated
for both average current drain and location accuracy. Data measured on Moto X shows
that when using the BaaS architecture, the battery life is 5 times longer than using
the traditional BT architecture.},
  doi       = {10.1145/3019612.3019724},
  isbn      = {9781450344869},
  keywords  = {mobile sensing, mobile device, energy efficiency, embedded system, cellphone development},
  location  = {Marrakech, Morocco},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3019612.3019724},
}

@Article{Kim2015,
  author     = {Kim, Lok-Won and Lee, Dong-U and Villasenor, John},
  journal    = {ACM Trans. Des. Autom. Electron. Syst.},
  title      = {Automated Iterative Pipelining for ASIC Design},
  year       = {2015},
  issn       = {1084-4309},
  month      = mar,
  number     = {2},
  volume     = {20},
  abstract   = {We describe an automated pipelining approach for optimally balanced pipeline implementation
that achieves low area cost as well as meeting timing requirements. Most previous
automatic pipelining methods have focused on Instruction Set Architecture (ISA)-based
designs and the main goal of such methods generally has been maximizing performance
as measured in terms of instructions per clock (IPC). By contrast, we focus on datapath-oriented
designs (e.g., DSP filters for image or communication processing applications) in
ASIC design flows. The goal of the proposed pipelining approach is to find the optimally
pipelined design that not only meets the user-specified target clock frequency, but
also seeks to minimize area cost of a given design. Unlike most previous approaches,
the proposed methods incorporate the use of accurate area and timing information (iteratively
achieved by synthesizing every interim pipelined design) to achieve higher accuracy
during design exploration. When compared with exhaustive design exploration that considers
all possible pipeline patterns, the two heuristic pipelining methods presented here
involve only a small area penalty (typically under 5%) while offering dramatically
reduced computational complexity. Experimental validation is performed with commercial
ASIC design tools and described for applications including polynomial function evaluation,
FIR filters, matrix multiplication, and discrete wavelet transform filter designs
with a 90nm standard cell library.},
  address    = {New York, NY, USA},
  articleno  = {28},
  doi        = {10.1145/2660768},
  issue_date = {February 2015},
  keywords   = {pipelined hardware architecture, pipelining, ASIC designs, design area optimization, Timing error resolution},
  numpages   = {24},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2660768},
}

@InProceedings{Wazid2020,
  author    = {Wazid, Mohammad and Bera, Basudeb and Mitra, Ankush and Das, Ashok Kumar and Ali, Rashid},
  booktitle = {Proceedings of the 2nd ACM MobiCom Workshop on Drone Assisted Wireless Communications for 5G and Beyond},
  title     = {Private Blockchain-Envisioned Security Framework for AI-Enabled IoT-Based Drone-Aided Healthcare Services},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {37–42},
  publisher = {Association for Computing Machinery},
  series    = {DroneCom '20},
  abstract  = {Internet of Drones (IoD) architecture is designed to support a co-ordinated access
for the airspace using the unmanned aerial vehicles (UAVs) known as drones. Recently,
IoD communication environment is extremely useful for various applications in our
daily activities. Artificial intelligence (AI)-enabled Internet of Things (IoT)-based
drone-aided healthcare service is a specialized environment which can be used for
different types of tasks, for instance, blood and urine samples collections, medicine
delivery and for the delivery of other medical needs including the current pandemic
of COVID-19. Due to wireless nature of communication among the deployed drones and
their ground station server, several attacks (for example, replay, man-in-the-middle,
impersonation and privileged-insider attacks) can be easily mounted by malicious attackers.
To protect such attacks, the deployment of effective authentication, access control
and key management schemes are extremely important in the IoD environment. Furthermore,
combining the blockchain mechanism with deployed authentication make it more robust
against various types of attacks. To mitigate such issues, we propose a private-blockchain
based framework for secure communication in an IoT-enabled drone-aided healthcare
environment. The blockchain-based simulation of the proposed framework has been carried
out to measure its impact on various performance parameters.},
  doi       = {10.1145/3414045.3415941},
  isbn      = {9781450381055},
  keywords  = {authentication, security, internet of drones (IoD), blockchain, privacy, healthcare},
  location  = {London, United Kingdom},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3414045.3415941},
}

@InProceedings{Ruan2018,
  author    = {Ruan, Guangchen and Wernert, Eric and Gniady, Tassie and Tuna, Esen and Sherman, William},
  booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
  title     = {High Performance Photogrammetry for Academic Research},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {PEARC '18},
  abstract  = {Photogrammetry is the process of computationally extracting a three-dimensional surface
model from a set of two-dimensional photographs of an object or environment. It is
used to build models of everything from terrains to statues to ancient artifacts.
In the past, the computational process was done on powerful PCs and could take weeks
for large datasets. Even relatively small objects often required many hours of compute
time to stitch together. With the availability of parallel processing options in the
latest release of state-of-the-art photogrammetry software, it is possible to leverage
the power of high performance computing systems on large datasets. In this paper we
present a particular implementation of a high performance photogrammetry service.
Though the service is currently based on a specific software package (Agisoft's PhotoScan),
our system architecture is designed around a general photogrammetry process that can
be easily adapted to leverage other photogrammetry tools. In addition, we report on
an extensive performance study that measured the relative impacts of dataset size,
software quality settings, and processing cluster size. Furthermore, we share lessons
learned that are useful to system administrators looking to establish a similar service,
and we describe the user-facing support components that are crucial for the success
of the service.},
  articleno = {45},
  doi       = {10.1145/3219104.3219148},
  isbn      = {9781450364461},
  keywords  = {HPC, photogrammetry, benchmarking, distributed processing, scalability, performance evaluation},
  location  = {Pittsburgh, PA, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3219104.3219148},
}

@InProceedings{Colmant2017,
  author    = {Colmant, Maxime and Felber, Pascal and Rouvoy, Romain and Seinturier, Lionel},
  booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
  title     = {WattsKit: Software-Defined Power Monitoring of Distributed Systems},
  year      = {2017},
  pages     = {514–523},
  publisher = {IEEE Press},
  series    = {CCGrid '17},
  abstract  = {The design and the deployment of energy-efficient distributed systems is a challenging
task, which requires software engineers to consider all the layers of a system, from
hardware to software. In particular, monitoring and analyzing the power consumption
of a distributed system spanning several---potentially heterogeneous---nodes becomes
particularly tedious when aiming at a finer granularity than observing the power consumption
of hosting nodes. While the state-of-the-art in software-defined power meters fails
to deliver adaptive solutions to offer such service-level perspective and to cope
with the diversity of hardware CPU architectures, this paper proposes to automatically
learn the power models of the nodes supporting a distributed system, and then to use
these inferred power models to better understand how the power consumption of the
system's processes is distributed across nodes at runtime.Our solution, named WattsKit,
offers a modular toolkit to build software-defined power meters "\`{a} la carte", thus
dealing with the diversity of user and hardware requirements. Beyond the demonstrated
capability of covering a wide diversity of CPU architectures with high accuracy, we
illustrate the benefits of adopting software-defined power meters to analyze the power
consumption of complex layered and distributed systems. In particular, we illustrate
the capability of our approach to monitor the power consumption of a system composed
of Docker Swarm, Weave,Elasticsearch, and Apache ZooKeeper. Thanks to WattsKit, developers
and administrators are now able to identify potential power leaks in their software
infrastructure.},
  doi       = {10.1109/CCGRID.2017.27},
  isbn      = {9781509066100},
  location  = {Madrid, Spain},
  numpages  = {10},
  url       = {https://doi.org/10.1109/CCGRID.2017.27},
}

@InProceedings{Putnam2014,
  author    = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
  booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
  title     = {A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services},
  year      = {2014},
  pages     = {13–24},
  publisher = {IEEE Press},
  series    = {ISCA '14},
  abstract  = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency,
and low cost. It is challenging to improve all of these factors simultaneously. To
advance datacenter capabilities beyond what commodity server designs can provide,
we have designed and built a composable, reconfigurablefabric to accelerate portions
of large-scale software services. Each instantiation of the fabric consists of a 6x8
2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One
FPGA is placed into each server, accessible through PCIe, and wired directly to other
FPGAs with pairs of 10 Gb SAS cablesIn this paper, we describe a medium-scale deployment
of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating
the Bing web search engine. We describe the requirements and architecture of the system,
detail the critical engineering challenges and solutions needed to make the system
robust in the presence of failures, and measure the performance, power, and resilience
of the system when ranking candidate documents. Under high load, the largescale reconfigurable
fabric improves the ranking throughput of each server by a factor of 95% for a fixed
latency distribution--- or, while maintaining equivalent throughput, reduces the tail
latency by 29%},
  isbn      = {9781479943944},
  location  = {Minneapolis, Minnesota, USA},
  numpages  = {12},
}

@Article{Putnam2014a,
  author     = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
  journal    = {SIGARCH Comput. Archit. News},
  title      = {A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services},
  year       = {2014},
  issn       = {0163-5964},
  month      = jun,
  number     = {3},
  pages      = {13–24},
  volume     = {42},
  abstract   = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency,
and low cost. It is challenging to improve all of these factors simultaneously. To
advance datacenter capabilities beyond what commodity server designs can provide,
we have designed and built a composable, reconfigurablefabric to accelerate portions
of large-scale software services. Each instantiation of the fabric consists of a 6x8
2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One
FPGA is placed into each server, accessible through PCIe, and wired directly to other
FPGAs with pairs of 10 Gb SAS cablesIn this paper, we describe a medium-scale deployment
of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating
the Bing web search engine. We describe the requirements and architecture of the system,
detail the critical engineering challenges and solutions needed to make the system
robust in the presence of failures, and measure the performance, power, and resilience
of the system when ranking candidate documents. Under high load, the largescale reconfigurable
fabric improves the ranking throughput of each server by a factor of 95% for a fixed
latency distribution--- or, while maintaining equivalent throughput, reduces the tail
latency by 29%},
  address    = {New York, NY, USA},
  doi        = {10.1145/2678373.2665678},
  issue_date = {June 2014},
  numpages   = {12},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2678373.2665678},
}

@InProceedings{Tiwary2020,
  author    = {Tiwary, Mayank and Mishra, Pritish and Jain, Shashank and Puthal, Deepak},
  booktitle = {Companion Proceedings of the Web Conference 2020},
  title     = {Data Aware Web-Assembly Function Placement},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {4–5},
  publisher = {Association for Computing Machinery},
  series    = {WWW '20},
  abstract  = {Existing container based serverless computing systems are limited by cold-start problems
and complex architecture for stateful services, multi-tenancy, etc. This paper presents
serverless functions to be placed as per data locality and executed as a web-assembly
sandbox, which results better execution latency and reduced network usage as compared
to the existing architectures. The designed serverless runtime features resource isolation
in terms of CPU, Memory, and file-system isolation and falicitates multi-tenancy executions.
The proposed architecture is evaluated using IoT workloads with different performance
metrics.},
  doi       = {10.1145/3366424.3382670},
  isbn      = {9781450370240},
  keywords  = {Multi-Tenancy, Servelress, Data Locality, Web-Assembly},
  location  = {Taipei, Taiwan},
  numpages  = {2},
  url       = {https://doi.org/10.1145/3366424.3382670},
}

@InProceedings{Scheuner2019,
  author    = {Scheuner, Joel and Leitner, Philipp},
  booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
  title     = {Performance Benchmarking of Infrastructure-as-a-Service (IaaS) Clouds with Cloud WorkBench},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {53–56},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '19},
  abstract  = {The continuing growth of the cloud computing market has led to an unprecedented diversity
of cloud services with different performance characteristics. To support service selection,
researchers and practitioners conduct cloud performance benchmarking by measuring
and objectively comparing the performance of different providers and configurations
(e.g., instance types in different data center regions). In this tutorial, we demonstrate
how to write performance tests for IaaS clouds using the Web-based benchmarking tool
Cloud WorkBench (CWB). We will motivate and introduce benchmarking of IaaS cloud in
general, demonstrate the execution of a simple benchmark in a public cloud environment,
summarize the CWB tool architecture, and interactively develop and deploy a more advanced
benchmark together with the participants.},
  doi       = {10.1145/3302541.3310294},
  isbn      = {9781450362863},
  keywords  = {performance, benchmarking, cloud computing},
  location  = {Mumbai, India},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3302541.3310294},
}

@InProceedings{Caulfield2016,
  author    = {Caulfield, Adrian M. and Chung, Eric S. and Putnam, Andrew and Angepat, Hari and Fowers, Jeremy and Haselman, Michael and Heil, Stephen and Humphrey, Matt and Kaur, Puneet and Kim, Joo-Young and Lo, Daniel and Massengill, Todd and Ovtcharov, Kalin and Papamichael, Michael and Woods, Lisa and Lanka, Sitaram and Chiou, Derek and Burger, Doug},
  booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
  title     = {A Cloud-Scale Acceleration Architecture},
  year      = {2016},
  publisher = {IEEE Press},
  series    = {MICRO-49},
  abstract  = {Hyperscale datacenter providers have struggled to balance the growing need for specialized
hardware (efficiency) with the economic benefits of homogeneity (manageability). In
this paper we propose a new cloud architecture that uses reconfigurable logic to accelerate
both network plane functions and applications. This Configurable Cloud architecture
places a layer of reconfigurable logic (FPGAs) between the network switches and the
servers, enabling network flows to be programmably transformed at line rate, enabling
acceleration of local applications running on the server, and enabling the FPGAs to
communicate directly, at datacenter scale, to harvest remote FPGAs unused by their
local servers. We deployed this design over a production server bed, and show how
it can be used for both service acceleration (Web search ranking) and network acceleration
(encryption of data in transit at high-speeds). This architecture is much more scalable
than prior work which used secondary rack-scale networks for inter-FPGA communication.
By coupling to the network plane, direct FPGA-to-FPGA messages can be achieved at
comparable latency to previous work, without the secondary network. Additionally,
the scale of direct inter-FPGA messaging is much larger. The average round-trip latencies
observed in our measurements among 24, 1000, and 250,000 machines are under 3, 9,
and 20 microseconds, respectively. The Configurable Cloud architecture has been deployed
at hyperscale in Microsoft's production datacenters worldwide.},
  articleno = {7},
  location  = {Taipei, Taiwan},
  numpages  = {13},
}

@Article{Chen2017,
  author     = {Chen, Min and Chen, Shigang and Cai, Zhiping},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {Counter Tree: A Scalable Counter Architecture for Per-Flow Traffic Measurement},
  year       = {2017},
  issn       = {1063-6692},
  month      = apr,
  number     = {2},
  pages      = {1249–1262},
  volume     = {25},
  abstract   = {Per-flow traffic measurement, which is to count the number of packets for each active
flow during a certain measurement period, has many applications in traffic engineering,
classification of routing distribution or network usage pattern, service provision,
anomaly detection, and network forensics. In order to keep up with the high throughput
of modern routers or switches, the online module for per-flow traffic measurement
should use high-bandwidth SRAM that allows fast memory accesses. Due to limited SRAM
space, exact counting, which requires to keep a counter for each flow, does not scale
to large networks consisting of numerous flows. Some recent work takes a different
approach to estimate the flow sizes using counter architectures that can fit into
tight SRAM. However, existing counter architectures have limitations, either still
requiring considerable SRAM space or having a small estimation range. In this paper,
we design a scalable counter architecture called Counter Tree, which leverages a 2-D
counter sharing scheme to achieve far better memory efficiency and in the meantime
extend estimation range significantly. Furthermore, we improve the performance of
Counter Tree by adding a status bit to each counter. Extensive experiments with real
network traces demonstrate that our counter architecture can produce accurate estimates
for flows of all sizes under very tight memory space.},
  doi        = {10.1109/TNET.2016.2621159},
  issue_date = {April 2017},
  numpages   = {14},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2016.2621159},
}

@InProceedings{Kim2015a,
  author    = {Kim, Yeseong and Imani, Mohsen and Patil, Shruti and Rosing, Tajana S.},
  booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
  title     = {CAUSE: Critical Application Usage-Aware Memory System Using Non-Volatile Memory for Mobile Devices},
  year      = {2015},
  pages     = {690–696},
  publisher = {IEEE Press},
  series    = {ICCAD '15},
  abstract  = {Mobile devices are severely limited in memory, which affects critical user-experience
metrics such as application service time. Emerging non-volatile memory (NVM) technologies
such as STT-RAM and PCM are ideal candidates to provide higher memory capacity with
negligible energy overhead. However, existing memory management systems overlook mobile
users application usage which provides crucial cues for improving user experience.
In this paper, we propose CAUSE, a novel memory system based on DRAM-NVM hybrid memory
architecture. CAUSE takes explicit account of the application usage patterns to distinguish
data criticality and identify suitable swap candidates. We also devise NVM hardware
design optimized for the access characteristics of the swapped pages. We evaluate
CAUSE on a real Android smartphone and NVSim simulator using user application usage
logs. Our experimental results show that the proposed technique achieves 32% faster
launch time for mobile applications while reducing energy cost by 90% and 44% on average
over non-optimized STT-RAM and PCM, respectively.},
  isbn      = {9781467383899},
  location  = {Austin, TX, USA},
  numpages  = {7},
}

@InProceedings{Sharakhov2014,
  author    = {Sharakhov, Nikita and Marojevic, Vuk and Romano, Ferdinando and Polys, Nicholas and Dietrich, Carl},
  booktitle = {Proceedings of the 19th International ACM Conference on 3D Web Technologies},
  title     = {Visualizing Real-Time Radio Spectrum Access with CORNET3D},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {109–116},
  publisher = {Association for Computing Machinery},
  series    = {Web3D '14},
  abstract  = {Modern web technology enables the 3D portrayal of real-time data. WebSocket connections
provide data over the web without the time-consuming overhead of HTTP requests. The
server-side "push" paradigm is particularly useful for creating novel tools such as
CORNET3D, where real-time 3D visualization is required. CORNET3D is an innovative
Web3D interface to a research and education test bed for Dynamic Spectrum Access (DSA).
Our system can drive several 2D and 3D portrayals of spectral data and radio performance
metrics from a live, online system. The testbed can further integrate the data portrayals
into a multi-user "serious game" to teach students about strategies for the optimal
use of spectrum resources by providing them with real-time scoring based on their
choices of radio transmission parameters. This paper describes the web service architecture
and Webd3D front end for our DSA testbed, detailing new methods for spectrum visualization
and the applications they enable.},
  doi       = {10.1145/2628588.2628598},
  isbn      = {9781450330152},
  keywords  = {HTML5, web applications, computer graphics, WebSockets, WebGL},
  location  = {Vancouver, British Columbia, Canada},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2628588.2628598},
}

@InProceedings{Talreja2019,
  author    = {Talreja, Disha and Lahiri, Kanishka and Kalambur, Subramaniam and Raghavendra, Prakash},
  booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
  title     = {Performance Scaling of Cassandra on High-Thread Count Servers},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {179–187},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '19},
  abstract  = {NoSQL databases are commonly used today in cloud deployments due to their ability
to "scale-out" and effectively use distributed computing resources in a data center.
At the same time, cloud servers are also witnessing rapid growth in CPU core counts,
memory bandwidth, and memory capacity. Hence, apart from scaling out effectively,
it's important to consider how such workloads "scale-up" within a single system, so
that they can make the best use of available resources. In this paper, we describe
our experiences studying the performance scaling characteristics of Cassandra, a popular
open-source, column-oriented database, on a single high-thread count dual socket server.
We demonstrate that using commonly used benchmarking practices, Cassandra does not
scale well on such systems. Next, we show how by taking into account specific knowledge
of the underlying topology of the server architecture, we can achieve substantial
improvements in performance scalability. We report on how, during the course of our
work, we uncovered an area for performance improvement in the official open-source
implementation of the Java platform with respect to NUMA awareness. We show how optimizing
this resulted in 27% throughput gain for Cassandra under studied configurations. As
a result of these optimizations, using standard workload generators, we obtained up
to 1.44x and 2.55x improvements in Cassandra throughput over baseline single and dual-socket
performance measurements respectively. On wider testing across a variety of workloads,
we achieved excellent performance scaling, averaging 98% efficiency within a socket
and 90% efficiency at the system-level.},
  doi       = {10.1145/3297663.3309668},
  isbn      = {9781450362399},
  keywords  = {nosql databases, performance scalability, cassandra, performance benchmarking},
  location  = {Mumbai, India},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3297663.3309668},
}

@InProceedings{Klaver2021,
  author    = {Klaver, Luuk and van der Knaap, Thijs and van der Geest, Johan and Harmsma, Edwin and van der Waaij, Bram and Pileggi, Paolo},
  booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
  title     = {Towards Independent Run-Time Cloud Monitoring},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {21–26},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '21},
  abstract  = {Cloud computing services are integral to the digital transformation. They deliver
greater connectivity, tremendous savings, and lower total cost of ownership. Despite
such benefits and benchmarking advances, costs are still quite unpredictable, performance
is unclear, security is inconsistent, and there is minimal control over aspects like
data and service locality. Estimating performance of cloud environments is very hard
for cloud consumers. They would like to make informed decisions about which provider
better suits their needs using specialized evaluation mechanisms. Providers have their
own tools reporting specific metrics, but they are potentially biased and often incomparable
across providers. Current benchmarking tools allow comparison but consumers need more
flexibility to evaluate environments under actual operating conditions for specialized
applications. Ours is early stage work and a step towards a monitoring solution that
enables independent evaluation of clouds for very specific application needs. In this
paper, we present our initial architecture of the Cloud Monitor that aims to integrate
existing and new benchmarks in a flexible and extensible way. By way of a simplistic
demonstrator, we illustrate the concept. We report some preliminary monitoring results
after a brief time of monitoring and are able to observe unexpected anomalies. The
results suggest an independent monitoring solution is a powerful enabler of next generation
cloud computing, not only for the consumer but potentially the whole ecosystem.},
  doi       = {10.1145/3447545.3451180},
  isbn      = {9781450383318},
  keywords  = {performance evaluation, run-time monitoring, benchmarking, cloud computing},
  location  = {Virtual Event, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3447545.3451180},
}

@InProceedings{You2015,
  author    = {You, Taewan and Martinez-Julia, Pedro and Skarmeta, Antonio and Jung, Heeyoung},
  booktitle = {The 10th International Conference on Future Internet},
  title     = {Design and Deployment of Federation Testbed in EU-KR for Identifier-Based Communications},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {13–16},
  publisher = {Association for Computing Machinery},
  series    = {CFI '15},
  abstract  = {SmartFIRE is the first intercontinental testbed, federating multiple small-scale testbeds
in South Korea and Europe, which exploits the benefits and building blocks of an OpenFlow-based
infrastructure. As a part of SmartFIRE, both ETRI and UMU designs and develops a federation
testbed for Identifier-based communications that all of communication services are
achieved by Identifier not by IP address. In order to manage and control the testbed,
we deploy a Measurement and Management Framework (OMF), further we will deploy SFA
aggregate manager to federate with other SmartFIRE testbed. In this paper we introduce
the federation testbed for ID-based communications including network connectivity,
architecture configuration, and federation architecture. Moreover, to exploit the
testbed, we design and implement two mobility use cases that we show seamless network
connection service under host's mobility, such as intra-domain handover and inter-domain
handover. Thus we can show result of the experimentation that the communication session
wouldn't be cut off even though communication entity moves to a different network.
Finally we refer future works for federation to cooperate with other SmartFIRE testbeds
and additional ID-based communication scenario.},
  doi       = {10.1145/2775088.2775100},
  isbn      = {9781450335645},
  keywords  = {Deployment, Federation Testbed, EU, Identifier-based communications, SmartFire, KR},
  location  = {Seoul, Republic of Korea},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2775088.2775100},
}

@InProceedings{Szabo2017,
  author    = {Szabo, Marton and Majdan, Andras and Pongracz, Gergely and Toka, Laszlo and Sonkoly, Balazs},
  booktitle = {Proceedings of the SIGCOMM Posters and Demos},
  title     = {Making the Data Plane Ready for NFV: An Effective Way of Handling Resources},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {97–99},
  publisher = {Association for Computing Machinery},
  series    = {SIGCOMM Posters and Demos '17},
  abstract  = {In order to enable carrier grade network services constructed from software-based
network functions, we need a novel data plane supporting high performance packet processing,
low latency and flexible, fine granular programmability and control. The network functions
implemented as virtual machines or containers use the same hardware resources (cpu,
memory) as the elements responsible for networking, therefore, a low-level resource
orchestrator which is capable of jointly controlling these resources is an indispensable
component. In this demonstration, we showcase our novel resource orchestrator (FERO)
on top of a data plane making use of open-source components such as, Docker, DPDK
and OVS. It is capable of i) generating an abstract model of the underlying hardware
architecture during the bootstrap process, ii) mapping the incoming network service
requests to available resources based on our recently proposed Service Graph embedding
engine and the generated graph model. The impact of the orchestration decision is
shown on-the-fly by real-time performance measurements on a graphical dashboard.},
  doi       = {10.1145/3123878.3131999},
  isbn      = {9781450350570},
  keywords  = {DPDK, SFC, SDN, NFV, Docker},
  location  = {Los Angeles, CA, USA},
  numpages  = {3},
  url       = {https://doi.org/10.1145/3123878.3131999},
}

@InProceedings{Burdusel2019,
  author    = {Burdusel, Alexandru and Zschaler, Steffen},
  booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
  title     = {Towards Scalable Search-Based Model Engineering with MDEOptimiser Scale},
  year      = {2019},
  pages     = {189–195},
  publisher = {IEEE Press},
  series    = {MODELS '19},
  abstract  = {Running scientific experiments using search-based model engineering (SBME) tools is
a complex task, that poses a number of challenges, starting from defining an experiment
workflow, to parameter tuning, finding optimal computational resources to run on,
collecting and interpreting metrics and making the entire process easily reproducible.Despite
the proliferation of easily accessible hardware, as a result of the increased availability
of infrastructure-as-a-service providers, many SBME tools are rarely using this technology
for accelerating experimentation. Running many experiments on a single machine implies
much longer waiting times and reduces the ability to increase the speed of iterations
when doing SBME research, thus, slowing down the entire process.In this paper, we
introduce a domain-specific language (DSL) and a framework that can be used to configure
and run experiments at scale, on cloud infrastructure, in a reproducible way. We will
describe our DSL and framework architecture along with an example to showcase how
a case study can be evaluated using two different model optimisation tools.},
  doi       = {10.1109/MODELS-C.2019.00032},
  isbn      = {9781728151250},
  keywords  = {evolutionary search, workflow, reproducible research, middleware, search based model engineering, model driven engineering, cloud},
  location  = {Munich, Germany},
  numpages  = {7},
  url       = {https://doi.org/10.1109/MODELS-C.2019.00032},
}

@InProceedings{Li2020,
  author    = {Li, Hailing and Zhang, Xiaohang and Cao, Shoufeng and He, Longtao and Zhang, Hui},
  booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
  title     = {Active Measurement of Open Resolver Service Nodes},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {CSAE 2020},
  abstract  = {Driven by the growing number of DNS requests on the Internet, the architecture of
the recursive resolver has become more huge and complex, especially for open resolvers
that provide resolution services to the public. There are many service nodes with
different roles in the open resolver, and the nodes that directly communicate with
the authoritative server are called recursive egress nodes. This paper proposed a
distributed measurement system and performed active measurement and analysis on the
characteristics of the egress node of open resolvers collected from passive DNS traffic
and third party active scanning. The results from 65 vantage points show that (1)
most open resolvers have dozens of recursive egress nodes, and (2) most open resolvers
have deployed at least one IPv6 address egress node, while IPv4 address still dominates
the service node configuration. (3) A small amount of recursive egress nodes is reused
by a large number of open resolvers, so that a large amount of DNS request traffic
on the Internet is concentrated on limited recursive egress nodes, which will reduce
the redundancy of DNS and cause cyber security risks. (4) The median distances between
most open resolvers with multiple egress nodes and the users usually exceed 1000 kilometers,
which will bring negative effect on the scheduling accuracy of CDN.},
  articleno = {61},
  doi       = {10.1145/3424978.3425039},
  isbn      = {9781450377720},
  keywords  = {Open resolver, Recursive egress node, DNS redirection, Distributed active measurement},
  location  = {Sanya, China},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3424978.3425039},
}

@Proceedings{2015,
  title     = {QoSA '15: Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
  year      = {2015},
  address   = {New York, NY, USA},
  isbn      = {9781450334709},
  publisher = {Association for Computing Machinery},
  abstract  = {Welcome to the 11th International ACM Sigsoft Conference on the Quality of Software
Architectures -- QoSA 2015. For more than a decade, QoSA has strived to advance the
state of the art of quality aspects of software architecture, focusing broadly on
its quality characteristics and how these relate to the design of software architectures.
Specific issues of interest are defining and modeling quality measures, evaluating
and managing architecture quality, linking architecture to requirements and implementation,
and preserving architecture quality throughout the system lifetime. Past themes for
QoSA include Architecting for Adaptivity (2014), The System View (2013), Evolving
Architectures (2012), Quality throughout the Software Lifecycle (2011), and Research
into Practice -- Reality and Gaps (2010).QoSA 2015's theme is "Software Architecture
for the 4th Industrial Revolution". After mechanization, mass production, and electronics,
the Internet is about to enable a new level of productivity in manufacturing. This
shall be enabled by smart cyber-physical systems connected to cloud computing services
and communicating using standardized semantics. In the near future, industrial big
data analytics on monitored sensor data shall improve the efficiency and individualization
of production facilities. This year's QoSA conference solicited contributions that
explore the various implications of this upcoming industrial revolution on software
architecture. This included reference architectures, software architectures adapting
at run time, architecture styles and patterns for cyber-physical and distributed systems.The
call for papers attracted 42 initial submissions from Asia, North America, Africa,
and Europe and 28 final submissions were considered during the review process. The
program committee accepted 11 full papers and 2 short papers that cover topics, such
as new architecture modeling approaches, architectural tactics for mobile computing,
cloud computing architectures, and cyberphysical systems. QoSA's 2015 proceedings
also include 2 papers from the WCOP 2015, the 20th International Doctoral Symposium
on Components and Architecture.QoSA 2015 is part of the federated events on component-based
software engineering and software architecture (CompArch 2015), which include WICSA
2015 (12th Working IEEE / IFIP Conference on Software Architecture) and CBSE 2015
(18th International ACM SIGSOFT Symposium on Component-Based Software Engineering).},
  location  = {Montr\'{e}al, QC, Canada},
}

@InProceedings{Hansen2020,
  author    = {Hansen, Casper and Hansen, Christian and Maystre, Lucas and Mehrotra, Rishabh and Brost, Brian and Tomasi, Federico and Lalmas, Mounia},
  booktitle = {Fourteenth ACM Conference on Recommender Systems},
  title     = {Contextual and Sequential User Embeddings for Large-Scale Music Recommendation},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {53–62},
  publisher = {Association for Computing Machinery},
  series    = {RecSys '20},
  abstract  = {Recommender systems play an important role in providing an engaging experience on
online music streaming services. However, the musical domain presents distinctive
challenges to recommender systems: tracks are short, listened to multiple times, typically
consumed in sessions with other tracks, and relevance is highly context-dependent.
In this paper, we argue that modeling users’ preferences at the beginning of a session
is a practical and effective way to address these challenges. Using a dataset from
Spotify, a popular music streaming service, we observe that a) consumption from the
recent past and b) session-level contextual variables (such as the time of the day
or the type of device used) are indeed predictive of the tracks a user will stream—much
more so than static, average preferences. Driven by these findings, we propose CoSeRNN,
a neural network architecture that models users’ preferences as a sequence of embeddings,
one for each session. CoSeRNN predicts, at the beginning of a session, a preference
vector, based on past consumption history and current context. This preference vector
can then be used in downstream tasks to generate contextually relevant just-in-time
recommendations efficiently, by using approximate nearest-neighbour search algorithms.
We evaluate CoSeRNN on session and track ranking tasks, and find that it outperforms
the current state of the art by upwards of 10% on different ranking metrics. Dissecting
the performance of our approach, we find that sequential and contextual information
are both crucial.},
  doi       = {10.1145/3383313.3412248},
  isbn      = {9781450375832},
  keywords  = {Context, Sequence, User Embeddings, Music Recommendation},
  location  = {Virtual Event, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3383313.3412248},
}

@InProceedings{Klein2015,
  author    = {Klein, John and Gorton, Ian},
  booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
  title     = {Runtime Performance Challenges in Big Data Systems},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {17–22},
  publisher = {Association for Computing Machinery},
  series    = {WOSP '15},
  abstract  = {Big data systems are becoming pervasive. They are distributed systems that include
redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient
to assure runtime performance in production. This is due to the scale of the deployed
system, the continually evolving workloads, and the unpredictable quality of service
of the shared infrastructure. Consequently, a solution for addressing performance
requirements needs sophisticated runtime observability and measurement. Observability
gives real-time insights into a system's health and status, both at the system and
application level, and provides historical data repositories for forensic analysis,
capacity planning, and predictive analytics. Due to the scale and heterogeneity of
big data systems, significant challenges exist in the design, customization and operations
of observability capabilities. These challenges include economical creation and insertion
of monitors into hundreds or thousands of computation and data nodes, efficient, low
overhead collection and storage of measurements (which is itself a big data problem),
and application-aware aggregation and visualization. In this paper we propose a reference
architecture to address these challenges, which uses a model-driven engineering toolkit
to generate architecture-aware monitors and application-specific visualizations.},
  doi       = {10.1145/2693561.2693563},
  isbn      = {9781450333405},
  keywords  = {observability, big data, model-driven engineering},
  location  = {Austin, Texas, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2693561.2693563},
}

@InProceedings{Faye2019,
  author    = {Faye, S\'{e}bastien and Melakessou, Foued and Mtalaa, Wassila and Gautier, Prune and AlNaffakh, Neamah and Khadraoui, Djamel},
  booktitle = {Proceedings of the 1st ACM International Workshop on Technology Enablers and Innovative Applications for Smart Cities and Communities},
  title     = {SWAM: A Novel Smart Waste Management Approach for Businesses Using IoT},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {38–45},
  publisher = {Association for Computing Machinery},
  series    = {TESCA'19},
  abstract  = {The waste recycling industry has grown considerably in the recent years and many solutions
have become democratized around smart waste collection. However, existing decision
support systems generally rely on a limited flow of information and offer an often
static or statistically based approach, focusing on specific use-cases (e.g., individuals,
municipalities). This paper introduces SWAM, a new smart waste collection platform
currently being elaborated in Luxembourg that targets businesses and large entities
(e.g., restaurants, shopping centers). SWAM aims to consider multiple sources of combined
information in its decision-making process and go further in the routing optimization
process. The platform notably uses ultrasonic sensors to measure the filling level
of containers, and smartphones with embedded intelligence to understand a driver's
actions. This paper presents our experience on the development and deployment of this
platform in Luxembourg, as well as the relevant options on the choice of sensing and
communication technologies available in the market. It also presents the system architecture
and two fundamental components. Firstly, a data management layer, which implements
models for analyzing and predicting the filling patterns of the containers. Secondly,
a multi-objective optimization layer, which compiles the collection routes that minimize
the impact on the environment and maximize the service quality. This paper is intended
to serve as a practical reference for the deployment of waste management systems,
which have many technological components and can greatly fluctuate depending on the
use cases to be covered.},
  doi       = {10.1145/3364544.3364824},
  isbn      = {9781450370158},
  keywords  = {Smart Waste Collection, Multi-Objective Optimization, IoT, WSN},
  location  = {New York, NY, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3364544.3364824},
}

@InProceedings{Voeroes2020,
  author    = {V\"{o}r\"{o}s, P\'{e}ter and Pongr\'{a}cz, Gergely and Laki, S\'{a}ndor},
  booktitle = {Proceedings of the 3rd P4 Workshop in Europe},
  title     = {Towards a Hybrid Next Generation NodeB},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {56–58},
  publisher = {Association for Computing Machinery},
  series    = {EuroP4'20},
  abstract  = {5G Radio Access Networks consists of two key services: User Plane Function (UPF) and
next generation NodeB (gNB). Though several papers have recently demonstrated that
the high-level UPF can be described in P4, for the lowest-level gNB service it is
more challenging and cannot purely be done with existing programmable switches. In
this paper, we show that gNB requires functionalities such as Automatic Repeat Request
(ARQ) and ciphering/deciphering that are not supported by the high speed P4-programmable
switches available in the market. To overcome these limitations, we propose a hybrid
approach where the majority of packet processing is done by a high speed P4-programmable
switch while the additional functionalities are solved by external services implemented
in DPDK. The coordination of packets among the services is also handled by the P4-switch.
Our preliminary results include the identification of functionalities required by
a gNB node for delivering user data, the design of a hybrid architecture, and the
performance evaluation of the buffering and re-transmission service. Finally, our
measurements demonstrate that the proposed hybrid approach is scalable and could be
an alternative to existing gNB solutions in the future.},
  doi       = {10.1145/3426744.3431328},
  isbn      = {9781450381819},
  location  = {Barcelona, Spain},
  numpages  = {3},
  url       = {https://doi.org/10.1145/3426744.3431328},
}

@InProceedings{Chatterjee2016,
  author    = {Chatterjee, Abhranil and Anjaria, Janit and Roy, Sourav and Ganguli, Arnab and Seal, Krishanu},
  booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  title     = {SAGEL: Smart Address Geocoding Engine for Supply-Chain Logistics},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SIGSPACIAL '16},
  abstract  = {With the recent explosion of e-commerce industry in India, the problem of address
geocoding, that is, transforming textual address descriptions to geographic reference,
such as latitude, longitude coordinates, has emerged as a core problem for supply
chain management. Some of the major areas that rely on precise and accurate address
geocoding are supply chain fulfilment, supply chain analytics and logistics. In this
paper, we present some of the challenges faced in practice while building an address
geocoding engine as a core capability at Flipkart. We discuss the unique challenges
of building a geocoding engine for a rapidly developing country like India, such as,
fuzzy region boundaries, dynamic topography and lack of convention in spellings of
toponyms, to name a few. We motivate the need for building a reliable and precise
address geocoding system from a business perspective and argue why some of the commercially
available solutions do not suffice for our requirements. SAGEL has evolved through
3 cycles of solution prototypes and pilot experiments. We describe the learnings from
each of these phases and how we incorporated them to get to the first production-ready
version. We describe how we store and index map data on a SolrCloud cluster of Apache
Solr, an open-source search platform, and the core algorithm for geocoding which works
post-retrieval in order to determine the best matches among a set of candidate results.
We give a brief description of the system architecture and provide accuracy results
of our geocoding engine by measuring deviations of geocoded customer addresses across
India, from verified latitude, longitude coordinates of those addresses, for a sizeable
address set. We also measure and report our system's ability to geocode up to different
region levels, like city, locality or building. We compare our results with those
of the geocoding service provided by Google against a set of addresses for which we
have verified latitude-longitude coordinates and show that our geocoding engine is
almost as accurate as Google's, while having a higher coverage.},
  articleno = {42},
  doi       = {10.1145/2996913.2996917},
  isbn      = {9781450345897},
  keywords  = {spatio-textual searching, storage and indexing, spatial data mining and knowledge discovery, geographic information retrieval},
  location  = {Burlingame, California},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2996913.2996917},
}

@InProceedings{Sukhoroslov2016,
  author    = {Sukhoroslov, Oleg and Volkov, Sergey and Afanasiev, Alexander},
  booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
  title     = {Program Autotuning as a Service: Opportunities and Challenges},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {148–155},
  publisher = {Association for Computing Machinery},
  series    = {UCC '16},
  abstract  = {Program autotuning is becoming an increasingly valuable tool for improving performance
portability across diverse target architectures, exploring trade-offs between several
criteria, or meeting quality of service requirements. Recent work on general autotuning
frameworks enabled rapid development of domain-specific autotuners reusing common
libraries of parameter types and search techniques. In this work we explore the use
of such frameworks to develop general-purpose online services for program autotuning
using the Software as a Service model. Beyond the common benefits of this model, the
proposed approach opens up a number of unique opportunities, such as collecting performance
data and utilizing it to improve further runs, or enabling remote online autotuning.
However, the proposed autotuning as a service approach also brings in several challenges,
such as accessing target systems, dealing with measurement latency, and supporting
execution of user-provided code. This paper presents the first step towards implementing
the proposed approach and addressing these challenges. We describe an implementation
of generic autotuning service that can be used for tuning arbitrary programs on user-provided
computing systems. The service is based on OpenTuner autotuning framework and runs
on Everest platform that enables rapid development of computational web services.
In contrast to OpenTuner, the service doesn't require installation of the framework,
allows users to avoid writing code and supports efficient parallel execution of measurement
tasks across multiple machines. The performance of the service is evaluated by using
it for tuning synthetic and real programs.},
  doi       = {10.1145/2996890.2996903},
  isbn      = {9781450346160},
  keywords  = {distributed computing, program autotuning, software as a service, web services},
  location  = {Shanghai, China},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2996890.2996903},
}

@InProceedings{Ilyas2015,
  author    = {Ilyas, Bambrik and Fedoua, Didi},
  booktitle = {Proceedings of the International Conference on Intelligent Information Processing, Security and Advanced Communication},
  title     = {A Load Management Algorithm For Wireless Mesh Networks},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {IPAC '15},
  abstract  = {The WMN (Wireless Mesh Network) is a new emerging technology that can render the field
of industrial network more efficient and profitable. Due to its versatility that allows
a flexible configuration, this kind of network is commonly considered as a very suitable
architecture for mobile clients. The difference between the WMNs and other dynamic
networks, such as the MANET (Mobile Ad-hoc Network), is that the Mesh network contains
static wireless nodes called MR (Mesh Routers). Consequently, the presence of this
infrastructure makes the WMN more suitable to provide QoS (Quality of Service). However,
the guarantee of QoS in a dynamic topology is a difficult task by comparison with
static networks. These difficulties are caused by the random movement of the clients,
the shared nature of the wireless channel, the complexity of multi-hop communications
and most importantly the management of the traffic load forwarded through the MRs.
In this paper, we propose a new algorithm for load balancing in WMN that can search
for alternative paths in order to deviate from the loaded MRs. The proposed algorithm
can operate with different metrics at the same time and applies the Genetic Algorithm
in case there is a large population of possible solutions.},
  articleno = {46},
  doi       = {10.1145/2816839.2816875},
  isbn      = {9781450334587},
  keywords  = {traffic load, WMN, mobile clients, Mesh Routers, Genetic Algorithm, QoS},
  location  = {Batna, Algeria},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2816839.2816875},
}

@InProceedings{Panteli2019,
  author    = {Panteli, Maria},
  booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
  title     = {Recommendation Systems Compliant with Legal and Editorial Policies: The BBC+ App Journey},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {529},
  publisher = {Association for Computing Machinery},
  series    = {RecSys '19},
  abstract  = {The BBC produces thousands of pieces of content every day and numerous BBC products
deliver this content to millions of users. For many years the content has been manually
curated (this is evident in the selection of stories on the front page of the BBC
News website and app for example). To support content creation and curation, a set
of editorial guidelines have been developed to build quality and trust in the BBC.
As personalisation becomes more important for audience engagement, we have been exploring
how algorithmically-driven recommendations could be integrated in our products. In
this talk we describe how we developed recommendation systems for the BBC+ app that
comply with legal and editorial policies and promote the values of the organisation.
We also discuss the challenges we face moving forward, extending the use of recommendation
systems for a public service media organisation like the BBC.The BBC+ app is the first
product to host in-house recommendations in a fully algorithmically-driven application.
The app surfaces short video clips and is targeted at younger audiences. The first
challenge we dealt with was content metadata. Content metadata are created for different
purposes and managed by different teams across the organisation making it difficult
to have reliable and consistent information. Metadata enrichment strategies have been
applied to identify content that is considered to be editorially sensitive, such as
political content, current legal cases, archived news, commercial content, and content
unsuitable for an under 16 audience. Metadata enrichment is also applied to identify
content that due care has not been taken such as poor titles, and spelling and grammar
mistakes. The first versions of recommendation algorithms exclude all editorially
risky content from the recommendations, the most serious of which is avoiding contempt
of court. In other cases we exclude content that could undermine our quality and trustworthiness.The
General Data Protection Regulation (GDPR) that recently came into effect had strong
implications on the design of our system architecture, the choice of the recommendation
models, and the implementation of specific product features. For example, the user
should be able to delete their data or switch off personalisation at any time. Our
system architecture should allow us to trace down and delete all data from that user
and switch to non-personalised content. The recommendations should also be explainable
and this led us to sometimes choosing a simpler model so that it is possible to more
easily explain why a user was recommended a particular type of content. Specific product
features were also added to enhance transparency and explainability. For example,
the user could view their history of watched items, delete any item, and get an explanation
of why a piece of content was recommended to them.At the BBC we aim to not only entertain
our audiences but also to inform and educate. These BBC values are also reflected
in our evaluation strategies and metrics. While we aim to increase audience engagement
we are also responsible for providing recent and diverse content that meets the needs
of all our audiences. Accuracy metrics such as Hit Rate and Normalized Discounted
Cumulative Gain (NDCG) can give a good estimate of the predictive performance of the
model. However, recency and diversity metrics have sometimes more weight in our products,
especially in applications delivering news content. What is more, qualitative evaluation
is very important before releasing any new model into production. We work closely
with editorial teams who provide feedback on the quality of the recommendations and
flag content not adhering to the BBC's values or the legal and editorial policies.The
development of the BBC+ app has been a great journey. We learned a lot about our content
metadata, the implications of GDPR in our system, and our evaluation strategies. We
created a minimum viable product that is compliant with legal and editorial policies.
However, a lot needs to be done to ensure the recommendations meet the quality standards
of the BBC. While excluding editorially sensitive content has limited the risk of
contempt of court, algorithmic fairness and impartiality still need to be addressed.
We encourage the community to look more into these topics and help us create the way
forward towards applications with responsible machine learning.},
  doi       = {10.1145/3298689.3346961},
  isbn      = {9781450362436},
  keywords  = {recommendations, public service, technology policy},
  location  = {Copenhagen, Denmark},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3298689.3346961},
}

@InProceedings{Zhang2015,
  author    = {Zhang, Tianwei and Lee, Ruby B.},
  booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
  title     = {CloudMonatt: An Architecture for Security Health Monitoring and Attestation of Virtual Machines in Cloud Computing},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {362–374},
  publisher = {Association for Computing Machinery},
  series    = {ISCA '15},
  abstract  = {Cloud customers need guarantees regarding the security of their virtual machines (VMs),
operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated
by the customer not knowing where his VM is executing, and on the semantic gap between
what the customer wants to know versus what can be measured in the cloud. We present
an architecture for monitoring a VM's security health, with the ability to attest
this to the customer in an unforgeable manner. We show a concrete implementation of
property-based attestation and a full prototype based on the OpenStack open source
cloud software.},
  doi       = {10.1145/2749469.2750422},
  isbn      = {9781450334020},
  location  = {Portland, Oregon},
  numpages  = {13},
  url       = {https://doi.org/10.1145/2749469.2750422},
}

@Article{Zhang2015a,
  author     = {Zhang, Tianwei and Lee, Ruby B.},
  journal    = {SIGARCH Comput. Archit. News},
  title      = {CloudMonatt: An Architecture for Security Health Monitoring and Attestation of Virtual Machines in Cloud Computing},
  year       = {2015},
  issn       = {0163-5964},
  month      = jun,
  number     = {3S},
  pages      = {362–374},
  volume     = {43},
  abstract   = {Cloud customers need guarantees regarding the security of their virtual machines (VMs),
operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated
by the customer not knowing where his VM is executing, and on the semantic gap between
what the customer wants to know versus what can be measured in the cloud. We present
an architecture for monitoring a VM's security health, with the ability to attest
this to the customer in an unforgeable manner. We show a concrete implementation of
property-based attestation and a full prototype based on the OpenStack open source
cloud software.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2872887.2750422},
  issue_date = {June 2015},
  numpages   = {13},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2872887.2750422},
}

@InProceedings{Xhonneux2018,
  author    = {Xhonneux, Mathieu and Duchene, Fabien and Bonaventure, Olivier},
  booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
  title     = {Leveraging EBPF for Programmable Network Functions with IPv6 Segment Routing},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {67–72},
  publisher = {Association for Computing Machinery},
  series    = {CoNEXT '18},
  abstract  = {With the advent of Software Defined Networks (SDN), Network Function Virtualisation
(NFV) or Service Function Chaining (SFC), operators expect networks to support flexible
services beyond the mere forwarding of packets. The network programmability framework
which is being developed within the IETF by leveraging IPv6 Segment Routing enables
the realisation of in-network functions.In this paper, we demonstrate that this vision
of in-network programmability can be realised. By leveraging the eBPF support in the
Linux kernel, we implement a flexible framework that allows network operators to encode
their own network functions as eBPF code that is automatically executed while processing
specific packets. Our lab measurements indicate that the overhead of calling such
eBPF functions remains acceptable. Thanks to eBPF, operators can implement a variety
of network functions. We describe the architecture of our implementation in the Linux
kernel. This extension has been released with Linux 4.18. We illustrate the flexibility
of our approach with three different use cases: delay measurements, hybrid networks
and network discovery. Our lab measurements also indicate that the performance penalty
of running eBPF network functions on Linux routers does not incur a significant overhead.},
  doi       = {10.1145/3281411.3281426},
  isbn      = {9781450360807},
  location  = {Heraklion, Greece},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3281411.3281426},
}

@InProceedings{Shah2019,
  author    = {Shah, Ryan and Nagaraja, Shishir},
  booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
  title     = {Do We Have the Time for IRM? Service Denial Attacks and SDN-Based Defences},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {496–501},
  publisher = {Association for Computing Machinery},
  series    = {ICDCN '19},
  abstract  = {Distributed sensor networks such as IoT deployments generate large quantities of measurement
data. Often, the analytics that runs on this data is available as a web service which
can be purchased for a fee. A major concern in the analytics ecosystem is ensuring
the security of the data. Often, companies offer Information Rights Management (IRM)
as a solution to the problem of managing usage and access rights of the data that
transits administrative boundaries. IRM enables individuals and corporations to create
restricted IoT data, which can have its flow from organisation to individual control
- disabling copying, forwarding, and allowing timed expiry. We describe our investigations
into this functionality and uncover a weak-spot in the architecture - its dependence
upon the accurate global availability of time. We present an amplified denial-of-service
attack which attacks time synchronisation and could prevent all the users in an organisation
from reading any sort of restricted data until their software has been re-installed
and re-configured. We argue that IRM systems built on current technology will be too
fragile for businesses to risk widespread use. We also present defences that leverage
the capabilities of Software-Defined Networks to apply a simple filter-based approach
to detect and isolate attack traffic.},
  doi       = {10.1145/3288599.3295582},
  isbn      = {9781450360944},
  location  = {Bangalore, India},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3288599.3295582},
}

@InProceedings{TolosanaCalasanz2014,
  author    = {Tolosana-Calasanz, Rafael and Ba\~{n}ares, Jos\'{e} \'{A}ngel and Rana, Omer and Pham, Congduc and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
  booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Enforcing Quality of Service on OpenNebula-Based Shared Clouds},
  year      = {2014},
  pages     = {651–659},
  publisher = {IEEE Press},
  series    = {CCGRID '14},
  abstract  = {With an increase in the number of monitoring sensors deployed on physical infrastructures,
there is a corresponding increase in data volumes that need to be processed. Data
measured or collected by sensors is typically processed at destination or "in-transit"
(i.e. from data capture to delivery to a user). When such data are processed in-transit
over a shared distributed computing infrastructure, it is useful to provide elastic
computational capability which can be adapted based on processing requirements and
demand. Where Service Level Agreements (SLAs) have been pre-agreed, such available
computational capacity needs to be shared in such a way that any Quality of Service
related constraints in such SLAs are not violated. This is particularly challenging
for time critical applications and with highly variable and unpredictable rates of
data generation (e.g. in Smart Grid applications where energy usage patterns may change
unpredictably). Previously, we proposed a Reference net based architectural model
for supporting QoS for multiple concurrent data streams being processed (prior to
delivery to a user) over a shared infrastructure. In this paper, we describe a practical
realisation of this architecture using the OpenNebula Cloud platform. We consider
our infrastructure to be composed of a number of nodes, each of which has multiple
processing units and data buffers. We utilize the "token bucket" model for regulating,
on a per stream basis, the data injection rate into each node. We subsequently demonstrate
how a streaming pipeline can be supported and managed using a dynamic control strategy
at each node.},
  doi       = {10.1109/CCGrid.2014.50},
  isbn      = {9781479927838},
  location  = {Chicago, Illinois},
  numpages  = {9},
  url       = {https://doi.org/10.1109/CCGrid.2014.50},
}

@InProceedings{Talasila2018,
  author    = {Talasila, Prasad and Kakrambe, Mihir and Rai, Anurag and Santy, Sebastin and Goveas, Neena and Deshpande, Bharat M.},
  booktitle = {Proceedings of the 19th International Conference on Distributed Computing and Networking},
  title     = {BITS Darshini: A Modular, Concurrent Protocol Analyzer Workbench},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICDCN '18},
  abstract  = {Network measurements are essential for troubleshooting and active management of networks.
Protocol analysis of captured network packet traffic is an important passive network
measurement technique used by researchers and network operations engineers. In this
work, we present a measurement workbench tool named BITS Darshini (Darshini in short)
to enable scientific network measurements.We have created Darshini as a modular, concurrent
web application that stores experimental meta-data and allows users to specify protocol
parse graphs. Darshini performs protocol analysis on a concurrent pipeline architecture,
persists the analysis to a database and provides the analysis results via a REST API
service. We formulate the problem of mapping protocol parse graph to a concurrent
pipeline as a graph embedding problem. Our tool, Darshini, performs protocol analysis
up to transport layer and is suitable for the study of small and medium-sized networks.
Darshini enables secure collaboration and consultations with experts.},
  articleno = {54},
  doi       = {10.1145/3154273.3154316},
  isbn      = {9781450363723},
  keywords  = {collaborative analysis, packet analyzer, measurement workbench, concurrent packet analysis, protocol parse graph, graph embedding, Network measurements},
  location  = {Varanasi, India},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3154273.3154316},
}

@InProceedings{Tomtsis2016,
  author    = {Tomtsis, Dimitrios and Kontogiannis, Sotirios and Kokkonis, George and Zinas, Nicholas},
  booktitle = {Proceedings of the SouthEast European Design Automation, Computer Engineering, Computer Networks and Social Media Conference},
  title     = {IoT Architecture for Monitoring Wine Fermentation Process of Debina Variety Semi-Sparkling Wine},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {42–47},
  publisher = {Association for Computing Machinery},
  series    = {SEEDA-CECNSM '16},
  abstract  = {This paper proposes a new system architecture and HTTP communication mechanism called
Smart Barrel System (Wine-SBS) for the process of monitoring Debina varietal sparkling
wine fermenting conditions, produced at the area of Zitsa Epirus, Greece. The system
includes microcontroller equipment with sensors that monitor wine attributes and storage
conditions, called CBS-sensor transceivers, which are distributed among the debina
fermentation vessels. The transmission of measurements, which occur periodically,
are sent to a central cloud system application service. The CBS-sensor data are collected
by a CBS-sensor collector and then follows an HTTP/2 request of multiplexed HTTP flows
to a remote application server.},
  doi       = {10.1145/2984393.2984398},
  isbn      = {9781450348102},
  keywords  = {wireless sensor network, Precision enology, wine fermentation monitoring system},
  location  = {Kastoria, Greece},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2984393.2984398},
}

@Article{Sapountzis2017,
  author     = {Sapountzis, Nikolaos and Spyropoulos, Thrasyvoulos and Nikaein, Navid and Salim, Umer},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {User Association in HetNets: Impact of Traffic Differentiation and Backhaul Limitations},
  year       = {2017},
  issn       = {1063-6692},
  month      = dec,
  number     = {6},
  pages      = {3396–3410},
  volume     = {25},
  abstract   = {Operators, struggling to continuously add capacity and upgrade their architecture
to keep up with data traffic increase, are turning their attention to denser deployments
that improve spectral efficiency. Denser deployments make the problem of user association
challenging, and much work has been devoted to finding algorithms that strike a tradeoff
between user quality of service, and network-wide performance load-balancing. Nevertheless,
the majority of these algorithms typically consider simple setups with a single type
of traffic, usually elastic non-guaranteed bit rate GBR. They also focus on the radio
access part, ignoring the backhaul topology and potential capacity limitations. Backhaul
constraints are emerging as a key performance bottleneck in future networks, partly
due to the continuous improvement of the radio interface, and partly due to the need
for inexpensive backhaul links to reduce capital and operational expenditures. To
this end, we propose an analytical framework for user association that jointly considers
radio access and backhaul network performance. Specifically, we derive an algorithm
that takes into account spectral efficiency, base station load, backhaul link capacities
and topology, and two traffic classes GBR and non-GBR in both the uplink and downlink
directions. We prove analytically an optimal user association rule that ends up maximizing
either an arithmetic or a weighted harmonic mean of the achieved performance along
different dimensions e.g., uplink and downlink performances or GBR and non-GBR performances.
We then use extensive simulations to study the impact of: 1 traffic differentiation;
and 2 backhaul capacity limitations and topology on key performance metrics.},
  doi        = {10.1109/TNET.2017.2746011},
  issue_date = {December 2017},
  numpages   = {15},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2017.2746011},
}

@InProceedings{Lattanzi2015,
  author    = {Lattanzi, Silvio and Mirrokni, Vahab},
  booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
  title     = {Distributed Graph Algorithmics: Theory and Practice},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {419–420},
  publisher = {Association for Computing Machinery},
  series    = {WSDM '15},
  abstract  = {As a fundamental tool in modeling and analyzing social, and information networks,
large-scale graph mining is an important component of any tool set for big data analysis.
Processing graphs with hundreds of billions of edges is only possible via developing
distributed algorithms under distributed graph mining frameworks such as MapReduce,
Pregel, Gigraph, and alike. For these distributed algorithms to work well in practice,
we need to take into account several metrics such as the number of rounds of computation
and the communication complexity of each round. For example, given the popularity
and ease-of-use of MapReduce framework, developing practical algorithms with good
theoretical guarantees for basic graph algorithms is a problem of great importance.In
this tutorial, we first discuss how to design and implement algorithms based on traditional
MapReduce architecture. In this regard, we discuss various basic graph theoretic problems
such as computing connected components, maximum matching, MST, counting triangle and
overlapping or balanced clustering. We discuss a computation model for MapReduce and
describe the sampling, filtering, local random walk, and core-set techniques to develop
efficient algorithms in this framework. At the end, we explore the possibility of
employing other distributed graph processing frameworks. In particular, we study the
effect of augmenting MapReduce with a distributed hash table (DHT) service and also
discuss the use of a new graph processing framework called ASYMP based on asynchronous
message-passing method. In particular, we will show that using ASyMP, one can improve
the CPU usage, and achieve significantly improved running time.},
  doi       = {10.1145/2684822.2697043},
  isbn      = {9781450333177},
  keywords  = {parallel computing, mapreduce algorithms, large scale data-mining},
  location  = {Shanghai, China},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2684822.2697043},
}

@InProceedings{Chrobot2014,
  author    = {Chrobot, Nina},
  booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  title     = {The Role of Processing Fluency in Online Consumer Behavior: Evaluating Fluency by Tracking Eye Movements},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {387–388},
  publisher = {Association for Computing Machinery},
  series    = {ETRA '14},
  abstract  = {The Internet enables people to extensively research products or services, and also
easily compare prices between offers [e.g. Baker et al. 2001]. Taking into account
the amount of information available on the Internet, acquisition of new information
can face some difficulties, especially when one wants to make a purchase decision.
Therefore, the ability to process relevant information fluently enables a user to
create a better experience and to become more efficient in gathering information related
to the purpose of the visit. This ability might be connected to the cognitive task
that can either be effortless or effortful, and may lead to a metacognitive experience
of either fluency or disfluency [Alter and Oppenheimer 2009]. Nevertheless, some e-commerce
websites are preferred over others and this preference varies between individuals.
This variation can be influenced by user's prior experience, cognitive sources but
also graphics or information architecture on the web page. Presented project aims
at applying the fluency concept to consumer behavior in online environment by studying
eye movements and promoting eye tracking as an objective measure.},
  doi       = {10.1145/2578153.2583037},
  isbn      = {9781450327510},
  location  = {Safety Harbor, Florida},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2578153.2583037},
}

@InProceedings{Zhang2020,
  author    = {Zhang, Chaoyun and Fiore, Marco and Ziemlicki, Cezary and Patras, Paul},
  booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
  title     = {Microscope: Mobile Service Traffic Decomposition for Network Slicing as a Service},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {MobiCom '20},
  abstract  = {The growing diversification of mobile services imposes requirements on network performance
that are ever more stringent and heterogeneous. Network slicing aligns mobile network
operation to this context, by enabling operators to isolate and customize network
resources on a per-service basis. A key input for provisioning resources to slices
is real-time information about the traffic demands generated by individual services.
Acquiring such knowledge is however challenging, as legacy approaches based on in-depth
inspection of traffic streams have high computational costs, which inflate with the
widening adoption of encryption over data and control traffic. In this paper, we present
a new approach to service-level demand estimation for slicing, which hinges on decomposition,
i.e., the inference of per-service demands from traffic aggregates. By operating on
total traffic volumes only, our approach overcomes the complexity and limitations
of legacy traffic classification techniques, and provides a suitable input to recent 'Network Slice as a Service' (NSaaS) models. We implement decomposition through Microscope,
a novel framework that uses deep learning to infer individual service demands from
complex spatiotemporal features hidden in traffic aggregates. Microscope (i) transforms
traffic data collected in irregular radio access deployments in a format suitable
for convolutional learning, and (ii) can accommodate a variety of neural network architectures,
including original 3D Deformable Convolutional Neural Networks (3D-DefCNNs) that we
explicitly design for decomposition. Experiments with measurement data collected in
an operational network demonstrate that Microscope accurately estimates per-service
traffic demands with relative errors below 1.2%. Further, tests in practical NSaaS
management use cases show that resource allocations informed by decomposition yield
affordable costs for the mobile network operator.},
  articleno = {38},
  doi       = {10.1145/3372224.3419195},
  isbn      = {9781450370851},
  keywords  = {neural networks, traffic decomposition, service demand estimation, network slicing, mobile network data traffic, deep learning},
  location  = {London, United Kingdom},
  numpages  = {14},
  url       = {https://doi.org/10.1145/3372224.3419195},
}

@InProceedings{Arndt2017,
  author    = {Arndt, Oliver Jakob and Spindeldreier, Christian and Wohnrade, Kevin and Pfefferkorn, Daniel and Neuenhahn, Martin and Blume, Holger},
  booktitle = {Proceedings of the 8th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
  title     = {FPGA Accelerated NoC-Simulation: A Case Study on the Intel Xeon Phi Ringbus Topology},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {HEART2017},
  abstract  = {Complex signal processing algorithms targeted on architectures with increasingly high
numbers of parallel processing units require high performance core-interconnections
(i.e., low latencies, high throughput, no pinch-offs or bottlenecks). Therefore, assisting
techniques, exploring characteristics of diverse topologies of common as well as innovative
Network-on-Chips (NoCs), are necessary for the development of chips with massive parallel
processing cores. In contrast to analytic NoC models, event driven NoC simulations
can handle even complex task graphs, but however feature long simulation times. Enabling
the simulation of even complex task graphs, in this work, we propose to use FPGA accelerated
simulation. While we extend such a simulator in order to imitate cache coherence communication-behavior,
we also present a translation of real measured profiles to task graphs for in-depth
simulation of the communication behavior of an existing NoC-based manycore. Therefore,
this approach is able to not only deal with synthetic scenarios, but analyse the communication
behavior of real world applications. Additionally, a simulation of the Histograms
of Oriented Gradients algorithm, running on the Intel Xeon Phi manycore, exhibiting
a 70-stop ring-bus, exemplifies this approach.},
  articleno = {21},
  doi       = {10.1145/3120895.3120916},
  isbn      = {9781450353168},
  location  = {Bochum, Germany},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3120895.3120916},
}

@InProceedings{Podiyan2015,
  author    = {Podiyan, Pradeep and Butakov, Sergey and Zavarsky, Pavol},
  booktitle = {Proceedings of the 8th ACM Conference on Security &amp; Privacy in Wireless and Mobile Networks},
  title     = {Study of Compliance of Android Location APIs with Geopriv},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {WiSec '15},
  abstract  = {This paper carefully examines the location APIs of Android OS as well as the Geopriv
standard architecture to study measures that are being taken by Android OS to protect
the location privacy of a user. Android offers various location APIs in its architecture
for the app developers to work on location based services (LBS). The results of this
evaluation will be compared with Geopriv standard architecture and its ways to enhance
location information privacy on mobile platforms. The review of functionality of location
APIs shows that Android has limited features such as Geofencing to have some extent
of location privacy for a typical user. Only few of the recommendation in distribution
segment of Geopriv with slightly different approach are similar to the protection
mechanisms offered by location APIs in Android. The paper proposes general steps that
can be taken to address location privacy issues on mobile devices.},
  articleno = {30},
  doi       = {10.1145/2766498.2774989},
  isbn      = {9781450336239},
  keywords  = {Geopriv, Android, location privacy},
  location  = {New York, New York},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2766498.2774989},
}

@InProceedings{Liu2017,
  author    = {Liu, Ling},
  booktitle = {Proceedings of the 2017 Workshop on Women in Cyber Security},
  title     = {Keynote: Privacy and Trust: Friend or Foe},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {11},
  publisher = {Association for Computing Machinery},
  series    = {CyberW '17},
  abstract  = {Internet of Things (IoT) and Big Data have fueled the development of fully distributed
computational architectures for future cyber systems from data analytics, machine
learning (ML) to artificial intelligence (AI). Trust and Privacy become two vital
and necessary measures for distributed management of IoT powered big data learning
systems and services. However, these two measures have been studied independently
in computer science, social science and law.Trust is widely considered as a critical
measure for the correctness, predictability, and resiliency (with respect to reliability
and security) of software systems, be it big data systems, IoT systems, machine learning
systems, or Artificial Intelligence systems. Privacy on the other hand is commonly
recognized as a personalization measure for imposing control on the ways of how data
is captured, accessed and analyzed, and the ways of how data analytic results from
ML models and AI systems should be released and shared.Broadly speaking, in human
society, we rely on three types of trust in our everyday work and life to achieve
a peaceful mind: (1) verifiable belief-driven trust, (2) statistical evidence based
trust, and (3) complex systemwide cognitive trust. Interestingly, privacy has been
a more controversial subject. On one hand, privacy is an important built-in dimension
of trust, which is deep rooted in human society, and a highly valued virtue in Western
civilization. Even though different human beings may have diverse levels of privacy
sensitivity, we all trust that our privacy is respected in our social and professional
environments, including at home, at work and in social commons. Thus, Privacy is a
perfect example of three-fold trust: belief-driven, statistical evident, and complex
cognitive trust. On the other hand, many view privacy (and privacy protection) as
an antagonistic measure of trust and one is often asked to show trust at the cost
of giving up on privacy.Are Privacy and Trust friend or foe? This keynote will share
my view to this question from multiple perspectives. I conjecture that the answer
to this question can fundamentally change the ways we conduct research in privacy
and trust in the next generation of big data enhanced cyber learning systems from
data mining, machine learning to artificial intelligence.},
  doi       = {10.1145/3139531.3139537},
  isbn      = {9781450353939},
  keywords  = {internet of things, deep learning, privacy, big data, trust},
  location  = {Dallas, Texas, USA},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3139531.3139537},
}

@InProceedings{Fiadino2014,
  author    = {Fiadino, Pierdomenico and Schiavone, Mirko and Casas, Pedro},
  booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
  title     = {Vivisecting Whatsapp through Large-Scale Measurements in Mobile Networks},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {133–134},
  publisher = {Association for Computing Machinery},
  series    = {SIGCOMM '14},
  abstract  = {WhatsApp, the new giant in instant multimedia messaging in mobile networks is rapidly
increasing its popularity, taking over the traditional SMS/MMS messaging. In this
paper we present the first large-scale characterization of WhatsApp, useful among
others to ISPs willing to understand the impacts of this and similar applications
on their networks. Through the combined analysis of passive measurements at the core
of a national mobile network, worldwide geo-distributed active measurements, and traffic
analysis at end devices, we show that: (i) the WhatsApp hosting architecture is highly
centralized and exclusively located in the US; (ii) video sharing covers almost 40%
of the total WhatsApp traffic volume; (iii) flow characteristics depend on the OS
of the end device; (iv) despite the big latencies to US servers, download throughputs
are as high as 1.5 Mbps; (v) users react immediately and negatively to service outages
through social networks feedbacks.},
  doi       = {10.1145/2619239.2631461},
  isbn      = {9781450328364},
  keywords  = {instant multimedia messaging, service outages, whatsapp, mobile networks, large-scale measurements},
  location  = {Chicago, Illinois, USA},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2619239.2631461},
}

@Article{Fiadino2014a,
  author     = {Fiadino, Pierdomenico and Schiavone, Mirko and Casas, Pedro},
  journal    = {SIGCOMM Comput. Commun. Rev.},
  title      = {Vivisecting Whatsapp through Large-Scale Measurements in Mobile Networks},
  year       = {2014},
  issn       = {0146-4833},
  month      = aug,
  number     = {4},
  pages      = {133–134},
  volume     = {44},
  abstract   = {WhatsApp, the new giant in instant multimedia messaging in mobile networks is rapidly
increasing its popularity, taking over the traditional SMS/MMS messaging. In this
paper we present the first large-scale characterization of WhatsApp, useful among
others to ISPs willing to understand the impacts of this and similar applications
on their networks. Through the combined analysis of passive measurements at the core
of a national mobile network, worldwide geo-distributed active measurements, and traffic
analysis at end devices, we show that: (i) the WhatsApp hosting architecture is highly
centralized and exclusively located in the US; (ii) video sharing covers almost 40%
of the total WhatsApp traffic volume; (iii) flow characteristics depend on the OS
of the end device; (iv) despite the big latencies to US servers, download throughputs
are as high as 1.5 Mbps; (v) users react immediately and negatively to service outages
through social networks feedbacks.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2740070.2631461},
  issue_date = {October 2014},
  keywords   = {service outages, large-scale measurements, mobile networks, whatsapp, instant multimedia messaging},
  numpages   = {2},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2740070.2631461},
}

@InProceedings{Mell2016,
  author    = {Mell, Peter and Shook, James and Harang, Richard},
  booktitle = {Proceedings of the 2nd Annual Industrial Control System Security Workshop},
  title     = {Measuring and Improving the Effectiveness of Defense-in-Depth Postures},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {15–22},
  publisher = {Association for Computing Machinery},
  series    = {ICSS '16},
  abstract  = {Defense-in-depth is an important security architecture principle that has significant
application to industrial control systems (ICS), cloud services, storehouses of sensitive
data, and many other areas. We claim that an ideal defense-in-depth posture is 'deep',
containing many layers of security, and 'narrow', the number of node independent attack
paths is minimized. Unfortunately, accurately calculating both depth and width is
difficult using standard graph algorithms because of a lack of independence between
multiple vulnerability instances (i.e., if an attacker can penetrate a particular
vulnerability on one host then they can likely penetrate the same vulnerability on
another host). To address this, we represent known weaknesses and vulnerabilities
as a type of colored attack graph. We measure depth and width through solving the
shortest color path and minimum color cut problems. We prove both of these to be NP-Hard
and thus for our solution we provide a suite of greedy heuristics. We then empirically
apply our approach to large randomly generated networks as well as to ICS networks
generated from a published ICS attack template. Lastly, we discuss how to use these
results to help guide improvements to defense-in-depth postures.},
  doi       = {10.1145/3018981.3018986},
  isbn      = {9781450347884},
  keywords  = {security, measurement, attack graph, defense in depth},
  location  = {Los Angeles, CA, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3018981.3018986},
}

@InProceedings{Kokkonis2017,
  author    = {Kokkonis, George and Kontogiannis, Sotirios and Tomtsis, Dimitrios},
  booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
  title     = {FITRA: A Neuro-Fuzzy Computational Algorithm Approach Based on an Embedded Water Planting System},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICC '17},
  abstract  = {This paper proposes a novel neuro-fuzzy computational algorithm for embedded irrigation
systems called FITRA. It presents a new system architecture for the process of continuously
monitoring environmental conditions and efficient irrigation of arable areas. The
system includes microcontroller equipment with multiple sensors interspersed all over
the field. Transmissions of measurements, which occur periodically, send to a central
cloud system Application Service (AS) assisted by a 3G network. The decision for irrigation
or not is made by a neuro-fuzzy algorithm. As an input for that algorithm are the
values taken from the interspersed sensors. As an output, this algorithm controls
the central solenoid water valve of the water planting system. The irrigation system
automatically adjusts to changing environmental conditions.},
  articleno = {39},
  doi       = {10.1145/3018896.3018934},
  isbn      = {9781450347747},
  keywords  = {neuro-fuzzy algorithms, soil sensor, agriculture, smart irrigation, smart farming, water planting systems, IoT},
  location  = {Cambridge, United Kingdom},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3018896.3018934},
}

@InProceedings{Ma2017,
  author    = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
  booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
  title     = {Simulating Conflict Detection in Heterogeneous Services of a Smart City: Demo Abstract},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {275–276},
  publisher = {Association for Computing Machinery},
  series    = {IoTDI '17},
  abstract  = {Despite the increasing intelligence of smart services and sophistication of IoT platforms,
the safety issues in smart cities are not addressed adequately, especially the safety
issues arising from the integration of smart services. Therefore, in this demo abstract,
we present CityGuard, a safety-aware watchdog architecture to detect conflicts among
actions of heterogeneous services considering both safety and performance requirements.
This demo simulates parts of New York City to depict how CityGuard identifies unsafe
actions and thus helps to prevent the city from safety hazards, detects two major
types of conflicts, i.e., device and environmental conflicts, and improves the overall
city performance in terms of multiple performance metrics. This demo complements the
full paper on CityGuard that appears in this conference [2].},
  doi       = {10.1145/3054977.3057290},
  isbn      = {9781450349666},
  keywords  = {Smart City, City Simulation, Conflict Detection, City Safety},
  location  = {Pittsburgh, PA, USA},
  numpages  = {2},
  url       = {https://doi.org/10.1145/3054977.3057290},
}

@InProceedings{Oliveira2021,
  author    = {Oliveira, Breno Silva and Ara\'{u}jo, \'{I}talo L. and Paiva, Joseane O. V. and Junior, Evilasio C. and Andrade, Rossana M. C.},
  booktitle = {XVII Brazilian Symposium on Information Systems},
  title     = {Refactoring Decision Based on Measurements for IoHT Apps},
  year      = {2021},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SBSI 2021},
  abstract  = {Internet of Things (IoT) provides smart objects with the ability to connect to the
Internet, allowing the exchange of information among them to provide a certain service
and the development of innovative applications in several domains, including e-Health,
in which it is called Internet of Health Things (IoHT). This domain can be critical
specially when the application deals with the monitoring of the user health in real-time,
what demands software quality assurance, even more than in other applications. Measures
can be used to support that, for example, measures can suggest which components need
refactoring to improve the software code, thus improving the application. In this
work, we report how to do that with two existing measures that guide the refactoring
process of an IoHT application for fall detection, called WatchAlert. These measures
indicate that changes in both the architecture and the algorithms for fall detection
should occur. After the refactoring, the app accuracy was improved from 73.3% to 92.7%.
We believe that this work can contribute to other studies focusing on developing applications
on the IoHT domain using a methodology, a set of refactoring techniques, and lessons
learned that could be replicated to improve the quality of this type of application.},
  articleno = {12},
  doi       = {10.1145/3466933.3466945},
  isbn      = {9781450384919},
  keywords  = {Refactoring, Fall detection, Measures, Internet of Things, e-Health},
  location  = {Uberl\^{a}ndia, Brazil},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3466933.3466945},
}

@InProceedings{Koupaee2019,
  author    = {Koupaee, Mahnaz},
  booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
  title     = {Mortality Prediction Using Medical Notes: Student Research Abstract},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {778–781},
  publisher = {Association for Computing Machinery},
  series    = {SAC '19},
  abstract  = {Mortality prediction is a critical task for assessing patients' conditions in Intensive
Care Units (ICU) of hospitals to improve decision-making and quality of care. Measurements
taken and recorded at different time points are the main source of information to
be used for tasks related to healthcare. However, the notes written by medical service
providers during patients' stay in hospital as a rich source of detailed information
is not sufficiently exploited. In this work, we propose a Convolutional Neural Network
(CNN) architecture to utilize the unstructured texts to predict the pre-discharge
and post-discharge mortality of ICU patients. Evaluations show high performance of
the proposed method in terms of precision and recall. Moreover, our method outperforms
the state of the art method by achieving a higher AUC.},
  doi       = {10.1145/3297280.3297648},
  isbn      = {9781450359337},
  keywords  = {convolutional neural network, medical notes, MIMIC, mortality prediction},
  location  = {Limassol, Cyprus},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3297280.3297648},
}

@InProceedings{Wen2020,
  author    = {Wen, Yana and Wei, Tingyue and Cui, Kewei and Ling, Bai and Zhang, Yahao and Huang, Meng},
  booktitle = {2020 6th International Conference on Robotics and Artificial Intelligence},
  title     = {Research on Belt and Road Big Data Visualization Based on Text Clustering Algorithm},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {121–125},
  publisher = {Association for Computing Machinery},
  series    = {ICRAI 2020},
  abstract  = {In the era of big data, people's visual needs for data expression are increasing.
In order to achieve better big data display effects, this article introduced the use
of text clustering algorithms to achieve data crawling and Echarts technology to realize
big data visualization. This system used mvvm's architecture and vue framework development
platform, ThinkPHP was used as the background framework, and ES6 related technologies
and specifications were used for application development. This system used Echarts,
IView, GIS technology and JavaScript development methods to demonstrate economic big
data module functions on the web side; Applied CSS3, HTML5, GIS technology to implement
project achievement module and university alliance module; Applied Echarts, HTML5,
JS function library technology to achieve national information module. This system
used stored procedure, database index optimization technology to achieve rapid screening
of massive data, and dynamically update and displayed related data through two-way
data binding. This system combined real-time location technology with GIS technology
to measure the distance between the user and the destination, and automatically plan
the tour route to provide related services. This system can provide feasibility suggestions
for strategic researchers or experts in related areas of the “Belt and Road”, and
provide theoretical basis and technical support.},
  doi       = {10.1145/3449301.3449322},
  isbn      = {9781450388597},
  keywords  = {big data visualization, Text clustering algorithm, One Belt One Road, Keywords-component},
  location  = {Singapore, Singapore},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3449301.3449322},
}

@InProceedings{GraciaTinedo2015,
  author    = {Gracia-Tinedo, Ra\'{u}l and Tian, Yongchao and Samp\'{e}, Josep and Harkous, Hamza and Lenton, John and Garc\'{\i}a-L\'{o}pez, Pedro and S\'{a}nchez-Artigas, Marc and Vukolic, Marko},
  booktitle = {Proceedings of the 2015 Internet Measurement Conference},
  title     = {Dissecting UbuntuOne: Autopsy of a Global-Scale Personal Cloud Back-End},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {155–168},
  publisher = {Association for Computing Machinery},
  series    = {IMC '15},
  abstract  = {Personal Cloud services, such as Dropbox or Box, have been widely adopted by users.
Unfortunately, very little is known about the internal operation and general characteristics
of Personal Clouds since they are proprietary services.In this paper, we focus on
understanding the nature of Personal Clouds by presenting the internal structure and
a measurement study of UbuntuOne (U1). We first detail the U$1$ architecture, core
components involved in the U1 metadata service hosted in the datacenter of Canonical,
as well as the interactions of U$1$ with Amazon S3 to outsource data storage. To our
knowledge, this is the first research work to describe the internals of a large-scale
Personal Cloud.Second, by means of tracing the U$1$ servers, we provide an extensive
analysis of its back-end activity for one month. Our analysis includes the study of
the storage workload, the user behavior and the performance of the U1 metadata store.
Moreover, based on our analysis, we suggest improvements to U1 that can also benefit
similar Personal Cloud systems.Finally, we contribute our dataset to the community,
which is the first to contain the back-end activity of a large-scale Personal Cloud.
We believe that our dataset provides unique opportunities for extending research in
the field.},
  doi       = {10.1145/2815675.2815677},
  isbn      = {9781450338486},
  keywords  = {personal cloud, performance analysis, measurement},
  location  = {Tokyo, Japan},
  numpages  = {14},
  url       = {https://doi.org/10.1145/2815675.2815677},
}

@Article{Kocher2020,
  author     = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
  journal    = {Commun. ACM},
  title      = {Spectre Attacks: Exploiting Speculative Execution},
  year       = {2020},
  issn       = {0001-0782},
  month      = jun,
  number     = {7},
  pages      = {93–101},
  volume     = {63},
  abstract   = {Modern processors use branch prediction and speculative execution to maximize performance.
For example, if the destination of a branch depends on a memory value that is in the
process of being read, CPUs will try to guess the destination and attempt to execute
ahead. When the memory value finally arrives, the CPU either discards or commits the
speculative computation. Speculative logic is unfaithful in how it executes, can access
the victim's memory and registers, and can perform operations with measurable side
effects.Spectre attacks involve inducing a victim to speculatively perform operations
that would not occur during correct program execution and which leak the victim's
confidential information via a side channel to the adversary. This paper describes
practical attacks that combine methodology from side-channel attacks, fault attacks,
and return-oriented programming that can read arbitrary memory from the victim's process.
More broadly, the paper shows that speculative execution implementations violate the
security assumptions underpinning numerous software security mechanisms, such as operating
system process separation, containerization, just-in-time (JIT) compilation, and countermeasures
to cache timing and side-channel attacks. These attacks represent a serious threat
to actual systems because vulnerable speculative execution capabilities are found
in microprocessors from Intel, AMD, and ARM that are used in billions of devices.Although
makeshift processor-specific countermeasures are possible in some cases, sound solutions
will require fixes to processor designs as well as updates to instruction set architectures
(ISAs) to give hardware architects and software developers a common understanding
as to what computation state CPU implementations are (and are not) permitted to leak.},
  address    = {New York, NY, USA},
  doi        = {10.1145/3399742},
  issue_date = {July 2020},
  numpages   = {9},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3399742},
}

@InProceedings{Migault2014,
  author    = {Migault, Daniel and Palomares, Daniel and Hendrik, Hendrik and Laurent, Maryline},
  booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
  title     = {Secure IPsec Based Offload Architectures for Mobile Data},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {95–104},
  publisher = {Association for Computing Machinery},
  series    = {Q2SWinet '14},
  abstract  = {Radio Access Network (RAN) are likely to be overloaded, and some places will not be
able to provide the necessary requested bandwidth. In order to respond to the demand
of bandwidth, overloaded RAN are currently offloading their traffic on WLAN. WLAN
Access Points like (ISP provided xDSL boxes) are untrusted, unreliable and do not
handle mobility. As a result, mobility, multihoming, and security cannot be handled
by the network anymore, and must be handled by the terminal. This paper positions
offload architectures based on IPsec and shows that IPsec can provide end-to-end security,
as well as seamless connectivity across IP networks. Then, the remaining of the paper
evaluates how mobility on these IPsec based architectures impacts the Quality of Service
(QoS) for real time applications such as an audio streaming service. QoS is measured
using network interruption time and POLQA. Measurements compare TCP/HLS and UDP/RTSP
over various IPsec configurations.},
  doi       = {10.1145/2642687.2642690},
  isbn      = {9781450330275},
  keywords  = {wlan offload architecture, terminal mobility, quality of service, IPsec multiple interfaces, IPsec mobility},
  location  = {Montreal, QC, Canada},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2642687.2642690},
}

@InProceedings{Esposito2019,
  author    = {Esposito, Christian and Pop, Florin and Choi, Chang},
  booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
  title     = {Session Details: Theme: Information Systems: SFECS - Sustainability of Fog/Edge Computing Systems Track},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SAC '19},
  abstract  = {Fog/Edge Computing paradigms are widely used in enterprises to address the emerging
challenges of big data analysis, because of their underlying scalable, flexible and
distributed data management schemes. The data centers in the Clouds are facing great
challenges on the burden of the consequent increasing the amount of data to be man-
aged and the additional requirements of location awareness and low latency at the
edge of network necessary by smart cites and factories. These are the reasons why
a centralized model cannot be an efficient solution for generated or required data
by the IoT devices in those applications and there is the progressive shift towards
fog nodes and smarted edge nodes mediating between the cloud and the IoT devices.
The Fog/Edge computing paradigm is a decentralized model that transfers a part of
low computing data analysis from the cloud to the intermediate (fog) nodes or the
edges, performing only high computing tasks in the cloud. This new approach tries
to minimize the three factors that negatively compromise the effective and efficient
application of the Cloud computing to smart cities and factories, or similar application
domains: the network bandwidth usage, decentralization of the data processing tasks
and reduced response latency for clients (IoT devices). Fog/Edge computing is a hierarchical
approach where the overall infrastructure is structured in multiple layers, each responsible
of offering a good coordination and data management to the nodes at the lower layer.
The lowest layer is usually composed of sensors and/or actuators that measure and/or
control the environment or a given business process, implemented as mobile devices
that are running a sensing/controlling application. In this case, combining Sustainable
computing with Fog and Edge computing represents a new approach for increasing quality-of-
service and efficiency of the system, creating the capability to present temporal
and geo-coded information, and increasing innovation, and co-designing sustainable
future large scale distributed systems. This new paradigm appears to offer a good
approach in handling the scale factor of the data size, reducing the network bandwidth
usage and the response latency of the system. In order to support specifically the
Fog/Edge architectures, there is a need, for instance, of location-awareness and computation
placement, replication and recovery. In many cases Edge resources would be required
for both computation and data storage to address the time and locality constraints.
There are multiple kinds of orchestration management solutions for virtualization
in this type of architecture with different characteristics and drawbacks. This results
in different restrictions for application definition, scalability, availability, load
balancing and so on. Also, virtualization may be needed at multiple levels in a Fog/Edge
architecture as it consists of the following levels of abstraction: at the sensing
level we have the IoT devices/smart things, at the Edge level there are the gateways
to a first collection and the data from the IoT devices and their preliminary processing,
at the Fog level we have an additional data management layer, and at the Cloud level
there is the compute/storage infrastructure with applications on top. Last, but not
least, the energy efficiency is particularly important at the IoT and edge level since
the devices may be equipped with a limited battery, possible difficult or impossible
to be charged. So, optimizing the energy consumption is a must. To address several
open research is- sues regarding sustainability of future Fog/Edge systems, this track
aims at solicit contributions highlighting challenges, state-of-the-art, and solutions
to a set of currently unresolved key questions including - but not limited to - performance,
modeling, optimization, energy-efficiency, reliability, security, privacy and techno-economic
aspects of Fog/Edge systems. Through addressing these concerns while understanding
their impacts and limitations, technological advancements will be channeled toward
more sustainable/efficient platforms for tomorrow's ever-connected systems.},
  doi       = {10.1145/3329391},
  isbn      = {9781450359337},
  location  = {Limassol, Cyprus},
  url       = {https://doi.org/10.1145/3329391},
}

@InProceedings{Achtzehn2015,
  author    = {Achtzehn, Andreas and Riihihj\"{a}rvi, Janne and Barri\'{\i}a Castillo, Irving Antonio and Petrova, Marina and M\"{a}h\"{o}nen, Petri},
  booktitle = {Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications},
  title     = {<i>CrowdREM</i>: Harnessing the Power of the Mobile Crowd for Flexible Wireless Network Monitoring},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {63–68},
  publisher = {Association for Computing Machinery},
  series    = {HotMobile '15},
  abstract  = {High-speed mobile broadband connections have opened exciting new opportunities to
collect sensor data from thousands or even millions of distributed mobile devices
for the purpose of crowdsourced decision making. In this paper, we propose CrowdREM
(crowdsourced radio environment mapping), a framework with the specific aim of monitoring
and modelling wireless cellular networks. CrowdREM enables operator-independent and
highly efficient collection of network performance data along all layers of the communications
protocol stack. Such extensive information on network load, spectrum usage, or local
coverage can help operators to optimize their networks and service quality and enable
improved consumer decision making. In this paper, we introduce the mbox{CrowdREM}
mobile architecture and show first results from a prototype implementation on open-source
mobile phones. We demonstrate the versatility of using commodity devices for network
and spectrum monitoring, and present the challenges originating from the use of uncalibrated
and low-precision measurement equipment. We have acquired an extensive data set from
using our prototype implementation in a 21-day measurement campaign covering more
than 1,000 hours of measurement data. From this we present and discuss the potential
derivation of tangible and relevant network performance and signal quality indicators,
which could, e.g., be conducted by independent parties.},
  doi       = {10.1145/2699343.2699348},
  isbn      = {9781450333917},
  keywords  = {drive testing, crowdsourcing, cellular networks, mobile},
  location  = {Santa Fe, New Mexico, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2699343.2699348},
}

@Proceedings{2014,
  title     = {RIIT '14: Proceedings of the 3rd Annual Conference on Research in Information Technology},
  year      = {2014},
  address   = {New York, NY, USA},
  isbn      = {9781450327114},
  publisher = {Association for Computing Machinery},
  abstract  = {It is with great pleasure that we welcome you to the 15th Annual Conference on Information
Technology Education (SIGITE 2014) and the 3rd Annual Conference on Research in Information
Technology (RIIT 2014). The theme this year is "Riding the Wave of Change in Information
Technology" and the many quality submissions we received allowed us to assemble one
of the strongest programs in the history of the conferences. As in past years, the
synergies between research and education in information technology are prevalent,
and several themes emerged from the accepted submissions. Networking, security, and
development remain popular with researchers, and interest in mobile computing, resource
measurement and management, capstone courses, and personalization has grown.The call
for participation attracted 111 submissions, 72 of which were submitted to SIGITE
and 39 to RIIT. Both numbers represent a larger pool than in recent years, demonstrating
that the conferences are of great interest in the community. Ninety-five of the submissions
were papers, with 59 papers submitted to SIGITE and 36 papers submitted to RIIT. SIGITE
has 27 papers in its program for an acceptance rate of 46% and RIIT has 14 papers
for an acceptance rate of 39%. All of the authors presenting should be congratulated
on their excellent work.A conference cannot happen without the help of its reviewers,
and this year was no exception. Fiftyfive reviewers worked diligently to ensure that
every paper had at least three independent reviews. It was a significant effort to
produce the 317 reviews that ended up in the system, and we thank the reviewers from
the bottom of our heart. New to the conferences this year was a meta review process,
in which 13 diligent meta reviewers together examined all reviews for each submission
and reconciled those reviews into a coherent message for each author. We hope the
meta review process enabled authors to have more substantive feedback on their work,
whether it appears in the final program or not.The conference runs from Thursday to
Saturday and each day offers something of interest to attendees. On Thursday our keynote
speaker is Dr. Flavio Villanustre, Vice-President of Technology Architecture &amp; Product
for LexisNexis and HPCC Systems. The day continues with a workshop on end-user development
activities and paper sessions for both SIGITE and RIIT. Thursday concludes with a
reception, which we know will be useful for networking with colleagues old and new.
Friday introduces a new presentation format, lightning talks on research in progress,
at the conferences. There are also paper sessions for SIGITE and RIIT, a poster session
in the afternoon and, of course, more opportunities for networking during lunch and
the breaks. Saturday offers a three-hour workshop on process-oriented guided inquiry
learning (POGIL) as well as a panel on mobile computing courses and some excellent
SIGITE papers. We also hope that you stay for the closing session where we will share
our plans for SIGITE/RIIT 2015 in Chicago.We hope you find the conference presentations
interesting and thought-provoking, you reconnect with colleagues you know, you find
new collaborators, and you submit the work that results to SIGITE or RIIT next year.
The excellence you see at SIGITE/RIIT 2014 depends on your energy and effort, and
we thank you for letting us be a part of it.},
  location  = {Atlanta, Georgia, USA},
}

@InProceedings{DijkstraSoudarissanane2021,
  author    = {Dijkstra-Soudarissanane, Sylvie and Klunder, Tessa and Brandt, Aschwin and Niamut, Omar},
  booktitle = {ACM International Conference on Interactive Media Experiences},
  title     = {Towards XR Communication for Visiting Elderly at Nursing Homes},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {319–321},
  publisher = {Association for Computing Machinery},
  series    = {IMX '21},
  abstract  = {Due to the current pandemic, the elderly in care homes are greatly affected by the
lack of contact with their families, resulting in various mental conditions (e.g.,
depression, feelings of loneliness) and deterioration of mental health for dementia
patients. In response, residents and family members increasingly resorted to mediated
communication to maintain social contact. To facilitate high-quality mediated social
contact between residents in nursing homes and remote family members, we developed
an Augmented Reality (AR)-based communication tool. The proposed demonstrator improved
this situation by providing a working communication tool that enables the elderly
to feel being together with their family by means of AR techniques. A complete end-to-end-chain
architecture is defined, where the aspects of capture, transmission, and rendering
are thoroughly investigated to fit the purpose of the use case. Based on an extensive
user study comprising user experience (UX) and quality of service (QoS) measurements,
each module is presented with the improvements made and the resulting higher quality
AR communication platform.},
  doi       = {10.1145/3452918.3467815},
  isbn      = {9781450383899},
  keywords  = {Augmented Reality, Social XR, Immersive Media, AR, Volumetric video, WebRTC, Conferencing, Communication},
  location  = {Virtual Event, USA},
  numpages  = {3},
  url       = {https://doi.org/10.1145/3452918.3467815},
}

@Proceedings{2014a,
  title     = {SIGITE '14: Proceedings of the 15th Annual Conference on Information Technology Education},
  year      = {2014},
  address   = {New York, NY, USA},
  isbn      = {9781450326865},
  publisher = {Association for Computing Machinery},
  abstract  = {It is with great pleasure that we welcome you to the 15th Annual Conference on Information
Technology Education (SIGITE 2014) and the 3rd Annual Conference on Research in Information
Technology (RIIT 2014). The theme this year is "Riding the Wave of Change in Information
Technology" and the many quality submissions we received allowed us to assemble one
of the strongest programs in the history of the conferences. As in past years, the
synergies between research and education in information technology are prevalent,
and several themes emerged from the accepted submissions. Networking, security, and
development remain popular with researchers, and interest in mobile computing, resource
measurement and management, capstone courses, and personalization has grown.The call
for participation attracted 111 submissions, 72 of which were submitted to SIGITE
and 39 to RIIT. Both numbers represent a larger pool than in recent years, demonstrating
that the conferences are of great interest in the community. Ninety-five of the submissions
were papers, with 59 papers submitted to SIGITE and 36 papers submitted to RIIT. SIGITE
has 27 papers in its program for an acceptance rate of 46% and RIIT has 14 papers
for an acceptance rate of 39%. All of the authors presenting should be congratulated
on their excellent work.A conference cannot happen without the help of its reviewers,
and this year was no exception. Fiftyfive reviewers worked diligently to ensure that
every paper had at least three independent reviews. It was a significant effort to
produce the 317 reviews that ended up in the system, and we thank the reviewers from
the bottom of our heart. New to the conferences this year was a meta review process,
in which 13 diligent meta reviewers together examined all reviews for each submission
and reconciled those reviews into a coherent message for each author. We hope the
meta review process enabled authors to have more substantive feedback on their work,
whether it appears in the final program or not.The conference runs from Thursday to
Saturday and each day offers something of interest to attendees. On Thursday our keynote
speaker is Dr. Flavio Villanustre, Vice-President of Technology Architecture &amp; Product
for LexisNexis and HPCC Systems. The day continues with a workshop on end-user development
activities and paper sessions for both SIGITE and RIIT. Thursday concludes with a
reception, which we know will be useful for networking with colleagues old and new.
Friday introduces a new presentation format, lightning talks on research in progress,
at the conferences. There are also paper sessions for SIGITE and RIIT, a poster session
in the afternoon and, of course, more opportunities for networking during lunch and
the breaks. Saturday offers a three-hour workshop on process-oriented guided inquiry
learning (POGIL) as well as a panel on mobile computing courses and some excellent
SIGITE papers. We also hope that you stay for the closing session where we will share
our plans for SIGITE/RIIT 2015 in Chicago.We hope you find the conference presentations
interesting and thought-provoking, you reconnect with colleagues you know, you find
new collaborators, and you submit the work that results to SIGITE or RIIT next year.
The excellence you see at SIGITE/RIIT 2014 depends on your energy and effort, and
we thank you for letting us be a part of it.},
  location  = {Atlanta, Georgia, USA},
}

@InProceedings{Coroller2018,
  author    = {Coroller, Stevan and Chabridon, Sophie and Laurent, Maryline and Conan, Denis and Leneutre, Jean},
  booktitle = {Proceedings of the 5th Workshop on Middleware and Applications for the Internet of Things},
  title     = {Position Paper: Towards End-to-End Privacy for Publish/Subscribe Architectures in the Internet of Things},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {35–40},
  publisher = {Association for Computing Machinery},
  series    = {M4IoT'18},
  abstract  = {The Internet of Things paradigm lacks end-to-end privacy solutions to consider its
full adoption in real life scenarios in the near future. The recent enactment of the
EU General Data Protection Regulation (GDPR) indeed emphasises the need for stronger
security and privacy measures for personal data processing and free movement, including
consent management and accountability by the data controller and processor. In this
paper, we suggest an architecture to enforce end-to-end data usage control in Distributed
Event-Based Systems (DEBS), from data producers to consumer services, taking into
account some of the GDPR requirements concerning consent management and data processing
transparency. Our architecture proposal is based on UCONABC usage control models,
which we overlap with a distributed hash table overlay for scalability and fault-tolerance
concerns, and across and within systems data usage control. Our proposal highlights
the benefits of combining both DEBS and end-user usage control architectures. To complete
our approach, we quickly survey existing encryption models that ensure data confidentiality
in topic-based Publish/Subscribe systems and highlight the remaining obstacles to
transpose them to content-based DEBS with an overlay of brokers.},
  doi       = {10.1145/3286719.3286727},
  isbn      = {9781450361187},
  keywords  = {IoT, Privacy, Usage Control, Content-based Distributed Event-Based Systems},
  location  = {Rennes, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3286719.3286727},
}

@InProceedings{Tilmans2016,
  author    = {Tilmans, Olivier and B\"{u}hler, Tobias and Vissicchio, Stefano and Vanbever, Laurent},
  booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
  title     = {Mille-Feuille: Putting ISP Traffic under the Scalpel},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {113–119},
  publisher = {Association for Computing Machinery},
  series    = {HotNets '16},
  abstract  = {For Internet Service Provider (ISP) operators, getting an accurate picture of how
their network behaves is challenging. Given the traffic volumes that their networks
carry and the impossibility to control end-hosts, ISP operators are typically forced
to randomly sample traffic, and rely on aggregated statistics. This provides coarse-grained
visibility, at a time resolution that is far from ideal (seconds or minutes). In this
paper, we present Mille-Feuille, a novel monitoring architecture that provides fine-grained
visibility over ISP traffic. Mille-Feuille schedules activation and deactivation of
traffic-mirroring rules, that are then provisioned network-wide from a central location,
within milliseconds. By doing so, Mille-Feuille combines the scalability of sampling
with the visibility and controllability of traffic mirroring. As a result, it supports
a set of monitoring primitives, ranging from checking key performance indicators (e.g.,
one-way delay) for single destinations to estimating traffic matrices in sub-seconds.
Our preliminary measurements on existing routers confirm that Mille-Feuille is viable
in practice.},
  doi       = {10.1145/3005745.3005762},
  isbn      = {9781450346610},
  location  = {Atlanta, GA, USA},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3005745.3005762},
}

@InProceedings{Singhvi2017,
  author    = {Singhvi, Arjun and Banerjee, Sujata and Harchol, Yotam and Akella, Aditya and Peek, Mark and Rydin, Pontus},
  booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
  title     = {Granular Computing and Network Intensive Applications: Friends or Foes?},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {157–163},
  publisher = {Association for Computing Machinery},
  series    = {HotNets-XVI},
  abstract  = {Computing/infrastructure as a service continues to evolve with bare metal, virtual
machines, containers and now serverless granular computing service offerings. Granular
computing enables developers to decompose their applications into smaller logical
units or functions, and run them on small, low cost and short lived computation containers
without having to worry about setting up servers - hence the term serverless computing.
While serverless environments can be used very cost effectively for large scale parallel
processing data analytics applications, it is less clear if network intensive packet
processing applications can also benefit from these new computing services as they
do not share the same characteristics. This paper examines the architectural constraints
as well as current serverless implementations to develop a position on this topic
and influence the next generation of computing services. We support our position through
measurement and experimentation on Amazon's AWS Lambda service with a few popular
network functions.},
  doi       = {10.1145/3152434.3152450},
  isbn      = {9781450355698},
  location  = {Palo Alto, CA, USA},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3152434.3152450},
}

@InProceedings{Renz2016,
  author    = {Renz, Jan and Hoffmann, Daniel and Staubitz, Thomas and Meinel, Christoph},
  booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
  title     = {Using A/B Testing in MOOC Environments},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {304–313},
  publisher = {Association for Computing Machinery},
  series    = {LAK '16},
  abstract  = {In recent years, Massive Open Online Courses (MOOCs) have become a phenomenon offering
the possibility to teach thousands of participants simultaneously. In the same time
the platforms used to deliver these courses are still in their fledgling stages. While
course content and didactics of those massive courses are the primary key factors
for the success of courses, still a smart platform may increase or decrease the learners
experience and his learning outcome. The paper at hand proposes the usage of an A/B
testing framework that is able to be used within an micro-service architecture to
validate hypotheses about how learners use the platform and to enable data-driven
decisions about new features and settings. To evaluate this framework three new features
(Onboarding Tour, Reminder Mails and a Pinboard Digest) have been identified based
on a user survey. They have been implemented and introduced on two large MOOC platforms
and their influence on the learners behavior have been measured. Finally this paper
proposes a data driven decision workflow for the introduction of new features and
settings on e-learning platforms.},
  doi       = {10.1145/2883851.2883876},
  isbn      = {9781450341905},
  keywords  = {MOOC, A/B testing, e-learning, controlled online tests, microservice},
  location  = {Edinburgh, United Kingdom},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2883851.2883876},
}

@InProceedings{Hatoum2014,
  author    = {Hatoum, Rima and Hatoum, Abbas and Ghaith, Alaa and Pujolle, Guy},
  booktitle = {Proceedings of the 12th ACM International Symposium on Mobility Management and Wireless Access},
  title     = {Qos-Based Joint Resource Allocation with Link Adaptation for SC-FDMA Uplink in Heterogeneous Networks},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {59–66},
  publisher = {Association for Computing Machinery},
  series    = {MobiWac '14},
  abstract  = {The LTE-based femtocell network is a promising solution adopted today to cope with
the huge cellular traffic requirements. In particular, the Uplink communication becomes
an attractive issue especially with the emerging of the interactive services and large
uploaded data volume. Intelligent allocation of the resources and interference management
are the main challenges in such context. In this paper, we propose a linear optimization
model for the SC-FDMA Uplink transmission aiming to adaptively allocate resources
with respect to the link quality. Both power and modulation and coding schemes are
independently assigned to each user over each allocated sub-channel. The cluster architecture
is adopted as a hybrid centralized/distributed network. The user differentiation strategy
ensures the QoS guarantee with respect to a priority level of each user. Taking into
account the specifications of the uplink communication, we confirm through comparative
simulations the outperformance of our proposal considering several metrics such as
throughput satisfaction rate, transmitted power, outage probability, special spectrum
reuse and others.},
  doi       = {10.1145/2642668.2642673},
  isbn      = {9781450330268},
  keywords  = {interference mitigation, uplink, QoS, resource allocation, link adaptation, SC-FDMA-femtocell},
  location  = {Montreal, QC, Canada},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2642668.2642673},
}

@Proceedings{2016,
  title     = {HotSos '16: Proceedings of the Symposium and Bootcamp on the Science of Security},
  year      = {2016},
  address   = {New York, NY, USA},
  isbn      = {9781450342773},
  publisher = {Association for Computing Machinery},
  abstract  = {Science of Security (SoS) emphasizes the advancement of research methods as well as
the development of new research results. This dual focus is intended to improve both
the confidence we gain from scientific results and also the capacity and efficiency
through which we address increasingly challenging technical problems.The HotSoS conferences
have focused on work related to one or more of the five Hard Problems identified by
the Science of Security community:•Scalability and composability in the construction
of secure systems•Policy-governed collaboration in handling data across different
domains of authority for security and privacy protection•Predictive security metrics
to guide choice-making in security engineering and response•Resilient architectures
that can deliver service despite compromised components•Human behavior, modeling users,
operators, and adversaries to support improved design and analysisA second and equally
major focus of the conferences is on the advancement of scientific methods, including
data gathering and analysis, experimental methods, and mathematical models for modeling
and reasoning. This includes the exploration of interactions among these methods to
enhance validity.},
  location  = {Pittsburgh, Pennsylvania},
}

@InProceedings{Hoffman2015,
  author    = {Hoffman, Mark E.},
  booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
  title     = {Student Board-Writing to Integrate Communication Skills and Content to Enhance Student Learning (Abstract Only)},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {684},
  publisher = {Association for Computing Machinery},
  series    = {SIGCSE '15},
  abstract  = {Students frequently use a whiteboard to individually demonstrate understanding or
interactively develop understanding in groups. The practice is employed to develop
content knowledge; however, an opportunity to intentionally develop communication
skills is overlooked. On the other hand, instructors carefully integrate instructional
organization and communication to maximize student content learning. Taken together,
this presents an opportunity for students to intentionally improve their communication
skills in the service of content learning. This poster details a "work in progress"
project where students follow organizational guidelines for written homework and board-writing
to facilitate in-class, problem solution presentation. Problem solution presentations
occur during one class period each week. Students are given colored pencils for written
homework and colored markers for board-writing. Student work including written homework
and board-writing was gathered from the 2013 and 2014 iterations of a sophomore-level
computer architecture course. Preliminary analysis of student work shows that students
either adopt the guidelines from the start or learn to use them through feedback and
practice. On the semester-end survey, students report that adopting guidelines for
written homework, board-writing, and color scheme improve presentation, and board-writing
improves student learning. Future work includes gathering data from more students
including recorded student presentations, developing quantitative scores to analyze
student work, and developing measures of student learning.},
  doi       = {10.1145/2676723.2691890},
  isbn      = {9781450329668},
  keywords  = {student board-writing, content learning, communication skills},
  location  = {Kansas City, Missouri, USA},
  numpages  = {1},
  url       = {https://doi.org/10.1145/2676723.2691890},
}

@InProceedings{Rauter2016,
  author    = {Rauter, Tobias and H\"{o}ller, Andrea and Iber, Johannes and Kreiner, Christian},
  booktitle = {Proceedings of the 2016 International Conference on Embedded Wireless Systems and Networks},
  title     = {Thingtegrity: A Scalable Trusted Computing Architecture for the Internet of Things},
  year      = {2016},
  address   = {USA},
  pages     = {23–34},
  publisher = {Junction Publishing},
  series    = {EWSN '16},
  abstract  = {Remote attestation is used to prove the integrity of one system (prover) to another
(challenger). The prover measures its configuration and transmits the result to the
challenger for verification. Common attestation methods lead to complex configuration
measurements (e.g., hash of all executables), which are updated every time one of
the software modules changes. The updated configuration has to be distributed to all
possible challengers since they need a reference to enable the verification. Recently,
an idea of reducing the complexity of the configuration measurement by taking into
account privileges of software modules has been presented. However, this approach
has not been exhaustively analyzed since, as yet, no implementation exists. Especially
in the Internet of Things (IoT) domain, where resources are constrained strictly while
devices are potentially physically exposed to adversaries, attestation methodologies
with reduced overhead are desireable. In this work we combine binary-, property- and
privilege-based remote attestation to integrate a trusted computing architecture transparently
into iotivity, an existing IoT middleware. As a first step, we aim to enable to attestation
of the integrity of complex devices with different services to constrained devices.
With the help of an illustrative simulated environment, we show that our architecture
reduces the effort of bootstrapping trusted relations, as well as updating single
modules in the whole system, even if software and devices from different vendors are
combined.},
  isbn      = {9780994988607},
  location  = {Graz, Austria},
  numpages  = {12},
}

@Article{Lao2020,
  author     = {Lao, Laphou and Li, Zecheng and Hou, Songlin and Xiao, Bin and Guo, Songtao and Yang, Yuanyuan},
  journal    = {ACM Comput. Surv.},
  title      = {A Survey of IoT Applications in Blockchain Systems: Architecture, Consensus, and Traffic Modeling},
  year       = {2020},
  issn       = {0360-0300},
  month      = feb,
  number     = {1},
  volume     = {53},
  abstract   = {Blockchain technology can be extensively applied in diverse services, including online
micro-payments, supply chain tracking, digital forensics, health-care record sharing,
and insurance payments. Extending the technology to the Internet of things (IoT),
we can obtain a verifiable and traceable IoT network. Emerging research in IoT applications
exploits blockchain technology to record transaction data, optimize current system
performance, or construct next-generation systems, which can provide additional security,
automatic transaction management, decentralized platforms, offline-to-online data
verification, and so on. In this article, we conduct a systematic survey of the key
components of IoT blockchain and examine a number of popular blockchain applications.In
particular, we first give an architecture overview of popular IoT-blockchain systems
by analyzing their network structures and protocols. Then, we discuss variant consensus
protocols for IoT blockchains, and make comparisons among different consensus algorithms.
Finally, we analyze the traffic model for P2P and blockchain systems and provide several
metrics. We also provide a suitable traffic model for IoT-blockchain systems to illustrate
network traffic distribution.},
  address    = {New York, NY, USA},
  articleno  = {18},
  doi        = {10.1145/3372136},
  issue_date = {May 2020},
  keywords   = {Blockchain, architecture, traffic modeling, IoT, consensus},
  numpages   = {32},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3372136},
}

@InProceedings{Ghaith2014,
  author    = {Ghaith, Shadi and Wang, Miao and Perry, Philip and Murphy, Liam},
  booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
  title     = {Software Contention Aware Queueing Network Model of Three-Tier Web Systems},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {273–276},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '14},
  abstract  = {Using modelling to predict the performance characteristics of software applications
typically uses Queueing Network Models representing the various system hardware resources.
Leaving out the software resources, such as the limited number of threads, in such
models leads to a reduced prediction accuracy. Accounting for Software Contention
is a challenging task as existing techniques to model software components are complex
and require deep knowledge of the software architecture. Furthermore, they also require
complex measurement processes to obtain the model's service demands. In addition,
solving the resultant model usually require simulation solvers which are often time
consuming.In this work, we aim to provide a simpler model for three-tier web software
systems which accounts for Software Contention that can be solved by time efficient
analytical solvers. We achieve this by expanding the existing "Two-Level Iterative
Queuing Modelling of Software Contention" method to handle the number of threads at
the Application Server tier and the number of Data Sources at the Database Server
tier. This is done in a generic manner to allow for extending the solution to other
software components like memory and critical sections. Initial results show that our
technique clearly outperforms existing techniques.},
  doi       = {10.1145/2568088.2576760},
  isbn      = {9781450327336},
  keywords  = {performance prediction, performance models, web applications, software contention},
  location  = {Dublin, Ireland},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2568088.2576760},
}

@InProceedings{Zimmermann2015,
  author    = {Zimmermann, Olaf},
  booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
  title     = {Metrics for Architectural Synthesis and Evaluation: Requirements and Compilation by Viewpoint: An Industrial Experience Report},
  year      = {2015},
  pages     = {8–14},
  publisher = {IEEE Press},
  series    = {SAM '15},
  abstract  = {During architectural analysis and synthesis, architectural metrics are established
tacitly or explicitly. In architectural evaluation, these metrics are then consulted
to assess whether architectures are fit for purpose and in line with recommended practices
and published architectural knowledge. This experience report presents a personal
retrospective of the author's use of architectural metrics during 20 years in IT architect
roles in professional services as well as research and development. This reflection
drives the identification of use cases, critical success factors and elements of risk
for architectural metrics management. An initial catalog of architectural metrics
is compiled next, which is organized by viewpoints and domains. The report concludes
with a discussion of practical impact of architectural metrics and potential research
topics in this area.},
  keywords  = {patterns, viewpoints, architectural metrics, architectural metrics management, enterprise information systems, architectural reviews, integration},
  location  = {Florence, Italy},
  numpages  = {7},
}

@InProceedings{Psilias2020,
  author    = {Psilias, Dimitrios and Milidonis, Athanasios and Voyiatzis, Ioannis},
  booktitle = {24th Pan-Hellenic Conference on Informatics},
  title     = {Architecture for Secure UAV Systems},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {99–102},
  publisher = {Association for Computing Machinery},
  series    = {PCI 2020},
  abstract  = {UAV applications are providing an extended range of services in society's needs. These
applications require high execution speed and security to all transmitted data. In
this paper an architecture is proposed for secure UAV applications. The architecture
consists of a microcontroller to execute the flight controller tasks and a FPGA for
implementing the security related tasks. The microcontroller is an Arduino which is
widely used in UAVs. Arduino communicates with all sensors and generates outputs needed
for controlling the UAV's motors. The circuit inside the FPGA encrypts/decrypts data
related to transmission. Measurements taken concerning the execution time and power
consumption, reveal the benefits of the extra hardware added for encryption/decryption
in comparison with those of a single microcontroller.},
  doi       = {10.1145/3437120.3437284},
  isbn      = {9781450388979},
  location  = {Athens, Greece},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3437120.3437284},
}

@InProceedings{Yu2016,
  author    = {Yu, Der-Yeuan and Ranganathan, Aanjhan and Masti, Ramya Jayaram and Soriente, Claudio and Capkun, Srdjan},
  booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking},
  title     = {SALVE: Server Authentication with Location Verification},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {401–414},
  publisher = {Association for Computing Machinery},
  series    = {MobiCom '16},
  abstract  = {The Location Service (LCS) proposed by the telecommunication industry is an architecture
that allows the location of mobile devices to be accessed in various applications.
We explore the use of LCS in location-enhanced server authentication, which traditionally
relies on certificates. Given recent incidents involving certificate authorities,
various techniques to strengthen server authentication were proposed. They focus on
improving the certificate validation process, such as pinning, revocation, or multi-path
probing. In this paper, we propose using the server's geographic location as a second
factor of its authenticity. Our solution, SALVE, achieves location-based server authentication
by using secure DNS resolution and by leveraging LCS for location measurements. We
develop a TLS extension that enables the client to verify the server's location in
addition to its certificate. Successful server authentication therefore requires a
valid certificate and the server's presence at a legitimate geographic location, e.g.,
on the premises of a data center. SALVE prevents server impersonation by remote adversaries
with mis-issued certificates or stolen private keys of the legitimate server. We develop
a prototype implementation and our evaluation in real-world settings shows that it
incurs minimal impact to the average server throughput. Our solution is backward compatible
and can be integrated with existing approaches for improving server authentication
in TLS.},
  doi       = {10.1145/2973750.2973766},
  isbn      = {9781450342261},
  keywords  = {location service, TLS, server authentication, location-based authentication},
  location  = {New York City, New York},
  numpages  = {14},
  url       = {https://doi.org/10.1145/2973750.2973766},
}

@InProceedings{Weiss2014,
  author    = {Weiss, Patrick and Heldmann, Marcus and Gabrecht, Alexander and Schweikard, Achim and M\"{u}nte, Thomas M. and Maehle, Erik},
  booktitle = {Proceedings of the 8th International Conference on Pervasive Computing Technologies for Healthcare},
  title     = {A Low Cost Tele-Rehabilitation Device for Training of Wrist and Finger Functions after Stroke},
  year      = {2014},
  address   = {Brussels, BEL},
  pages     = {422–425},
  publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
  series    = {PervasiveHealth '14},
  abstract  = {There is a need for robotic rehabilitation devices that improve the outcome while
reducing the cost of therapy. This paper presents a device for training of supination/pronation,
dorsal wrist extension, and finger manipulation after stroke. The system exhibits
modularity in terms of the communication architecture and different optional components.
User interfaces (UI) can be implemented on different kinds of devices including a
Rasperry Pi single-board computer on which a Qt-based graphical UI was run in this
instance. Tele-rehabilitation functionality is included using SSL-encrypted RESTful
web services on a three-tier architecture. Expensive sensors were omitted in order
to have a cost-effective system which is a requirement for home-based rehabilitation.
The current-based torque sensing is evaluated by comparing current measurements to
force-torque sensor values. After canceling out the static friction, the low error
justified the omission of an additional sensor.},
  doi       = {10.4108/icst.pervasivehealth.2014.255331},
  isbn      = {9781631900112},
  keywords  = {robotic rehabilitation, stroke, tele-rehabilitation, wrist and finger functions, home health care},
  location  = {Oldenburg, Germany},
  numpages  = {4},
  url       = {https://doi.org/10.4108/icst.pervasivehealth.2014.255331},
}

@InProceedings{Seabra2019,
  author    = {Seabra, Matheus and Naz\'{a}rio, Marcos Felipe and Pinto, Gustavo},
  booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
  title     = {REST or GraphQL? A Performance Comparative Study},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {123–132},
  publisher = {Association for Computing Machinery},
  series    = {SBCARS '19},
  abstract  = {Given the variety of architectural models that can be used, a frequent questioning
among software development practitioners is: which architectural model to use? To
respond this question regarding performance issues, three target applications have
been studied, each written using two models web services architectures: REST and GraphQL.
Through research of performance metrics of response time and the average transfer
rate between the requests, it was possible to deduce the particularities of each architectural
model in terms of performance metrics. It was observed that migrating to GraphQL.
resulted in an increase in performance in two-thirds of the tested applications, with
respect to average number of requests per second and transfer rate of data. However,
it was noticed that services after migration for GraphQL performed below its REST
counterpart for workloads above 3000 requests, ranging from 98 to 2159 Kbytes per
second after the migration study. On the other hand, for more trivial workloads, services
on both REST and GraphQL architectures presented similar performances, where values
between REST and GraphQL services ranged from 6.34 to 7.68 requests per second for
workloads of 100 requests.},
  doi       = {10.1145/3357141.3357149},
  isbn      = {9781450376372},
  keywords  = {Modelo arquitetural, REST, Teste de desempenho, GraphQL},
  location  = {Salvador, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3357141.3357149},
}

@InProceedings{Zhang2015b,
  author    = {Zhang, Cong and Liu, Jiangchuan},
  booktitle = {Proceedings of the 25th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
  title     = {On Crowdsourced Interactive Live Streaming: A Twitch.Tv-Based Measurement Study},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {55–60},
  publisher = {Association for Computing Machinery},
  series    = {NOSSDAV '15},
  abstract  = {Empowered by today's rich tools for media generation and collaborative production,
the multimedia service paradigm is shifting from the conventional single source, to
multi-source, to many sources, and now toward crowdsource. Such crowdsourced live
streaming platforms as Twitch.tv allow general users to broadcast their content to
massive viewers, thereby greatly expanding the content and user bases. The resources
available for these non-professional broadcasters however are limited and unstable,
which potentially impair the streaming quality and viewers' experience. The diverse
live interactions among the broadcasters and viewers can further aggravate the problem.In
this paper, we present an initial investigation on the modern crowdsourced live streaming
systems. Taking Twitch as a representative, we outline their inside architecture using
both crawled data and captured traffic of local broadcasters/viewers. Closely examining
the access data collected in a two-month period, we reveal that the view patterns
are determined by both events and broadcasters' sources. Our measurements explore
the unique source- and event-driven views, showing that the current delay strategy
on the viewer's side substantially impacts the viewers' interactive experience, and
there is significant disparity between the long broadcast latency and the short live
messaging latency. On the broadcaster's side, the dynamic uploading capacity is a
critical challenge, which noticeably affects the smoothness of live streaming for
viewers.},
  doi       = {10.1145/2736084.2736091},
  isbn      = {9781450333528},
  keywords  = {view statistics, crowdsourced live streaming, interactive latency, Twitch.tv},
  location  = {Portland, Oregon},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2736084.2736091},
}

@InProceedings{Scully2015,
  author    = {Scully, Timothy and Dobo\v{s}, Jozef and Sturm, Timo and Jung, Yvonne},
  booktitle = {Proceedings of the 20th International Conference on 3D Web Technology},
  title     = {3drepo.Io: Building the next Generation Web3D Repository with AngularJS and X3DOM},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {235–243},
  publisher = {Association for Computing Machinery},
  series    = {Web3D '15},
  abstract  = {This paper presents a novel open source web-based 3D version control system positioned
directly within the context of the recent strategic plan for digitising the construction
sector in the United Kingdom. The aim is to achieve reduction of cost and carbon emissions
in the built environment by up to 20% simply by properly managing digital information
and 3D models. Even though previous works in the field concentrated mainly on defining
novel WebGL frameworks and later on the efficiency of 3D data delivery over the Internet,
there is still the emerging need for a practical solution that would provide ubiquitous
access to 3D assets, whether it is for large international enterprises or individual
members of the general public. We have, therefore, developed a novel platform leveraging
the latest open web-based technologies such as AngularJS and X3DOM in order to define
an industrial-strength collaborative cloud hosting service 3drepo.io. Firstly, we
introduce the work and outline the high-level system architecture as well as improvements
in relation to previous work. Next, we describe database and front-end considerations
with emphasis on scalability and enhanced security. Finally, we present several performance
measurement experiments and a selection of real-life industrial use cases. We conclude
that jQuery provides performance benefits over AngularJS when manipulating large scene
graphs in web browsers.},
  doi       = {10.1145/2775292.2775312},
  isbn      = {9781450336475},
  keywords  = {version control, X3DOM, 3D repo, AngularJS, BIM},
  location  = {Heraklion, Crete, Greece},
  numpages  = {9},
  url       = {https://doi.org/10.1145/2775292.2775312},
}

@InProceedings{Khooi2020,
  author    = {Khooi, Xin Zhe and Csikor, Levente and Kang, Min Suk and Divakaran, Dinil Mon},
  booktitle = {Proceedings of the SIGCOMM '20 Poster and Demo Sessions},
  title     = {In-Network Defense against AR-DDoS Attacks},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {18–20},
  publisher = {Association for Computing Machinery},
  series    = {SIGCOMM '20},
  abstract  = {The prevalence of the disruptive amplified reflection DDoS (AR-DDoS) attacks is one
of the biggest concerns of all network operators today. The increasing magnitude of
new attacks are rendering existing measures (e.g., scrubbing services) inefficient.
This work demonstrates DIDA, an efficient, topology independent, in-line AR-DDoS detection
and mitigation architecture that operates entirely in the data plane.},
  doi       = {10.1145/3405837.3411375},
  isbn      = {9781450380485},
  keywords  = {detection and mitigation, programmable switches, denial-of-service attacks, amplification attacks, in-network, reflection attacks},
  location  = {Virtual event},
  numpages  = {3},
  url       = {https://doi.org/10.1145/3405837.3411375},
}

@InProceedings{Jun2017,
  author    = {Jun, Tae Joon and Yoo, Myong Hwan and Kim, Daeyoung and Cho, Kyu Tae and Lee, Seung Young and Yeun, Kyuoke},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
  title     = {HPC Supported Mission-Critical Cloud Architecture},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {223–232},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '17},
  abstract  = {Tactical Operations Center (TOC) system in military field is an advanced computer
system composed of multiple servers and desktops to interlock internal/external weapon
systems processing mission-critical applications in combat situation. However, the
current TOC system has several limitations such as difficulty of integrating tactical
weapon systems including missile launch system and radar system into the single TOC
system due to the heterogeneity of HW and SW between systems, and an inefficient computing
resource management for the weapon systems.In this paper, we proposed a novel HPC
supported mission-critical Cloud architecture as TOC for Surface-to-Air-Missile (SAM)
system with OpenStack Cloud OS, Data Distribution Service (DDS), and GPU virtualization
techniques. With this approach, our system provides elastic resource management over
the weapon systems with virtual machines, integration of heterogeneous systems with
different kinds of guest OS, real-time, reliable, and high-speed communication between
the virtual machines and virtualized GPU resource over the virtual machines. Evaluation
of our TOC system includes DDS performance measurement over 10Gbps Ethernet and QDR
InfiniBand networks on the virtualized environment with OpenStack Cloud OS, and GPU
virtualization performance evaluation with two different methods, PCI pass-through
and remote-API. With the evaluation results, we conclude that our system provides
reasonable performance in the combat situation compared to the previous TOC system
while additionally supports scalable and elastic use of computing resource through
the virtual machines.},
  doi       = {10.1145/3030207.3044531},
  isbn      = {9781450344043},
  keywords  = {cloud computing, tactical operations center, data distribution service, gpgpu},
  location  = {L'Aquila, Italy},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3030207.3044531},
}

@InProceedings{Vidas2014,
  author    = {Vidas, Timothy and Tan, Jiaqi and Nahata, Jay and Tan, Chaur Lih and Christin, Nicolas and Tague, Patrick},
  booktitle = {Proceedings of the 4th ACM Workshop on Security and Privacy in Smartphones &amp; Mobile Devices},
  title     = {A5: Automated Analysis of Adversarial Android Applications},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {39–50},
  publisher = {Association for Computing Machinery},
  series    = {SPSM '14},
  abstract  = {Mobile malware is growing - both in overall volume and in number of existing variants
- at a pace rapid enough that systematic manual, human analysis is becoming increasingly
difficult. As a result, there is a pressing need for techniques and tools that provide
automated analysis of mobile malware samples. We present A5, an open source automated
system to process Android malware. A5 is a hybrid system combining static and dynamic
malware analysis techniques. Android's architecture permits many different paths for
malware to react to system events, any of which may result in malicious behavior.
Key innovations in A5 consist of novel methods of interacting with mobile malware
to better coerce malicious behavior, and in combining both virtual and physical pools
of Android platforms to capture behavior that could otherwise be missed. The primary
output of A5 is a set of network threat indicators and intrusion detection system
signatures that can be used to detect and prevent malicious network activity. We detail
A5's distributed design and demonstrate applicability of our interaction techniques
using examples from real malware. Additionally, we compare A5 with other automated
systems and provide performance measurements of an implementation, using a published
dataset of 1,260 unique malware samples, showing that A5 can quickly process large
amounts of malware. We provide a public web interface to our implementation of A5
that allows third parties to use A5 as a web service.},
  doi       = {10.1145/2666620.2666630},
  isbn      = {9781450331555},
  keywords  = {dynamic analysis, mobile malware, sandbox, virtualization, static analysis, malicious behavior},
  location  = {Scottsdale, Arizona, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2666620.2666630},
}
{10.1145/3052973.3053028,
author = {Inci, Mehmet Sinan and Eisenbarth, Thomas and Sunar, Berk},
title = {Hit by the Bus: QoS Degradation Attack on Android},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3053028},
doi = {10.1145/3052973.3053028},
abstract = {Mobile apps need optimal performance and responsiveness to rise amongst numerous rivals
on the market. Further, some apps like media streaming or gaming apps cannot even
function properly with a performance below a certain threshold. In this work, we present
the first performance degradation attack on Android OS that can target rival apps
using a combination of logical channel leakages and low-level architectural bottlenecks
in the underlying hardware. To show the viability of the attack, we design a proof-of-concept
app and test it on various mobile platforms. The attack runs covertly and brings the
target to the level of unresponsiveness. With less than 10% CPU time in the worst
case, it requires minimal computational effort to run as a background service, and
requires only the UsageStats permission from the user. We quantify the impact of our
attack using 11 popular benchmark apps, running 44 different tests.} The measured
QoS degradation varies across platforms and applications, reaching a maximum of 90%
in some cases. The attack combines the leakage from logical channels with low-level
architectural bottlenecks to design a malicious app that can covertly degrade Quality
of Service (QoS) of any targeted app. Furthermore, our attack code has a small footprint
and is not detected by the Android system as malicious. Finally, our app can pass
the Google Play Store malware scanner, Google Bouncer, as well as the top malware
scanners in the Play Store.},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {716–727},
numpages = {12},
keywords = {mobile security, mobile malware, performance degradation, QoS attack},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@InProceedings{BenFakih2015,
  author    = {Ben Fakih, Hichem and Elhossini, Ahmed and Juurlink, Ben},
  booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  title     = {An Efficient and Flexible FPGA Implementation of a Face Detection System (Abstract Only)},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {261},
  publisher = {Association for Computing Machinery},
  series    = {FPGA '15},
  abstract  = {Robust and rapid face detection systems are constantly gaining more interest, since
they represent the first stone for many challenging tasks in the field of computer
vision. In this paper a software-hardware co-design approach is presented, that enables
the detection of frontal faces in real time. A complete hardware implementation of
all components taking part of the face detection is introduced. This work is based
on the object detection framework of Viola and Jones, which makes use of a cascade
of classifiers to reduce the computation time. The proposed architecture is flexible,
as it allows the use of multiple instances of the face detector. This makes developers
free to choose the speed range and reserved resources for this task. The current implementation
runs on the Zynq SoC and receives images over IP network, which allows exposing the
face detection task as a remote service that can be consumed from any device connected
to the network. We performed several measurements for the final detector and the software
equivalent. Using three Evaluator cores, the ZedBoard system achieves a maximal average
frame rate of 13.4 FPS when analysing an image containing 640x480 pixels. This stands
for an improvement of 5.25 times compared to the software solution and represents
acceptable results for most real-time systems. On the ZC706 system, a higher frame
rate of 16.58 FPS is achieved. The proposed hardware solution achieved 92% accuracy,
which is low compared to the software solution (97%) due to different scaling algorithm.
The proposed solution achieved higher frame rate compared to other solutions found
in the literature.},
  doi       = {10.1145/2684746.2689095},
  isbn      = {9781450333153},
  keywords  = {zynq, copmuter visioin, fpga, face detection, viola and jones},
  location  = {Monterey, California, USA},
  numpages  = {1},
  url       = {https://doi.org/10.1145/2684746.2689095},
}

@InProceedings{Chung2017,
  author    = {Chung, Taejoong and van Rijswijk-Deij, Roland and Choffnes, David and Levin, Dave and Maggs, Bruce M. and Mislove, Alan and Wilson, Christo},
  booktitle = {Proceedings of the 2017 Internet Measurement Conference},
  title     = {Understanding the Role of Registrars in DNSSEC Deployment},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {369–383},
  publisher = {Association for Computing Machinery},
  series    = {IMC '17},
  abstract  = {The Domain Name System (DNS) provides a scalable, flexible name resolution service.
Unfortunately, its unauthenticated architecture has become the basis for many security
attacks. To address this, DNS Security Extensions (DNSSEC) were introduced in 1997.
DNSSEC's deployment requires support from the top-level domain (TLD) registries and
registrars, as well as participation by the organization that serves as the DNS operator.
Unfortunately, DNSSEC has seen poor deployment thus far: despite being proposed nearly
two decades ago, only 1% of .com, .net, and .org domains are properly signed.In this
paper, we investigate the underlying reasons why DNSSEC adoption has been remarkably
slow. We focus on registrars, as most TLD registries already support DNSSEC and registrars
often serve as DNS operators for their customers. Our study uses large-scale, longitudinal
DNS measurements to study DNSSEC adoption, coupled with experiences collected by trying
to deploy DNSSEC on domains we purchased from leading domain name registrars and resellers.
Overall, we find that a select few registrars are responsible for the (small) DNSSEC
deployment today, and that many leading registrars do not support DNSSEC at all, or
require customers to take cumbersome steps to deploy DNSSEC. Further frustrating deployment,
many of the mechanisms for conveying DNSSEC information to registrars are error-prone
or present security vulnerabilities. Finally, we find that using DNSSEC with third-party
DNS operators such as Cloudflare requires the domain owner to take a number of steps
that 40% of domain owners do not complete. Having identified several operational challenges
for full DNSSEC deployment, we make recommendations to improve adoption.},
  doi       = {10.1145/3131365.3131373},
  isbn      = {9781450351188},
  keywords  = {registrar, DNS, DNS security extension, public key infrastructure, PKI, DNS operator, DNSSEC},
  location  = {London, United Kingdom},
  numpages  = {15},
  url       = {https://doi.org/10.1145/3131365.3131373},
}

@InProceedings{Efthymiopoulos2016,
  author    = {Efthymiopoulos, Nikolaos and Efthymiopoulou, Maria and Christakidis, Athanasios},
  booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
  title     = {Experimentation on Low Delay and Stable Congestion Control for P2P Video Streaming},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {PCI '16},
  abstract  = {In recent years, a number of research efforts have focused on using peer-to-peer (P2P)
systems in order to provide live streaming (LS) and Video-on-Demand (VoD) services.
Most of them focused on the development of distributed P2P block schedulers for content
exchange among the participating peers and on the architecture of the overlay graph
(P2P overlay) that interconnects the set of these peers. Currently, the effort has
shifted towards the combination of P2P systems with cloud infrastructures. By deploying
monitoring and control architectures they use resources from the cloud in order to
enhance the QoS, thus achieving an attractive trade-off between stability and low
cost operation. However, there is a lack of research effort on the congestion control
layer of these systems while the existing congestion control architectures in use
are not suited for P2P traffic. This paper proposes a P2P congestion control protocol
suitable for LS and VoD that: i) is capable to manage sequential traffic to multiple
network destinations, ii) efficiently exploits the available bandwidth, iii) accurately
measures the idle peers' resources, iv) it avoids network congestion, and v) is friendly
to other TCP generated traffic. Our proposed algorithms and protocol have been implemented,
tested and evaluated through a series of real experiments in the context of STEER
[20].},
  articleno = {48},
  doi       = {10.1145/3003733.3003756},
  isbn      = {9781450347891},
  keywords  = {video streaming, P2P, congestion control},
  location  = {Patras, Greece},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3003733.3003756},
}

@InProceedings{Zibitsker2020,
  author    = {Zibitsker, Boris and Lupersolsky, Alex},
  booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
  title     = {How to Apply Modeling to Compare Options and Select the Appropriate Cloud Platform},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {16},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '20},
  abstract  = {Organizations want to take advantage of the flexibility and scalability of Cloud platforms.
By migrating to the Cloud, they hope to develop and implement new applications faster
with lower cost. Amazon AWS, Microsoft Azure, Google, IBM, Oracle and others Cloud
providers support different DBMS like Snowflake, Redshift, Teradata Vantage, and others.
These platforms have different architectures, mechanisms of allocation and management
of resources, and levels of sophistication of DBMS optimizers which affect performance,
scalability and cost. As a result, the response time, CPU Service Time and the number
of I/Os for the same query, accessing the similar table in the Cloud could be significantly
different than On Prem. In order to select the appropriate Cloud platform as a first
step we perform a Workload Characterization for On Prem Data Warehouse. Each Data
Warehouse workload represents a specific line of business and includes activity of
many users generating concurrently simple and complex queries accessing data from
different tables. Each workload has different demands for resources and different
Response Time and Throughput Service Level Goals. In this presentation we will review
results of the workload characterization for an On Prem Data Warehouse environment.
During the second step we collected measurement data for standard TPC-DS benchmark
tests performed in AWS Vantage, Redshift and Snowflake Cloud platform for different
sizes of the data sets and different number of concurrent users. During the third
step we used the results of the workload characterization and measurement data collected
during the benchmark to modify BEZNext On Prem Closed Queueing model to model individual
Clouds. And finally, during the fourth step we used our Model to take into consideration
differences in concurrency, priorities and resource allocation to different workloads.
BEZNext optimization algorithms incorporating Graduate search mechanism are used to
find the AWS instance type and minimum number of instances which will be required
to meet SLGs for each of the workloads. Publicly available information about the cost
of the different AWS instances is used to predict the cost of supporting workloads
in the Cloud month by month during next 12 months.},
  doi       = {10.1145/3375555.3384938},
  isbn      = {9781450371094},
  keywords  = {seasonality determination, service level goals, workload forecasting, benchmarking, workload characterization, cloud platform, optimization., modeling},
  location  = {Edmonton AB, Canada},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3375555.3384938},
}

@InProceedings{Celma2016,
  author    = {Celma, Oscar},
  booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
  title     = {The Exploit-Explore Dilemma in Music Recommendation},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {377},
  publisher = {Association for Computing Machinery},
  series    = {RecSys '16},
  abstract  = {Were The Rolling Stones right when they said, "You can't always get what you want;
but if you try sometime you get what you need"? Recommendation systems are the crystal
ball of the Internet: predicting user intentions, making sense of big data, and delivering
what people are looking for before they even know they want it. Pandora radio is best
known for the Music Genome Project; the most unique and richly labeled music catalog
of 1.5 million+ tracks. While this content-based approach to music recommendation
is extremely effective and still used today as the foundation to the leading online
radio service, Pandora has also collected more than a decade of contextual listener
feedback in the form of more than 65 billion thumbs from 79M+ monthly active users
who have created more than 9 billion stations. This session will look at how the interdisciplinary
team at Pandora goes about making sense of these massive data sets to successfully
make large scale music recommendations to our listeners.As opposed to more traditional
recommender systems which need only to recommend a single item or set of items, Pandora's
recommenders must provide an evolving set of sequential items, which constantly keep
the experience new and exciting. In this talk I will present a dynamic ensemble learning
system that combines musicological data and machine learning models to provide a truly
personalized experience. This approach allows us to switch from a lean back experience
(exploitation) to a more exploration mode to discover new music tailored specifically
to users individual tastes. To exemplify this, I will present a recently launched
product led by the research team, Thumbprint Radio.Following this session the audience
will have an in-depth understanding of how Pandora uses science to determine the perfect
balance of familiarity, discovery, repetition and relevance for each individual listener,
measures and evaluates user satisfaction, and how our online and offline architecture
stack plays a critical role in our success.},
  doi       = {10.1145/2959100.2959122},
  isbn      = {9781450340359},
  keywords  = {machine listening, ensemble learning, content-based recommendation, thumbprint radio, A/B online testing, offline evaluation, exploit-explore dilemma},
  location  = {Boston, Massachusetts, USA},
  numpages  = {1},
  url       = {https://doi.org/10.1145/2959100.2959122},
}

@InProceedings{Marquez2018,
  author    = {Marquez, Cristina and Gramaglia, Marco and Fiore, Marco and Banchs, Albert and Costa-Perez, Xavier},
  booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
  title     = {How Should I Slice My Network? A Multi-Service Empirical Evaluation of Resource Sharing Efficiency},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {191–206},
  publisher = {Association for Computing Machinery},
  series    = {MobiCom '18},
  abstract  = {By providing especially tailored instances of a virtual network,network slicing allows
for a strong specialization of the offered services on the same shared infrastructure.
Network slicing has profound implications on resource management, as it entails an
inherent trade-off between: (i) the need for fully dedicated resources to support
service customization, and (ii) the dynamic resource sharing among services to increase
resource efficiency and cost-effectiveness of the system. In this paper, we provide
a first investigation of this trade-off via an empirical study of resource management
efficiency in network slicing. Building on substantial measurement data collected
in an operational mobile network (i) we quantify the efficiency gap introduced by
non-reconfigurable allocation strategies of different kinds of resources, from radio
access to the core of the network, and (ii) we quantify the advantages of their dynamic
orchestration at different timescales. Our results provide insights on the achievable
efficiency of network slicing architectures, their dimensioning, and their interplay
with resource management algorithms.},
  doi       = {10.1145/3241539.3241567},
  isbn      = {9781450359030},
  keywords  = {resource management, network efficiency, network slicing},
  location  = {New Delhi, India},
  numpages  = {16},
  url       = {https://doi.org/10.1145/3241539.3241567},
}

@InProceedings{Seneviratne2014,
  author    = {Seneviratne, Janaka and Parampalli, Udaya and Kulik, Lars},
  booktitle = {Proceedings of the Twelfth Australasian Information Security Conference - Volume 149},
  title     = {An Authorised Pseudonym System for Privacy Preserving Location Proof Architectures},
  year      = {2014},
  address   = {AUS},
  pages     = {47–56},
  publisher = {Australian Computer Society, Inc.},
  series    = {AISC '14},
  abstract  = {An emerging class of Location Based Services (LBSs) needs verified mobile device locations
for service provision. For example, an automated car park billing system requires
verified locations of cars to confirm the place and the duration of parked cars. Location
Proof Architectures (LPAs) allow a user (or a device on behalf of its user) to obtain
a proof of its presence at a location from a trusted third party. A major concern
in LPAs is to preserve user location privacy. To achieve this a user's identity and
location data should be maintained separately with additional measures that prevent
leaking sensitive identity and location data. In this paper, we present a privacy
preserving LPA in which users appear under pseudonyms. Our main contribution is a
third party free pseudonym registering protocol based on blind signature schemes.
We show that our protocol allows to build a pseudonym system with a guaranteed degree
of privacy agreed at the time of pseudonym registration. We also demonstrate that
a pseudonym can be authenticated across different organizations in an LPA. Our system
ensures that (i) only authenticated users can register their pseudonyms, (ii) the
pseudonyms have a consistent degree of privacy at the point of registration and (iii)
a user cannot take another user's pseudonym.},
  isbn      = {9781921770326},
  keywords  = {pseudonym, location proof architecture, privacy},
  location  = {Auckland, New Zealand},
  numpages  = {10},
}

@InProceedings{Hsieh2014,
  author    = {Hsieh, Chih-Ming and Samie, Farzad and Srouji, M. Sammer and Wang, Manyi and Wang, Zhonglei and Henkel, J\"{o}rg},
  booktitle = {Proceedings of the 2014 International Conference on Hardware/Software Codesign and System Synthesis},
  title     = {Hardware/Software Co-Design for a Wireless Sensor Network Platform},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {CODES '14},
  abstract  = {Wireless sensor networks have become shared resources providing sensing services to
monitor ambient environment. The tasks performed by the sensor nodes and the network
structure are becoming more and more complex so that they cannot be handled efficiently
by traditional sensor nodes any more. The traditional sensor node architecture, which
has software implementation running on a fixed hardware design, is no longer fit to
the changing requirements when new applications with complex computation are added
to this shared infrastructure due to several reasons. First, the operation behavior
changes because of the application requirements and the environmental conditions which
makes a fixed architecture not efficient all the time. Second, to collaborate with
other already deployed sensor networks and to maintain an efficient network structure,
the sensor nodes require flexible communication capabilities. Furthermore, the information
required to determine an efficient hardware/software co-design under the system constraints
cannot be known a priori. Therefore a platform which can adapt to run-time situations
will play an important role in wireless sensor networks. In this paper, we present
a hardware/software co-design framework for a wireless sensor platform, which can
adaptively change its hardware/software configuration to accelerate complex operations
and provides a flexible communication mechanism to deal with complex network structures.
We perform real-world measurements on our prototype to analyze its capabilities. In
addition, our case studies with prototype implementation and network simulations show
the energy savings of the sensor network application by using the proposed design
with run-time adaptivity.},
  articleno = {1},
  doi       = {10.1145/2656075.2656086},
  isbn      = {9781450330510},
  keywords  = {FPGA, sensor networks, multi-radio, reconfiguration, hardware accelerator, low power},
  location  = {New Delhi, India},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2656075.2656086},
}

@InProceedings{Kurose2015,
  author    = {Kurose, James F.},
  booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  title     = {Research Challenges and Opportunities in a Mobility-Centric World},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {290},
  publisher = {Association for Computing Machinery},
  series    = {MobiCom '15},
  abstract  = {The Internet recently passed an historic inflection point, with the number of broadband
mobile devices surpassing the number of wired PCs and servers connected to the Internet.
Mobility now profoundly affects the architecture, services and applications in both
the wireless and wired domains. In this "bottom up" talk, we begin by discussing several
specific mobility-related challenges and recent results in areas including mobility
measurement (including privacy considerations) and modeling, and context-sensitive
services. We then take a broader look at current and future challenges, and conclude
by discussing several NSF investments in programs and projects in area of mobile networking.},
  doi       = {10.1145/2789168.2790089},
  isbn      = {9781450336192},
  keywords  = {mobility, computer networks, architecture, measurement modeling},
  location  = {Paris, France},
  numpages  = {1},
  url       = {https://doi.org/10.1145/2789168.2790089},
}

@InProceedings{Wei2018,
  author    = {Wei, Tianshu and Chen, Xiaoming and Li, Xin and Zhu, Qi},
  booktitle = {Proceedings of the International Conference on Computer-Aided Design},
  title     = {Model-Based and Data-Driven Approaches for Building Automation and Control},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICCAD '18},
  abstract  = {Smart buildings in the future are complex cyber-physical-human systems that involve
close interactions among embedded platform (for sensing, computation, communication
and control), mechanical components, physical environment, building architecture,
and occupant activities. The design and operation of such buildings require a new
set of methodologies and tools that can address these heterogeneous domains in a holistic,
quantitative and automated fashion. In this paper, we will present our design automation
methods for improving building energy efficiency and offering comfortable services
to occupants at low cost. In particular, we will highlight our work in developing
both model-based and data-driven approaches for building automation and control, including
methods for co-scheduling heterogeneous energy demands and supplies, for integrating
intelligent building energy management with grid optimization through a proactive
demand response framework, for optimizing HVAC control with deep reinforcement learning,
and for accurately measuring in-building temperature by combining prior modeling information
with few sensor measurements based upon Bayesian inference.},
  articleno = {26},
  doi       = {10.1145/3240765.3243485},
  isbn      = {9781450359504},
  keywords  = {smart buildings, deep reinforcement learning, model-based design, Bayesian inference, data-driven, model predictive control},
  location  = {San Diego, California},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3240765.3243485},
}

@InProceedings{Antonescu2014,
  author    = {Antonescu, Alexandru-Florian and Braun, Torsten},
  booktitle = {Proceedings of the 2014 ACM SIGCOMM Workshop on Distributed Cloud Computing},
  title     = {Modeling and Simulation of Concurrent Workload Processing in Cloud-Distributed Enterprise Information Systems},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {11–16},
  publisher = {Association for Computing Machinery},
  series    = {DCC '14},
  abstract  = {Cloud Computing enables provisioning and distribution of highly scalable services
in a reliable, on-demand and sustainable manner. Distributed Enterprise Information
Systems (dEIS) are a class of applications with important economic value and with
strong requirements in terms of performance and reliability. In order to validate
dEIS architectures, stability, scaling and SLA compliance, large testing deployments
are necessary, adding complexity to the design and testing of such systems. To fill
this gap, we present and validate a methodology for modeling and simulating such complex
distributed systems using the CloudSim cloud computing simulator, based on measurement
data from an actual distributed system. We present an approach for creating a performance-based
model of a distributed cloud application using recorded service performance traces.
We then show how to integrate the created model into CloudSim. We validate the CloudSim
simulation model by comparing performance traces gathered during distributed concurrent
experiments with simulation results using different VM configurations. We demonstrate
the usefulness of using a cloud simulator for modeling properties of real cloud-distributed
applications.},
  doi       = {10.1145/2627566.2627575},
  isbn      = {9781450329927},
  keywords  = {cloud computing, performance profiling, distributed applications, modelling and simulation},
  location  = {Chicago, Illinois, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2627566.2627575},
}

@InProceedings{Cui2015,
  author    = {Cui, Yong and Lai, Zeqi and Wang, Xin and Dai, Ningwei and Miao, Congcong},
  booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  title     = {QuickSync: Improving Synchronization Efficiency for Mobile Cloud Storage Services},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {592–603},
  publisher = {Association for Computing Machinery},
  series    = {MobiCom '15},
  abstract  = {Mobile cloud storage services have gained phenomenal success in recent few years.
In this paper, we identify, analyze and address the synchronization (sync) inefficiency
problem of modern mobile cloud storage services. Our measurement results demonstrate
that existing commercial sync services fail to make full use of available bandwidth,
and generate a large amount of unnecessary sync traffic in certain circumstance even
though the incremental sync is implemented. These issues are caused by the inherent
limitations of the sync protocol and the distributed architecture. Based on our findings,
we propose QuickSync, a system with three novel techniques to improve the sync efficiency
for mobile cloud storage services, and build the system on two commercial sync services.
Our experimental results using representative workloads show that QuickSync is able
to reduce up to 52.9% sync time in our experiment settings.},
  doi       = {10.1145/2789168.2790094},
  isbn      = {9781450336192},
  keywords  = {performance, mobile cloud storage, measurement},
  location  = {Paris, France},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2789168.2790094},
}

@InProceedings{Orsini2016,
  author    = {Orsini, Chiara and King, Alistair and Giordano, Danilo and Giotsas, Vasileios and Dainotti, Alberto},
  booktitle = {Proceedings of the 2016 Internet Measurement Conference},
  title     = {BGPStream: A Software Framework for Live and Historical BGP Data Analysis},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {429–444},
  publisher = {Association for Computing Machinery},
  series    = {IMC '16},
  abstract  = {We present BGPStream, an open-source software framework for the analysis of both historical
and real-time Border Gateway Protocol (BGP) measurement data. Although BGP is a crucial
operational component of the Internet infrastructure, and is the subject of research
in the areas of Internet performance, security, topology, protocols, economics, etc.,
there is no efficient way of processing large amounts of distributed and/or live BGP
measurement data. BGPStream fills this gap, enabling efficient investigation of events,
rapid prototyping, and building complex tools and large-scale monitoring applications
(e.g., detection of connectivity disruptions or BGP hijacking attacks). We discuss
the goals and architecture of BGPStream. We apply the components of the framework
to different scenarios, and we describe the development and deployment of complex
services for global Internet monitoring that we built on top of it.},
  doi       = {10.1145/2987443.2987482},
  isbn      = {9781450345262},
  keywords  = {bgp measurement, network monitoring, internet measurement, bgp monitoring, internet routing, network measurement, real-time monitoring},
  location  = {Santa Monica, California, USA},
  numpages  = {16},
  url       = {https://doi.org/10.1145/2987443.2987482},
}

@InProceedings{Xiao2019,
  author    = {Xiao, Ao and Liu, Yunhao and Li, Yang and Qian, Feng and Li, Zhenhua and Bai, Sen and Liu, Yao and Xu, Tianyin and Xin, Xianlong},
  booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
  title     = {An In-Depth Study of Commercial MVNO: Measurement and Optimization},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {457–468},
  publisher = {Association for Computing Machinery},
  series    = {MobiSys '19},
  abstract  = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs),
which operate on top of the existing cellular infrastructures of base carriers while
offering cheaper or more flexible data plans compared to those of the base carriers.
In this paper, we present a nearly two-year measurement study towards understanding
various key aspects of today's MVNO ecosystem, including its architecture, performance,
economics, customers, and the complex interplay with the base carrier. Our study focuses
on a large commercial MVNO with reviseabout 1 million customers, operating atop a
nation-wide base carrier. Our measurements clarify several key concerns raised by
MVNO customers, such as inaccurate billing and potential performance discrimination
with the base carrier. We also leverage big data analytics and machine learning to
optimize an MVNO's key businesses such as data plan reselling and customer churn mitigation.
Our proposed techniques can help achieve %will lead to higher revenues and improved
services for commercial MVNOs.},
  doi       = {10.1145/3307334.3326070},
  isbn      = {9781450366618},
  keywords  = {mvno, network performance, machine learning, churn mitigation, data prediction},
  location  = {Seoul, Republic of Korea},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3307334.3326070},
}

@Article{Li2020a,
  author     = {Li, Yang and Zheng, Jianwei and Li, Zhenhua and Liu, Yunhao and Qian, Feng and Bai, Sen and Liu, Yao and Xin, Xianlong},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {Understanding the Ecosystem and Addressing the Fundamental Concerns of Commercial MVNO},
  year       = {2020},
  issn       = {1063-6692},
  month      = jun,
  number     = {3},
  pages      = {1364–1377},
  volume     = {28},
  abstract   = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs),
which operate on top of existing cellular infrastructures of base carriers, while
offering cheaper or more flexible data plans compared to those of the base carriers.
In this paper, we present a two-year measurement study towards understanding various
fundamental aspects of today's MVNO ecosystem, including its architecture, customers,
performance, economics, and the complex interplay with the base carrier. Our study
focuses on a large commercial MVNO with one million customers, operating atop a nation-wide
base carrier. Our measurements clarify several key concerns raised by MVNO customers,
such as inaccurate billing and potential performance discrimination with the base
carrier. We also leverage big data analytics, statistical modeling, and machine learning
to address the MVNO's key concerns with regard to data usage prediction, data plan
reselling, customer churn mitigation, and billing delay reduction. Our proposed techniques
can help achieve higher revenues and improved services for commercial MVNOs.},
  doi        = {10.1109/TNET.2020.2981514},
  issue_date = {June 2020},
  numpages   = {14},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2020.2981514},
}

@InProceedings{Buccafurri2019,
  author    = {Buccafurri, Francesco and Musarella, Lorenzo and Nardone, Roberto},
  booktitle = {Proceedings of the 23rd International Database Applications &amp; Engineering Symposium},
  title     = {Enabling Propagation in Web of Trust by Ethereum},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {IDEAS '19},
  abstract  = {Web of Trust offers a way to bind identities with the corresponding public keys. It
relies on a distributed architecture, where each user could play the role of certificate
signer. With the widespread diffusion of social networks, the trust propagation is
a matter of growing interest. This paper proposes an approach enabling the propagation
in Web of Trust by means of Ethereum. The usage of Ethereum eliminates the necessity
of single-organization trusted services, which is, in general, not realistic. Although
the information stored on Ethereum is public, the privacy of users is protected because
trust chains involve only Ethereum addresses and strong measures are implemented to
contrast their malicious de-anonymization. The approach relies on the usage of a smart
contract for storing the status of certificate signatures and to manage revocations.
When a user u wants to trust another user v, the smart contract checks the presence
of trust chains originating from root nodes of u.},
  articleno = {9},
  doi       = {10.1145/3331076.3331108},
  isbn      = {9781450362498},
  keywords  = {trust propagation, blockchain, smart contract, social network, pretty good privacy, Ethereum},
  location  = {Athens, Greece},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3331076.3331108},
}

@InProceedings{Nadgowda2018,
  author    = {Nadgowda, Shripad and Isci, Canturk},
  booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
  title     = {Drishti: Disaggregated and Interoperable Security Analytics Framework for Cloud},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {528},
  publisher = {Association for Computing Machinery},
  series    = {SoCC '18},
  abstract  = {Application and platform security has always been critical for the success of any
business. Traditionally, applications were deployed directly on physical servers.
As a result, there are myriad traditional security solutions that were developed around
this model to run as local agents on the systems they monitor and protect. These solutions
are then refined and standardized with decades of experience. With the emergence of
virtualization, cloud and particularly containerization, use of these solutions is
becoming challenging with consolidation and scale. As we begin to deploy hundreds
of cloud instances on a single node, traditional solutions, designed for local execution
do not scale out. At the same time, the clean separation of a virtual machine (VM)
or a container from the platform itself, and maturing introspection and inspection
APIs provide a simple, practical way to decouple monitored from the monitors [3].
Furthermore, as the scope of cloud security expands from simple monitoring and auditing
to more complex learning based analytics, analytics components are further offloaded
to separate data services, where they can burn extensive cycles, and in some cases
use specialized hardware for security analytics, out of the critical path of the monitored
applications [5]. As a result, traditional agent-based tightly-coupled model is being
replaced by a more dis-aggregated {system, observation, analytics, actions} architecture.To
implement such dis-aggregated model in practice, first system state needs to be transferred
from cloud platform to analytic platform. File system more generally is representative
of the system state that persists features of interest for security analytics like
processes, metrics, configurations, packages across various files. Remote replication
or snapshotting [1] of whole file system is very in-efficient, since only small set
of files are accessed during the analytics. As a result, a new family of cloud-native
security solutions have recently emerged in the field that uses various specialized
data collection techniques[2, 4]. These techniques perform out-of band introspection
of systems to interpret and extract required system features from the file system
to essentially serialize system state into data. This data is then transferred to
an analytic platform for analysis. Unlike the traditional security solutions that
work locally against the system's standard POSIXy file system interfaces, these emerging
security analytics "work from data" on the analytic platform. However since the target
system is now available as "data", existing agent-based security solutions become
incompatible to work against the system. One mitigating solution is to rewrite all
existing solutions, which requires huge amount of resources and effort.In Drishti,
we address this challenge from a fundamentally different perspective. Instead of rewriting
security solutions to work from data, we make the data work for traditional security
applications. We achieve this by developing a pseudo-system interface over systems
data collected from cloud instances. With this approach, existing solutions run unmodified,
as "black box" software over this system interface, as if they were running on the
actual cloud instance. Drishti framework is our realization of this approach. It is
logically the inverse of the first step of cloud-native security analytics that convert
system state into data. With Drishti we transform data back to system on the analytic
platform by orchestrating two file system components. First, a standard native system
interface is re-calibrated over the system data through our new FUSE file system,
confuse or ClOud Native Filesystem in UserSpacE. Second, we mimic the "effect" of
an agent installation via an overlay file system based on the the agent image. Within
the Drishti framework the underlying data looks like a standard POSIX system to each
on-boarded security solution. This allows us to run existing agent-based security
solutions as is, but still decoupled from the actual system. Drishti also provides
a standard and interoperable platform for designing new security analytic solutions.Overall,
Drishti demonstrates a novel, pragmatic and highly-practical approach for bringing
security analytics into the cloud. It enables us to leverage existing solutions built
based on decades of experience by eliminating the need for reinventing the wheel for
cloud.},
  doi       = {10.1145/3267809.3275470},
  isbn      = {9781450360111},
  location  = {Carlsbad, CA, USA},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3267809.3275470},
}

@Article{Xie2020,
  author     = {Xie, Kun and Chen, Yuxiang and Wang, Xin and Xie, Gaogang and Cao, Jiannong and Wen, Jigang},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {Accurate and Fast Recovery of Network Monitoring Data: A GPU Accelerated Matrix Completion},
  year       = {2020},
  issn       = {1063-6692},
  month      = jun,
  number     = {3},
  pages      = {958–971},
  volume     = {28},
  abstract   = {Gaining a full knowledge of end-to-end network performance is important for some advanced
network management and services. Although it becomes increasingly critical, end-to-end
network monitoring usually needs active probing of the path and the overhead will
increase quadratically with the number of network nodes. To reduce the measurement
overhead, matrix completion is proposed recently to predict the end-to-end network
performance among all node pairs by only measuring a small set of paths. Despite its
potential, applying matrix completion to recover the missing data suffers from low
recovery accuracy and long recovery time. To address the issues, we propose MC-GPU
to exploit Graphics Processing Units (GPUs) to enable parallel matrix factorization
for high-speed and highly accurate Matrix Completion. To well exploit the special
architecture features of GPUs for both task independent and data-independent parallel
task execution, we propose several novel techniques: similar OD (origin and destination)
pairs reordering taking advantage of the locality-sensitive hash (LSH) functions,
balanced matrix partition, and parallel matrix completion. We implement the proposed
MC-GPU on the GPU platform and evaluate the performance using real trace data. We
compare the proposed MC-GPU with the state of the art matrix completion algorithms,
and our results demonstrate that MC-GPU can achieve significantly faster speed with
high data recovery accuracy.},
  doi        = {10.1109/TNET.2020.2976129},
  issue_date = {June 2020},
  numpages   = {14},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2020.2976129},
}

@InProceedings{Tzallas2018,
  author    = {Tzallas, Alexandros T. and Katertsidis, Nikolaos and Glykos, Konstantinos and Segkouli, Sofia and Votis, Konstantinos and Tzovaras, Dimitrios and Barru\'{e}, Cristian and Paliokas, Ioannis and Cort\'{e}s, Ulises},
  booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
  title     = {Designing a Gamified Social Platform for People Living with Dementia and Their Live-in Family Caregivers},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {476–481},
  publisher = {Association for Computing Machinery},
  series    = {PETRA '18},
  abstract  = {In the current paper, a social gamified platform for people living with dementia and
their live-in family caregivers, integrating a broader diagnostic approach and interactive
interventions is presented. The CAREGIVERSPRO-MMD (C-MMD) platform constitutes a support
tool for the patient and the informal caregiver - also referred to as the dyad - that
strengthens self-care, and builds community capacity and engagement at the point of
care. The platform is implemented to improve social collaboration, adherence to treatment
guidelines through gamification, recognition of progress indicators and measures to
guide management of patients with dementia, and strategies and tools to improve treatment
interventions and medication adherence. Moreover, particular attention was provided
on guidelines, considerations and user requirements for the design of a User-Centered
Design (UCD) platform. The design of the platform has been based on a deep understanding
of users, tasks and contexts in order to improve platform usability, and provide adaptive
and intuitive User Interfaces with high accessibility. In this paper, the architecture
and services of the C-MMD platform are presented, and specifically the gamification
aspects.},
  doi       = {10.1145/3197768.3201560},
  isbn      = {9781450363907},
  keywords  = {Dementia, Cloud Platform, Interventions, Social Networking, Caregivers, Gamification, Self-management},
  location  = {Corfu, Greece},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3197768.3201560},
}

@Article{Akiki2017,
  author     = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
  journal    = {ACM Trans. Comput.-Hum. Interact.},
  title      = {Visual Simple Transformations: Empowering End-Users to Wire Internet of Things Objects},
  year       = {2017},
  issn       = {1073-0516},
  month      = apr,
  number     = {2},
  volume     = {24},
  abstract   = {Empowering end-users to wire Internet of Things (IoT) objects (things and services)
together would allow them to more easily conceive and realize interesting IoT solutions.
A challenge lies in devising a simple end-user development approach to support the
specification of transformations, which can bridge the mismatch in the data being
exchanged among IoT objects. To tackle this challenge, we present Visual Simple Transformations
(ViSiT) as an approach that allows end-users to use a jigsaw puzzle metaphor for specifying
transformations that are automatically converted into underlying executable workflows.
ViSiT is explained by presenting meta-models and an architecture for implementing
a system of connected IoT objects. A tool is provided for supporting end-users in
visually developing and testing transformations. Another tool is also provided for
allowing software developers to modify, if they wish, a transformation's underlying
implementation. This work was evaluated from a technical perspective by developing
transformations and measuring ViSiT's efficiency and scalability and by constructing
an example application to show ViSiT's practicality. A study was conducted to evaluate
this work from an end-user perspective, and its results showed positive indications
of perceived usability, learnability, and the ability to conceive real-life scenarios
for ViSiT.},
  address    = {New York, NY, USA},
  articleno  = {10},
  doi        = {10.1145/3057857},
  issue_date = {May 2017},
  keywords   = {End-user development, transformations, internet of things},
  numpages   = {43},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3057857},
}

@InProceedings{Walter2017,
  author    = {Walter, J\"{u}rgen and Stier, Christian and Koziolek, Heiko and Kounev, Samuel},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
  title     = {An Expandable Extraction Framework for Architectural Performance Models},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {165–170},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '17 Companion},
  abstract  = {Providing users with Quality of Service (QoS) guarantees and the prevention of performance
problems are challenging tasks for software systems. Architectural performance models
can be applied to explore performance properties of a software system at design time
and run time. At design time, architectural performance models support reasoning on
effects of design decisions. At run time, they enable automatic reconfigurations by
reasoning on the effects of changing user behavior. In this paper, we present a framework
for the extraction of architectural performance models based on monitoring log files
generalizing over the targeted architectural modeling language. Using the presented
framework, the creation of a performance model extraction tool for a specific modeling
formalism requires only the implementation of a key set of object creation routines
specific to the formalism. Our framework integrates them with extraction techniques
that apply to many architectural performance models, e.g., resource demand estimation
techniques. This lowers the effort to implement performance model extraction tools
tremendously through a high level of reuse. We evaluate our framework presenting builders
for the Descartes Modeling Language (DML) and the Palladio Component Model(PCM). For
the extracted models we compare simulation results with measurements receiving accurate
results.},
  doi       = {10.1145/3053600.3053634},
  isbn      = {9781450348997},
  keywords  = {descartes modeling language, palladio component model, automated performance model extraction, builder pattern},
  location  = {L'Aquila, Italy},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3053600.3053634},
}

@Article{Maqsood2016,
  author     = {Maqsood, Tahir and Khalid, Osman and Irfan, Rizwana and Madani, Sajjad A. and Khan, Samee U.},
  journal    = {ACM Comput. Surv.},
  title      = {Scalability Issues in Online Social Networks},
  year       = {2016},
  issn       = {0360-0300},
  month      = sep,
  number     = {2},
  volume     = {49},
  abstract   = {The last decade witnessed a tremendous increase in popularity and usage of social
network services, such as Facebook, Twitter, and YouTube. Moreover, advances in Web
technologies coupled with social networks has enabled users to not only access, but
also generate, content in many forms. The overwhelming amount of produced content
and resulting network traffic gives rise to precarious scalability issues for social
networks, such as handling a large number of users, infrastructure management, internal
network traffic, content dissemination, and data storage. There are few surveys conducted
to explore the different dimensions of social networks, such as security, privacy,
and data acquisition. Most of the surveys focus on privacy or security-related issues
and do not specifically address scalability challenges faced by social networks. In
this survey, we provide a comprehensive study of social networks along with their
significant characteristics and categorize social network architectures into three
broad categories: (a) centralized, (b) decentralized, and (c) hybrid. We also highlight
various scalability issues faced by social network architectures. Finally, a qualitative
comparison of presented architectures is provided, which is based on various scalability
metrics, such as availability, latency, interserver communication, cost of resources,
and energy consumption, just to name a few.},
  address    = {New York, NY, USA},
  articleno  = {40},
  doi        = {10.1145/2968216},
  issue_date = {November 2016},
  keywords   = {social network, Scalability, hybrid social networks, decentralized social networks, centralized social networks},
  numpages   = {42},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2968216},
}

@Article{DeCicco2014,
  author     = {De Cicco, Luca and Mascolo, Saverio},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {An Adaptive Video Streaming Control System: Modeling, Validation, and Performance Evaluation},
  year       = {2014},
  issn       = {1063-6692},
  month      = apr,
  number     = {2},
  pages      = {526–539},
  volume     = {22},
  abstract   = {Adaptive video streaming is a relevant advancement with respect to classic progressive
download streaming a la YouTube. Among the different approaches, the video stream-switching
technique is getting wide acceptance, being adopted by Microsoft, Apple, and popular
video streaming services such as Akamai, Netflix, Hulu, Vudu, and Livestream. In this
paper, we present a model of the automatic video stream-switching employed by one
of these leading video streaming services along with a description of the client-side
communication and control protocol. From the control architecture point of view, the
automatic adaptation is achieved by means of two interacting control loops having
the controllers at the client and the actuators at the server: One loop is the buffer
controller, which aims at steering the client playout buffer to a target length by
regulating the server sending rate; the other one implements the stream-switching
controller and aims at selecting the video level. A detailed validation of the proposed
model has been carried out through experimental measurements in an emulated scenario.},
  doi        = {10.1109/TNET.2013.2253797},
  issue_date = {April 2014},
  keywords   = {adaptive video streaming, modeling, stream-switching, performance evaluation},
  numpages   = {14},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2013.2253797},
}

@Article{Burny2021,
  author     = {Burny, Nicolas and Vanderdonckt, Jean},
  journal    = {Proc. ACM Hum.-Comput. Interact.},
  title      = {UiLab, a Workbench for Conducting and Reproducing Experiments in GUI Visual Design},
  year       = {2021},
  month      = may,
  number     = {EICS},
  volume     = {5},
  abstract   = {With the continuously increasing number and variety of devices, the study of visual
design of their Graphical User Interfaces grows in importance and scope, particularly
for new devices, including smartphones, tablets, and large screens. Conducting a visual
design experiment typically requires defining and building a GUI dataset with different
resolutions for different devices, computing visual design measures for the various
configurations, and analyzing their results. This workflow is very time- and resource-consuming,
therefore limiting its reproducibility. To address this problem, we present UiLab,
a cloud-based workbench that parameterizes the settings for conducting an experiment
on visual design of Graphical User Interfaces, for facilitating the design of such
experiments by automating some workflow stages, and for fostering their reproduction
by automating their deployment. Based on requirements elicited for UiLab, we define
its conceptual model to delineate the borders of services of the software architecture
to support the new workflow. We exemplify it by demonstrating a system walkthrough
and we assess its impact on experiment reproducibility in terms of design and development
time saved with respect to a classical workflow. Finally, we discuss potential benefits
brought by this workbench with respect to reproducing experiments in GUI visual design
and existing shortcomings to initiate future avenues. We publicly release UiLab source
code on a GitHub repository.},
  address    = {New York, NY, USA},
  articleno  = {196},
  doi        = {10.1145/3457143},
  issue_date = {June 2021},
  keywords   = {usability evaluation, visual design, user interface evaluation, aesthetics},
  numpages   = {31},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3457143},
}

@InProceedings{Falkner2016,
  author    = {Falkner, Katrina and Szabo, Claudia and Chiprianov, Vanea},
  booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
  title     = {Model-Driven Performance Prediction of Systems of Systems},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {44},
  publisher = {Association for Computing Machinery},
  series    = {MODELS '16},
  abstract  = {Systems of Systems exhibit characteristics that pose difficulty in modelling and predicting
their overall performance capabilities, including the presence of operational independence,
emergent behaviour, and evolutionary development. When considering Systems of Systems
within the autonomous defence systems context, these aspects become increasingly critical,
as performance constraints are typically driven by hard constraints on space, weight
and power.System execution modelling languages and tools permit early prediction of
the performance of model-driven systems, however the focus to date has been on understanding
the performance of a model rather than determining if it meets performance requirements,
and only subsequently carrying out analysis to reveal the causes of any requirement
violations. Such an analysis is even more difficult when applied to several systems
cooperating to achieve a common goal - a System of Systems (SoS).The successful integration
of systems within a SoS context has been identified as one of the most substantial
challenges facing military systems development [2]. Accordingly, there is a critical
need to understand the non-functional aspects of the SoS (such as quality of service,
power, size, cost and scalable management of communications), and to explore how these
non-functional aspects evolve under new conditions and deployment scenarios. It is
crucial that we develop methodologies for modelling and understanding non-functional
properties early in the development and integration cycle to better inform our understanding
of the impact of emergent behaviour and evolution within the SoS.We propose an integrated
approach to performance prediction of model-driven real time embedded defence systems
and systems of systems [1]. Our architectural prototyping system supports a scenario-driven
experimental platform for evaluating model suitability within a set of deployment
and real-time performance constraints. We present an overview of our performance prediction
system, demonstrating the integration of modelling, execution and performance analysis,
and discuss a case study to illustrate our approach. Our work employs state-of-the-art
model-driven engineering techniques to facilitate SoS performance prediction and analysis
at design time, either before the SoS is built and deployed, or during its lifetime
when required to evolve.Our model-driven performance prediction platform supports
a scenario-driven experimental environment for evaluating a SoS within the context
of a specific deployment (modelling geographical distribution) and integration constraints.
The main contributions of our work are: (a) a modeling methodology that captures diverse
perspectives of the performance modeling of Systems of Systems; (b) a performance
analysis engine that captures metrics associated with these perspectives and (c) a
case study showing the performance evaluaton of a system of systems and its evolution
as a result of the performance analysis. We discuss how our approach to modelling
supports the specific characteristics of an SoS, and illustrate this through a case
study, based on a "Blue Ocean" scenario, demonstrating how we may obtain performance
predictions within a SoS with emergent and evolutionary properties. Within the context
of our environment, we define models for the individual systems within our System
of Systems, defined for representative workload to predict execution costs, i.e. CPU,
memory usage and network usage, within a generic situation. Our modelling environment
supports the generation of executable forms of these models, which may then be executed
above realistic deployment scenarios in order to obtain predictions of System of System
performance.},
  doi       = {10.1145/2976767.2987689},
  isbn      = {9781450343213},
  location  = {Saint-malo, France},
  numpages  = {1},
  url       = {https://doi.org/10.1145/2976767.2987689},
}

@InProceedings{Klugman2018,
  author    = {Klugman, Noah and Dutta, Prabal},
  booktitle = {Proceedings of the First Workshop on Data Acquisition To Analysis},
  title     = {Set and Forget Sensing with Applets on IFTTT},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {23–24},
  publisher = {Association for Computing Machinery},
  series    = {DATA '18},
  abstract  = {Rich data sets can be collected trivially by bootstrapping off mobile phones and cloud
services. We describe an end-to-end system built with IFTTT that requires no code
to collect arrival and departure times from a geographic area on the campus of the
University of California, Berkeley. This system was configured and deployed in less
than one half hour, cost nothing to deploy or run, and functioned without interruption
for seven months, taking 463 measurements of a single participant. Along with providing
the data set, which provides some insight into the working life of a graduate student,
we describe each part of the system architecture and discuss how a model of sensing-as-an-applet
enables data streams with de-facto standardized, high reliability, and close-to-no-barrier
of entry.},
  doi       = {10.1145/3277868.3277880},
  isbn      = {9781450360494},
  keywords  = {sensing at scale, trigger-action programming, IFTTT},
  location  = {Shenzhen, China},
  numpages  = {2},
  url       = {https://doi.org/10.1145/3277868.3277880},
}

@InProceedings{Ruangvanich2018,
  author    = {Ruangvanich, Supparang and Nilsook, Prachyanun},
  booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
  title     = {Personality Learning Analytics System in Intelligent Virtual Learning Environment},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {245–250},
  publisher = {Association for Computing Machinery},
  series    = {ICIT 2018},
  abstract  = {In this paper, the researchers propose a conceptual for system architecture of learning
analytics process in the intelligent learning environment. Within this concept, today's
competitive business environment need for businesses in order to implement the monitor
and analyze the user-generated data on their own and their competitors. The achievement
of competitive advantage is often necessary to listen to and understand what customers
are saying about competitors' products and services. Not only personality analytics
but also the conceptual description can capture an intelligent learning environment,
and it is the analytic tools that are used to improve learning and education. The
researchers also discuss how learning analytics is developed in different fields.
It closely tied to, a series of other fields of study including business intelligence,
web analytics, academic analytics, educational data mining, and action analytics.
The researchers believe that conceptual of personality analytics in the intelligent
learning environment can play an essential role in managing and analyzing personality
and contribute to the concept of personality analytics in the intelligent learning
environment. The results of this research could be summarized as follows: learning
analytics process should be used as measuring and collecting data about learners and
learning with the aim of improving teaching and learning practice through analysis
of the data. By achieving this process, it should collect data to report or analyze
the happening about the learner. Then, instructors monitor learning what is happening
now, while as learning analytics should get what is going to happen in the future
for learners. Finally, instructors take action to feedback learners.},
  doi       = {10.1145/3301551.3301582},
  isbn      = {9781450366298},
  keywords  = {Virtual Learning Environment, Learning Analytics, Intelligent Environment, Personal Analytics, System Architecture},
  location  = {Hong Kong, Hong Kong},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3301551.3301582},
}

@InProceedings{Irish2014,
  author    = {Irish, Andrew T. and Iland, Daniel and Isaacs, Jason T. and Hespanha, Jo\~{a}o P. and Belding, Elizabeth M. and Madhow, Upamanyu},
  booktitle = {Proceedings of the 6th Annual Workshop on Wireless of the Students, by the Students, for the Students},
  title     = {Using Crowdsourced Satellite SNR Measurements for 3D Mapping and Real-Time GNSS Positioning Improvement},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {5–8},
  publisher = {Association for Computing Machinery},
  series    = {S3 '14},
  abstract  = {Geopositioning using Global Navigation Satellite Systems (GNSS), such as the Global
Positioning System (GPS), is inaccurate in urban environments due to frequent non-line-of-sight
(NLOS) signal reception. This poses a major problem for mobile services that benefit
from accurate urban localization, such as navigation, hyperlocal advertising, and
geofencing applications. However, urban NLOS signal reception can be exploited in
two ways. First, one can use satellite signal-to-noise ratio (SNR) measurements crowdsourced
from mobile devices to create 3D environment maps. This is possible because, for example,
the SNR of signals obstructed by buildings is lower on average than that of line-of-sight
(LOS) signals. Second, in a sort of reverse process called Shadow Matching, SNR readings
from a particular device at an instant in time can be compared to 3D maps to provide
real-time localization improvement. In this paper we give a brief overview of how
such a system works and describe a scalable, low-cost, software-only architecture
that implements it.},
  doi       = {10.1145/2645884.2645890},
  isbn      = {9781450330732},
  keywords  = {localization improvement, shadow matching, 3d mapping, crowdsourcing, gnss, gps},
  location  = {Maui, Hawaii, USA},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2645884.2645890},
}

@InProceedings{Kastanakis2018,
  author    = {Kastanakis, Savvas and Sermpezis, Pavlos and Kotronis, Vasileios and Dimitropoulos, Xenofontas},
  booktitle = {Proceedings of the 2018 Workshop on Mobile Edge Communications},
  title     = {CABaRet: Leveraging Recommendation Systems for Mobile Edge Caching},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {19–24},
  publisher = {Association for Computing Machinery},
  series    = {MECOMM'18},
  abstract  = {Joint caching and recommendation has been recently proposed for increasing the efficiency
of mobile edge caching. While previous works assume collaboration between mobile network
operators and content providers (who control the recommendation systems), this might
be challenging in today's economic ecosystem, with existing protocols and architectures.
In this paper, we propose an approach that enables cache-aware recommendations without
requiring a network and content provider collaboration. We leverage information provided
publicly by the recommendation system, and build a system that provides cache-friendly
and high-quality recommendations. We apply our approach to the YouTube service, and
conduct measurements on YouTube video recommendations and experiments with video requests,
to evaluate the potential gains in the cache hit ratio. Finally, we analytically study
the problem of caching optimization under our approach. Our results show that significant
caching gains can be achieved in practice; 8 to 10 times increase in the cache hit
ratio from cache-aware recommendations, and an extra 2 times increase from caching
optimization.},
  doi       = {10.1145/3229556.3229563},
  isbn      = {9781450359061},
  keywords  = {Recommendation Systems, Joint Caching and Recommendation, Mobile Edge Networks},
  location  = {Budapest, Hungary},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3229556.3229563},
}

@InProceedings{Harsh2019,
  author    = {Harsh, Piyush and Ribera Laszkowski, Juan Francisco and Edmonds, Andy and Quang Thanh, Tran and Pauls, Michael and Vlaskovski, Radoslav and Avila-Garc\'{\i}a, Orlando and Pages, Enric and Gort\'{a}zar Bellas, Francisco and Gallego Carrillo, Micael},
  booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
  title     = {Cloud Enablers For Testing Large-Scale Distributed Applications},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {35–42},
  publisher = {Association for Computing Machinery},
  series    = {UCC '19 Companion},
  abstract  = {Testing large-scale distributed systems (also known as testing in the large) is a
challenge that spreads across different technical domains and areas of expertise.
Current methods and tools provide some minimal guarantees in relation to the correctness
of their functional properties and have serious limitations when evaluating their
extra-functional properties in realistic conditions, such as scalability, availability
and performance efficiency. Cloud Testing and more specifically "testing in the cloud''
has arisen to tackle those challenges. In this new paradigm, cloud-based environment
and infrastructure are used to run realistic end-to-end and/or system-level tests,
collect test data and analyse them. In this paper we present a set of cloud-native
services to take from the tester the responsibility of managing the resources and
complementary services required to simulate realistic operational conditions and production
environments. Specifically, they provide cloud testing capabilities such as logs and
measurements collection from both testing jobs and system under test; test data analytics
and visualization; provisioning and operation of additional services and processes
to replicate realistic production ecosystems; support to scalability and diversity
of underlying testing infrastructure; and replication of the operational conditions
of the software under test through its instrumentation. We present the architecture
of the cloud testing solution and the detailed design of each of the services; we
also evaluate their relative contribution to satisfy different needs in the context
of test execution.},
  doi       = {10.1145/3368235.3368838},
  isbn      = {9781450370448},
  keywords  = {reliability, cloud testing, continuous testing, testing, continuous integration, large-scale distributed systems, scalability},
  location  = {Auckland, New Zealand},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3368235.3368838},
}

@InProceedings{Patane2019,
  author    = {Patan\'{e}, Giancarlo M. M. and Valastro, Gianluca C. and Sambo, Yusuf A. and Ozturk, Metin and Hussain, Sajjad and Imran, Muhammad A. and Panno, Daniela},
  booktitle = {Proceedings of the 23rd IEEE/ACM International Symposium on Distributed Simulation and Real Time Applications},
  title     = {Flexible SDN/NFV-Based SON Testbed for 5G Mobile Networks},
  year      = {2019},
  pages     = {79–86},
  publisher = {IEEE Press},
  series    = {DS-RT '19},
  abstract  = {In the next few years, a considerable innovation concerning the design of the future
5G mobile networks will be a concrete step towards enabling effective high throughput
and low latency services. Software Defined Networking (SDN), Network Function Virtualization
(NFV) and Self Organizing Network (SON) are considered the enabling technologies to
achieve these goals. In this paper, assuming a Control-Data Separation Architecture
(CDSA), we propose a flexible SDN/NFV-based SON testbed, for future 5G mobile networks.
The main contribution of our work is to cover the need for a CDSA based testbed, enabling
the investigation of the NG-SON capabilities for practical implementations. We implement
two different testbed setups, a real one and a virtualized one, both based on the
FlexRAN and OpenAirInterface software tools. First, we implement a specific case study,
i.e., the RAN entities activation/deactivation procedures. Next, we carry out time
measurements, concerning the aforementioned procedures, in order to prove proper testbed
functioning. Finally, we validate the C-SON and D-SON capabilities of our testbed,
considering the features of the results.},
  isbn      = {9781728129235},
  keywords  = {OpenAirInterface, NFV, Cloud-RAN, FlexRAN, 5G, SDN},
  location  = {Cosenza, Italy},
  numpages  = {8},
}

@Proceedings{2014b,
  title     = {SIGCOMM '14: Proceedings of the 2014 ACM Conference on SIGCOMM},
  year      = {2014},
  address   = {New York, NY, USA},
  isbn      = {9781450328364},
  publisher = {Association for Computing Machinery},
  abstract  = {Welcome to ACM SIGCOMM 2014!This year's conference continues the SIGCOMM tradition
of being the premier forum for the presentation of research on networking and communications.
The technical program this year features a set of outstanding papers that cover a
wide variety of areas including network architecture, software defined networks, data
center networks, wireless networks, network services, congestion management, security,
privacy, measurement and analysis.This year's call for papers attracted 242 submissions
from all over the world. The 54 member Technical Program Committee along with a selected
group of external experts carefully considered all of the submissions over two rounds
of reviewing including an author feedback period - with a total of 968 detailed reviews
completed. The TPC meeting to select the final program was held at ICSI, Berkeley,
in late April 2014. At the conclusion of the meeting, the committee had assembled
a wonderful program composed of 45 papers, to be presented over three days at the
conference. The quality of submissions was extremely high as reflected in the final
technical program.},
  location  = {Chicago, Illinois, USA},
}

@Article{Bibi2021,
  author     = {Bibi, Iram and Akhunzada, Adnan and Malik, Jahanzaib and Khan, Muhammad Khurram and Dawood, Muhammad},
  journal    = {ACM Trans. Internet Technol.},
  title      = {Secure Distributed Mobile Volunteer Computing with Android},
  year       = {2021},
  issn       = {1533-5399},
  month      = sep,
  number     = {1},
  volume     = {22},
  abstract   = {Volunteer Computing provision of seamless connectivity that enables convenient and
rapid deployment of greener and cheaper computing infrastructure is extremely promising
to complement next-generation distributed computing systems. Undoubtedly, without
tactile Internet and secure VC ecosystems, harnessing its full potentials and making
it an alternative viable and reliable computing infrastructure is next to impossible.
Android-enabled smart devices, applications, and services are inevitable for Volunteer
computing. Contrarily, the progressive developments of sophisticated Android malware
may reduce its exponential growth. Besides, Android malwares are considered the most
potential and persistent cyber threat to mobile VC systems. To secure Android-based
mobile volunteer computing, the authors proposed MulDroid, an efficient and self-learning
autonomous hybrid (Long-Short-Term Memory, Convolutional Neural Network, Deep Neural
Network) multi-vector Android malware threat detection framework. The proposed mechanism
is highly scalable with well-coordinated infrastructure and self-optimizing capabilities
to proficiently tackle fast-growing dynamic variants of sophisticated malware threats
and attacks with 99.01% detection accuracy. For a comprehensive evaluation, the authors
employed current state-of-the-art malware datasets (Android Malware Dataset, Androzoo)
with standard performance evaluation metrics. Moreover, MulDroid is compared with
our constructed contemporary hybrid DL-driven architectures and benchmark algorithms.
Our proposed mechanism outperforms in terms of detection accuracy with a trivial tradeoff
speed efficiency. Additionally, a 10-fold cross-validation is performed to explicitly
show unbiased results.},
  address    = {New York, NY, USA},
  articleno  = {2},
  doi        = {10.1145/3428151},
  issue_date = {February 2022},
  keywords   = {deep learning (DL), Volunteer computing (VC), android malware, tactile internet},
  numpages   = {21},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3428151},
}

@inproceedings{10.1145/3143434.3143443,
author = {Bogner, Justus and Wagner, Stefan and Zimmermann, Alfred},
title = {Automatically Measuring the Maintainability of Service- and Microservice-Based Systems: A Literature Review},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143443},
doi = {10.1145/3143434.3143443},
abstract = {In a time of digital transformation, the ability to quickly and efficiently adapt
software systems to changed business requirements becomes more important than ever.
Measuring the maintainability of software is therefore crucial for the long-term management
of such products. With Service-based Systems (SBSs) being a very important form of
enterprise software, we present a holistic overview of such metrics specifically designed
for this type of system, since traditional metrics - e.g. object-oriented ones - are
not fully applicable in this case. The selected metric candidates from the literature
review were mapped to 4 dominant design properties: size, complexity, coupling, and
cohesion. Microservice-based Systems (μSBSs) emerge as an agile and fine-grained variant
of SBSs. While the majority of identified metrics are also applicable to this specialization
(with some limitations), the large number of services in combination with technological
heterogeneity and decentralization of control significantly impacts automatic metric
collection in such a system. Our research therefore suggest that specialized tool
support is required to guarantee the practical applicability of the presented metrics
to μSBSs.},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {107–115},
numpages = {9},
keywords = {maintainability, metrics, service-based systems, SOA, microservices},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@Article{Belsis2014,
  author     = {Belsis, Petros and Pantziou, Grammati},
  journal    = {Personal Ubiquitous Comput.},
  title      = {A K-Anonymity Privacy-Preserving Approach in Wireless Medical Monitoring Environments},
  year       = {2014},
  issn       = {1617-4909},
  month      = jan,
  number     = {1},
  pages      = {61–74},
  volume     = {18},
  abstract   = {With the proliferation of wireless sensor networks and mobile technologies in general,
it is possible to provide improved medical services and also to reduce costs as well
as to manage the shortage of specialized personnel. Monitoring a person's health condition
using sensors provides a lot of benefits but also exposes personal sensitive information
to a number of privacy threats. By recording user-related data, it is often feasible
for a malicious or negligent data provider to expose these data to an unauthorized
user. One solution is to protect the patient's privacy by making difficult a linkage
between specific measurements with a patient's identity. In this paper we present
a privacy-preserving architecture which builds upon the concept of k-anonymity; we
present a clustering-based anonymity scheme for effective network management and data
aggregation, which also protects user's privacy by making an entity indistinguishable
from other k similar entities. The presented algorithm is resource aware, as it minimizes
energy consumption with respect to other more costly, cryptography-based approaches.
The system is evaluated from an energy-consuming and network performance perspective,
under different simulation scenarios.},
  address    = {Berlin, Heidelberg},
  doi        = {10.1007/s00779-012-0618-y},
  issue_date = {January 2014},
  keywords   = {Clustering, Remote medical monitoring, Sensors, Anonymity},
  numpages   = {14},
  publisher  = {Springer-Verlag},
  url        = {https://doi.org/10.1007/s00779-012-0618-y},
}

@Article{Cerina2020,
  author     = {Cerina, L. and Santambrogio, M. D.},
  journal    = {SIGBED Rev.},
  title      = {SAGe: A Configurable Code Generator for Efficient Symbolic Analysis of Time-Series},
  year       = {2020},
  month      = jul,
  number     = {1},
  pages      = {12–17},
  volume     = {17},
  abstract   = {Some of the most recent applications and services revolve around the analysis of time-series,
which generally exhibits chaotic characteristics. This behavior brought back the necessity
to simplify their representation to discover meaningful patterns and extract information
efficiently. Furthermore, recent trends show how computation is moving back from the
Cloud to the Edge of network, meaning that algorithms should be compatible with low-power
embedded devices. A family of methods called Symbolic Analysis (SA) tries to solve
this issue, reducing the dimensionality of the original data in a set of symbolic
words and providing distance metrics for the obtained symbols. However, SA is usually
implemented using application-specific tools, which are not easily adaptable, or mathematical
environments (e.g. R, Julia) that do not ensure portability, or that require additional
work to maximize computing performance. We propose here SAGe: a code generation tool
that helps the user to prototype efficient and portable code, starting from a high-level
representation of SA requirements. Other than exploiting similarities between SA pipelines,
SAGe employs general code templates to build and deploy the code on different architectures,
such as embedded devices, microcontrollers, and FPGAs. Preliminary results show a
speedup up to 223x against Python implementations running on an x86 desktop machine
and a notable increase in computational efficiency on a reconfigurable device.},
  address    = {New York, NY, USA},
  doi        = {10.1145/3412821.3412823},
  issue_date = {February 2020},
  numpages   = {6},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3412821.3412823},
}

@InProceedings{Rafii2017,
  author    = {Rafii, Fadoua and Hassani, Badr Dine Rossi and Kbir, M'hamed A\"{\i}t},
  booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
  title     = {New Approach for Microarray Data Decision Making with Respect to Multiple Sources},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {BDCA'17},
  abstract  = {Microarray technology is an innovative technology, which has brought changes to the
biological fields. It is considered as an interesting advent for worthwhile researches.
It has permitted simultaneous measurements of the hundreds of activities of genes.
However, most of users and specifically researchers and biologists find difficulties
while extracting and interpreting this kind of data, also the results of Microarray
experiments are stored in multiple and different databases. The present paper focuses
on providing a global architecture for making decisions on Microarray data, by taking
advantages from the semantic web technologies and the data mining techniques. The
major goal consists on getting decisions about a given disease from many experiment
data distributed on many sources over the net. The input dataset, real elements array
form, is retrieved from the integrated experiments designed for cancer studies. This
work is interested to two huge Microarray databases: GEO and ArrayExpress. The integration
was based on semantic web technologies used to integrate data from several Web sites
and Microarray data sources. This can be done by a user to combine several experiments
that treat the same disease or phenomenon in order to have more significant results.
Also a user can upload a specific dataset, via Web services provided by a laboratory,
that can be combined with other data, containing the same genes and treating the same
disease, and receive results of data mining techniques proposed by this laboratory.
We suppose that each laboratory has its own Web services that can receive data which
respects a predefined format.},
  articleno = {106},
  doi       = {10.1145/3090354.3090463},
  isbn      = {9781450348522},
  keywords  = {Data mining, Microarray, Semantic web},
  location  = {Tetouan, Morocco},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3090354.3090463},
}

@InProceedings{Sljivo2018,
  author    = {\v{S}ljivo, Amina and Kerkhove, Dwight and Moerman, Ingrid and De Poorter, Eli and Hoebeke, Jeroen},
  booktitle = {Proceedings of the 10th Workshop on Ns-3},
  title     = {Interactive Web Visualizer for IEEE 802.11ah Ns-3 Module},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {23–29},
  publisher = {Association for Computing Machinery},
  series    = {WNS3 '18},
  abstract  = {The main purpose of running ns-3 simulations is to generate relevant data sets for
further study. There are two strategies to generate output from ns-3, either using
generic predefined bulk output mechanisms or using the ns-3's Tracing system. Both
require parsing the raw output data to extract and process the data of interest to
obtain meaningful information. However, parsing such output is in most cases time
consuming and prone to mistakes. Post-processing is even harder when a large number
of simulations needs to be analyzed and even the tracing system cannot simplify this
task. Moreover, results obtained this way are only available once the simulation is
finished.Therefore, we developed a user-friendly interactive visualization and post-processing
tool for IEEE 802.11ah called ahVisualizer. Beside the topology and MAC configuration,
ahVisualizer also plots our traces for each node over time during the simulation,
as well as averages and standard deviations for each traced parameter. It can compare
all the measured values across different simulations. Users can easily download figures
and data in various formats. Moreover, it includes a post-processing tool which plots
desired series, with desired fixed parameters, from a large set of simulations. This
paper presents the ahVisualizer, its services and its architecture and shows how this
tool enables much faster and easier data analysis and monitoring of ns-3 simulations
with 802.11ah.},
  doi       = {10.1145/3199902.3199904},
  isbn      = {9781450364133},
  keywords  = {visualization, Wi-Fi HaLow, ns-3, post-processing, analysis, distributed simulations, IEEE 802.11ah},
  location  = {Surathkal, India},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3199902.3199904},
}

@Article{Ying2017,
  author     = {Ying, Xuhang and Zhang, Jincheng and Yan, Lichao and Chen, Yu and Zhang, Guanglin and Chen, Minghua and Chandra, Ranveer},
  journal    = {ACM Trans. Intell. Syst. Technol.},
  title      = {Exploring Indoor White Spaces in Metropolises},
  year       = {2017},
  issn       = {2157-6904},
  month      = aug,
  number     = {1},
  volume     = {9},
  abstract   = {It is a promising vision to exploit white spaces, that is, vacant VHF and UHF TV channels,
to meet the rapidly growing demand for wireless data services in both outdoor and
indoor scenarios. While most prior works have focused on outdoor white space, the
indoor story is largely open for investigation. Motivated by this observation and
discovering that 70% of the spectrum demand comes from indoor environment, we carry
out a comprehensive study to explore indoor white spaces. We first conduct a large-scale
measurement study and compare outdoor and indoor TV spectrum occupancy at 30+ diverse
locations in a typical metropolis—Hong Kong. Our results show that abundant white
spaces are available in different areas in Hong Kong, which account for more than
50% and 70% of the entire TV spectrum in outdoor and indoor scenarios, respectively.
Although there are substantially more white spaces indoors than outdoors, there have
been very few solutions for identifying indoor white space. To fill in this gap, we
develop the first data-driven, low-cost indoor white space identification system for
White-space Indoor Spectrum EnhanceR (WISER), to allow secondary users to identify
white spaces for communication without sensing the spectrum themselves. We design
the architecture and algorithms to address the inherent challenges. We build a WISER
prototype and carry out real-world experiments to evaluate its performance. Our results
show that WISER can identify 30%--40% more indoor white spaces with negligible false
alarms, as compared to alternative baseline approaches.},
  address    = {New York, NY, USA},
  articleno  = {9},
  doi        = {10.1145/3059149},
  issue_date = {October 2017},
  keywords   = {TV white spaces, sensor placement, clustering algorithms},
  numpages   = {25},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3059149},
}

@Article{Shi2021,
  author     = {Shi, Cong and Liu, Jian and Liu, Hongbo and Chen, Yingying},
  journal    = {ACM Trans. Internet Things},
  title      = {WiFi-Enabled User Authentication through Deep Learning in Daily Activities},
  year       = {2021},
  issn       = {2691-1914},
  month      = may,
  number     = {2},
  volume     = {2},
  abstract   = {User authentication is a critical process in both corporate and home environments
due to the ever-growing security and privacy concerns. With the advancement of smart
cities and home environments, the concept of user authentication is evolved with a
broader implication by not only preventing unauthorized users from accessing confidential
information but also providing the opportunities for customized services corresponding
to a specific user. Traditional approaches of user authentication either require specialized
device installation or inconvenient wearable sensor attachment. This article supports
the extended concept of user authentication with a device-free approach by leveraging
the prevalent WiFi signals made available by IoT devices, such as smart refrigerator,
smart TV, and smart thermostat, and so on. The proposed system utilizes the WiFi signals
to capture unique human physiological and behavioral characteristics inherited from
their daily activities, including both walking and stationary ones. Particularly,
we extract representative features from channel state information (CSI) measurements
of WiFi signals, and develop a deep-learning-based user authentication scheme to accurately
identify each individual user. To mitigate the signal distortion caused by surrounding
people’s movements, our deep learning model exploits a CNN-based architecture that
constructively combines features from multiple receiving antennas and derives more
reliable feature abstractions. Furthermore, a transfer-learning-based mechanism is
developed to reduce the training cost for new users and environments. Extensive experiments
in various indoor environments are conducted to demonstrate the effectiveness of the
proposed authentication system. In particular, our system can achieve over 94% authentication
accuracy with 11 subjects through different activities.},
  address    = {New York, NY, USA},
  articleno  = {13},
  doi        = {10.1145/3448738},
  issue_date = {May 2021},
  keywords   = {User authentication, IoT, WiFi signals},
  numpages   = {25},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3448738},
}

@InProceedings{Olariu2017,
  author    = {Olariu, Stephan and Florin, Ryan},
  booktitle = {Proceedings of the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
  title     = {Vehicular Clouds Research: What is Missing?},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {77–84},
  publisher = {Association for Computing Machinery},
  series    = {DIVANet '17},
  abstract  = {Vehicular Clouds (VCs) have become an active research topic. However, even a cursory
look reveals that the VC literature of recent years is full of papers discussing fanciful
VC architectures and services that often seem too good to be true. And many of them
are. It seems to us that promoting VC models without any regard to their practical
feasibility is apt to discredit the VC concept altogether. Part of the problem stems
from the fact that some authors do not seem to be concerned with the obvious fact
that moving vehicles' residency times in the VC may, indeed, be very short and, therefore,
so is their contribution to the amount of useful work performed. Should a vehicle
running a user job leave the VC prematurely, the amount of work performed by that
vehicle may be lost, unless special precautions are taken. Such precautionary measures
involve either some flavor of checkpointing or some form of redundant job assignment.
Both approaches have consequences in terms of overhead and impact job completion time.
The success of conventional cloud computing (CC) is attributable to the ability to
provide quantifiable functional characteristics such as scalability, reliability and
availability. By the same token, if the VCs are to see a widespread adoption, the
same quantitative aspects have to be addressed here, too. Feasibility issues in terms
of sufficient compute power, communication bandwidth, reliability, availability, and
job duration time are all fundamental quantitative aspects of VCs that need to be
studied and understood before one can claim with any degree of certainty that they
can support the workload for which they are intended. The first contribution of this
paper is to make a case for the stringent need to address quantitatively the performance
characteristics of VC architectures and proposed services. Our second contribution
is to point out directions and challenges facing the VC community.},
  doi       = {10.1145/3132340.3132358},
  isbn      = {9781450351645},
  keywords  = {availability, acm proceedings, vehicular clouds, reliability, redundancy, cloud computing},
  location  = {Miami, Florida, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3132340.3132358},
}

@InProceedings{Chakaravarthy2021,
  author    = {Chakaravarthy, Ravikumar V. and Kwon, Hyun and Jiang, Hua},
  booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
  title     = {Vision Control Unit in Fully Self Driving Vehicles Using Xilinx MPSoC and Opensource Stack},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {311–317},
  publisher = {Association for Computing Machinery},
  series    = {ASPDAC '21},
  abstract  = {Fully self-driving (FSD) vehicles are becoming increasing popular over the last few
years and companies are investing significantly into its research and development.
In the recent years, FSD technology innovators like Tesla, Google etc. have been working
on proprietary autonomous driving stacks and have been able to successfully bring
the vehicle to the roads. On the other end, organizations like Autoware Foundation
and Baidu are fueling the growth of self-driving mobility using open source stacks.
These organizations firmly believe in enabling autonomous driving technology for everyone
and support developing software stacks through the open source community that is SoC
vendor agnostic. In this proposed solution we describe a vision control unit for a
fully self-driving vehicle developed on Xilinx MPSoC platform using open source software
components.The vision control unit of an FSD vehicle is responsible for camera video
capture, image processing and rendering, AI algorithm processing, data and meta-data
transfer to next stage of the FSD pipeline. In this proposed solution we have used
many open source stacks and frameworks for video and AI processing. The processing
of the video pipeline and algorithms take full advantage of the pipelining and parallelism
using all the heterogenous cores of the Xilinx MPSoC. In addition, we have developed
an extensible, scalable, adaptable and configurable AI backend framework, XTA, for
acceleration purposes that is derived from a popular, open source AI backend framework,
TVM-VTA. XTA uses all the MPSoC cores for its computation in a parallel and pipelined
fashion. XTA also adapts to the compute and memory parameters of the system and can
scale to achieve optimal performance for any given AI problem. The FSD system design
is based on a distributed system architecture and uses open source components like
Autoware for autonomous driving algorithms, ROS and Distributed Data Services as a
messaging middleware between the functional nodes and a real-time kernel to coordinate
the actions. The details of image capture, rendering and AI processing of the vision
perception pipeline will be presented along with the performance measurements of the
vision pipeline.In this proposed solution we will demonstrate some of the key use
cases of vision perception unit like surround vision and object detection. In addition,
we will also show the capability of Xilinx MPSoC technology to handle multiple channels
of real time camera and the integration with the Lidar/Radar point cloud data to feed
into the decision-making unit of the overall system. The system is also designed with
the capability to update the vision control unit through Over the Air Update (OTA).
It is also envisioned that the core AI engine will require regular updates with the
latest training values; hence a built-in platform level mechanism supporting such
capability is essential for real world deployment.},
  doi       = {10.1145/3394885.3431616},
  isbn      = {9781450379991},
  keywords  = {MPSoC, XTA, Vision Pipeline, Autoware, AI, Heterogenous Processing, FSD, ROS},
  location  = {Tokyo, Japan},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3394885.3431616},
}

@InProceedings{Zhang2014,
  author    = {Zhang, Jiaxing and Qiu, Hanjiao and Shamsabadi, Salar Shahini and Birken, Ralf and Schirner, Gunar},
  booktitle = {ICCPS '14: ACM/IEEE 5th International Conference on Cyber-Physical Systems (with CPS Week 2014)},
  title     = {WiP Abstract: System-Level Integration of Mobile Multi-Modal Multi-Sensor Systems},
  year      = {2014},
  address   = {USA},
  pages     = {227},
  publisher = {IEEE Computer Society},
  series    = {ICCPS '14},
  abstract  = {Heterogeneous roaming sensor systems have gained significant importance in many domains
of civil infrastructure performance inspection as they accelerate data collection
and analysis. However, designing such systems is challenging due to the immense complexity
in the heterogeneity and processing demands of the involved sensors. Unifying frameworks
are needed to simplify development, deployment and operation of roaming sensors and
computing units.To address the sensing needs, we propose SIROM3, a Scalable Intelligent
Roaming Multi-Modal Multi-Sensor framework. SIROM3 incorporates a CPS approach for
infrastructure performance monitoring to address the following challenges: 1. Scalability
and expandability. It offers a scalable and expandable solution enabling diversity
in sensing and the growth in processing platforms from sensors to control centers.
2. Fusion foundations. SIROM3 enables fusion of data collected by logically and geo-spatially
distributed sensors. 3. Big data handling. Automatic collection, categorization, storage
and manipulation of heterogeneous large volume of data streams. 4. Automation. SIROM3
minimizes human interaction through full automation from data acquisition to visualization
of the fused results.Illustrated in Fig. 1, SIROM3 realizes scalability and expandability
in a system-level design approach encapsulating common functionality across hierarchical
components in a Run-Time Environment (RTE). The RTE deploys a layered design paradigm
defining services in both software and hardware architectures. Equipped with multiple
RTE-enabled Multi-Sensor Aggregators (MSA), an array of Roaming Sensor Systems (RSS)
operate as mobile agents attached to vehicles to provide distributed computing services
regulated by Fleet Control and Management (FCM) center via communication network.
A series of foundational services including the Precise Timing Protocol (PTP), GPS
timing systems, Distance Measurement Instruments (DMI) through middleware services
(CORBA) embedded in the RTE build the fusion foundations for data correlation and
analysis. A Heterogeneous Stream File-system Overlay (HSFO) alleviates the big data
challenge. It facilitates storing, processing, categorizing and fusing large heterogeneous
data stream collected by versatile sensors. A GIS visualization module is integrated
for visual analysis and monitoring.SIROM3 enables coordination and collaboration across
sensors, MSAs and RSSes, which produce high volume of heterogeneous data stored in
HSFO. To fuse the data efficiently, SIROM3 contains an expandable plugin system (part
of RTE) for rapid algorithm prototyping using data streams in the architectural hierarchy
(i.e. from MSAs to FCM) via the HSFO API. This makes an ideal test-bed to develop
new algorithms and methodologies expanding CPS principles to civil infrastructure
performance monitoring. In result, SIROM3 simplifies the development, construction
and operation of roaming multi-modal multi-sensor systems.We demonstrate the efficiency
of SIROM3 by automating the assessment of road surface conditions at the city scale.
We realized an RSS with 6 MSAs and 30 heterogeneous sensors, including radars, microphones,
GPS and cameras, all deployed onto a van sponsored by the VOTERS (Versatile Onboard
Traffic Embedded Roaming Sensors) project. Over 20 terabytes of data have been collected,
aggregated, fused, analyzed and geo-spatially visualized using SIROM3 for studying
the pavement conditions of the city of Brockton, MA covering 300 miles. The expandability
of SIROM3 is shown by adding a millimeter-wave radar needing less than 50 lines of
C++ code for system integration. SIROM3 offers a unified solution for comprehensive
roadway assessment and evaluation. The integrated management of big data (from collection
to automated processing) is an ideal research platform for automated assessment of
civil infrastructure performance.},
  doi       = {10.1109/ICCPS.2014.6843740},
  isbn      = {9781479949304},
  location  = {Berlin, Germany},
  numpages  = {1},
  url       = {https://doi.org/10.1109/ICCPS.2014.6843740},
}

@Article{Orwat2016,
  author     = {Orwat, Carsten and Bless, Roland},
  journal    = {SIGCOMM Comput. Commun. Rev.},
  title      = {Values and Networks: Steps Toward Exploring Their Relationships},
  year       = {2016},
  issn       = {0146-4833},
  month      = may,
  number     = {2},
  pages      = {25–31},
  volume     = {46},
  abstract   = {Many technical systems of the Information and Communication Technology (ICT) sector
enable, structure and/or constrain social interactions. Thereby, they influence or
implement certain values, including human rights, and affect or raise conflicts among
values. The ongoing developments toward an "Internet of everything'' is likely to
lead to further value conflicts. This trend illustrates that a better understanding
of the relationships between social values and networks is urgently needed because
it is largely unknown what values lie behind protocols, design principles, or technical
and organizational options of the Internet. This paper focuses on the complex steps
of realizing human rights in Internet architectures and protocols as well as in Internet-based
products and services. Besides direct implementation of values in Internet protocols,
there are several other options that can indirectly contribute to realizing human
rights via political processes and market choices. Eventually, a better understanding
of what values can be realized by networks in general, what technical measures may
affect certain values, and where complementary institutional developments are needed
may lead toward a methodology for considering technical and institutional systems
together.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2935634.2935640},
  issue_date = {April 2016},
  keywords   = {network design, rules, human rights, values, communication protocols, institutions, governance},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2935634.2935640},
}

@InProceedings{Calzarossa2021,
  author    = {Calzarossa, Maria Carla and Massari, Luisa and Tessera, Daniele},
  booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
  title     = {Performance Monitoring Guidelines},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {109–114},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '21},
  abstract  = {Monitoring, that is, the process of collecting measurements on infrastructures and
services, is an important subject of performance engineering. Although monitoring
is not a new education topic, nowadays its relevance is rapidly increasing and its
application is particularly demanding due to the complex distributed architectures
of new and emerging technologies. As a consequence, monitoring has become a "must
have" skill for students majoring in computer science and in computing-related fields.
In this paper, we present a set of guidelines and recommendations to plan, design
and setup sound monitoring projects. Moreover, we investigate and discuss the main
challenges to be faced to build confidence in the entire monitoring process and ensure
measurement quality. Finally, we describe practical applications of these concepts
in teaching activities.},
  doi       = {10.1145/3447545.3451195},
  isbn      = {9781450383318},
  keywords  = {performance monitoring, active measurements, measurement platforms, passive measurements, performance engineering, measurement quality},
  location  = {Virtual Event, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3447545.3451195},
}

@Article{Ismail2020,
  author     = {Ismail, Leila and Materwala, Huned},
  journal    = {ACM Comput. Surv.},
  title      = {Computing Server Power Modeling in a Data Center: Survey, Taxonomy, and Performance Evaluation},
  year       = {2020},
  issn       = {0360-0300},
  month      = jun,
  number     = {3},
  volume     = {53},
  abstract   = {Data centers are large-scale, energy-hungry infrastructure serving the increasing
computational demands as the world is becoming more connected in smart cities. The
emergence of advanced technologies such as cloud-based services, internet of things
(IoT), and big data analytics has augmented the growth of global data centers, leading
to high energy consumption. This upsurge in energy consumption of the data centers
not only incurs the issue of surging high cost (operational and maintenance) but also
has an adverse effect on the environment. Dynamic power management in a data center
environment requires the cognizance of the correlation between the system and hardware-level
performance counters and the power consumption. Power consumption modeling exhibits
this correlation and is crucial in designing energy-efficient optimization strategies
based on resource utilization. Several works in power modeling are proposed and used
in the literature. However, these power models have been evaluated using different
benchmarking applications, power-measurement techniques, and error-calculation formulas
on different machines. In this work, we present a taxonomy and evaluation of 24 software-based
power models using a unified environment, benchmarking applications, power-measurement
techniques, and error formulas, with the aim of achieving an objective comparison.
We use different server architectures to assess the impact of heterogeneity on the
models’ comparison. The performance analysis of these models is elaborated in the
article.},
  address    = {New York, NY, USA},
  articleno  = {58},
  doi        = {10.1145/3390605},
  issue_date = {June 2020},
  keywords   = {Data center, machine learning, server power consumption modeling, green computing, energy-efficiency, resource utilization},
  numpages   = {34},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3390605},
}

@InProceedings{Sardara2018,
  author    = {Sardara, Mauro and Muscariello, Luca and Compagno, Alberto},
  booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
  title     = {A Transport Layer and Socket API for (h)ICN: Design, Implementation and Performance Analysis},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {137–147},
  publisher = {Association for Computing Machinery},
  series    = {ICN '18},
  abstract  = {In this paper we present the design of a transport layer and socket API that can be
used in several ICN architectures such as NDN, CCN and hICN. The current design makes
it possible to expose an API that is simple to insert in current applications and
easy to use to develop novel ones. The proliferation of connected applications for
very different use cases and services with wide spectrum of requirements suggests
that several transport services will coexist in the Internet. This is just about to
happen with QUIC, MPTCP, LEDBAT as the most notable ones but is expected to grow and
diversify with the advent of applications for 5G, IoT, MEC with heterogeneous connectivity.
The advantages of ICN have to be measurable from the application, end-services and
in the network, with relevant key performance indicators. We have implemented an high
speed transport stack with most of the designed features that we present in this paper
with extensive experiments and benchmarks to show the scalability of the current systems
in different use cases.},
  doi       = {10.1145/3267955.3267972},
  isbn      = {9781450359597},
  keywords  = {socket API, ICN, transport services},
  location  = {Boston, Massachusetts},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3267955.3267972},
}

@InProceedings{Ryoo2017,
  author    = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  title     = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {469–480},
  publisher = {Association for Computing Machinery},
  series    = {ISCA '17},
  abstract  = {With increasing deployment of virtual machines for cloud services and server applications,
memory address translation overheads in virtualized environments have received great
attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss
necessitates up to 24 memory references for one guest to host translation. While dedicated
page walk caches and such recent enhancements eliminate many of these memory references,
our measurements on the Intel Skylake processors indicate that many programs in virtualized
mode of execution still spend hundreds of cycles for translations that do not hit
in the TLBs.This paper presents an innovative scheme to reduce the cost of address
translations by using a very large Translation Lookaside Buffer that is part of memory,
the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses
required in commonly used 2D walks with radix-4 type of page tables. Even if many
of the 24 accesses may hit in the page walk caches, the aggregated cost of the many
hits plus the overhead of occasional misses from page walk caches still exceeds the
cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space,
TLB entries (as opposed to multiple page table entries) can be cached in large L2
and L3 data caches, yielding significant benefits. Through detailed evaluation running
SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves
performance by approximately 10% on average. The improvement is more than 16% for
5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate
nearly all TLB misses in 8-core systems.},
  doi       = {10.1145/3079856.3080210},
  isbn      = {9781450348928},
  keywords  = {Die-Stacked DRAM, Virtualization, Very Large TLB, Address Translation},
  location  = {Toronto, ON, Canada},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3079856.3080210},
}

@Article{Ryoo2017a,
  author     = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
  journal    = {SIGARCH Comput. Archit. News},
  title      = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
  year       = {2017},
  issn       = {0163-5964},
  month      = jun,
  number     = {2},
  pages      = {469–480},
  volume     = {45},
  abstract   = {With increasing deployment of virtual machines for cloud services and server applications,
memory address translation overheads in virtualized environments have received great
attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss
necessitates up to 24 memory references for one guest to host translation. While dedicated
page walk caches and such recent enhancements eliminate many of these memory references,
our measurements on the Intel Skylake processors indicate that many programs in virtualized
mode of execution still spend hundreds of cycles for translations that do not hit
in the TLBs.This paper presents an innovative scheme to reduce the cost of address
translations by using a very large Translation Lookaside Buffer that is part of memory,
the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses
required in commonly used 2D walks with radix-4 type of page tables. Even if many
of the 24 accesses may hit in the page walk caches, the aggregated cost of the many
hits plus the overhead of occasional misses from page walk caches still exceeds the
cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space,
TLB entries (as opposed to multiple page table entries) can be cached in large L2
and L3 data caches, yielding significant benefits. Through detailed evaluation running
SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves
performance by approximately 10% on average. The improvement is more than 16% for
5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate
nearly all TLB misses in 8-core systems.},
  address    = {New York, NY, USA},
  doi        = {10.1145/3140659.3080210},
  issue_date = {May 2017},
  keywords   = {Die-Stacked DRAM, Address Translation, Very Large TLB, Virtualization},
  numpages   = {12},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3140659.3080210},
}

@InProceedings{Pan2016,
  author    = {Pan, Xiang and Yegneswaran, Vinod and Chen, Yan and Porras, Phillip and Shin, Seungwon},
  booktitle = {Proceedings of the 2016 ACM International Workshop on Security in Software Defined Networks &amp; Network Function Virtualization},
  title     = {HogMap: Using SDNs to Incentivize Collaborative Security Monitoring},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {7–12},
  publisher = {Association for Computing Machinery},
  series    = {SDN-NFV Security '16},
  abstract  = {Cyber Threat Intelligence (CTI) sharing facilitates a comprehensive understanding
of adversary activity and enables enterprise networks to prioritize their cyber defense
technologies. To that end, we introduce HogMap, a novel software-defined infrastructure
that simplifies and incentivizes collaborative measurement and monitoring of cyber-threat
activity. HogMap proposes to transform the cyber-threat monitoring landscape by integrating
several novel SDN-enabled capabilities: (i) intelligent in-place filtering of malicious
traffic, (ii) dynamic migration of interesting and extraordinary traffic and (iii)
a software-defined marketplace where various parties can opportunistically subscribe
to and publish cyber-threat intelligence services in a flexible manner.We present
the architectural vision and summarize our preliminary experience in developing and
operating an SDN-based HoneyGrid, which spans three enterprises and implements several
of the enabling capabilities (e.g., traffic filtering, traffic forwarding and connection
migration). We find that SDN technologies greatly simplify the design and deployment
of such globally distributed and elastic HoneyGrids.},
  doi       = {10.1145/2876019.2876023},
  isbn      = {9781450340786},
  keywords  = {cyber threat intelligence, honeygrid, marketplace, sdn, honeynet},
  location  = {New Orleans, Louisiana, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2876019.2876023},
}

@InProceedings{Rizvi2015,
  author    = {Rizvi, Syed and Ryoo, Jungwoo and Kissell, John and Aiken, Bill},
  booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication},
  title     = {A Stakeholder-Oriented Assessment Index for Cloud Security Auditing},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {IMCOM '15},
  abstract  = {Cloud computing is an emerging computing model that provides numerous advantages to
organizations (both service providers and customers) in terms of massive scalability,
lower cost, and flexibility, to name a few. Despite these technical and economical
advantages of cloud computing, many potential cloud consumers are still hesitant to
adopt cloud computing due to security and privacy concerns. This paper describes some
of the unique cloud computing security factors and subfactors that play a critical
role in addressing cloud security and privacy concerns. To mitigate these concerns,
we develop a security metric tool to provide information to cloud users about the
security status of a given cloud vendor. The primary objective of the proposed metric
is to produce a security index that describes the security level accomplished by an
evaluated cloud computing vendor. The resultant security index will give confidence
to different cloud stakeholders and is likely to help them in decision making, increase
the predictability of the quality of service, and allow appropriate proactive planning
if needed before migrating to the cloud. To show the practicality of the proposed
metric, we provide two case studies based on the available security information about
two well-known cloud service providers (CSP). The results of these case studies demonstrated
the effectiveness of the security index in determining the overall security level
of a CSP with respect to the security preferences of cloud users.},
  articleno = {55},
  doi       = {10.1145/2701126.2701226},
  isbn      = {9781450333771},
  keywords  = {data privacy, cloud security, security metrics, cloud auditing},
  location  = {Bali, Indonesia},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2701126.2701226},
}

@InProceedings{VasanthaAzhagu2016,
  author    = {VasanthaAzhagu, A. Kannaki and Gnanasekar, J. M.},
  booktitle = {Proceedings of the International Conference on Informatics and Analytics},
  title     = {Cloud Computing Overview, Security Threats and Solutions-A Survey},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICIA-16},
  abstract  = {Cloud Computing aims to provide computing everywhere. It delivers computing resources
on demand over internet in terms of anything anywhere anytime concept. It provides
everything as a service to its users like infrastructure platform software hardware
workplace data and security. Cloud computing has made revolutionary transformations
in the government and business. Cloud Computing transforms the databases and application
software to the huge data centers, where the management of the services and data may
not be trustworthy. To verify the correctness, integrity, confidentially and availability
of data in the cloud, in this paper, we focus on various cloud computing security
threats and solution that have been used since security is an important measure for
quality of service.},
  articleno = {109},
  doi       = {10.1145/2980258.2982046},
  isbn      = {9781450347563},
  keywords  = {Availability, Cloud Computing, Deployment Security threats, Integrity, Quality of Service (QoS)},
  location  = {Pondicherry, India},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2980258.2982046},
}

@InProceedings{Becker2015,
  author    = {Becker, Matthias and Lehrig, Sebastian and Becker, Steffen},
  booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
  title     = {Systematically Deriving Quality Metrics for Cloud Computing Systems},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {169–174},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '15},
  abstract  = {In cloud computing, software architects develop systems for virtually unlimited resources
that cloud providers account on a pay-per-use basis. Elasticity management systems
provision these resources autonomously to deal with changing workload. Such changing
workloads call for new objective metrics allowing architects to quantify quality properties
like scalability, elasticity, and efficiency, e.g., for requirements/SLO engineering
and software design analysis. In literature, initial metrics for these properties
have been proposed. However, current metrics lack a systematic derivation and assume
knowledge of implementation details like resource handling. Therefore, these metrics
are inapplicable where such knowledge is unavailable.To cope with these lacks, this
short paper derives metrics for scalability, elasticity, and efficiency properties
of cloud computing systems using the goal question metric (GQM) method. Our derivation
uses a running example that outlines characteristics of cloud computing systems. Eventually,
this example allows us to set up a systematic GQM plan and to derive an initial set
of six new metrics. We particularly show that our GQM plan allows to classify existing
metrics.},
  doi       = {10.1145/2668930.2688043},
  isbn      = {9781450332484},
  keywords  = {metric, elasticity, cloud computing, gqm, efficiency, slo, analysis, scalability},
  location  = {Austin, Texas, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2668930.2688043},
}

@Article{Yadav2014,
  author     = {Yadav, Nikita and Khatri, Sujata and Singh, V. B.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Developing an Intelligent Cloud for Higher Education},
  year       = {2014},
  issn       = {0163-5948},
  month      = feb,
  number     = {1},
  pages      = {1–5},
  volume     = {39},
  abstract   = {With rapid development in the IT world, technologies are becoming more dynamic and
advanced. Today, technologies are changing with customer requirements. In the IT world,
research is carried out to make technology better to meet the requirements that change
with time. With the advancement in the IT world, online services have proliferated.
Now a days, cloud computing is the hottest buzzword in the IT world. Cloud computing
is not limited to the E-Governance and business worlds, but is also making a great
impact in the education world. With growing demand for education, technologies and
research, all universities and education institutions have their eyes on cloud computing.
The main pillars of educational institutions are students, faculties, administrations
and libraries. Faculty and students do research and need quality data while students
of a particular field need a subject-oriented knowledge. Manually getting these kinds
of data is time consuming as students depend on literature, books, different kind
of software and hardware. With cloud computing in higher education, cost-effective
measures can be taken to minimize the dependency on books, hardware and software.
In this paper, we discuss how Artifical Intelligence based cloud computing in higher
education will improve quality and ease the process of getting e-resources (software/hardware
platform, storage etc.). This study will help in understanding effective cost-cutting
measures. We also discuss how cloud computing in the library and administration will
brighten the education prospects.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2557833.2557854},
  issue_date = {January 2014},
  keywords   = {higher education, E-administration, E-library, cloud computing, E-learning},
  numpages   = {5},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2557833.2557854},
}

@InProceedings{Lehrig2015,
  author    = {Lehrig, Sebastian and Eikerling, Hendrik and Becker, Steffen},
  booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
  title     = {Scalability, Elasticity, and Efficiency in Cloud Computing: A Systematic Literature Review of Definitions and Metrics},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {83–92},
  publisher = {Association for Computing Machinery},
  series    = {QoSA '15},
  abstract  = {Context: In cloud computing, there is a multitude of definitions and metrics for scalability,
elasticity, and efficiency. However, stakeholders have little guidance for choosing
fitting definitions and metrics for these quality properties, thus leading to potential
misunderstandings. For example, cloud consumers and providers cannot negotiate reliable
and quantitative service level objectives directly understood by each stakeholder.
Objectives: Therefore, we examine existing definitions and metrics for these quality
properties from the viewpoint of cloud consumers, cloud providers, and software architects
with regard to commonly used concepts. Methods: We execute a systematic literature
review (SLR), reproducibly collecting common concepts in definitions and metrics for
scalability, elasticity, and efficiency. As quality selection criteria, we assess
whether existing literature differentiates the three properties, exemplifies metrics,
and considers typical cloud characteristics and cloud roles. Results: Our SLR yields
418 initial results from which we select 20 for in-depth evaluation based on our quality
selection criteria. In our evaluation, we recommend concepts, definitions, and metrics
for each property. Conclusions: Software architects can use our recommendations to
analyze the quality of cloud computing applications. Cloud providers and cloud consumers
can specify service level objectives based on our metric suggestions.},
  doi       = {10.1145/2737182.2737185},
  isbn      = {9781450334709},
  keywords  = {systematic literature review, cloud computing, cloud, scalability, metrics, elasticity, efficiency, definitions},
  location  = {Montr\'{e}al, QC, Canada},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2737182.2737185},
}

@InProceedings{AlGhuwairi2016,
  author    = {Al-Ghuwairi, Abdel-Rahman and Eid, Hazem and Aloran, Mohammad and Salah, Zaher and Baarah, Aladdin Hussein and Al-oqaily, Ahmad A.},
  booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
  title     = {A Mutation-Based Model to Rank Testing as a Service (TaaS) Providers in Cloud Computing},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICC '16},
  abstract  = {With the increase of cloud computing service models, the need to measure and evaluate
them are increased as well. In this paper, we proposed a novel measurement approach
for the purpose of evaluating the quality of Testing as a Service (TaaS), which is
considered as one of the most recent outstanding model within cloud computing environment.
(TaaS) as outstanding model include the provision of multi-sub services, such as enabling
cloud customer to verify his own code through the use of cloud provider resources.
Its goes without questioning that testing over web environment requires high level
of resources, time, and effort. Therefore, it should take high attention toward the
quality of the used testing technique. Where, the quality of testing technique associated
with set of attributes that has the ability to determine testing effectiveness. Thus,
in this paper we propose a measurement approach to evaluate the effectiveness of TaaS,
over cloud computing environment which relies on the use of mutation score. The main
contribution of the proposed model represent in the use of mutation score to evaluate
cloud providers ability to perform TaaS, and rank them according to the percentage
of TaaS effectiveness.},
  articleno = {18},
  doi       = {10.1145/2896387.2896403},
  isbn      = {9781450340632},
  keywords  = {Mutation, Effectiveness, Cloud services, Testing as a services, Cloud computing, Measurement},
  location  = {Cambridge, United Kingdom},
  numpages  = {5},
  url       = {https://doi.org/10.1145/2896387.2896403},
}

@InProceedings{Tse2018,
  author    = {Tse, Daniel and Yuen, Hok Hin and He, Qiran and Wang, Chaoya and Yu, Jiheng},
  booktitle = {Proceedings of the 2nd International Conference on E-Commerce, E-Business and E-Government},
  title     = {The Security Vulnerabilities of On-Demand and Sharing Economy},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {47–53},
  publisher = {Association for Computing Machinery},
  series    = {ICEEG '18},
  abstract  = {The cloud computing has been widely in on-demand-and-sharing service economy and has
become a hotspot in recent years especially in the IT industry which really lead to
some changes in human's daily life. However, many users and researchers believed that
the information security is the most significant challenge in cloud computing. Therefore,
this paper aims to discover the threats and vulnerabilities of the cloud storage which
is the most common application originating from the cloud computing. This research
utilized a quantitative approach and all qualified respondents were asked to complete
an online questionnaire. The result shows that (1) Data loss and leakage is the biggest
threat in using cloud storage application (2) Abuse use of cloud computational resources
is the most severe impact in cloud storage application (3) Respondents with different
backgrounds have the different perspectives towards the cloud service (4) The countermeasures
to minimize the security vulnerability are flexibility in choosing the protective
measures, strengthen the infrastructure, improve the password authentication and strengthen
the authorization.},
  doi       = {10.1145/3234781.3234787},
  isbn      = {9781450364904},
  keywords  = {cloud storage application, threats, vulnerabilities, on-demand-and-sharing economy, cloud computing},
  location  = {Hong Kong, Hong Kong},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3234781.3234787},
}

@InProceedings{Kirsal2015,
  author    = {Kirsal, Yonal and Ever, Yoney Kirsal and Mostarda, Leonardo and Gemikonakli, Orhan},
  booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
  title     = {Analytical Modelling and Performability Analysis for Cloud Computing Using Queuing System},
  year      = {2015},
  pages     = {643–647},
  publisher = {IEEE Press},
  series    = {UCC '15},
  abstract  = {In recent years, cloud computing becomes a new computing model emerged from the rapid
development of the internet. Users can reach their resources with high flexibility
using the cloud computing systems all over the world. However, such systems are prone
to failures. In order to obtain realistic quality of service (QoS) measurements, failure
and recovery behaviours of the system should be considered. System's failures and
repairs are associated with availability context in QoS measurements. In this paper,
performance issues are considered with the availability of the system. Markov Reward
Model (MRM) method is used to get QoS measurements. The mean queue length (MQL) results
are calculated using the MRM. The results explicitly show that failures and repairs
affect the system performance significantly.},
  isbn      = {9780769556970},
  keywords  = {quality of service, cloud computing, queuing system, analytical modelling, performability analysis},
  location  = {Limassol, Cyprus},
  numpages  = {5},
}

@Proceedings{2015a,
  title     = {VTDC '15: Proceedings of the 8th International Workshop on Virtualization Technologies in Distributed Computing},
  year      = {2015},
  address   = {New York, NY, USA},
  isbn      = {9781450335737},
  publisher = {Association for Computing Machinery},
  abstract  = {During the past few years, we have begun to see a convergence of cloud computing and
high performance computing (HPC) infrastructures, technologies, and applications.
In HPC, applications have since long predominantly been parallel batch jobs with execution
times measured in hours or even days, managed by mature batch and scheduling systems
developed and refined over decades. With the introduction of clouds, providing elastic
capacity and new programming models for internet-type applications, also traditional
HPC users have begun to explore new methods to solve their problems. Cloud applications
often come with large numbers of shorter tasks, frequently pipelined and sometimes
combined with long-running service-type application components, not too different
from what has been seen in HPC since long. Building on previous successful and highly
attended VTDC workshops, this 8th edition is a forum to dwell from these synergies
and exchange ideas among researchers in the broad area of virtualization technologies
in distributed computing in order to further the forefronts of both HPC and cloud
computing.The technical program is what defines the workshop. For the establishment
of the program, we are grateful to all authors and to the program committee that has
provided each paper with an average of over 6 high quality reviews, hopefully contributing
both to the paper quality and to each author's future research. We thank the invited
speakers, Dr. John Russell Lange (University of Pittsburgh, USA), Dr. Abhishek Gupta
(Intel Corp, USA), and Prof. Guillaume Pierre (IRISA / Rennes 1 University, France),
for their presentations, each highlighting a different aspect on the convergence of
HPC and cloud computing.},
  location  = {Portland, Oregon, USA},
}

@InProceedings{Stephanakis2015,
  author    = {Stephanakis, Ioannis M. and Chochliouros, Ioannis P. and Sfakianakis, Evangelos and Shirazi, Noorulhassan},
  booktitle = {Proceedings of the 16th International Conference on Engineering Applications of Neural Networks (INNS)},
  title     = {Anomaly Detection In Secure Cloud Environments Using a Self-Organizing Feature Map (SOFM) Model For Clustering Sets of R-Ordered Vector-Structured Features},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {EANN '15},
  abstract  = {Cloud computing delivers services over virtualized networks to many end-users. Cloud
services are characterized by such attributes as on-demand self-service, broad network
access, resource pooling, rapid and elastic resource provisioning and metered services
of various qualities. Cloud networks provide data as well as multimedia and video
services. Cloud computing for critical structure IT is a relative new area of potential
applications. Cloud networks are classified into private cloud networks, public cloud
networks and hybrid cloud networks. Anomaly detection systems are defined as a branch
of intrusion detection systems that deal with identifying anomalous events with respect
to normal system behavior. A novel application of a Self-Organizing-Feature Map (SOFM)
of reduced/aggregate sets of ordered vector structured features that are used for
detecting anomalies in the context of secure cloud environments is herein proposed.
Multivalue inputs consist of reduced/aggregate ordered sets of vector and binary features.
The nodes of the SOFM - after training - are indicative of local distributions of
feature measurements during normal cloud operation. Anomalies are detected as outliers
of the trained SOFM. Each structured vector consists of binary as well as histogram
data. The aggregated Canberra distance is used to order histogram data whereas the
Jaccard distance is used for multivalue binary data. The so-called Cross-Order Distance
Matrix is defined for both cases. The distance depends upon the selection of a similarity/distance
measure and a method for operating upon the elements of the Cross-Order Distance Matrix.
Several methods of estimating the distance between two ordered sets of features are
investigated in the course of this paper.},
  articleno = {27},
  doi       = {10.1145/2797143.2797145},
  isbn      = {9781450335805},
  keywords  = {Reduced/aggregate-ordering, Secure cloud networks, Self-Organizing Feature Maps (SOFMs), clustering, Canberra distance, intrusion detection, Jaccard distance},
  location  = {Rhodes, Island, Greece},
  numpages  = {9},
  url       = {https://doi.org/10.1145/2797143.2797145},
}

@InProceedings{Gao2019,
  author    = {Gao, Zhijun and Gao, Yuxin and Xu, Jingjing},
  booktitle = {Proceedings of the 37th ACM International Conference on the Design of Communication},
  title     = {Designing Metrics to Evaluate the Help Center of Baidu Cloud},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SIGDOC '19},
  abstract  = {Help centers are mainly designed to assist users with their product uses. The question
as to how we measure the quality of a help center remains unanswered. As the first
step of a joint research initiated by Peking University and Baidu Cloud that aims
to develop a set of computable metrics to evaluate the quality of help centers, this
experience report shares the results of data analysis on correlation between user
behavioral data and technical documentation quality. The documents and data we use
are a suite of cloud computing services provided by Baidu Cloud. The report begins
with an introduction of the research goal; following reviews on the related work,
it then lays out the design of the experiments with user data collected from Baidu
Cloud. In our experiments, we categorize all documents into three groups and try to
identify which metrics would affect documentation quality most. The result shows that
the key index that contributes most to the model is PV/UV. At last, the report concludes
with our current experimental efforts and future work in our plan.},
  articleno = {28},
  doi       = {10.1145/3328020.3353936},
  isbn      = {9781450367905},
  keywords  = {web metrics, help center evaluation, technical information, quality evaluation},
  location  = {Portland, Oregon},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3328020.3353936},
}

@InProceedings{AriasCabarcos2018,
  author    = {Arias-Cabarcos, Patricia and Almen\'{a}rez, Florina and D\'{\i}az-S\'{a}nchez, Daniel and Mar\'{\i}n, Andr\'{e}s},
  booktitle = {Proceedings of the 2nd International Workshop on Multimedia Privacy and Security},
  title     = {FRiCS: A Framework for Risk-Driven Cloud Selection},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {18–26},
  publisher = {Association for Computing Machinery},
  series    = {MPS '18},
  abstract  = {Our devices and interactions in a world where physical and digital realities are more
and more blended, generate a continuum of multimedia data that needs to be stored,
shared and processed to provide services that enrich our daily lives. Cloud computing
plays a key role in these tasks, dissolving resource allocation and computational
boundaries, but it also requires advanced security mechanisms to protect the data
and provide privacy guarantees. Therefore, security assurance must be evaluated before
offloading tasks to a cloud provider, a process which is currently manual, complex
and inadequate for dynamic scenarios. However, though there are many tools for evaluating
cloud providers according to quality of service criteria, automated categorization
and selection based on risk metrics is still challenging. To address this gap, we
present FRiCS, a Framework for Risk-driven Cloud Selection, which contributes with:
1) a set of cloud security metrics and risk-based weighting policies, 2) distributed
components for metric extraction and aggregation, and 3) decision-making plugins for
ranking and selection. We have implemented the whole system and conducted a case-study
validation based on public cloud providers' security data, showing the benefits of
the proposed approach.},
  doi       = {10.1145/3267357.3267362},
  isbn      = {9781450359887},
  keywords  = {risk-driven security, cloud computing, decision making, security metrics, cloud-based multimedia systems},
  location  = {Toronto, Canada},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3267357.3267362},
}

@InProceedings{Farias2016,
  author    = {Farias, Victor A. E. and Sousa, Fl\'{a}vio R. C. and Maia, Jos\'{e} G. R. and Gomes, Jo\~{a}o P. P. and Machado, Javam C.},
  booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Machine Learning Approach for Cloud NoSQL Databases Performance Modeling},
  year      = {2016},
  pages     = {617–620},
  publisher = {IEEE Press},
  series    = {CCGRID '16},
  abstract  = {Cloud computing is a successful, emerging paradigm that supports on-demand services
with pay-as-you-go model. With the exponential growth of data, NoSQL databases have
been used to manage data in the cloud. In these newly emerging settings, mechanisms
to guarantee Quality of Service heavily relies on performance predictability, i.e.,
the ability to estimate the impact of concurrent query execution on the performance
of individual queries in a continuously evolving workload. This paper presents a performance
modeling approach for NoSQL databases in terms of performance metrics which is capable
of capturing the non-linear effects caused by concurrency and distribution aspects.
Experimental results confirm that our performance modeling can accurately predict
mean response time measurements under a wide range of workload configurations.},
  doi       = {10.1109/CCGrid.2016.83},
  isbn      = {9781509024520},
  keywords  = {cloud computing, NoSQL, performance modeling},
  location  = {Cartagena, Columbia},
  numpages  = {4},
  url       = {https://doi.org/10.1109/CCGrid.2016.83},
}

@InProceedings{Aske2017,
  author    = {Aske, Austin and Zhao, Xinghui},
  booktitle = {Proceedings of The10th International Conference on Utility and Cloud Computing},
  title     = {An Actor-Based Framework for Edge Computing},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {199–200},
  publisher = {Association for Computing Machinery},
  series    = {UCC '17},
  abstract  = {The Actor model provides inherent parallelism, along with other convenient features
to build large-scale distributed systems. In this paper, we present ActorEdge, an
Actor based distributed framework for edge computing. ActorEdge provides straitforward
integration with existing technologies, while enabling application developers to dynamically
utilize computational resources on the edge of the clouds. ActorEdge has proven to
outperform cloud computing options by providing superior quality of service, measuring
a 10x lower latency, 30% less jitter, and greater bandwidth. Using this framework,
programmers can easily develop and deploy their applications on a heterogeneous system,
including cloud servers/data centers, edge servers, and mobile devices.},
  doi       = {10.1145/3147213.3149214},
  isbn      = {9781450351492},
  keywords  = {mobile clouds, cloud computing, edge computing, actors},
  location  = {Austin, Texas, USA},
  numpages  = {2},
  url       = {https://doi.org/10.1145/3147213.3149214},
}

@InProceedings{Fang2017,
  author    = {Fang, Zhou and Yu, Tong and Mengshoel, Ole J. and Gupta, Rajesh K.},
  booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  title     = {QoS-Aware Scheduling of Heterogeneous Servers for Inference in Deep Neural Networks},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {2067–2070},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '17},
  abstract  = {Deep neural networks (DNNs) are popular in diverse fields such as computer vision
and natural language processing. DNN inference tasks are emerging as a service provided
by cloud computing environments. However, cloud-hosted DNN inference faces new challenges
in workload scheduling for the best Quality of Service (QoS), due to dependence on
batch size, model complexity and resource allocation. This paper represents the QoS
metric as a utility function of response delay and inference accuracy. We first propose
a simple and effective heuristic approach that keeps low response delay and satisfies
the requirement on processing throughput. Then we describe an advanced deep reinforcement
learning (RL) approach that learns to schedule from experience. The RL scheduler is
trained to maximize QoS, using a set of system statuses as the input to the RL policy
model. Our approach performs scheduling actions only when there are free GPUs, thus
reduces scheduling overhead over common RL schedulers that run at every continuous
time step. We evaluate the schedulers on a simulation platform and demonstrate the
advantages of RL over heuristics.},
  doi       = {10.1145/3132847.3133045},
  isbn      = {9781450349185},
  keywords  = {web service, deep neural networks inference, deep reinforcement learning, reinforcement learning, qos aware scheduling},
  location  = {Singapore, Singapore},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3132847.3133045},
}

@InProceedings{Yu2015,
  author    = {Yu, Ning and Gu, Feng and Guo, Xuan and He, Zaobo},
  booktitle = {Proceedings of the 18th Symposium on Communications &amp; Networking},
  title     = {A Fine-Grained Flow Control Model for Cloud-Assisted Data Broadcasting},
  year      = {2015},
  address   = {San Diego, CA, USA},
  pages     = {24–31},
  publisher = {Society for Computer Simulation International},
  series    = {CNS '15},
  abstract  = {Cloud-assisted data broadcasting is an emerging application where cloud computing
assists data broadcasting to extend the capacity of system computing and improve the
interactivity of the conventional media. However, with the increase in scale, it brings
the difficulty on the complexity to provide the sufficient quality of service for
diverse receivers. In order to obtain a fine-grained flow rate as well as the system
stability, we propose a model based on parallel scheduling, fair queue and Proportional-Integral-Derivative
(PID) controller to cope with these challenges. PID controller takes advantage of
the feedback of the statistical output stream and automatically adjusts the transmission
flow so that the system can achieve the fine-grained multiplexing performance. Meanwhile,
we adopt a set of novel metrics to monitor and measure the quality of flow control
in order to weaken the negative impact of coarse-grained flow to user-end devices
to the minimum level. Extensive simulations and evaluations have illustrated the superiority
of the proposed model in the performance and the quality of service in terms of proposed
measurement metrics.},
  isbn      = {9781510801004},
  keywords  = {impact energy, quality of service, fair queue, cloud-assisted data broadcasting, fine-grained flow control, energy metric, time division multiplexing, user-end devices, impact power, proportional-integral-derivative (PID) controller, heterogeneous network},
  location  = {Alexandria, Virginia},
  numpages  = {8},
}

@Proceedings{2015b,
  title     = {QoSA '15: Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
  year      = {2015},
  address   = {New York, NY, USA},
  isbn      = {9781450334709},
  publisher = {Association for Computing Machinery},
  abstract  = {Welcome to the 11th International ACM Sigsoft Conference on the Quality of Software
Architectures -- QoSA 2015. For more than a decade, QoSA has strived to advance the
state of the art of quality aspects of software architecture, focusing broadly on
its quality characteristics and how these relate to the design of software architectures.
Specific issues of interest are defining and modeling quality measures, evaluating
and managing architecture quality, linking architecture to requirements and implementation,
and preserving architecture quality throughout the system lifetime. Past themes for
QoSA include Architecting for Adaptivity (2014), The System View (2013), Evolving
Architectures (2012), Quality throughout the Software Lifecycle (2011), and Research
into Practice -- Reality and Gaps (2010).QoSA 2015's theme is "Software Architecture
for the 4th Industrial Revolution". After mechanization, mass production, and electronics,
the Internet is about to enable a new level of productivity in manufacturing. This
shall be enabled by smart cyber-physical systems connected to cloud computing services
and communicating using standardized semantics. In the near future, industrial big
data analytics on monitored sensor data shall improve the efficiency and individualization
of production facilities. This year's QoSA conference solicited contributions that
explore the various implications of this upcoming industrial revolution on software
architecture. This included reference architectures, software architectures adapting
at run time, architecture styles and patterns for cyber-physical and distributed systems.The
call for papers attracted 42 initial submissions from Asia, North America, Africa,
and Europe and 28 final submissions were considered during the review process. The
program committee accepted 11 full papers and 2 short papers that cover topics, such
as new architecture modeling approaches, architectural tactics for mobile computing,
cloud computing architectures, and cyberphysical systems. QoSA's 2015 proceedings
also include 2 papers from the WCOP 2015, the 20th International Doctoral Symposium
on Components and Architecture.QoSA 2015 is part of the federated events on component-based
software engineering and software architecture (CompArch 2015), which include WICSA
2015 (12th Working IEEE / IFIP Conference on Software Architecture) and CBSE 2015
(18th International ACM SIGSOFT Symposium on Component-Based Software Engineering).},
  location  = {Montr\'{e}al, QC, Canada},
}

@InProceedings{Silva2020,
  author    = {Silva, Jorge Luiz Machado da and de Fran\c{c}a, Breno B. Nicolau and Rubira, Cec\'{\i}lia Mary Fischer},
  booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
  title     = {Generating Trustworthiness Adaptation Plans Based on Quality Models for Cloud Platforms},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {141–150},
  publisher = {Association for Computing Machinery},
  series    = {SBCARS '20},
  abstract  = {Cloud computing platforms can offer many benefits related to the provision of service
processing and storage for hosting client applications. Trustworthiness can be defined
as the trust of a customer in a cloud service and its provider; however, the assurance
of this property is not trivial. First, trustworthiness in general is not composed
by a single quality attribute, but by the combination of multiple attributes, such
as data privacy, performance, reliability, etc. Second, during runtime clients can
experience a change of the trustworthiness level required by their application due
to the degradation of the cloud service. This article presents a solution that monitors
during runtime the set of quality attributes of a specific application and generates
adaptation plans in order to certify that an adequate resource amount be provided
by the cloud in order to keep its trustworthiness level. Our solution is based on
quality models to compute the metric associated to each non-functional requirement
and their combination them into different types of trustworthiness levels. The main
contribution of the solution is to provide an approach which deals with multiple requirements
at the same time (or simultaneously) during runtime in order to adapt the cloud resources
to keep the trustworthiness level required by the application. The solution was evaluated
by an experiment considering a scenario where the application trustworthiness level
was composed by three quality attributes: data privacy, performance and reliability.
Initial results have shown that the approach is feasible in terms of the execution
of the adaptation plans during runtime to certify the trustworthiness level required
by the application.},
  doi       = {10.1145/3425269.3425272},
  isbn      = {9781450387545},
  keywords  = {Cloud Computing, Trustworthiness, Adaptation Planning, Self-adaptive Systems},
  location  = {Natal, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3425269.3425272},
}

@InProceedings{Henning2021,
  author    = {Henning, S\"{o}ren and Hasselbring, Wilhelm},
  booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
  title     = {How to Measure Scalability of Distributed Stream Processing Engines?},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {85–88},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '21},
  abstract  = {Scalability is promoted as a key quality feature of modern big data stream processing
engines. However, even though research made huge efforts to provide precise definitions
and corresponding metrics for the term scalability, experimental scalability evaluations
or benchmarks of stream processing engines apply different and inconsistent metrics.
With this paper, we aim to establish general metrics for scalability of stream processing
engines. Derived from common definitions of scalability in cloud computing, we propose
two metrics: a load capacity function and a resource demand function. Both metrics
relate provisioned resources and load intensities, while requiring specific service
level objectives to be fulfilled. We show how these metrics can be employed for scalability
benchmarking and discuss their advantages in comparison to other metrics, used for
stream processing engines and other software systems.},
  doi       = {10.1145/3447545.3451190},
  isbn      = {9781450383318},
  keywords  = {cloud computing, stream processing, metrics, scalability},
  location  = {Virtual Event, France},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3447545.3451190},
}

@InProceedings{Marwan2017,
  author    = {Marwan, M. and Kartit, A. and Ouahmane, H.},
  booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
  title     = {Protecting Medical Data in Cloud Storage Using Fault-Tolerance Mechanism},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {214–219},
  publisher = {Association for Computing Machinery},
  series    = {ICSDE '17},
  abstract  = {Given the fact that cloud computing offers cost-efficient storage systems, medical
organizations are more interested in using this alternative solution to safeguard
their patients' data. Equally interestingly, users are charged based typically on
the amount of occupied storage space. Basically, this concept is meant to cut costs
and improve the quality of healthcare services. Consequently, implementing cloud storage
would help clients to manage their data efficiently. Besides, it allows users to outsource
the storage process by using virtual storage systems instead of local ones. Despite
its significant impact in healthcare domain, adopting this paradigm to save medical
data on remote servers poses serious challenges, especially security risks. Currently,
various cryptographic techniques have been used to ensure data confidentiality and
to avoid data disclosure. Globally, this model uses traditional cryptosystems such
as AES, RSA to address security issues in cloud storage. As far as we know, there
are only a few works in literature that deal with availability and data recovery in
cloud computing. In general, the classical approach which is based on backup or replication
is not suitable for cloud environment due to the highly dynamic nature of this model.
The intent of this work is to enhance the reliability of cloud storage in order to
meet security requirements. In this study, we propose a novel method based on Shamir's
Secret Share Scheme and multi-cloud concept to avoid data loss and unauthorized access.
More precisely, this technique seeks to divide consumers' data into several portions
using Shamir's Secret Share to prevent privacy disclosure. Based on these considerations,
we store these created portions in different nodes to minimize security risks, particularly
internal attacks. To sum up, this method is designed to ensure fault-tolerance, which
is the main subject of this study. In fact, we need just certain shares to reconstruct
the secret data rather than using all parts. The experimental results are in accordance
with the theoretical assumptions behind this model, and hence, confirm that the proposed
framework provides necessary measures for preventing data loss in cloud storage.},
  doi       = {10.1145/3128128.3128161},
  isbn      = {9781450352819},
  keywords  = {cloud computing, medical image, fault tolerance, security},
  location  = {Rabat, Morocco},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3128128.3128161},
}

@InProceedings{Lehrig2015a,
  author    = {Lehrig, Sebastian and Becker, Steffen},
  booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
  title     = {The CloudScale Method for Software Scalability, Elasticity, and Efficiency Engineering: A Tutorial},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {329–331},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '15},
  abstract  = {In cloud computing, software engineers design systems for virtually unlimited resources
that cloud providers account on a pay-per-use basis. Elasticity management systems
provision these resource autonomously to deal with changing workloads. Such workloads
call for new objective metrics allowing engineers to quantify quality properties like
scalability, elasticity, and efficiency. However, software engineers currently lack
engineering methods that aid them in engineering their software regarding such properties.
Therefore, the CloudScale project developed tools for such engineering tasks. These
tools cover reverse engineering of architectural models from source code, editors
for manual design/adaption of such models, as well as tools for the analysis of modeled
and operating software regarding scalability, elasticity, and efficiency. All tools
are interconnected via ScaleDL, a common architectural language, and the CloudScale
Method that leads through the engineering process. In this tutorial, we execute our
method step-by-step such that every tool and ScaleDL are briefly introduced.},
  doi       = {10.1145/2668930.2688818},
  isbn      = {9781450332484},
  keywords  = {engineering, cloudscale, tutorial, metrics, efficiency, scalability, elasticity, cloud computing, software analysis, method},
  location  = {Austin, Texas, USA},
  numpages  = {3},
  url       = {https://doi.org/10.1145/2668930.2688818},
}

@InProceedings{Uhlir2016,
  author    = {Uhlir, Vojtech and Tomanek, Ondrej and Kencl, Lukas},
  booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
  title     = {Latency-Based Benchmarking of Cloud Service Providers},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {263–268},
  publisher = {Association for Computing Machinery},
  series    = {UCC '16},
  abstract  = {With the ever-increasing trend of migration of applications to the Cloud environment,
there is a growing need to thoroughly evaluate quality of the Cloud service itself,
before deciding upon a hosting provider. Benchmarking the Cloud services is difficult
though, due to the complex nature of the Cloud Computing setup and the diversity of
locations, of applications and of their specific service requirements. However, such
comparison may be crucial for decision making and for troubleshooting of services
offered by the intermediate businesses - the so-called Cloud tenants. Existing cross-sectional
studies and benchmarking methodologies provide only a shallow comparison of Cloud
services, whereas state-of-the-art tooling for specific comparisons of application-performance
parameters, such as for example latency, is insufficient. In this work, we propose
a novel methodology for benchmarking of Cloud-service providers, which is based on
latency measurements collected via active probing, and can be tailored to specific
application needs. Furthermore, we demonstrate its applicability on a practical longitudinal
study of real measurements of two major Cloud-service providers - Amazon and Microsoft.},
  doi       = {10.1145/2996890.3007870},
  isbn      = {9781450346160},
  location  = {Shanghai, China},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2996890.3007870},
}

@InProceedings{Sacco2020,
  author    = {Sacco, Alessio and Esposito, Flavio and Marchetto, Guido},
  booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
  title     = {A Distributed Reinforcement Learning Approach for Energy and Congestion-Aware Edge Networks},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {546–547},
  publisher = {Association for Computing Machinery},
  series    = {CoNEXT '20},
  abstract  = {The abiding attempt of automation has also pervaded computer networks, with the ability
to measure, analyze, and control themselves in an automated manner, by reacting to
changes in the environment (e.g., demand) while exploiting existing flexibilities.
When provided with these features, networks are often referred to as "self-driving".
Network virtualization and machine learning are the drivers. In this regard, the provision
and orchestration of physical or virtual resources are crucial for both Quality of
Service guarantees and cost management in the edge/cloud computing ecosystem. Auto-scaling
mechanisms are hence essential to effectively manage the lifecycle of network resources.
In this poster, we propose Relevant, a distributed reinforcement learning approach
to enable distributed automation for network orchestrators. Our solution aims at solving
the congestion control problem within Software-Defined Network infrastructures, while
being mindful of the energy consumption, helping resources to scale up and down as
traffic demands fluctuate and energy optimization opportunities arise.},
  doi       = {10.1145/3386367.3431670},
  isbn      = {9781450379489},
  keywords  = {self-driving networks, reinforcement learning, auto-scaling},
  location  = {Barcelona, Spain},
  numpages  = {2},
  url       = {https://doi.org/10.1145/3386367.3431670},
}

@InProceedings{Kuang2015a,
  author    = {Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
  booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Modeling Cross-Architecture Co-Tenancy Performance Interference},
  year      = {2015},
  pages     = {231–240},
  publisher = {IEEE Press},
  series    = {CCGRID '15},
  abstract  = {Cloud computing has become a dominant computing paradigm to provide elastic, affordable
computing resources to end users. Due to the increased computing power of modern machines
powered by multi/many-core computing, data centers often co-locate multiple virtual
machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing
and competition. Applications or VMs co-locating in one physical machine can interfere
with each other despite of the promise of performance isolation through virtualization.
Modeling and predicting co-run interference therefore becomes critical for data center
job scheduling and QoS (Quality of Service) assurance. Co-run interference can be
categorized into two metrics, sensitivity and pressure, where the former denotes how
an application's performance is affected by its co-run applications, and the latter
measures how it impacts the performance of its co-run applications. This paper shows
that sensitivity and pressure are both application- and architecture-dependent. Further,
we propose a regression model that predicts an application's sensitivity and pressure
across architectures with high accuracy. This regression model enables a data center
scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate
with another VMs/applications.},
  doi       = {10.1109/CCGrid.2015.152},
  isbn      = {9781479980062},
  location  = {Shenzhen, China},
  numpages  = {10},
  url       = {https://doi.org/10.1109/CCGrid.2015.152},
}

@Article{Avgeris2019,
  author     = {Avgeris, Marios and Dechouniotis, Dimitrios and Athanasopoulos, Nikolaos and Papavassiliou, Symeon},
  journal    = {ACM Trans. Internet Technol.},
  title      = {Adaptive Resource Allocation for Computation Offloading: A Control-Theoretic Approach},
  year       = {2019},
  issn       = {1533-5399},
  month      = apr,
  number     = {2},
  volume     = {19},
  abstract   = {Although mobile devices today have powerful hardware and networking capabilities,
they fall short when it comes to executing compute-intensive applications. Computation
offloading (i.e., delegating resource-consuming tasks to servers located at the edge
of the network) contributes toward moving to a mobile cloud computing paradigm. In
this work, a two-level resource allocation and admission control mechanism for a cluster
of edge servers offers an alternative choice to mobile users for executing their tasks.
At the lower level, the behavior of edge servers is modeled by a set of linear systems,
and linear controllers are designed to meet the system’s constraints and quality of
service metrics, whereas at the upper level, an optimizer tackles the problems of
load balancing and application placement toward the maximization of the number the
offloaded requests. The evaluation illustrates the effectiveness of the proposed offloading
mechanism regarding the performance indicators, such as application average response
time, and the optimal utilization of the computational resources of edge servers.},
  address    = {New York, NY, USA},
  articleno  = {23},
  doi        = {10.1145/3284553},
  issue_date = {April 2019},
  keywords   = {feedback control, Edge computing, linear modeling},
  numpages   = {20},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3284553},
}

@InProceedings{Ilyushkin2017,
  author    = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Papadopoulos, Alessandro V. and Ghit, Bogdan and Epema, Dick and Iosup, Alexandru},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
  title     = {An Experimental Performance Evaluation of Autoscaling Policies for Complex Workflows},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {75–86},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '17},
  abstract  = {Simplifying the task of resource management and scheduling for customers, while still
delivering complex Quality-of-Service (QoS), is key to cloud computing. Many autoscaling
policies have been proposed in the past decade to decide on behalf of cloud customers
when and how to provision resources to a cloud application utilizing cloud elasticity
features. However, in prior work, when a new policy is proposed, it is seldom compared
to the state-of-the-art, and is often compared only to static provisioning using a
predefined QoS target. This reduces the ability of cloud customers and of cloud operators
to choose and deploy an autoscaling policy. In our work, we conduct an experimental
performance evaluation of autoscaling policies, using as application model workflows,
a commonly used formalism for automating resource management for applications with
well-defined yet complex structure. We present a detailed comparative study of general
state-of-the-art autoscaling policies, along with two new workflow-specific policies.
To understand the performance differences between the 7 policies, we conduct various
forms of pairwise and group comparisons. We report both individual and aggregated
metrics. Our results highlight the trade-offs between the suggested policies, and
thus enable a better understanding of the current state-of-the-art.},
  doi       = {10.1145/3030207.3030214},
  isbn      = {9781450344043},
  keywords  = {dag, cloud computing, metrics, supply, workflows, auto-scaling, scheduling, elasticity, clouds, opennebula, spec, demand, autoscaling, level of parallelism, performance, directed acyclic graph, workloads},
  location  = {L'Aquila, Italy},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3030207.3030214},
}

@InProceedings{Slivar2016,
  author    = {Slivar, Ivan and Skorin-Kapov, Lea and Suznjevic, Mirko},
  booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
  title     = {Cloud Gaming QoE Models for Deriving Video Encoding Adaptation Strategies},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {MMSys '16},
  abstract  = {Cloud gaming has been recognized as a promising shift in the online game industry,
with the aim being to deliver high-quality graphics games to any type of end user
device. The concepts of cloud computing are leveraged to render the game scene as
a video stream which is then delivered to players in real-time. Given high bandwidth
and strict latency requirements, a key challenge faced by cloud game providers lies
in configuring the video encoding parameters so as to maximize player Quality of Experience
(QoE) while meeting bandwidth availability constraints. In this paper we address this
challenge by conducting a subjective laboratory study involving 52 players and two
different games aimed at identifying QoE-driven video encoding adaptation strategies.
Empirical results are used to derive analytical QoE estimation models as functions
of bitrate and framerate, while also taking into account game type and player skill.
Results have shown that under certain identified bandwidth conditions, reductions
of framerate lead to QoE improvements due to improved graphics quality. Given that
results indicate that different QoE-driven video adaptation policies should likely
be applied for different types of games, we further report on objective video metrics
that may be used to classify games for the purpose of choosing an appropriate and
QoE-driven video codec configuration strategy.},
  articleno = {18},
  doi       = {10.1145/2910017.2910602},
  isbn      = {9781450342971},
  keywords  = {cloud gaming QoE, cloud gaming, QoE modeling, QoE},
  location  = {Klagenfurt, Austria},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2910017.2910602},
}

@InProceedings{Sun2016,
  author    = {Sun, Degang and Zhang, Jie and Fan, Wei and Wang, Tingting and Liu, Chao and Huang, Weiqing},
  booktitle = {Proceedings of the 4th ACM International Workshop on Security in Cloud Computing},
  title     = {SPLM: Security Protection of Live Virtual Machine Migration in Cloud Computing},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {2–9},
  publisher = {Association for Computing Machinery},
  series    = {SCC '16},
  abstract  = {Virtual machine live migration technology, as an important support for cloud computing,
has become a central issue in recent years. The virtual machines' runtime environment
is migrated from the original physical server to another physical server, maintaining
the virtual machines running at the same time. Therefore, it can make load balancing
among servers and ensure the quality of service. However, virtual machine migration
security issue cannot be ignored due to the immature development of it. This paper
we analyze the security threats of the virtual machine migration, and compare the
current proposed protection measures. While, these methods either rely on hardware,
or lack adequate security and expansibility. In the end, we propose a security model
of live virtual machine migration based on security policy transfer and encryption,
named as SPLM (Security Protection of Live Migration) and analyze its security and
reliability, which proves that SPLM is better than others. This paper can be useful
for the researchers to work on this field. The security study of live virtual machine
migration in this paper provides a certain reference for the research of virtualization
security, and is of great significance.},
  doi       = {10.1145/2898445.2898446},
  isbn      = {9781450342858},
  keywords  = {live migration, virtual machine, virtualization, cloud computing, security},
  location  = {Xi'an, China},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2898445.2898446},
}

@Article{Slivar2018,
  author     = {Slivar, Ivan and Suznjevic, Mirko and Skorin-Kapov, Lea},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {Game Categorization for Deriving QoE-Driven Video Encoding Configuration Strategies for Cloud Gaming},
  year       = {2018},
  issn       = {1551-6857},
  month      = jun,
  number     = {3s},
  volume     = {14},
  abstract   = {Cloud gaming has been recognized as a promising shift in the online game industry,
with the aim of implementing the “on demand” service concept that has achieved market
success in other areas of digital entertainment such as movies and TV shows. The concepts
of cloud computing are leveraged to render the game scene as a video stream that is
then delivered to players in real-time. The main advantage of this approach is the
capability of delivering high-quality graphics games to any type of end user device;
however, at the cost of high bandwidth consumption and strict latency requirements.
A key challenge faced by cloud game providers lies in configuring the video encoding
parameters so as to maximize player Quality of Experience (QoE) while meeting bandwidth
availability constraints. In this article, we tackle one aspect of this problem by
addressing the following research question: Is it possible to improve service adaptation
based on information about the characteristics of the game being streamed? To answer
this question, two main challenges need to be addressed: the need for different QoE-driven
video encoding (re-)configuration strategies for different categories of games, and
how to determine a relevant game categorization to be used for assigning appropriate
configuration strategies. We investigate these problems by conducting two subjective
laboratory studies with a total of 80 players and three different games. Results indicate
that different strategies should likely be applied for different types of games, and
show that existing game classifications are not necessarily suitable for differentiating
game types in this context. We thus further analyze objective video metrics of collected
game play video traces as well as player actions per minute and use this as input
data for clustering of games into two clusters. Subjective results verify that different
video encoding configuration strategies may be applied to games belonging to different
clusters.},
  address    = {New York, NY, USA},
  articleno  = {56},
  doi        = {10.1145/3132041},
  issue_date = {August 2018},
  keywords   = {Cloud gaming, video codec configuration strategies, game categorization, Quality of Experience},
  numpages   = {24},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3132041},
}

@Article{Ilyushkin2018,
  author     = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Bauer, Andr\'{e} and Papadopoulos, Alessandro V. and Epema, Dick and Iosup, Alexandru},
  journal    = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
  title      = {An Experimental Performance Evaluation of Autoscalers for Complex Workflows},
  year       = {2018},
  issn       = {2376-3639},
  month      = apr,
  number     = {2},
  volume     = {3},
  abstract   = {Elasticity is one of the main features of cloud computing allowing customers to scale
their resources based on the workload. Many autoscalers have been proposed in the
past decade to decide on behalf of cloud customers when and how to provision resources
to a cloud application based on the workload utilizing cloud elasticity features.
However, in prior work, when a new policy is proposed, it is seldom compared to the
state-of-the-art, and is often compared only to static provisioning using a predefined
quality of service target. This reduces the ability of cloud customers and of cloud
operators to choose and deploy an autoscaling policy, as there is seldom enough analysis
on the performance of the autoscalers in different operating conditions and with different
applications. In our work, we conduct an experimental performance evaluation of autoscaling
policies, using as application model workflows, a popular formalism for automating
resource management for applications with well-defined yet complex structures. We
present a detailed comparative study of general state-of-the-art autoscaling policies,
along with two new workflow-specific policies. To understand the performance differences
between the seven policies, we conduct various experiments and compare their performance
in both pairwise and group comparisons. We report both individual and aggregated metrics.
As many workflows have deadline requirements on the tasks, we study the effect of
autoscaling on workflow deadlines. Additionally, we look into the effect of autoscaling
on the accounted and hourly based charged costs, and we evaluate performance variability
caused by the autoscaler selection for each group of workflow sizes. Our results highlight
the trade-offs between the suggested policies, how they can impact meeting the deadlines,
and how they perform in different operating conditions, thus enabling a better understanding
of the current state-of-the-art.},
  address    = {New York, NY, USA},
  articleno  = {8},
  doi        = {10.1145/3164537},
  issue_date = {April 2018},
  keywords   = {elasticity, benchmarking, metrics, Autoscaling, scientific workflows},
  numpages   = {32},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3164537},
}

@InProceedings{Michael2017,
  author    = {Michael, Nicolas and Ramannavar, Nitin and Shen, Yixiao and Patil, Sheetal and Sung, Jan-Lung},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
  title     = {CloudPerf: A Performance Test Framework for Distributed and Dynamic Multi-Tenant Environments},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {189–200},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '17},
  abstract  = {The evolution of cloud-computing imposes many challenges on performance testing and
requires not only a different approach and methodology of performance evaluation and
analysis, but also specialized tools and frameworks to support such work. In traditional
performance testing, typically a single workload was run against a static test configuration.
The main metrics derived from such experiments included throughput, response times,
and system utilization at steady-state. While this may have been sufficient in the
past, where in many cases a single application was run on dedicated hardware, this
approach is no longer suitable for cloud-based deployments. Whether private or public
cloud, such environments typically host a variety of applications on distributed shared
hardware resources, simultaneously accessed by a large number of tenants running heterogeneous
workloads. The number of tenants as well as their activity and resource needs dynamically
change over time, and the cloud infrastructure reacts to this by reallocating existing
or provisioning new resources. Besides metrics such as the number of tenants and overall
resource utilization, performance testing in the cloud must be able to answer many
more questions: How is the quality of service of a tenant impacted by the constantly
changing activity of other tenants? How long does it take the cloud infrastructure
to react to changes in demand, and what is the effect on tenants while it does so?
How well are service level agreements met? What is the resource consumption of individual
tenants? How can global performance metrics on application- and system-level in a
distributed system be correlated to an individual tenant's perceived performance?In
this paper we present CloudPerf, a performance test framework specifically designed
for distributed and dynamic multi-tenant environments, capable of answering all of
the above questions, and more. CloudPerf consists of a distributed harness, a protocol-independent
load generator and workload modeling framework, an extensible statistics framework
with live-monitoring and post-analysis tools, interfaces for cloud deployment operations,
and a rich set of both low-level as well as high-level workloads from different domains.},
  doi       = {10.1145/3030207.3044530},
  isbn      = {9781450344043},
  keywords  = {cloud, multi-tenancy, statistics collection, load generation, performance testing, workload modeling},
  location  = {L'Aquila, Italy},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3030207.3044530},
}

@InProceedings{Lv2018,
  author    = {Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
  booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
  title     = {CounterMiner: Mining Big Performance Data from Hardware Counters},
  year      = {2018},
  pages     = {613–626},
  publisher = {IEEE Press},
  series    = {MICRO-51},
  abstract  = {Modern processors typically provide a small number of hardware performance counters
to capture a large number of microarchitecture events1. These counters can easily
generate a huge amount (e.g., GB or TB per day) of data, which we call big performance
data in cloud computing platforms with more than thousands of servers and millions
of complex workloads running ina"24/7/365" manner. The big performance data provides
a precious foundation for root cause analysis of performance bottlenecks, architecture
and compiler optimization, and many more. However, it is challenging to extract value
from the big performance data due to: 1) the many unperceivable errors (e.g., outliers
and missing values); and 2) the difficulty of obtaining insights, e.g., relating events
to performance.In this paper, we propose CounterMiner, a rigorous methodology that
enables the measurement and understanding of big performance data by using data mining
and machine learning techniques. It includes three novel components: 1) using data
cleaning to improve data quality by replacing outliers and filling in missing values;
2) iteratively quantifying, ranking, and pruning events based on their importance
with respect to performance; 3) quantifying interaction intensity between two events
by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from
the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results
show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing
10 events on 4 hardware counters. We also conduct a real-world case study, showing
that identifying important configuration parameters of Spark programs by event importance
is much faster than directly ranking the importance of these parameters.},
  doi       = {10.1109/MICRO.2018.00056},
  isbn      = {9781538662403},
  keywords  = {performance counters, computer architecture, big data, data mining},
  location  = {Fukuoka, Japan},
  numpages  = {14},
  url       = {https://doi.org/10.1109/MICRO.2018.00056},
}

@Proceedings{2016a,
  title     = {ICPE '16 Companion: Companion Publication for ACM/SPEC on International Conference on Performance Engineering},
  year      = {2016},
  address   = {New York, NY, USA},
  isbn      = {9781450341479},
  publisher = {Association for Computing Machinery},
  abstract  = {The 7th ACM/SPEC International Conference on Performance Engineering (ICPE 2016) takes
place in Delft in The Netherlands in March 2016. The conference grew out of the ACM
Workshop on Software Performance (WOSP since 1998) and the SPEC International Performance
Engineering Workshop (SIPEW since 2008), with the goal of integrating theory and practice
in the field of performance engineering. It is a great pleasure for us to offer an
outstanding technical program this year, which we believe will allow researchers and
practitioners to present their visions and latest innovation, and to exchange ideas
within the community.Overall, we received 89 high quality submissions across all three
tracks. The main Research Track attracted 57 submissions with 19 accepted (33% acceptance
rate) for presentation at the conference. Among them were 16 full papers and three
short papers. Each paper received at least three reviews from experienced program
committee members. In the Work-In-Progress and Vision Track, six out of 15 contributions
were selected. The Industry and Experience Track received 17 submissions, of which
seven were selected for inclusion in the program. The accepted papers were organized
into five research track sessions, two industry track sessions, and one WiP and vision
track session. Three best paper candidates were also selected: two research papers
and one industry paper.We are proud to have three excellent keynote speakers as part
of our technical program: Bianca Schroeder from University of Toronto, Canada, presenting
"Case studies from the real world: The importance of measurement and analysis in building
better systems"Wilhelm Hasselbring from Kiel University, Germany, discussing "Microservices
for Scalability"Angelo Corsaro, Chief Technology Officer at PrismTech, talking about
"Cloudy, Foggy and Misty Internet of Things"In addition, the program includes four
tutorials, a doctoral symposium, a poster and demo track, the SPEC Distinguished Dissertation
Award, and three interesting workshops, including the International Workshop on Large-Scale
Testing (LT), the 2nd International Workshop on Performance Analysis of Big data Systems
(PABS), and the 2nd Workshop on Challenges in Performance Methods for Software Development
(WOSPC).The program covers traditional ICPE topics such as software and systems performance
modeling and prediction, analysis and optimization, characterization and profiling,
as well as application of performance engineering theory and techniques to several
practical fields, including distributed systems, cloud computing, storage, energy,
big data, virtualized systems and containers.},
  location  = {Delft, The Netherlands},
}

@Proceedings{2016b,
  title     = {ICPE '16: Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
  year      = {2016},
  address   = {New York, NY, USA},
  isbn      = {9781450340809},
  publisher = {Association for Computing Machinery},
  abstract  = {The 7th ACM/SPEC International Conference on Performance Engineering (ICPE 2016) takes
place in Delft in The Netherlands in March 2016. The conference grew out of the ACM
Workshop on Software Performance (WOSP since 1998) and the SPEC International Performance
Engineering Workshop (SIPEW since 2008), with the goal of integrating theory and practice
in the field of performance engineering. It is a great pleasure for us to offer an
outstanding technical program this year, which we believe will allow researchers and
practitioners to present their visions and latest innovation, and to exchange ideas
within the community.Overall, we received 89 high quality submissions across all three
tracks. The main Research Track attracted 57 submissions with 19 accepted (33% acceptance
rate) for presentation at the conference. Among them were 16 full papers and three
short papers. Each paper received at least three reviews from experienced program
committee members. In the Work-In-Progress and Vision Track, six out of 15 contributions
were selected. The Industry and Experience Track received 17 submissions, of which
seven were selected for inclusion in the program. The accepted papers were organized
into five research track sessions, two industry track sessions, and one WiP and vision
track session. Three best paper candidates were also selected: two research papers
and one industry paper.We are proud to have three excellent keynote speakers as part
of our technical program: Bianca Schroeder from University of Toronto, Canada, presenting
"Case studies from the real world: The importance of measurement and analysis in building
better systems"Wilhelm Hasselbring from Kiel University, Germany, discussing "Microservices
for Scalability"Angelo Corsaro, Chief Technology Officer at PrismTech, talking about
"Cloudy, Foggy and Misty Internet of Things"In addition, the program includes four
tutorials, a doctoral symposium, a poster and demo track, the SPEC Distinguished Dissertation
Award, and three interesting workshops, including the International Workshop on Large-Scale
Testing (LT), the 2nd International Workshop on Performance Analysis of Big data Systems
(PABS), and the 2nd Workshop on Challenges in Performance Methods for Software Development
(WOSPC).The program covers traditional ICPE topics such as software and systems performance
modeling and prediction, analysis and optimization, characterization and profiling,
as well as application of performance engineering theory and techniques to several
practical fields, including distributed systems, cloud computing, storage, energy,
big data, virtualized systems and containers.},
  location  = {Delft, The Netherlands},
}

@InProceedings{Buchet2015,
  author    = {Buchet, Micka\"{e}l and Chazal, Fr\'{e}d\'{e}ric and Oudot, Steve Y. and Sheehy, Donald R.},
  booktitle = {Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms},
  title     = {Efficient and Robust Persistent Homology for Measures},
  year      = {2015},
  address   = {USA},
  pages     = {168–180},
  publisher = {Society for Industrial and Applied Mathematics},
  series    = {SODA '15},
  abstract  = {A new paradigm for point cloud data analysis has emerged recently, where point clouds
are no longer treated as mere compact sets but rather as empirical measures. A notion
of distance to such measures has been defined and shown to be stable with respect
to perturbations of the measure. This distance can easily be computed pointwise in
the case of a point cloud, but its sublevel-sets, which carry the geometric information
about the measure, remain hard to compute or approximate. This makes it challenging
to adapt many powerful techniques based on the Euclidean distance to a point cloud
to the more general setting of the distance to a measure on a metric space.We propose
an efficient and reliable scheme to approximate the topological structure of the family
of sublevel-sets of the distance to a measure. We obtain an algorithm for approximating
the persistent homology of the distance to an empirical measure that works in arbitrary
metric spaces. Precise quality and complexity guarantees are given with a discussion
on the behavior of our approach in practice.},
  location  = {San Diego, California},
  numpages  = {13},
}

@InProceedings{Karadimce2017,
  author    = {Karadimce, Aleksandar and Davcev, Danco},
  booktitle = {Proceedings of the 15th International Conference on Advances in Mobile Computing &amp; Multimedia},
  title     = {Bayesian Network Model for Estimating User Satisfaction of Multimedia Cloud Services},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {3–12},
  publisher = {Association for Computing Machinery},
  series    = {MoMM2017},
  abstract  = {The focus of this research is given to find a metric and determine the quality of
the offered multimedia cloud services from an end users perception. The Quality of
Experience (QoE) has been introduced to measure the quality features, which is used
to determine the end-to-end user perceived quality of the used multimedia service.
In this study, we have used students satisfaction survey, which provides direct subjective
data on need, habits, and frequency of using different multimedia services. This data
has been used for validation of the proposed Bayesian Network model for interactive
estimation of the acceptability of multimedia cloud services based on the user preferences.},
  doi       = {10.1145/3151848.3151850},
  isbn      = {9781450353007},
  keywords  = {Quality of Experience, mobile cloud services, Bayesian Network, survey evaluation},
  location  = {Salzburg, Austria},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3151848.3151850},
}

@Proceedings{2018,
  title     = {SIGMOD '18: Proceedings of the 2018 International Conference on Management of Data},
  year      = {2018},
  address   = {New York, NY, USA},
  isbn      = {9781450347037},
  publisher = {Association for Computing Machinery},
  abstract  = {It is our pleasure to share with you the proceedings of SIGMOD 2018, the 44th ACM
SIGMOD International Conference on Management of Data, in Houston, Texas. For many
people, the words 'Houston, Texas' conjure up images of cowboy hats and oil rigs.
This is not without reason. More than 20 Fortune 500 oil and gas companies are headquartered
in Houston, and Texas beef is legendary. But less appreciated is that Houston is a
vibrant and diverse city. By the usual metrics it is the most racially and ethnically
diverse city in the United States. That diversity helps to make Houston a foodie's
paradise, with wonderful Mexican, Tex-Mex, Vietnamese, and Chinese restaurants, and
great Southern options, such as soul food and Cajun. Not to mention the best Texas-style
barbecue!The SIGMOD conference is being held at the Marriott Marquis Houston, overlooking
downtown Houston's Discovery Green park. Adjacent to Discovery Green are Minute Maid
Park and the Toyota Center, home of baseball's Houston Astros and basketball's Houston
Rockets, respectively. The conference banquet is at Minute Maid Park. Downtown Houston
is a short car or train ride from great Houston museum district attractions such as
the Menil Collection and the world-class shopping of the Houston Galleria area. And
to repeat, everywhere you go in Houston, you'll find great food!This year's technical
program features 90 research papers selected from 461 submissions, 15 industrial papers
selected from 40 submissions, two invited industrial papers, 35 demonstration papers
selected from 108 submissions, and 5 tutorials selected from 14 submissions (two of
which were merged into a 2-session tutorial). There are 15 research sessions, 4 industry
sessions, an invited special session, and two demonstration sessions. The two invited
keynotes were chosen to broaden the SIGMOD community's understanding of areas having
a major effect on data management: Eric Brewer, VP of Infrastructure at Google and
faculty member at UC Berkeley, talking about the effect of container technology on
cloud computing; and Pedro Domingos, Professor at University of Washington, talking
about machine learning-what works, what doesn't, and where the field is headed. Like
last year, the keynotes are followed by a plenary session of teaser talks, where each
presenter gives a one-minute summary of their paper, to give attendees a high-level
view of the conference and help them decide which sessions to attend.There are two
changes in the session organization from recent years, whose goal is to make the program
more compact and interesting for attendees. First, tutorials are presented during
the main conference on Tuesday through Thursday, rather than on Friday after the main
conference is over. Second, to ensure there are at most four parallel sessions in
each time slot, each research paper presentation is allocated either 20 minutes or
10 minutes. The decision of long vs. short presentations had several phases. During
the reviewing process, PC members were asked to recommend whether each paper, if accepted,
should be a long or short presentation. Then research PC group leaders made a recommendation
for each of the accepted papers they supervised -- definitely 20 minutes, 20 minutes
if there's time available, borderline, or definitely 10 minutes -- based on reviews,
reviewer discussions, and their own judgment, without knowing the identity of authors.
Their recommendation is not necessarily a quality metric. They recommended 'definitely
10' for some papers highly-rated by reviewers, because the topic was narrow, could
be explained in 10 minutes, or couldn't be explained in 20 minutes so extra time wouldn't
help. For borderline papers, the final decision was based on many factors, such as
topic diversity, institutional diversity, and the time available in the relevant session.The
Research Program Committee consisted of a Program Chair, two Program Vice Chairs,
15 group leaders, and 173 Program Committee Members. There were two rounds of submissions,
with deadlines in July and November, respectively. Initially, each paper received
three reviews. Additional reviews were solicited in cases where the reviewers did
not have enough confidence, or where there was a significant score discrepancy in
the first three reviews. Papers were extensively discussed online. Of the 458 submissions,
20 were desk rejected (i.e., without reviews), 9 were accepted based on the first
round of reviews, and 327 were rejected. Authors of the remaining 102 papers were
asked to revise their papers to address reviewers' criticisms; 81 of those revisions
were ultimately accepted. While the entire program committee worked hard to select
an excellent program, the chairs and area leaders are especially grateful to the following
program committee members for their very high quality work on the committee: Ashraf
Aboulnaga, Manos Athanassoulis, Sebastian Breβ, Graham Cormode, Sudipto Das, Khuzaima
Daudjee, Aaron Elmore, Ada Fu, Michael Hay, Yuxiong He, Yannis Katsis, Alexandra Meliou,
Dan Olteanu, Andrew Pavlo, Peter Pietzuch, Lucian Popa, Semih Salihoglu, Ryan Stutsman,
Yufei Tao, and Alexander Thomson.The program also includes industry papers, demonstrations,
tutorials, workshops, a Student Research Competition, and a New Researcher Symposium.
We thank the organizers of all the technical events, including research PC vice-chairs
Xin Luna Dong and Mohamed Mokbel, industrial PC chairs Samuel Madden and Neoklis Polyzotis,
demonstration chairs Georgia Koutrika and Feifei Li, tutorial chairs Ihab Ilyas and
Stratis Viglas, workshop chairs Ihab Ilyas and Benny Kimelfeld, Student Research Competition
chairs Alvin Cheung and Jana Giceva, and New Research Symposium chairs Katja Hose
and Eugene Wu. We are also grateful to the CMT team, who modified their reviewing
system to accommodate new aspects of this year's PC process.},
  location  = {Houston, TX, USA},
}

@InProceedings{Esposito2019a,
  author    = {Esposito, Christian and Pop, Florin and Choi, Chang},
  booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
  title     = {Session Details: Theme: Information Systems: SFECS - Sustainability of Fog/Edge Computing Systems Track},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SAC '19},
  abstract  = {Fog/Edge Computing paradigms are widely used in enterprises to address the emerging
challenges of big data analysis, because of their underlying scalable, flexible and
distributed data management schemes. The data centers in the Clouds are facing great
challenges on the burden of the consequent increasing the amount of data to be man-
aged and the additional requirements of location awareness and low latency at the
edge of network necessary by smart cites and factories. These are the reasons why
a centralized model cannot be an efficient solution for generated or required data
by the IoT devices in those applications and there is the progressive shift towards
fog nodes and smarted edge nodes mediating between the cloud and the IoT devices.
The Fog/Edge computing paradigm is a decentralized model that transfers a part of
low computing data analysis from the cloud to the intermediate (fog) nodes or the
edges, performing only high computing tasks in the cloud. This new approach tries
to minimize the three factors that negatively compromise the effective and efficient
application of the Cloud computing to smart cities and factories, or similar application
domains: the network bandwidth usage, decentralization of the data processing tasks
and reduced response latency for clients (IoT devices). Fog/Edge computing is a hierarchical
approach where the overall infrastructure is structured in multiple layers, each responsible
of offering a good coordination and data management to the nodes at the lower layer.
The lowest layer is usually composed of sensors and/or actuators that measure and/or
control the environment or a given business process, implemented as mobile devices
that are running a sensing/controlling application. In this case, combining Sustainable
computing with Fog and Edge computing represents a new approach for increasing quality-of-
service and efficiency of the system, creating the capability to present temporal
and geo-coded information, and increasing innovation, and co-designing sustainable
future large scale distributed systems. This new paradigm appears to offer a good
approach in handling the scale factor of the data size, reducing the network bandwidth
usage and the response latency of the system. In order to support specifically the
Fog/Edge architectures, there is a need, for instance, of location-awareness and computation
placement, replication and recovery. In many cases Edge resources would be required
for both computation and data storage to address the time and locality constraints.
There are multiple kinds of orchestration management solutions for virtualization
in this type of architecture with different characteristics and drawbacks. This results
in different restrictions for application definition, scalability, availability, load
balancing and so on. Also, virtualization may be needed at multiple levels in a Fog/Edge
architecture as it consists of the following levels of abstraction: at the sensing
level we have the IoT devices/smart things, at the Edge level there are the gateways
to a first collection and the data from the IoT devices and their preliminary processing,
at the Fog level we have an additional data management layer, and at the Cloud level
there is the compute/storage infrastructure with applications on top. Last, but not
least, the energy efficiency is particularly important at the IoT and edge level since
the devices may be equipped with a limited battery, possible difficult or impossible
to be charged. So, optimizing the energy consumption is a must. To address several
open research is- sues regarding sustainability of future Fog/Edge systems, this track
aims at solicit contributions highlighting challenges, state-of-the-art, and solutions
to a set of currently unresolved key questions including - but not limited to - performance,
modeling, optimization, energy-efficiency, reliability, security, privacy and techno-economic
aspects of Fog/Edge systems. Through addressing these concerns while understanding
their impacts and limitations, technological advancements will be channeled toward
more sustainable/efficient platforms for tomorrow's ever-connected systems.},
  doi       = {10.1145/3329391},
  isbn      = {9781450359337},
  location  = {Limassol, Cyprus},
  url       = {https://doi.org/10.1145/3329391},
}

@InProceedings{Nobre2018,
  author    = {Nobre, Ricardo and Reis, Lu\'{\i}s and Bispo, Jo\~{a}o and Carvalho, Tiago and Cardoso, Jo\~{a}o M.P. and Cherubin, Stefano and Agosta, Giovanni},
  booktitle = {Proceedings of the 9th Workshop and 7th Workshop on Parallel Programming and RunTime Management Techniques for Manycore Architectures and Design Tools and Architectures for Multicore Embedded Computing Platforms},
  title     = {Aspect-Driven Mixed-Precision Tuning Targeting GPUs},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {26–31},
  publisher = {Association for Computing Machinery},
  series    = {PARMA-DITAM '18},
  abstract  = {Writing mixed-precision kernels allows to achieve higher throughput together with
outputs whose precision remain within given limits. The recent introduction of native
half-precision arithmetic capabilities in several GPUs, such as NVIDIA P100 and AMD
Vega 10, contributes to make precision-tuning even more relevant as of late. However,
it is not trivial to manually find which variables are to be represented as half-precision
instead of single- or double-precision. Although the use of half-precision arithmetic
can speed up kernel execution considerably, it can also result in providing non-usable
kernel outputs, whenever the wrong variables are declared using the half-precision
data-type. In this paper we present an automatic approach for precision tuning. Given
an OpenCL kernel with a set of inputs declared by a user (i.e., the person responsible
for programming and/or tuning the kernel), our approach is capable of deriving the
mixed-precision versions of the kernel that are better improve upon the original with
respect to a given metric (e.g., time-to-solution, energy-to-solution). We allow the
user to declare and/or select a metric to measure and to filter solutions based on
the quality of the output. We implement a proof-of-concept of our approach using an
aspect-oriented programming language called LARA. It is capable of generating mixed-precision
kernels that result in considerably higher performance when compared with the original
single-precision floating-point versions, while generating outputs that can be acceptable
in some scenarios.},
  doi       = {10.1145/3183767.3183776},
  isbn      = {9781450364447},
  keywords  = {mixed-precision, aspect-driven, GPGPU},
  location  = {Manchester, United Kingdom},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3183767.3183776},
}

@Article{Squillante2014,
  author     = {Squillante, Mark S.},
  journal    = {SIGMETRICS Perform. Eval. Rev.},
  title      = {Session Details: Special Issue on the Workshop on MAthematical Performance Modeling and Analysis (MAMA 2014)},
  year       = {2014},
  issn       = {0163-5999},
  month      = sep,
  number     = {2},
  volume     = {42},
  abstract   = {The complexity of computer systems, networks and applications, as well as the advancements
in computer technology, continue to grow at a rapid pace. Mathematical analysis, modeling
and optimization have been playing, and continue to play, an important role in research
studies to investigate fundamental issues and trade-offs at the core of performance
problems in the design and implementation of complex computer systems, networks and
applications.On June 20, 2014, the 16th Workshop on MAthematical performance Modeling
and Analysis (MAMA 2014) was held in Austin TX, USA, sponsored by ACM SIGMETRICS,
and held in conjunction with SIGMETRICS 2014. This workshop seeks to bring together
researchers working on the mathematical, methodological and theoretical aspects of
performance analysis, modeling and optimization. It is intended to provide a forum
at SIGMETRICS conferences for talks on early research in the more mathematical areas
of computer performance analysis. These talks tend to be based on very recent research
results (including work in progress) or on new research results that will be otherwise
submitted only to a journal (or recently have been submitted to a journal). Thus,
part of the goal is to complement and supplement the SIGMETRICS Conference program
with such talks without removing any theoretical contributions from the main technical
program. Furthermore, we continue to experience the desired result of having abstracts
from previous MAMA workshops appear as full papers in the main program of subsequent
SIGMETRICS and related conferences.All submissions were reviewed by at least 4 members
of the program committee, from which a total of 13 were selected for presentation
at the MAMA 2014 workshop. This special issue of Performance Evaluation Review includes
extended abstracts relating to these presentations (arranged in the order of their
presentation), which cover a wide range of topics in the area of mathematical performance
analysis, modeling and optimization.The study of Gelenbe examines the backlog of energy
and of data packets in a sensor node that harvests energy, computing the properties
of energy and data backlogs and discussing system stability. Meyfroyt derives asymptotic
results for the coverage ratio under a specific class of spatial stochastic models
(Cooperative Sequential Adsorption) and investigates the scalability of the Trickle
communication protocol algorithm. The study of Tune and Roughan applies the principle
of maximum entropy to develop fast traffic matrix synthesis models, with the future
goal of developing realistic spatio-temporal traffic matrices. Bradonji\'{c} et al. compare
and contrast the capacity, congestion and reliability requirements for alternative
connectivity models of large-scale data centers relative to fat trees. The study of
Rochman et al. considers the problem of resource placement in network applications,
based on a largescale service faced with regionally distributed demands for various
resources in cloud computing. Xie and Lui investigate the design and analysis of a
rating system and a mechanism to encourage users to participate in crowdsourcing and
to incentivize workers to develop high-quality solutions. The study of Asadi et al.
formulates a general problem for the joint per-user mode selection, connection activation
and resource scheduling of connections using both LTE and WiFi resources within the
context of device-todevice communications. Zheng and Tan consider a nonconvex joint
rate and power control optimization to achieve egalitarian fairness (max-min weighted
fairness) in wireless networks, exploiting the nonlinear Perron-Frobenius theory and
nonnegative matrix theory. The study of Goldberg et al. derives an asymptotically
optimal control policy for a stochastic capacity problem of dynamically matching supply
resources and uncertain demand, based on connections with lost-sales inventory models.
Ghaderi et al. investigate a dynamic stochastic bin packing problem, analyzing the
fluid limits of the system under an asymptotic best-fit algorithm and showing it asymptotically
minimizes the number of servers used in steady state. The study of Tizghadam and Leon-Garcia
examines the impact of overlaying or removing a subgraph on the Moore-Penrose inverse
of the Laplacian matrix of an existing network topology and proposes an iterative
method to find key performance measures. Miyazawa considers a two-node generalized
Jackson network in a phase-type setting as a special case of a Markov-modulated twodimensional
reflecting random walk and analyzes the tail asymptotics for this reflecting process.
The study of Squillante et al. investigates improvement in scalability of search in
networks through the use of multiple random walks, deriving bounds on the hitting
time to a set of nodes and on various performance metrics.},
  address    = {New York, NY, USA},
  doi        = {10.1145/3264284},
  issue_date = {September 2014},
  numpages   = {1},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3264284},
}

@InProceedings{Pegus2015,
  author    = {Pegus, Patrick and Cecchet, Emmanuel and Shenoy, Prashant},
  booktitle = {Proceedings of the 6th ACM Multimedia Systems Conference},
  title     = {Video BenchLab Demo: An Open Platform for Video Realistic Streaming Benchmarking},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {101–104},
  publisher = {Association for Computing Machinery},
  series    = {MMSys '15},
  abstract  = {In this demonstration, we present an open, flexible and realistic benchmarking platform
named Video BenchLab to measure the performance of streaming media workloads. While
Video BenchLab can be used with any existing media server, we provide a set of tools
for researchers to experiment with their own platform and protocols. The components
include a MediaDrop video server, a suite of tools to bulk insert videos and generate
streaming media workloads, a dataset of freely available video and a client runtime
to replay videos in the native video players of real Web browsers such as Firefox,
Chrome and Internet Explorer. Various metrics are collected to capture the quality
of video playback and identify issues that can happen during video replay. Finally,
we provide a Dashboard to manage experiments, collect results and perform analytics
to compare performance between experiments.The demonstration showcases all the BenchLab
video components including a MediaDrop server accessed by real web browsers running
locally and in the cloud. We demo the whole experiment lifecycle from creation to
deployment as well as result collection and analysis.},
  doi       = {10.1145/2713168.2723146},
  isbn      = {9781450333511},
  keywords  = {benchmarking, web browsers, video, streaming},
  location  = {Portland, Oregon},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2713168.2723146},
}

@Article{Li2017,
  author     = {Li, Ning and Jiang, Hong and Feng, Dan and Shi, Zhan},
  journal    = {ACM Trans. Storage},
  title      = {Customizable SLO and Its Near-Precise Enforcement for Storage Bandwidth},
  year       = {2017},
  issn       = {1553-3077},
  month      = feb,
  number     = {1},
  volume     = {13},
  abstract   = {Cloud service is being adopted as a utility for large numbers of tenants by renting
Virtual Machines (VMs). But for cloud storage, unpredictable IO characteristics make
accurate Service-Level-Objective (SLO) enforcement challenging. As a result, it has
been very difficult to support simple-to-use and technology-agnostic SLO specifying
a particular value for a specific metric (e.g., storage bandwidth). This is because
the quality of SLO enforcement depends on performance error and fluctuation that measure
the precision of SLO enforcement. High precision of SLO enforcement is critical for
user-oriented performance customization and user experiences. To address this challenge,
this article presents V-Cup, a framework for VM-oriented customizable SLO and its
near-precise enforcement. It consists of multiple auto-tuners, each of which exports
an interface for a tenant to customize the desired storage bandwidth for a VM and
enable the storage bandwidth of the VM to converge on the target value with a predictable
precision. We design and implement V-Cup in the Xen hypervisor based on the fair sharing
scheduler for VM-level resource management. Our V-Cup prototype evaluation shows that
it achieves satisfying performance guarantees through near-precise SLO enforcement.},
  address    = {New York, NY, USA},
  articleno  = {6},
  doi        = {10.1145/2998454},
  issue_date = {March 2017},
  keywords   = {storage management, end-to-end control, Cloud storage, service-level objective},
  numpages   = {25},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2998454},
}

@InProceedings{Pacot2020,
  author    = {Pacot, Mark Phil B. and Marcos, Nelson},
  booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
  title     = {Cloud Removal from Aerial Images Using Generative Adversarial Network with Simple Image Enhancement},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {77–81},
  publisher = {Association for Computing Machinery},
  series    = {ICIGP 2020},
  abstract  = {The atmospheric condition of the presence of clouds is one of the biggest problems
in most aerial imaging systems. It degrades the visual quality of images leading to
the loss of information for ground scenes. Hence, an effective cloud removal algorithm
is a significant factor for this kind of problem and other related applications. The
proposed cloud removal technique using the generative adversarial network with simple
image enhancement (SIE-GAN) is a useful tool in removing cloud formations, most notably
in images acquired using Unmanned Aerial Vehicle System (UAVs). This technique showed
flexibility in performing the given task with satisfactory results, which is a gauge
based on No-Reference Image Quality Metric, specifically the Perception-based Image
Quality Evaluator (PIQE). Also, the proposed algorithm outperformed some of existing
cloud removal algorithms by producing a better quality output when tested on the too-cloudy
satellite images. Overall, the authors introduced a new frontier in generating cloud-free
aerial images and added a valuable contribution to the array of cloud removal algorithms.},
  doi       = {10.1145/3383812.3383838},
  isbn      = {9781450377201},
  keywords  = {cloud removal, generative adversarial network, unmanned aerial vehicle system, no-reference image quality metric, simple image enhancement},
  location  = {Singapore, Singapore},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3383812.3383838},
}

@Article{GarciaDorado2017,
  author     = {Garc\'{\i}a-Dorado, Jos\'{e} Luis},
  journal    = {ACM Trans. Internet Technol.},
  title      = {Bandwidth Measurements within the Cloud: Characterizing Regular Behaviors and Correlating Downtimes},
  year       = {2017},
  issn       = {1533-5399},
  month      = aug,
  number     = {4},
  volume     = {17},
  abstract   = {The search for availability, reliability, and quality of service has led cloud infrastructure
customers to disseminate their services, contents, and data over multiple cloud data
centers, often involving several Cloud service providers (CSPs). The consequence of
this is that a large amount of data must be transmitted across the public Cloud. However,
little is known about the bandwidth dynamics involved. To address this, we have conducted
a measurement campaign for bandwidth between 18 data centers of four major CSPs. This
extensive campaign allowed us to characterize the resulting time series of bandwidth
as the addition of a stationary component and some infrequent excursions (typically
downtimes). While the former provides a description of the bandwidth users can expect
in the Cloud, the latter is closely related to the robustness of the Cloud (i.e.,
the occurrence of downtimes is correlated). Both components have been studied further
by applying factor analysis, specifically analysis of variance, as a mechanism to
formally compare data centers’ behaviors and extract generalities. The results show
that the stationary process is closely related to the data center locations and CSPs
involved in transfers that, fortunately, make the Cloud more predictable and allow
the set of reported measurements to be extrapolated. On the other hand, although correlation
in the Cloud is low, that is, only 10% of the measured pair of paths showed some correlation,
we found evidence that such correlation depends on the particular relationships between
pairs of data centers with little connection to more general factors. Positively,
this implies that data centers either in the same area or within the same CSP do not
show qualitatively more correlation than other data centers, which eases the deployment
of robust infrastructures. On the downside, this metric is scarcely generalizable
and, consequently, calls for exhaustive monitoring.},
  address    = {New York, NY, USA},
  articleno  = {39},
  doi        = {10.1145/3093893},
  issue_date = {September 2017},
  keywords   = {traffic correlation, inter-cloud, TCP bandwidth, ANOVA, Public cloud},
  numpages   = {25},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3093893},
}

@Article{Zhao2017,
  author     = {Zhao, Zhiwei and Dong, Wei and Bu, Jiajun and Gu, Tao and Min, Geyong},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {Accurate and Generic Sender Selection for Bulk Data Dissemination in Low-Power Wireless Networks},
  year       = {2017},
  issn       = {1063-6692},
  month      = apr,
  number     = {2},
  pages      = {948–959},
  volume     = {25},
  abstract   = {Data dissemination is a fundamental service offered by low-power wireless networks.
Sender selection is the key to the dissemination performance and has been extensively
studied. Sender impact metric plays a significant role in sender selection, since
it determines which senders are selected for transmission. Recent studies have shown
that spatial link diversity has a significant impact on the efficiency of broadcast.
However, the existing metrics overlook such impact. Besides, they consider only gains
but ignore the costs of sender candidates. As a result, existing works cannot achieve
accurate estimation of the sender impact. Moreover, they cannot well support data
dissemination with network coding, which is commonly used for lossy environments.
In this paper, we first propose a novel sender impact metric, namely,  $gamma $ ,
which jointly exploits link quality and spatial link diversity to calculate the gain/cost
ratio of the sender candidates. Then, we develop a generic sender selection scheme
based on the  $gamma $  metric called  $gamma $ -component that can generally support
both types of dissemination using native packets and network coding. Extensive evaluations
are conducted through real testbed experiments and large-scale simulations. The performance
results and analysis show that  $gamma $  achieves far more accurate impact estimation
than the existing works. In addition, the dissemination protocols based on  $gamma
$ -component outperform the existing protocols in terms of completion time and transmissions
by 20.5% and 23.1%, respectively.},
  doi        = {10.1109/TNET.2016.2614129},
  issue_date = {April 2017},
  numpages   = {12},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2016.2614129},
}

@InProceedings{Huang2014,
  author    = {Huang, Chun-Ying and Chen, Po-Han and Huang, Yu-Ling and Chen, Kuan-Ta and Hsu, Cheng-Hsin},
  booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
  title     = {Measuring the Client Performance and Energy Consumption in Mobile Cloud Gaming},
  year      = {2014},
  publisher = {IEEE Press},
  series    = {NetGames '14},
  abstract  = {Mobile cloud gaming allows gamers to play games on resource-constrained mobile devices,
and a measurement study to quality the client performance and energy consumption is
crucial to attract and retain the gamers. In this paper, we adopt an open source cloud
gaming platform to conduct extensive experiments on real mobile clients. Our experiment
results show two major findings that are of interests to researchers, developers,
and gamers. First, compared to mobile native games, mobile cloud games save energy
by up to 30%. Second, the frame rate, bit rate, and resolution all affect the decoders'
resource consumption, while frame rate imposes the highest impact. These findings
shed some light on the further enhancements of the emerging mobile cloud gaming platforms.},
  articleno = {5},
  location  = {Nagoya, Japan},
  numpages  = {3},
}

@InProceedings{Kalyvianaki2016,
  author    = {Kalyvianaki, Evangelia and Fiscato, Marco and Salonidis, Theodoros and Pietzuch, Peter},
  booktitle = {Proceedings of the 2016 International Conference on Management of Data},
  title     = {THEMIS: Fairness in Federated Stream Processing under Overload},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {541–553},
  publisher = {Association for Computing Machinery},
  series    = {SIGMOD '16},
  abstract  = {Federated stream processing systems, which utilise nodes from multiple independent
domains, can be found increasingly in multi-provider cloud deployments, internet-of-things
systems, collaborative sensing applications and large-scale grid systems. To pool
resources from several sites and take advantage of local processing, submitted queries
are split into query fragments, which are executed collaboratively by different sites.
When supporting many concurrent users, however, queries may exhaust available processing
resources, thus requiring constant load shedding. Given that individual sites have
autonomy over how they allocate query fragments on their nodes, it is an open challenge
how to ensure global fairness on processing quality experienced by queries in a federated
scenario.We describe THEMIS, a federated stream processing system for resource-starved,
multi-site deployments. It executes queries in a globally fair fashion and provides
users with constant feedback on the experienced processing quality for their queries.
THEMIS associates stream data with its source information content (SIC), a metric
that quantifies the contribution of that data towards the query result, based on the
amount of source data used to generate it. We provide the BALANCE-SIC distributed
load shedding algorithm that balances the SIC values of result data. Our evaluation
shows that the BALANCE-SIC algorithm yields balanced SIC values across queries, as
measured by Jain's Fairness Index. Our approach also incurs a low execution time overhead.},
  doi       = {10.1145/2882903.2882943},
  isbn      = {9781450335317},
  keywords  = {federated data stream processing, fairness, tuple shedding, approximate data processing},
  location  = {San Francisco, California, USA},
  numpages  = {13},
  url       = {https://doi.org/10.1145/2882903.2882943},
}

@InProceedings{Kalyvianaki2016a,
  author    = {Kalyvianaki, Evangelia and Fiscato, Marco and Salonidis, Theodoros and Pietzuch, Peter},
  booktitle = {Proceedings of the 2016 International Conference on Management of Data},
  title     = {THEMIS: Fairness in Federated Stream Processing under Overload},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {541–553},
  publisher = {Association for Computing Machinery},
  series    = {SIGMOD '16},
  abstract  = {Federated stream processing systems, which utilise nodes from multiple independent
domains, can be found increasingly in multi-provider cloud deployments, internet-of-things
systems, collaborative sensing applications and large-scale grid systems. To pool
resources from several sites and take advantage of local processing, submitted queries
are split into query fragments, which are executed collaboratively by different sites.
When supporting many concurrent users, however, queries may exhaust available processing
resources, thus requiring constant load shedding. Given that individual sites have
autonomy over how they allocate query fragments on their nodes, it is an open challenge
how to ensure global fairness on processing quality experienced by queries in a federated
scenario.We describe THEMIS, a federated stream processing system for resource-starved,
multi-site deployments. It executes queries in a globally fair fashion and provides
users with constant feedback on the experienced processing quality for their queries.
THEMIS associates stream data with its source information content (SIC), a metric
that quantifies the contribution of that data towards the query result, based on the
amount of source data used to generate it. We provide the BALANCE-SIC distributed
load shedding algorithm that balances the SIC values of result data. Our evaluation
shows that the BALANCE-SIC algorithm yields balanced SIC values across queries, as
measured by Jain's Fairness Index. Our approach also incurs a low execution time overhead.},
  doi       = {10.1145/2882903.2882943},
  isbn      = {9781450335317},
  keywords  = {federated data stream processing, fairness, tuple shedding, approximate data processing},
  location  = {San Francisco, California, USA},
  numpages  = {13},
  url       = {https://doi.org/10.1145/2882903.2882943},
}

@InProceedings{Islam2018,
  author    = {Islam, Bashima and Islam, Md Tamzeed and Nirjon, Shahriar},
  booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
  title     = {Glimpse.3D: A Motion-Triggered Stereo Body Camera for 3D Experience Capture and Preview},
  year      = {2018},
  pages     = {176–187},
  publisher = {IEEE Press},
  series    = {IPSN '18},
  abstract  = {The Glimpse.3D is a body-worn camera that captures, processes, stores, and transmits
3D visual information of a real-world environment using a low cost camera-based sensor
system that is constrained by its limited processing capability, storage, and battery
life. The 3D content is viewed on a mobile device such as a smartphone or a virtual
reality headset. This system can be used in applications such as capturing and sharing
3D content in the social media, training people in different professions, and post-facto
analysis of an event. Glimpse.3D uses off-the-shelf hardware and standard computer
vision algorithms. Its novelty lies in the ability to optimally control camera data
acquisition and processing stages to guarantee the desired quality of captured information
and battery life. The design of the controller is based on extensive measurements
and modeling of the relationships between the linear and angular motion of a body-worn
camera and the quality of generated 3D point clouds as well as the battery life of
the system. To achieve this, we 1) devise a new metric to quantify the quality of
generated 3D point clouds, 2) formulate an optimization problem to find an optimal
trigger point for the camera system that prolongs its battery life while maximizing
the quality of captured 3D environment, and 3) make the model adaptive so that the
system evolves and its performance improves over time.},
  doi       = {10.1109/IPSN.2018.00046},
  isbn      = {9781538652985},
  keywords  = {body camera, 3D-reconstruction},
  location  = {Porto, Portugal},
  numpages  = {12},
  url       = {https://doi.org/10.1109/IPSN.2018.00046},
}

@InProceedings{Islam2018a,
  author    = {Islam, Bashima and Islam, Md Tamzeed and Nirjon, Shahriar},
  booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
  title     = {A Motion-Triggered Stereo Camera for 3D Experience Capture: Demo Abstract},
  year      = {2018},
  pages     = {134–135},
  publisher = {IEEE Press},
  series    = {IPSN '18},
  abstract  = {This demo is an implementation of our motion-triggered camera system that captures,
processes, stores, and transmits 3D visual information of a real-world environment
using a low-cost camera-based sensor system that is constrained by its limited processing
capability, storage, and battery life. This system can be used in applications such
as capturing and sharing 3D content in the social media, training people in different
professions, and post-facto analysis of an event. This system uses off-the-shelf hardware
and standard computer vision algorithms. Its novelty lies in the ability to optimally
control camera data acquisition and processing stages to guarantee the desired quality
of captured information and battery life. The design of the controller is based on
extensive measurements and modeling of the relationships between the linear and angular
motion of a camera and the quality of generated 3D point clouds as well as the battery
life of the system. To achieve this, we 1) devise a new metric to quantify the quality
of generated 3D point clouds, 2) formulate an optimization problem to find an optimal
trigger point for the camera system and prolongs its battery life while maximizing
the quality of captured 3D environment, and 3) make the model adaptive so that the
system evolves and its performance improves over time.},
  doi       = {10.1109/IPSN.2018.00030},
  isbn      = {9781538652985},
  location  = {Porto, Portugal},
  numpages  = {2},
  url       = {https://doi.org/10.1109/IPSN.2018.00030},
}

@InProceedings{Kassir2021,
  author    = {Kassir, Saadallah and de Veciana, Gustavo and Wang, Nannan and Wang, Xi and Palacharla, Paparao},
  booktitle = {Proceedings of the Twenty-Second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  title     = {Joint Update Rate Adaptation in Multiplayer Cloud-Edge Gaming Services: Spatial Geometry and Performance Tradeoffs},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {191–200},
  publisher = {Association for Computing Machinery},
  series    = {MobiHoc '21},
  abstract  = {In this paper, we analyze the performance of Multiplayer Cloud Gaming (MCG) systems.
To that end, we introduce a model and new MCG-Quality of Service (QoS) metric that
captures the freshness of the players' updates and fairness in their gaming experience.
We introduce an efficient measurement-based Joint Multiplayer Rate Adaptation (JMRA)
algorithm that optimizes the MCG-QoS by overcoming large (possibly varying) network
transport delays by increasing the associated players' update rates. The resulting
MCG-QoS is shown to be Schur-concave in the network delays, leading to natural characterizations
and performance comparisons associated with the players' spatial geometry and network
congestion. In particular, joint rate adaptation enables service providers to combat
variability in network delays and players' geographic spread to achieve high service
coverage. This, in turn, allows us to explore the spatial density and capacity of
compute resources that need to be provisioned. Finally, we leverage tools from majorization
theory, to show how service placement decisions can be made to improve the robustness
of the MCG-QoS to stochastic network delays.},
  doi       = {10.1145/3466772.3467048},
  isbn      = {9781450385589},
  keywords  = {Rate Adaptation, Multiplayer Cloud Gaming, Network Resource Provisioning, Service Placement, Edge Computing},
  location  = {Shanghai, China},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3466772.3467048},
}

@Article{AlAbbasi2019,
  author     = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet and Ra, Moo-Ryong},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {Multi-Tier Caching Analysis in CDN-Based Over-the-Top Video Streaming Systems},
  year       = {2019},
  issn       = {1063-6692},
  month      = apr,
  number     = {2},
  pages      = {835–847},
  volume     = {27},
  abstract   = {Internet video traffic has been rapidly increasing and is further expected to increase
with the emerging 5G applications, such as higher definition videos, the IoT, and
augmented/virtual reality applications. As end users consume video in massive amounts
and in an increasing number of ways, the content distribution network CDN should be
efficiently managed to improve the system efficiency. The streaming service can include
multiple caching tiers, at the distributed servers and the edge routers, and efficient
content management at these locations affects the quality of experience QoE of the
end users. In this paper, we propose a model for video streaming systems, typically
composed of a centralized origin server, several CDN sites, and edge-caches located
closer to the end user. We comprehensively consider different systems design factors,
including the limited caching space at the CDN sites, allocation of CDN for a video
request, choice of different ports or paths from the CDN and the central storage,
bandwidth allocation, the edge-cache capacity, and the caching policy. We focus on
minimizing a performance metric, stall duration tail probability SDTP, and present
a novel and efficient algorithm accounting for the multiple design flexibilities.
The theoretical bounds with respect to the SDTP metric are also analyzed and presented.
The implementation of a virtualized cloud system managed by Openstack demonstrates
that the proposed algorithms can significantly improve the SDTP metric compared with
the baseline strategies.},
  doi        = {10.1109/TNET.2019.2900434},
  issue_date = {April 2019},
  numpages   = {13},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2019.2900434},
}

@Article{Dey2019,
  author     = {Dey, Tamal K. and Shi, Dayu and Wang, Yusu},
  journal    = {ACM J. Exp. Algorithmics},
  title      = {SimBa: An Efficient Tool for Approximating Rips-Filtration Persistence via <u class="uu">Sim</u>plicial <u class="uu">Ba</u>tch Collapse},
  year       = {2019},
  issn       = {1084-6654},
  month      = jan,
  volume     = {24},
  abstract   = {In topological data analysis, a point cloud data P extracted from a metric space is
often analyzed by computing the persistence diagram or barcodes of a sequence of Rips
complexes built on P indexed by a scale parameter. Unfortunately, even for input of
moderate size, the size of the Rips complex may become prohibitively large as the
scale parameter increases. Starting with the Sparse Rips filtration introduced by
Sheehy, some existing methods aim to reduce the size of the complex to improve time
efficiency as well. However, as we demonstrate, existing approaches still fall short
of scaling well, especially for high-dimensional data. In this article, we investigate
the advantages and limitations of existing approaches. Based on insights gained from
the experiments, we propose an efficient new algorithm, called SimBa, for approximating
the persistent homology of Rips filtrations with quality guarantees. Our new algorithm
leverages a batch-collapse strategy as well as a new Sparse Rips-like filtration.
We experiment on a variety of low- and high-dimensional datasets. We show that our
strategy presents a significant size reduction and that our algorithm for approximating
Rips filtration persistence is an order of magnitude faster than existing methods
in practice.},
  address    = {New York, NY, USA},
  articleno  = {1.5},
  doi        = {10.1145/3284360},
  issue_date = {2019},
  keywords   = {persistent homology, Topological data analysis, simplicial maps, approximation, rips filtration},
  numpages   = {16},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3284360},
}

@InProceedings{Ibrahim2016,
  author    = {Ibrahim, Abdallah Ali Zainelabden A. and Kliazovich, Dzmitry and Bouvry, Pascal},
  booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Service Level Agreement Assurance between Cloud Services Providers and Cloud Customers},
  year      = {2016},
  pages     = {588–591},
  publisher = {IEEE Press},
  series    = {CCGRID '16},
  abstract  = {Cloud services providers deliver cloud services to cloud customers on pay-per-use
model while the quality of the provided services are defined using service level agreements
also known as SLAs. Unfortunately, there is no standard mechanism which exists to
verify and assure that delivered services satisfy the signed SLA agreement in an automatic
way. There is no guarantee in terms of quality. Those applications have many performance
metrics. In this doctoral thesis, we propose a framework for SLA assurance, which
can be used by both cloud providers and cloud users. Inside the proposed framework,
we will define the performance metrics for the different applications. We will assess
the applications performance in different testing environment to assure good services
quality as mentioned in SLA. The proposed framework will be evaluated through simulations
and using testbed experiments. After testing the applications performance by measuring
the performance metrics, we will review the time correlations between those metrics.},
  doi       = {10.1109/CCGrid.2016.56},
  isbn      = {9781509024520},
  keywords  = {simulation, data centers, performance, metrics, quality of experience, applications, quality of services, cloud computing, service level agreement},
  location  = {Cartagena, Columbia},
  numpages  = {4},
  url       = {https://doi.org/10.1109/CCGrid.2016.56},
}

@InProceedings{Zhou2016,
  author    = {Zhou, Ke and Redi, Miriam and Haines, Andrew and Lalmas, Mounia},
  booktitle = {Proceedings of the 25th International Conference on World Wide Web},
  title     = {Predicting Pre-Click Quality for Native Advertisements},
  year      = {2016},
  address   = {Republic and Canton of Geneva, CHE},
  pages     = {299–310},
  publisher = {International World Wide Web Conferences Steering Committee},
  series    = {WWW '16},
  abstract  = {Native advertising is a specific form of online advertising where ads replicate the
look-and-feel of their serving platform. In such context, providing a good user experience
with the served ads is crucial to ensure long-term user engagement. In this work,
we explore the notion of ad quality, namely the effectiveness of advertising from
a user experience perspective. We design a learning framework to predict the pre-click
quality of native ads. More specifically, we look at detecting offensive native ads,
showing that, to quantify ad quality, ad offensive user feedback rates are more reliable
than the commonly used click-through rate metrics. We then conduct a crowd-sourcing
study to identify which criteria drive user preferences in native advertising. We
translate these criteria into a set of ad quality features that we extract from the
ad text, image and advertiser, and then use them to train a model able to identify
offensive ads. We show that our model is very effective in detecting offensive ads,
and provide in-depth insights on how different features affect ad quality. Finally,
we deploy a preliminary version of such model and show its effectiveness in the reduction
of the offensive ad feedback rate.},
  doi       = {10.1145/2872427.2883053},
  isbn      = {9781450341431},
  keywords  = {native advertising, image and text, features, ad quality, ad feedback, offensive rate, pre-click experience},
  location  = {Montr\'{e}al, Qu\'{e}bec, Canada},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2872427.2883053},
}

@InProceedings{Pegus2015a,
  author    = {Pegus, Patrick and Cecchet, Emmanuel and Shenoy, Prashant},
  booktitle = {Proceedings of the 6th ACM Multimedia Systems Conference},
  title     = {Video BenchLab: An Open Platform for Realistic Benchmarking of Streaming Media Workloads},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {165–176},
  publisher = {Association for Computing Machinery},
  series    = {MMSys '15},
  abstract  = {In this paper, we present an open, flexible and realistic benchmarking platform named
Video BenchLab to measure the performance of streaming media workloads. While Video
BenchLab can be used with any existing media server, we provide a set of tools for
researchers to experiment with their own platform and protocols. The components include
a MediaDrop video server, a suite of tools to bulk insert videos and generate streaming
media workloads, a dataset of freely available video and a client runtime to replay
videos in the native video players of real Web browsers such as Firefox, Chrome and
Internet Explorer. We define simple metrics that are able to capture the quality of
video playback and identify issues that can happen during video replay. Finally, we
provide a Dashboard to manage experiments, collect results and perform analytics to
compare performance between experiments.We present a series of experiments with Video
BenchLab to illustrate how the video specific metrics can be used to measure the user
perceived experience in real browsers when streaming videos. We also show Internet
scale experiments by deploying clients in data centers distributed all over the globe.
All the software, datasets, workloads and results used in this paper are made freely
available on SourceForge for anyone to reuse and expand.},
  doi       = {10.1145/2713168.2723145},
  isbn      = {9781450333511},
  keywords  = {web browsers, streaming, video, benchmarking},
  location  = {Portland, Oregon},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2713168.2723145},
}

@Article{Liao2015,
  author     = {Liao, Jing and Finch, Mark and Hoppe, Hugues},
  journal    = {ACM Trans. Graph.},
  title      = {Fast Computation of Seamless Video Loops},
  year       = {2015},
  issn       = {0730-0301},
  month      = oct,
  number     = {6},
  volume     = {34},
  abstract   = {Short looping videos concisely capture the dynamism of natural scenes. Creating seamless
loops usually involves maximizing spatiotemporal consistency and applying Poisson
blending. We take an end-to-end view of the problem and present new techniques that
jointly improve loop quality while also significantly reducing processing time. A
key idea is to relax the consistency constraints to anticipate the subsequent blending,
thereby enabling looping of low-frequency content like moving clouds and changing
illumination. We also analyze the input video to remove an undesired bias toward short
loops. The quality gains are demonstrated visually and confirmed quantitatively using
a new gradient-domain consistency metric. We improve system performance by classifying
potentially loopable pixels, masking the 2D graph cut, pruning graph-cut labels based
on dominant periods, and optimizing on a coarse grid while retaining finer detail.
Together these techniques reduce computation times from tens of minutes to nearly
real-time.},
  address    = {New York, NY, USA},
  articleno  = {197},
  doi        = {10.1145/2816795.2818061},
  issue_date = {November 2015},
  keywords   = {blend-aware consistency, video textures, cinemagraphs},
  numpages   = {10},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2816795.2818061},
}

@InProceedings{Lindsey2014,
  author    = {Lindsey, Aaron and Yeh, Hsin-Yi (Cindy) and Wu, Chih-Peng and Thomas, Shawna and Amato, Nancy M.},
  booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
  title     = {Improving Decoy Databases for Protein Folding Algorithms},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {717–724},
  publisher = {Association for Computing Machinery},
  series    = {BCB '14},
  abstract  = {Predicting protein structures and simulating protein folding are two of the most important
problems in computational biology today. Simulation methods rely on a scoring function
to distinguish the native structure (the most energetically stable) from non-native
structures. Decoy databases are collections of non-native structures used to test
and verify these functions.We present a method to evaluate and improve the quality
of decoy databases by adding novel structures and removing redundant structures. We
test our approach on 17 different decoy databases of varying size and type and show
significant improvement across a variety of metrics. We also test our improved databases
on a popular modern scoring function and show that they contain a greater number of
native-like structures than the original databases, thereby producing a more rigorous
database for testing scoring functions.},
  doi       = {10.1145/2649387.2660839},
  isbn      = {9781450328944},
  keywords  = {protein folding, decoy databases, sampling methods},
  location  = {Newport Beach, California},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2649387.2660839},
}

@InProceedings{Nagin2019,
  author    = {Nagin, Kenneth and Kassis, Andre and Lorenz, Dean and Barabash, Katherine and Raichstein, Eran},
  booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage},
  title     = {Estimating Client QoE from Measured Network QoS},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {188},
  publisher = {Association for Computing Machinery},
  series    = {SYSTOR '19},
  abstract  = {This research is done in the context of the SliceNet project [4] that aims to extend
5G infrastructure with cognitive management of cross-domain, cross-layer network slices
[1], with emphasis on Quality of Experience (QoE) for vertical industries. The provisioning
of network slices with proper QoE guarantees is seen as one of the key enablers of
future 5G-enabled networks. The challenge is to assess the QoE experienced by the
vertical application and its users without requiring the applications or the users
to measure and report QoE related metrics back to the provider. To address this challenge,
we propose a method for deriving application-level QoE from network-level Quality
of Service (QoS) measurements, easily accessible by the provider. In particular, we
describe a PoC where QoE, perceived by application users, is estimated from low level
network monitoring data, by applying cognitive methods. Our main goal is enabling
the cloud provider to support the desired E2E QoE-based Service Level Agreements (SLAs),
e.g. by monitoring QoS metrics within the provider's domain to optimize resource allocation
through provider's actuators. Additional benefit can be achieved by applying the same
technique to troubleshoot issues in the provider's infrastructure. In this work, we
employed classical statistical methods to assess the relationship between the application-level
QoE and the network-level QoS.},
  doi       = {10.1145/3319647.3325849},
  isbn      = {9781450367493},
  location  = {Haifa, Israel},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3319647.3325849},
}

@InProceedings{Haq2017,
  author    = {Haq, Osama and Raja, Mamoon and Dogar, Fahad R.},
  booktitle = {Proceedings of the 26th International Conference on World Wide Web},
  title     = {Measuring and Improving the Reliability of Wide-Area Cloud Paths},
  year      = {2017},
  address   = {Republic and Canton of Geneva, CHE},
  pages     = {253–262},
  publisher = {International World Wide Web Conferences Steering Committee},
  series    = {WWW '17},
  abstract  = {Many popular cloud applications use inter-data center paths; yet, little is known
about the characteristics of these ``cloud paths''. Over an eighteen month period,
we measure the inter-continental cloud paths of three providers (Amazon, Google, and
Microsoft) using client side (VM-to-VM) measurements. We find that cloud paths are
more predictable compared to public Internet paths, with an order of magnitude lower
loss rate and jitter at the tail (95th percentile and beyond) compared to public Internet
paths. We also investigate the nature of packet losses on these paths (e.g., random
vs. bursty) and potential reasons why these paths may be better in quality. Based
on our insights, we consider how we can further improve the quality of these paths
with the help of existing loss mitigation techniques. We demonstrate that using the
cloud path in conjunction with a detour path can mask most of the cloud losses, resulting
in up to five 9's of network availability for applications.},
  doi       = {10.1145/3038912.3052560},
  isbn      = {9781450349130},
  keywords  = {inter-data center networks, loss rate, detour routing, cloud paths reliability, cloud availability, latency, bandwidth},
  location  = {Perth, Australia},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3038912.3052560},
}

@InProceedings{MartinezOrtiz2019,
  author    = {Martinez-Ortiz, Andres-Leonardo and Lizcano, David and Ortega, Miguel},
  booktitle = {Proceedings of the 14th International Workshop on Automation of Software Test},
  title     = {Software Metrics Artifacts Making Web Quality Measurable: AST 2019 Invited Paper},
  year      = {2019},
  pages     = {1–6},
  publisher = {IEEE Press},
  series    = {AST '19},
  abstract  = {Mining open source repositories introduces an effective approach to put in practice
empirical software engineering in a variety of technologies. Kernel development (Linux)
first and then Internet (Chromium) and more recently cloud orchestration (Kubernetes)
and machine learning (TensorFlow) are fundamental pieces not just for open source
ecosystem but also for the industry leading software innovation. Empirical software
engineering sustains a better understanding of these projects, reducing even more
the barriers for adoption. In this work we focus on empirical quality assessment developing
software metrics artifacts to make web components quality measurable. After reviewing
the state of the art and main frameworks for software measurement, we will present
our proposal for the empirical evaluation of quality metrics for web components, data
collection, measurement and prediction, discussing main benefits and some drawback
of the selected approach, which will be aimed at future works.},
  doi       = {10.1109/AST.2019.000-2},
  keywords  = {web technologies, open source, quality metrics, software engineering},
  location  = {Montreal, Quebec, Canada},
  numpages  = {6},
  url       = {https://doi.org/10.1109/AST.2019.000-2},
}

@InProceedings{Silva2018,
  author    = {Silva, Gabriel Costa and R\'{e}, Reginaldo and Silva, Marco Aur\'{e}lio Graciotto},
  booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
  title     = {Evaluating Efficiency, Effectiveness and Satisfaction of AWS and Azure from the Perspective of Cloud Beginners},
  year      = {2018},
  address   = {USA},
  pages     = {114–125},
  publisher = {IBM Corp.},
  series    = {CASCON '18},
  abstract  = {Quality has long been regarded as an important driver of cloud adoption. In particular,
quality in use (QiU) of cloud platforms may drive cloud beginners to the cloud platform
that offers the best cloud experience. Cloud beginners are critical to the cloud market
because they currently represent nearly a third of cloud users. We carried out three
experiments to measure the QiU (dependent variable) of public cloud platforms (independent
variable) regarding efficiency, effectiveness and satisfaction. AWS EC2 and Azure
Virtual Machines are the two cloud services used as representative proxies to evaluate
cloud platforms (treatments). Eleven undergraduate students with limited cloud knowledge
(participants) manually created 152 VMs (task) using the web interface of cloud platforms
(instrument) following seven different configurations (trials) for each cloud platform.
Whereas AWS performed significantly better than Azure for efficiency (p-value not
exceeding 0.001, A-statistic = 0.68), we could not find a significant difference between
platforms for effectiveness (p-value exceeding 0.05) - although the effect size was
found relevant (odds ratio = 0.41). Regarding satisfaction, most of our participants
perceived the AWS as (i) having the best GUI to benefiting user interaction, (ii)
the easiest platform to use, and (iii) the preferred cloud platform for creating VMs.
Once confirmed by independent replications, our results suggest that AWS outperforms
Azure regarding QiU. Therefore, cloud beginners might have a better cloud experience
starting off their cloud projects by using AWS rather than Azure. In addition, our
results may help to explain the AWS's cloud leadership.},
  keywords  = {experimentation, cloud platforms, quality in use},
  location  = {Markham, Ontario, Canada},
  numpages  = {12},
}

@InProceedings{Legrand2015,
  author    = {Legrand, H\'{e}l\`{e}ne and Boubekeur, Tamy},
  booktitle = {Proceedings of the 7th Conference on High-Performance Graphics},
  title     = {Morton Integrals for High Speed Geometry Simplification},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {105–112},
  publisher = {Association for Computing Machinery},
  series    = {HPG '15},
  abstract  = {Real time geometry processing has progressively reached a performance level that makes
a number of signal-inspired primitives practical for on-line applications scenarios.
This often comes through the joint design of operators, data structure and even dedicated
hardware. Among the major classes of geometric operators, filtering and super-sampling
(via tessellation) have been successfully expressed under high-performance constraints.
The subsampling operator i.e., adaptive simplification, remains however a challenging
case for non-trivial input models. In this paper, we build a fast geometry simplification
algorithm over a new concept: Morton Integrals. By summing up quadric error metric
matrices along Morton-ordered surface samples, we can extract concurrently the nodes
of an adaptive cut in the so-defined implicit hierarchy, and optimize all simplified
vertices in parallel. This approach is inspired by integral images and exploits recent
advances in high performance spatial hierarchy construction and traversal. As a result,
our GPU implementation can downsample a mesh made of several millions of polygons
at interactive rates, while providing better quality than uniform simplification and
preserving important salient features. We present results for surface meshes, polygon
soups and point clouds, and discuss variations of our approach to account for per-sample
attributes and alternatives error metrics.},
  doi       = {10.1145/2790060.2790071},
  isbn      = {9781450337076},
  keywords  = {Morton code, GPU algorithms, mesh simplification, adaptive clustering},
  location  = {Los Angeles, California},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2790060.2790071},
}

@Article{AlAbbasi2018,
  author     = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {Video Streaming in Distributed Erasure-Coded Storage Systems: Stall Duration Analysis},
  year       = {2018},
  issn       = {1063-6692},
  month      = aug,
  number     = {4},
  pages      = {1921–1932},
  volume     = {26},
  abstract   = {The demand for global video has been burgeoning across industries. With the expansion
and improvement of video-streaming services, cloud-based video is evolving into a
necessary feature of any successful business for reaching internal and external audiences.
This paper considers video streaming over distributed systems where the video segments
are encoded using an erasure code for better reliability, thus being the first work
to our best knowledge that considers video streaming over erasure-coded distributed
cloud systems. The download time of each coded chunk of each video segment is characterized,
and the ordered statistics over the choice of the erasure-coded chunks is used to
obtain the playback time of different video segments. Using the playback times, bounds
on the moment generating function on the stall duration are used to bound the mean
stall duration. Moment generating function-based bounds on the ordered statistics
are also used to bound the stall duration tail probability, which determines the probability
that the stall time is greater than a pre-defined number. These two metrics, mean
stall duration and the stall duration tail probability, are important quality of experience
QoE measures for the end users. Based on these metrics, we formulate an optimization
problem to jointly minimize the convex combination of both the QoE metrics averaged
over all requests over the placement and access of the video content. The non-convex
problem is solved using an efficient iterative algorithm. Numerical results show a
significant improvement in QoE metrics for cloud-based video compared to the considered
baselines.},
  doi        = {10.1109/TNET.2018.2851379},
  issue_date = {August 2018},
  numpages   = {12},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2018.2851379},
}

@InProceedings{Yen2018,
  author    = {Yen, Chih-Hsuan and Chen, Wei-Ming and Hsiu, Pi-Cheng and Kuo, Tei-Wei},
  booktitle = {Proceedings of the International Conference on Computer-Aided Design},
  title     = {Differentiated Handling of Physical Scenes and Virtual Objects for Mobile Augmented Reality},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICCAD '18},
  abstract  = {Mobile devices running augmented reality applications consume considerable energy
for graphics-intensive workloads. This paper presents a scheme for the differentiated
handling of camera-captured physical scenes and computer-generated virtual objects
according to different perceptual quality metrics. We propose online algorithms and
their realtime implementations to reduce energy consumption through dynamic frame
rate adaptation while maintaining the visual quality required for augmented reality
applications. To evaluate system efficacy, we integrate our scheme into Android and
conduct extensive experiments on a commercial smartphone with various application
scenarios. The results show that the proposed scheme can achieve energy savings of
up to 39.1% in comparison to the native graphics system in Android while maintaining
satisfactory visual quality.},
  articleno = {36},
  doi       = {10.1145/3240765.3240798},
  isbn      = {9781450359504},
  keywords  = {energy savings, mobile systems, augmented reality, visual quality, frame frame adaptation},
  location  = {San Diego, California},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3240765.3240798},
}

@InProceedings{Munoz2019,
  author    = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
  booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
  title     = {HADAS: Analysing Quality Attributes of Software Configurations},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {13–16},
  publisher = {Association for Computing Machinery},
  series    = {SPLC '19},
  abstract  = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses
of SPLs rely on solvers to navigate complex dependencies among features and find legal
solutions. Variability analysis tools are complex due to the diversity of products
and domain-specific knowledge. On that, while there are experimental studies that
analyse quality attributes, the knowledge is not easily accessible for developers,
and its appliance is not trivial. Aiming to allow the industry to quality-explore
SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects
quality attributes metrics in a cloud repository, and (2) reasons about it helping
developers with quality attributes requirements.},
  doi       = {10.1145/3307630.3342385},
  isbn      = {9781450366687},
  keywords  = {variability, numerical, attribute, software product line, model, NFQA},
  location  = {Paris, France},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3307630.3342385},
}

@InProceedings{Weber2014,
  author    = {Weber, Andreas and Herbst, Nikolas and Groenda, Henning and Kounev, Samuel},
  booktitle = {Proceedings of the 2nd International Workshop on Hot Topics in Cloud Service Scalability},
  title     = {Towards a Resource Elasticity Benchmark for Cloud Environments},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {HotTopiCS '14},
  abstract  = {Auto-scaling features offered by today's cloud infrastructures provide increased flexibility
especially for customers that experience high variations in the load intensity over
time. However, auto-scaling features introduce new system quality attributes when
considering their accuracy, timing, and boundaries. Therefore, distinguishing between
different offerings has become a complex task, as it is not yet supported by reliable
metrics and measurement approaches. In this paper, we discuss shortcomings of existing
approaches for measuring and evaluating elastic behavior and propose a novel benchmark
methodology specifically designed for evaluating the elasticity aspects of modern
cloud platforms. The benchmark is based on open workloads with realistic load variation
profiles that are calibrated to induce identical resource demand variations independent
of the underlying hardware performance. Furthermore, we propose new metrics that capture
the accuracy of resource allocations and de-allocations, as well as the timing aspects
of an auto-scaling mechanism explicitly.},
  articleno = {5},
  doi       = {10.1145/2649563.2649571},
  isbn      = {9781450330596},
  keywords  = {Resource, Elasticity, Supply, Demand, Load Profile},
  location  = {Dublin, Ireland},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2649563.2649571},
}

@InProceedings{Tesfamicael2019,
  author    = {Tesfamicael, Aklilu Daniel and Liu, Vicky and Foo, Ernest and Caelli, Bill},
  booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
  title     = {QoE Estimation Model for a Secure Real-Time Voice Communication System in the Cloud},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ACSW 2019},
  abstract  = {As moving towards cloud-based real-time services, we are witnessing the shift from
a technology-driven services to service provisioning paradigms, that is, from Quality
of Service (QoS) to Quality of Experience (QoE). User experience and satisfaction
are placed at the epicenter of the system design. QoE is a measurement of user experience
on the provided service by a system. Often QoE is measured by subjective mechanisms,
such as user experience surveys and mean opinion scores (MOS) methods, which can be
a costly and time-consuming process. Using an adequate QoE model to measure user experience
of perceived quality is cost-effective, compared to using time-consuming subjective
surveys. Applying an adequate QoE model to assess user experience is advantageous
for cloud-based real-time services such as voice and video. This study uses a formula-based
QoE estimation model to estimate and predict QoE prior to the deployment or during
the planning stage of the system service. This study investigates a real-world scenario
of a company that recently moved to its premises-based real-time trading communication
system (TCS) to a public cloud. A simulation system using OPNET is also implemented
to illustrate the usefulness of the model. Our result shows that the effect of delay
on the users experience of the service provided by the cloud-based TCS is minimum
comparing to packet loss rate (PLR) and Jitter. However, it has been observed that
the overhead of the different security settings of the TCS system had no major negative
impact to the user experience. The proposed model can be used as a QoE control mechanism
and network optimization for cloud-based TCS services.},
  articleno = {10},
  doi       = {10.1145/3290688.3290705},
  isbn      = {9781450366038},
  keywords  = {QoS, TCS, Real-time, VoIP, QoE, E-Model},
  location  = {Sydney, NSW, Australia},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3290688.3290705},
}

@InProceedings{AliEldin2014,
  author    = {Ali-Eldin, Ahmed and Seleznjev, Oleg and Sj\"{o}stedt-de Luna, Sara and Tordsson, Johan and Elmroth, Erik},
  booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
  title     = {Measuring Cloud Workload Burstiness},
  year      = {2014},
  address   = {USA},
  pages     = {566–572},
  publisher = {IEEE Computer Society},
  series    = {UCC '14},
  abstract  = {Workload burstiness and spikes are among the main reasons for service disruptions
and decrease in the Quality-of-Service (QoS) of online services. They are hurdles
that complicate autonomic resource management of data enters. In this paper, we review
the state-of-the-art in online identification of workload spikes and quantifying burstiness.
The applicability of some of the proposed techniques is examined for Cloud systems
where various workloads are co-hosted on the same platform. We discuss Sample Entropy
(Samp En), a measure used in biomedical signal analysis, as a potential measure for
burstiness. A modification to the original measure is introduced to make it more suitable
for Cloud workloads.},
  doi       = {10.1109/UCC.2014.87},
  isbn      = {9781479978816},
  numpages  = {7},
  url       = {https://doi.org/10.1109/UCC.2014.87},
}

@InProceedings{Su2016,
  author    = {Su, Guoxin and Rosenblum, David S. and Tamburrelli, Giordano},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering},
  title     = {Reliability of Run-Time Quality-of-Service Evaluation Using Parametric Model Checking},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {73–84},
  publisher = {Association for Computing Machinery},
  series    = {ICSE '16},
  abstract  = {Run-time Quality-of-Service (QoS) assurance is crucial for business-critical systems.
Complex behavioral performance metrics (PMs) are useful but often difficult to monitor
or measure. Probabilistic model checking, especially parametric model checking, can
support the computation of aggregate functions for a broad range of those PMs. In
practice, those PMs may be defined with parameters determined by run-time data. In
this paper, we address the reliability of QoS evaluation using parametric model checking.
Due to the imprecision with the instantiation of parameters, an evaluation outcome
may mislead the judgment about requirement violations. Based on a general assumption
of run-time data distribution, we present a novel framework that contains light-weight
statistical inference methods to analyze the reliability of a parametric model checking
output with respect to an intuitive criterion. We also present case studies in which
we test the stability and accuracy of our inference methods and describe an application
of our framework to a cloud server management problem.},
  doi       = {10.1145/2884781.2884814},
  isbn      = {9781450339001},
  keywords  = {run-time evaluation, data distribution, probabilistic model checking, reliability, Quality-of-Service},
  location  = {Austin, Texas},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2884781.2884814},
}

@InProceedings{Zheng2014,
  author    = {Zheng, Yixin and Li, Linglong and Zhang, Lin},
  booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
  title     = {Poster Abstract: PiMi Air Community: Getting Fresher Indoor Air by Sharing Data and Know-Hows},
  year      = {2014},
  pages     = {283–284},
  publisher = {IEEE Press},
  series    = {IPSN '14},
  abstract  = {PiMiair.org is a participatory indoor air quality data sharing project we launched
in January 2014. Over 200 PiMi air boxes, a low-cost indoor air quality monitor, were
given out to volunteer users across China. The PiMi air boxes measure the approximate
indoor particulate matter concentration, and the ambient temperate and humidity. When
a user accesses the PiMi air box for his personal air quality data on his smartphone,
the data is relayed to the backend PiMi cloud server for analysis. Accumulating large
amount of indoor air quality data under different circumstances, the PiMi cloud server
is able to use statistical learning methodologies to detect point of interests (POIs)
in the data series, and asks users to label their activities or events at the POIs.
Together with the user-reported physicality information on the indoor environments,
PiMiair.org is able to quantitatively evaluate the impacts of the environment physicality
and human behaviors on the indoor air quality, and mine the knowledges on how to alleviate
indoor air pollution. We believe that by sharing these knowledge among the community,
healthier breathing environments could be nurtured for the well-being of the public.},
  isbn      = {9781479931460},
  keywords  = {human factors, indoor air quality, participatory sensing},
  location  = {Berlin, Germany},
  numpages  = {2},
}

@Article{Baliyan2016,
  author     = {Baliyan, Niyati and Kumar, Sandeep},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {A Hierarchical Fuzzy System for Quality Assessment of Semantic Web Application as a Service},
  year       = {2016},
  issn       = {0163-5948},
  month      = feb,
  number     = {1},
  pages      = {1–7},
  volume     = {41},
  abstract   = {Semantic Web enabled applications are becoming popular due to the presence of their
machine comprehensible description, which makes them easily sharable across machines.
If such applications are deployed as services to the user through the Cloud, they
can facilitate transparency and reusability. There exist no attributes, metrics, or
models for monitoring the quality of such applications. In the current work, a hierarchical
fuzzy system for quality assessment of Semantic Web based applications delivered as
services on the Cloud, is proposed. The quality attributes proposed herein have been
validated through the standard IEEE-1061 validation framework. Experimental results
reveal that the proposed hierarchical fuzzy system handles the multiplicity of quality
attributes, and can be used for the relative ranking of Semantic Web applications
available as services},
  address    = {New York, NY, USA},
  doi        = {10.1145/2853073.2853085},
  issue_date = {January 2016},
  keywords   = {Quality Metrics, Fuzzy Logic, Cloud, Semantic Web},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2853073.2853085},
}

@InProceedings{Claypool2014,
  author    = {Claypool, Mark and Finkel, David},
  booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
  title     = {The Effects of Latency on Player Performance in Cloud-Based Games},
  year      = {2014},
  publisher = {IEEE Press},
  series    = {NetGames '14},
  abstract  = {Cloud-based games are an increasingly popular method to distribute and play computer
games on the Internet. While there has been some work studying network aspects of
cloud-based games and examining the effects of latency on traditional games, there
has not been sufficient research on the impact of latency on cloud-based games nor
a comparison of the impact of latency on cloud-based games versus traditional games.
This paper presents the results of two user studies that measure the objective and
subjective effects of latency on cloud-based games, one study using the commercial
cloud game system OnLive and the other study using the academic cloud game system
GamingAnywhere. Analysis of the results shows both quality of experience and user
performance degrade linearly with an increase in latency. More significantly, latency
affects cloud-based games in a manner most similar to that of traditional first-person
avatar games, the most sensitive class of games, despite the fact that the cloud-based
games may have a different user perspective. These results have implications for cloud-based
game designers and cloud system developers.},
  articleno = {2},
  location  = {Nagoya, Japan},
  numpages  = {6},
}

@InProceedings{Lopez2017a,
  author    = {L\'{o}pez, Cindy and Heinsen, Rene and Huh, Eui-Nam},
  booktitle = {Proceedings of the 2017 International Conference on Machine Learning and Soft Computing},
  title     = {Improving Availability Applying Intelligent Replication in Federated Cloud Storage Based on Log Analysis},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {148–153},
  publisher = {Association for Computing Machinery},
  series    = {ICMLSC '17},
  abstract  = {This study is focusing on improving the availability of federated storage services
in order to provide better quality-of-service (QoS) to the customer with the minimum
use of resources. One of the most efficient solutions to get the best experience in
the cloud is to combine the services offered. In order for this to happen, there exist
different approaches for selecting the best subset of services to reach the optimal
performance. However, those works focus on one time selection processes, despite of
customer's requirements are continuously changing and demanding adaptable storage
service. In this research, I propose a method to improve storage availability through
log sentiment analysis and intelligent replication. This methodology is based on the
merging of two types of log analysis and the measurement of availability and performance
metrics in order to select the best subset of services in cloud storage service federation.},
  doi       = {10.1145/3036290.3036321},
  isbn      = {9781450348287},
  keywords  = {availability, replication, subset selection, log analysis, Federated Cloud Storage, performance, cloud computing, sentiment analysis},
  location  = {Ho Chi Minh City, Vietnam},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3036290.3036321},
}

@InProceedings{Zhang2017,
  author    = {Zhang, Wenxiao and Han, Bo and Hui, Pan},
  booktitle = {Proceedings of the Workshop on Virtual Reality and Augmented Reality Network},
  title     = {On the Networking Challenges of Mobile Augmented Reality},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {24–29},
  publisher = {Association for Computing Machinery},
  series    = {VR/AR Network '17},
  abstract  = {In this paper, we conduct a reality check for Augmented Reality (AR) on mobile devices.
We dissect and measure the cloud-offloading feature for computation-intensive visual
tasks of two popular commercial AR systems. Our key finding is that their cloud-based
recognition is still not mature and not optimized for latency, data usage and energy
consumption. In order to identify the opportunities for further improving the Quality
of Experience (QoE) for mobile AR, we break down the end-to-end latency of the pipeline
for typical cloud-based mobile AR and pinpoint the dominating components in the critical
path.},
  doi       = {10.1145/3097895.3097900},
  isbn      = {9781450350556},
  keywords  = {networking challenges, end-to-end latency, Augmented reality, cloud offloading, reality check},
  location  = {Los Angeles, CA, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3097895.3097900},
}

@Article{Chen2017a,
  author     = {Chen, Qi and Liu, Ye and Liu, Guangchi and Yang, Qing and Shi, Xianming and Gao, Hongwei and Su, Lu and Li, Quanlong},
  journal    = {ACM Trans. Embed. Comput. Syst.},
  title      = {Harvest Energy from the Water: A Self-Sustained Wireless Water Quality Sensing System},
  year       = {2017},
  issn       = {1539-9087},
  month      = sep,
  number     = {1},
  volume     = {17},
  abstract   = {Water quality data is incredibly important and valuable, but its acquisition is not
always trivial. A promising solution is to distribute a wireless sensor network in
water to measure and collect the data; however, a drawback exists in that the batteries
of the system must be replaced or recharged after being exhausted. To mitigate this
issue, we designed a self-sustained water quality sensing system that is powered by
renewable bioenergy generated from microbial fuel cells (MFCs). MFCs collect the energy
released from native magnesium oxidizing microorganisms (MOMs) that are abundant in
natural waters. The proposed energy-harvesting technology is environmentally friendly
and can provide maintenance-free power to sensors for several years. Despite these
benefits, an MFC can only provide microwatt-level power that is not sufficient to
continuously power a sensor. To address this issue, we designed a power management
module to accumulate energy when the input voltage is as low as 0.33V. We also proposed
a radio-frequency (RF) activation technique to remotely activate sensors that otherwise
are switched off in default. With this innovative technique, a sensor’s energy consumption
in sleep mode can be completely avoided. Additionally, this design can enable on-demand
data acquisitions from sensors. We implement the proposed system and evaluate its
performance in a stream. In 3-month field experiments, we find the system is able
to reliably collect water quality data and is robust to environment changes.},
  address    = {New York, NY, USA},
  articleno  = {3},
  doi        = {10.1145/3047646},
  issue_date = {January 2018},
  keywords   = {Energy harvesting, power management, microbial fuel cell, water quality monitoring, radio-frequency (RF) activation},
  numpages   = {24},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3047646},
}

@InProceedings{Chen2019,
  author    = {Chen, Wenyu and Xiong, Wei and Cheng, Jierong and Li, Yusha},
  booktitle = {Proceedings of the 2019 5th International Conference on Robotics and Artificial Intelligence},
  title     = {Automatic Dimensional Measurement Using Datums Generated from Point Clouds},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {59–63},
  publisher = {Association for Computing Machinery},
  series    = {ICRAI '19},
  abstract  = {Dimensional measurement is critical for quality control. Manual dimensional measurement
using standard gauges can only be applied on a few datums. To measure a huge number
of datums, a component needs to be scanned into a point cloud and measured digitally.
For precision components, datum generation on the scanned point cloud is labor-intensive.
Given a raw point cloud from scanner, this paper proposes an automatic dimensional
measurement solution with an adaptive local registration algorithm and an adaptive
datum generation algorithm. Using datums on the CAD model as reference, the adaptive
local registration algorithm selects local regions on the scanned model to compensate
the local deviation between the CAD model and the scanned model. After that, with
outliers and noises in the raw data, the adaptive datum generation algorithm creates
the correct datums on the scanned model adaptive to the actual geometry. Dimensional
measurement based on the generated datums can be achieved automatically. Moreover,
the solution does not require users to manually preprocess the point cloud, such as
outlier and noise removal. As such, it improves the productivity in dimensional inspection.},
  doi       = {10.1145/3373724.3373726},
  isbn      = {9781450372350},
  keywords  = {Inspection, Datum generation, Dimensional measurement},
  location  = {Singapore, Singapore},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3373724.3373726},
}

@InProceedings{Kaemaeraeinen2014,
  author    = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Xiao, Yu and Yl\"{a}-J\"{a}\"{a}ski, Antti},
  booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
  title     = {Towards Pervasive and Mobile Gaming with Distributed Cloud Infrastructure},
  year      = {2014},
  publisher = {IEEE Press},
  series    = {NetGames '14},
  abstract  = {Cloud gaming, where the game is rendered in the cloud and is streamed to an end-user
device through a thin client, is rapidly gaining ground. Latency is still a key challenge
to cloud gaming: highly interactive games can become unplayable even with response
delays below 100 ms. To overcome this issue, we propose to deploy gaming services
on a more distributed cloud infrastructure, and to instantiate gaming servers in close
proximity of the user when necessary in order to shorten the response delay. Our prototype
distributed cloud gaming platform also allows flexible configuration of gaming controls
and video streams, enabling the use of public displays in mobile cloud gaming. We
test our prototype with two games in different deployment scenarios, and measure the
response delay and power consumption of the mobile devices. Our experiment results
confirm that it is feasible to improve the quality of gaming experience through the
deployment strategies provided by the proposed system.},
  articleno = {16},
  location  = {Nagoya, Japan},
  numpages  = {6},
}

@InProceedings{Chhetri2016,
  author    = {Chhetri, Mohan Baruwal and Vo, Quoc Bao and Kowalczyk, Ryszard},
  booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
  title     = {CL-SLAM: Cross-Layer SLA Monitoring Framework for Cloud Service-Based Applications},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {30–36},
  publisher = {Association for Computing Machinery},
  series    = {UCC '16},
  abstract  = {Modern applications are increasingly being composed from multiple components that
require and consume services at different layers of the cloud stack. The diverse,
dynamic and unpredictable nature of both cloud services and application workloads
makes quality-assured provision of such cloud service-based applications (CSBAs) a
major challenge. While elasticity and autoscaling gives CSBA providers the ability
to scale cloud resources on-demand, they require a comprehensive, system-wide view
of the application performance in order to make timely, cost-effective and performance-efficient
scaling decisions. In this paper, we propose, develop and validate CL-SLAM - a Cross-Layer
SLA Monitoring Framework for CSBAs. Its main features include (a) realtime, fine-grained
visibility into CSBA performance, (b) visual descriptive analytics to identify correlations
and inter-dependencies between cross-layer performance metrics, (c) temporal profiling
of CSBA performance, (d) proactive monitoring, detection and root-cause analysis of
SLA violation, and (e) support for both reactive and proactive adaptation in support
of quality-assured CSBA provision. We validate our approach through a prototype implementation.},
  doi       = {10.1145/2996890.2996906},
  isbn      = {9781450346160},
  keywords  = {cross-layer SLA monitoring, cloud service-based application},
  location  = {Shanghai, China},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2996890.2996906},
}

@InProceedings{Oprescu2015,
  author    = {Oprescu, Ana-Maria and (Vintila) Filip, Alexandra and Kielmann, Thilo},
  booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
  title     = {Fast Pareto Front Approximation for Cloud Instance Pool Optimization},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {1443–1444},
  publisher = {Association for Computing Machinery},
  series    = {GECCO Companion '15},
  abstract  = {Computing the Pareto Set (PS) of optimal cloud schedules in terms of cost and makespan
for a given application and set of cloud instance types is NP-complete. Moreover,
cloud instances' volatility requires fast PS recomputations. While genetic algorithms
(GA) are a promising approach, little knowledge of an approximated PS's quality leads
to GAs running for overly many generations, contradicting the goal of quickly computing
an approximate solution. We address this with MOO-GA, our GA enhanced with a domain-tailored
termination criteria delivering fast, well-approximated Pareto sets. We compare to
NSGAIII using PS convergence and diversity, and computational effort metrics. Results
show MOO-GA consistently computing better quality Pareto sets within one second on
average (df=98, p-value&lt;10-3).},
  doi       = {10.1145/2739482.2764720},
  isbn      = {9781450334884},
  keywords  = {pareto frontier, genetic algorithms, infrastructure-as-a-service},
  location  = {Madrid, Spain},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2739482.2764720},
}

@InProceedings{Li2020b,
  author    = {Li, Yen-Chun and Hsu, Chia-Hsin and Lin, Yu-Chun and Hsu, Cheng-Hsin},
  booktitle = {Proceedings of the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications},
  title     = {Performance Measurements on a Cloud VR Gaming Platform},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {37–45},
  publisher = {Association for Computing Machinery},
  series    = {QoEVMA'20},
  abstract  = {As cloud gaming and Virtual Reality (VR) games become popular in the game industry,
game developers engage in these fields to boost their sales. Because cloud gaming
possesses the merit of lifting computation loads from client devices to servers, it
solves the high resource consumption issue of VR games on regular clients. However,
it is important to know where is the bottleneck of the cloud VR gaming platform and
how can it be improved in the future. In this paper, we conduct extensive experiments
on the state-of-the-art cloud VR gaming platform--Air Light VR (ALVR). In particular,
we analyze the performance of ALVR using both Quality-of-Service and Quality-of-Experience
metrics. Our experiments reveal that latency (up to 90 ms RTT) has less influence
on user experience compared to bandwidth limitation (as small as 35 Mbps) and packet
loss rate (as high as 8%) . Moreover, we find that VR gamers can hardly notice the
difference between the gaming experience with different latency values (between 0
and 90 ms RTT). Such findings shed some lights on how to further improve the cloud
VR gaming platform, e.g., a budget of up to 90 ms RTT may be used to absorb network
dynamics when bandwidth is insufficient.},
  doi       = {10.1145/3423328.3423497},
  isbn      = {9781450381581},
  keywords  = {measurement, prototype, computer games, cloud computing, virtual reality},
  location  = {Seattle, WA, USA},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3423328.3423497},
}

@InProceedings{Alcivar2019,
  author    = {Alcivar, Nayeth I. Solorzano and Gallego, Diego Carrera and Quijije, Lissenia Sornoza and Quelal, Marco Mendoza},
  booktitle = {Proceedings of the 2019 2nd International Conference on Computers in Management and Business},
  title     = {Developing a Dashboard for Monitoring Usability of Educational Games Apps for Children},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {70–75},
  publisher = {Association for Computing Machinery},
  series    = {ICCMB 2019},
  abstract  = {Nowadays digital game applications or interactive children's educational games implemented
in mobile devices (to be identified as Apps), are beginning to be widely used to complement
children's education, particularly during early childhood education. However, digital
game Apps do not generate a timely collection of data that could be obtained, so that
with a proper interpretation they can serve as a guide in making decisions about the
content, types, and level of games that should be created as digital tools to support
children's education. In this article, is indicated how through the development of
a dashboard, linked to a database in the cloud, it is possible to obtain and present
information that allows measuring the use and playability and usability factors for
these types of Apps, in an orderly and precise manner. For the development of the
dashboard and its link in real time with the Apps to monitor, JavaScript was used
through the framework Sails.js and the database implemented in PostgreSQL. In parallel,
for the data transmission tests, two mobile applications were implemented in Android,
one programmed in Unity and the second using Adobe Animate. Both Apps were designed
by recording internal data in JSON file format. To analyze and obtain results, we
used PQM metrics 2014 (Playability Quality Model), and we applied an adapted theory
which helps to facilitate the identification of factors affecting the use and adoption
of information systems and technologies in Latin American local contexts. The Pilot
tests were carried out with children from 4 to 8 years attending schools of marginal
areas in the city of Guayaquil, Ecuador. These children with little knowledge of technology
use, facilitate better evaluation of different scenarios to measure the behavioral
use of the Apps and their contents without significant influence of previous knowledge
about digital educational games. This article presents the first results of an extensive
and longitudinal multidisciplinary research, relevant to organizations and people
involved in early childhood education.},
  doi       = {10.1145/3328886.3328892},
  isbn      = {9781450361682},
  keywords  = {Apps, Dashboard, Latin America, Usability, Technology Adoption and Education, Ecuador, Children, Digital Games, MIDI},
  location  = {Cambridge, United Kingdom},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3328886.3328892},
}

@InProceedings{Mekuria2016,
  author    = {Mekuria, Rufael and Cesar, Pablo},
  booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
  title     = {MP3DG-PCC, Open Source Software Framework for Implementation and Evaluation of Point Cloud Compression},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {1222–1226},
  publisher = {Association for Computing Machinery},
  series    = {MM '16},
  abstract  = {We present MP3DG-PCC, an open source framework for design, implementation and evaluation
of point cloud compression algorithms. The framework includes objective quality metrics,
lossy and lossless anchor codecs, and a test bench for consistent comparative evaluation.
The framework and proposed methodology is in use for the development of an international
point cloud compression standard in MPEG. In addition, the library is integrated with
the popular point cloud library, making a large number of point cloud processing available
and aligning the work with the broader open source community.},
  doi       = {10.1145/2964284.2973806},
  isbn      = {9781450336031},
  keywords  = {point cloud compression, evaluation, compression, 3d virtual reality},
  location  = {Amsterdam, The Netherlands},
  numpages  = {5},
  url       = {https://doi.org/10.1145/2964284.2973806},
}

@InProceedings{Vijayan2020,
  author    = {Vijayan, Vipin and Gu, Shawn and Krebs, Eric T. and Meng, Lei and Milenkovi\'{c}, Tijana},
  booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
  title     = {Pairwise Versus Multiple Global Network Alignment},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {BCB '20},
  abstract  = {This abstract is based on the following paper: Vijayan, Vipin, Shawn Gu, Eric T. Krebs,
Lei Meng, and Tijana Milenkovi\'{c}. "Pairwise Versus Multiple Global Network Alignment."
IEEE Access 8 (2020): 41961--41974.Proteins, the major macromolecules of life, interact
with each other to carry out cellular functioning. Thus, analyses of protein-protein
interaction (PPI) networks can yield important insights into biological function,
disease, and evolution. While biotechnological advancements have made PPI network
data available for many species, functions of many proteins in many of these species
remain unknown. One way to uncover these functions is to transfer biological knowledge
from a well-studied species to a poorly-studied one. Genomic sequence alignment, which
has revolutionized our biomedical understanding, can be used for this purpose. However,
sequence alignment has a major drawback: it does not consider interactions between
proteins (which are ultimately what carry out function). So, biological network alignment
(NA) can be used in a complementary fashion to predict protein functional knowledge
that sequence alignment alone cannot predict. Specifically, NA compares PPI networks
of different species to find regions of their similarity (or conservation), thus allowing
for the transfer of functional knowledge across conserved network (rather than just
sequence) regions.Like genomic sequence alignment, NA can be local or global. Just
as the recent trend in the NA field, we also focus on global NA, which can be pairwise
(PNA) and multiple (MNA). While PNA aligns two networks, MNA can align more than two
networks at once. Since MNA can capture conserved network regions between more networks
than PNA, it is hypothesized that MNA leads to deeper biological insights compared
to PNA. However, due to different outputs of PNA and MNA, a PNA method is only compared
to other PNA methods, and an MNA method is only compared to other MNA methods. Comparison
of PNA against MNA must be done to evaluate whether MNA indeed yields more biologically
meaningful alignments than PNA, as only this would justify MNA's higher computational
complexity.We introduce a framework that allows for this. We evaluate eight prominent
PNA and MNA methods, on synthetic and real-world biological networks, using topological
and functional alignment quality measures. We compare PNA against MNA in both a pairwise
(native to PNA) and multiple (native to MNA) manner. PNA is expected to lead to higher-quality
alignments than MNA under the pairwise evaluation framework. Indeed, this is what
we find. MNA is expected to lead to higher-quality alignments than PNA under the multiple
evaluation framework. Shockingly, we find this not always to hold; PNA is often better
than MNA in this framework, depending on the choice of evaluation test. Thus, we believe
that any new MNA methods should be compared not just to existing MNA methods, but
also to existing PNA methods using our evaluation framework, to properly judge the
quality of alignments that they produce. Also, we confirm empirically that PNA is
faster than MNA in both evaluation frameworks. These results indicate that currently,
MNA offers little advantage over PNA; in order for MNA to gain an advantage, a drastic
redesign of MNA's current algorithmic principles might be needed.},
  articleno = {4},
  doi       = {10.1145/3388440.3414205},
  isbn      = {9781450379649},
  keywords  = {multi-network comparison, protein function prediction, biological network alignment},
  location  = {Virtual Event, USA},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3388440.3414205},
}

@InProceedings{Wamser2016,
  author    = {Wamser, Florian and Seufert, Michael and H\"{o}fner, Steffen and Tran-Gia, Phuoc},
  booktitle = {Proceedings of the 2016 Workshop on QoE-Based Analysis and Management of Data Communication Networks},
  title     = {Concept for Client-Initiated Selection of Cloud Instances for Improving QoE of Distributed Cloud Services},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {49–54},
  publisher = {Association for Computing Machinery},
  series    = {Internet-QoE '16},
  abstract  = {We introduce a concept for client-initiated selection of service location and service
quality for improving the Quality of Experience (QoE) of general cloud services. It
is loosely based on the HTTP adaptive streaming approach (e.g., MPEG DASH). A manifest
file compiled by the cloud service provider specifies the available service locations
and qualities, from which the user selects the optimal service instance based on contextual
information obtained from client measurements and user preferences. The proposed concept
is defined and is implemented in two client-based decision algorithms for improving
the QoE of a simple picture gallery cloud service. These decision algorithms are evaluated
and their impact on the service delivery is discussed. The evaluation shows that it
is possible to improve the service location and quality selection by light-weight
client-based algorithms.},
  doi       = {10.1145/2940136.2940143},
  isbn      = {9781450344258},
  keywords  = {Quality of Experience, Cloud Services, Client-based Access for Cloud Services},
  location  = {Florianopolis, Brazil},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2940136.2940143},
}

@InProceedings{Evans2015,
  author    = {Evans, Kieran and Jones, Andrew and Preece, Alun and Quevedo, Francisco and Rogers, David and Spasi\'{c}, Irena and Taylor, Ian and Stankovski, Vlado and Taherizadeh, Salman and Trnkoczy, Jernej and Suciu, George and Suciu, Victor and Martin, Paul and Wang, Junchao and Zhao, Zhiming},
  booktitle = {Proceedings of the 10th Workshop on Workflows in Support of Large-Scale Science},
  title     = {Dynamically Reconfigurable Workflows for Time-Critical Applications},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {WORKS '15},
  abstract  = {Cloud-based applications that depend on time-critical data processing or network throughput
require the capability of reconfiguring their infrastructure on demand as and when
conditions change. Although the ability to apply quality of service constraints on
the current Cloud offering is limited, there are ongoing efforts to change this. One
such effort is the European funded SWITCH project that aims to provide a programming
model and toolkit to help programmers specify quality of service and quality of experience
metrics of their distributed application and to provide the means to specify the reconfiguration
actions which can be taken to maintain these requirements. In this paper, we present
an approach to application reconfiguration by applying a workflow methodology to implement
a prototype involving multiple reconfiguration scenarios of a distributed real-time
social media analysis application, called Sentinel. We show that by using a lightweight
RPC-based workflow approach, we can monitor a live application in real time and spawn
dependency-based workflows to reconfigure the underlying Docker containers that implement
the distributed components of the application. We propose to use this prototype as
the basis for part of the SWITCH workbench, which will support more advanced programmable
infrastructures.},
  articleno = {7},
  doi       = {10.1145/2822332.2822339},
  isbn      = {9781450339896},
  keywords  = {quality of service, workflows, time-critical applications, quality of experience, dynamic data driven systems},
  location  = {Austin, Texas},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2822332.2822339},
}

@InProceedings{Debattista2019,
  author    = {Debattista, Jeremy and Attard, Judie and Brennan, Rob and O'Sullivan, Declan},
  booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
  title     = {Is the LOD Cloud at Risk of Becoming a Museum for Datasets? Looking Ahead towards a Fully Collaborative and Sustainable LOD Cloud},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {850–858},
  publisher = {Association for Computing Machinery},
  series    = {WWW '19},
  abstract  = {The Linked Open Data (LOD) cloud has been around since 2007. Throughout the years,
this prominent depiction served as the epitome for Linked Data and acted as a starting
point for many. In this article we perform a number of experiments on the dataset
metadata provided by the LOD cloud, in order to understand better whether the current
visualised datasets are accessible and with an open license. Furthermore, we perform
quality assessment of 17 metrics over accessible datasets that are part of the LOD
cloud. These experiments were compared with previous experiments performed on older
versions of the LOD cloud. The results showed that there was no improvement on previously
identified problems. Based on our findings, we therefore propose a strategy and architecture
for a potential collaborative and sustainable LOD cloud.},
  doi       = {10.1145/3308560.3317075},
  isbn      = {9781450366755},
  keywords  = {LOD cloud, metadata quality, sustainable services, Linked Data, data quality},
  location  = {San Francisco, USA},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3308560.3317075},
}

@InProceedings{Shatnawi2018,
  author    = {Shatnawi, Anas and Orr\`{u}, Matteo and Mobilio, Marco and Riganelli, Oliviero and Mariani, Leonardo},
  booktitle = {Proceedings of the 1st International Workshop on Software Health},
  title     = {Cloudhealth: A Model-Driven Approach to Watch the Health of Cloud Services},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {40–47},
  publisher = {Association for Computing Machinery},
  series    = {SoHeal '18},
  abstract  = {Cloud systems are complex and large systems where services provided by different operators
must coexist and eventually cooperate. In such a complex environment, controlling
the health of both the whole environment and the individual services is extremely
important to timely and effectively react to misbehaviours, unexpected events, and
failures. Although there are solutions to monitor cloud systems at different granularity
levels, how to relate the many KPIs that can be collected about the health of the
system and how health information can be properly reported to operators are open questions.This
paper reports the early results we achieved in the challenge of monitoring the health
of cloud systems. In particular we present CloudHealth, a model-based health monitoring
approach that can be used by operators to watch specific quality attributes. The Cloud-Health
Monitoring Model describes how to operationalize high level monitoring goals by dividing
them into subgoals, deriving metrics for the subgoals, and using probes to collect
the metrics. We use the CloudHealth Monitoring Model to control the probes that must
be deployed on the target system, the KPIs that are dynamically collected, and the
visualization of the data in dashboards.},
  doi       = {10.1145/3194124.3194130},
  isbn      = {9781450357302},
  keywords  = {quality model, software health, monitoring, cloud service, monitoring model, metrics, KPI},
  location  = {Gothenburg, Sweden},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3194124.3194130},
}

@InProceedings{Klugman2014,
  author    = {Klugman, Noah and Rosa, Javier and Pannuto, Pat and Podolsky, Matthew and Huang, William and Dutta, Prabal},
  booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
  title     = {Grid Watch: Mapping Blackouts with Smart Phones},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {HotMobile '14},
  abstract  = {The power grid is one of humanity's most significant engineering undertakings and
it is essential in developed and developing nations alike. Currently, transparency
into the power grid relies on utility companies and more fine-grained insight is provided
by costly smart meter deployments. We claim that greater visibility into power grid
conditions can be provided in an inexpensive and crowd-sourced manner independent
of utility companies by leveraging existing smartphones. Our key insight is that an
unmodified smartphone can detect power outages by monitoring changes to its own power
state, locally verifying these outages using a variety of sensors that reduce the
likelihood of false power outage reports, and corroborating actual reports with other
phones through data aggregation in the cloud. The proposed approach enables a decentralized
system that can scale, potentially providing researchers and concerned citizens with
a powerful new tool to analyze the power grid and hold utility companies accountable
for poor power quality. This paper demonstrates the viability of the basic idea, identifies
a number of challenges that are specific to this application as well as ones that
are common to many crowd-sourced applications, and highlights some improvements to
smartphone operating systems that could better support such applications in the future.},
  articleno = {1},
  doi       = {10.1145/2565585.2565607},
  isbn      = {9781450327428},
  keywords  = {side channel information, power monitoring, smartphone applications, smart grid, crowdsourcing},
  location  = {Santa Barbara, California},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2565585.2565607},
}

@InProceedings{Grohmann2019,
  author    = {Grohmann, Johannes and Nicholson, Patrick K. and Iglesias, Jesus Omana and Kounev, Samuel and Lugones, Diego},
  booktitle = {Proceedings of the 20th International Middleware Conference},
  title     = {Monitorless: Predicting Performance Degradation in Cloud Applications with Machine Learning},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {149–162},
  publisher = {Association for Computing Machinery},
  series    = {Middleware '19},
  abstract  = {Today, software operation engineers rely on application key performance indicators
(KPIs) for sizing and orchestrating cloud resources dynamically. KPIs are monitored
to assess the achievable performance and to configure various cloud-specific parameters
such as flavors of instances and autoscaling rules, among others. Usually, keeping
KPIs within acceptable levels requires application expertise which is expensive and
can slow down the continuous delivery of software. Expertise is required because KPIs
are normally based on application-specific quality-of-service metrics, like service
response time and processing rate, instead of generic platform metrics, like those
typical across various environments (e.g., CPU and memory utilization, I/O rate, etc.)In
this paper, we investigate the feasibility of outsourcing the management of application
performance from developers to cloud operators. In the same way that the serverless
paradigm allows the execution environment to be fully managed by a third party, we
discuss a monitorless model to streamline application deployment by delegating performance
management. We show that training a machine learning model with platform-level data,
collected from the execution of representative containerized services, allows inferring
application KPI degradation. This is an opportunity to simplify operations as engineers
can rely solely on platform metrics -- while still fulfilling application KPIs --
to configure portable and application agnostic rules and other cloud-specific parameters
to automatically trigger actions such as autoscaling, instance migration, network
slicing, etc.Results show that monitorless infers KPI degradation with an accuracy
of 97% and, notably, it performs similarly to typical autoscaling solutions, even
when autoscaling rules are optimally tuned with knowledge of the expected workload.},
  doi       = {10.1145/3361525.3361543},
  isbn      = {9781450370097},
  keywords  = {Cloud computing, DevOps, Machine learning, Monitoring},
  location  = {Davis, CA, USA},
  numpages  = {14},
  url       = {https://doi.org/10.1145/3361525.3361543},
}

@InProceedings{Adegboyega2015,
  author    = {Adegboyega, Abiola},
  booktitle = {Proceedings of the International Workshop on Virtualization Technologies},
  title     = {An Adaptive Resource Provisioning Scheme for Effective QoS Maintenance in the IaaS Cloud},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {VT15},
  abstract  = {Effective bandwidth provisioning is of vital importance in the virtualized cloud where
tenants with unique SLAs share a finite network. Different tenants collocated on the
same physical server deployed with increasing VM density necessitates Quality of Service
(QoS) provisioning beginning at the hypervisor. Recent efforts at provisioning the
cloud network through various reservation methodologies have achieved some measure
of success. However most of them do not account for the entire path over which application
components communicate and cannot provide the necessary Service Level Agreement (SLA).
Cloud applications components often communicate across multiple network devices aggregated
into layers connected over finite bandwidth links that affect application response.
Furthermore, traffic to and from tenant applications display volatility. In view of
this, we design a virtual network reservation framework that is mindful of application
performance across multiple network devices &amp; traffic volatility. Our network reservation
framework is based on a forecasting engine motivated by the volatility existent in
traffic to and from virtualized cloud environments. This forecasting engine is able
to maintain SLAs by employing dynamic time-series models to develop novel bandwidth
provisioning thresholds that adapt to the time-variation in tenant workloads. We test
the effectiveness of our methods in the OpenStack cloud environment focusing on traffic
directionality in the datacenter network, VM density and QoS across multiple flows
competing for finite bandwidth. Our forecasting method offers a 25% improvement in
prediction accuracy over existing methods while the reservation framework maintains
SLAs at 95%.},
  articleno = {2},
  doi       = {10.1145/2835075.2835078},
  isbn      = {9781450337328},
  keywords  = {Virtualization, QoS, Forecasting, SDN, Volatility},
  location  = {Vancouver, BC, Canada},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2835075.2835078},
}

@InProceedings{Zhao2017a,
  author    = {Zhao, Yang and Xia, Nai and Tian, Chen and Li, Bo and Tang, Yizhou and Wang, Yi and Zhang, Gong and Li, Rui and Liu, Alex X.},
  booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
  title     = {Performance of Container Networking Technologies},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {1–6},
  publisher = {Association for Computing Machinery},
  series    = {HotConNet '17},
  abstract  = {Container networking is now an important part of cloud virtualization architectures.
It provides network access for containers by connecting both virtual and physical
network interfaces. The performance of container networking has multiple dependencies,
and each factor may significantly affect the performance. In this paper, we perform
systematic experiments to study the performance of container networking technologies.
For every measurement result, we try our best to qualify influencing factors.},
  doi       = {10.1145/3094405.3094406},
  isbn      = {9781450350587},
  keywords  = {Container, Measurement, Networking},
  location  = {Los Angeles, CA, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3094405.3094406},
}

@InProceedings{Herbst2015,
  author    = {Herbst, Nikolas Roman and Kounev, Samuel and Weber, Andreas and Groenda, Henning},
  booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
  title     = {BUNGEE: An Elasticity Benchmark for Self-Adaptive IaaS Cloud Environments},
  year      = {2015},
  pages     = {46–56},
  publisher = {IEEE Press},
  series    = {SEAMS '15},
  abstract  = {Today's infrastructure clouds provide resource elasticity (i.e. auto-scaling) mechanisms
enabling self-adaptive resource provisioning to reflect variations in the load intensity
over time. These mechanisms impact on the application performance, however, their
effect in specific situations is hard to quantify and compare. To evaluate the quality
of elasticity mechanisms provided by different platforms and configurations, respective
metrics and benchmarks are required. Existing metrics for elasticity only consider
the time required to provision and deprovision resources or the costs impact of adaptations.
Existing benchmarks lack the capability to handle open workloads with realistic load
intensity profiles and do not explicitly distinguish between the performance exhibited
by the provisioned underlying resources, on the one hand, and the quality of the elasticity
mechanisms themselves, on the other hand.In this paper, we propose reliable metrics
for quantifying the timing aspects and accuracy of elasticity. Based on these metrics,
we propose a novel approach for benchmarking the elasticity of Infrastructure-as-a-Service
(IaaS) cloud platforms independent of the performance exhibited by the provisioned
underlying resources. We show that the proposed metrics provide consistent ranking
of elastic platforms on an ordinal scale. Finally, we present an extensive case study
of real-world complexity demonstrating that the proposed approach is applicable in
realistic scenarios and can cope with different levels of resource efficiency.},
  location  = {Florence, Italy},
  numpages  = {11},
}

@InProceedings{Schwind2019,
  author    = {Schwind, Anika and Haberzettl, Lorenz and Wamser, Florian and Ho\ss{}feld, Tobias},
  booktitle = {Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks},
  title     = {QoE Analysis of Spotify Audio Streaming and App Browsing},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {25–30},
  publisher = {Association for Computing Machinery},
  series    = {Internet-QoE'19},
  abstract  = {Spotify is the most-listened audio streaming provider in 2019 with 217 million active
users per month. Providers are therefore interested in the quality and functionality
of Spotify in order to provide their users with the best possible streaming quality.
While video streaming services such as Netflix and their streaming approach have been
extensively explored in previous research, audio streaming services like Spotify and
their corresponding behavior at certain network conditions have not been considered
in detail yet. In this paper, we perform a QoE analysis under various network conditions
and examine the app browsing performance of the audio streaming platform Spotify using
its native Android mobile application. We have developed a measurement tool that emulates
a user listening to audio through Spotify. While streaming, application and network
layer parameters are captured that have a high correlation to the user's QoE. The
paper shows a baseline scenario including the streaming of a single song as well as
playlist streaming behavior. Next, the effect of interruptions on the streaming behavior
is evaluated and finally, the influence of network impairments on QoE key performance
indicators such as initial delay is shown.},
  doi       = {10.1145/3349611.3355546},
  isbn      = {9781450369275},
  keywords  = {mobile application, audio streaming, spotify, qoe, browsing},
  location  = {Los Cabos, Mexico},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3349611.3355546},
}

@InProceedings{Rueth2018,
  author    = {R\"{u}th, Jan and Glebke, Ren\'{e} and Wehrle, Klaus and Causevic, Vedad and Hirche, Sandra},
  booktitle = {Proceedings of the 2018 Morning Workshop on In-Network Computing},
  title     = {Towards In-Network Industrial Feedback Control},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {14–19},
  publisher = {Association for Computing Machinery},
  series    = {NetCompute '18},
  abstract  = {Controlling physical machinery and processes is at the core of production automation.
However, challenged by inflexibility, automation and control is evaluating to outsource
this control to resourceful cloud environments. While this enables to derive better
control through a plethora of measurements, it challenges the control quality through
delay introduced through networks.In this paper, we show how to unify control and
communication by offloading delay sensitive control tasks from the cloud to local
network elements --- a previously unexplored area for in-network processing --- enabling
both, ultra-high quality-of-control and scalable orchestration through cloud environments.
Our implementation demonstrates how we combine state of the art control with communication.
We achieve this by expressing the control and the datapath in P4 which we synthesize
to BPF programs that we execute in XDP environments on Netronome SmartNICs. Further,
we highlight the demands of control towards communication to build more involved and
complex in-network controllers.},
  doi       = {10.1145/3229591.3229592},
  isbn      = {9781450359085},
  location  = {Budapest, Hungary},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3229591.3229592},
}

@InProceedings{Hutchison2017,
  author    = {Hutchison, Dylan and Howe, Bill and Suciu, Dan},
  booktitle = {Proceedings of the 4th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond},
  title     = {LaraDB: A Minimalist Kernel for Linear and Relational Algebra Computation},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {BeyondMR'17},
  abstract  = {Analytics tasks manipulate structured data with variants of relational algebra (RA)
and quantitative data with variants of linear algebra (LA). The two computational
models have overlapping expressiveness, motivating a common programming model that
affords unified reasoning and algorithm design. At the logical level we propose LARA,
a lean algebra of three operators, that expresses RA and LA as well as relevant optimization
rules. We show a series of proofs that position LARA at just the right level of expressiveness
for a middleware algebra: more explicit than MapReduce but more general than RA or
LA. At the physical level we find that the LARA operators afford efficient implementations
using a single primitive that is available in a variety of backend engines: range
scans over partitioned sorted maps.To evaluate these ideas, we implemented the LARA
operators as range iterators in Apache Accumulo, a popular implementation of Google's
BigTable. First we show how LARA expresses a sensor quality control task, and we measure
the performance impact of optimizations LARA admits on this task. Second we show that
the LARADB implementation outperforms Accumulo's native MapReduce integration on a
core task involving join and aggregation in the form of matrix multiply, especially
at smaller scales that are typically a poor fit for scale-out approaches. We find
that LARADB offers a conceptually lean framework for optimizing mixed-abstraction
analytics tasks, without giving up fast record-level updates and scans.},
  articleno = {2},
  doi       = {10.1145/3070607.3070608},
  isbn      = {9781450350198},
  location  = {Chicago, IL, USA},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3070607.3070608},
}

@InProceedings{Nasser2015,
  author    = {Nasser, Soliman and Barry, Andew and Doniec, Marek and Peled, Guy and Rosman, Guy and Rus, Daniela and Volkov, Mikhail and Feldman, Dan},
  booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
  title     = {Fleye on the Car: Big Data Meets the Internet of Things},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {382–383},
  publisher = {Association for Computing Machinery},
  series    = {IPSN '15},
  abstract  = {Vehicle-based vision algorithms, such as the collision alert systems [4], are able
to interpret a scene in real-time and provide drivers with immediate feedback. However,
such technologies are based on cameras on the car, limited to the vicinity of the
car, severely limiting their potential. They cannot find empty parking slots, bypass
traffic jams, or warn about dangers outside the car's immediate surrounding. An intelligent
driving system augmented with additional sensors and network inputs may significantly
reduce the number of accidents, improve traffic congestion, and care for the safety
and quality of people's lives.We propose an open-code system, called Fleye, that consists
of an autonomous drone (nano quadrotor) that carries a radio camera and flies few
meters in front and above the car. The streaming video is transmitted in real time
from the quadcopter to Amazon's EC2 cloud together with information about the driver,
the drone, and the car's state. The output is then transmitted to the "smart glasses"
of the driver. The control of the drone, as well as the sensor data collection from
the driver, is done by low cost (&lt;30$) minicomputer. Most computation is done in the
cloud, allowing straightforward integration of multiple vehicle behaviour and additional
sensors, as well as greater computational capability.},
  doi       = {10.1145/2737095.2742919},
  isbn      = {9781450334754},
  keywords  = {video streaming, internet of things, quadrotors, collision alert system},
  location  = {Seattle, Washington},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2737095.2742919},
}

@InProceedings{Qamar2014,
  author    = {Qamar, Ahmad M. and Afyouni, Imad and Rahman, Md. Abdur and Rehman, Faizan Ur and Hussain, Delwar and Basalamah, Saleh and Lbath, Ahmed},
  booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  title     = {A GIS-Based Serious Game Interface for Therapy Monitoring},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {589–592},
  publisher = {Association for Computing Machinery},
  series    = {SIGSPATIAL '14},
  abstract  = {In this paper, we present a novel idea of a map-based therapy environment for people
with Hemiplegia. The therapy environment is designed according to the suggestions
of therapists, which consists of a spatial map browsing serious game augmented with
our novel multi-sensory natural user interface (NUI). The NUI is based on 3D motion
sensors that can recognize different hand and body gestures used for browsing a 3D
or 2D map. The 3D motion sensors work in a non-invasive way; hence, they do not require
any wearable body attachments and can be used at home without assistance from the
therapists. The map-browsing environment provides an immersive experience to the disabled
users, which helps in performing therapy in an interesting and entertaining manner.
We have developed analytics for measuring certain quality of health improvement metrics
from each type of spatial map browsing movements. The 3D motion sensors have been
tested with Nokia, Google, ESRI, and a number of other maps that allow a subject to
visualize and browse the 3D and 2D maps of the world. The map browsing session data
shows the nature of big data; hence, the session data is stored in a cloud environment.
Our developed serious game environment is web-based; thus anyone having the appropriate
low cost sensor hardware can plug it in and start experiencing a natural way of hands
free map browsing. We have deployed our framework in a hospital that treats Hemiplegic
patients. Based on the feedback obtained, the developed platform shows a huge potential
for use in hospitals that provide physiotherapy services as well as at patients' home
as an assistive therapeutic service.},
  doi       = {10.1145/2666310.2666376},
  isbn      = {9781450331319},
  keywords  = {therapy, kinect, GIS, leap, e-health, serious games},
  location  = {Dallas, Texas},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2666310.2666376},
}

@InProceedings{Sun2015,
  author    = {Sun, Peng and Vanbever, Laurent and Rexford, Jennifer},
  booktitle = {Proceedings of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research},
  title     = {Scalable Programmable Inbound Traffic Engineering},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SOSR '15},
  abstract  = {With the rise of video streaming and cloud services, enterprise and access networks
receive much more traffic than they send, and must rely on the Internet to offer good
end-to-end performance. These edge networks often connect to multiple ISPs for better
performance and reliability, but have only limited ways to influence which of their
ISPs carries the traffic for each service. In this paper, we present Sprite, a software-defined
solution for flexible inbound traffic engineering (TE). Sprite offers direct, fine-grained
control over inbound traffic, by announcing different public IP prefixes to each ISP,
and performing source network address translation (SNAT) on outbound request traffic.
Our design achieves scalability in both the data plane (by performing SNAT on edge
switches close to the clients) and the control plane (by having local agents install
the SNAT rules). The controller translates high-level TE objectives, based on client
and server names, as well as performance metrics, to a dynamic network policy based
on real-time traffic and performance measurements. We evaluate Sprite with live data
from "in the wild" experiments on an EC2-based testbed, and demonstrate how Sprite
dynamically adapts the network policy to achieve high-level TE objectives, such as
balancing YouTube traffic among ISPs to improve video quality.},
  articleno = {12},
  doi       = {10.1145/2774993.2775063},
  isbn      = {9781450334518},
  keywords  = {software-defined networking, scalability, traffic engineering},
  location  = {Santa Clara, California},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2774993.2775063},
}

@InProceedings{Loh2019,
  author    = {Loh, Frank and Vomhoff, Viktoria and Wamser, Florian and Metzger, Florian and Ho\ss{}feld, Tobias},
  booktitle = {Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks},
  title     = {Traffic Measurement Study on Video Streaming with the Amazon Echo Show},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {31–36},
  publisher = {Association for Computing Machinery},
  series    = {Internet-QoE'19},
  abstract  = {The Amazon Echo Show is one of the most widely used smart speakers with the ability
to stream video. Due to its popularity, the traffic profiles of such devices are of
interest to network operators and providers. This work presents a measurement study
of the Amazon Echo Show in terms of network traffic and streaming behavior. More than
470,hours of streaming data are collected and analyzed at network layer. Based on
this, streaming quality is derived at application layer. The study quantifies the
traffic and shows that streaming with the Amazon Echo Show is comparable to streaming
with a native web browser, but in a more conservative way.},
  doi       = {10.1145/3349611.3355543},
  isbn      = {9781450369275},
  keywords  = {alexa, traffic analysis, qoe, amazon echo, streaming},
  location  = {Los Cabos, Mexico},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3349611.3355543},
}

@InProceedings{Huang2021,
  author    = {Huang, Tianchi and Zhang, Rui-Xiao and Sun, Lifeng},
  booktitle = {Proceedings of the 31st ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
  title     = {Deep Reinforced Bitrate Ladders for Adaptive Video Streaming},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {66–73},
  publisher = {Association for Computing Machinery},
  series    = {NOSSDAV '21},
  abstract  = {In the typical transcoding pipeline for adaptive video streaming, raw videos are pre-chunked
and pre-encoded according to a set of resolution-bitrate or resolution-quality pairs
on the server-side, where the pair is often named as bitrate ladder. Different from
existing heuristics, we argue that a good bitrate ladder should be optimized by considering
video content features, network capacity, and storage costs on the cloud. We propose
DeepLadder, a per-chunk optimization scheme which adopts state-of-the-art deep reinforcement
learning (DRL) method to optimize the bitrate ladder w.r.t the above concerns. Technically,
DeepLadder selects the proper setting for each video resolution autoregressively.
We use over 8,000 video chunks, measure over 1,000,000 perceptual video qualities,
collect real-world network traces for more than 50 hours, and invent faithful virtual
environments to help train DeepLadder efficiently. Across a series of comprehensive
experiments on both Constant Bitrate (CBR) and Variable Bitrate (VBR)-encoded videos,
we demonstrate significant improvements in average video quality bandwidth utilization,
and storage overhead in comparison to prior work as well as the ability to be deployed
in the real-world transcoding framework.},
  doi       = {10.1145/3458306.3458873},
  isbn      = {9781450384353},
  keywords  = {bitrate ladder, adaptive video streaming},
  location  = {Istanbul, Turkey},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3458306.3458873},
}

@InProceedings{Assaf2015,
  author    = {Assaf, Ahmad and Senart, Aline and Troncy, Rapha\"{e}l},
  booktitle = {Proceedings of the 24th International Conference on World Wide Web},
  title     = {Roomba: Automatic Validation, Correction and Generation of Dataset Metadata},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {159–162},
  publisher = {Association for Computing Machinery},
  series    = {WWW '15 Companion},
  abstract  = {Data is being published by both the public and private sectors and covers a diverse
set of domains ranging from life sciences to media or government data. An example
is the Linked Open Data (LOD) cloud which is potentially a gold mine for organizations
and individuals who are trying to leverage external data sources in order to produce
more informed business decisions. Considering the significant variation in size, the
languages used and the freshness of the data, one realizes that spotting spam datasets
or simply finding useful datasets without prior knowledge is increasingly complicated.
In this paper, we propose Roomba, a scalable automatic approach for extracting, validating,
correcting and generating descriptive linked dataset profiles. While Roomba is generic,
we target CKAN-based data portals and we validate our approach against a set of open
data portals including the Linked Open Data (LOD) cloud as viewed on the DataHub.
The results demonstrate that the general state of various datasets and groups, including
the LOD cloud group, needs more attention as most of the datasets suffer from bad
quality metadata and lack some informative metrics that are required to facilitate
dataset search.},
  doi       = {10.1145/2740908.2742827},
  isbn      = {9781450334730},
  keywords  = {dataset profile, data quality, linked data, metadata},
  location  = {Florence, Italy},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2740908.2742827},
}

@InProceedings{Gibson2016,
  author    = {Gibson, Marsalis T. and Rosa, Javier and Brewer, Eric A.},
  booktitle = {Proceedings of the 7th Annual Symposium on Computing for Development},
  title     = {MDB: A Metadata Tracking Microcontroller Micro-Database},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ACM DEV '16},
  abstract  = {This work in progress explores a database designed to enable data sharing on custom
hardware data collection devices and prototypes. Projects and systems are frequently
based on the Arduino framework, examples include ODK's FoneAstra [3], the Open Energy
Monitor [7], and the Grove system of sensors [5]. The Arduino platform is targeted
because of its ease of use, community support, and low cost as a data collecting device
compared to other off-the-shelf sensors. However, there is a need for a framework
suitable for microcontrollers that enable ease of integration into other data collection
systems. This includes the ability to synchronize data with collection and aggregation
devices designed to work offline as well as the ability to track sensors and describe
data sources for other machines and users. To address the issue, we propose a solution
based on an existing small database usable on the Arduino platform that would integrate
into the Mezuri [6] data collection system. The database is designed to fit within
the running memory constraints on a microcontroller to store sensor data with relatively
few fields per reading on flash media. This framework, with explicit support for metadata,
enables users in emerging regions to directly measure physical quantities as well
as indirectly measure human behavior in future development projects involving direct
sensing. The database can be used by a non-expert. In particular, we investigate the
qualities that a technically inclined social scientist would look for when storing
such data on microcontrollers. To enable Mezuri integration we will support metadata
as a first class object accessible with additional utility functions and native synchronization
support.},
  articleno = {36},
  doi       = {10.1145/3001913.3006645},
  isbn      = {9781450346498},
  keywords  = {Metadata, Arduino, Embedded Databases, Emerging Regions, Sensors, Data Collection, Microcontroller},
  location  = {Nairobi, Kenya},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3001913.3006645},
}

@Article{Moghaddam2019,
  author     = {Moghaddam, Sara Kardani and Buyya, Rajkumar and Ramamohanarao, Kotagiri},
  journal    = {ACM Comput. Surv.},
  title      = {Performance-Aware Management of Cloud Resources: A Taxonomy and Future Directions},
  year       = {2019},
  issn       = {0360-0300},
  month      = aug,
  number     = {4},
  volume     = {52},
  abstract   = {The dynamic nature of the cloud environment has made the distributed resource management
process a challenge for cloud service providers. The importance of maintaining quality
of service in accordance with customer expectations and the highly dynamic nature
of cloud-hosted applications add new levels of complexity to the process. Advances
in big-data learning approaches have shifted conventional static capacity planning
solutions to complex performance-aware resource management methods. It is shown that
the process of decision-making for resource adjustment is closely related to the behavior
of the system, including the utilization of resources and application components.
Therefore, a continuous monitoring of system attributes and performance metrics provides
the raw data for the analysis of problems affecting the performance of the application.
Data analytic methods, such as statistical and machine-learning approaches, offer
the required concepts, models, and tools to dig into the data and find general rules,
patterns, and characteristics that define the functionality of the system. Obtained
knowledge from the data analysis process helps to determine the changes in the workloads,
faulty components, or problems that can cause system performance to degrade. A timely
reaction to performance degradation can avoid violations of service level agreements,
including performing proper corrective actions such as auto-scaling or other resource
adjustment solutions. In this article, we investigate the main requirements and limitations
of cloud resource management, including a study of the approaches to workload and
anomaly analysis in the context of performance management in the cloud. A taxonomy
of the works on this problem is presented that identifies main approaches in existing
research from the data analysis side to resource adjustment techniques. Finally, considering
the observed gaps in the general direction of the reviewed works, a list of these
gaps is proposed for future researchers to pursue.},
  address    = {New York, NY, USA},
  articleno  = {84},
  doi        = {10.1145/3337956},
  issue_date = {September 2019},
  keywords   = {Anomaly detection, resource management, big-data analytics, performance management},
  numpages   = {37},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3337956},
}

@InProceedings{Malavolta2020,
  author    = {Malavolta, Ivano and Grua, Eoin Martino and Lam, Cheng-Yu and de Vries, Randy and Tan, Franky and Zielinski, Eric and Peters, Michael and Kaandorp, Luuk},
  booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering Workshops},
  title     = {A Framework for the Automatic Execution of Measurement-Based Experiments on Android Devices},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {61–66},
  publisher = {Association for Computing Machinery},
  series    = {ASE '20},
  abstract  = {Conducting measurement-based experiments is fundamental for assessing the quality
of Android apps in terms of, e.g., energy consumption, CPU, and memory usage. However,
orchestrating such experiments is not trivial as it requires large boilerplate code,
careful setup of measurement tools, and the adoption of various empirical best practices
scattered across the literature. All together, those factors are slowing down the
scientific advancement and harming experiments' replicability in the mobile software
engineering area.In this paper we present Android Runner (AR), a framework for automatically
executing measurement-based experiments on native and web apps running on Android
devices. In AR, an experiment is defined once in a descriptive fashion, and then its
execution is fully automatic, customizable, and replicable. AR is implemented in Python
and it can be extended with third-party profilers.AR has been used in more than 25
scientific studies primarily targeting performance and energy efficiency.},
  doi       = {10.1145/3417113.3422184},
  isbn      = {9781450381284},
  location  = {Virtual Event, Australia},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3417113.3422184},
}

@InProceedings{Belkaroui2018,
  author    = {Belkaroui, Rami and Bertaux, Aur\'{e}lie and Labbani, Ouassila and Hugol-Gential, Cl\'{e}mentine and Nicolle, Christophe},
  booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
  title     = {Towards Events Ontology Based on Data Sensors Network for Viticulture Domain},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {IOT '18},
  abstract  = {Wine Cloud project is the first "Big Data" platform on the french viticulture value
chain. The aim of this platform is to provide a complete traceability of the life
cycle of the wine, from the wine-grower to the consumer. In particular, Wine Cloud
may qualify as an agricultural decision platform that will be used for vine life cycle
management in order to predict the occurrence of major risks (vine diseases, grape
vine pests, physiological risks, fermentation stoppage, oxidation of vine, etc...).
Also to make wine production more rational by offering winegrower a set of recommendation
regarding their strategy's of production development.The proposed platform "Wine Cloud"
is based on heterogeneous sensors network (agricultural machines, plant sensors and
measuring stations) deployed throughout a vineyard. These sensors allow for capturing
data from the agricultural process and remote monitoring vineyards in the Internet
of Things (IoT) era. However, the sensors data from different source is hard to work
together for lack of semantic. Therefore, the task of coherently combining heterogeneous
sensors data becomes very challenging. The integration of heterogeneous data from
sensors can be achieved by data mining algorithms able to build correlations. Nevertheless,
the meaning and the value of these correlations is difficult to perceive without highlighting
the meaning of the data and the semantic description of the measured environment.In
order to bridge this gap and build causality relationships form heterogeneous sensor
data, we propose an ontology-based approach, that consists in exploring heterogeneous
sensor data (light, temperature, atmospheric pressure, etc) in terms of ontologies
enriched with semantic meta-data describing the life cycle of the monitored environment.},
  articleno = {44},
  doi       = {10.1145/3277593.3277619},
  isbn      = {9781450365642},
  keywords  = {semantic sensor data, smart viticulture, ontologies, event ontology, IoT, big data},
  location  = {Santa Barbara, California, USA},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3277593.3277619},
}

@InProceedings{Chikhaoui2021,
  author    = {Chikhaoui, Amina and Lemarchand, Laurent and Boukhalfa, Kamel and Boukhobza, Jalil},
  booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
  title     = {<i>StorNIR</i>, a Multi-Objective Replica Placement Strategy for Cloud Federations},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {50–59},
  publisher = {Association for Computing Machinery},
  series    = {SAC '21},
  abstract  = {Federation of clouds makes it possible to transparently extend the resources of Cloud
Service Providers (CSPs). For storage services several metrics need to be considered
to satisfy customers QoS, that is storage performance, network latency and data availability.
Data replication is a key strategy to optimize such metrics. For a CSP, member of
a Federation, an effective placement of customers data object replicas is crucial
to satisfy QoS demands. In this paper, we modeled the replica placement problem as
a multi-objective optimization problem (MOOP) taking into account the local storage
classes, other federation CSPs (external) storage services, and customers requirements.
To solve this problem, we propose StorNIR a cost-efficient data object Storing scheme
based on NSGAII upgraded with Injection and Reparation operators. StorNIR is a matheuristic
that consists in hybridizing an exact method with NSGAII meta-heuristic. A repair
operator was designed to make the solutions feasible with regards to the system constraints
(storage volume, IOPs, etc). StorNIR performed better than both NSGAII meta-heuristic
and the exact method in terms of quality of solutions and scalability. The repair
function improves the NSGAII meta-heuristic up to 7 times with 7.4% more extra time
execution. On average, StorNIR enhances by 17 times the quality of the initial solutions
calculated by CPLEX in terms of Hypervolume. In addition, the designed matheuristic
approach can be generalized to other meta-heuristics than NSGAII such as MOPSO meta-heuristic.},
  doi       = {10.1145/3412841.3441886},
  isbn      = {9781450381048},
  location  = {Virtual Event, Republic of Korea},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3412841.3441886},
}

@InProceedings{Chaudhary2021,
  author    = {Chaudhary, Akash and Belani, Manshul and Maheshwari, Naman and Parnami, Aman},
  booktitle = {Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction},
  title     = {Verbose : Designing a Context-Based Educational System for Improving Communicative Expressions},
  year      = {2021},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {MobileHCI '21},
  abstract  = {ESL (English as a second language) speakers tend to follow the tone structure of
their first language, making their speech difficult to understand for native speakers,
thereby limiting their opportunities for education and employment. To address this
problem, we build an interactive smartphone-based educational mobile application using
the user-centered design process. This application teaches English intonations based
on globally consistent pitch patterns through conversations with a trained chat assistant,
which inculcates expert linguists’ teaching principles. After co-designing the application’s
parameters with primary stakeholders and expert visual designers, we assess its effectiveness
by measuring the pre and post-performance of the users after the system usage, using
various quantitative measures, like intonation scores, SEQ, and SUS. Feedback from
users suggests that ESL speakers find significant improvement in the perception of
their vocal expressions, thereby highlighting the necessity of such a system in improving
the quality of conversations that people have in general.},
  articleno = {41},
  doi       = {10.1145/3447526.3472057},
  isbn      = {9781450383288},
  keywords  = {Stress-timed language, Learning application, Context-based learning, Intonations, Communicative expressions},
  location  = {Toulouse &amp; Virtual, France},
  numpages  = {13},
  url       = {https://doi.org/10.1145/3447526.3472057},
}

@InProceedings{Saha2017,
  author    = {Saha, Debanshee and Shinde, Manasi and Thadeshwar, Shail},
  booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
  title     = {IoT Based Air Quality Monitoring System Using Wireless Sensors Deployed in Public Bus Services},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICC '17},
  abstract  = {The ambient air quality monitoring network involves the measurement of a number of
air pollutants at various locations in the city so as to maintain a sustainable air
quality. It is the need of hour to monitor air quality in order to reduce air pollution.
Exposure to air pollution can lead to respiratory and cardiovascular diseases, which
is estimated to be the cause for 620,000 early deaths in 2010, and the impact on health
due to air pollution in India has been calculated at 3 percent of its GDP. In recent
years, air pollution has acquired critical dimensions and the air quality in most
cities that monitor outdoor air pollution fail to meet WHO guidelines for safe levels.
Air pollution is a major environmental change that causes many hazardous effects on
human beings which need to be controlled. With the advancements in technology, several
innovations have been made in the field of communications that are transitioning to
the Internet of Things (IoT). In this domain, Wireless Sensor Networks (WSN) are one
of those independent sensing devices to monitor physical and environmental conditions
along with thousands of applications in other fields. In this paper, we are proposing
the deployment of WSN sensor nodes in public transport buses for the constant monitoring
of air pollution. The data regarding the air pollution particles such as emissions,
smoke, and other pollutants will be collected via sensors on the public transport
bus and the data will be aggregated and transmitted to the nearest sink node. Using
the concept of the Internet of Things (IoT) the collected data will be uploaded on
the cloud server also called as the IoT cloud where a large amount of the data is
stored. This data can then be accessed at any point to analyze and accurate measures
can be taken to map the air pollution.},
  articleno = {87},
  doi       = {10.1145/3018896.3025135},
  isbn      = {9781450347747},
  keywords  = {smart city, air pollution, wireless sensor networks, internet of things (IoT)},
  location  = {Cambridge, United Kingdom},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3018896.3025135},
}

@InProceedings{Zertal2017,
  author    = {Zertal, Soumia and Batouche, Mohamed Chawki},
  booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
  title     = {A Hybrid Approach for Optimized Composition of Cloud Services},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {BDCA'17},
  abstract  = {The increasing use of Cloud services as well as the increasing demands of complex
cloud services creates the need for a dynamic and adaptive composition of services,
in a decentralized and large scale environment, where the quality of services may
increase or decrease. Early attempts for dynamic composition of services have been
proposed. But they are limited by their ability to adapt when deploying in highly
dynamic and open environments. For better performance measurements, we use, in this
paper, the Particle Swarm Optimization (PSO) algorithm to find and provide the services
that meets the user's query. To assess the utility of each service, we take into consideration
its values of service quality provided in the past. The latter is represented by the
mechanism of stigmergy which uses the pheromone as a means of communication between
services.},
  articleno = {12},
  doi       = {10.1145/3090354.3090366},
  isbn      = {9781450348522},
  keywords  = {Stigmegy, Optimization, Particle Swarm Optimization, Service Composition, Cloud Computing},
  location  = {Tetouan, Morocco},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3090354.3090366},
}

@InProceedings{Tasiopoulos2017,
  author    = {Tasiopoulos, Argyrios G. and Atarashi, Ray and Psaras, Ioannis and Pavlou, George},
  booktitle = {Proceedings of the Workshop on QoE-Based Analysis and Management of Data Communication Networks},
  title     = {On the Bitrate Adaptation of Shared Media Experience Services},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {25–30},
  publisher = {Association for Computing Machinery},
  series    = {Internet QoE '17},
  abstract  = {In Shared Media Experience Services (SMESs), a group of people is interested in streaming
consumption in a synchronised way, like in the case of cloud gaming, live streaming,
and interactive social applications. However, group synchronisation comes at the expense
of other Quality of Experience (QoE) factors due to both the dynamic and diverse network
conditions that each group member experiences. Someone might wonder if there is a
way to keep a group synchronised while maintaining the highest possible QoE for each
one of its members. In this work, at first we create a Quality Assessment Framework
capable of evaluating different SMESs improvement approaches with respect to traditional
metrics like media bitrate quality, playback disruption, and end user desynchronisation.
Secondly, we focus on the bitrate adaptation for improving the QoE of SMESs, as an
incrementally deployable end user triggered approach, and we formulate the problem
in the context of Adaptive Real Time Dynamic Programming (ARTDP). Finally, we develop
and apply a simple QoE aware bitrate adaptation mechanism that we compare against
youtube live-streaming traces to find that it improves the youtube performance by
more than 30%.},
  doi       = {10.1145/3098603.3098608},
  isbn      = {9781450350563},
  keywords  = {Bitrate Adaptation, QoE Assessment Framework, Shared Media Experience Services (SMESs)},
  location  = {Los Angeles, CA, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3098603.3098608},
}

@Article{Zeng2020,
  author     = {Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv},
  journal    = {ACM Comput. Surv.},
  title      = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study},
  year       = {2020},
  issn       = {0360-0300},
  month      = jun,
  number     = {3},
  volume     = {53},
  abstract   = {Recent years have witnessed the booming of big data analytical applications (BDAAs).
This trend provides unrivaled opportunities to reveal the latent patterns and correlations
embedded in the data, and thus productive decisions may be made. This was previously
a grand challenge due to the notoriously high dimensionality and scale of big data,
whereas the quality of service offered by providers is the first priority. As BDAAs
are routinely deployed on Clouds with great complexities and uncertainties, it is
a critical task to manage the service level agreements (SLAs) so that a high quality
of service can then be guaranteed. This study performs a systematic literature review
of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review
surveys the challenges and contemporary approaches along this direction centering
on SLA. A research taxonomy is proposed to formulate the results of the systematic
literature review. A new conceptual SLA model is defined and a multi-dimensional categorization
scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding
of managing SLAs and the motivation of trends for future research.},
  address    = {New York, NY, USA},
  articleno  = {46},
  doi        = {10.1145/3383464},
  issue_date = {June 2020},
  keywords   = {SLA metrics, Big data, big data analytics application, service level agreement, SLA, service layer},
  numpages   = {40},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3383464},
}

@InProceedings{Symeonidis2020,
  author    = {Symeonidis, Panagiotis and Mitropoulos, Pantelis and Taskaris, Simeon and Vakkas, Theodoros and Adamopoulou, Eleni and Karakirios, Dimitrios and Salamalikis, Vasileios and Kosmopoulos, Georgios and Kazantzidis, Andreas},
  booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
  title     = {ThermiAir: An Innovative Air Quality Monitoring System for Airborne Particulate Matter in Thermi, Greece},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {775–778},
  publisher = {Association for Computing Machinery},
  series    = {ICEGOV 2020},
  abstract  = {This paper presents the development of an innovative air quality monitoring platform
for the Municipality of Thermi in Thessaloniki. The monitoring network consists of
25 low cost but very accurate IoT sensors measuring the concentration of Particulate
Matter (PM 10, PM 2.5, PM 1.0). Using these new generation of sensors, it is feasible
to monitor air quality at city block level, revealing the spatial pattern of air pollution,
and thus allowing local and regional agencies to design and apply the most suitable
policies and measures to tackle the air pollution problem. The real time measurements
are stored in the Cloud and are disseminated to the citizens and the local authorities'
stakeholders through a web and a mobile app. The web application provides an air quality
dashboard which presents the overall air quality in the Municipality. Both the Air
Quality Index (AQI) and raw concentration data are used. Various types of presentations
are available including maps and charts. The web application provides also a three-day
air quality forecast using the Copernicus forecast data. The mobile app provides easy
access to the real time data in a simple to understand way, suitable for the public
users.},
  doi       = {10.1145/3428502.3428618},
  isbn      = {9781450376747},
  keywords  = {data analytics, Air pollution, geographic information systems, Air quality, IoT sensors},
  location  = {Athens, Greece},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3428502.3428618},
}

@InProceedings{Taraghi2020,
  author    = {Taraghi, Babak and Zabrovskiy, Anatoliy and Timmerer, Christian and Hellwagner, Hermann},
  booktitle = {Proceedings of the 11th ACM Multimedia Systems Conference},
  title     = {CAdViSE: Cloud-Based Adaptive Video Streaming Evaluation Framework for the Automated Testing of Media Players},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {349–352},
  publisher = {Association for Computing Machinery},
  series    = {MMSys '20},
  abstract  = {Attempting to cope with fluctuations of network conditions in terms of available bandwidth,
latency and packet loss, and to deliver the highest quality of video (and audio) content
to users, research on adaptive video streaming has attracted intense efforts from
the research community and huge investments from technology giants. How successful
these efforts and investments are, is a question that needs precise measurements of
the results of those technological advancements. HTTP-based Adaptive Streaming (HAS)
algorithms, which seek to improve video streaming over the Internet, introduce video
bitrate adaptivity in a way that is scalable and efficient. However, how each HAS
implementation takes into account the wide spectrum of variables and configuration
options, brings a high complexity to the task of measuring the results and visualizing
the statistics of the performance and quality of experience. In this paper, we introduce
CAdViSE, our Cloud-based Adaptive Video Streaming Evaluation framework for the automated
testing of adaptive media players. The paper aims to demonstrate a test environment
which can be instantiated in a cloud infrastructure, examines multiple media players
with different network attributes at defined points of the experiment time, and finally
concludes the evaluation with visualized statistics and insights into the results.},
  doi       = {10.1145/3339825.3393581},
  isbn      = {9781450368452},
  keywords  = {HTTP adaptive streaming, MPEG-DASH, automated testing, network emulation, quality of experience, media players},
  location  = {Istanbul, Turkey},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3339825.3393581},
}

@InProceedings{Tomei2019,
  author    = {Tomei, Matthew and Schwing, Alexander and Narayanasamy, Satish and Kumar, Rakesh},
  booktitle = {Proceedings of the 2019 Workshop on Hot Topics in Video Analytics and Intelligent Edges},
  title     = {Sensor Training Data Reduction for Autonomous Vehicles},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {45–50},
  publisher = {Association for Computing Machinery},
  series    = {HotEdgeVideo'19},
  abstract  = {Ensuring safety and reliability of autonomous vehicles requires good learning models
which, in turn, require a large amount of real-world training data. Data produced
by in-vehicle sensors (e.g., cameras, LIDARs, IMUs, etc.) can be used for training;
however, both local storage and transmission of this sensor data to the cloud for
subsequent use in training can be prohibitively expensive due to the staggering volume
of data produced by these sensors, especially the cameras. In this paper, we perform
the first exploration of techniques for reducing video frames in a way that the quality
of training for autonomous vehicles is minimally affected. We particularly focus on
utility aware data reduction schemes where the potential contribution of a video frame
to enhancing the quality of learning (or utility) is explicitly considered during
data reduction. Since actual utility of a video frame cannot be computed online, we
use surrogate utility metrics to decide what video frames to keep for training and
which ones to discard. Our results show that utility-aware data reduction schemes
can reduce the amount of camera data required for training by as much as $16times$
compared to random sampling for the same quality of learning (in terms of IoU).},
  doi       = {10.1145/3349614.3356028},
  isbn      = {9781450369282},
  keywords  = {sensor, semantic segmentation, compression, machine learning, self driving car, autonomous vehicle, data reduction, active learning},
  location  = {Los Cabos, Mexico},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3349614.3356028},
}

@InProceedings{Shekhar2017,
  author    = {Shekhar, Shashank and Gokhale, Aniruddha},
  booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
  title     = {Dynamic Resource Management Across Cloud-Edge Resources for Performance-Sensitive Applications},
  year      = {2017},
  pages     = {707–710},
  publisher = {IEEE Press},
  series    = {CCGrid '17},
  abstract  = {A large number of modern applications and systems are cloud-hosted, however, limitations
in performance assurances from the cloud, and the longer and often unpredictable end-to-end
network latencies between the end user and the cloud can be detrimental to the response
time requirements of the applications, specifically those that have stringent Quality
of Service (QoS) requirements. Although edge resources, such as cloudlets, may alleviate
some of the latency concerns, there is a general lack of mechanisms that can dynamically
manage resources across the cloud-edge spectrum. To address these gaps, this research
proposes Dynamic Data Driven Cloud and Edge Systems (D3CES). It uses measurement data
collected from adaptively instrumenting the cloud and edge resources to learn and
enhance models of the distributed resource pool. In turn, the framework uses the learned
models in a feedback loop to make effective resource management decisions to host
applications and deliver their QoS properties. D3CES is being evaluated in the context
of a variety of cyber physical systems, such as smart city, online games, and augmented
reality applications.},
  doi       = {10.1109/CCGRID.2017.120},
  isbn      = {9781509066100},
  keywords  = {CPS, Edge Computing, IoT, DDDAS, Cloud Computing, Fog Computing, Resource Management},
  location  = {Madrid, Spain},
  numpages  = {4},
  url       = {https://doi.org/10.1109/CCGRID.2017.120},
}

@InProceedings{Lottarini2018,
  author    = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A. and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark},
  booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
  title     = {Vbench: Benchmarking Video Transcoding in the Cloud},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {797–809},
  publisher = {Association for Computing Machinery},
  series    = {ASPLOS '18},
  abstract  = {This paper presents vbench, a publicly available benchmark for cloud video services.
We are the first study, to the best of our knowledge, to characterize the emerging
video-as-a-service workload. Unlike prior video processing benchmarks, vbench's videos
are algorithmically selected to represent a large commercial corpus of millions of
videos. Reflecting the complex infrastructure that processes and hosts these videos,
vbench includes carefully constructed metrics and baselines. The combination of validated
corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and
compression. We demonstrate the importance of video selection with a microarchitectural
study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial
corpus that are not visible in other video corpuses. Our experiments with GPUs under
vbench's scoring scenarios reveal that context is critical: GPUs are well suited for
live-streaming, while for video-on-demand shift costs from compute to storage and
network. Counterintuitively, they are not viable for popular videos, for which highly
compressed, high quality copies are required. We instead find that popular videos
are currently well-served by the current trajectory of software encoders.},
  doi       = {10.1145/3173162.3173207},
  isbn      = {9781450349116},
  keywords  = {video transcoding, accelerator, benchmark},
  location  = {Williamsburg, VA, USA},
  numpages  = {13},
  url       = {https://doi.org/10.1145/3173162.3173207},
}

@Article{Lottarini2018a,
  author     = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A. and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark},
  journal    = {SIGPLAN Not.},
  title      = {Vbench: Benchmarking Video Transcoding in the Cloud},
  year       = {2018},
  issn       = {0362-1340},
  month      = mar,
  number     = {2},
  pages      = {797–809},
  volume     = {53},
  abstract   = {This paper presents vbench, a publicly available benchmark for cloud video services.
We are the first study, to the best of our knowledge, to characterize the emerging
video-as-a-service workload. Unlike prior video processing benchmarks, vbench's videos
are algorithmically selected to represent a large commercial corpus of millions of
videos. Reflecting the complex infrastructure that processes and hosts these videos,
vbench includes carefully constructed metrics and baselines. The combination of validated
corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and
compression. We demonstrate the importance of video selection with a microarchitectural
study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial
corpus that are not visible in other video corpuses. Our experiments with GPUs under
vbench's scoring scenarios reveal that context is critical: GPUs are well suited for
live-streaming, while for video-on-demand shift costs from compute to storage and
network. Counterintuitively, they are not viable for popular videos, for which highly
compressed, high quality copies are required. We instead find that popular videos
are currently well-served by the current trajectory of software encoders.},
  address    = {New York, NY, USA},
  doi        = {10.1145/3296957.3173207},
  issue_date = {February 2018},
  keywords   = {video transcoding, accelerator, benchmark},
  numpages   = {13},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3296957.3173207},
}

@InProceedings{Imai2018,
  author    = {Imai, Shigeru and Patterson, Stacy and Varela, Carlos A.},
  booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
  title     = {Uncertainty-Aware Elastic Virtual Machine Scheduling for Stream Processing Systems},
  year      = {2018},
  pages     = {62–71},
  publisher = {IEEE Press},
  series    = {CCGrid '18},
  abstract  = {Stream processing systems deployed on the cloud need to be elastic to effectively
accommodate workload variations over time. Performance models can predict maximum
sustainable throughput (MST) as a function of the number of VMs allocated. We present
a scheduling framework that incorporates three statistical techniques to improve Quality
of Service (QoS) of cloud stream processing systems: (i) uncertainty quantification
to consider variance in the MST model; (ii) online learning to update MST model as
new performance metrics are gathered; and (iii) workload models to predict input data
stream rates assuming regular patterns occur over time. Our framework can be parameterized
by a QoS satisfaction target that statistically finds the best performance/cost tradeoff.
Our results illustrate that each of the three techniques alone significantly improves
QoS, from 52% to 73-81% QoS satisfaction rates on average for eight benchmark applications.
Furthermore, applying all three techniques allows us to reach 98.62% QoS satisfaction
rate with a cost less than twice the cost of the optimal (in hindsight) VM allocations,
and half of the cost of allocating VMs for the peak demand in the workload.},
  doi       = {10.1109/CCGRID.2018.00021},
  isbn      = {9781538658154},
  location  = {Washington, District of Columbia},
  numpages  = {10},
  url       = {https://doi.org/10.1109/CCGRID.2018.00021},
}

@InProceedings{Kuhlenkamp2019,
  author    = {Kuhlenkamp, J\"{o}rn and Werner, Sebastian and Borges, Maria C. and El Tal, Karim and Tai, Stefan},
  booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
  title     = {An Evaluation of FaaS Platforms as a Foundation for Serverless Big Data Processing},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {1–9},
  publisher = {Association for Computing Machinery},
  series    = {UCC'19},
  abstract  = {Function-as-a-Service (FaaS), offers a new alternative to operate cloud-based applications.
FaaS platforms enable developers to define their application only through a set of
service functions, relieving them of infrastructure management tasks, which are executed
automatically by the platform. Since its introduction, FaaS has grown to support workloads
beyond the lightweight use-cases it was originally intended for, and now serves as
a viable paradigm for big data processing. However, several questions regarding FaaS
platform quality are still unanswered. Specifically, the impact of automatic infrastructure
management on serverless big data applications remains unexplored.In this paper, we
propose a novel evaluation method (SIEM) to understand the impact of these tasks.
For this purpose, we introduce new metrics to quantify quality in different big data
application scenarios. We show an application of SIEM by evaluating the four major
FaaS providers, and contribute results and new insights for FaaS-based big data processing.},
  doi       = {10.1145/3344341.3368796},
  isbn      = {9781450368940},
  keywords  = {serverless, benchmarking, big data processing, cloud computing},
  location  = {Auckland, New Zealand},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3344341.3368796},
}

@InProceedings{Ughetta2020,
  author    = {Ughetta, William and Kernighan, Brian W.},
  booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
  title     = {The Old Bailey and OCR: Benchmarking AWS, Azure, and GCP with 180,000 Page Images},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {DocEng '20},
  abstract  = {The Proceedings of the Old Bailey is a corpus of over 180,000 page images of court
records printed from April 1674 to April 1913 and presents a comprehensive challenge
for Optical Character Recognition (OCR) services. The Old Bailey is an ideal benchmark
for historical document OCR, representing more than two centuries of variations in
documents, including spellings, formats, and printing and preservation qualities.
In addition to its historical and sociological significance, the Old Bailey is filled
with imperfections that reflect the reality of coping with large-scale historical
data. Most importantly, the Old Bailey contains human transcriptions for each page,
which can be used to help measure OCR accuracy. Since humans do make mistakes in transcriptions,
the relative performance of OCR services will be more informative than their absolute
performance. This paper compares three leading commercial OCR cloud services: Amazon
Web Services's Textract (AWS); Microsoft Azure's Cognitive Services (Azure); and Google
Cloud Platform's Vision (GCP). Benchmarking involved downloading over 180,000 images,
executing the OCR, and measuring the error rate of the OCR text against the human
transcriptions. Our results found that AWS had the lowest median error rate, Azure
had the lowest median round trip time, and GCP had the best combination of a low error
rate and a low duration.},
  articleno = {19},
  doi       = {10.1145/3395027.3419595},
  isbn      = {9781450380003},
  keywords  = {Old Bailey, Historical Documents, Amazon Web Services, Optical Character Recognition, Microsoft Azure, Google Cloud Platform},
  location  = {Virtual Event, CA, USA},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3395027.3419595},
}

@InProceedings{Byholm2014,
  author    = {Byholm, Benjamin and Porres, Iv\'{a}n},
  booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Cost-Efficient, Reliable, Utility-Based Session Management in the Cloud},
  year      = {2014},
  pages     = {102–111},
  publisher = {IEEE Press},
  series    = {CCGRID '14},
  abstract  = {We present a model and system for cost-efficient and reliable management of sessions
in a Cloud, based on the von Neumann-Morgenstern utility theorem. Our model enables
a web application provider to maximize profit while maintaining a desired quality
of service. The objective is to determine whether, when, where, and how long to store
a session, given multiple storage options with various properties, e.g. cost, capacity,
and reliability. Reliability is affected by three factors: how often session state
is stored, how many stores are used, and how reliable those stores are. To account
for these factors, we use a Markovian reliability model and treat the valid storage
options for each session as a von Neumann-Morgenstern lottery. We proceed by representing
the resulting problem as a knapsack problem, which can be heuristically solved for
a good compromise between efficiency and effectiveness. We analyze the results from
a discrete-event simulation involving multiple session management policies, including
two utility-based policies: a greedy heuristic policy intended to give real-time performance
and a reference policy based on solving the linear programming relaxation of the knapsack
problem, giving a theoretical upper bound on achievable utility. As the focus of this
work is exploratory, rather than performance-based, we do not directly measure the
time required for solving the model. Instead, we give the computational complexity
of the algorithms. Our results indicate that otherwise unprofitable services become
profitable through utility-based session management in a cloud setting. However, if
the costs are much lower than the expected revenues, all policies manage to turn a
profit. Different policies performed the best under different circumstances.},
  doi       = {10.1109/CCGrid.2014.22},
  isbn      = {9781479927838},
  keywords  = {analytical models, and serviceability, availability, utility theory, simulation, web-based services, distributed systems, markov processes, reliability},
  location  = {Chicago, Illinois},
  numpages  = {10},
  url       = {https://doi.org/10.1109/CCGrid.2014.22},
}

@InProceedings{Berger2020,
  author    = {Berger, Arthur},
  booktitle = {Proceedings of the 38th ACM International Conference on Design of Communication},
  title     = {Designing an Analytics Approach for Technical Content},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SIGDOC '20},
  abstract  = {Working on an enterprise cloud product, my documentation team rethought our approach
to content analytics. Despite a variety of tools and awareness of industry best practices,
my team felt stuck using analytics only in annual or on-demand reports to management,
instead of to produce value for our end users. We employed Design Thinking practices
to guide a multifaceted user research project that led to changes in the way that
we created documentation and automated quality content checks. Key takeaways include
to involve the technical documentation team in identifying not only what metrics to
collect, but also how to collect, report, and use the metrics in order to increase
buy-in and the likelihood that data analytics about content leads to meaningful change
within the content itself.},
  articleno = {7},
  doi       = {10.1145/3380851.3416742},
  isbn      = {9781450375252},
  keywords  = {Data analytics, design thinking,, content strategy},
  location  = {Denton, TX, USA},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3380851.3416742},
}

@InProceedings{Branco2020,
  author    = {Branco, Te\'{o}filo T. and Kawashita, Ilka M. and de S\'{a}-Soares, Filipe and Monteiro, Cl\'{a}udio N.},
  booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
  title     = {An IoT Application Case Study to Optimize Electricity Consumption in the Government Sector},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {70–81},
  publisher = {Association for Computing Machinery},
  series    = {ICEGOV 2020},
  abstract  = {This paper presents a case study where sensor modules supported by Internet of Things
(IoT) technology were used to monitor and control electricity consumption of air conditioning
units in an innovation center of a public government institution. This study evaluates
alternatives to improve the management of electricity consumption in Salvador City
Hall's facilities. To contribute to the economy and sustainability of the Administration,
we aim to increase the efficiency of the processes currently adopted. Our focus is
on minimizing electricity waste and reducing costs. Installed sensor modules measure
electricity consumption and control the operation of air conditioning equipment, allowing
the administrator to manage the operation of these devices. The installation of smart
sensor modules connected to an IoT platform allows energy consumption data to be sent
to a computing Cloud and to be monitored remotely through dashboards generated by
specialized software. A quantitative analysis was conducted to measure the efficiency
of the air conditioning control system and identify opportunities for applying the
IoT solution to control natural resources in the public sector. The monitoring of
these signals subsidized the analyzes required for informed decision making of interventions
to improve the system's stability and promote the reduction of consumption. Also,
the system has demonstrated its ability to protect air conditioners, monitor the quality
of the power supplied, proactively control consumption, and establish appropriate
user behaviors for reducing consumption. Results demonstrated the feasibility of implementing
automated systems to improve the consumption of natural resources in the public sector.
We also identified some managerial behaviors required to enable this type of technological
solution.},
  doi       = {10.1145/3428502.3428511},
  isbn      = {9781450376747},
  keywords  = {Internet of Thinks (IoT), Sustainability, Innovation, Smart Technologies, E-government},
  location  = {Athens, Greece},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3428502.3428511},
}

@InProceedings{Cano2016,
  author    = {Cano, Ignacio and Aiyar, Srinivas and Krishnamurthy, Arvind},
  booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
  title     = {Characterizing Private Clouds: A Large-Scale Empirical Analysis of Enterprise Clusters},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {29–41},
  publisher = {Association for Computing Machinery},
  series    = {SoCC '16},
  abstract  = {There is an increasing trend in the use of on-premise clusters within companies. Security,
regulatory constraints, and enhanced service quality push organizations to work in
these so called private cloud environments. On the other hand, the deployment of private
enterprise clusters requires careful consideration of what will be necessary or may
happen in the future, both in terms of compute demands and failures, as they lack
the public cloud's flexibility to immediately provision new nodes in case of demand
spikes or node failures.In order to better understand the challenges and tradeoffs
of operating in private settings, we perform, to the best of our knowledge, the first
extensive characterization of on-premise clusters. Specifically, we analyze data ranging
from hardware failures to typical compute/storage requirements and workload profiles,
from a large number of Nutanix clusters deployed at various companies.We show that
private cloud hardware failure rates are lower, and that load/demand needs are more
predictable than in other settings. Finally, we demonstrate the value of the measurements
by using them to provide an analytical model for computing durability in private clouds,
as well as a machine learning-driven approach for characterizing private clouds' growth.},
  doi       = {10.1145/2987550.2987584},
  isbn      = {9781450345255},
  keywords  = {Private clouds, Measurements, Reliability, Performance},
  location  = {Santa Clara, CA, USA},
  numpages  = {13},
  url       = {https://doi.org/10.1145/2987550.2987584},
}

@InProceedings{Marculescu2021,
  author    = {Marculescu, Diana},
  booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
  title     = {When Climate Meets Machine Learning: Edge to Cloud ML Energy Efficiency},
  year      = {2021},
  publisher = {IEEE Press},
  series    = {ISLPED '21},
  abstract  = {A large portion of current cloud and edge workloads feature Machine Learning (ML)
tasks, thereby requiring a deep understanding of their energy efficiency. While the
holy grail for judging the quality of a ML model has largely been testing accuracy,
and only recently its resource usage, neither of these metrics translate directly
to energy efficiency, runtime, or mobile device battery lifetime. This work uncovers
the need for building accurate, platform-specific power and latency models for ML
and efficient hardware-aware ML design methodologies, thus allowing machine learners
and hardware designers to identify not just the best accuracy ML model configuration,
but also those that satisfy given hardware constraints.},
  articleno = {37},
  doi       = {10.1109/ISLPED52811.2021.9502472},
  keywords  = {hardware-aware ML, quantization, model compression, neural architecture search},
  location  = {Boston, Massachusetts},
  numpages  = {1},
  url       = {https://doi.org/10.1109/ISLPED52811.2021.9502472},
}

@InProceedings{Rossi2016,
  author    = {Rossi, Chuck and Shibley, Elisa and Su, Shi and Beck, Kent and Savor, Tony and Stumm, Michael},
  booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  title     = {Continuous Deployment of Mobile Software at Facebook (Showcase)},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {12–23},
  publisher = {Association for Computing Machinery},
  series    = {FSE 2016},
  abstract  = {Continuous deployment is the practice of releasing software updates to production
as soon as it is ready, which is receiving increased adoption in industry. The frequency
of updates of mobile software has traditionally lagged the state of practice for cloud-based
services for a number of reasons. Mobile versions can only be released periodically.
Users can choose when and if to upgrade, which means that several different releases
coexist in production. There are hundreds of Android hardware variants, which increases
the risk of having errors in the software being deployed.  Facebook has made significant
progress in increasing the frequency of its mobile deployments. Over a period of 4
years, the Android release has gone from a deployment every 8 weeks to a deployment
every week. In this paper, we describe in detail the mobile deployment process at
FB. We present our findings from an extensive analysis of software engineering metrics
based on data collected over a period of 7 years. A key finding is that the frequency
of deployment does not directly affect developer productivity or software quality.
We argue that this finding is due to the fact that increasing the frequency of continuous
deployment forces improved release and deployment automation, which in turn reduces
developer workload. Additionally, the data we present shows that dog-fooding and obtaining
feedback from alpha and beta customers is critical to maintaining release quality.},
  doi       = {10.1145/2950290.2994157},
  isbn      = {9781450342186},
  keywords  = {Software release, Mobile code testing, Continuous deployment, Continuous delivery, Agile development},
  location  = {Seattle, WA, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2950290.2994157},
}

@InProceedings{Pang2018a,
  author    = {Pang, Haitian and Zhang, Cong and Wang, Fangxin and Hu, Han and Wang, Zhi and Liu, Jiangchuan and Sun, Lifeng},
  booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
  title     = {Optimizing Personalized Interaction Experience in Crowd-Interactive Livecast: A Cloud-Edge Approach},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {1217–1225},
  publisher = {Association for Computing Machinery},
  series    = {MM '18},
  abstract  = {Enabling users to interact with broadcasters and audience, the crowd-interactive livecast
greatly improves viewer's quality of experience (QoE) and attracts millions of daily
active users recently. In addition to striking the balance between resource utilization
and viewers' QoE met in the traditional video streaming service, this novel service
needs to take supererogatory efforts to improve the interaction QoE, which reflects
the viewer interaction experience. To tackle this issue, we conduct measurement studies
over a large-scale dataset crawled from a representative livecast service provider.
We observe that the individual's interaction pattern is quite heterogeneous: only
10% viewers proactively participate in the interaction, and the rest viewers usually
watch passively. Incorporating the insight into the emerging cloud-edge architecture,
we propose a framework PIECE, which optimizes the Personalized Interaction Experience
with Cloud-Edge architecture (PIECE) for intelligent user access control and livecast
distribution. In particular, we first devise a novel deep neural network based algorithm
to predict users' interaction intensity using the historical viewer pattern. We then
design an algorithm to maximize the individual's QoE, by strategically matching viewer
sessions and transcoding-delivery paths over cloud-edge infrastructure. Finally, we
use trace-driven experiments to verify the effectiveness of PIECE. Our results show
that our prediction algorithm outperforms the state-of-the-art algorithms with a much
smaller mean absolute error (40% reduction). Furthermore, in comparison with the cloud-based
video delivery strategy, the proposed framework can simultaneously improve the average
viewers QoE (26% improvement) and interaction QoE (21% improvement), while maintaining
a high streaming bitrate.},
  doi       = {10.1145/3240508.3240642},
  isbn      = {9781450356657},
  keywords  = {cloud-edge, interactive live streaming, viewer interaction},
  location  = {Seoul, Republic of Korea},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3240508.3240642},
}

@InProceedings{Sethuraman2021,
  author    = {Sethuraman, Manasvini and Sarma, Anirudh and Dhekne, Ashutosh and Ramachandran, Umakishore},
  booktitle = {Proceedings of the 12th ACM Multimedia Systems Conference},
  title     = {Foresight: Planning for Spatial and Temporal Variations in Bandwidth for Streaming Services on Mobile Devices},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {227–240},
  publisher = {Association for Computing Machinery},
  series    = {MMSys '21},
  abstract  = {Spatiotemporal variation in cellular bandwidth availability is well-known and could
affect a mobile user's quality of experience (QoE), especially while using bandwidth
intensive streaming applications such as movies, podcasts, and music videos during
commute. If such variations are made available to a streaming service in advance it
could perhaps plan better to avoid sub-optimal performance while the user travels
through regions of low bandwidth availability. The intuition is that such future knowledge
could be used to buffer additional content in regions of higher bandwidth availability
to tide over the deficits in regions of low bandwidth availability. Foresight is a
service designed to provide this future knowledge for client apps running on a mobile
device. It comprises three components: (a) a crowd-sourced bandwidth estimate reporting
facility, (b) an on-cloud bandwidth service that records the spatiotemporal variations
in bandwidth and serves queries for bandwidth availability from mobile users, and
(c) an on-device bandwidth manager that caters to the bandwidth requirements from
client apps by providing them with bandwidth allocation schedules. Foresight is implemented
in the Android framework. As a proof of concept for using this service, we have modified
an open-source video player---Exoplayer---to use the results of Foresight in its video
buffer management. Our performance evaluation shows Foresight's scalability. We also
showcase the opportunity that Foresight offers to ExoPlayer to enhance video quality
of experience (QoE) despite spatiotemporal bandwidth variations for metrics such as
overall higher bitrate of playback, reduction in number of bitrate switches, and reduction
in the number of stalls during video playback.},
  doi       = {10.1145/3458305.3463384},
  isbn      = {9781450384346},
  keywords  = {bandwidth management, spatiotemporal bandwidth information},
  location  = {Istanbul, Turkey},
  numpages  = {14},
  url       = {https://doi.org/10.1145/3458305.3463384},
}

@InProceedings{Ganesan2015,
  author    = {Ganesan, Deepak},
  booktitle = {Proceedings of the 2015 Workshop on Wireless of the Students, by the Students, &amp; for the Students},
  title     = {Towards Ultra-Low Power Wearable Health Sensing with Sparse Sampling and Asymmetric Communication},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {34},
  publisher = {Association for Computing Machinery},
  series    = {S3 '15},
  abstract  = {Wearable sensors offer tremendous opportunities for accelerating biomedical discovery,
and improving population-scale health and wellness. There is a growing appetite for
health analytics -- we are no longer content with wearables that count steps and calories,
we want to measure physiology, behavior, activities, cognition, affect, and other
parameters with the expectation that such data will lead to deep insights that can
improve quality of life.But a chasm separates expectations and reality. How do we
extract such insights from sensor platforms with tiny energy budgets? How do we communicate
high-rate sensor data to the cloud for enabling deep analytics while operating within
these energy budgets? How do we deal with noise, confounders, and artifacts that make
insights hard to extract from signals collected in real-world settings?In this talk,
I will discuss a few strategies to tackle these problems. I will discuss how we can
design an low-power computational eyeglass that continually tracks eye and visual
context by leveraging sparsity, how we can transfer data at Megabits/second from wearables
while operating at tens of micro-watts of power, and how we can leverage these techniques
in the context of mobile health.},
  doi       = {10.1145/2801694.2801710},
  isbn      = {9781450337014},
  keywords  = {mobile health, backscatter communication, eye tracking},
  location  = {Paris, France},
  numpages  = {1},
  url       = {https://doi.org/10.1145/2801694.2801710},
}

@InProceedings{Anagnostou2019,
  author    = {Anagnostou, Anastasia and Taylor, Simon J. E. and Abubakar, Nura Tijjani and Kiss, Tamas and DesLauriers, James and Gesmier, Gregoire and Terstyanszky, Gabor and Kacsuk, Peter and Kovacs, Jozsef},
  booktitle = {Proceedings of the Winter Simulation Conference},
  title     = {Towards a Deadline-Based Simulation Experimentation Framework Using Micro-Services Auto-Scaling Approach},
  year      = {2019},
  pages     = {2749–2758},
  publisher = {IEEE Press},
  series    = {WSC '19},
  abstract  = {There is growing number of research efforts in developing auto-scaling algorithms
and tools for cloud resources. Traditional performance metrics such as CPU, memory
and bandwidth usage for scaling up or down resources are not sufficient for all applications.
For example, modeling and simulation experimentation is usually expected to yield
results within a specific timeframe. In order to achieve this often the quality of
experiments is compromised either by restricting the parameter space to be explored
or by limiting the number of replications required to give statistical confidence.
In this paper, we present early stages of a deadline-based simulation experimentation
framework using a micro-services auto-scaling approach. A case study of an agent-based
simulation of a population physical activity behavior is used to demonstrate our framework.},
  isbn      = {9781728132839},
  location  = {National Harbor, Maryland},
  numpages  = {10},
}

@InProceedings{Rahman2017,
  author    = {Rahman, Mahmudur and Hong, Hua-Jun and Rahman, Amatur and Tsai, Pei-Hsuan and Afrin, Afia and Uddin, Md Yusuf Sarwar and Venkatasubramanian, Nalini and Hsu, Cheng-Hsin},
  booktitle = {Proceedings of the 16th Workshop on Adaptive and Reflective Middleware},
  title     = {Adaptive Sensing Using Internet-of-Things with Constrained Communications},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ARM '17},
  abstract  = {In this paper, we design and implement an Internet-of-Things (IoT) based platform
for developing cities using environmental sensing as driving application with a set
of air quality sensors that periodically upload sensor data to the cloud. Ubiquitous
and free WiFi access is unavailable in most developing cities; IoT deployments must
leverage 3G cellular connections that are expensive and metered. In order to best
utilize the limited 3G data plan, we envision two adaptation strategies to drive sensing
and sensemaking. The first technique is an infrastructure-level adaptation approach
where we adjust sensing intervals of periodic sensors so that the data volume remains
bounded within the plan. The second approach is at the information-level where application-specific
analytics are deployed on board devices (or the edge) through container technologies
(Docker and Kubernetes); the use case focuses on multimedia sensors that process captured
raw information to lower volume semantic data that is communicated. This approach
is implemented through the EnviroSCALE (Environmental Sensing and Community Alert
Network) platform, an inexpensive Raspberry Pi based environmental sensing system
that periodically publishes sensor data over a 3G connection with a limited data plan.
We outline our deployment experience of EnviroSCALE in Dhaka city, the capital of
Bangladesh. For information-level adaptation, we enhanced EnviroSCALE with Docker
containers with rich media analytics, along Kubernetes for provisioning IoT devices
and deploying the Docker images. To limit data communication overhead, the Docker
images are preloaded in the board but a small footprint of analytic code is transferred
whenever required. Our experiment results demonstrate the practicality of adaptive
sensing and triggering rich sensing analytics via user-specified criteria, even over
constrained data connections.},
  articleno = {6},
  doi       = {10.1145/3152881.3152887},
  isbn      = {9781450351683},
  location  = {Las Vegas, Nevada},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3152881.3152887},
}

@InProceedings{Agossou2021,
  author    = {Agossou, B. Emmanuel and Toshiro, Takahara},
  booktitle = {Proceedings of the Conference on Information Technology for Social Good},
  title     = {IoT &amp; AI Based System for Fish Farming: Case Study of Benin},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {259–264},
  publisher = {Association for Computing Machinery},
  series    = {GoodIT '21},
  abstract  = {Agriculture including aquaculture has been changing through multiple technological
transformations in recent years. The Internet of Things (IoT) and Artificial Intelligence
(AI) are providing remarkable technological innovations on fish farming. In this research,
we present an automated IoT and AI-based system to improve fish farming. The proposed
system uses multiple sensors to measure in real-time water quality chemical parameters
such as: temperature, pH, turbidity, electrical conductivity, total dissolved solids,
etc., from the fish pond and send them on a cloud database to allow fish farmers to
access them in realtime with their devices (mobile phone, PC, tablets). The system
contains three web applications which fish farmers can use. The first web application
enables farmers with realtime visualizations of sensors data, issues alerts and remote
pumps controls. Fish farmers can use the second web application for fish disease detection
and to receive suggestions for diseases' care. This would help to classify two fish
diseases which are: Epizootic Ulcerative Syndrome(EUS), and Ichthyophthirus(Ich).
The third web application is a digital community platform for knowledge sharing, capacity
building, market opportunities and collaboration among fish farmers. Our system can
help reduce human efforts, reinforce capacity building, increase fish production and
market opportunities for fish farmers.},
  doi       = {10.1145/3462203.3475873},
  isbn      = {9781450384780},
  keywords  = {MQTT, IoT, ESP32, eFish Farm, Convolutional Neural Network, Arduino, AI, Smart Fish Farming},
  location  = {Roma, Italy},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3462203.3475873},
}

@Article{Hossfeld2017,
  author     = {Hossfeld, Tobias},
  journal    = {SIGCOMM Comput. Commun. Rev.},
  title      = {2016 International Teletraffic Congress (ITC 28) Report},
  year       = {2017},
  issn       = {0146-4833},
  month      = may,
  number     = {2},
  pages      = {30–35},
  volume     = {47},
  abstract   = {The 28th International Teletraffic Congress (ITC 28) was held on 12--16 September
2016 at the University of W"urzburg, Germany. The conference was technically cosponsored
by the IEEE Communications Society and the Information Technology Society within VDE,
and in cooperation with ACM SIGCOMM. ITC 28 provided a forum for leading researchers
from academia and industry to present and discuss the latest advances and developments
in design, modelling, measurement, and performance evaluation of communication systems,
networks, and services. The main theme of ITC 28, emph{Digital Connected World},
reflects the evolution of communications and networking, which is continually changing
the world we are living in. The technical program was composed of 37 contributed full
papers, 6 short demo papers and three keynote addresses. Three workshops dedicated
to timely topics were sponsored: Programmability for Cloud Networks and Applications,
Quality of Experience Centric Management, Quality Engineering for a Reliable Internet
of Services.See ITC 28 Homepage: url{https://itc28.org/}},
  address    = {New York, NY, USA},
  doi        = {10.1145/3089262.3089268},
  issue_date = {April 2017},
  keywords   = {Performance Analysis and Modeling, Virtualization, Measurements, Video Streaming, Caching, Traffic and Network Management, Softwarization, Wireless and Cellular, Information Centric Networks, Clouds and Data Center},
  numpages   = {6},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3089262.3089268},
}

@InProceedings{Santos2020,
  author    = {Santos, Guilherme and Paulino, Herv\'{e} and Vardasca, Tom\'{e}},
  booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
  title     = {QoE-Aware Auto-Scaling of Heterogeneous Containerized Services (and Its Application to Health Services)},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {242–249},
  publisher = {Association for Computing Machinery},
  series    = {SAC '20},
  abstract  = {Containerized service is currently a widely adopted solution to deploy services in
the cloud. However, many companies offer a very diverse set of Web accessible services
that are subjected to very distinctive workloads. Consequently, to correctly provision
the right amount of resources for each of these services is a challenge. In this paper
we propose the Autonomic ConTainerized Service Scaler (ACTS), an autonomic system
able to horizontally and vertically scale a set of heterogeneous containerized services
subjected to different workloads. The adaptation decisions depended on a set of high-level
Quality of Experience (QoE) metrics centered on the services' end-user. We have applied
ACTS to some of the digital services of the Shared Services of the Ministry of Health
(SPMS) public company. The experimental results show that our solution is able to
adequately adapt the configuration of each service, as a direct response to alterations
on its workload.},
  doi       = {10.1145/3341105.3373915},
  isbn      = {9781450368667},
  keywords  = {quality of experience, health care, auto-scaling, containers},
  location  = {Brno, Czech Republic},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3341105.3373915},
}

@Article{He2020,
  author     = {He, Xin and Liu, Qiong and Yang, You},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {Make Full Use of Priors: Cross-View Optimized Filter for Multi-View Depth Enhancement},
  year       = {2020},
  issn       = {1551-6857},
  month      = dec,
  number     = {4},
  volume     = {16},
  abstract   = {Multi-view video plus depth (MVD) is the promising and widely adopted data representation
for future 3D visual applications and interactive media. However, compression distortions
on depth videos impede the development of such applications, and filters are crucially
needed for the quality enhancement at the terminal side. Cross-view priors can intuitively
be involved in filter design, but these priors are also distorted in compression and
thus the contribution of them can hardly be considered in previous research. In this
article, we propose a cross-view optimized filter for depth map quality enhancement
by making full use of inner- and cross-view priors. We dedicate to evaluate the contributions
of distorted cross-view priors in filtering the current view of depth, and then both
inner- and cross-view priors can be involved in the filter design. Thus, distortions
of cross-view priors are not barriers again as before. For the purpose of that, mutual
information guided cross-view consistency is designed to evaluate the contributions
of cross-view priors from compression distortions of MVD. After that, under the framework
of global optimization, both inner- and cross-view priors are modeled and taken to
minimize the designed energy function where both data accuracy and spatial smoothness
are modeled. The experimental results show that the proposed model outperforms state-of-the-art
methods, where 3.289 dB and 0.0407 average gains on peak signal-to-noise ratio and
structural similarity metrics can be obtained, respectively. For the subjective evaluations,
object details and structure information are recovered in the compressed depth video.
We also verify our method via several practical applications, including virtual view
synthesis for smooth interaction and point cloud for 3D modeling for accuracy evaluation.
In these verifications, the ringing and malposition artifacts on object contours are
properly handled for interactive video, and discontinuous object surfaces are restored
for 3D modeling. All of these results suggest that compression distortions in MVD
can be properly filtered by the proposed model, which provides a promising solution
for future bandwidth constrained 3D and interactive visual applications.},
  address    = {New York, NY, USA},
  articleno  = {127},
  doi        = {10.1145/3408293},
  issue_date = {January 2021},
  keywords   = {global optimization, Multi-view video plus depth, view consistency},
  numpages   = {19},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3408293},
}

@InProceedings{Shanthasheela2018,
  author    = {Shanthasheela, A. and Shanmugavadivu, P.},
  booktitle = {Proceedings of the 2018 International Conference on Electronics and Electrical Engineering Technology},
  title     = {An Exploratory Analysis of Speckle Noise Removal Methods for Satellite Images},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {217–222},
  publisher = {Association for Computing Machinery},
  series    = {EEET '18},
  abstract  = {Satellite images captured in a variety of modalities serve as the primary source for
many applications. Satellite image processing extracts the image /spectral information
represented in the form of pixels, classifies those pixels based on the similarity
measures and further analyzes the inherent data, as per the requirements. The foremost
objective of satellite processing is to automatically categorize the pixels in an
image into the respective land cover class labels or themes. These pixels are classified
by its spectral information and it is determined by the relative reflectance in various
bands of wavelength. The accuracy and outcomes of any satellite image processing procedure,
irrespective of the application domain, directly depends on its quality. Satellite
images are invariably degraded by speckle noise. Hence, preprocessing the images for
speckle noise suppression and/or cloud removal is deemed an inevitable component in
satellite image processing. Researchers have proposed a spectrum of methods for speckle
noise/cloud removal. A detailed review on the significant research publications on
speckle noise removal are summarized in this article. The consolidation of methodology
merits and demerits of the select research articles are presented in this paper. This
review article on speckle noise removal is designed as a ready-reference for those
researchers working in satellite image processing.},
  doi       = {10.1145/3277453.3277484},
  isbn      = {9781450365413},
  keywords  = {Satellite images, SAR, RADAR, Review, Speckle Noise, Noise filters, Literature Survey},
  location  = {Tianjin, China},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3277453.3277484},
}

@InProceedings{Ho2019,
  author    = {Ho, P. C. W. and Fok, W. W. T. and Chan, C. K. K. and Yeung, H. H. Au and Ng, H. W. and Wong, S. L. and Ngai, S. Y. and Kwok, P. H. and Ho, Y. S. and Chan, K. H.},
  booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
  title     = {Flipping the Learning and Teaching of Reading Strategies and Comprehension through a Cloud-Based Interactive Big Data Reading Platform},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {185–191},
  publisher = {Association for Computing Machinery},
  series    = {ICEIT 2019},
  abstract  = {This study investigates the learning approach of the designed Flipped Reading Platform
(FRP) and its effects on primary school students' general Chinese reading and comprehension
capabilities. This study was undertaken as part of the Quality Education Fund project
in Hong Kong, titled "Flipped Reading: Enhancing the Learning and Teaching of Reading
Strategies and Comprehension in Chinese via an Interactive Cloud Platform."This paper
presents the design of the Interactive Cloud Platform FRP, which incorporates elements
of both reading strategies and learning activities, and investigates the changes in
students' reading performance, applied strategies, and active learning level with
the application of FRP. The results show the experimental students using the FRP in
the pilot scheme generally gained more in three stages of reading comprehension, and
that low-achieving students learned reading strategies better. Analysis of FRP log
activities shows students' active engagement in reading and perceived competence.
Different learning outcomes were also found within the experimental group, categorized
by BYOD and non-BYOD classes. Implications of the study show the effectiveness of
FRP, and the design demonstrates how the reading measures integrated the assessment
indicators of both international and local standards in the domain of Chinese Language
reading. Further research can be developed to examine individual online reading performance
and learning behaviour on FRP.},
  doi       = {10.1145/3318396.3318427},
  isbn      = {9781450362672},
  keywords  = {e-Learning, Big data, reading strategy, Chinese Language, Cloud Platform, Flipped reading},
  location  = {Cambridge, United Kingdom},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3318396.3318427},
}

@InProceedings{Palomares2014,
  author    = {Palomares, Daniel and Migault, Daniel and Hendrik, Hendrik and Laurent, Maryline and Pujolle, Guy},
  booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
  title     = {Elastic Virtual Private Cloud},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {127–131},
  publisher = {Association for Computing Machinery},
  series    = {Q2SWinet '14},
  abstract  = {Several Virtual Private Networks are based on IPsec. However, IPsec has not been designed
with elasticity in mind, which makes clusters of IPsec security gateways hard to manage
for providing high Service Level Agreement (SLA). Thus, these SG clusters need management
techniques to maintain their Quality of Service. For example, ISPs use VPNs to secure
millions of communications when offloading End-Users from Radio Access Networks towards
alternative access networks such as WLANs. Additionally, Virtual Private Cloud (VPC)
providers also handle thousands of VPN connections when remote EUs access private
clouds services. This paper describes how to provide Traffic Management (TM) and High
Availability (HA) for VPN infrastructures by sharing or transferring an IPsec session.
TM and HA have been implemented and evaluated over a 2-nodes cluster. We measured
their impact on a real time audio streaming simulating a phone conversation. We found
out that over a 2 minutes conversation, the impact on QoS measured with POLQA while
applying TM or HA, is less than 3%.},
  doi       = {10.1145/2642687.2642704},
  isbn      = {9781450330275},
  keywords  = {high availability, VPN management, IPSEC, virtual private cloud, IKEV2, QoS, POLQA, context transfer},
  location  = {Montreal, QC, Canada},
  numpages  = {5},
  url       = {https://doi.org/10.1145/2642687.2642704},
}

@Article{Mountantonakis2016,
  author     = {Mountantonakis, Michalis and Tzitzikas, Yannis},
  journal    = {Proc. VLDB Endow.},
  title      = {On Measuring the Lattice of Commonalities among Several Linked Datasets},
  year       = {2016},
  issn       = {2150-8097},
  month      = aug,
  number     = {12},
  pages      = {1101–1112},
  volume     = {9},
  abstract   = {A big number of datasets has been published according to the principles of Linked
Data and this number keeps increasing. Although the ultimate objective is linking
and integration, it is not currently evident how connected the current LOD cloud is.
Measurements (and indexes) that involve more than two datasets are not available although
they are important: (a) for obtaining complete information about one particular URI
(or set of URIs) with provenance (b) for aiding dataset discovery and selection, (c)
for assessing the connectivity between any set of datasets for quality checking and
for monitoring their evolution over time, (d) for constructing visualizations that
provide more informative overviews. Since it would be prohibitively expensive to perform
all these measurements in a na\"{\i}ve way, in this paper we introduce indexes (and their
construction algorithms) that can speedup such tasks. In brief, we introduce (i) a
namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and
transitive closure of the owl:sameAs relationships encountered in the datasets, (iii)
a semantics-aware element index (that exploits the aforementioned indexes), and finally
(iv) two lattice-based incremental algorithms for speeding up the computation of the
intersection of URIs of any set of datasets. We discuss the speedup obtained by the
introduced indexes and algorithms through comparative results and finally we report
measurements about connectivity of the LOD cloud that have never been carried out
so far.},
  doi        = {10.14778/2994509.2994527},
  issue_date = {August 2016},
  numpages   = {12},
  publisher  = {VLDB Endowment},
  url        = {https://doi.org/10.14778/2994509.2994527},
}

@InProceedings{Versluis2018,
  author    = {Versluis, Laurens and van Eyk, Erwin and Iosup, Alexandru},
  booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
  title     = {An Analysis of Workflow Formalisms for Workflows with Complex Non-Functional Requirements},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {107–112},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '18},
  abstract  = {Cloud and datacenter operators offer progressively more sophisticated service level
agreements to customers. The Quality-of-Service guarantees by these operators have
started to entail non-functional requirements customers have regarding their applications.
At the same time, expressing applications as workflows in datacenters is increasingly
more common. Currently, non-functional requirements (NFRs) can only be defined on
entire workflows and cannot be changed at runtime, possibly wasting valuable resources.
To move towards modifiable NFRs at the task level, there is a need for a formalism
capable of expressing this. Existing formalisms do not support this level of granularity
or are restricted to a subset of NFRs. In this work, we investigate the current support
for NFRs in existing formalisms. Using a library containing workflows with and without
NFRs, we inspect the capability of existing formalisms to express these requirements.
Additionally, we create and evaluate five metrics to qualitatively and quantitatively
compare each formalism. Our main findings are that although current formalisms do
not support arbitrary NFRs per-task, the Directed Acyclic Graphs (DAGs) formalism
is the most suitable to extend.},
  doi       = {10.1145/3185768.3186297},
  isbn      = {9781450356299},
  keywords  = {cloud, workflow, non-functional requirement, formalism, datacenter},
  location  = {Berlin, Germany},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3185768.3186297},
}

@InProceedings{Moreno2017,
  author    = {Moreno, Gabriel A. and Papadopoulos, Alessandro V. and Angelopoulos, Konstantinos and C\'{a}mara, Javier and Schmerl, Bradley},
  booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
  title     = {Comparing Model-Based Predictive Approaches to Self-Adaptation: CobRA and PLA},
  year      = {2017},
  pages     = {42–53},
  publisher = {IEEE Press},
  series    = {SEAMS '17},
  abstract  = {Modern software-intensive systems must often guarantee certain quality requirements
under changing run-time conditions and high levels of uncertainty. Self-adaptation
has proven to be an effective way to engineer systems that can address such challenges,
but many of these approaches are purely reactive and adapt only after a failure has
taken place. To overcome some of the limitations of reactive approaches (e.g., lagging
behind environment changes and favoring short-term improvements), recent proactive
self-adaptation mechanisms apply ideas from control theory, such as model predictive
control (MPC), to improve adaptation. When selecting which MPC approach to apply,
the improvement that can be obtained with each approach is scenario-dependent, and
so guidance is needed to better understand how to choose an approach for a given situation.
In this paper, we compare CobRA and PLA, two approaches that are inspired by MPC.
CobRA is a requirements-based approach that applies control theory, whereas PLA is
architecture-based and applies stochastic analysis. We compare the two approaches
applied to RUBiS, a benchmark system for web and cloud application performance, discussing
the required expertise needed to use both approaches and comparing their run-time
performance with respect to different metrics.},
  doi       = {10.1109/SEAMS.2017.2},
  isbn      = {9781538615508},
  keywords  = {self-adaptation, adaptive system, model predictive control, CobRA, latency, PLA},
  location  = {Buenos Aires, Argentina},
  numpages  = {12},
  url       = {https://doi.org/10.1109/SEAMS.2017.2},
}

@InProceedings{Widder2018,
  author    = {Widder, David Gray and Hilton, Michael and K\"{a}stner, Christian and Vasilescu, Bogdan},
  booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
  title     = {I'm Leaving You, Travis: A Continuous Integration Breakup Story},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {165–169},
  publisher = {Association for Computing Machinery},
  series    = {MSR '18},
  abstract  = {Continuous Integration (CI) services, which can automatically build, test, and deploy
software projects, are an invaluable asset in distributed teams, increasing productivity
and helping to maintain code quality. Prior work has shown that CI pipelines can be
sophisticated, and choosing and configuring a CI system involves tradeoffs. As CI
technology matures, new CI tool offerings arise to meet the distinct wants and needs
of software teams, as they negotiate a path through these tradeoffs, depending on
their context. In this paper, we begin to uncover these nuances, and tell the story
of open-source projects falling out of love with Travis, the earliest and most popular
cloud-based CI system. Using logistic regression, we quantify the effects that open-source
community factors and project technical factors have on the rate of Travis abandonment.
We find that increased build complexity reduces the chances of abandonment, that larger
projects abandon at higher rates, and that a project's dominant language has significant
but varying effects. Finally, we find the surprising result that metrics of configuration
attempts and knowledge dispersion in the project do not affect the rate of abandonment.},
  doi       = {10.1145/3196398.3196422},
  isbn      = {9781450357166},
  location  = {Gothenburg, Sweden},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3196398.3196422},
}

@InProceedings{Bhattacharya2016,
  author    = {Bhattacharya, Adrija and Choudhury, Sankhayan},
  booktitle = {Proceedings of the International Conference on Informatics and Analytics},
  title     = {An Efficient Service Selection Approach through a Goodness Measure of the Participating QoS},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICIA-16},
  abstract  = {The service repository in cloud consists of atomic services those need to be composed
as per the requirement of consumers. In general, various providers offer different
atomic services with same functionalities. These are called similar services and the
service selection is the process to choose the best one among them based on the associated
Quality of Services. Thus a service selection problem for satisfying the requirement
of a consumer with given constraints is conceptualized as a multi-objective optimization
problem. Sometime it involves the objectives that have conflict among them and as
a result the complexity of the problem increases. In such cases users are requested
to provide the feedback on the required QoS and accordingly the solution is offered.
This demands sufficient domain knowledge from a user that may not be feasible in real
cases. As a result the offered solution may deviate from the intended one. In this
work we have proposed a method to calculate an overall measure of a service considering
all QoS. It converts the multi-objective problem to single objective. This reduces
the exponential complexity of NP-Hard problem into a problem solvable in polynomial
time. The proposed Service Selection algorithm does not require any feedback from
the users. The algorithm is capable to offer a moderate solution to users considering
all requested QoS. The experiment shows that almost in every case the proposed algorithm
is able to deliver a solution satisfying all QoS as referred by a user.},
  articleno = {94},
  doi       = {10.1145/2980258.2980451},
  isbn      = {9781450347563},
  keywords  = {Service Selection, Goodness, QoS},
  location  = {Pondicherry, India},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2980258.2980451},
}

@Article{Olson2017,
  author     = {Olson, Judith S. and Wang, Dakuo and Olson, Gary M. and Zhang, Jingwen},
  journal    = {ACM Trans. Comput.-Hum. Interact.},
  title      = {How People Write Together Now: Beginning the Investigation with Advanced Undergraduates in a Project Course},
  year       = {2017},
  issn       = {1073-0516},
  month      = mar,
  number     = {1},
  volume     = {24},
  abstract   = {Today's commercially available word processors allow people to write collaboratively
in the cloud, both in the familiar asynchronous mode and now in synchronous mode as
well. This opens up new ways of working together. We examined the data traces of collaborative
writing behavior in student teams’ use of Google Docs to discover how they are writing
together now. We found that student teams write both synchronously and asynchronously,
take fluid roles in the writing and editing of the documents, and show a variety of
styles of collaborative writing, including writing from scratch, beginning with an
outline, pasting in a related example as a template to organize their own writing,
and three more. We also found that the document serves as a place where they share
a number of things not included in the final document, including links or references
to related materials, the assignment requirements from the instructor, and informal
discussions to coordinate the collaboration or to structure the document. We computed
a number of measures to depict a group's collaboration behavior and asked external
graders to score these documents for quality. We found that the documents that included
balanced participation and/or exhibited leadership were judged higher in quality,
as were those that were longer. We then suggested system design implications and behavioral
guidelines to support people writing together better, and concluded the paper with
future research directions.},
  address    = {New York, NY, USA},
  articleno  = {4},
  doi        = {10.1145/3038919},
  issue_date = {March 2017},
  keywords   = {Google docs, collaboration, co-authoring, writing style},
  numpages   = {40},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3038919},
}

@InProceedings{Du2020,
  author    = {Du, Yifan and Sailhan, Fran\c{c}oise and Issarny, Val\'{e}rie},
  booktitle = {MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
  title     = {IAM&nbsp;– Interpolation and Aggregation on the Move: Collaborative Crowdsensing for Spatio-Temporal Phenomena},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {337–346},
  publisher = {Association for Computing Machinery},
  series    = {MobiQuitous '20},
  abstract  = {Crowdsensing allows citizens to contribute to the monitoring of their living environment
using the sensors embedded in their mobile devices, e.g., smartphones. However, crowdsensing
at scale involves significant communication, computation, and financial costs due
to the dependence on cloud infrastructures for the analysis (e.g., interpolation and
aggregation) of spatio-temporal data. This limits the adoption of crowdsensing by
activists although sorely needed to inform our knowledge of the environment. As an
alternative to the centralized analysis of crowdsensed observations, this paper introduces
a fully distributed interpolation-mediated aggregation approach running on smartphones.
To achieve so efficiently, we model the interpolation as a distributed tensor completion
problem, and we introduce a lightweight aggregation strategy that anticipates the
likelihood of future encounters according to the quality of the interpolation. Our
approach thus shifts the centralized post-processing of crowdsensed data to distributed
pre-processing on the move, based on opportunistic encounters of crowdsensors through
state-of-the-art D2D networking. The evaluation using a dataset of quantitative environmental
measurements collected from 550 crowdsensors over 1 year shows that our solution significantly
reduces –and may even eliminate– the dependence on the cloud infrastructure, while
it incurs a limited resource cost on end devices. Meanwhile, the overall data accuracy
remains comparable to that of the centralized approach.},
  doi       = {10.1145/3448891.3448918},
  isbn      = {9781450388405},
  keywords  = {Aggregation, Interpolation, Opportunistic Relay, Crowdsensing, Pervasive Computing, Ubiquitous Sensing},
  location  = {Darmstadt, Germany},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3448891.3448918},
}

@Article{Mountantonakis2018,
  author     = {Mountantonakis, Michalis and Tzitzikas, Yannis},
  journal    = {J. Data and Information Quality},
  title      = {Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets},
  year       = {2018},
  issn       = {1936-1955},
  month      = jan,
  number     = {3},
  volume     = {9},
  abstract   = {Although the ultimate objective of Linked Data is linking and integration, it is not
currently evident how connected the current Linked Open Data (LOD) cloud is. In this
article, we focus on methods, supported by special indexes and algorithms, for performing
measurements related to the connectivity of more than two datasets that are useful
in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference,
i.e., for obtaining complete information about a set of entities, including provenance
information; (c) Data Quality Assessment and Improvement, i.e., for assessing the
connectivity between any set of datasets and monitoring their evolution over time,
as well as for estimating data veracity; (d) Dataset Visualizations; and various other
tasks. Since it would be prohibitively expensive to perform all these measurements
in a na\"{\i}ve way, in this article, we introduce indexes (and their construction algorithms)
that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix
index, (ii) a sameAs catalog for computing the symmetric and transitive closure of
the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware
element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based
incremental algorithms for speeding up the computation of the intersection of URIs
of any set of datasets. For enhancing scalability, we propose parallel index construction
algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved
speedup using either a single machine or a cluster of machines, and we provide insights
regarding the factors that affect efficiency. Finally, we report measurements about
the connectivity of the (billion triples-sized) LOD cloud that have never been carried
out so far.},
  address    = {New York, NY, USA},
  articleno  = {15},
  doi        = {10.1145/3165713},
  issue_date = {March 2018},
  keywords   = {spark, Data quality, connectivity, lattice of measurements, big data, dataset selection, mapreduce, dataset discovery, linked data},
  numpages   = {49},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3165713},
}

@InProceedings{PerezPalacin2015,
  author    = {Perez-Palacin, Diego and Mirandola, Raffaela and Monterisi, Federico and Montoli, Andrea},
  booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
  title     = {QoS-Driven Probabilistic Runtime Evaluations of Virtual Machine Placement on Hosts},
  year      = {2015},
  pages     = {90–94},
  publisher = {IEEE Press},
  series    = {UCC '15},
  abstract  = {We tackle the cloud providers challenge of virtual machine placement when the client
experienced Quality of Service (QoS) is of paramount importance and resource demand
of virtual machines varies over time. To this end, this work investigates approaches
that leverage measured dynamic data for placement decisions. Relying on dynamic data
to guide decisions has, on the one hand, the potential to optimize hardware utilization,
while, on the other hand, increases the risk on the provided QoS. In this context,
we present three probabilistic methods for evaluation of host suitability to allocate
new virtual machines. We also present experiments results that illustrate the differences
in the outcomes of presented approaches.},
  isbn      = {9780769556970},
  location  = {Limassol, Cyprus},
  numpages  = {5},
}

@InProceedings{Kotlar2018,
  author    = {Kotlar, Alex V. and Wingo, Thomas S.},
  booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
  title     = {Tutorial: Rapidly Identifying Disease-Associated Rare Variants Using Annotation and Machine Learning at Whole-Genome Scale Online},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {558},
  publisher = {Association for Computing Machinery},
  series    = {BCB '18},
  abstract  = {Accurately identifying disease-associated alleles from large sequencing experiments
remains challenging. During this tutorial, participants will learn how to use a new
variant annotation and filtering web app called Bystro (https://bystro.io/) to analyze
sequencing experiments. Bystro is the first online, cloud-based application that makes
variant annotation and filtering accessible to all researchers for even the largest,
terabyte-sized whole-genome experiments containing thousands of samples. Using its
general-purpose, natural-language filtering engine, attendees will be shown how to
perform quality control measures and identify alleles of interest. They will then
be guided in exporting those variants, and using them in both a regression context
by performing rare-variant association tests in R, as well as classification context
by training new machine learning models in Python's scikit-learn library.},
  doi       = {10.1145/3233547.3233666},
  isbn      = {9781450357944},
  keywords  = {rare-variant association tests, variant classification, bioinformatics, machine learning},
  location  = {Washington, DC, USA},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3233547.3233666},
}

@InProceedings{Liu2019,
  author    = {Liu, Xiaofeng and Zou, Hui and Niu, Wanyu and Song, Yuqing and He, Wenzhang},
  booktitle = {Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing},
  title     = {An Approach of Traffic Accident Scene Reconstruction Using Unmanned Aerial Vehicle Photogrammetry},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {31–34},
  publisher = {Association for Computing Machinery},
  series    = {SSIP 2019},
  abstract  = {Accurate and detailed information of traffic accident scene is important for accident
investigation, and the current investigation methods (tape measuring and total station
survey) always need highway closure, and their working process is time-consuming.
From different angles or at different altitudes, unmanned aerial vehicle (UAV) can
monitor accident site without interrupting the traffic flow, therefore, UAV is introduced
for accident scene reconstruction. Firstly, the method framework of accident scene
reconstruction was proposed, in which UAV was used to take pictures of accident site,
and imaging system was adopted to reconstruct the 2D and 3D accident scene. Then,
3D reconstruction, point cloud generation, and model optimization were presented.
Next, a UAV flight experiment was conducted for traffic accident scene reconstruction,
and two evaluation indexes, signal-to-noise ratio and structural similarity, were
introduced to assess the image quality of accident scene reconstruction. The case
study demonstrates that compared with current methods, the proposed method is efficient;
moreover, the effect of accident scene reconstruction is satisfactory.},
  doi       = {10.1145/3365245.3365247},
  isbn      = {9781450372435},
  keywords  = {unmanned aerial vehicle, traffic investigation, Scene reconstruction, photogrammetry},
  location  = {Prague, Czech Republic},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3365245.3365247},
}

@InProceedings{Noura2020,
  author    = {Noura, Mahda and Heil, Sebastian and Gaedke, Martin},
  booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
  title     = {Natural Language Goal Understanding for Smart Home Environments},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {IoT '20},
  abstract  = {One of the main challenges of the Internet of Things (IoT) is to enable end-users
without technical experience to use, control or monitor smart devices. However, enabling
end-users to interact with these smart devices in an intuitive and natural way becomes
increasingly important as they become more pervasive in our homes, workplaces and
public environments. Voice-based interfaces are the emerging trend to provide a more
natural human-device interaction in smart environments. Such interfaces require Natural
Language Understanding (NLU) approaches to identify the meaning of end-users' voice
inputs. Designing voice interfaces that are not limited to a small, fixed set of pre-defined
commands is far from trivial. Existing voice-based solutions in the smart home domain
either restrict the end-users to follow a strict language pattern, do not support
indirect goals, require a large training dataset, or need a voice assistant located
in the cloud. In this paper, we propose an approach for understanding end-users goals
from voice inputs in smart homes. Our approach alleviates the need for end-users to
learn or remember concrete operations of the devices and specific words/pattern structures
rather it enables them to control their smart homes based on the desired goals (effects).
We evaluate the approach through application to a collection of 253 goals from real
end-users and report on quality metrics. The results demonstrate that our solution
provides a good accuracy, high precision and acceptable recall for understanding end-users
goals in the smart home domain.},
  articleno = {1},
  doi       = {10.1145/3410992.3410996},
  isbn      = {9781450387583},
  keywords  = {goal recognition, natural language understanding, voice interface, internet of things, smart home},
  location  = {Malm\"{o}, Sweden},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3410992.3410996},
}

@Article{Papadopoulos2016,
  author     = {Papadopoulos, Alessandro Vittorio and Ali-Eldin, Ahmed and \r{A}rz\'{e}n, Karl-Erik and Tordsson, Johan and Elmroth, Erik},
  journal    = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
  title      = {PEAS: A Performance Evaluation Framework for Auto-Scaling Strategies in Cloud Applications},
  year       = {2016},
  issn       = {2376-3639},
  month      = aug,
  number     = {4},
  volume     = {1},
  abstract   = {Numerous auto-scaling strategies have been proposed in the past few years for improving
various Quality of Service (QoS) indicators of cloud applications, for example, response
time and throughput, by adapting the amount of resources assigned to the application
to meet the workload demand. However, the evaluation of a proposed auto-scaler is
usually achieved through experiments under specific conditions and seldom includes
extensive testing to account for uncertainties in the workloads and unexpected behaviors
of the system. These tests by no means can provide guarantees about the behavior of
the system in general conditions. In this article, we present a Performance Evaluation
framework for Auto-Scaling (PEAS) strategies in the presence of uncertainties. The
evaluation is formulated as a chance constrained optimization problem, which is solved
using scenario theory. The adoption of such a technique allows one to give probabilistic
guarantees of the obtainable performance. Six different auto-scaling strategies have
been selected from the literature for extensive test evaluation and compared using
the proposed framework. We build a discrete event simulator and parameterize it based
on real experiments. Using the simulator, each auto-scaler’s performance is evaluated
using 796 distinct real workload traces from projects hosted on the Wikimedia foundations’
servers, and their performance is compared using PEAS. The evaluation is carried out
using different performance metrics, highlighting the flexibility of the framework,
while providing probabilistic bounds on the evaluation and the performance of the
algorithms. Our results highlight the problem of generalizing the conclusions of the
original published studies and show that based on the evaluation criteria, a controller
can be shown to be better than other controllers.},
  address    = {New York, NY, USA},
  articleno  = {15},
  doi        = {10.1145/2930659},
  issue_date = {September 2016},
  keywords   = {Performance evaluation, elasticity, cloud computing, randomized optimization, auto-scaling},
  numpages   = {31},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2930659},
}

@Article{Cheng2020,
  author     = {Cheng, Yingying and Jia, Xiaohua},
  journal    = {IEEE/ACM Trans. Netw.},
  title      = {NAMP: Network-Aware Multipathing in Software-Defined Data Center Networks},
  year       = {2020},
  issn       = {1063-6692},
  month      = apr,
  number     = {2},
  pages      = {846–859},
  volume     = {28},
  abstract   = {Data center networks employ parallel paths to perform load balancing. Existing traffic
splitting schemes propose weighted traffic distribution across multiple paths via
a centralized view. An SDN controller computes the traffic splitting ratio of a flow
group among all the paths, and implements the ratio by creating multiple rules in
the flow table of OpenFlow switches. However, since the number of rules in TCAM-based
flow table is limited, it is not scalable to implement the ideal splitting ratio for
every flow group. Existing solutions, WCMP and Niagara, aim at reducing the maximum
oversubscription of all egress ports and reducing traffic imbalance, respectively.
However, the transmission time of flow groups, which measures the quality of cloud
services, is sub-optimal in existing solutions that ignore heterogeneous network bandwidth.
We propose and implement NAMP, a multipathing scheme considering the network heterogeneity,
to efficiently optimize the transmission time of flow groups. Experimental results
show that NAMP reduces the transmission time by up to 45.4% than Niagara, up
to 50% than WCMP, and up to 60% than ECMP.},
  doi        = {10.1109/TNET.2020.2971587},
  issue_date = {April 2020},
  numpages   = {14},
  publisher  = {IEEE Press},
  url        = {https://doi.org/10.1109/TNET.2020.2971587},
}

@InProceedings{Klein2015a,
  author    = {Klein, John and Gorton, Ian},
  booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
  title     = {Runtime Performance Challenges in Big Data Systems},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {17–22},
  publisher = {Association for Computing Machinery},
  series    = {WOSP '15},
  abstract  = {Big data systems are becoming pervasive. They are distributed systems that include
redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient
to assure runtime performance in production. This is due to the scale of the deployed
system, the continually evolving workloads, and the unpredictable quality of service
of the shared infrastructure. Consequently, a solution for addressing performance
requirements needs sophisticated runtime observability and measurement. Observability
gives real-time insights into a system's health and status, both at the system and
application level, and provides historical data repositories for forensic analysis,
capacity planning, and predictive analytics. Due to the scale and heterogeneity of
big data systems, significant challenges exist in the design, customization and operations
of observability capabilities. These challenges include economical creation and insertion
of monitors into hundreds or thousands of computation and data nodes, efficient, low
overhead collection and storage of measurements (which is itself a big data problem),
and application-aware aggregation and visualization. In this paper we propose a reference
architecture to address these challenges, which uses a model-driven engineering toolkit
to generate architecture-aware monitors and application-specific visualizations.},
  doi       = {10.1145/2693561.2693563},
  isbn      = {9781450333405},
  keywords  = {observability, big data, model-driven engineering},
  location  = {Austin, Texas, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2693561.2693563},
}

@InProceedings{Adkins2018,
  author    = {Adkins, Joshua and Ghena, Branden and Jackson, Neal and Pannuto, Pat and Rohrer, Samuel and Campbell, Bradford and Dutta, Prabal},
  booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
  title     = {Applications on the Signpost Platform for City-Scale Sensing: Demo Abstract},
  year      = {2018},
  pages     = {124–125},
  publisher = {IEEE Press},
  series    = {IPSN '18},
  abstract  = {City-scale sensing holds the promise of enabling deeper insight into how our urban
environments function. Applications such as observing air quality and measuring traffic
flows can have powerful impacts, allowing city planners and citizen scientists alike
to understand and improve their world. However, the path from conceiving applications
to implementing them is fraught with difficulty. A successful city-scale deployment
requires physical installation, power management, and communications---all challenging
tasks standing between a good idea and a realized one.The Signpost platform, presented
at IPSN 2018, has been created to address these challenges. Signpost enables easy
deployment by relying on harvested, solar energy and wireless networking rather than
their wired counterparts. To further lower the bar to deploying applications, the
platform provides the key resources necessary to support its pluggable sensor modules
in their distributed sensing tasks. In this demo, we present the Signpost hardware
and several applications running on a deployment of Signposts on UC Berkeley's campus,
including distributed, energy-adaptive traffic monitoring and fine grained weather
reporting. Additionally we show the cloud infrastructure supporting the Signpost deployment,
specifically the ability to push new applications and parameters down to existing
sensors, with the goal of demonstrating that the existing deployment can serve as
a future testbed.},
  doi       = {10.1109/IPSN.2018.00025},
  isbn      = {9781538652985},
  location  = {Porto, Portugal},
  numpages  = {2},
  url       = {https://doi.org/10.1109/IPSN.2018.00025},
}

@InProceedings{Shi2019,
  author    = {Shi, Shu and Gupta, Varun and Jana, Rittwik},
  booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
  title     = {Freedom: Fast Recovery Enhanced VR Delivery Over Mobile Networks},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {130–141},
  publisher = {Association for Computing Machinery},
  series    = {MobiSys '19},
  abstract  = {In this paper we design and implement Freedom, a mobile VR system that deliver high
quality VR content on today's mobile devices using 4G/LTE cellular networks. Compared
to existing state-of-the-art, Freedom does not rely on any video frame pre- rendering
or viewpoint prediction. We send a latency-adaptive VAM frame that contains pixels
around the FoV. This allows the clients to render locally at a high refresh rate of
60 Hz to accommodate and compensate for the user's head movements before the next
server update arrives. We demonstrate that Freedom is the first system in the world
that can support dynamic and live 8K resolution VR content, while adapting to the
real-world latency variations experienced in cellular networks. Compared to streaming
the whole 360° panoramic VR content, we show that Freedom achieves up to 80% bandwidth
savings. Finally, we provide detailed end to end latency measurements of actual VR
systems by running extensive experiments in a private LTE testbed using a Mobile Edge
Cloud (MEC).},
  doi       = {10.1145/3307334.3326087},
  isbn      = {9781450366618},
  keywords  = {remote rendering, 360 video, mobile vr, motion-to-update latency, mobile edge cloud},
  location  = {Seoul, Republic of Korea},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3307334.3326087},
}

@InProceedings{Wu2016,
  author    = {Wu, Chao and Jia, Jia and Zhu, Wenwu and Chen, Xu and Yang, Bowen and Zhang, Yaoxue},
  booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
  title     = {Affective Contextual Mobile Recommender System},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {1375–1384},
  publisher = {Association for Computing Machinery},
  series    = {MM '16},
  abstract  = {Exponential growth of media consumption in online social networks demands effective
recommendation to improve the quality of experience especially for on-the-go mobile
users. By means of large-scale trace-driven measurements over mobile Twitter traces
from users, we reveal the significance of affective features in shaping users' social
media behaviors. Existing recommender systems however, rarely support this psychological
effect in real-life. To capture this effect, in this paper we propose Kaleido, a real
mobile system to achieve an affect-aware learning-based social media recommendation.Specifically,
we design a machine learning mechanism to infer the affective feature within media
contents. Furthermore, a cluster-based latent bias model is provided for jointly training
the affect, behavior and social contexts. Our comprehensive experiments on Android
prototype expose a superior prediction accuracy of 82%, with more than 20% accuracy
improvement over existing mobile recommender systems. Moreover, by enabling users
to offload their machine learning procedures to the deployed edge-cloud testbed, our
system achieves speed-up of a factor of 1,000 against the local data training execution
on smartphones.},
  doi       = {10.1145/2964284.2964327},
  isbn      = {9781450336031},
  keywords  = {social networks, affective computing, recommender system, mobile application},
  location  = {Amsterdam, The Netherlands},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2964284.2964327},
}

@InProceedings{Misra2014,
  author    = {Misra, Prasant Kumar and Hu, Wen and Jin, Yuzhe and Liu, Jie and Souza de Paula, Amanda and Wirstrom, Niklas and Voigt, Thiemo},
  booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
  title     = {Energy Efficient GPS Acquisition with Sparse-Gps},
  year      = {2014},
  pages     = {155–166},
  publisher = {IEEE Press},
  series    = {IPSN '14},
  abstract  = {Following rising demands in positioning with GPS, low-cost receivers are becoming
widely available; but their energy demands are still too high. For energy efficient
GPS sensing in delay-tolerant applications, the possibility of offloading a few milliseconds
of raw signal samples and leveraging the greater processing power of the cloud for
obtaining a position fix is being actively investigated. In an attempt to reduce the
energy cost of this data offloading operation, we propose Sparse-GPS1: a new computing
framework for GPS acquisition via sparse approximation. Within the framework, GPS
signals can be efficiently compressed by random ensembles. The sparse acquisition
information, pertaining to the visible satellites that are embedded within these limited
measurements, can subsequently be recovered by our proposed representation dictionary.
By extensive empirical evaluations, we demonstrate the acquisition quality and energy
gains of Sparse-GPS. We show that it is twice as energy efficient than offloading
uncompressed data, and has 5-10 times lower energy costs than standalone GPS; with
a median positioning accuracy of 40 m.},
  isbn      = {9781479931460},
  keywords  = {sparse approximation, energy efficiency, synchronization, location sensing, compressed sensing, gps},
  location  = {Berlin, Germany},
  numpages  = {12},
}

@InProceedings{Kaemaeraeinen2018,
  author    = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Eerik\"{a}inen, Jukka and Yl\"{a}-J\"{a}\"{a}ski, Antti},
  booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
  title     = {CloudVR: Cloud Accelerated Interactive Mobile Virtual Reality},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {1181–1189},
  publisher = {Association for Computing Machinery},
  series    = {MM '18},
  abstract  = {High quality immersive Virtual Reality experience currently requires a PC setup with
cable connected head mounted display, which is expensive and restricts user mobility.
This paper presents CloudVR which is a system for cloud accelerated interactive mobile
VR. It is designed to provide short rotation and interaction latencies through panoramic
rendering and dynamic object placement. CloudVR also includes rendering optimizations
to reduce server-side computational load and bandwidth requirements between the server
and client. Performance measurements with a CloudVR prototype suggest that the optimizations
make it possible to double the server's framerate and halve the amount of bandwidth
required and that small objects can be quickly moved at run time to client device
for rendering to provide shorter interaction latency. A small-scale user study indicates
that CloudVR users do not notice small network latencies (20ms) and even much longer
ones (100-200ms) become non-trivial to detect when they do not affect the interaction
with objects. Finally, we present a design of CloudVR extension to multi-user scenarios.},
  doi       = {10.1145/3240508.3240620},
  isbn      = {9781450356657},
  keywords  = {rendering, unity, virtual reality, edge computing, cloud, optimization},
  location  = {Seoul, Republic of Korea},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3240508.3240620},
}

@InProceedings{Kaliszan2019,
  author    = {Kaliszan, Damian and F\"{u}rst, Steffen and Gienger, Michael and Gogolenko, Sergiy and Meyer, Norbert and Petruczynik, Sebastian},
  booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
  title     = {Comparative Benchmarking of HPC Systems for GSS Applications: GSS Applications in the HPC Ecosystem},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {43–52},
  publisher = {Association for Computing Machinery},
  series    = {HPC Asia 2019},
  abstract  = {The work undertaken in this paper was done in the Centre of Excellence for Global
Systems Science (CoeGSS), an interdisciplinary project, funded by the European Commission.The
project provides decision-support in the face of global challenges. It brings together
HPC and global systems science. This paper presents a proposition of GSS benchmark
with the aim to find the most suitable HPC architecture and the best HPC system which
allows to run GSS applications effectively. The GSS provides evidence about global
systems challenges, e.g. the network structure of the world economy, energy, water
and food supply systems, the global financial system or the global city system, and
the scientific community.The outcome of the analysis is defining a benchmark which
represents the GSS environment in the best way. Three exemplary challenges were defined
as pilot applications: Health Habits, Green Growth and Global Urbanisation extended
with additional applications from GSS ecosystem: Iterative proportional fitting (IPF),
Data rastering - a preprocessing process converting all vectorial representations
of georeferenced data into raster files to be later used as simulation input, Weather
Research and Forecasting (WRF) model, CMAQ/CCTM (Community Air Multiscale Quality
Modelling System/The CMAQ Chemistry-Transport Mode), CM1 (Cloud Modelling), ABMS (Agent-based
Modelling and Simulation), OpenSWPC (An Open-source Seismic Wave Propagation Code).
The above list seems to be quite rich and reflects the real GSS world as much as possible,
having in mind, for example the real-world applications availability.Additionally,
the authors tested new HPC platforms based on Intel® Xeon® Gold 6140, AMD EpycTM,
ARM Hi1616 and IBM Power8+. Due to the hardware availability, the testbed consisted
of a limited number of nodes. This restricted the ability to provide full tests of
scalability for given applications. However, this small number of available computational
units (cores) can provide valuable outcome including architecture comparison for different
applications based on execution times, TDPs1 and TCO2. These are the basic metrics
used for providing a ranking of HPC architectures. Finally, this document is thought
to be valuable information for the GSS community for future purposes and analysis
to determine their specific demands as well as - in general - to help develop a mature
final benchmark set reflecting the GSS environment requirements and specialty. As
none of the existing benchmarks is dedicated to the GSS community, the authors decided
to create one by calling it a GSS benchmark to serve and help GSS users in their future
work.},
  doi       = {10.1145/3293320.3293326},
  isbn      = {9781450366328},
  keywords  = {Global Systems Science, HPC benchmarks, parallel applications, e-Infrastructure evaluation},
  location  = {Guangzhou, China},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3293320.3293326},
}

@InProceedings{Vani2020,
  author    = {Vani, K. Suvarna and M., Arul Raj and M., Padmaja and Kumar, K. Praveen and A., Jitendra and A., Ravi Raja},
  booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
  title     = {Detection and Extraction of Roads Using Cartosat-2 High Resolution Satellite Imagery},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {7–11},
  publisher = {Association for Computing Machinery},
  series    = {ICIGP 2020},
  abstract  = {The extraction of roads from panchromatic images has many insightful applications
in the fields of urban planning, setting up of transportation, disaster management
and cartography in geographical information systems (GIS). The process of extracting
roads from high-resolution images is composite, due to the presence of different noises
(i.e., buildings, shadows, clouds etc.). Various image processing techniques and various
quality measures are applied on the high-resolution remote sensing satellite images
to improve the quality of the image and interactively extract the information of roads.
Cartosat-2 images available in Bhuvan website of ISRO are taken for testing the validity
of proposed method. The proposed method enhances the images using Contrast Limited
Adaptive Histogram Equalization (CLAHE) and Line Detector for detecting road segments.
Connected component analysis (CCA) is performed on segmented image for connection
of disconnected objects in the segmented image. Morphological operations fill the
holes caused by the presence of shadows, buildings and trees on the road surface.},
  doi       = {10.1145/3383812.3383823},
  isbn      = {9781450377201},
  keywords  = {cartosat-2 dataset, line segment detector, morphological operations, GIS, image processing},
  location  = {Singapore, Singapore},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3383812.3383823},
}

@InProceedings{Nguyen2014,
  author    = {Nguyen, Hoang Minh and W\"{u}nsche, Burkhard and Delmas, Patrice and Lutteroth, Christof},
  booktitle = {Proceedings of the 29th International Conference on Image and Vision Computing New Zealand},
  title     = {Identifying Low Confidence Mesh Regions: Uncertainty Measures and Segmentation},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {48–53},
  publisher = {Association for Computing Machinery},
  series    = {IVCNZ '14},
  abstract  = {3D digital models have become an important part of diverse applications ranging from
computer games, virtual reality, architectural design to visual impact studies. One
common method to create 3D models is to create a point cloud using laser scanners,
structured lighting sensors, or image-based modelling techniques, and then construct
a 3D mesh, and texture-map it using photographs of the observed scene. Attributed
to the inherent properties of general 3D scenes such as occluded or inaccessible parts,
reflective surfaces, lighting conditions or poor-quality inputs, 3D models produced
by these approaches often exhibit unsatisfactory and erroneous mesh regions. In many
cases, it is desirable to identify and extract such regions so that they can be constructed
or corrected through other means. While much effort has been invested into the problem
of 3D reconstructions, the task of evaluating existing models and preparing them for
subsequent enhancement processes has been largely neglected. In this paper, we present
a novel method for automatically detecting and segmenting mesh regions with low confidence
in their correctness. The confidence estimation is achieved by exploiting and integrating
various uncertainty measures such as geometric distances, normal variations and texture
discrepancies. Low-confidence mesh regions are isolated and removed in such a way
that the extracted region's boundary is as simple as possible in order to facilitate
subsequent automatic or manual improvement of these regions. Segmentation is achieved
by minimising an energy function that takes the genus and boundary length and smoothness
of the extracted regions into account.},
  doi       = {10.1145/2683405.2683409},
  isbn      = {9781450331845},
  keywords  = {Uncertainty Measure, Mesh Classification, 3D Reconstruction},
  location  = {Hamilton, New Zealand},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2683405.2683409},
}

@InProceedings{Amorim2019,
  author    = {de Amorim, Irandir O. and de Melo, Jose F. V. and Balieiro, Andson M. and Santos, Bruno B. dos},
  booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
  title     = {An Evolutionary Approach for Video Application Energy Consumption Estimation in Mobile Devices},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {169–175},
  publisher = {Association for Computing Machinery},
  series    = {WebMedia '19},
  abstract  = {In the last years, the multimedia traffic has increased significantly and the mobile
devices (e.g. smart phones and tablets) have been widely used to consume this content
type. Video applications demand high energy consumption of the device because they
perform complex operations and deal with a large data amount. Although hardware improvements
in the mobile devices have been achieved, the advances in battery technology have
not kept the same pace. In this respect, the combination of video applications with
the limited battery capacity of the mobile devices has challenged the academia and
industry in the development of techniques for energy management and provision of quality
of experience (QoE) to the user. Energy consumption estimation models may assist these
techniques, as well as, the decision made process when the computational offloading
from the mobile device to the cloud is considered. This paper presents an evolutionary
approach based on Genetic Algorithms (GAs) and Swarm Particle Optimization (PSO) for
energy consumption estimation in mobile devices running video applications. The proposal
is directly applicable to different model types (linear and non-linear ones), without
the linearization cost, and it is evaluated in terms of mean squared error (MSE),
using energy consumption measurement data of videos with different configurations.
Results show the superiority of our proposal in comparison to the literature that
adopts the Ordinary Least Squares method.},
  doi       = {10.1145/3323503.3349545},
  isbn      = {9781450367639},
  keywords  = {energy consumption model for mobile devices, genetic algorithms, particle swarm optimization, video application},
  location  = {Rio de Janeiro, Brazil},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3323503.3349545},
}

@InProceedings{Li2014,
  author    = {Li, Liqun and Shen, Guobin and Zhao, Chunshui and Moscibroda, Thomas and Lin, Jyh-Han and Zhao, Feng},
  booktitle = {Proceedings of the 20th Annual International Conference on Mobile Computing and Networking},
  title     = {Experiencing and Handling the Diversity in Data Density and Environmental Locality in an Indoor Positioning Service},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {459–470},
  publisher = {Association for Computing Machinery},
  series    = {MobiCom '14},
  abstract  = {Diversity in training data density and environment locality is intrinsic in the real-world
deployment of indoor localization systems and has a major impact on the performance
of existing localization approaches. In this paper, through micro-benchmarks, we find
that fingerprint-based approaches are preferable in scenarios where a dense database
is available; while model-based approaches are the method of choice in the case of
sparse data. It should be noted, however, that practical situations are complex. A
single deployment often features both sparse and dense sampled areas. Furthermore,
the internal layout affects the propagation of radio signals and exhibits environmental
impacts. A certain number of measurement samples may be sufficient for one part of
the building, but entirely insufficient for another. Thus, finding the right indoor
localization algorithm for a given large-scale deployment is challenging, if not impossible;
there is no one-size-fits-all indoor localization approach.Realizing the fundamental
fact that the quality of the location database capturing the actual radio map dictates
localization accuracy, in this paper, we propose Modellet, an algorithmic approach
that optimally approximates the actual radio map by unifying model-based and fingerprint-based
approaches. Modellet represents the radio map using a fingerprint-cloud that incorporates
both measured real fingerprints and virtual fingerprints, which are computed from
models with a local support, based on the key concept of the supporting set. We evaluate
Modellet with data collected from an office building as well as 13 large-scale deployment
venues (shopping malls and airports), located across China, U.S., and Germany. Comparing
Modellet with two representative baseline approaches, RADAR and EZPerfect, demonstrates
that Modellet effectively adapts to different data densities and environmental conditions,
substantially outperforming existing approaches.},
  doi       = {10.1145/2639108.2639118},
  isbn      = {9781450327831},
  keywords  = {fingerprint, indoor localization, model},
  location  = {Maui, Hawaii, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2639108.2639118},
}

@InProceedings{TolosanaCalasanz2014a,
  author    = {Tolosana-Calasanz, Rafael and Ba\~{n}ares, Jos\'{e} \'{A}ngel and Rana, Omer and Pham, Congduc and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
  booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Enforcing Quality of Service on OpenNebula-Based Shared Clouds},
  year      = {2014},
  pages     = {651–659},
  publisher = {IEEE Press},
  series    = {CCGRID '14},
  abstract  = {With an increase in the number of monitoring sensors deployed on physical infrastructures,
there is a corresponding increase in data volumes that need to be processed. Data
measured or collected by sensors is typically processed at destination or "in-transit"
(i.e. from data capture to delivery to a user). When such data are processed in-transit
over a shared distributed computing infrastructure, it is useful to provide elastic
computational capability which can be adapted based on processing requirements and
demand. Where Service Level Agreements (SLAs) have been pre-agreed, such available
computational capacity needs to be shared in such a way that any Quality of Service
related constraints in such SLAs are not violated. This is particularly challenging
for time critical applications and with highly variable and unpredictable rates of
data generation (e.g. in Smart Grid applications where energy usage patterns may change
unpredictably). Previously, we proposed a Reference net based architectural model
for supporting QoS for multiple concurrent data streams being processed (prior to
delivery to a user) over a shared infrastructure. In this paper, we describe a practical
realisation of this architecture using the OpenNebula Cloud platform. We consider
our infrastructure to be composed of a number of nodes, each of which has multiple
processing units and data buffers. We utilize the "token bucket" model for regulating,
on a per stream basis, the data injection rate into each node. We subsequently demonstrate
how a streaming pipeline can be supported and managed using a dynamic control strategy
at each node.},
  doi       = {10.1109/CCGrid.2014.50},
  isbn      = {9781479927838},
  location  = {Chicago, Illinois},
  numpages  = {9},
  url       = {https://doi.org/10.1109/CCGrid.2014.50},
}

@InProceedings{Kateja2015,
  author    = {Kateja, Rajat and Baranasuriya, Nimantha and Navda, Vishnu and Padmanabhan, Venkata N.},
  booktitle = {Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies},
  title     = {DiversiFi: Robust Multi-Link Interactive Streaming},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {CoNEXT '15},
  abstract  = {Real-time, interactive streaming for applications such as audio-video conferencing
(e.g., Skype) and cloud-based gaming depends critically on the network providing low
latency, jitter, and packet loss, much more so than on-demand streaming (e.g., YouTube)
does. However, WiFi networks pose a challenge; our analysis of data from a large VoIP
provider and from our own measurements shows that the WiFi access link is a significant
cause of poor streaming experience.To improve streaming quality over WiFi, we present
DiversiFi, which takes advantage of the diversity of WiFi links available in the vicinity,
even when the individual links are poor. Leveraging such cross-link spatial and channel
diversity outperforms both traditional link selection and the temporal diversity arising
from retransmissions on the same link. It also provides significant gains over and
above the PHY-layer spatial diversity provided by MIMO. Our experimental evaluation
shows that, for a client with two NICs, enabling replication across two WiFi links
helps cut down the poor call rate (PCR) for VoIP by 2.24x.Finally, we present the
design and implementation of DiversiFi, which enables it to operate with single-NIC
clients, and with either minimally modified APs or unmodified APs augmented with a
middlebox. Over 61 runs, where the baseline average PCR is 4.9%, DiversiFi running
with a single NIC, switching between two links, helps cut the PCR down to 0%, while
duplicating wastefully only 0.62% of the packets and impacting competing TCP throughput
by only 2.5%. Thus, DiversiFi provides the benefit of multi-link diversity for real-time
interactive streaming in a manner that is deployable and imposes little overhead,
thereby ensuring coexistence with other applications.},
  articleno = {35},
  doi       = {10.1145/2716281.2836120},
  isbn      = {9781450334129},
  keywords  = {multi-path, wi-fi, VoIP, real-time streaming},
  location  = {Heidelberg, Germany},
  numpages  = {13},
  url       = {https://doi.org/10.1145/2716281.2836120},
}

@Proceedings{2015c,
  title     = {MobiCom '15: Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  year      = {2015},
  address   = {New York, NY, USA},
  isbn      = {9781450336192},
  publisher = {Association for Computing Machinery},
  abstract  = {Welcome to ACM MobiCom 2015, the 21st Annual International Conference on Mobile Computing
and Networking. MobiCom is the premier forum for publishing and presenting cutting-edge
research in mobile systems and wireless networks. The technical program this year
features 38 outstanding papers that cover a wide variety of topics including energy,
sensing, security, wireless access, applications, localization, Internet of things,
mobile cloud, measurement and analysis. We created a new Experience track this year
to encourage authors to present extensive experiences with implementation, deployment,
and operations of mobile ncomputing and wireless networks. One of the accepted papers
is an Experience paper on cellular networks.This year's call for papers attracted
207 qualified submissions from across the globe that were carefully reviewed by 46
Technical Program Committee (TPC) members (+2 TPC chairs) along with a selected group
of external experts. The TPC was formed with the goal of covering diverse research
expertise as well as diverse perspectives and approaches. The TPC included researchers
from 12 countries including China, France, Germany, India, Italy, Singapore, South
Korea, Spain, Sweden, Switzerland, UK, and USA. 25% of the members were female, the
highest ever in the history of MobiCom. We also had broad industry participation with
TPC members from Alcatel-Lucent, Google, HP, IBM, Microsoft, NEC, and Telefonica.The
paper review process was double-blinded and carried out in three phases. In the first
phase, each paper was reviewed by at least three TPC members, and the top 112 papers
were selected for the second phase. In addition to reviewer scores, reviewer confidence
and normalization with respect to other papers in a reviewer's pile, were also considered
in selecting papers. In the second phase, each paper was reviewed by at least two
more reviewers followed by an online, often intense, discussion, producing 68 papers
for the final phase. The final TPC meeting was held on May 28th and 29th in Salt Lake
City, Utah. These 68 papers were organized by their topic areas, and discussed at
length at the meeting. Eventually, 38 papers were shortlisted for inclusion in the
program and a shepherd from the TPC was assigned to each of these papers. As the last
step, each of the shortlisted papers was shepherded through a "blind" process where
the authors interacted with all the reviewers and the shepherd to address the review
comments without knowing the reviewers' or the shepherds' identities. The end result
is an exciting technical program composed of 38 very high quality papers.During the
review process, Prof. Robin Kravets, the TPC co-chair of MobiCom 2013, handled the
papers that were co-authored by TPC chairs, and those that had conflict-of-interest
with both TPC chairs. To ensure fairness and preserve the anonymity of all authors
and reviewers, the assignment of reviewers, the reviews and discussions of these papers
were done out of band without any exposure to the TPC chairs.},
  location  = {Paris, France},
}

@Article{Ma2021,
  author     = {Ma, Xiaohe and Kang, Kaizhang and Zhu, Ruisheng and Wu, Hongzhi and Zhou, Kun},
  journal    = {ACM Trans. Graph.},
  title      = {Free-Form Scanning of Non-Planar Appearance with Neural Trace Photography},
  year       = {2021},
  issn       = {0730-0301},
  month      = jul,
  number     = {4},
  volume     = {40},
  abstract   = {We propose neural trace photography, a novel framework to automatically learn high-quality
scanning of non-planar, complex anisotropic appearance. Our key insight is that free-form
appearance scanning can be cast as a geometry learning problem on unstructured point
clouds, each of which represents an image measurement and the corresponding acquisition
condition. Based on this connection, we carefully design a neural network, to jointly
optimize the lighting conditions to be used in acquisition, as well as the spatially
independent reconstruction of reflectance from corresponding measurements. Our framework
is not tied to a specific setup, and can adapt to various factors in a data-driven
manner. We demonstrate the effectiveness of our framework on a number of physical
objects with a wide variation in appearance. The objects are captured with a light-weight
mobile device, consisting of a single camera and an RGB LED array. We also generalize
the framework to other common types of light sources, including a point, a linear
and an area light.},
  address    = {New York, NY, USA},
  articleno  = {124},
  doi        = {10.1145/3450626.3459679},
  issue_date = {August 2021},
  keywords   = {illumination multiplexing, SVBRDF, optimal lighting pattern},
  numpages   = {13},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3450626.3459679},
}

@InProceedings{Raj2018,
  author    = {Raj, Vinay and Ravichandra, S.},
  booktitle = {2018 3rd IEEE International Conference on Recent Trends in Electronics, Information Communication Technology (RTEICT)},
  title     = {Microservices: A perfect SOA based solution for Enterprise Applications compared to Web Services},
  year      = {2018},
  month     = {May},
  pages     = {1531-1536},
  abstract  = {The Software Engineering community has defined different types of architectures to build applications. One among them is Service Oriented Architecture(SOA) which has created significant impact the way software applications are built. There are many implementations of SOA like Web Services, REST services etc. But Web Services and REST services do not fully follow all the principles of SOA. Microservices as an architectural style recently emerged from SOA by which we can develop business requirements with loosely coupled, self deploying and scalable services. Microservices have gained more popularity in application development as they are easy to understand, scale and deploy. In this paper we discuss principles of SOA, major drawbacks of web services and benefits of Microservices over SOA based web services. We have highlighted the importance of Microservices in software development. This paper gives information for architects as to why choose Microservices architecture over web services. We have also discussed metrics used for calculating Coupling between services and we evaluated by considering a smart payment application for ecommerce which is built using both the styles. We observed that Microservices architectural style has less coupling between services compared to Web Service style based on the metric values of the application.},
  doi       = {10.1109/RTEICT42901.2018.9012140},
  keywords  = {Service-oriented architecture;Couplings;Measurement;Computer architecture;Business;Service Oriented Architecture(SOA);Web Services;Microservices;Coupling;Metrics},
}

@InProceedings{Rosa2020,
  author    = {Rosa, Thatiane de Oliveira and Goldman, Alfredo and Guerra, Eduardo Martins},
  booktitle = {2020 IEEE International Conference on Software Architecture Companion (ICSA-C)},
  title     = {How ‘micro’ are your services?},
  year      = {2020},
  month     = {March},
  pages     = {75-78},
  abstract  = {Microservice is an architectural style that proposes that a complex system should be developed from small and independent services that work together. There is not a welldefined boundary about when a software architecture can be considered based on microservices or not. Because of that, defining microservices context and infrastructure is challenging, especially to characterize aspects related to microservice size, data consistency, and microservices coupling. Thus, it is crucial to understand the microservices-based software characteristics, to comprehend the impact of some evolutions on architecture, and evaluate how much a particular architecture fits the microservices architectural style. Therefore, based on bibliographic research and case studies conducted in academical and industrial environments, we aim to propose a model to characterize the architecture structure based on the main guidelines of the microservice architectural style. This model introduces dimensions that measure characteristics based on modules size, coupling to data sources, and service collaboration. This study should facilitate the mapping, measurement, and monitoring of different impacts generated in the software architecture from increments and refactoring performed. This work is on the initial development stage and as a result, we expected that the model supports architectural decisions that consider different quality attributes to achieve the right balance between service independence and collaboration for a given system.},
  doi       = {10.1109/ICSA-C50368.2020.00023},
  keywords  = {Measurement;Databases;Couplings;Complexity theory;Software;Computer architecture;Software architecture;software architecture;microservices;characterization model},
}

@InProceedings{Santos2020a,
  author    = {Santos, Nuno and Rito Silva, António},
  booktitle = {2020 IEEE International Conference on Software Architecture (ICSA)},
  title     = {A Complexity Metric for Microservices Architecture Migration},
  year      = {2020},
  month     = {March},
  pages     = {169-178},
  abstract  = {Monolith applications tend to be difficult to deploy, upgrade, maintain, and understand. Microservices, on the other hand, have the advantages of being independently developed, tested, deployed, scaled and, more importantly, easier to change and maintain. This paper addresses the problem of migrating a monolith to a microservices architecture. Therefore, we address two research questions: (1) Can we define the cost of decomposition in terms of the effort to redesign a functionality, which is implemented in the monolith as an ACID transaction, into several distributed transactions? (2) Considering several similarity measures between domain entities, which provide a better decomposition when they are compared using the proposed complexity metric? To answer the first research question, we propose a complexity metric, for each functionality of the monolith application, that measures the impact of relaxing the functionality consistency on the architecture redesign and implementation. Regarding the second research question, we experiment with four similarity measures, each based on a different type of information collected from monolith functionality implementation. We evaluated our approach with three monolith systems and compared our complexity metric against industry metrics of cohesion and coupling. We also evaluated the different similarity measures in terms of the complexity of the decomposition they produce. We were able to correctly correlate the complexity metric with other metrics of cohesion and coupling defined in other research and we conclude that no single combination of similarity measures outperforms the other, which is confirmed by the existing research. Additionally, we conclude that the approach can help on an incremental migration to microservices, which, actually, is the strategy proposed by the industry experts.},
  doi       = {10.1109/ICSA47634.2020.00024},
  keywords  = {Measurement;Complexity theory;Business;Computer architecture;Tools;Industries;Couplings;Monolith applications, Microservices, Complexity metrics, Architecture migration, Architecture evolution},
}

@InProceedings{Avritzer2020,
  author    = {Avritzer, Alberto},
  booktitle = {2020 IEEE International Conference on Software Architecture Companion (ICSA-C)},
  title     = {Challenges and Approaches for the Assessment of Micro-Service Architecture Deployment Alternatives in DevOps : A tutorial presented at ICSA 2020},
  year      = {2020},
  month     = {March},
  pages     = {1-2},
  abstract  = {The goal of this tutorial is to provide an overview of challenges and approaches for architecture/dependability assessment in the context of DevOps and microservices. Specifically, we present approaches that employ operational data obtained from production-level application performance management (APM) tools, giving access to operational workload profiles, architectural information, failure models, and security intrusions. We use this data to automatically create and conFigure architecture assessments based on models, load tests, and resilience benchmarks. The focus of this tutorial is on approaches that employ production usage, because these approaches provide more accurate recommendations for microservice architecture dependability assessment than approaches that do not consider production usage. We present an overview of (1) the state-of-the-art approaches for obtaining operational data from production systems using APM tools, (2) the challenges of dependability for DevOps and microservices, (3) selected approaches based on operational data to assess dependability. The architecture assessment focus of this tutorial is on scalability, resilience, survivability, and security. Particularly, we present a demo of the automated approach for the evaluation of a domain-based scalability and security metric assessment that is based on the microservice architecture ability to satisfy the performance requirement under load and/or intrusions. We illustrate the approach by presenting experimental results using a benchmark microservice architecture.},
  doi       = {10.1109/ICSA-C50368.2020.00007},
  keywords  = {Tutorials;Computer architecture;Scalability;Security;Tools;Data models;Production;micro-service architectures;operational profile;security intrusions},
}

@InProceedings{Parimala2016,
  author    = {Parimala, N. and Kohar, Rachna},
  booktitle = {2016 Eleventh International Conference on Digital Information Management (ICDIM)},
  title     = {A quality metric for BPEL process under evolution},
  year      = {2016},
  month     = {Sep.},
  pages     = {197-202},
  abstract  = {In Service-Oriented Architecture (SOA), behaviour of a business process is specified using Business Process Execution Language (BPEL) which is a XML based language. In today's competitive market, enterprises change their business processes frequently. Changes in BPEL process may affect the quality of BPEL process for the consumer. It is desirable to measure and evaluate the BPEL process quality when changes occur. Metrics are vastly used to provide a quantitative measure for the quality. In this paper, BPEL Process Usefulness Metric under Evolution (BUME) is proposed to measure quality of a BPEL process when it evolves. The applicability of the metric is demonstrated using simulated data for different versions of a BPEL process.},
  doi       = {10.1109/ICDIM.2016.7829777},
  keywords  = {Measurement;Business;Complexity theory;Documentation;Service-oriented architecture;Context;Business Process;BPEL;Change;Metric;Quality;Usefulness},
}

@InProceedings{Honamore2016,
  author    = {Honamore, Suhas and Kumar, Lov and Rath, Santanu Ku.},
  booktitle = {2016 International Conference on Internet of Things and Applications (IOTA)},
  title     = {Analysis of control flow complexity metrics for web service composition},
  year      = {2016},
  month     = {Jan},
  pages     = {389-394},
  abstract  = {In service oriented computing, web services are combined to meet the interoperability demands in different heterogeneous and distributed applications. However, incisively measuring the control flow complexity of Web Service Composition (WSC) is not an easy task due to characteristics of distributed, loose-coupling, and heterogeneity. In Service Oriented Architecture (SOA), Business Process Execution Language (BPEL) is used to describe the combination of web services. This paper mainly focuses on the complexity measurement of web service composition from BPEL. Petri-net is one of the models to represent the work flow. The BPEL of WSC is converted into Petri-net based model and by extracting the information of places, transitions, and their interrelationship; the complexity is measured for that Petri-net model. Two metric sets are considered for analysis of the WSC's complexity, which are identified by studying the workflow's execution dependency relations. The first metric set describes the static features, and second metric set describes about the dynamic complexity of business process.},
  doi       = {10.1109/IOTA.2016.7562758},
  keywords  = {Complexity theory;Business;Service-oriented architecture;Atmospheric modeling;Weight measurement;Service oriented architecture;BPEL;WSC;Petri-net;Complexity metrics},
}

@InProceedings{Parekh2018,
  author    = {Parekh, Nikunj and Kurunji, Swathi and Beck, Alan},
  booktitle = {2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)},
  title     = {Monitoring Resources of Machine Learning Engine In Microservices Architecture},
  year      = {2018},
  month     = {Nov},
  pages     = {486-492},
  abstract  = {Microservices architecture facilitates building distributed scalable software products, usually deployed in a cloud environment. Monitoring microservices deployed in a Kubernetes orchestrated distributed advanced analytics machine learning engines is at the heart of many cloud resource management solutions. In addition, measuring resource utilization at more granular level such as per query or sub-query basis in an MPP Machine Learning Engine (MLE) is key to resource planning and is also the focus of our work. In this paper we propose two mechanisms to measure resource utilization in Teradata Machine Learning Engine (MLE). First mechanism is the Cluster Resource Monitoring (CRM). CRM is a high-level resource measuring mechanism for IT administrators and analytics users to visualize, plot, generates alerts and perform live and historical-analytics on overall cluster usage statistics. Second mechanism is the Query Resource Monitoring (QRM). QRM enables IT administrators and MLE users to measure compute resource utilization per individual query and its sub-queries. When query takes long time, QRM provides insights. This is useful to identify expensive phases within a query that tax certain resources more and skew the work distribution. We show the results of proposed mechanisms and highlight use-cases.},
  doi       = {10.1109/IEMCON.2018.8614791},
  keywords  = {Monitoring;Resource management;Engines;Containers;Machine learning;Maximum likelihood estimation;Customer relationship management;Big Data;Data Analytics;Machine Learning;Docker;Linux Containers (LXC);Massively Parallel Processing (MPP);MapReduce;Kubernetes;Monitoring;and Workload Skew},
}

@InProceedings{Abid2015,
  author    = {Abid, Kashif Sohail and Abid, Asif Sohail and Ansari, M. Mohsen},
  booktitle = {2015 IEEE International Conference on Multimedia Big Data},
  title     = {A Better Approach for Conceptual Readability of WSDL},
  year      = {2015},
  month     = {April},
  pages     = {260-263},
  abstract  = {Issues that concerns with the inter-operability on a heterogeneous environment can easily be address using the flexible platform of Service Oriented Architecture (SOA). Web service is an implementation and modeling of Service Oriented Architecture (SOA). Web service description language (WSDL)is a standard describing a web service in XML form. This Description can be categorized in two parts i.e. Structural and non-structural. The readability of a web service helps the consumer to understand it easily, it is suggested to provide sufficient details about functionality scope and limitation of scope in WSDL, so that it can easily be understandable. Readability depends upon interaction of two variables i.e. Text and reader. The maximum details about a web service could lead to it's reproduction by business competitor, and it may helps in maximizing vulnerabilities in it. This paper focuses on a technique for computing readability index by a detail analysis of WSDL document. This readability index obtain using this approach helps the producer of a web service to adjust readability, so that it can easily be understandable by consumer. The better readability index can also leads the provider to a better service discovery. To calculate Readability Index, extraction of WSDL file components was performed. After extraction of key concepts, they were mapped with the Domain Ontology. The words that were not mapped in the ontology, synonyms are employed by consulting the Word Net. Final readability was obtained using Simplified Dale Chall readability index (DaCw). The Web Service Readability can be measure more precisely by considering words that were not found in the mapping process.},
  doi       = {10.1109/BigMM.2015.52},
  keywords  = {Indexes;Ontologies;Mathematical model;Service-oriented architecture;Web sites;XML;Web Services;Readability;WSDL},
}

@InProceedings{FernandesMiotodeOliveiradosSantos2019,
  author    = {Fernandes Mioto de Oliveira dos Santos, Eduardo and Lima Werner, Claudia Maria},
  booktitle = {2019 International Conference on Information Systems and Software Technologies (ICI2ST)},
  title     = {A Survey on Microservices Criticality Attributes on Established Architectures},
  year      = {2019},
  month     = {Nov},
  pages     = {149-155},
  abstract  = {The microservice oriented software architecture considers the delegation of responsibilities by separate components, thus creating a set of interconnected but independent services. Information about the most critical microservices is relevant to software architects and other decision-makers, thus guiding the maintenance and evolution of architecture in a more assertive and guided way. This paper aims to observe the need for a method to measure criticality in a microservice oriented architecture, motivated by this purpose, during August 2019, a survey with twenty experienced participants from the industry and academia was conducted, where the lack of a grounded method to measure the criticality on established architectures was observed.},
  doi       = {10.1109/ICI2ST.2019.00028},
  keywords  = {Computer architecture;ISO Standards;Atmospheric measurements;Particle measurements;Service-oriented architecture;Computational modeling;Microservices;Criticality;Attributes;Established Architectures;Survey},
}

@InProceedings{Zhang2014a,
  author    = {Zhang, Lili and Yu, Shusong and Ding, Xiangqian and Wang, Xiaodong},
  booktitle = {2014 Sixth International Conference on Intelligent Human-Machine Systems and Cybernetics},
  title     = {Research on IOT RESTful Web Service Asynchronous Composition Based on BPEL},
  year      = {2014},
  month     = {Aug},
  pages     = {62-65},
  volume    = {1},
  abstract  = {In recent years, The Internet of Things(IOT) is one of the hottest research topics. It was originally defined as connected all the things through the sensing devices to the Internet. In addition, Service-Oriented methodology has gradually drawn people's attention. Therefore, integrated The IOT with Service-Oriented methodology is very important. But now IOT service composition is mostly synchronous and service model is more complex. RESTful web services have been widely recognized and used because of their lightweight and succinct. RESTful web services introduce a new kind of abstraction, the resource, so that they are hard to compose using the Business Process Execution Language (BPEL). In order to compose asynchronous RESTful web services and make use of various IOT services, this paper proposes an asynchronous RESTful web service recursive measure, which is based on the BPEL extention. First, design the architecture of IOT RESTful web services, the architecture is divided into six layers so that it can integrate The IOT and RESTful web services effectively. Second, we show how to invoke the RESTful web services from the IOT and publish a BPEL process as a RESTful web service by extending BPEL. Finally, through an experiment to verify the correctness and validity of the proposed method in this paper.},
  doi       = {10.1109/IHMSC.2014.23},
  keywords  = {Service-oriented architecture;Computer architecture;Drugs;Servers;Security;Educational institutions;Internet of Things;RESTful Web Service;BPEL;Service Asynchronous Composition;IOT Architecture},
}

@InProceedings{Kumar2019,
  author    = {Kumar, T Sathis and Latha, K},
  booktitle = {2019 International Conference on Smart Systems and Inventive Technology (ICSSIT)},
  title     = {Interoperability Performance in Adaptive Middleware for Enterprise Business Applications},
  year      = {2019},
  month     = {Nov},
  pages     = {652-656},
  abstract  = {To improve the presentation of B2B (Business to Business) and B2C (Business to Consumer) regarding venture wide Service Oriented Architecture (SOA), we need middleware interoperability particularly with agent building to be specific CORBA (Common Object Request Broker Architecture) proposed by Object Management Group ORB programming named ORBeline. Unmistakable models for client server correspondences have just been created and executed specifically Handle Driven ORB (H-ORB), Forwarding ORB (F-ORB), and the Adaptive ORB (A- ORB). This paper concentrates how to improve the presentation of the interoperability in Adaptive ORB (A-ORB) as for client server collaboration in N-level engineering alongside multithreading condition. We have presented a strategy called linear discriminant interoperable support learning method and how it will in general be used for improving the presentation of interoperability is examined. The outcome gives the framework conduct especially the impact of message measure, between hub deferrals; torpidity and flexibility of solicitation/reaction administration times for the A-ORB engineering are broke down.},
  doi       = {10.1109/ICSSIT46314.2019.8987957},
  keywords  = {Interoperability;Multithreading;Threshold;Object Request Broker},
}

@Article{Chen2021,
  author   = {Chen, Jeng-Chung and Chen, Chun-Chih and Shen, Chih-Hsiung and Chen, Ho-Wen},
  journal  = {IEEE Internet of Things Journal},
  title    = {User Integration in Two IoT Sustainable Services by Evaluation Grid Method},
  year     = {2021},
  issn     = {2327-4662},
  pages    = {1-1},
  abstract = {To meet the need for sustainable development, Taiwan has been spreading a network of micro-monitoring stations to measure the environmental quality in rivers and air to protect people from environmental pollution. As a result, more and more information technology companies develop Internet of Things (IoT) services for this job. However, most IoT services are screened out of the market because lacking design thinking. Therefore, including users’ desires in IoT products and services is a critical determinant for their survival in this permanently changing market. Thus, this study proposed a systematic framework to identify the users’ desires from different stakeholders to determine technological development. For this, we use the Evaluation Grid Method (EGM) to explore the users’ desires by a series of in-depth interviews and visualize the user’s response as a hierarchical evaluation map of attraction. After that, an IoT prototype is built and used to capture the insightful feedback of respondents. Meanwhile, we adopt the minimum viable product (MVP) design principles to develop two prototypes that manage a wastewater treatment plant and household environment. Overall, this study proposes an applicable user integration procedure to help IT engineers develop the IoT for sustainable service. This study also confirms that the MVP method can help to accelerate user integration. We propose a service-oriented IoT architecture in technology development and develop a decision-making service of human dispatch in operating environmental facilities and a context-awareness service for environmental control.},
  doi      = {10.1109/JIOT.2021.3091688},
  keywords = {Internet of Things;Ventilation;Water pollution;Wastewater treatment;Interviews;Companies;Air pollution;User integration;sustainable development;IoT;minimum viable product (MVP);evaluation grid method (EGM).},
}

@InProceedings{Chaudhari2016,
  author    = {Chaudhari, Nikhil and Bhadoria, Robin Singh and Prasad, Siddharth},
  booktitle = {2016 8th International Conference on Computational Intelligence and Communication Networks (CICN)},
  title     = {Information Handling and Processing Using Enterprise Service Bus in Service-Oriented Architecture System},
  year      = {2016},
  month     = {Dec},
  pages     = {418-421},
  abstract  = {Information is key factor in delivering service across networks. Messaging is important aspect in handing information using Enterprise Service Bus (ESB) in Service Oriented Architecture (SOA). Such information is generally passes and used as interaction parameters upon communication between two parties that could be carried out amongst multiple services. Integration between multiple application services could be strengthened by adopting this methodology which is important to handle web services over networks. ESB is messaging middleware framework that helps in designing and developing web services through which software intermediary could be possible. It is a kind of depletion layer that efficiently handles various overheads during communication and interaction between multiple application services. This paper details about issues related to application services with message handling and control. Testing and simulation has been carried out on - AdroitLogic UltraESB, WSO2 ESB and Red Hat JBoss Fuse ESBs. Several parameters like total and average message counts, overall bytes measure, overall message received and sent, processing time of messages, and memory allocation.},
  doi       = {10.1109/CICN.2016.88},
  issn      = {2472-7555},
  keywords  = {Service-oriented architecture;Time factors;Message systems;Business;Fuses;Concurrent computing;Service-Oriented Architecture (SOA);Enterprise Service Bus (ESB);Message Handling;Middleware},
}

@Article{Wang2020,
  author   = {Wang, Chen and Ma, Hui and Chen, Gang and Hartmann, Sven and Branke, Jürgen},
  journal  = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  title    = {Robustness Estimation and Optimisation for Semantic Web Service Composition With Stochastic Service Failures},
  year     = {2020},
  issn     = {2471-285X},
  pages    = {1-16},
  abstract = {Service-oriented architecture (SOA) is a widely adopted software engineering paradigm that encourages modular and reusable applications. One popular application of SOA is web service composition, which aims to loosely couple web services to accommodate complex goals not achievable through any individual web service. Many approaches have been proposed to construct composite services with optimized Quality of Service (QoS), assuming that QoS of web services never changes. However, the constructed composite services may not perform well and may not be executable later due to its component services' failure. Therefore, it is important to build composite services that are robust to stochastic service failures. Two challenges of building robust composite services are to efficiently generate service composition with near-optimal quality in a large search space of available services and to accurately measure the robustness of composite services considering all possible failure scenarios. This article proposes a novel two-stage GA-based approach to robust web service composition with an adaptive evolutionary control and an efficient robustness measurement. This approach can generate robust composite service at the design phase, which can cope with stochastic service failures and maintain high quality at the time of execution. We have conducted experiments with benchmark datasets to evaluate the performance of our proposed approach. Our experiments show that our method can produce highly robust composite services, achieving outstanding performance consistently in the event of stochastic service failures, on service repositories with varying sizes.},
  doi      = {10.1109/TETCI.2020.3027870},
  keywords = {Quality of service;Robustness;Web services;Estimation;Optimization;Approximation methods;Genetic algorithms;Combinatorial optimisation;genetic algorithm;robust optimisation;web service composition},
}

@InProceedings{Sun2021,
  author    = {Sun, Yu and Mao, Shaojie and Huang, Songhua and Mao, Xiaobin},
  booktitle = {2021 2nd Information Communication Technologies Conference (ICTC)},
  title     = {Load Balancing Method for Service Scheduling of Command Information System},
  year      = {2021},
  month     = {May},
  pages     = {297-301},
  abstract  = {In order to satisfy the capability generation requirement of command information system, a load balancing method for service scheduling is studied. Considering that a work which will be finished by command information system based on service-oriented architecture is composed of several jobs, the paper analyzes the work completion process in detail. Then, a method to measure the load on a service which is scheduled to participate in a work is designed. On that basis, a load balancing mathematical model for service scheduling is established and a model solving algorithm that is based on greedy strategy and has polynomial time is put forward. Simulated experimental results show that the method proposed in this paper can allocate load to the services of command information system in a balanced way.},
  doi       = {10.1109/ICTC51749.2021.9441601},
  keywords  = {Atmospheric measurements;Load management;Particle measurements;Scheduling;Communications technology;Service-oriented architecture;Mathematical model;command information system;service oriented architecture;service scheduling;load balancing;greedy strategy},
}

@InProceedings{Langermeier2018,
  author    = {Langermeier, Melanie and Bauer, Bernhard},
  booktitle = {2018 IEEE 22nd International Enterprise Distributed Object Computing Workshop (EDOCW)},
  title     = {A Model-Based Method for the Evaluation of Project Proposal Compliance within EA Planning},
  year      = {2018},
  month     = {Oct},
  pages     = {97-106},
  abstract  = {The business model and IT infrastructure of organizations is continually changing. Trends like microservices and digital transformation demand an adaption of the business models and IT infrastructure in order to stay competitive. It is important to ensure the compliance of these new projects with the current goals and principles. The discipline of Enterprise Architecture Planning provides methods for the structured development of the business and IT of an organization. In this paper we propose a tool-supported method for EA planning to evaluate to the project compliance based on established models. Different analyses are used to support the architect during project planning. Gap and impact analysis are used to ensure the change consistency. The compliance with the current strategy is finally evaluated with view generation and metric calculation. Foundation of the method is a generic generic analysis architecture execution environment (A2E), that provides us with the required flexibility to adapt to different needs and meta models. The method and the proposed analyses are evaluated within a case study from a medium-sized software product company.},
  doi       = {10.1109/EDOCW.2018.00024},
  issn      = {2325-6605},
  keywords  = {Planning;Computer architecture;Proposals;Adaptation models;Organizations;Analytical models;Architecture Analysis, Architecture Evaluation, Enterprise Architecture, Enterprise Architecture Planning},
}

@InProceedings{OULMAHDI2018,
  author    = {OULMAHDI, Mohamed and CHASSOT, Christophe and VAN WAMBEKE, Nicolas},
  booktitle = {2018 International Conference on Smart Communications in Network Technologies (SaCoNeT)},
  title     = {Extensible and Adaptive Architecture for an Evolutive Transport Layer},
  year      = {2018},
  month     = {Oct},
  pages     = {102-107},
  abstract  = {The world of communications and networking knows and important evolution over the years. While this evolution is concretized by a deployment of many modern protocols at most of protocol layers, the Transport one continues to use old TCP and UDP protocols. This despite that an important number of modern protocols and mechanisms have been proposed. In this context, we study in this paper the obstacle of the deployment of new transport protocols and propose a new architecture to support the deployment and the adaptation of new Transport solutions. This was achieved by adding extensibility and adaptability capabilities using service-oriented and component-based paradigms. The architecture performances are studied at the end to measure the impact and the benefits of the new architecture comparing to classical Transport protocol.},
  doi       = {10.1109/SaCoNeT.2018.8585730},
  keywords  = {Transport protocols;Internet;Reliability;Aerodynamics;Semantics;Telecommunications;Transport layer;service-oriented;component-based;TCP;Aeronautical Telecommunications Network (ATN).},
}

@InProceedings{Pulparambil2016,
  author    = {Pulparambil, Supriya and Baghdadi, Youcef},
  booktitle = {2016 IEEE Students' Conference on Electrical, Electronics and Computer Science (SCEECS)},
  title     = {SOA maturity model a frame of reference},
  year      = {2016},
  month     = {March},
  pages     = {1-6},
  abstract  = {Service Oriented Architecture (SOA) is an architectural style that supports service orientation. In reality, SOA is much more than architecture. SOA adoption is prerequisite for organization to excel their service deliveries, as the delivery platforms are shifting to mobile, cloud and social media. A maturity model is a tool to accelerate enterprise SOA adoption, however it depends on how it should be applied. This paper presents a literature review of existing maturity models and proposes 5 major aspects that a maturity model has to address to improve SOA practices of an enterprise. A maturity model can be used as: (i) a roadmap for SOA adoption, (ii) a reference guide for SOA adoption, (iii) a tool to gauge maturity of process execution, (iv) a tool to measure the effectiveness of SOA motivations, and (v) a review tool for governance framework. This paper also sheds light on how SOA maturity assessment can be modeled. A model for SOA process execution maturity and perspective maturity assessment has been proposed along with a framework to include SOA scope of adoption.},
  doi       = {10.1109/SCEECS.2016.7509323},
  keywords  = {Service-oriented architecture;Semiconductor optical amplifiers;Capability maturity model;Organizations;Standards organizations;Industries;SOA maturity model;assessment;execution maturity;perspective maturity;scope;framework},
}

@InProceedings{Camilli2020,
  author    = {Camilli, Matteo and Colarusso, Carmine and Russo, Barbara and Zimeo, Eugenio},
  booktitle = {2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  title     = {Domain Metric Driven Decomposition of Data-Intensive Applications},
  year      = {2020},
  month     = {Oct},
  pages     = {189-196},
  abstract  = {The microservices architectural style is picking up more and more momentum in IT industry for the development of systems as loosely coupled, collaborating services. Companies that undergo the migration of their own applications have aspirations such as increasing maintainability and the scale of operation. Such a process is worthwhile but not easy, since it should ensure atomic improvements to the overall architecture for each migration step. Furthermore, the systematic evaluation of migration steps becomes cumbersome without sensible optimization metrics that take into account performance and scalability under expected operational conditions. Recent lines of research recognize this task as challenging, especially in data-intensive applications where known approaches based, for instance, on Domain Driven Design may not be adequate. In this paper, we introduce an approach to evaluate a migration in an iterative way and recognize whether it represents an improvement in terms of performance and scalability. The approach leverages a Domain Metric-based analysis to quantitatively evaluate alternative architectures. We exemplified the envisioned approach on a data-intensive application case study in the domain of smart mobility. Preliminary results from our controlled experiments show the effectiveness of our approach to support systematic and automated evaluation of migration processes.},
  doi       = {10.1109/ISSREW51248.2020.00071},
  keywords  = {Scalability;Testing;Measurement;Computer architecture;Roads;Business;Servers;Microservices;Decomposition;Performance Analysis;Scalability Analysis;Domain Metric},
}

@InProceedings{Delgado2018,
  author    = {Delgado, Andrea},
  booktitle = {2018 XLIV Latin American Computer Conference (CLEI)},
  title     = {Monitoring and Analyzing Service Execution from Business Processes: An AXIS Extension},
  year      = {2018},
  month     = {Oct},
  pages     = {582-589},
  abstract  = {Implementing Business Processes (BPs) with services (and microservices) is nowadays the main way to support the execution of automated activities in processes, both within the organization itself, and externally interacting with customers, suppliers and other participants. In order to do so, it is important not only to model and implement services but also to define Quality of Service (QoS) characteristics for services, to monitor and evaluate their execution. Although there are many proposals for services monitoring and evaluation from the services point of view, there are not many from the BPs perspective. In this paper we present a reference architecture for service monitoring tools, along with a prototype implementation as an extension of the web services execution environment AXIS2. We show that existing service measures and new ones can be defined into the monitor to collect execution data and relate this data with BPs execution, to measure BPs and service execution in an integrated manner.},
  doi       = {10.1109/CLEI.2018.00075},
  keywords  = {Monitoring;Computer architecture;Tools;Quality of service;Service-oriented architecture;Time measurement;Business processes;measuring business processes and services;Quality of Service (QoS);service monitoring},
}

@InProceedings{NikDaud2014,
  author    = {Nik Daud, Nik Marsyahariani and Wan Kadir, Wan M. N.},
  booktitle = {2014 8th. Malaysian Software Engineering Conference (MySEC)},
  title     = {Static and dynamic classifications for SOA structural attributes metrics},
  year      = {2014},
  month     = {Sep.},
  pages     = {130-135},
  abstract  = {Evaluating qualities of software based on software structural attributes such as coupling and cohesion are frequently done in practice as these attributes directly have impacts on value of higher level quality. Concerning oneself with structural attributes values early on helps developers to predict quality attributes level in the software. Service-Oriented Architecture (SOA) is an architectural concept where services are used as building blocks in developing new software. Lots of structural attributes metrics related to SOA had been proposed these recent years, which triggered an investigation to classify these metrics based on specific criteria. In this paper, we introduce classifications for SOA based structural attributes metrics, where the metrics are restricted to coupling, cohesion and complexity metrics. These metrics are classified based on software static and dynamic aspects with some brief introduction for each metric. By classifying these SOA based structural attributes metrics, it will allow user to avoid redundancy in proposing similar metrics thus increases the reusability of existing metrics.},
  doi       = {10.1109/MySec.2014.6986002},
  keywords  = {Couplings;Service-oriented architecture;Complexity theory;Software measurement;Semiconductor optical amplifiers;Structural attributes metric;Service Oriented Architecture;metrics classification},
}

@InProceedings{Tummalapalli2020a,
  author    = {Tummalapalli, Sahithi and Kumar, Lov and Neti, Lalita Bhanu Murthy and Krishna, Aneesh},
  booktitle = {2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
  title     = {An Empirical Analysis on the Role of WSDL Metrics in Web Service Anti-Pattern Prediction},
  year      = {2020},
  month     = {Dec},
  pages     = {559-564},
  abstract  = {Service-Oriented Architecture (SOA) is one of the most well-known models for designing web systems. SOA system evolution and maintenance is challenging because of its distributive nature and secondly due to the demand of designing high-quality, stable interfaces. This evolution leads to a problem called Anti-patterns in web services. It is observed that these anti-patterns negatively impact the evolution and maintenance of software systems, making the early detection and correction of them a primary concern for the software developers. The primary motivation of this work is to investigate the relationship between the Web Service Description Language(WSDL) metrics and anti-patterns in web services. This research aims to develop an automatic method for the detection of web service anti-patterns. The core idea of the methodology defined is to identify the most crucial WSDL metrics with the association of various feature selection techniques for the prediction of anti-patterns. Experimental results show that the model developed by using all the WSDL quantity metrics(AM) shows a bit high performance compared to the models developed with the other metric sets. Experimental results also showed that the performance of the models generated using Decision Tree(DT) and Major Voting Ensemble(MVE) is high compared to the models generated using other classifier techniques.},
  doi       = {10.1109/HPCC-SmartCity-DSS50907.2020.00070},
  keywords  = {Measurement;Computational modeling;High performance computing;Conferences;Maintenance engineering;Software systems;Feature extraction;Anti-patterns;Web Services;WSDL metrics;Neural Networks;Feature Selection;Data Sampling;Machine Learning},
}

@InProceedings{Gomes2016,
  author    = {Gomes, Luiza Barcelos Gualberto and Farias, Pedro Porfírio Muniz and Bessa Albuquerque, Adriano and Herden, Adriana},
  booktitle = {2016 11th Iberian Conference on Information Systems and Technologies (CISTI)},
  title     = {Software measure based on BPMN activity points},
  year      = {2016},
  month     = {June},
  pages     = {1-6},
  abstract  = {BPMN usage has been extended beyond the workflow systems and service-oriented architecture. Development methodologies have been proposed using BPMN to describe use cases, to specify the activities flow that forms each scenario of general purpose systems and business process execution with web services. This article proposes a metric to estimate software size, called BPMN Activity Points based on activities counting from three different perspectives, in which these scores are increasingly detailed and refined.},
  doi       = {10.1109/CISTI.2016.7521492},
  keywords  = {Unified modeling language;Logic gates;Business;Software measurement;Service-oriented architecture;Context;BPMN;metric to estimate;software size},
}

@InProceedings{Li2016,
  author    = {Li, Zhinan and Yang, Xiaodong},
  booktitle = {2016 IEEE International Conference on Web Services (ICWS)},
  title     = {A Reliability-Oriented Web Service Discovery Scheme with Cross-Layer Design in MANET},
  year      = {2016},
  month     = {June},
  pages     = {404-411},
  abstract  = {Web service technologies are playing an increasingly important role in service-oriented architecture design and application convergence over Mobile ad hoc networks (MANET). Due to the decentralized administration and dynamic wireless connectivity problems, accomplishing reliable service discovery in MANET faces a large number of challenges. In order to relieve the communication inefficiency among service providers and clients caused mainly by the unpredictable node mobility, this paper proposes a cross-layer service discovery scheme which enables improved network efficiency and reduced resource consumption. Firstly a network-layer based underlay framework is presented. It specifically establishes a reliability-oriented source routing mechanism which is equipped with a novel reliability-maximized path selection metric and a backup path support fast route recovery strategy. The cross-layer design is prudentially realized by piggybacking service discovery procedures on the reliability enhanced underlay routing mechanism. Simulation analysis verifies that the proposed scheme improves service discovery reliability by achieving low rediscovery frequency, and guarantees high network efficiency by providing reduced service discovery delay and control overhead.},
  doi       = {10.1109/ICWS.2016.59},
  keywords  = {Reliability;Routing;Correlation;Routing protocols;Mobile ad hoc networks;Nickel;Web services;path reliability;web service;service discovery;backup path;cross-layer},
}

@InProceedings{Nuraini2014,
  author    = {Nuraini, Aminah and Widyani, Yani},
  booktitle = {2014 International Conference on Data and Software Engineering (ICODSE)},
  title     = {Software with service oriented architecture quality assessment},
  year      = {2014},
  month     = {Nov},
  pages     = {1-6},
  abstract  = {Service Oriented Architecture (SOA) is becoming popular since its flexibility fulfill the need of rapidly changing enterprise requirement. Therefore, expectation of a good quality software with SOA is getting higher. To address this need, this paper presents a guideline to conduct quality assessment using an existing tool. The quality assessment model is designed by selecting the relevant quality factors, choosing an appropriate quality to metric mapping method, identifying the relevant metrics, and mapping each quality factor to the metrics. Using the model, the quality assessment process is prepared by identifying data and selecting the appropriate tools. The chosen tool may require some modification. The proposed quality assessment guideline can help the software quality assurance team to assess quality of their software with SOA. The proposed guideline has been used to assess the quality of an existing sofware with SOA (Bonita BPM). The result is considered as promising, although several improvement are still needed.},
  doi       = {10.1109/ICODSE.2014.7062707},
  keywords  = {Measurement;Quality assessment;Service-oriented architecture;Q-factor;Guidelines;Semiconductor optical amplifiers;SOA;software quality assessment;quality factor;quality metrics},
}

@InProceedings{Wijayanto2014,
  author    = {Wijayanto, Arie Wahyu and Suhardi},
  booktitle = {2014 International Conference on ICT For Smart Society (ICISS)},
  title     = {Service oriented architecture design using SOMA for optimizing public satisfaction in government agency: Case study: BPN - National Land Authority of Indonesia},
  year      = {2014},
  month     = {Sep.},
  pages     = {49-55},
  abstract  = {Service oriented architecture (SOA) enables organizations to easily integrate systems, data, and business processes. Implementation of SOA solution in private sector is widely used and successfully proven to increase their profit. But there are different challenge in public sector which is not profit oriented and has different business model. In public sector, user satisfaction on government agencies is one of common indicator to measure quality of public service. This paper presents SOA solution for public sector using SOMA to conduct a service integration for optimizing public satisfaction. We also combined SWOT and Porter's Value Chain to support business modelling analysis. The result shows that there is a simplicity and feasibility for users to access the service after SOA integration, which improves user satisfaction.},
  doi       = {10.1109/ICTSS.2014.7013150},
  keywords  = {Service-oriented architecture;Unified modeling language;Government;Analytical models;Computer architecture;Service Engineering;SOMA;Service Design;Service Oriented Architecture;Public Sector},
}

@InProceedings{Mcheick2014,
  author    = {Mcheick, Hamid and Mohammad, Atif Farid},
  booktitle = {2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE)},
  title     = {The evident use of evidence theory in big data analytics using cloud computing},
  year      = {2014},
  month     = {May},
  pages     = {1-6},
  abstract  = {We live in the world of evidence. This research survey comprises of several research works and has an example implying dempster-shafer theory of evidence. We have witnessed several advances in computational performance, which have brought us the design and development of high-performance computing simulation tools. It is a fact that we have to account for uncertainty, while generating such high-performance systems using such simulation tools can fail in service performance predictions. We have seen that evidence theory is utilized to measure uncertainty in terms of the uncertain measures of belief and plausibility. It is also witnessed in computing community that Cloud computing has provided a flexible and scalable infrastructures to grow beyond contemporary borders to the organizations as wells the users everyday use of services. It also has increased availability of high-performance computing applications to small/ medium-sized businesses as well as academic users to work with. This paper also sheds light on Cloud computing and Service-Oriented Architecture.},
  doi       = {10.1109/CCECE.2014.6901158},
  issn      = {0840-7789},
  keywords  = {Abstracts;Forgery;Computational modeling;Data models;Personnel;Cloud Computing;Evidence Theory;High-Performance Computing;Simulation},
}

@InProceedings{Marmsoler2018,
  author    = {Marmsoler, Diego},
  booktitle = {2018 International Symposium on Theoretical Aspects of Software Engineering (TASE)},
  title     = {On Syntactic and Semantic Dependencies in Service-Oriented Architectures},
  year      = {2018},
  month     = {Aug},
  pages     = {132-137},
  abstract  = {In service oriented architectures, components provide services on their output ports and consume services from other components on their input ports. Thereby, a component is said to depend on another component if the former consumes a service provided by the latter. This notion of dependency (which we call syntactic dependency) is used by many architecture analysis tools as a measure for system maintainability. With this paper, we introduce a weaker notion of dependency, still sufficient, however, to guarantee semantic independence between components. Thereby, we discover the concepts of weak and strong semantic dependency and prove that strong semantic dependency indeed implies syntactic dependency. Our alternative notion of dependency paves the way to more precise dependency analysis tools. Moreover, our results about the different types of dependencies can be used for the verification of semantic independence.},
  doi       = {10.1109/TASE.2018.00025},
  keywords  = {Syntactic Dependency;Semantic Dependency;Service Oriented Architectures},
}

@Article{Sun2019,
  author   = {Sun, Chang-Ai and Dai, Hepeng and Wang, Guan and Towey, Dave and Chen, Tsong Yueh and Cai, Kai-Yuan},
  journal  = {IEEE Transactions on Services Computing},
  title    = {Dynamic Random Testing of Web Services: A Methodology and Evaluation},
  year     = {2019},
  issn     = {1939-1374},
  pages    = {1-1},
  abstract = {In recent years, Service Oriented Architecture (SOA) has been increasingly adopted to develop distributed applications in the context of the Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of web services. In this paper, we propose a dynamic random testing (DRT) technique for web services, which is an improvement over the widely-practiced random testing (RT) and partition testing (PT). We examine key issues when adapting DRT to the context of SOA, including a framework, guidelines for parameter settings, and a prototype for such an adaptation. Empirical studies are reported where DRT is used to test three real-life web services, and mutation analysis is employed to measure the effectiveness. Our experimental results show that, compared with the three baseline techniques, RT, Adaptive Testing (AT) and Random Partition Testing (RPT), DRT demonstrates higher fault-detection effectiveness with a lower test case selection overhead. Furthermore, the theoretical guidelines of parameter setting for DRT are confirmed to be effective. The proposed DRT and the prototype provide an effective and efficient approach for testing web services.},
  doi      = {10.1109/TSC.2019.2960496},
  keywords = {Testing;Service-oriented architecture;Guidelines;Reliability;Prototypes;Software Testing;Random Testing;Dynamic Random Testing;Web Service;Service Oriented Architecture},
}

@InProceedings{Johnsen2018,
  author    = {Johnsen, Frank T. and Landmark, Lars and Hauge, Mariann and Larsen, Erlend and Kure, Øivind},
  booktitle = {MILCOM 2018 - 2018 IEEE Military Communications Conference (MILCOM)},
  title     = {Publish/Subscribe Versus a Content-Based Approach for Information Dissemination},
  year      = {2018},
  month     = {Oct},
  pages     = {1-9},
  abstract  = {NATO has identified the WS-Notification standard from OASIS to support event-driven communication in the NATO enterprise and when building coalition networks. Using this standard promotes interoperability. However, there is significant overhead associated with WS-Notification since it is built on SOAP Web services (WS). Overhead can be problematic in networks with scarce resources. In this paper we perform a small-scale comparative evaluation of overhead of WS-Notification with another publish/subscribe standard: Message Queuing Telemetry Transport (MQTT). We also measure how these standards compare to the novel approach of content-based networking under the same networking conditions. We use the Named Data Networking (NDN) flavor of content-based networking for our experiment. Though fundamentally different, these approaches can be used to realize the Service-Oriented Architecture (SOA) paradigm. The drawback of standard publish/subscribe approaches is that they usually rely on a broker, which constitutes a single point of failure. NDN, on the other hand, has no broker which makes it interesting to consider for tactical networks. We use NATO Friendly Force Information (NFFI), which is much used for friendly force tracking, as the data format for the payload in all our tests. In the paper we focus on the respective approaches' network resource consumption. Based on the results we argue that the content-based approach seems promising and should be investigated further.},
  doi       = {10.1109/MILCOM.2018.8599786},
  issn      = {2155-7586},
  keywords  = {Standards;Service-oriented architecture;Protocols;IP networks;Buildings;Force;Simple object access protocol},
}

@Article{Akbar2018,
  author   = {Akbar, Adnan and Kousiouris, George and Pervaiz, Haris and Sancho, Juan and Ta-Shma, Paula and Carrez, Francois and Moessner, Klaus},
  journal  = {IEEE Access},
  title    = {Real-Time Probabilistic Data Fusion for Large-Scale IoT Applications},
  year     = {2018},
  issn     = {2169-3536},
  pages    = {10015-10027},
  volume   = {6},
  abstract = {Internet of Things (IoT) data analytics is underpinning numerous applications, however, the task is still challenging predominantly due to heterogeneous IoT data streams, unreliable networks, and ever increasing size of the data. In this context, we propose a two-layer architecture for analyzing IoT data. The first layer provides a generic interface using a service oriented gateway to ingest data from multiple interfaces and IoT systems, store it in a scalable manner and analyze it in real-time to extract high-level events; whereas second layer is responsible for probabilistic fusion of these high-level events. In the second layer, we extend state-of-the-art event processing using Bayesian networks in order to take uncertainty into account while detecting complex events. We implement our proposed solution using open source components optimized for large-scale applications. We demonstrate our solution on real-world use-case in the domain of intelligent transportation system where we analyzed traffic, weather, and social media data streams from Madrid city in order to predict probability of congestion in real-time. The performance of the system is evaluated qualitatively using a web-interface where traffic administrators can provide the feedback about the quality of predictions and quantitatively using F-measure with an accuracy of over 80%.},
  doi      = {10.1109/ACCESS.2018.2804623},
  keywords = {Real-time systems;Probabilistic logic;Uncertainty;Data mining;Meteorology;Bayes methods;Data analysis;Complex event processing;data analysis;internet of things;real-time systems;intelligent transportation systems},
}

@InProceedings{Harrer2015,
  author    = {Harrer, Simon and Geiger, Matthias and Preißinger, Christian R. and Bimamisa, David and Schuberth, Stephan J.A. and Wirtz, Guido},
  booktitle = {2015 IEEE Symposium on Service-Oriented System Engineering},
  title     = {Improving the Static Analysis Conformance of BPEL Engines with BPELlint},
  year      = {2015},
  month     = {March},
  pages     = {31-39},
  abstract  = {Today, process-aware systems are ubiquitous. They are built by leveraging process languages for both business and implementation perspectives. In the typical context of a Web Services-based Service-oriented Architecture, the obvious choice to implement service orchestrations is still the Business Process Execution Language (BPEL). For BPEL, a variety of open source and commercial engines have emerged. Although the BPEL standard document defines a set of static analysis rules which should be checked by engines prior to deployment to be standard conformant, previous work revealed that most engines are not capable of revealing all violations of these constraints, resulting in costly runtime errors later on. In this paper, we aim to improve the static analysis conformance of BPEL engines. We implement the tool BPELlint that validates 71 static analysis rules of the BPEL specification, show that the tool can be easily integrated into the deployment process of existing engines, and evaluate its performance to measure the effect on the time to deploy. The results demonstrate that BPELlint can improve the static analysis conformance of BPEL engines with an acceptable performance overhead.},
  doi       = {10.1109/SOSE.2015.21},
  keywords  = {Engines;XML;Load modeling;Standards;Semantics;Business;Analytical models;Static analysis;Standard conformance;BPEL},
}

@InProceedings{Tripathi2018,
  author    = {Tripathi, Manish K and Chaubisa, Divyanshu and Kumar, Lov and Murthy Neti, Lalita Bhanu},
  booktitle = {2018 15th IEEE India Council International Conference (INDICON)},
  title     = {Prediction of Quality of Service Parameters Using Aggregate Software Metrics and Machine Learning Techniques},
  year      = {2018},
  month     = {Dec},
  pages     = {1-6},
  abstract  = {In todays Service-Oriented Architecture (SOA) world, software systems are built by composing web services offered by Service Providers (SPs). There are different SPs offering services for the same set of functional requirements. Service providers are expected to be highly competitive in their offerings to enhance their market. The quality of web services is an important factor that differentiates one service provider from another. Twelve parameters are identified by which quality of service can be measured. The prediction of these twelve QoS parameters help SPs to enhance the quality of their service. Each web service is realized by several programming files. CK and object oriented metrics of the underlying Java files of the web services are important features for predicting QoS parameters of the web service. The aggregated measure, mean, is chosen to be a feature in predicting the QoS parameters in earlier studies. We propose to build prediction models using 16 aggregate measures and show that there is significant difference between these aggregate measures. We find best feature subset using six feature selection techniques and build prediction models using Extreme Learning Machines with different kernels. We show that feature selection techniques might not enhance prediction accuracies and the ensemble algorithm out performs other learning algorithms.},
  doi       = {10.1109/INDICON45594.2018.8986987},
  issn      = {2325-9418},
  keywords  = {Measurement;Quality of service;Web services;Object oriented modeling;Predictive models;Aggregates;Feature extraction;PCA: Principal Component Analysis;ELM: Extreme Learning Machine;RBF: Radial Basis Function;Feature selection;Aggregation Metrics},
}

@InProceedings{Barnwal2019,
  author    = {Barnwal, Anil and Jangade, Rajesh and Pugla, Satyakam},
  booktitle = {2019 9th International Conference on Cloud Computing, Data Science Engineering (Confluence)},
  title     = {Analyzing and Predicting the Allocation and Utilization of Resources in Cloud Computing System},
  year      = {2019},
  month     = {Jan},
  pages     = {56-62},
  abstract  = {The increasing use of cloud computing, constructed on good research in utility computing, networking, virtualization and web services provides some important benefits such as flexibility, cost reduction and easy availability for people using the system. These advantages are expected to increase the demand for more cloud services which further increase the installation of more clouds and its customer base. These demands lead to many technical issues such as applications of internet services, service oriented architecture including high scalability and availability, fault tolerance. So the core issue is to develop techniques for balancing of load effectively. It is clear from the fact that the measure and complexity makes these systems infeasible for assignment of centralized jobs to specific servers. So there is need of productive distributed solutions. In the current paper three proposed load balancing solutions for distributed environment is investigated. They are biased Random Sampling, Honeybee Foraging and Active Clustering.},
  doi       = {10.1109/CONFLUENCE.2019.8776974},
  keywords  = {Cloud computing;Servers;Load management;Workstations;Task analysis;Resource management;Complexity theory;Cloud Computing; Cloud Services;SaaS;PaaS;IaaS;Active Clustering;Random Sampling;Honeybee Foraging},
}

@Article{Zhao2017b,
  author   = {Zhao, Feng and Nian, Guodong and Jin, Hai and Yang, Laurence T. and Zhu, Yajun},
  journal  = {IEEE Systems Journal},
  title    = {A Hybrid eBusiness Software Metrics Framework for Decision Making in Cloud Computing Environment},
  year     = {2017},
  issn     = {1937-9234},
  month    = {June},
  number   = {2},
  pages    = {1049-1059},
  volume   = {11},
  abstract = {Developing high-quality software is essential for eBusiness organizations to cope with drastic market competition. With the development of cloud computing technologies, eBusiness systems and applications pay more attention to open endedness. In a cloud computing environment, eBusiness systems have the ability to provide information technology resources on demand. Traditional software metric methods in distributed systems and applications are technical and project driven, making the market demand and internal practical operation not perfectly balanced within a cloud-computing-based eBusiness corporation. To address this issue, this paper presents a hybrid framework based on the goal/question/metric paradigm to evaluate the quality and efficiency of previous software products, projects, and development organizations in a cloud computing environment. In our approach, to support decision making at the project and organization levels, three angular metrics are used, i.e., project metrics, product metrics, and organization metrics. Furthermore, an improved radial-basis-function-based model is also provided to manage existing projects and design new projects. Experimental results on a well-known eBusiness organization show that the proposed framework is effective, efficient, and operational. Moreover, using the described decision-making algorithm, the predicted data are very close to actual results on the software cost, the fault rate, the development workload, etc., which are greatly helpful in achieving high-quality software.},
  doi      = {10.1109/JSYST.2015.2443049},
  keywords = {Computational modeling;Organizations;Software metrics;Cloud computing;Data models;Cloud computing;decision making;eBusiness;prediction;radial basis function (RBF);software metrics},
}

@Article{AhamedAhanger2020,
  author   = {Ahamed Ahanger, Tariq and Tariq, Usman and Ibrahim, Atef and Ullah, Imdad and Bouteraa, Yassine},
  journal  = {IEEE Access},
  title    = {ANFIS-Inspired Smart Framework for Education Quality Assessment},
  year     = {2020},
  issn     = {2169-3536},
  pages    = {175306-175318},
  volume   = {8},
  abstract = {In the education sector, the Internet of Things (IoT) technology, integrated with fog-cloud computing, has offered productive services. Motivated by this, the smart recommender system offers the facility to the students to opt for the course and college based on the education quality. This research provides an IoT-fog-cloud paradigm for evaluating the academic environment with a perspective to enhance quality education. Specifically, IoT technology is incorporated to gather data about the academic environment that directly and indirectly influence the quality of education. Using the Bayesian Modeling Technique, the data collected is analyzed utilizing a fog-cloud computing framework to quantify the measure of the probability of education quality (PoEQ). Moreover, the Education Quality Assurance Index (EQAI) is calculated to analyze the quality assessment over a temporal scale. Furthermore, predictive decision-making is performed for quality estimation using the Adaptive Neuro-Fuzzy Inference System (ANFIS). The experimental simulation on 4 challenging datasets namely C1 (2124 instances), C2 (2112), C3 (2139), and C4 (2109) shows the effectiveness of the proposed framework. Simulation findings are compared with state-of-the-art techniques to measure the overall performance enhancement of the proposed system. Also, the mathematical analysis was carried out to assess the analytical performance of the proposed framework.},
  doi      = {10.1109/ACCESS.2020.3019682},
  keywords = {Education;Quality assessment;Internet of Things;Analytical models;Decision making;Real-time systems;Recommender systems;Adaptive neuro-fuzzy inference system;smart recommender system;Internet of Things (IoT);fog-cloud computing},
}

@Article{Xu2018,
  author   = {Xu, Han and Qiu, Xiwei and Sheng, Yongpan and Luo, Liang and Xiang, Yanping},
  journal  = {IEEE Access},
  title    = {A Qos-Driven Approach to the Cloud Service Addressing Attributes of Security},
  year     = {2018},
  issn     = {2169-3536},
  pages    = {34477-34487},
  volume   = {6},
  abstract = {Recently, cloud computing has been widely used by relying on its powerful resource integration and computing abilities. In the cloud computing system (CCS), the quality of service (QoS) is an important service evaluation criterion from provider and client perspectives, which directly affects the client experience and profit of the cloud providers. Thus, a precise evaluation of the QoS can help the cloud provider develop reasonable resource allocation strategies for improving the client experience. The performance metric is usually adopted to quantify QoS. Many approaches and methods for evaluating performance have been widely studied. However, another important metric, i.e., security, does not receive adequate attention in the evaluation of QoS. More importantly, security also has serious effects on the performance metric, that is, complex security-performance (S-P) correlations. To address these issues, this paper first builds a Markov model to analyze and assess the security of the CCS that captures two critical security factors, i.e., malicious attacks and the security protection mechanism. Then, a hierarchical modeling approach is presented to flexibly build the connection between security and the service performance. Finally, we propose a correlation metric to quantify random service performance. This correlation metric comprehensively considers the effect of the security factors and thus becomes more realistic and precise. The experimental results reveal the dynamic change of performance caused by the security factors and demonstrate the important S-P correlation. Therefore, security cannot be ignored in the modeling and evaluation of the QoS metric.},
  doi      = {10.1109/ACCESS.2018.2849594},
  keywords = {Security;Cloud computing;Quality of service;Measurement;Correlation;Servers;Computational modeling;Cloud service;quality of service;security modeling;performance modeling},
}

@InProceedings{Gustamas2017,
  author    = {Gustamas, R. Gargista and Shidik, Guruh Fajar},
  booktitle = {2017 International Seminar on Application for Technology of Information and Communication (iSemantic)},
  title     = {Analysis of network infrastructure performance on cloud computing},
  year      = {2017},
  month     = {Oct},
  pages     = {169-174},
  abstract  = {Cloud Computing offers more convenience than conventional that provide custom Virtual Machine (VM) for any computation requirements. Network connectivity is closely related to the quality of cloud infrastructure itself. This paper focus in preliminary study to test the performance of cloud infrastructure with two type test. First test to measure Network performance and the second to measure cloud computation performance. OpenStack was used as cloud computing software infrastructure. We perform simple cloud infrastructure topology which is divided into three zones, there are Internal Zone, External Zone and Outside Cloud Infrastructure Zone. The parameter tested in this research are quality of bandwidth, latency, jitter and also Processing time during rendering process. The results show VM from simple topology cloud computing which is used to render video, able to perform processing time that slightly longer than using personal computer (PC) with same specification. The network side has been considering as a key of degradation render performance in cloud computing.},
  doi       = {10.1109/ISEMANTIC.2017.8251864},
  keywords  = {Cloud computing;Rendering (computer graphics);Servers;Topology;Network topology;Bandwidth;Jitter;Cloud Computing;Network Performance;IAAS;Rendering},
}

@InProceedings{Muralitharan2017,
  author    = {Muralitharan, D. Boobala and Reebha, S. Arockia Babi and Saravanan, D.},
  booktitle = {2017 International Conference on IoT and Application (ICIOT)},
  title     = {Optimization of performance and scheduling of HPC applications in cloud using cloudsim and scheduling approach},
  year      = {2017},
  month     = {May},
  pages     = {1-6},
  abstract  = {Cloud computing is emerging as a promising alternative to supercomputers for some High-Performance Computing (HPC) applications. Cloud computing is an essential component of the back bone of the Internet of Things (IoT). Clouds are needed to support huge numbers of interactions with varying quality requirements. Hence, Service quality will be a vital differentiator among cloud providers. In order to differentiate themselves from their competitors, cloud providers should offer best services that meet customers' expectations. A quality model can be used to represent, measure and compare the quality of the providers, such that a mutual understanding can be established among clouds take holders. With cloud as an additional deployment option, HPC users and providers faces the challenges of dealing with highly heterogeneous resources, where the variability spans across a wide range of processor configurations, interconnects, virtualization environments, and pricing models. HPC applications are increasingly being used in academia and laboratories for scientific research and in industries for business and analytics. Cloud computing offers the benefits of virtualization, elasticity of resources and elimination of cluster setup cost and time to HPC applications users. Effort was taken for holistic viewpoint to answer the questions - why and who should choose cloud for HPC, for what applications and how the cloud can be used for HPC? Comprehensive performance and cost evaluation and analysis of running a set of HPC applications on a range of platforms, varying from supercomputers to clouds was carried out. Further, performance of HPC applications is improved in cloud by optimizing HPC applications' characteristics for cloud and cloud virtualization mechanisms for HPC. In this paper, a novel heuristics for online application-aware job scheduling in multi-platform environments is presented. Experimental results and Simulations using CloudSim show that current clouds cannot substitute supercomputers but can effectively complement them.},
  doi       = {10.1109/ICIOTA.2017.8073634},
  keywords  = {Cloud computing;Virtualization;Supercomputers;Computational modeling;Servers;Processor scheduling;Resource management;Cloud computing;High-Performance Computing (HPC);Job scheduling;CloudSim},
}

@Article{Li2020c,
  author   = {Li, Keqin},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Quantitative Modeling and Analytical Calculation of Elasticity in Cloud Computing},
  year     = {2020},
  issn     = {2168-7161},
  month    = {Oct},
  number   = {4},
  pages    = {1135-1148},
  volume   = {8},
  abstract = {Elasticity is a fundamental feature of cloud computing and can be considered as a great advantage and a key benefit of cloud computing. One key challenge in cloud elasticity is lack of consensus on a quantifiable, measurable, observable, and calculable definition of elasticity and systematic approaches to modeling, quantifying, analyzing, and predicting elasticity. Another key challenge in cloud computing is lack of effective ways for prediction and optimization of performance and cost in an elastic cloud platform. The present paper makes the following significant contributions. First, we present a new, quantitative, and formal definition of elasticity in cloud computing, i.e., the probability that the computing resources provided by a cloud platform match the current workload. Our definition is applicable to any cloud platform and can be easily measured and monitored. Furthermore, we develop an analytical model to study elasticity by treating a cloud platform as a queueing system, and use a continuous-time Markov chain (CTMC) model to precisely calculate the elasticity value of a cloud platform by using an analytical and numerical method based on just a few parameters, namely, the task arrival rate, the service rate, the virtual machine start-up and shut-down rates. In addition, we formally define auto-scaling schemes and point out that our model and method can be easily extended to handle arbitrarily sophisticated scaling schemes. Second, we apply our model and method to predict many other important properties of an elastic cloud computing system, such as average task response time, throughput, quality of service, average number of VMs, average number of busy VMs, utilization, cost, cost-performance ratio, productivity, and scalability. In fact, from a cloud consumer's point of view, these performance and cost metrics are even more important than the elasticity metric. Our study in this paper has two significance. On one hand, a cloud service provider can predict its performance and cost guarantee using the results developed in this paper. On the other hand, a cloud service provider can optimize its elastic scaling scheme to deliver the best cost-performance ratio. To the best of our knowledge, this is the first paper that analytically and comprehensively studies elasticity, performance, and cost in cloud computing. Our model and method significantly contribute to the understanding of cloud elasticity and management of elastic cloud computing systems.},
  doi      = {10.1109/TCC.2017.2665549},
  keywords = {Cloud computing;Markov processes;Computational modeling;Analytical models;Pricing;Quality of service;Optimization;Queueing analysis;Cloud computing;continuous-time Markov chain;cost-performance ratio;elasticity;queueing model},
}

@Article{Guerron2020,
  author   = {Guerron, Ximena and Abrahão, Silvia and Insfran, Emilio and Fernández-Diego, Marta and González-Ladrón-De-Guevara, Fernando},
  journal  = {IEEE Access},
  title    = {A Taxonomy of Quality Metrics for Cloud Services},
  year     = {2020},
  issn     = {2169-3536},
  pages    = {131461-131498},
  volume   = {8},
  abstract = {A large number of metrics with which to assess the quality of cloud services have been proposed over the last years. However, this knowledge is still dispersed, and stakeholders have little or no guidance when choosing metrics that will be suitable to evaluate their cloud services. The objective of this paper is, therefore, to systematically identify, taxonomically classify, and compare existing quality of service (QoS) metrics in the cloud computing domain. We conducted a systematic literature review of 84 studies selected from a set of 4333 studies that were published from 2006 to November 2018. We specifically identified 470 metric operationalizations that were then classified using a taxonomy, which is also introduced in this paper. The data extracted from the metrics were subsequently analyzed using thematic analysis. The findings indicated that most metrics evaluate quality attributes related to performance efficiency (64%) and that there is a need for metrics that evaluate other characteristics, such as security and compatibility. The majority of the metrics are used during the Operation phase of the cloud services and are applied to the running service. Our results also revealed that metrics for cloud services are still in the early stages of maturity - only 10% of the metrics had been empirically validated. The proposed taxonomy can be used by practitioners as a guideline when specifying service level objectives or deciding which metric is best suited to the evaluation of their cloud services, and by researchers as a comprehensive quality framework in which to evaluate their approaches.},
  doi      = {10.1109/ACCESS.2020.3009079},
  keywords = {Measurement;Cloud computing;Taxonomy;Quality of service;Systematics;NIST;Elasticity;Software quality;metrics;cloud services;systematic literature review},
}

@Article{Zheng2014a,
  author   = {Zheng, Xianrong and Martin, Patrick and Brohman, Kathryn and Xu, Li Da},
  journal  = {IEEE Transactions on Industrial Informatics},
  title    = {CLOUDQUAL: A Quality Model for Cloud Services},
  year     = {2014},
  issn     = {1941-0050},
  month    = {May},
  number   = {2},
  pages    = {1527-1536},
  volume   = {10},
  abstract = {Cloud computing is an important component of the backbone of the Internet of Things (IoT). Clouds will be required to support large numbers of interactions with varying quality requirements. Service quality will therefore be an important differentiator among cloud providers. In order to distinguish themselves from their competitors, cloud providers should offer superior services that meet customers' expectations. A quality model can be used to represent, measure, and compare the quality of the providers, such that a mutual understanding can be established among cloud stakeholders. In this paper, we take a service perspective and initiate a quality model named CLOUDQUAL for cloud services. It is a model with quality dimensions and metrics that targets general cloud services. CLOUDQUAL contains six quality dimensions, i.e., usability, availability, reliability, responsiveness, security, and elasticity, of which usability is subjective, whereas the others are objective. To demonstrate the effectiveness of CLOUDQUAL, we conduct empirical case studies on three storage clouds. Results show that CLOUDQUAL can evaluate their quality. To demonstrate its soundness, we validate CLOUDQUAL with standard criteria and show that it can differentiate service quality.},
  doi      = {10.1109/TII.2014.2306329},
  keywords = {Cloud computing;Security;Availability;Quality of service;Measurement;Cloud computing;Internet of Things (IoT);quality model;validity criteria},
}

@InProceedings{Cedillo2015,
  author    = {Cedillo, Priscila and Jimenez-Gomez, Javier and Abrahao, Silvia and Insfran, Emilio},
  booktitle = {2015 IEEE International Conference on Services Computing},
  title     = {Towards a Monitoring Middleware for Cloud Services},
  year      = {2015},
  month     = {June},
  pages     = {451-458},
  abstract  = {Cloud Computing represents a new trend in the development and use of software. Many organizations are currently adopting the use of services that are hosted in the cloud by employing the Software as a Service (SaaS) model. Services are typically accompanied by a Service Level Agreement (SLA), which defines the quality terms that a provider offers to its customers. Many monitoring tools have been proposed to report compliance with the SLA. However, they have some limitations when changes to monitoring requirements must be made and because of the complexity involved in capturing low-level raw data from services at runtime. In this paper, we propose the design of a platform-independent monitoring middleware for cloud services, which supports the monitoring of SLA compliance and provides a report containing SLA violations that may help stakeholders to make decisions regarding how to improve the quality of cloud services. Moreover, our middleware definition is based on the use of models@run.time, which allows the dynamic change of quality requirements and/or the dynamic selection of different metric operationalizations (i.e., Calculation formulas) with which to measure the quality of services. In order to demonstrate the feasibility of our approach, we show the instantiation of the proposed middleware that can be used to monitor services when deployed on the Microsoft Azure© platform.},
  doi       = {10.1109/SCC.2015.68},
  keywords  = {Monitoring;Middleware;Measurement;Radiation detectors;Engines;Runtime;Software as a service;Cloud Computing;Software as a Service;Monitoring;Middleware;Quality of Service;Models@run.time},
}

@InProceedings{KumarKoditala2018,
  author    = {Kumar Koditala, Nikhil and Shekar Pandey, Purnendu},
  booktitle = {2018 International Conference on Research in Intelligent and Computing in Engineering (RICE)},
  title     = {Water Quality Monitoring System Using IoT and Machine Learning},
  year      = {2018},
  month     = {Aug},
  pages     = {1-5},
  abstract  = {World Economic Forum ranked drinking water crisis as one of the global risk, due to which around 200 children are dying per day. Drinking unsafe water alone causes around 3.4 million deaths per year. Despite the advancements in technology, sufficient quality measures are not present to measure the quality of drinking water. By focusing on the above issue, this paper proposes a low cost water quality monitoring system using emerging technologies such as IoT, Machine Learning and Cloud Computing which can replace traditional way of quality monitoring. This helps in saving people of rural areas from various dangerous diseases such as fluorosis, bone deformities etc. The proposed model also has a capacity to control temperature of water and adjusts it so as to suit environment temperature. Based on our model we have achieved R-squared score of 0.933.},
  doi       = {10.1109/RICE.2018.8509050},
  keywords  = {Temperature sensors;Monitoring;Temperature measurement;Cloud computing;Sensor systems;Electronic mail;Water quality monitoring;Internet of Things;Machine learning;Cloud Computing;MQTT;Rural Development},
}

@InProceedings{Atan2016,
  author    = {Rodziah binti Atan},
  booktitle = {2016 2nd International Conference on Science in Information Technology (ICSITech)},
  title     = {Enhancing service quality through Service Level Agreement (SLA) full implementation},
  year      = {2016},
  month     = {Oct},
  pages     = {1-1},
  abstract  = {Various SLA monitoring systems are proposed by different features and abilities to evaluate the agreed SLA. The current SLA monitoring systems in cloud computing for its structural, behavioral characteristics and situation are also in place. The systematic reviews of a well-known methods and approaches shows a significant numbers of researches been done in this area. Based on the number of effort and researches, the quality of services should proportionately increase alongside them. We look this matter from the perspectives of enforcement, that evident the stand of quality of services. Service Level Agreement (SLA) enforcement impact measures is a potential research area to be explored. Assumptions that this study is making are, SLA management will become better by a firm enforcement, where every customers are responsible to launch report of bugs or mischief of services such as unsatisfactory quality or service unavailability to a collection pool, and the provider will react immediately to the complaints so that the total downtime not exceeding the SLA value, with efficient enforcement. This study establishes fundamental theory to measure enforcement impact to SLA monitoring and management. We proposed eight activity phases from formulating until analyzing and decision formation. Descriptive statistics is utilized to analyze the extracted data. The SLA validation detection is the most frequent purpose of SLA monitoring systems in cloud by 58% and throughput is checked as an attribute target by 28%. The self-monitoring SLA, self-healing system, hierarchical structure are recognized points of SLA monitoring systems which need improvement before the enforcement could be based upon.},
  doi       = {10.1109/ICSITech.2016.7852595},
  keywords  = {Service Level Agreement;enforcement;monitoring;cloud computing;quality of service},
}

@InProceedings{Indrawati2018,
  author    = {Indrawati and Puspita, Fitri Maya and Erlita, Sri and Nadeak, Inosensius and Arisha, Bella},
  booktitle = {2018 International Conference on Information and Communications Technology (ICOIACT)},
  title     = {LINGO-based optimization problem of cloud computing of bandwidth consumption in the Internet},
  year      = {2018},
  month     = {March},
  pages     = {436-441},
  abstract  = {Optimization problem is an important issue in the network Internet. With the dynamic approach in modeling networks, we can strengthen network performance and ensure that the cost will be minimized and profit of provider can be maximized. This research aims to study, analyze the scheme for cloud networking and formulate a plan of new models of dynamic networks and can work under a cloud of wireless networks. Mixed Integer Non Linear Programming (MINLP) is an integer linear programming model to optimize a particular purpose. In MINLP process, the objective function is determined beforehand. The optimal solution of MINLP lies in the majority of decision variables that can be an integer, Boolean or fractions. Model Cloud computing is one of the areas that is most discussed and promising in modern computer science. Cloud computing is a computing model in which resources such as processors, storage, network and software information that can be accessed by customers via the Internet. In the cloud computing implementation, we require a good traffic for performance and reliability of the system is maintained. QoS (Quality of Services) refers to the distribution of bandwidth. QoS is used as a measure of whether or not the characteristics of the network to meet the needs of different services that use the same infrastructure. Tests carried out on the quality of service parameters, namely, delay, packet loss, throughput and bandwidth. To formulate and solve optimization problems used LINGO software applications. The results show that by designing the optimization problem, the cost of consumption of the demand of the internet can be reduced; the maximum profit for the provider can be increased.},
  doi       = {10.1109/ICOIACT.2018.8350688},
  keywords  = {Cloud computing;Bandwidth;Servers;Optimization;Mathematical model;Web and internet services;Pricing;MINLP;cloud computing;QoS;LINGO;optimal solution},
}

@InProceedings{Seong2016,
  author    = {Chaemin Seong and Minsoo Jang and Kyungshik Lim},
  booktitle = {2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN)},
  title     = {Context-aware HTTP Adaptive Streaming in mobile cloud environments},
  year      = {2016},
  month     = {July},
  pages     = {1062-1067},
  abstract  = {With advances of cloud computing, seamless video streaming from video server to cloud client has been one of technical challenges for multimedia cloud applications. Especially in case that Desktop-as-a-Service (DaaS) as a major cloud application is deployed via wireless networks, it could raise a new set of issues to be addressed. To solve the problem, we propose a Cloud-based Context-aware HTTP Adaptive Streaming (C2HAS) agent located at cloud server. The goal of the agent is to maximize the video quality of seamless streaming perceived by cloud client, given a dynamically changing network context. From network context we derive a major metric for adapting and maximizing the video quality perceived by cloud clients, which is the throughput ratio of backbone networks and access networks. Based on the metric, we can provide a maximal quality of seamless video streaming to cloud users who might be connected via distant and/or lossy wireless links. The experimental performance analysis shows that the C2HAS agent could be a viable solution for cloud-based multimedia applications.},
  doi       = {10.1109/ICUFN.2016.7536961},
  issn      = {2165-8536},
  keywords  = {Streaming media;Cloud computing;Servers;Context;Mobile communication;Video recording;Quality assessment;context awareness;HTTP adaptive streaming;virtual desktop infrastructure;mobile cloud computing component},
}

@InProceedings{Karadimce2016,
  author    = {Karadimce, Aleksandar and Davcev, Danco},
  booktitle = {2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX)},
  title     = {Perception of quality in cloud computing based services},
  year      = {2016},
  month     = {June},
  pages     = {1-6},
  abstract  = {Cloud computing consists of hardware and software resources, available on the Internet as a set of services for users. This technology aims to provide stable, reliable and encapsulated dynamic information and communication environment for end users to be able to simultaneously access shared resources that are available anywhere and at any time. The major benefit of cloud computing is used to improve the perception of quality for the client requests. Commonly in the communications industry, the term Quality of Experience (QoE) is used as a measure for the user perception of service from the user's point of view. In this research, we propose a classification of cloud-based services based on objective and subjective characteristics for perception of quality. The main contribution in this paper is a novel approach based on Bayesian modeling for efficient assessment of QoE perception for cloud-based services considering the level of interactivity, service complexity, usage domain, and multimedia-intensity.},
  doi       = {10.1109/QoMEX.2016.7498925},
  keywords  = {Cloud computing;Quality of service;Multimedia communication;Mobile communication;Mobile handsets;Computational modeling;Bayes methods;cloud services;perception of quality;QoE;bayesian network},
}

@Article{Chen2020,
  author   = {Chen, Yunliang and Wang, Lizhe and Chen, Xiaodao and Ranjan, Rajiv and Zomaya, Albert Y. and Zhou, Yuchen and Hu, Shiyan},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Stochastic Workload Scheduling for Uncoordinated Datacenter Clouds with Multiple QoS Constraints},
  year     = {2020},
  issn     = {2168-7161},
  month    = {Oct},
  number   = {4},
  pages    = {1284-1295},
  volume   = {8},
  abstract = {Cloud computing is now a well-adopted computing paradigm. With unprecedented scalability and flexibility, the computational cloud is able to carry out large scale computing tasks in parallel. The datacenter cloud is a new cloud computing model that uses multi-datacenter architectures for large scale massive data processing or computing. In datacenter cloud computing, the overall efficiency of the cloud depends largely on the workload scheduler, which allocates clients' tasks to different Cloud datacenters. Developing high performance workload scheduling techniques in Cloud computing imposes a great challenge which has been extensively studied. Most previous works aim only at minimizing the completion time of all tasks. However, timeliness is not the only concern, reliability and security are also very important. In this work, a comprehensive Quality of Service (QoS) model is proposed to measure the overall performance of datacenter clouds. An advanced Cross-Entropy based stochastic scheduling (CESS) algorithm is developed to optimize the accumulative QoS and sojourn time of all tasks. Experimental results show that our algorithm improves accumulative QoS and sojourn time by up to 56.1 and 25.4 percent respectively compared to the baseline algorithm. The runtime of our algorithm grows only linearly with the number of Cloud datacenters and tasks. Given the same arrival rate and service rate ratio, our algorithm steadily generates scheduling solutions with satisfactory QoS without sacrificing sojourn time.},
  doi      = {10.1109/TCC.2016.2586048},
  keywords = {Cloud computing;Quality of service;Processor scheduling;Computational modeling;Scheduling;Data centers;Resource management;Cloud computing;datacenter clouds;quality of service;workload scheduling},
}

@InProceedings{Skourletopoulos2018,
  author    = {Skourletopoulos, Georgios and Mavromoustakis, Constandinos X. and Mastorakis, George and Batalla, Jordi Mongay and Song, Houbing and Sahalos, John N. and Pallis, Evangelos},
  booktitle = {2018 IEEE International Conference on Communications (ICC)},
  title     = {Elasticity Debt Analytics Exploitation for Green Mobile Cloud Computing: An Equilibrium Model},
  year      = {2018},
  month     = {May},
  pages     = {1-6},
  abstract  = {Mobile cloud computing is being accepted as the model for mobile users to ubiquitously access a shared pool of cloud computing resources, data and services on-demand. In this context, elasticity debt analytics can be harnessed as a measure for efficient scheduling of cloud resources and guarantee of quality of service requirements. This paper proposes a novel green-driven, game theoretic approach to minimizing the elasticity debt on mobile cloud-based service level, investigating the case when a task is offloaded, scheduled and executed on a mobile cloud computing system. The decision to offload a mobile device user's task on cloud affects the level of elasticity debt minimization for the provided services. The research problem is formulated as an elasticity debt quantification game, elaborating on an incentive mechanism to: (a) predict elasticity debt and mitigate the risk of service overutilization, (b) achieve scalability as the number of mobile device user requests for cloud resources increases or decreases accordingly, and (c) optimize cloud resource provisioning, parameterizing the current pool of active users per service. The experimental results prove the effectiveness of the equilibrium model, which allocates the mobile device user requests to high elasticity debt-level services and facilitate elasticity debt minimization for greener mobile cloud computing environments.},
  doi       = {10.1109/ICC.2018.8422956},
  issn      = {1938-1883},
  keywords  = {Elasticity;Cloud computing;Mobile handsets;Games;Computational modeling;Minimization;Green products},
}

@InProceedings{Tirta2017,
  author    = {Tirta, Manggiardi B.W. and Shidik, Guruh Fajar},
  booktitle = {2017 International Seminar on Application for Technology of Information and Communication (iSemantic)},
  title     = {Evaluation performance of cloud computing with network attached storage for video render},
  year      = {2017},
  month     = {Oct},
  pages     = {157-163},
  abstract  = {One of the benefits of Cloud Computing is the use of virtual machines for efficiency and resource utilization. The study utilizes a virtual machine on cloud computing technology for video rendering needs and is integrated with Network Attached Storage (NAS) storage methods, a centralized storage method that uses network media to connect storage media with users. The rendering process is then analyzed using several metering tools to measure the rendering time frame, VM Utilization, network performance, and NAS Network Performance. The results show that rendering takes longer, then CPU Utilization shows a maximum of 77%, Memory Utilization 55%, and Network Utilization 10%. The bandwidth available between NAS and VM storage in a cloud computing system only generates a maximum of 295.1 Mbps, which should reach 1 Gbps. The quality of video rendering in VM cloud computing shows similar results with rendering on physical computers, the results obtained from testing between frames using mean-square error algorithm.},
  doi       = {10.1109/ISEMANTIC.2017.8251862},
  keywords  = {Rendering (computer graphics);Cloud computing;Testing;Servers;Process control;Virtual machining;Central Processing Unit;cloud computing;video render;network attached storage},
}

@InProceedings{Zhang2019,
  author    = {Zhang, Ziyi and Guo, Caishan and Sun, Yuyan and Hu, Kaiqiang and Wang, Qinghai and Wu, Yuzhao and Cai, Zexiang},
  booktitle = {2019 IEEE International Conference on Smart Cloud (SmartCloud)},
  title     = {Cloud Computing Placement Optimization Under Ubiquitous Power Internet of Things Background},
  year      = {2019},
  month     = {Dec},
  pages     = {13-18},
  abstract  = {With the development of power system and the introduction of the Energy Internet, the implementation of Ubiquitous Power Internet of Things (UPIoT) is necessary for power utilities to meet the demands of Integrated Energy Applications. Massive heterogeneous data from various devices surge into power system via UPIoT, which puts heavy burden on data processing capabilities of power system. Cloud computing is an effective measure to provide big data processing capabilities and the establishment of cloud computing for power system is of great significance. Firstly, the architecture of UPIoT and the cloud computing system based on UPIoT background are analyzed. Considering the characteristics of power system, a distributed cloud computing architecture for power system is proposed. A coordinated placement optimization strategy based on minimum cost and satisfaction of quality of service for the proposed architecture is formulated. Based on a given case, the placement optimization simulations are studied. The simulation results prove that the proposed architecture is cost-efficient and the proposed optimization strategy is effective and efficient.},
  doi       = {10.1109/SmartCloud.2019.00012},
  keywords  = {Cloud computing;Power systems;Computer architecture;Power generation;Data centers;Optimization;Quality of service;cloud computing;edge computing;ubiquitous power internet of things;placement optimization;cost;quality of service},
}

@InProceedings{Abdulwahid2020,
  author    = {Abdulwahid, Ali Hadi},
  booktitle = {2020 9th International Conference on Renewable Energy Research and Application (ICRERA)},
  title     = {IoT Based Water Quality Monitoring System for Rural Areas},
  year      = {2020},
  month     = {Sep.},
  pages     = {279-282},
  abstract  = {To ensure that safety is guaranteed, it is essential to implement monitoring in real-time for the quality of potable water. This work is about the use of Internet of Things (IoT) technology to develop an affordable system to control water quality in real-time. Several sensors are integrated into the system to measure various chemical and physical water properties, such as conductivity, pH, turbidity, and temperature. The core controller, which can also be the microprocessor, manages the processing of data captured by the sensor. The visualization of data can be accomplished on cloud computing via the Internet.},
  doi       = {10.1109/ICRERA49962.2020.9242798},
  issn      = {2572-6013},
  keywords  = {Sensors;Temperature sensors;Temperature measurement;Water quality;Monitoring;Sensor systems;Internet of Things;water quality monitoring;Internet of Things;Arduino;cloud computing},
}

@Article{Azadi2019,
  author   = {Azadi, Majid and Emrouznejad, Ali and Ramezani, Fahimeh and Hussain, Farookh K.},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Efficiency measurement of cloud service providers using network data envelopment analysis},
  year     = {2019},
  issn     = {2168-7161},
  pages    = {1-1},
  abstract = {An increasing number of organizations and businesses around the world use cloud computing services to improve their performance in the competitive marketplace. However, one of the biggest challenges in using cloud computing services is performance measurement and the selection of the best cloud service providers (CSPs) based on quality of service (QoS) requirements (Duan, 2017). To address this shortcoming in this article we propose a network data envelopment analysis (DEA) method in measuring the efficiency of CSPs. When network dimensions are taken into consideration, a more comprehensive analysis is enabled where divisional efficiency is reflected in overall efficiency estimates. This helps managers and decision makers in organizations to make accurate decisions in selecting cloud services. In the current study, variable returns to scale (VRS), the non-oriented network slacks-based measure (SBM) model and input-oriented and output-oriented SBM models are applied to measure the performance of 18 CSPs. The obtained results show the superiority of the network DEA model and they also demonstrate that the proposed model can evaluate and rank CSPs much better than compared to traditional DEA models.},
  doi      = {10.1109/TCC.2019.2927340},
  keywords = {Cloud computing;Quality of service;Measurement;Analytical models;Computational modeling;Data envelopment analysis;Information technology;Cloud service providers (CSPs);Performance measures;Efficiency measurement;Data Envelopment Analysis;Network slacks-based measure (SBM) model},
}

@InProceedings{FlinckLindstroem2020,
  author    = {Flinck Lindström, Sebastian and Wetterberg, Markus and Carlsson, Niklas},
  booktitle = {2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)},
  title     = {Cloud Gaming: A QoE Study of Fast-paced Single-player and Multiplayer Gaming},
  year      = {2020},
  month     = {Dec},
  pages     = {34-45},
  abstract  = {Cloud computing offers an attractive solution for modern computer games. By moving the increasingly demanding graphical calculations (e.g., generation of real-time video streams) to the cloud, consumers can play games using small, cheap devices. While cloud gaming has many advantages and is increasingly deployed, not much work has been done to understand the underlying factors impacting players' user experience when moving the processing to the cloud. In this paper, we study the impact of the quality of service (QoS) factors most affecting the players' quality of experience (QoE) and in-game performance. In particular, these relationships are studied from multiple perspectives using complementing analysis methods applied on the data collected via instrumented user tests. During the tests, we manipulated the players' network conditions and collected low-level QoS metrics and in-game performance, and after each game, the users answered questions capturing their QoE. New insights are provided using different correlation/auto-correlation/cross-correlation statistics, regression models, and a thorough breakdown of the QoS metric most strongly correlated with the users' QoE. We find that the frame age is the most important QoS metric for predicting in-game performance and QoE, and that spikes in the frame age caused by large frame transfers can have extended negative impact as they can cause processing backlogs. The study emphasizes the need to carefully consider and optimize the parts making up the frame age, including dependencies between the processing steps. By lowering the frame age, more enjoyable gaming experiences can be provided.},
  doi       = {10.1109/UCC48980.2020.00023},
  keywords  = {Games;Quality of experience;Measurement;Quality of service;Servers;Packet loss;Cloud gaming;Cloud computing;cloud gaming;QoE;frame age;single-player;multiplayer;gaming;fast-paced;performance},
}

@InProceedings{Biondi2019,
  author    = {Biondi, Katalina and Al-Masri, Eyhab and Baiocchi, Orlando and Jeyaraman, Suganya and Pospisil, Eric and Boyer, Graham and de Souza, Cleonilson Protasio},
  booktitle = {2019 International Conference in Engineering Applications (ICEA)},
  title     = {Air Pollution Detection System using Edge Computing},
  year      = {2019},
  month     = {July},
  pages     = {1-6},
  abstract  = {Existing solutions to measuring air quality can be expensive and potentially mutes high air pollution events. The IoT Pollution Project is exploring how IoT concepts can be applied with smart systems to detect pollution in real-time. Using a network of Raspberry Pi prototypes, the project aims to measure heavily populated areas around the City of Tacoma, while building a real-time interface measuring current air quality. The project also explores the use of edge computing as an alternative to cloud computing. The vast expansion of IoT devices poses threats to the infrastructure of cloud computing as more devices process and store data to the cloud. The project demonstrates how edge devices can alleviate the work done on the cloud by calculating rolling averages over a time interval on the edge device and then deploying the data to the cloud. The project uses Microsoft Azure Framework, IoT concepts and edge computing concepts to build the project architecture.},
  doi       = {10.1109/CEAP.2019.8883458},
  keywords  = {Cloud computing;Air pollution;Edge computing;Atmospheric measurements;Pollution measurement;Internet of Things;Raspberry Pi;Microsoft Azure;LoRa;Pollution Detection;Environmental Monitoring;Real-Time Data;IoT;Edge Computing;Cloud Computing;Air Quality},
}

@InProceedings{Zhang2014b,
  author    = {Zhang, Xiaodong and Dechen, Zhan and Nie, Lanshun and Zhao, Tianqi and Xiong, Xiao},
  booktitle = {2014 International Conference on Service Sciences},
  title     = {An Optimal Service-Selection Model Based on Capability and Quality of Resource Service},
  year      = {2014},
  month     = {May},
  pages     = {47-52},
  abstract  = {Most of the researches on optimal service selection are based on the assumption that the capabilities of the services fully meet the requirements. Their limitation is the ignorance of the resources which is the basic factor supporting the implementation of services and it may cause a waste of resources. In cloud computing environment which benefits from its large-scale, there are a large number of resources. Therefore, the waste of resources in it would be a big problem. This paper introduces 'service equivalent' as the basic metric to measure the capabilities of service resources and proposes an optimal service selection model based on capability and quality of service resources and algorithm, in order to solve the issues about the matching capability of service resource and the optimal selection of service resource based on quality. Finally it proves that the model can effectively reduce the waste of resources by the test, which achieves the expected goal.},
  doi       = {10.1109/ICSS.2014.39},
  issn      = {2165-3836},
  keywords  = {Quality of service;Optimization;Heuristic algorithms;Business;Vehicles;Algorithm design and analysis;Load modeling;service capability matching;service equivalent;service capability model;QoS model},
}

@InProceedings{Brataas2017,
  author    = {Brataas, Grunnar and Herbst, Nikolas and Ivansek, Simon and Polutnik, Jure},
  booktitle = {2017 IEEE International Conference on Autonomic Computing (ICAC)},
  title     = {Scalability Analysis of Cloud Software Services},
  year      = {2017},
  month     = {July},
  pages     = {285-292},
  abstract  = {Cloud computing theoretically offers its customers unlimited cloud resources. However, the scalability of software services is often limited by their underlying architecture. In contrast to current scalability analysis approaches, we make work parameters, quality thresholds, as well as the resource space explicit in a conceptually consistent set of equations. We propose two scalability metric functions based on these equations. The resource scalability metric function describes the relation between the capacity of the multi-tier cloud software service and its use of cloud resources, whereas the cost scalability metric function replaces cloud resources with cost. We validate using the Cloud-Store application. CloudStore follows the TPC-W specification, representing an online book store. We have experimented with 21 different public Amazon Web Service configurations and two private OpenStack configurations.},
  doi       = {10.1109/ICAC.2017.34},
  issn      = {2474-0756},
  keywords  = {Measurement;Scalability;Cloud computing;Computer architecture;Aerospace electronics;cloud;service;scalability;metric;measurement;cost},
}

@InProceedings{RamosdaPaixao2018,
  author    = {Ramos da Paixão, Ermínio Augusto and Vieira, Rafael Fogarolli and Araújo, Welton Vasconcelos and Cardoso, Diego Lisboa},
  booktitle = {2018 Third International Conference on Fog and Mobile Edge Computing (FMEC)},
  title     = {Optimized load balancing by dynamic BBU-RRH mapping in C-RAN architecture},
  year      = {2018},
  month     = {April},
  pages     = {100-104},
  abstract  = {Cloud Radio Access Network (C-RAN) is one of the key architectures for the next generation of mobile networks (5G) that aim at centralized processing and management, collaborative radios and cloud computing in real time. These features enable the architectures to make a rational adjustment to the connection between remote radio heads (RRHs) and baseband units (BBUs) dynamically. This is important since if this feature is neglected, it can cause difficulties such as blocked calls and low-quality connections. This study investigates this area and proposes an optimized mapping model for RRH-BBU that seeks a more equitable and effective balancing. The Key Performance Indicator (KPI) of blocked calls was used for this to measure the quality of service (QoS). A particle Swarm algorithm (PSO) was created to minimize the number of blocked calls and additionally balancing the processing load between the BBUs. Scenario from literature was employed that consists of 19 RRHs distributed in a geographic area, which can be mapped in a BBU pool that manages two BBUs with three sectors each. The initial configuration on average, led to 80 blocked calls. The results obtained by the PSO show that there was a reduction of up to 100% of blocked calls, as well as a more equitable load distribution between the BBUs. Additionally, realistic scenarios with different user profiles were also included, since they demonstrate that these factors have a direct impact on the load generated in the BBUs and hence, affect their balance.},
  doi       = {10.1109/FMEC.2018.8364051},
  keywords  = {Computer architecture;Resource management;Quality of service;Load modeling;Edge computing;Radio access networks;Mathematical model;C-RAN;RRHs;BBUs;PSO;balance},
}

@Article{LilloCastellano2015,
  author   = {Lillo-Castellano, J. M. and Mora-Jiménez, I. and Santiago-Mozos, R. and Chavarría-Asso, F. and Cano-González, A. and García-Alberola, A. and Rojo-Álvarez, J. L.},
  journal  = {IEEE Journal of Biomedical and Health Informatics},
  title    = {Symmetrical Compression Distance for Arrhythmia Discrimination in Cloud-Based Big-Data Services},
  year     = {2015},
  issn     = {2168-2208},
  month    = {July},
  number   = {4},
  pages    = {1253-1263},
  volume   = {19},
  abstract = {The current development of cloud computing is completely changing the paradigm of data knowledge extraction in huge databases. An example of this technology in the cardiac arrhythmia field is the SCOOP platform, a national-level scientific cloud-based big data service for implantable cardioverter defibrillators. In this scenario, we here propose a new methodology for automatic classification of intracardiac electrograms (EGMs) in a cloud computing system, designed for minimal signal preprocessing. A new compression-based similarity measure (CSM) is created for low computational burden, so-called weighted fast compression distance, which provides better performance when compared with other CSMs in the literature. Using simple machine learning techniques, a set of 6848 EGMs extracted from SCOOP platform were classified into seven cardiac arrhythmia classes and one noise class, reaching near to 90% accuracy when previous patient arrhythmia information was available and 63% otherwise, hence overcoming in all cases the classification provided by the majority class. Results show that this methodology can be used as a high-quality service of cloud computing, providing support to physicians for improving the knowledge on patient diagnosis.},
  doi      = {10.1109/JBHI.2015.2412175},
  keywords = {Dictionaries;Informatics;Databases;Biomedical measurement;Heart beat;Hospitals;Complexity theory;Cardiac Arrhythmia Classification;Implantable Defibrillator;Intracardiac Electrogram;Weighted Fast Compression Distance;Big Data Analytics.;Big data analytics;cardiac arrhythmia classification;implantable defibrillator;intracardiac electrogram;weighted fast compression distance},
}

@Article{Kumar2014,
  author   = {Kumar, Neeraj and Chilamkurti, Naveen and Zeadally, Sherali and Jeong, Young-Sik},
  journal  = {The Computer Journal},
  title    = {Achieving Quality of Service (QoS) Using Resource Allocation and Adaptive Scheduling in Cloud Computing with Grid Support},
  year     = {2014},
  issn     = {1460-2067},
  month    = {Feb},
  number   = {2},
  pages    = {281-290},
  volume   = {57},
  abstract = {In the past few years, cloud computing has emerged as a new reliable, scalable and flexible virtual computing environment (VCE). In this new VCE, users can use the available resources as a service by paying for that service according to the time for which these resources are used. It remains a significant challenge to achieve quality of service (QoS) in a VCE with the available resources. The main goal is to schedule the available resources so that the overall QoS delivered by the VCE can be improved. Resources are assumed to be located both at local and global sites. We propose a three-step scheme: resource selection, scheduling of users requests with shared resources and a new Resource Allocation and Adaptive Job Scheduling algorithm, which improves the QoS delivered by the cloud. For job scheduling, we define a new weight metric that is used to efficiently schedule jobs competing for available resources. Our proposed strategy increases the reliability of resource availability for a job and reduces the job completion time, which in turn increases the QoS delivered to end-users. We evaluate our proposed scheme using well-known heuristics. The results obtained show that our proposed scheme considerably reduces the job execution time, and increases the reliability of resource availability for job execution and throughput.},
  doi      = {10.1093/comjnl/bxt024},
  keywords = {cloud computing;job scheduling;quality of service;grid;performance},
}

@Article{Shen2015,
  author   = {Shen, Haiying and Lin, Yuhua and Li, Ting},
  journal  = {IEEE Transactions on Computers},
  title    = {Combining Efficiency, Fidelity, and Flexibility in Resource Information Services},
  year     = {2015},
  issn     = {1557-9956},
  month    = {Feb},
  number   = {2},
  pages    = {353-367},
  volume   = {64},
  abstract = {A large-scale resource sharing system (e.g., collaborative cloud computing and grid computing) creates a virtual supercomputer by providing an infrastructure for sharing tremendous amounts of resources (e.g., computing, storage, and data) distributed over the Internet. A resource information service, which collects resource data and provides resource search functionality for locating desired resources, is a crucial component of the resource sharing system. In addition to resource discovery speed and cost (i.e., efficiency), the ability to accurately locate all satisfying resources (i.e., fidelity) is also an important metric for evaluating service quality. Previously, a number of resource information service systems have been proposed based on Distributed Hash Tables (DHTs) that offer scalable key-based lookup functions. However, these systems either achieve high fidelity at low efficiency, or high efficiency at low fidelity. Moreover, some systems have limited flexibility by only providing exact-matching services or by describing a resource using a pre-defined list of attributes. This paper presents a resource information service that offers high efficiency and fidelity without restricting resource expressiveness, while also providing a similar-matching service. Extensive simulation and PlanetLab experimental results show that the proposed service outperforms other services in terms of efficiency, fidelity, and flexibility; it dramatically reduces overhead and yields significant enhancements in efficiency and fidelity.},
  doi      = {10.1109/TC.2013.222},
  keywords = {Information services;Vectors;Resource management;Transforms;Central Processing Unit;Cloud computing;Aerospace electronics;Resource sharing systems;collaborative cloud computing (CCC);grid computing;resource information service;resource discovery;distributed hash table (DHT)},
}

@InProceedings{Gavra2020,
  author    = {Gavra, Vlad-Dacian and Pop, Ovidiu Aurel},
  booktitle = {2020 IEEE 26th International Symposium for Design and Technology in Electronic Packaging (SIITME)},
  title     = {Usage of ZigBee and LoRa wireless technologies in IoT systems},
  year      = {2020},
  month     = {Oct},
  pages     = {221-224},
  abstract  = {IoT systems are based sensors and actuators to enable ubiquitous sensing to measure environment parameters from delicate ecologies and natural environments to urban environments. By connecting these sensors and actuators to a big network, like internet, an automatization can be performed, and repetitive actions can be done in background by the IoT ecosystem and save a lot of time. IoT can do such things in Home Automation, Smart Cities and even in Air Quality analysis. IoT solution are dependent on the way sensors are transmitting data to cloud or up to the internet. This paper will present the benefits of using Zig Bee instead of using traditional Wi-Fi sensor and present some of the characteristics of LoRa sensors. Cloud computing contributed to the expansion of the IoT systems by offloading local IoT devices of computation intensive tasks. Fog computing brings Cloud closer to the sensors and by doing this minimize communication latencies.},
  doi       = {10.1109/SIITME50350.2020.9292150},
  issn      = {2642-7036},
  keywords  = {Zigbee;Sensors;Protocols;Edge computing;Internet of Things;Wireless sensor networks;Wireless communication;IoT;ZigBEE;LoRa},
}

@InProceedings{Dai2015,
  author    = {Dai, Yangyang and Lou, Yuansheng and Lu, Xin},
  booktitle = {2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics},
  title     = {A Task Scheduling Algorithm Based on Genetic Algorithm and Ant Colony Optimization Algorithm with Multi-QoS Constraints in Cloud Computing},
  year      = {2015},
  month     = {Aug},
  pages     = {428-431},
  volume    = {2},
  abstract  = {Task scheduling problem in cloud computing environment is NP-hard problem, which is difficult to obtain exact optimal solution and is suitable for using intelligent optimization algorithms to approximate the optimal solution. Meanwhile, quality of service (QoS) is an important indicator to measure the performance of task scheduling. In this paper, a novel task scheduling algorithm MQoS-GAAC with multi-QoS constraints is proposed, considering the time-consuming, expenditure, security and reliability in the scheduling process. The algorithm integrates ant colony optimization algorithm (ACO) with genetic algorithm (GA). To generate the initial pheromone efficiently for ACO, GA is invoked. With the designed fitness function, 4-dimensional QoS objectives are evaluated. Then, ACO is utilized to seek out the optimum resource. The experiment indicates that the proposed algorithm has preferable performance both in balancing resources and guaranteeing QoS.},
  doi       = {10.1109/IHMSC.2015.186},
  keywords  = {Genetic algorithms;Quality of service;Cloud computing;Algorithm design and analysis;Scheduling algorithms;Ant colony optimization;task scheduling;cloud computing;genetic algorithm;ant colony optimization algorithm;QoS},
}

@InProceedings{Bashar2017,
  author    = {Bashar, Abul},
  booktitle = {2017 IEEE 7th International Advance Computing Conference (IACC)},
  title     = {BN-Based Approach for Predictive Admission Control of Cloud Services},
  year      = {2017},
  month     = {Jan},
  pages     = {59-64},
  abstract  = {A phenomenal growth in the demand for Cloud Computing services by the cloud consumers has necessitated the efficient and proactive management of the data center hosted services having varied characteristics. One of the major issues concerning both the cloud service providers and consumers is the provisioning of highest level of Quality of Service (QoS) under unpredictable service demands, while maintaining required revenue targets. Traditional Admission Control (AC) approaches which are usually mathematical or analytical in nature, have limited performance levels in the situations where service types, QoS parameters and user demands become highly unpredictable. To this end, an opportunity exists to utilize the self-learning capabilities of Machine Learning (ML) approaches to incorporate predictive and adaptive Admission Control of service requests without violating the Service Level Agreements (SLA) and simultaneously ensuring targeted revenue to the providers. This paper proposes, implements and evaluates a Bayesian Networks based predictive modeling framework (termed as BNSAC) to provide an autonomic Admission Control of cloud service requests. In summary, the BN-based model learns the historical behavior of the system involving various performance metrics (indicators) and predicts the desired unknown metric (e.g. SLA parameter) for making admission control decisions. It presents simulated experimental results involving various service demand scenarios which provide insights into the feasibility and applicability of the proposed approach for improving the QoS in the cloud computing setup.},
  doi       = {10.1109/IACC.2017.0027},
  issn      = {2473-3571},
  keywords  = {Cloud computing;Admission control;Predictive models;Measurement;Time factors;Bayes methods;Cloud Services;Admission Control;Bayesian Networks;Predictive Control},
}

@InProceedings{Lim2014,
  author    = {Lim, Erbin and Thiran, Philippe},
  booktitle = {2014 IEEE International Conference on Cloud Engineering},
  title     = {Communication of Technical QoS among Cloud Brokers},
  year      = {2014},
  month     = {March},
  pages     = {403-409},
  abstract  = {Service brokers are commonly used in the cloud computing paradigm to represent service requesters to select a service provider. They act as an intermediary between the two parties. One model of the cloud computing paradigm involves 3 layers, the user, the SaaS provider and the Cloud provider. The selection of service requesters is challenging due to the different levels of Quality of Service that each service provider can provide. In this paper we propose a unique mechanism that allows communication between service brokers in different layers in order to further improve this selection. In addition, we introduce a metric, efficiency, which service brokers can use to deterministically compare service providers with each other.},
  doi       = {10.1109/IC2E.2014.92},
  keywords  = {Quality of service;Monitoring;Measurement;Cloud computing;Availability;Computer architecture},
}

@InProceedings{Chatterjee2016a,
  author    = {Chatterjee, Subarna and Misra, Sudip},
  booktitle = {2016 IEEE Wireless Communications and Networking Conference},
  title     = {QoS estimation and selection of CSP in oligopoly environment for Internet of Things},
  year      = {2016},
  month     = {April},
  pages     = {1-6},
  abstract  = {This work focuses on an automated selection of Cloud Service Provider (CSP) for a naive end-user in an IoT scenario. In traditional cloud computing model, the end-users are knowledgeable about the Virtual Machines (VMs) and are technically aware of their requirements in terms of the computing cores, processing abilities, and storage requirements. In case of IoT, the users are envisioned to be widespread from naive, unsophisticated people to even objects or things who are devoid of the required knowledge and expertise. Further, in IoT technology, multiple Cloud Service Providers (CSPs) may possess the potential of serving an IoT application. Therefore, it is required for the end-user to judiciously select a single CSP based on the maximum obtainable Quality of Service (QoS) from a CSP. This work proposes an algorithm QoS based Automated Selection of CSP (QASeC) for automated selection of a CSP from a set of nominated CSPs based on the maximum achievable QoS. The work identifies and models the QoS parameters for every CSP and defines a QoS utility metric for each CSP. Based on the metric, the work proposes an optimization for selection of the appropriate CSP and the cloud gateway associated with it. From the obtained results, we infer the suitability of QASeC in real-life IoT scenarios.},
  doi       = {10.1109/WCNC.2016.7564810},
  issn      = {1558-2612},
  keywords  = {Internet of things;Quality of service;Delays;Cloud computing;Logic gates;Nickel;Uplink;Wireless Sensor Networks;Cloud Service Provider;Oligopoly;Internet of Things;Quality of Service},
}

@InProceedings{Lu2014,
  author    = {Yao Lu and Liu, Yao and Dey, Sujit},
  booktitle = {2014 International Conference on Computing, Networking and Communications (ICNC)},
  title     = {Enhancing Cloud Mobile 3D display gaming user experience by asymmetric graphics rendering},
  year      = {2014},
  month     = {Feb},
  pages     = {368-374},
  abstract  = {With the arrival of auto-stereoscopic 3D displays for mobile devices, and emergence of more 3D content, there is much anticipation for 3D mobile multimedia experiences, including 3D display gaming. Simultaneously, with the emergence of cloud computing, more mobile applications are being developed to take advantage of the elastic cloud resources. In this paper, we explore the possibility of developing Cloud-based 3D Mobile Gaming, where the 3D video rendering and encoding is performed on cloud servers, with the resulting 3D video streamed over wireless networks to mobile devices with 3D displays for a true 3D mobile gaming experience. However, with the significantly higher bit rate requirement for 3D video, ensuring user experience may be a challenge, both in terms of 3D video quality and network delay (response time), considering the bandwidth constraints and fluctuations of wireless networks. In this paper, we propose a new asymmetric graphics rendering approach which can significantly reduce the video encoding bit rate needed for a certain video quality, thereby making it easier to transmit the video over wireless network. However, since asymmetric rendering may also impair the graphics quality, we need to be able to understand and measure its impact. We conduct subjective tests to study and model the impairments due to asymmetric rendering and network delay, thereby developing a user experience model for cloud based mobile 3D display gaming. By conducting subsequent subjective tests, we prove the correctness of the impairment functions and the resulting user experience model. We also conduct experiments using real 4G-LTE network profile. Experimental results show that by making use of the user experience model, it is possible to set appropriate graphics rendering parameters according to network constraints, such that the user experience can be maintained to a high level.},
  doi       = {10.1109/ICCNC.2014.6785362},
  keywords  = {Three-dimensional displays;Rendering (computer graphics);Delays;Games;Mobile communication;Streaming media},
}

@Article{Lu2015,
  author   = {Lu, Yao and Liu, Yao and Dey, Sujit},
  journal  = {IEEE Journal of Selected Topics in Signal Processing},
  title    = {Cloud Mobile 3D Display Gaming User Experience Modeling and Optimization by Asymmetric Graphics Rendering},
  year     = {2015},
  issn     = {1941-0484},
  month    = {April},
  number   = {3},
  pages    = {517-532},
  volume   = {9},
  abstract = {With the arrival of auto-stereoscopic 3D displays for mobile devices, and emergence of more 3D content, there is much anticipation for 3D mobile multimedia experiences, including 3D display gaming. Simultaneously, with the emergence of cloud computing, more mobile applications are being developed to take advantage of the elastic cloud resources. In this paper, we explore the possibility of developing Cloud Mobile 3D Display Gaming, where the 3D video rendering and encoding is performed on cloud servers, with the resulting 3D video streamed over wireless networks to mobile devices with 3D displays for a true 3D mobile gaming experience. However, with the significantly higher bitrate requirement for 3D video, ensuring user experience may be a challenge, both in terms of 3D video quality and network delay (response time), considering the bandwidth constraints and fluctuations of wireless networks. In this paper, we propose a new asymmetric graphics rendering approach which can significantly reduce the video encoding bitrate needed for a certain video quality, thereby making it easier to transmit the video over wireless network. However, since asymmetric graphics rendering may also impair the graphics quality, we need to be able to understand and measure its impact. We conduct subjective tests to study and model the impairments due to asymmetric graphics rendering and network delay, thereby developing a user experience model for cloud based mobile 3D display gaming. By conducting subsequent subjective tests, we prove the correctness of the impairment functions and the resulting user experience model. Furthermore, given any network condition, we propose to solve the problem of selecting the optimal graphics rendering factors for the left and right views so as to maximize user experience of cloud mobile 3D display gaming. In order to solve this problem, we first develop a model to estimate the resulting video bitrate of the rendered 3D video when certain graphics rendering factors are used. Next, we derive a model to predict the delay given the available network bandwidth and the video bitrate of the rendered 3D video. We use the above two models together with a branch and bound algorithm to solve the optimization problem and determine the optimal values for the left and right view rendering factors. Experiments conducted using real 4G-LTE network profiles on commercial cloud service demonstrate the feasibility of significant improvement in user experience when the proposed optimization algorithm is used to dynamically select optimal rendering factors according to changing network conditions.},
  doi      = {10.1109/JSTSP.2015.2396475},
  keywords = {Three-dimensional displays;Rendering (computer graphics);Games;Streaming media;Mobile communication;Delays;3D;asymmetric graphics rendering;branch and bound;cloud Mobile Gaming;subjective testing;user experience},
}

@InProceedings{Fang2017a,
  author    = {Fang, Dongfeng and Ye, Feng and Qian, Yi and Sharif, Hamid},
  booktitle = {2017 IEEE International Conference on Electro Information Technology (EIT)},
  title     = {An efficient incentive mechanism for cloud-based mobile sensor network},
  year      = {2017},
  month     = {May},
  pages     = {229-234},
  abstract  = {Mobile sensor networks (MSNs) enable extensive applications of data collection, such as accident report in transportation and health prediction in public health. Incentive mechanism (IM) is applied for sensing user (SU) recruitment. However, the IM used in traditional MSN is not efficient due to limited information of SU used for recruitment. With the development of cloud computing technology, cloud-based MSN is the trend to use more information of SUs for IM design to improve its efficiency. In this paper, a novel cloud-based MSN model is presented. Three parties are considered, including data request party, cloud-based platform and SUs. A data quality model is proposed to measure the credit level of SUs. In addition, with consideration of social connections of SUs, a SU recruitment strategy is presented. SUs are divided into first and second degrees based on how they join the sensing task. The utility functions of first degree SUs and cloud-based platform are presented, respectively. At last, an efficient IM is proposed by formulating a Stackelberg game. The performance of the proposed IM on data quality and SU recruitment time comparing with other method are presented and discussed. The simulation results illustrate that the proposed IM ensures data quality for data request party and recruits SUs more efficiently.},
  doi       = {10.1109/EIT.2017.8053360},
  issn      = {2154-0373},
  keywords  = {Sensors;Recruitment;Data models;Cloud computing;Computational modeling;Games},
}

@InProceedings{Laghari2017,
  author    = {Laghari, Asif Ali and He, Hui and Shafiq, Muhammad and Khan, Asiya},
  booktitle = {2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN)},
  title     = {Impact of storage of mobile on quality of experience (QoE) at user level accessing cloud},
  year      = {2017},
  month     = {May},
  pages     = {1402-1409},
  abstract  = {Quality of Experience (QoE) is referred as level of user's satisfaction, enjoyment, learning and evaluation about the services or products. Recently quality of experience is used for improvement in product development life cycle after getting feedback from end users and service providers also use QoE to measure quality of services (QoS) of their services. Cloud computing provides services such as storage, web hosting, operating system environment, application development platform and CPU resources pay per use. End users are accessing cloud services via mobile apps, but enormous amount of temporary/cache data is generated by apps, so internal storage of mobile devices are filled quickly. Mobile device without any space in internal storage has huge impact on the performance when accessing the cloud services, which degrade the QoE of end users for particular cloud app and services. This paper presents the results of experiments conducted using two mobile devices HTC and Samsung to analyze the impact on end user's QoE during accessing cloud, when internal storage of HTC mobile device is filled and Samsung having 10 GB free space. Finally, on the basis of experimental results future changes in cloud apps are suggested for service provider to improve end user's QoE.},
  doi       = {10.1109/ICCSN.2017.8230340},
  issn      = {2472-8489},
  keywords  = {Cloud computing;Random access memory;Mobile handsets;Mobile communication;Memory management;Loading;qaulity of experience (QoE);performance;cloud service providers;mobile app;internal storage;WeChat},
}

@Article{Singh2020,
  author   = {Singh, Sukhpal and Chana, Inderveer and Buyya, Rajkumar},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {STAR: SLA-aware Autonomic Management of Cloud Resources},
  year     = {2020},
  issn     = {2168-7161},
  month    = {Oct},
  number   = {4},
  pages    = {1040-1053},
  volume   = {8},
  abstract = {Cloud computing has recently emerged as an important service to manage applications efficiently over the Internet. Various cloud providers offer pay per use cloud services that requires Quality of Service (QoS) management to efficiently monitor and measure the delivered services through Internet of Things (IoT) and thus needs to follow Service Level Agreements (SLAs). However, providing dedicated cloud services that ensure user's dynamic QoS requirements by avoiding SLA violations is a big challenge in cloud computing. As dynamism, heterogeneity and complexity of cloud environment is increasing rapidly, it makes cloud systems insecure and unmanageable. To overcome these problems, cloud systems require self-management of services. Therefore, there is a need to develop a resource management technique that automatically manages QoS requirements of cloud users thus helping the cloud providers in achieving the SLAs and avoiding SLA violations. In this paper, we present SLA-aware autonomic resource management technique called STAR which mainly focuses on reducing SLA violation rate for the efficient delivery of cloud services. The performance of the proposed technique has been evaluated through cloud environment. The experimental results demonstrate that STAR is efficient in reducing SLA violation rate and in optimizing other QoS parameters which effect efficient cloud service delivery.},
  doi      = {10.1109/TCC.2017.2648788},
  keywords = {Cloud computing;Quality of service;Resource management;Monitoring;Reliability;Service level agreements;Software as a service;Autonomic cloud;resource provisioning;cloud computing;resource scheduling;quality of service;service level agreement},
}

@InProceedings{Ma2020a,
  author    = {Ma, Kun and Bagula, Antoine and Ajayi, Olasupo and Nyirenda, Clement},
  booktitle = {ICC 2020 - 2020 IEEE International Conference on Communications (ICC)},
  title     = {Aiming at QoS: A Modified DE Algorithm for Task Allocation in Cloud Computing},
  year      = {2020},
  month     = {June},
  pages     = {1-7},
  abstract  = {The Cloud computing system is characterized by large scale servers being utilized by an even larger number of users. It is a system where there is the need to frequently and efficiently schedule and manage different application tasks, with varied service requirements. One of the challenges of Cloud computing is managing the quality of service (QoS) rendered to users, specifically scheduling tasks between users and Cloud resources in a timely manner. Cloud users usually have widely diverse QoS requirements and meeting these simultaneously is also a challenge. In this paper, in order to improve on Cloud resource allocation and specifically to tailor it towards meeting varied QoS requirements of users, we proposed a new algorithm which combines Differential Evolution with the Shapley Value economic mode. This combination allows us measure the contribution of each virtual machine (VM), so as to improve the probability of obtaining a better tasks-to-resource allocation thereby improving user satisfaction. From results of conducted experiments, when compared with the traditional DE (Differential Evolution) algorithm and the conventional task-VM binding policy in CloudSim, both for allocations where special QoS requirements are required and in instances of multiple QoS requirements; the modified Shapley value based DE algorithm (SVBDA) shows significant improvement.},
  doi       = {10.1109/ICC40277.2020.9148980},
  issn      = {1938-1883},
  keywords  = {Quality of service;Task analysis;Cloud computing;Resource management;Games;Bandwidth;Processor scheduling;Cloud computing;Differential evolution algorithm;Execution time/cost minimizing;NP hard problem;Quality of service (QoS);Shapley value;Tasks scheduling;Virtual machine},
}

@Article{Lyu2017,
  author   = {Lyu, Xinchen and Tian, Hui and Sengul, Cigdem and Zhang, Ping},
  journal  = {IEEE Transactions on Vehicular Technology},
  title    = {Multiuser Joint Task Offloading and Resource Optimization in Proximate Clouds},
  year     = {2017},
  issn     = {1939-9359},
  month    = {April},
  number   = {4},
  pages    = {3435-3447},
  volume   = {66},
  abstract = {Proximate cloud computing enables computationally intensive applications on mobile devices, providing a rich user experience. However, remote resource bottlenecks limit the scalability of offloading, requiring optimization of the offloading decision and resource utilization. To this end, in this paper, we leverage the variability in capabilities of mobile devices and user preferences. Our system utility metric is a measure of quality of experience (QoE) based on task completion time and energy consumption of a mobile device. We propose a heuristic offloading decision algorithm (HODA), which is semidistributed and jointly optimizes the offloading decision, and communication and computation resources to maximize system utility. Our main contribution is to reduce the problem to a submodular maximization problem and prove its NP-hardness by decomposing it into two subproblems: 1) optimization of communication and computation resources solved by quasiconvex and convex optimization and 2) offloading decision solved by submodular set function optimization. HODA reduces the complexity of finding the local optimum to O(K3), where K is the number of mobile users. Simulation results show that HODA performs within 5% of the optimal on average. Compared with other solutions, HODA's performance is significantly superior as the number of users increases.},
  doi      = {10.1109/TVT.2016.2593486},
  keywords = {Mobile communication;Cloud computing;Mobile handsets;Optimization;Base stations;Uplink;Energy consumption;Mobile cloud computing;multiuser offloading;proximate cloud;resource optimization},
}

@InProceedings{Zhou2015,
  author    = {Zhou, Nianjun and Mohindra, Ajay},
  booktitle = {2015 IEEE International Conference on Services Computing},
  title     = {Causality-Driven Performance Monitoring and Scaling Automation for Managed Solutions},
  year      = {2015},
  month     = {June},
  pages     = {467-474},
  abstract  = {A key feature of Cloud computing is its agility and flexibility to support the scalability needs of business solutions. Currently, the agility is only limited to the scalability of the compute, memory and storage. To improve an application's agility, we need to monitor & measure solution level metrics and associate the performance of the metrics to the business agility needs of the solution by making real-time scalability or change decisions. In this paper, we illustrate a scaling decision mechanism utilizing the monitoring data from infrastructure, middleware, and business level metrics. We use these performance metrics as input to a causality analysis model to make architecture changes or scalability decisions. Mathematically, we define the causality as a graph to link the changes in the measured metric values to the action of the solution change. The causality analysis follows scalability principles as best practices. They are a) the principle of performance scalability b) principle of contribution margin for scalability, and c) principle of the least cost of SLA compliance. We define these scalability principles as the rules to ensure that the business stakeholder of the solution can maintain or improve their business quality or profit margins as the computing capability scales up or down. To implement those principles, we need to establish the linkages of the business metrics to the decision of changes. To make such linkage, we first utilize causality analysis to identify feasible scaling actions, and then associate those actions with the system, application, and business performance metrics. With the help of causality analysis, we implement a performance monitoring and scaling automation framework for managed solutions using an Open Source Monitoring system.},
  doi       = {10.1109/SCC.2015.70},
  keywords  = {Scalability;Measurement;Monitoring;Business;Computer architecture;Media;Servers;Scalability;Business Monitoring;Performance Metrics;Service Level Agreements;Decision;Trade-Off},
}

@Article{Cedillo2021,
  author   = {Cedillo, Priscila and Insfran, Emilio and Abrahão, Silvia and Vanderdonckt, Jean},
  journal  = {IEEE Access},
  title    = {Empirical Evaluation of a Method for Monitoring Cloud Services Based on Models at Runtime},
  year     = {2021},
  issn     = {2169-3536},
  pages    = {55898-55919},
  volume   = {9},
  abstract = {Cloud computing is being adopted by commercial and governmental organizations driven by the need to reduce the operational cost of their information technology resources and search for a scalable and flexible way to provide and release their software services. In this computing model, the Quality of Services (QoS) is agreed between service providers and their customers through Service Level Agreements (SLA). There is thus a need for systematic approaches with which to assess the quality of cloud services and their compliance with the SLA. In previous work, we introduced a generic method for Monitoring cloud Services using models at RunTime (MoS@RT), which allows the monitoring requirements or the metric operationalizations of these requirements to be changed at runtime without the modification of the underlying infrastructure. In this paper, we present the design of a monitoring infrastructure that supports the proposed method with its instantiation to a specific platform and reports the results of an experiment carried out to evaluate the perceived efficacy of 58 undergraduate students when using the infrastructure to configure the monitoring of cloud services deployed on the Microsoft Azure platform. The results show that the participants perceived MoS@RT to be easy to use, useful, and they also expressed their intention to use the method in the future. Although further experiments must be carried out to strengthen these results, MoS@RT has proved to be a promising monitoring method for cloud services.},
  doi      = {10.1109/ACCESS.2021.3071417},
  keywords = {Monitoring;Cloud computing;Tools;Runtime;Quality of service;Measurement;Service level agreements;Cloud computing;models@runtime;quality of service (QoS);services monitoring;software as a service (SaaS)},
}

@InProceedings{Zhang2020a,
  author    = {Zhang, Yilei and Zhang, Xiao and Zhang, Peiyun and Luo, Jun},
  booktitle = {2020 IEEE International Conference on Services Computing (SCC)},
  title     = {Credible and Online QoS Prediction for Services in Unreliable Cloud Environment},
  year      = {2020},
  month     = {Nov},
  pages     = {272-279},
  abstract  = {With the widespread adoption of cloud computing, Service-Orientated Architecture (SOA) facilitates the deployment of large-scale online applications in many key areas where quality and reliability are critical. In order to ensure the performance of cloud applications, Quality of Service (QoS) is widely used as a key metric to enable QoS-driven service selection, composition, adaption, etc. Since QoS data observed by users is sparse due to technical constraints, previous studies have proposed prediction approaches to solve this problem. However, the dynamic nature of the cloud environment requires timely prediction of time-varying QoS values. In addition, unreliable QoS data from untrustworthy users may significantly affect the prediction accuracy. In this paper, we propose a credible online QoS prediction approach to address these challenges. We evaluate user credibility through a reputation mechanism and employ online learning techniques to provide QoS prediction results at runtime. The proposed approach is evaluated on a large-scale real-world QoS dataset, and the experimental results demonstrate its effectiveness and efficiency in unreliable cloud environment.},
  doi       = {10.1109/SCC49832.2020.00043},
  issn      = {2474-2473},
  keywords  = {Cloud computing;Runtime;Service computing;Quality of service;Real-time systems;Service-oriented architecture;Reliability;QoS Prediction;Cloud Services;Collaborative Filtering;Reputation;Online Learning},
}

@InProceedings{Pendlebury2016,
  author    = {Pendlebury, John and Emeakaroha, Vincent C. and O'Shea, David and Cafferkey, Neil and Morrison, John P. and Lynn, Theo},
  booktitle = {2016 2nd International Conference on Cloud Computing Technologies and Applications (CloudTech)},
  title     = {SOMBA - automated anomaly detection for Cloud quality of service},
  year      = {2016},
  month     = {May},
  pages     = {71-79},
  abstract  = {Cloud computing has transformed the standard model of service provisioning, allowing the delivery of on-demand services over the Internet. With its inherent requirements for elastic scalability and a pay-as-you-go pricing model, an additional level of complexity is added to its Quality of Service (QoS) management. This has made service provisioning more prone to performance anomalies due to the large-scale and evolving nature of Clouds. Existing methods for anomaly detection based on QoS monitoring in the Cloud rely on probabilistic methods, which are not computationally easy and are often valid for very short times before system dynamics change. We posit that more minimalistic approaches including automated techniques are needed for effective anomaly detection to support QoS enforcement in Clouds. In this paper, we present an automated anomaly detection scheme that recognises and adapts to changes in Clouds for efficient multi-metric performance anomaly detection to guarantee service quality. It includes a monitoring tool for collating performance data in real time for analysis and an anomaly detection technique based on an unsupervised machine learning strategy. Based on a Cloud service provisioning use case scenario, we evaluate our anomaly detection technique and compare it against two statistical anomaly detection approaches to demonstrate its efficiency.},
  doi       = {10.1109/CloudTech.2016.7847681},
  keywords  = {Monitoring;Quality of service;Cloud computing;Visualization;Resource management},
}

@Article{Belgaum2020,
  author   = {Belgaum, Mohammad Riyaz and Musa, Shahrulniza and Alam, Muhammad Mansoor and Su’ud, Mazliham Mohd},
  journal  = {IEEE Access},
  title    = {A Systematic Review of Load Balancing Techniques in Software-Defined Networking},
  year     = {2020},
  issn     = {2169-3536},
  pages    = {98612-98636},
  volume   = {8},
  abstract = {The traditional networks are facing difficulties in managing the services offered by cloud computing, big data, and the Internet of Things as the users have become more dependent on their services. Software-Defined Networking (SDN) has pulled enthusiasm in the integration process of technologies and function as per the user's requirements for both academia and industry, and it has begun to be embraced in actual framework usage. The emergence of SDN has given another idea to empower the focal programmability of the system. Because of the increasing demand and the scarcity of resources, the load balancing issue needs to be addressed efficiently to manage the incoming traffic and resources and to improve network performance. One of the most critical issues is the role of the controller in SDN to balance the load for having a better Quality of Service (QoS). Though there are few survey articles written on load balancing, there is no detail and systematic review conducted in load balancing in SDN. Hence, this paper extends and reviews the discussion with a taxonomy of current emerging load balancing techniques in SDN systematically by categorizing the techniques as conventional and artificial intelligence-based techniques to improve the service quality. The review also includes the study of metrics and parameters which have been used to measure the performance. This review would allow gaining more information on load balancing approaches in SDN and enables the researchers to fill the current research gaps.},
  doi      = {10.1109/ACCESS.2020.2995849},
  keywords = {Load management;Quality of service;Switches;Systematics;Measurement;Software;Artificial intelligence;conventional;load balancing;review;SDN;software-defined networking;systematic},
}

@InProceedings{Wong2019,
  author    = {Wong, Tong-Sheng and Chan, Gaik-Yee and Chua, Fang-Fang},
  booktitle = {2019 International Conference on Information Networking (ICOIN)},
  title     = {Adaptive Preventive and Remedial Measures in Resolving Cloud Quality of Service Violation},
  year      = {2019},
  month     = {Jan},
  pages     = {473-479},
  abstract  = {Cloud Computing acts as a paradigm to support on-demand computing services, from applications to storage, manage and processing capabilities. One of the major challenges in delivering and accessing cloud applications is the management of Quality of Service (QoS) and cloud service providers are mandated to adhere to Service Level Agreement (SLA) in providing quality cloud services to the users. The agreement matching is important for both parties to ensure satisfaction and expectation level. This proposed work aims to resolve cloud QoS violation with the implementation of adaptive preventive and remedial mechanisms. Preventive measure such as horizontal scaling is used to optimize the performance of a running cloud service in order to prevent the cloud service to downgrade to QoS violation condition. Remedial action on the other hand, is to provide fault tolerance using replication for faulty cloud service to recover from failure incidents or already violation condition. Experimental results have demonstrated the feasibility and effectiveness of applying horizontal scaling in preventing and replication in rectifying cloud QoS violations based on response time and throughput.},
  doi       = {10.1109/ICOIN.2019.8718133},
  issn      = {1976-7684},
  keywords  = {Cloud computing;Quality of service;Fault tolerance;Fault tolerant systems;Time factors;Scalability;Throughput;Cloud Computing;Fault Tolerance;Quality of Service Violation;Replication;Scalability},
}

@Article{Liu2019a,
  author   = {Liu, Ying and Wang, Ke and Ge, Liang and Ye, Lei and Cheng, Jingde},
  journal  = {IEEE Access},
  title    = {Adaptive Evaluation of Virtual Machine Placement and Migration Scheduling Algorithms Using Stochastic Petri Nets},
  year     = {2019},
  issn     = {2169-3536},
  pages    = {79810-79824},
  volume   = {7},
  abstract = {More and more mobile applications rely on the combination of both mobile and cloud computing technology to bring out their full potential. The cloud is usually used for providing additional computing resources that cannot be handled efficiently by the mobile devices. Cloud usage, however, results in several challenges related to the management of virtualized resources. A large number of scheduling algorithms are proposed to balance between performance and cost of data center. Due to huge cost and time consuming of measure-based and simulation method, this paper proposes an adaptive method to evaluate scheduling algorithms. In this method, the virtual machine placement and migration process are modeled by using Stochastic Reward Nets. Different scheduling methods are described as reward functions to perform the adaptive evaluation. Two types of performance metrics are also discussed: one is about quality of service, such as system availability, mean waiting time, and mean service time, and the other is the cost of runtime, such as energy consumption and cost of migration. Compared to a simulation method, the analysis model in this paper only modifies the reward function for different scheduling algorithms and does not need to reconstruct the process. The numeric results suggest that it also has a good accuracy and can quantify the influence of scheduling algorithms on both quality of service and cost of runtime.},
  doi      = {10.1109/ACCESS.2019.2923592},
  keywords = {Scheduling algorithms;Cloud computing;Servers;Adaptation models;Stochastic processes;Virtual machining;Quality of service;Adaptive evaluation;virtual machine placement and migration;scheduling algorithms;stochastic reward net},
}

@InProceedings{Wang2020a,
  author    = {Wang, Chengrong and Zhang, Xiaodong and Chu, Dianhui},
  booktitle = {2020 5th International Conference on Computational Intelligence and Applications (ICCIA)},
  title     = {Research on Service Composition Optimization Method Based on Composite Services QoS},
  year      = {2020},
  month     = {June},
  pages     = {206-210},
  abstract  = {With the development of Cloud Computing, Internet of Things, and the advent of the era of Big Data, the types and scale of services are getting larger and larger, and the problem space of service composition is exploding. In order to measure the composite services quality of different combination schemes, this paper shows the calculation method of composite services QoS (Quality of Service), and improves the Ant Colony Algorithm by introducing Skyline calculation to further improve the efficiency of service composition and respond to user quickly. Finally, it is verified on the real QoS data set, and the feasibility and effectiveness of the method are proved through experiments.},
  doi       = {10.1109/ICCIA49625.2020.00046},
  keywords  = {Quality of service;Throughput;Task analysis;Time factors;Genetic algorithms;Computer science;Electronic mail;QoS;ant colony algorithm;service composition;skyline},
}

@Article{Shi2017,
  author   = {Shi, Lei and Shi, Yi and Wei, Xing and Ding, Xu and Wei, Zhenchun},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  title    = {Cost Minimization Algorithms for Data Center Management},
  year     = {2017},
  issn     = {1558-2183},
  month    = {Jan},
  number   = {1},
  pages    = {60-71},
  volume   = {28},
  abstract = {Due to the increasing usage of cloud computing applications, it is important to minimize energy cost consumed by a data center, and simultaneously, to improve quality of service via data center management. One promising approach is to switch some servers in a data center to the idle mode for saving energy while to keep a suitable number of servers in the active mode for providing timely service. In this paper, we design both online and offline algorithms for this problem. For the offline algorithm, we formulate data center management as a cost minimization problem by considering energy cost, delay cost (to measure service quality), and switching cost (to change servers's active/idle mode). Then, we analyze certain properties of an optimal solution which lead to a dynamic programming based algorithm. Moreover, by revising the solution procedure, we successfully eliminate the recursive procedure and achieve an optimal offline algorithm with a polynomial complexity. For the online algorithm, We design it by considering the worst case scenario for future workload. In simulation, we show this online algorithm can always provide near-optimal solutions.},
  doi      = {10.1109/TPDS.2016.2549016},
  keywords = {Servers;Algorithm design and analysis;Switches;Heuristic algorithms;Optimization;Minimization;Delays;Data center management;offline algorithm;dynamic programming;online algorithm},
}

@Article{Zhu2020,
  author   = {Zhu, Zongwei and Han, Guangjie and Jia, Gangyong and Shu, Lei},
  journal  = {IEEE Internet of Things Journal},
  title    = {Modified DenseNet for Automatic Fabric Defect Detection With Edge Computing for Minimizing Latency},
  year     = {2020},
  issn     = {2327-4662},
  month    = {Oct},
  number   = {10},
  pages    = {9623-9636},
  volume   = {7},
  abstract = {As an essential step in quality control, fabric defect detection plays an important role in the textile manufacturing industry. The traditional manual detection method is inaccurate and incurs a high cost; as a result, it is gradually being replaced by deep learning algorithms based on cloud computing. However, a high data transmission latency between end devices and the cloud has a significant impact on textile production efficiency. In contrast, edge computing, which provides services near end devices by deploying network, computing and storage facilities at the edge of the Internet, can effectively solve the above-mentioned problem. In this article, we propose a deep-learning-based fabric defect detection method for edge computing scenarios. First, this article modifies the structure of DenseNet to better suit a resource-constrained edge computing scenario. To better assess the proposed model, an optimized cross-entropy loss function is also formulated. Afterward, six feasible expansion schemes are utilized to enhance the data set according to the characteristics of various defects in fabric samples. To balance the distribution of samples, proportions of various defect types are used to determine the number of enhancements. Finally, a fabric defect detection system is established to test the performance of the optimized model used on edge devices in a real-world textile industry scenario. Experimental results demonstrate that compared with the conventional convolutional neural network (CNN), the proposed optimized model attains an average improvement of 18% in the area under the curve (AUC) metric for 11 defects. Data transmission is reduced by approximately 50% and latency is reduced by 32% in the Cambricon 1H8 platform compared with a cloud platform.},
  doi      = {10.1109/JIOT.2020.2983050},
  keywords = {Fabrics;Production;Computational modeling;Edge computing;Cloud computing;Image edge detection;Adaptation models;Convolutional neural network (CNN);edge computing;fabric defect detection;image processing},
}

@InProceedings{Ma2018,
  author    = {Ma, Kun and Bagula, Antoine and Mauwa, Hope and Celesti, Antonio},
  booktitle = {2018 IEEE 6th International Conference on Future Internet of Things and Cloud (FiCloud)},
  title     = {Modelling Cloud Federation: A Fair Profit Distribution Strategy Using the Shapley Value},
  year      = {2018},
  month     = {Aug},
  pages     = {393-398},
  abstract  = {Cloud computing provides software (Software as a Service), platform (Platform as a Service) and infrastructure (Infrastructure as a Service) services to its users by integrating IT resources into a large-scale and scalable resource pool through the virtualisation technology. However, the single cloud resource provider model currently implemented by many providers may fail short to meet the dynamic nature of cloud users' requirements. Cloud federation can mitigate this issue by optimising cloud resource allocation through sharing and re-usability of available resources. This paper revisit the problem of cloud engineering by tackling the key issue of the fair distribution of profit between cloud resource providers, which, to the best of our knowledge, has only been scarcely addressed by the research and practitioners' community. We propose a method that enables the cloud federation to map the contribution of resources of the participants to the federations into a quality of service metric used to achieve a cloud federation. Building upon a federation game implementation, we reveal the possibilities and benefits of different federation compositions using the Shapley value of each resource provider as a way of implementing a fair profit sharing strategy. Using extensions of the CloudSim package, we present simulation results that demonstrate the fairness of our proposed method and strategy.},
  doi       = {10.1109/FiCloud.2018.00063},
  keywords  = {Cloud computing;Quality of service;Task analysis;Resource management;Games;Reliability;Security;cloud computing, federation, game theory, tasks scheduling, fair profit distribution, resource allocation, quality of service (QoS)},
}

@Article{Fantacci2020,
  author   = {Fantacci, Romano and Picano, Benedetta},
  journal  = {IEEE Transactions on Vehicular Technology},
  title    = {Performance Analysis of a Delay Constrained Data Offloading Scheme in an Integrated Cloud-Fog-Edge Computing System},
  year     = {2020},
  issn     = {1939-9359},
  month    = {Oct},
  number   = {10},
  pages    = {12004-12014},
  volume   = {69},
  abstract = {The recent growth in intensive services and applications demand has triggered the functional integration of cloud computing with edge computing capabilities. One of the main goals is to allow a fast processing to tasks with strict real time constraints in order to lower the task dropping probability due to expiration of the associated deadlines. This paper deals with the performance evaluation and optimization of a three layers cloud-fog-edge computing infrastructure by resorting to the use of queueing theory results. In particular, a Markov queueing system model with reneging is proposed for the cloud subsystem, in order to consider the premature computation requests departure due to their deadline expiration. Furthermore, a computational resources allocation method is proposed with the aim at maximizing the social welfare metric, constrained to specific quality of service requirements. Finally, the proposed queueing theory analysis as well as of the computational resources allocation approach is validated by comparing the obtained analytical predictions with simulation results.},
  doi      = {10.1109/TVT.2020.3008926},
  keywords = {Cloud computing;Computational modeling;Optimization;Queueing analysis;Delays;Servers;Edge computing;Mobile computing;offloading;queueing theory;performance optimization},
}

@InProceedings{Ran2014,
  author    = {Ran, Yongyi and Shi, Youkang and Yang, Enzhong and Chen, Shuangwu and Yang, Jian},
  booktitle = {2014 IEEE Globecom Workshops (GC Wkshps)},
  title     = {Dynamic resource allocation for video transcoding with QoS guaranteeing in cloud-based DASH system},
  year      = {2014},
  month     = {Dec},
  pages     = {144-149},
  abstract  = {Due to diverse network conditions and heterogeneous devices, there may be various video demands with different video qualities and formats from the client side. Compared to keeping all necessary copies for the same video, video transcoding in real-time should be an essential solution. The complex nature of video transcoding enables cloud computing to be uniquely suitable for dynamically providing transcoding resource. However, due to the fluctuation and uncertainty of the future transcoding demand, it is still a challenge to dynamically determine the optimal resource allocation to save cost while guaranteeing the Quality of Service (QoS). Overload may result in the transcoding jitter and increase the lateness which directly affects video freezes while over-provisioning naturally increases the cost. To address this problem, in this paper, by defining the transcoding jitter probability as a metric of QoS, we proposed a dynamic resource allocation algorithm based on the large deviation principle, which is capable of proactive calculating the optimal number of transcoding nodes for the upcoming transcoding demand subject to the transcoding jitter probability below a desired threshold. Finally, the experiments are performed on a cloud-based prototype system to show the attainable performance of the proposed resource allocation algorithm and verify that the proposed algorithm can make a good tradeoff between cost saving and QoS guaranteeing.},
  doi       = {10.1109/GLOCOMW.2014.7063421},
  issn      = {2166-0077},
  keywords  = {Transcoding;Streaming media;Jitter;Quality of service;Bit rate;Heuristic algorithms;Dynamic scheduling;Dynamic resource allocation;video transcoding;cloud computing;quality of service;cost saving},
}

@InProceedings{AlJawad2015,
  author    = {Al-Jawad, Ahmed and Trestian, Ramona and Shah, Purav and Gemikonakli, Orhan},
  booktitle = {Proceedings of the 2015 1st IEEE Conference on Network Softwarization (NetSoft)},
  title     = {BaProbSDN: A probabilistic-based QoS routing mechanism for Software Defined Networks},
  year      = {2015},
  month     = {April},
  pages     = {1-5},
  abstract  = {Over the past decade there has been an exponential increase in the Internet traffic especially with the proliferation of cloud computing and other distributed data services. This explosion of data traffic with its dynamically changing traffic patterns and flows might result in degradation of the network performance. In this context, there is a need for an intelligent and efficient network management system that delivers guaranteed services. To this extent, this paper proposes BaProbSDN, a probabilistic Quality of Service (QoS) routing mechanism for Software Defined Networks (SDN). The QoS routing algorithm employs the bandwidth availability metric as a QoS routing constraint for unicast data delivery. BaProbSDN makes use of Bayes' theorem and Bayesian network model to determine the link probability in order to select the route that satisfies the given bandwidth constraint. The performance of the proposed probabilistic QoS routing algorithm was tested in a simulation-based environment and compared against the widest-shortest path routing (WSR) algorithm. The results demonstrate that BaProbSDN can achieve up to 8.02% decrease in the bandwidth blocking rate when compared to WSR in the presence of link update inaccuracies of threshold and time delay.},
  doi       = {10.1109/NETSOFT.2015.7116128},
  keywords  = {Bandwidth;Quality of service;Routing;Measurement;Control systems;Probabilistic logic;Bayes methods;Software Defined Networks;OpenFlow;Quality of Service Routing;Bayesian Network},
}

@InProceedings{Tchernykh2014,
  author    = {Tchernykh, Andrei and Lozano, Luz and Schwiegelshohn, Uwe and Bouvry, Pascal and Pecero, Johnatan E. and Nesmachnow, Sergio},
  booktitle = {2014 IEEE 3rd International Conference on Cloud Networking (CloudNet)},
  title     = {Bi-objective online scheduling with quality of service for IaaS clouds},
  year      = {2014},
  month     = {Oct},
  pages     = {307-312},
  abstract  = {This paper focuses on the bi-objective experimental analysis of online scheduling in the Infrastructure as a Service model of Cloud computing. In this model, customer have the choice between different service levels. Each service level is associated with a price per unit of job execution time and a slack factor that determines the maximal time span to deliver the requested amount of computing resources. It is responsibility of the system and its scheduling algorithm to guarantee the corresponding quality of service for all accepted jobs. We do not consider any optimistic scheduling approach, that is, a job cannot be accepted if its service guarantee will not be observed assuming that all accepted jobs receive the requested resources. We analyze several scheduling algorithms with different cloud configurations and workloads and use the maximization of the provider income and minimization of the total power consumption of a schedule as additional objectives. Therefore, we cannot expect finding a unique solution to a given problem but a set of nondominated solutions also known as Pareto optima. Then we assess the performance of different scheduling algorithms by using a set coverage metric to compare them in terms of Pareto dominance. Based on the presented case study, we claim that a simple algorithm can provide the best energy and income trade-offs. This scheduling algorithm performs well in different scenarios with a variety of workloads and cloud configurations.},
  doi       = {10.1109/CloudNet.2014.6969013},
  keywords  = {Power demand;Degradation;Measurement;Processor scheduling;Educational institutions;Energy efficiency;Resource management;Cloud computing;Service Level Agreement;Energy Efficiency;Scheduling},
}

@InProceedings{Hussain2020,
  author    = {Hussain, Razin Farhan and Pakravan, Alireza and Salehi, Mohsen Amini},
  booktitle = {2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
  title     = {Analyzing the Performance of Smart Industry 4.0 Applications on Cloud Computing Systems},
  year      = {2020},
  month     = {Dec},
  pages     = {11-18},
  abstract  = {Cloud-based Deep Neural Network (DNN) applications that make latency-sensitive inference are becoming an indispensable part of Industry 4.0. Due to the multi-tenancy and resource heterogeneity, both inherent to the cloud computing environments, the inference time of DNN-based applications are stochastic. Such stochasticity, if not captured, can potentially lead to low Quality of Service (QoS) or even a disaster in critical sectors, such as Oil and Gas industry. To make Industry 4.0 robust, solution architects and researchers need to understand the behavior of DNN-based applications and capture the stochasticity exists in their inference times. Accordingly, in this study, we provide a descriptive analysis of the inference time from two perspectives. First, we perform an application-centric analysis and statistically model the execution time of four categorically different DNN applications on both Amazon and Chameleon clouds. Second, we take a resource-centric approach and analyze a rate-based metric in form of Million Instruction Per Second (MIPS) for heterogeneous machines in the cloud. This non-parametric modeling, achieved via Jackknife and Bootstrap re-sampling methods, provides the confidence interval of MIPS for heterogeneous cloud machines. The findings of this research can be helpful for researchers and cloud solution architects to develop solutions that are robust against the stochastic nature of the inference time of DNN applications in the cloud and can offer a higher QoS to their users and avoid unintended outcomes.},
  doi       = {10.1109/HPCC-SmartCity-DSS50907.2020.00003},
  keywords  = {Measurement;Cloud computing;Analytical models;Oils;High performance computing;Conferences;Neural networks;Deep Neural Network Applications;Industry 4.0;Cloud Platform;Heterogeneous Machines;Inference Time},
}

@InProceedings{AlSaidAhmad2018,
  author    = {Al-Said Ahmad, Amro and Andras, Peter},
  booktitle = {2018 Fifth International Symposium on Innovation in Information and Communication Technology (ISIICT)},
  title     = {Measuring and Testing the Scalability of Cloud-based Software Services},
  year      = {2018},
  month     = {Oct},
  pages     = {1-8},
  abstract  = {Performance and scalability testing and measurements of cloud-based software services are critically important in the context of rapid growth of cloud computing and supporting the delivery of these services. Cloud-based software services performance aspects are interrelated, both elasticity and efficiency are depending on the delivery of a sufficient level of scalability performance. In this work, we focused on testing and measuring the scalability of cloud-based software services in technical terms. This paper uses technical scalability metrics that address both volume and quality scaling, that inspired by earlier technical metrics of elasticity. We show how our technical scalability metrics can be integrated into an earlier utility oriented metric of scalability. We demonstrate the application of the metrics using a practical example and discuss the importance of them.},
  doi       = {10.1109/ISIICT.2018.8613297},
  keywords  = {Scalability;Software;Elasticity;Software measurement;Testing;Cloud computing;Measurement;Performance;Testing;Scalability;Software-as-a-Service (SaaS);Metrics},
}

@InProceedings{Zhu2018,
  author    = {Zhu, Hongbin and Wang, Haifeng and Luo, Xiliang and Qian, Hua},
  booktitle = {2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)},
  title     = {AN ONLINE LEARNING APPROACH TO WIRELESS COMPUTATION OFFLOADING},
  year      = {2018},
  month     = {Nov},
  pages     = {678-682},
  abstract  = {Fog computing extends cloud computing and services to the edge of networks, bringing advantages of the cloud closer to where data is created and acted upon. To support real time applications, latency performance is a crucial metric in fog computing. In this paper, we consider a sequential decision-making problem for computation offloading with unknown dynamics in which a mobile user offloads its arrival tasks to associated fog nodes (FNs) at each time slot. The queue of arrival tasks at each FN is modeled as a Markov chain. In order to provide satisfactory quality of experience, the network latency, which is directly associated with the queue condition, needs to be minimized. Taking advantage of reinforcement learning, the sequential decision-making problem is formulated as a restless multi-armed bandit problem. We construct a policy with interleaved exploration and exploitation stages, which achieves a regret with sub-linear order. Both analytical and simulation results validate the effectiveness of the proposed method in dealing with sequential decision-making problem.},
  doi       = {10.1109/GlobalSIP.2018.8646562},
  keywords  = {Task analysis;Markov processes;Edge computing;Cloud computing;Quality of experience;Decision making;Optimization;Fog computing;quality of experience (QoE);restless multi-armed bandit (RMAB);Markov chain},
}

@InProceedings{Si2014,
  author    = {Si, Pengbo and Yu, F. Richard and Zhang, Yanhua},
  booktitle = {2014 IEEE International Conference on Communications (ICC)},
  title     = {Joint cloud and radio resource management for video transmissions in mobile cloud computing networks},
  year      = {2014},
  month     = {June},
  pages     = {2270-2275},
  abstract  = {In mobile cloud computing (MCC) systems, the resource in both the cloud and the mobile network should be carefully managed. Cloud resource management and radio resource management have traditionally been addressed separately in previous works. In this paper, we propose to jointly study dynamic cloud and radio resource management so as to improve end-to-end performance of adaptive video transmissions in MCC systems. Video application quality of service performance, distortion, is adopted as the performance measure. An important video application layer parameter, intra-refreshing rate, is optimized to improve the video distortion performance. We formulate the problem as a stochastic restless bandits optimization problem, which facilitates the distributed MCC architecture and simplifies the computation and implementation due to its “indexibility” property. Simulation results are presented to show the effectivenes of the proposed scheme.},
  doi       = {10.1109/ICC.2014.6883661},
  issn      = {1938-1883},
  keywords  = {Servers;Mobile communication;Mobile computing;Resource management;Cloud computing;Bandwidth;Wireless communication},
}

@InProceedings{Toka2017,
  author    = {Toka, Lászlíó and Lajtha, Balázs and Hosszu, Éva and Formanek, Bence and Géhberger, Dániel and Tapolcai, János},
  booktitle = {IEEE INFOCOM 2017 - IEEE Conference on Computer Communications},
  title     = {A resource-aware and time-critical IoT framework},
  year      = {2017},
  month     = {May},
  pages     = {1-9},
  abstract  = {Internet of Things (IoT) systems produce great amount of data, but usually have insufficient resources to process them in the edge. Several time-critical IoT scenarios have emerged and created a challenge of supporting low latency applications. At the same time cloud computing became a success in delivering computing as a service at affordable price with great scalability and high reliability. We propose an intelligent resource allocation system that optimally selects the important IoT data streams to transfer to the cloud for processing. The optimization runs on utility functions computed by predictor algorithms that forecast future events with some probabilistic confidence based on a dynamically recalculated data model. We investigate ways of reducing specifically the upload bandwidth of IoT video streams and propose techniques to compute the corresponding utility functions. We built a prototype for a smart squash court and simulated multiple courts to measure the efficiency of dynamic allocation of network and cloud resources for event detection during squash games. By continuously adapting to the observed system state and maximizing the expected quality of detection within the resource constraints our system can save up to 70% of the resources compared to the naive solution.},
  doi       = {10.1109/INFOCOM.2017.8057143},
  keywords  = {Cloud computing;Bandwidth;Streaming media;Cameras;Quality of service;Uplink;Resource management;Internet of Things;cloud computing;cloud control;resource provisioning;adaptive;dynamic;QoS;QoE},
}

@InProceedings{Mazza2014,
  author    = {Mazza, Daniela and Tarchi, Daniele and Corazza, Giovanni E.},
  booktitle = {2014 IEEE Global Communications Conference},
  title     = {A user-satisfaction based offloading technique for smart city applications},
  year      = {2014},
  month     = {Dec},
  pages     = {2783-2788},
  abstract  = {The Smart cities applications are gaining an increasing interest among administrations, citizens and technologists for their suitability in managing the everyday life. One of the major challenges is regarding the possibility of managing in an efficient way the presence of multiple applications in a Wireless Heterogeneous Network (HetNet) environment, alongside the presence of a Mobile Cloud Computing (MCC) infrastructure. In this context we propose a utility function model derived from the economic world aiming to measure the Quality of Service (QoS), in order to choose the best access point in a HetNet to offload part of an application on the MCC, aiming to save energy for the Smart Mobile Devices (SMDs) and to reduce computational time. We distinguish three different types of application, considering different offloading percentage of computation and analyzing how the cell association algorithm allows energy saving and shortens computation time. The results show that when the network is overloaded, the proposed utility function allows to respect the target values by achieving higher throughput values, and reducing the energy consumption and the computational time.},
  doi       = {10.1109/GLOCOM.2014.7037229},
  issn      = {1930-529X},
  keywords  = {Throughput;Quality of service;Mobile communication;Servers;Mobile computing;Mobile handsets;Energy consumption},
}

@Article{Sacco2021,
  author   = {Sacco, Alessio and Flocco, Matteo and Esposito, Flavio and Marchetto, Guido},
  journal  = {IEEE Transactions on Network and Service Management},
  title    = {Supporting Sustainable Virtual Network Mutations With Mystique},
  year     = {2021},
  issn     = {1932-4537},
  month    = {Sep.},
  number   = {3},
  pages    = {2714-2727},
  volume   = {18},
  abstract = {The abiding attempt of automation has also permeated the networks, with the ability to measure, analyze, and control themselves in an automated manner, by reacting to changes in the environment (e.g., demand). When provided with these features, networks are often labeled as “self-driving” or “autonomous”. In this regard, the provision and orchestration of physical or virtual resources are crucial for both Quality of Service (QoS) guarantees and cost management in the edge/cloud computing environment. To effectively manage the lifecycle of these resources, an auto-scaling mechanism is essential. However, traditional threshold-based and recent Machine Learning (ML)-based policies are often unable to address the soaring complexity of networks due to their centralized approach. By relying on multi-agent reinforcement learning, we propose Mystique, a solution that learns from the load on links to establish the minimal set of active network resources. As traffic demands ebb and flow, our adaptive and self-driving solution can scale up and down and also react to failures in a fully automated, flexible, and efficient manner. Our results demonstrate that the presented solution can reduce network energy consumption while providing an adequate service level, outperforming other benchmark auto-scaling approaches.},
  doi      = {10.1109/TNSM.2021.3059647},
  keywords = {Reinforcement learning;Quality of experience;Load modeling;Topology;Routing;Monitoring;Complexity theory;SDN;reinforcement learning;auto-scaling;network management},
}

@InProceedings{Winzinger2020,
  author    = {Winzinger, Stefan and Wirtz, Guido},
  booktitle = {2020 IEEE International Conference on Service Oriented Systems Engineering (SOSE)},
  title     = {Applicability of Coverage Criteria for Serverless Applications},
  year      = {2020},
  month     = {Aug},
  pages     = {49-56},
  abstract  = {Serverless computing is a popular trend in cloud computing based on serverless functions. These functions are stateless which can be utilized by the cloud platform provider to scale functions dynamically. While these small functions are easy to test in isolation, integrating them with other resources provided by the cloud platform provider or third parties creates a complex system whose emerging behavior is hard to test. Integration tests help test the interaction of the serverless functions with other resources and their environment. However, it is hard to decide if a test case is adequate and focuses on the critical parts of the system. Therefore, coverage criteria can be used to measure the coverage of the relevant software components and help assess the quality of the test suite. In this paper, we identified serverless applications based on serverless functions on GitHub and used them to investigate which coverage criteria can be used to capture the interaction of serverless functions with other resources. Furthermore, we show a general approach to implement the measurement of the coverage on FaaS platforms. Thus, developers have means to test the adequacy of their applications on any FaaS platform.},
  doi       = {10.1109/SOSE49046.2020.00013},
  issn      = {2642-6587},
  keywords  = {Memory;FAA;Cloud computing;Logic gates;Biological system modeling;Data models;Focusing;Serverless Computing;FaaS;Integration Testing;Coverage Criteria},
}

@InProceedings{Ekanayake2017,
  author    = {Ekanayake, Wijaya and Amarasinghe, Heli and Karmouch, Ahmed},
  booktitle = {2017 14th IEEE Annual Consumer Communications Networking Conference (CCNC)},
  title     = {SDN-based IaaS for mobile computing},
  year      = {2017},
  month     = {Jan},
  pages     = {179-184},
  abstract  = {Mobile Cloud Computing enables resource limited mobile devices to support rich application services. Among three types of cloud services, Infrastructure-as-a-Service (IaaS) clouds provides compute infrastructure for mobile applications on demand. In IaaS-based mobile clouds, latency and bandwidth requirements can considered as critical factors impacting Quality of Service (QoS). Opposed to centralized clouds, geographically distributed clouds realize higher QoS benefiting the proximity to the end user. In this paper, we propose an IaaS framework with regional datacenters for mobile clouds. With the benefits of software-defined networking (SDN), we address impacts on QoS during mobility by serving mobile user via the optimum datacenter. A test-bed was developed to measure the performance of service allocation and relocation in proposed framework.},
  doi       = {10.1109/CCNC.2017.7983102},
  issn      = {2331-9860},
  keywords  = {Cloud computing;Mobile communication;Quality of service;IP networks;Mobile computing;Mobile handsets;Resource management;Cloud Computing;Mobile Clouds;Infrastructure-as-a-Service;Software-defined-networking},
}

@Article{Nguyen2019,
  author   = {Nguyen, Tien-Dung and Huh, Eui-Nam and Jo, Minho},
  journal  = {IEEE Internet of Things Journal},
  title    = {Decentralized and Revised Content-Centric Networking-Based Service Deployment and Discovery Platform in Mobile Edge Computing for IoT Devices},
  year     = {2019},
  issn     = {2327-4662},
  month    = {June},
  number   = {3},
  pages    = {4162-4175},
  volume   = {6},
  abstract = {Mobile edge computing (MEC) is used to offload services (tasks) from cloud computing in order to deliver those services to mobile Internet of Things (IoT) devices near mobile edge nodes. However, even though there are advantages to MEC, we face many significant problems, such as how a service provider (SP) deploys requested services efficiently on a destination MEC node, and how to discover existing services in neighboring MEC nodes to save edge resources. In this paper, we present a decentralized and revised content-centric networking (CCN)-based MEC service deployment/discovery protocol and platform. We organized a gateway in every area according to a three-tiered hierarchical MEC network topology to reduce computing overhead at the centralized controller. We revised CCN to introduce a protocol to help SP deploy their service on MEC node and assist MEC node discover services in neighboring nodes. By using our proposed protocol, MEC nodes can deploy or discover the requested service instances in the proximity of IoT devices to reduce transmission delay. We also present a mathematical model to calculate the round trip time to guarantee quality of service. Numerical experiments measure the performance of our proposed method with various mobile IoT device services. The results show that the proposed service deployment protocol and platform reduce the average service delay by up to 50% compared to legacy cloud. In addition, the proposed method outperforms the legacy protocol of the MEC environment in service discovery time.},
  doi      = {10.1109/JIOT.2018.2875489},
  keywords = {Cloud computing;Protocols;Edge computing;Internet of Things;Quality of service;Delays;5G mobile communication;Cloud;fog;Internet of Things (IoT);mobile edge computing (MEC);offloading;service deployment;service discovery},
}

@Article{He2018,
  author   = {He, Jianhua and Wei, Jian and Chen, Kai and Tang, Zuoyin and Zhou, Yi and Zhang, Yan},
  journal  = {IEEE Internet of Things Journal},
  title    = {Multitier Fog Computing With Large-Scale IoT Data Analytics for Smart Cities},
  year     = {2018},
  issn     = {2327-4662},
  month    = {April},
  number   = {2},
  pages    = {677-686},
  volume   = {5},
  abstract = {Analysis of Internet of Things (IoT) sensor data is a key for achieving city smartness. In this paper a multitier fog computing model with large-scale data analytics service is proposed for smart cities applications. The multitier fog is consisted of ad-hoc fogs and dedicated fogs with opportunistic and dedicated computing resources, respectively. The proposed new fog computing model with clear functional modules is able to mitigate the potential problems of dedicated computing infrastructure and slow response in cloud computing. We run analytics benchmark experiments over fogs formed by Rapsberry Pi computers with a distributed computing engine to measure computing performance of various analytics tasks, and create easy-to-use workload models. Quality of services (QoS) aware admission control, offloading, and resource allocation schemes are designed to support data analytics services, and maximize analytics service utilities. Availability and cost models of networking and computing resources are taken into account in QoS scheme design. A scalable system level simulator is developed to evaluate the fog-based analytics service and the QoS management schemes. Experiment results demonstrate the efficiency of analytics services over multitier fogs and the effectiveness of the proposed QoS schemes. Fogs can largely improve the performance of smart city analytics services than cloud only model in terms of job blocking probability and service utility.},
  doi      = {10.1109/JIOT.2017.2724845},
  keywords = {Edge computing;Computational modeling;Cloud computing;Smart cities;Quality of service;Data analysis;Analytical models;Data analytics;fog computing;Internet of Things (IoT);quality of services (QoS);Raspberry Pi;smart cities;Spark},
}

@InBook{Wu2020a,
  author    = {Wu, Chu‐ge and Wang, Ling},
  pages     = {371-384},
  publisher = {Wiley},
  title     = {An Estimation of Distribution Algorithm to Optimize the Utility of Task Scheduling Under Fog Computing Systems},
  year      = {2020},
  isbn      = {9781119551768},
  abstract  = {The Internet of Things (IoT) is realized initially today. A large amount of data is produced and a range of IoT services are settled down. Based on it, a range of responsive IoT applications arise. To satisfy the quality of experience (QoE) of users, the applications are needed to be processed in a timely manner. Compared with traditional cloud computing systems, fog computing is one of the promising solutions to processing the huge amount of local data and decreasing the end‐to‐end latency. Different time‐dependent functions are adopted to measure the utility of different tasks and in this work, the resource allocation and task scheduling problem under the fog system is considered to maximize the sum of the utility of tasks. And an estimation of distributed algorithm to maximum the task utility (uEDA) with a repair procedure and local search is adopted to determine the task processing order and computing node allocation. The comparative results show that the performance of our algorithm exceeds significantly the heuristic method on the utility metrics.},
  booktitle = {Fog Computing: Theory and Practice},
  doi       = {10.1002/9781119551713.ch14},
  keywords  = {Task analysis;Internet of Things;Edge computing;Computational modeling;Processor scheduling;Scheduling;Optimization},
  url       = {https://ieeexplore.ieee.org/document/9116755},
}

@Article{Candeia2015,
  author   = {Candeia, David and Santos, Ricardo Araújo and Lopes, Raquel},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Business-Driven Long-Term Capacity Planning for SaaS Applications},
  year     = {2015},
  issn     = {2168-7161},
  month    = {July},
  number   = {3},
  pages    = {290-303},
  volume   = {3},
  abstract = {Capacity Planning is one of the activities developed by Information Technology departments over the years, it aims at estimating the amount of resources needed to offer a computing service. This activity contributes to achieving high Quality of Service levels and also to pursuing better economic results for companies. In the Cloud Computing context, one plausible scenario is to have Software-as-a-Service (SaaS) providers that build their IT infrastructure acquiring resources from Infrastructure-as-a-Service (IaaS) providers. SaaS providers can reduce operational costs and complexity by buying instances from a reservation market, but then need to predict the number of instances needed in the long-term. This work investigates how important is the capacity planning in this context and how simple business-driven heuristics for long-term capacity planning impact on the profit achieved by SaaS providers. Simulation experiments were performed using synthetic e-commerce workloads. Our analysis show that proposed heuristics increase SaaS provider profit, on average, at 9.6501 percent per year. Analysing such results we demonstrate that capacity planning is still an important activity, contributing to the increase of SaaS providers profit. Besides, a good capacity planning may also avoid bad reputation due to unacceptable performance, which is a gain very hard to measure.},
  doi      = {10.1109/TCC.2015.2424877},
  keywords = {Capacity planning;Cloud computing;Measurement;Contracts;Quality of service;Planning;Capacity Planning;Cloud Computing;Software-as-a-Service;Capacity planning;cloud computing;software-as-a-service},
}

@Article{Noormohammadpour2018,
  author   = {Noormohammadpour, Mohammad and Raghavendra, Cauligi S.},
  journal  = {IEEE Communications Surveys Tutorials},
  title    = {Datacenter Traffic Control: Understanding Techniques and Tradeoffs},
  year     = {2018},
  issn     = {1553-877X},
  month    = {Secondquarter},
  number   = {2},
  pages    = {1492-1525},
  volume   = {20},
  abstract = {Datacenters provide cost-effective and flexible access to scalable compute and storage resources necessary for today's cloud computing needs. A typical datacenter is made up of thousands of servers connected with a large network and usually managed by one operator. To provide quality access to the variety of applications and services hosted on datacenters and maximize performance, it deems necessary to use datacenter networks effectively and efficiently. Datacenter traffic is often a mix of several classes with different priorities and requirements. This includes user-generated interactive traffic, traffic with deadlines, and long-running traffic. To this end, custom transport protocols and traffic management techniques have been developed to improve datacenter network performance. In this tutorial paper, we review the general architecture of datacenter networks, various topologies proposed for them, their traffic properties, general traffic control challenges in datacenters and general traffic control objectives. The purpose of this paper is to bring out the important characteristics of traffic control in datacenters and not to survey all existing solutions (as it is virtually impossible due to massive body of existing research). We hope to provide readers with a wide range of options and factors while considering a variety of traffic control mechanisms. We discuss various characteristics of datacenter traffic control, including management schemes, transmission control, traffic shaping, prioritization, load balancing, multipathing, and traffic scheduling. Next, we point to several open challenges as well as new and interesting networking paradigms. At the end of this paper, we briefly review inter-datacenter networks that connect geographically dispersed datacenters, which have been receiving increasing attention recently and pose interesting and novel research problems. To measure the performance of datacenter networks, different performance metrics have been used, such as flow completion times, deadline miss rate, throughput, and fairness. Depending on the application and user requirements, some metrics may need more attention. While investigating different traffic control techniques, we point out the tradeoffs involved in terms of costs, complexity, and performance. We find that a combination of different traffic control techniques may be necessary at particular entities and layers in the network to improve the variety of performance metrics. We also find that despite significant research efforts, there are still open problems that demand further attention from the research community.},
  doi      = {10.1109/COMST.2017.2782753},
  keywords = {Network topology;Topology;Servers;Bandwidth;Tutorials;Optical switches;Datacenters;traffic control;tradeoffs;management schemes;transmission control;traffic shaping;load balancing;flow prioritization;multipath routing;traffic scheduling},
}

@Article{AbdelBaky2019,
  author   = {AbdelBaky, Moustafa and Parashar, Manish},
  journal  = {IEEE Transactions on Services Computing},
  title    = {A General Performance and QoS Model for Distributed Software-Defined Environments},
  year     = {2019},
  issn     = {1939-1374},
  pages    = {1-1},
  abstract = {The landscape for cloud services and cyberinfrastructure offerings has increased drastically over the past few years. Initially, users moved their applications to the cloud to take advantage of a pay-per-usage model and on-demand access. However, as more cloud providers joined the market, users shifted their goals for using cloud computing from cost reduction to resilience, agility, and optimization. These goals can be achieved by dynamically combining services from multiple providers, for example, to avoid data center or cloud zone outages or to take advantage of extensive offerings with different price points. However, to efficiently support application deployment in this dynamic environment, new models and tools that can measure the application performance and the Quality of Service (QoS) of different configurations are required. The goal of this work is to evaluate the application performance and the QoS of a distributed Software-Defined Environment as well as calculate the QoS of alternative configurations from the underlying pool of services. In particular, we present a mathematical model and a tool for evaluating the performance and QoS of batch application workflows in a distributed environment. We experimentally evaluate the proposed model using a bioinformatics workflow running on infrastructure services from multiple cloud providers.},
  doi      = {10.1109/TSC.2019.2928300},
  keywords = {Cloud computing;Quality of service;Computational modeling;Data models;Optimization;Mathematical model;Tools;QoS modeling;performance modeling;multi-cloud;software-define environments},
}

@Article{Cheikhrouhou2021,
  author   = {Cheikhrouhou, Omar and Mahmud, Redowan and Zouari, Ramzi and Ibrahim, Muhammad and Zaguia, Atef and Gia, Tuan Nguyen},
  journal  = {IEEE Access},
  title    = {One-Dimensional CNN Approach for ECG Arrhythmia Analysis in Fog-Cloud Environments},
  year     = {2021},
  issn     = {2169-3536},
  pages    = {103513-103523},
  volume   = {9},
  abstract = {Cardiovascular diseases are considered the number one cause of death across the globe which can be primarily identified by the abnormal heart rhythms of the patients. By generating electrocardiogram (ECG) signals, wearable Internet of Things (IoT) devices can consistently track the patient’s heart rhythms. Although Cloud-based approaches for ECG analysis can achieve some levels of accuracy, they still have some limitations, such as high latency. Conversely, the Fog computing infrastructure is more powerful than edge devices but less capable than Cloud computing for executing compositionally intensive data analytic software. The Fog infrastructure can consist of Fog-based gateways directly connected with the wearable devices to offer many advanced benefits, including low latency and high quality of services. To address these issues, a modular one-dimensional convolution neural network (1D-CNN) approach is proposed in this work. The inference module of the proposed approach is deployable over the Fog infrastructure for analysing the ECG signals and initiating the emergency countermeasures within a minimum delay, whereas its training module is executable on the computationally enriched Cloud data centers. The proposed approach achieves the F1-measure score ≈1 on the MIT-BIH Arrhythmia database when applying GridSearch algorithm with the cross-validation method. This approach has also been implemented on a single-board computer and Google Colab-based hybrid Fog-Cloud infrastructure and embodied to a remote patient monitoring system that shows 25% improvement in the overall response time.},
  doi      = {10.1109/ACCESS.2021.3097751},
  keywords = {Electrocardiography;Cloud computing;Logic gates;Feature extraction;Security;Edge computing;Wearable computers;Internet of Things;ECG analysis;1D-CNN;fog computing;hybrid fog-cloud;heart disease},
}

@InProceedings{Zhou2018,
  author    = {Zhou, Peipei and Ruan, Zhenyuan and Fang, Zhenman and Shand, Megan and Roazen, David and Cong, Jason},
  booktitle = {2018 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  title     = {Doppio: I/O-Aware Performance Analysis, Modeling and Optimization for In-memory Computing Framework},
  year      = {2018},
  month     = {April},
  pages     = {22-32},
  abstract  = {In conventional Hadoop MapReduce applications, I/O used to play a heavy role in the overall system performance. More recently, a study from the Apache Spark community- state-of-the-art in-memory cluster computing framework- reports that I/O is no longer the bottleneck and has a marginal performance impact on applications like SQL processing. However, we observe that simply replacing HDDs with SSDs in a Spark cluster can have over 10x performance improvement for certain stages in large-scale production-quality genome processing. Therefore, one key question arises: How does I/O quanti- tatively impact the performance of today's big data applications developed using in-memory cluster computing frameworks like Apache Spark? In this paper we select an important yet complex application- the Spark-based Genome Analysis ToolKit (GATK4)-to guide our modeling. We first use different combinations of HDDs and SSDs to measure the I/O impact on GATK4 and change the CPU core number to discover the relation between computation and I/O access. By combining with Spark's underlying implementations, we further analyze the inherent cause of the above observations and build our model based on the analysis. Although we are building upon GATK4, our model maintains generality to other applications. Experimental results show that we can achieve a performance prediction error rate within 10% for typical Spark applications of both iterative and shuffle-heavy algorithms. Finally, we further extend our model to a broader area-that of optimal configuration selection in the public cloud. In Google Cloud, our model enables us to save 38% to 57% of cost for genome sequencing compared with its recommended default configurations. Currently, more and more companies are adopting cloud computing for specific workloads. Our proposed model can have a huge impact on their choices, while also enabling them to significantly reduce their costs.},
  doi       = {10.1109/ISPASS.2018.00011},
  keywords  = {Sparks;Genomics;Bioinformatics;Computational modeling;Analytical models;Cloud computing;Performance Modeling;Apache Spark;I/O Aware;Computational Genomics;Cost Optimization;Genome Analysis Toolkit;GATK4},
}

@InProceedings{Yu2020,
  author    = {Yu, Zhixing and He, Kejing and Chen, Chao and Wang, Jian},
  booktitle = {2020 IEEE Intl Conf on Parallel Distributed Processing with Applications, Big Data Cloud Computing, Sustainable Computing Communications, Social Computing Networking (ISPA/BDCloud/SocialCom/SustainCom)},
  title     = {Live Container Migration via Pre-restore and Random Access Memory},
  year      = {2020},
  month     = {Dec},
  pages     = {102-109},
  abstract  = {Container technology is increasingly being used for virtualization due to its ability to isolate the operating environment of the program. In cloud computing environment, we need to migrate containers between different hosts for load balancing or downtime maintenance. However, during the migration process, the container will be temporarily shut down, and the service will be unavailable. Therefore, the time cost is an essential indicator to measure the quality of the migration process. To achieve live container migration, we propose a pre-restore method and a complete random access memory (RAM) based method to migrate containers. Extensive experiments validate the effectiveness of our methods in reducing downtime and improving the efficiency of container migration.},
  doi       = {10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00039},
  keywords  = {Cloud computing;Random access memory;Containers;Maintenance engineering;Load management;Time measurement;Virtualization;Container;Live migration;Downtime;Pre-restore;RAM},
}

@InProceedings{Roy2015,
  author    = {Sudipta Singha Roy and Tamjid Haque Sarker and Hashem, M. M. A.},
  booktitle = {2015 2nd International Conference on Electrical Information and Communication Technologies (EICT)},
  title     = {A novel trust measurement system for cloud-based marketplace},
  year      = {2015},
  month     = {Dec},
  pages     = {49-54},
  abstract  = {Cloud Computing is an enormously growing phenomenon in the present days enabling IT related services to run in a more dynamic and scalable way than the previous days and cloud marketplace is becoming more competitive with the entrance of new cloud service providers (CSP) offering similar functionalities. The basic obstacles in the way of success of cloud marketplace are the numerous shortcomings in reliable monitoring and identifying reliable cloud service provider based on consumers' preferred services. In this paper, a multi-faceted Entrusted Trust Management (ETM) system architecture is introduced that can support the customers in reliably choosing the trustworthy cloud service provider (CSP) as well as properly weight the information sources that provide feedbacks about the quality of services (QoS) of the CSPs. This ETM system works on several issues to measure the trust value of the providers on specific domain and overall trust value. Firstly, measurement of the trust value of the specific domain of provider on the basis of certainty and uncertainty. Secondly, measurement of overall trust value of the provider from these domain specific trust values. Thirdly, measurement of “Degree of Conflict” between the rating/feedback of consumers and experts. And finally, measurement of trustworthiness of the information sources which provide the rating of the provider on the basis of the SLA between the provider and the consumer. At last, our proposed system is experimented using real datasets.},
  doi       = {10.1109/EICT.2015.7391921},
  keywords  = {Cloud computing;Cloud Service Provider;Trust Models;Trust Management;CAIQ;Self-Assessment},
}

@Article{Gao2020,
  author   = {Gao, Zihan and Hao, Wanming and Han, Zhuo and Yang, Shouyi},
  journal  = {IEEE Access},
  title    = {Q-Learning-Based Task Offloading and Resources Optimization for a Collaborative Computing System},
  year     = {2020},
  issn     = {2169-3536},
  pages    = {149011-149024},
  volume   = {8},
  abstract = {Mobile edge computing (MEC) can effectively overcome the shortcomings of high-latency in mobile cloud computing (MCC) by deploying the cloud resources, e.g., storage and computational capability, to the edge. However, the limited computation capability of the MEC restricts the scalability of offloading. Therefore, the basic requirements of the MEC system are to explore effective offloading decisions and resource allocation methods. To address it, we develop a collaborative computing system composed of local computing (mobile device), MEC (edge cloud) and MCC (central cloud). Based on the proposed collaborative computing system, we design a novel Q-learning based computation offloading (QLCOF) policy to achieve the optimal resource allocation and offloading scheme by prescheduling the computation side for each task from a global perspective. Specifically, we first model the offloading decision process as a Markov decision process (MDP) and design a state loss function (STLF) to measure the quality of experience (QoE). After that, we define the cumulation of STLFs as the system loss function (SYLF) and formulate an SYLF minimization problem. Due to the difficulty to directly solve the formulated problem, we decompose it into multiple subproblems and preferentially optimize the transmission power and computation frequency of the edge cloud by the quasi-convex bisection and polynomial analysis method, respectively. Based on the precalculated offline transmission power and edge cloud computation frequency, we develop a Q-learning based offloading (QLOF) scheme to minimize the SYLF by optimizing offloading decisions. Finally, the numeral results show that the proposed QLOF scheme effectively reduces the SYLF under different parameters.},
  doi      = {10.1109/ACCESS.2020.3015993},
  keywords = {Cloud computing;Task analysis;Collaboration;Mobile handsets;Computational modeling;Resource management;Loss measurement;Mobile edge computing (MEC);mobile cloud computing (MCC);offloading;resources allocation;MDP;Q-learning},
}

@InProceedings{RicoBautista2020,
  author    = {Rico-Bautista, Dewar and Maestre-Gongora, Gina and Guerrero, Cesar D.},
  booktitle = {2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)},
  title     = {Smart University:IoT adoption model},
  year      = {2020},
  month     = {July},
  pages     = {821-826},
  abstract  = {Higher education organizations see in new technologies an opportunity to improve the quality of their academic and administrative processes. However, the existence and application of new technologies in this type of institutions does not imply that it really has the expected impact on their processes, so many adoption initiatives fail generating losses and discouragement towards change. This happens mainly because more work is done on technology than on the realities of the institutions. One way to prevent this type of problem and redirect efforts is to align the adoption process itself. As artificial intelligence, cloud computing, IoT (Internet of Things) and Big Data technologies become stronger, it is necessary to have tools at hand that have the capacity to measure the level of adoption by institutions. The objective of measuring adoption by the processes and realities of higher education institutions, with a focus on their users. Many models have been proposed to understand why users accept or use technologies. Among those that have emerged for the study of technology adoption are TAM, UTAUT, UTAUT2, DOI, TPB, TRA, among others. This characterization allows us to conclude about the need for alignment and integration of technology with the organization's processes, calling for greater interaction with senior management.},
  doi       = {10.1109/WorldS450073.2020.9210369},
  keywords  = {Organizations;Internet of Things;Information technology;Computational modeling;Education;Big Data;Cloud computing;Adopting smart technology;Characterization;Process;IoT;Smart university},
}

@InProceedings{Abtahizadeh2015,
  author    = {Abtahizadeh, S. Amirhossein and Khomh, Foutse and Guéhéneuc, Yann-Gaël},
  booktitle = {2015 IEEE 34th International Performance Computing and Communications Conference (IPCCC)},
  title     = {How green are cloud patterns?},
  year      = {2015},
  month     = {Dec},
  pages     = {1-8},
  abstract  = {Cloud Patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. Yet, energy consumption is the biggest challenge that cloud computing systems (the backbone of today's high-tech economy) face today. In fact, 10% of the world's electricity is now being consumed by servers, laptops, tablets and smartphones. Energy consumption has complex dependencies on the hardware platform, and the multiple software layers. The hardware, its firmware, the operating system, and the various software components used by a cloud application, all contribute to determining the energy footprint. Hence, even though increasing a data center efficiency will eventually improve energy efficiency, the internal design of cloud-based applications can be improved to lower energy consumption. In this paper, we conduct an empirical study on a RESTful multi-threaded application deployed in the cloud, to investigate the individual and the combined impact of three cloud patterns (e.g., Local Database proxy, Local Sharding Based Router and Priority Queue) on the energy consumption of cloud based applications. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Developers and software architects can make use of these results to guide their design decisions.},
  doi       = {10.1109/PCCC.2015.7410295},
  issn      = {2374-9628},
  keywords  = {Cloud computing;Energy consumption;Databases;Energy measurement;Servers;Quality of service;Cloud Patterns;Energy Consumption;Energy Efficiency;Sharding;Priority Message Queue},
}

@Article{Liu2015,
  author   = {Liu, Jiangchuan and Zhu, Wenwu and Ebrahimi, Touradj and Apostolopoulos, John and Hua, Xian-Sheng and Wu, Chuan},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology},
  title    = {Introduction to the Special Section on Visual Computing in the Cloud: Fundamentals and Applications},
  year     = {2015},
  issn     = {1558-2205},
  month    = {Dec},
  number   = {12},
  pages    = {1885-1887},
  volume   = {25},
  abstract = {Cloud computing involves a large number of terminals connected through a real-time high-speed network (such as the Internet). The adoption rates for private and hybrid cloud services increased to 40% in 2013, with computing shifting from on-premise infrastructure to the cloud. To keep pace with the ever-accelerating rate of innovation, companies are moving to the cloud. However, visual computing in the cloud brings great challenges, such as how to measure and then improve the quality of experience in cloud computing. This Special Section provides the image/video community a forum to present new academic research and industrial development in running visual computing services in the cloud. This Special Section aims to address fundamental and practical aspects of visual computing in the cloud, such as how to build cloud platforms that can cope with seemingly unlimited supply of content coming from traditional media sources as well as new media uploaded to the Internet (YouTube, Facebook, etc.); how to leverage cloud technology to build high-quality image/video browsing and delivery experiences for a global audience; how to ingest, encode, process, adapt, as well as protect contents and privacy of users; how to provide both on-demand and live-streaming capabilities; how to tag image/video and allow consumers to access the image/video contents with high availability; how to support image/video services in mobile devices; and how to perform real-time image/video analytics in the cloud, to mention a few among a diverse range of challenges.},
  doi      = {10.1109/TCSVT.2015.2472955},
  keywords = {Special issues and sections;Cloud computing;Social network services;Streaming media;Transcoding},
}

@Article{Raj2021,
  author    = {Vinay Raj and Ravichandra Sadam},
  journal   = {{SN} Computer Science},
  title     = {Evaluation of {SOA}-Based Web Services and Microservices Architecture Using Complexity Metrics},
  year      = {2021},
  month     = {jul},
  number    = {5},
  volume    = {2},
  doi       = {10.1007/s42979-021-00767-6},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs42979-021-00767-6},
}

@Book{Alhajj2018,
  editor    = {Reda Alhajj and Jon Rokne},
  publisher = {Springer New York},
  title     = {Encyclopedia of Social Network Analysis and Mining},
  year      = {2018},
  doi       = {10.1007/978-1-4939-7131-2},
  url       = {https://doi.org/10.1007%2F978-1-4939-7131-2},
}

@Book{Rosa2021,
  editor    = {Marcello La Rosa and Shazia Sadiq and Ernest Teniente},
  publisher = {Springer International Publishing},
  title     = {Advanced Information Systems Engineering},
  year      = {2021},
  doi       = {10.1007/978-3-030-79382-1},
  url       = {https://doi.org/10.1007%2F978-3-030-79382-1},
}

@Book{Paiva2021,
  editor    = {Ana C. R. Paiva and Ana Rosa Cavalli and Paula Ventura Martins and Ricardo P{\'{e}}rez-Castillo},
  publisher = {Springer International Publishing},
  title     = {Quality of Information and Communications Technology},
  year      = {2021},
  doi       = {10.1007/978-3-030-85347-1},
  url       = {https://doi.org/10.1007%2F978-3-030-85347-1},
}

@Book{Sunyaev2020,
  author    = {Ali Sunyaev},
  publisher = {Springer International Publishing},
  title     = {Internet Computing},
  year      = {2020},
  doi       = {10.1007/978-3-030-34957-8},
  url       = {https://doi.org/10.1007%2F978-3-030-34957-8},
}

@Book{Brambilla2021,
  editor    = {Marco Brambilla and Richard Chbeir and Flavius Frasincar and Ioana Manolescu},
  publisher = {Springer International Publishing},
  title     = {Web Engineering},
  year      = {2021},
  doi       = {10.1007/978-3-030-74296-6},
  url       = {https://doi.org/10.1007%2F978-3-030-74296-6},
}

@Book{Shishkov2021,
  editor    = {Boris Shishkov},
  publisher = {Springer International Publishing},
  title     = {Business Modeling and Software Design},
  year      = {2021},
  doi       = {10.1007/978-3-030-79976-2},
  url       = {https://doi.org/10.1007%2F978-3-030-79976-2},
}

@Article{Slimani2020,
  author    = {Sarra Slimani and Tarek Hamrouni and Faouzi Ben Charrada},
  journal   = {Cluster Computing},
  title     = {Service-oriented replication strategies for improving quality-of-service in cloud computing: a survey},
  year      = {2020},
  month     = {may},
  number    = {1},
  pages     = {361--392},
  volume    = {24},
  doi       = {10.1007/s10586-020-03108-z},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10586-020-03108-z},
}

@InCollection{Oliveira2015,
  author    = {Lucas Bueno Ruas Oliveira and Felipe Augusto Amaral and Diogo B. Martins and Flavio Oquendo and Elisa Yumi Nakagawa},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer Berlin Heidelberg},
  title     = {{RoboSeT}: A Tool to Support Cataloging and Discovery of Services for Service-Oriented Robotic Systems},
  year      = {2015},
  pages     = {114--132},
  doi       = {10.1007/978-3-662-48134-9_7},
  url       = {https://doi.org/10.1007%2F978-3-662-48134-9_7},
}

@Article{Bogner2021,
  author    = {Justus Bogner and Jonas Fritzsch and Stefan Wagner and Alfred Zimmermann},
  journal   = {Empirical Software Engineering},
  title     = {Industry practices and challenges for the evolvability assurance of microservices},
  year      = {2021},
  month     = {jul},
  number    = {5},
  volume    = {26},
  doi       = {10.1007/s10664-021-09999-9},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10664-021-09999-9},
}

@Book{Shen2020,
  editor    = {Xuemin (Sherman) Shen and Xiaodong Lin and Kuan Zhang},
  publisher = {Springer International Publishing},
  title     = {Encyclopedia of Wireless Networks},
  year      = {2020},
  doi       = {10.1007/978-3-319-78262-1},
  url       = {https://doi.org/10.1007%2F978-3-319-78262-1},
}

@InCollection{Engel2018,
  author    = {Thomas Engel and Melanie Langermeier and Bernhard Bauer and Alexander Hofmann},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {Evaluation of Microservice Architectures: A Metric and Tool-Based Approach},
  year      = {2018},
  pages     = {74--89},
  doi       = {10.1007/978-3-319-92901-9_8},
  url       = {https://doi.org/10.1007%2F978-3-319-92901-9_8},
}

@Book{Bollin2021,
  editor    = {Andreas Bollin and Vadim Ermolayev and Heinrich C. Mayr and Mykola Nikitchenko and Aleksander Spivakovsky and Mykola Tkachuk and Vitaliy Yakovyna and Grygoriy Zholtkevych},
  publisher = {Springer International Publishing},
  title     = {Information and Communication Technologies in Education, Research, and Industrial Applications},
  year      = {2021},
  doi       = {10.1007/978-3-030-77592-6},
  url       = {https://doi.org/10.1007%2F978-3-030-77592-6},
}

@Book{Goedicke2021,
  editor    = {Michael Goedicke and Erich Neuhold and Kai Rannenberg},
  publisher = {Springer International Publishing},
  title     = {Advancing Research in Information and Communication Technology},
  year      = {2021},
  doi       = {10.1007/978-3-030-81701-5},
  url       = {https://doi.org/10.1007%2F978-3-030-81701-5},
}

@Book{Filipe2021,
  editor    = {Joaquim Filipe and Micha{\l} {\'{S}}mia{\l}ek and Alexander Brodsky and Slimane Hammoudi},
  publisher = {Springer International Publishing},
  title     = {Enterprise Information Systems},
  year      = {2021},
  doi       = {10.1007/978-3-030-75418-1},
  url       = {https://doi.org/10.1007%2F978-3-030-75418-1},
}

@Book{Latifi2021,
  editor    = {Shahram Latifi},
  publisher = {Springer International Publishing},
  title     = {{ITNG} 2021 18th International Conference on Information Technology-New Generations},
  year      = {2021},
  doi       = {10.1007/978-3-030-70416-2},
  url       = {https://doi.org/10.1007%2F978-3-030-70416-2},
}

@Article{Monteiro2020,
  author    = {Davi Monteiro and Paulo Henrique M. Maia and Lincoln S. Rocha and Nabor C. Mendon{\c{c}}a},
  journal   = {Service Oriented Computing and Applications},
  title     = {Building orchestrated microservice systems using declarative business processes},
  year      = {2020},
  month     = {aug},
  number    = {4},
  pages     = {243--268},
  volume    = {14},
  doi       = {10.1007/s11761-020-00300-2},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11761-020-00300-2},
}

@InCollection{Branzov2019,
  author    = {Todor Branzov and Krassimira Ivanova and Mladen Georgiev},
  booktitle = {Modeling and Using Context},
  publisher = {Springer International Publishing},
  title     = {Service-Microservice Architecture for Context-Aware Content Delivery in National Geoinformation Center of Bulgaria},
  year      = {2019},
  pages     = {40--50},
  doi       = {10.1007/978-3-030-34974-5_4},
  url       = {https://doi.org/10.1007%2F978-3-030-34974-5_4},
}

@Book{Pappas2019,
  editor    = {Ilias O. Pappas and Patrick Mikalef and Yogesh K. Dwivedi and Letizia Jaccheri and John Krogstie and Matti Mäntymäki},
  publisher = {Springer International Publishing},
  title     = {Digital Transformation for a Sustainable Society in the 21st Century},
  year      = {2019},
  doi       = {10.1007/978-3-030-29374-1},
  url       = {https://doi.org/10.1007%2F978-3-030-29374-1},
}

@Book{Augusto2021,
  editor    = {Adriano Augusto and Asif Gill and Selmin Nurcan and Iris Reinhartz-Berger and Rainer Schmidt and Jelena Zdravkovic},
  publisher = {Springer International Publishing},
  title     = {Enterprise, Business-Process and Information Systems Modeling},
  year      = {2021},
  doi       = {10.1007/978-3-030-79186-5},
  url       = {https://doi.org/10.1007%2F978-3-030-79186-5},
}

@InCollection{Khan2017,
  author    = {Gitosree Khan and Sabnam Sengupta and Anirban Sarkar},
  booktitle = {Requirements Engineering for Service and Cloud Computing},
  publisher = {Springer International Publishing},
  title     = {Formal Modeling of Enterprise Cloud Bus System: A High Level Petri-Net Based Approach},
  year      = {2017},
  pages     = {121--149},
  doi       = {10.1007/978-3-319-51310-2_6},
  url       = {https://doi.org/10.1007%2F978-3-319-51310-2_6},
}

@InCollection{Wedeniwski2015,
  author    = {Sebastian Wedeniwski},
  booktitle = {The Mobility Revolution in the Automotive Industry},
  publisher = {Springer Berlin Heidelberg},
  title     = {Strategy, Business Model and Architecture in Today's Automotive Industry},
  year      = {2015},
  pages     = {75--238},
  doi       = {10.1007/978-3-662-47788-5_3},
  url       = {https://doi.org/10.1007%2F978-3-662-47788-5_3},
}

@Book{Tatnall2020,
  editor    = {Arthur Tatnall},
  publisher = {Springer International Publishing},
  title     = {Encyclopedia of Education and Information Technologies},
  year      = {2020},
  doi       = {10.1007/978-3-030-10576-1},
  url       = {https://doi.org/10.1007%2F978-3-030-10576-1},
}

@Book{Sun2021a,
  editor    = {Xingming Sun and Xiaorui Zhang and Zhihua Xia and Elisa Bertino},
  publisher = {Springer International Publishing},
  title     = {Advances in Artificial Intelligence and Security},
  year      = {2021},
  doi       = {10.1007/978-3-030-78621-2},
  url       = {https://doi.org/10.1007%2F978-3-030-78621-2},
}

@Book{Krzhizhanovskaya2020,
  editor    = {Valeria V. Krzhizhanovskaya and G{\'{a}}bor Z{\'{a}}vodszky and Michael H. Lees and Jack J. Dongarra and Peter M. A. Sloot and S{\'{e}}rgio Brissos and Jo{\~{a}}o Teixeira},
  publisher = {Springer International Publishing},
  title     = {Computational Science {\textendash} {ICCS} 2020},
  year      = {2020},
  doi       = {10.1007/978-3-030-50423-6},
  url       = {https://doi.org/10.1007%2F978-3-030-50423-6},
}

@InCollection{Staron2017,
  author    = {Miroslaw Staron},
  booktitle = {Automotive Software Architectures},
  publisher = {Springer International Publishing},
  title     = {Evaluation of Automotive Software Architectures},
  year      = {2017},
  pages     = {151--177},
  doi       = {10.1007/978-3-319-58610-6_6},
  url       = {https://doi.org/10.1007%2F978-3-319-58610-6_6},
}

@Article{Davami2021,
  author    = {Fatemeh Davami and Sahar Adabi and Ali Rezaee and Amir Masoud Rahmani},
  journal   = {Computing},
  title     = {Fog-based architecture for scheduling multiple workflows with high availability requirement},
  year      = {2021},
  month     = {feb},
  doi       = {10.1007/s00607-021-00905-1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00607-021-00905-1},
}

@InCollection{Loucopoulos2016,
  author    = {Pericles Loucopoulos and Evangelia Kavakli},
  booktitle = {Domain-Specific Conceptual Modeling},
  publisher = {Springer International Publishing},
  title     = {Capability Oriented Enterprise Knowledge Modeling: The {CODEK} Approach},
  year      = {2016},
  pages     = {197--215},
  doi       = {10.1007/978-3-319-39417-6_9},
  url       = {https://doi.org/10.1007%2F978-3-319-39417-6_9},
}

@Book{Buchmann2021,
  editor    = {Robert Andrei Buchmann and Andrea Polini and Björn Johansson and Dimitris Karagiannis},
  publisher = {Springer International Publishing},
  title     = {Perspectives in Business Informatics Research},
  year      = {2021},
  doi       = {10.1007/978-3-030-87205-2},
  url       = {https://doi.org/10.1007%2F978-3-030-87205-2},
}

@Article{Sikri2019,
  author    = {Monika Sikri},
  journal   = {Service Oriented Computing and Applications},
  title     = {An adaptive and scalable framework for automated service discovery},
  year      = {2019},
  month     = {mar},
  number    = {1},
  pages     = {67--79},
  volume    = {13},
  doi       = {10.1007/s11761-019-00255-z},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11761-019-00255-z},
}

@Book{Fu2021,
  editor    = {Weina Fu and Yuan Xu and Shui-Hua Wang and Yudong Zhang},
  publisher = {Springer International Publishing},
  title     = {Multimedia Technology and Enhanced Learning},
  year      = {2021},
  doi       = {10.1007/978-3-030-82565-2},
  url       = {https://doi.org/10.1007%2F978-3-030-82565-2},
}

@InCollection{Staron2020,
  author    = {Miroslaw Staron},
  booktitle = {Automotive Software Architectures},
  publisher = {Springer International Publishing},
  title     = {Evaluation of Automotive Software Architectures},
  year      = {2020},
  month     = {dec},
  pages     = {189--213},
  doi       = {10.1007/978-3-030-65939-4_8},
  url       = {https://doi.org/10.1007%2F978-3-030-65939-4_8},
}

@Article{Moens2013,
  author    = {Hendrik Moens and Eddy Truyen and Stefan Walraven and Wouter Joosen and Bart Dhoedt and Filip De Turck},
  journal   = {Journal of Network and Systems Management},
  title     = {Cost-Effective Feature Placement of Customizable Multi-Tenant Applications in the Cloud},
  year      = {2013},
  month     = {feb},
  number    = {4},
  pages     = {517--558},
  volume    = {22},
  doi       = {10.1007/s10922-013-9265-5},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10922-013-9265-5},
}

@Book{Fujita2021,
  editor    = {Hamido Fujita and Ali Selamat and Jerry Chun-Wei Lin and Moonis Ali},
  publisher = {Springer International Publishing},
  title     = {Advances and Trends in Artificial Intelligence. From Theory to Practice},
  year      = {2021},
  doi       = {10.1007/978-3-030-79463-7},
  url       = {https://doi.org/10.1007%2F978-3-030-79463-7},
}

@Article{Siavvas2021,
  author    = {Miltiadis Siavvas and Dionysios Kehagias and Dimitrios Tzovaras and Erol Gelenbe},
  journal   = {Software Quality Journal},
  title     = {A hierarchical model for quantifying software security based on static analysis alerts and software metrics},
  year      = {2021},
  month     = {may},
  number    = {2},
  pages     = {431--507},
  volume    = {29},
  doi       = {10.1007/s11219-021-09555-0},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11219-021-09555-0},
}

@InCollection{Chauhan2016,
  author    = {Muhammad Aufeef Chauhan and Muhammad Ali Babar and Christian W. Probst},
  booktitle = {Product-Focused Software Process Improvement},
  publisher = {Springer International Publishing},
  title     = {A Process Framework for Designing Software Reference Architectures for Providing Tools as a Service},
  year      = {2016},
  pages     = {111--126},
  doi       = {10.1007/978-3-319-49094-6_8},
  url       = {https://doi.org/10.1007%2F978-3-319-49094-6_8},
}

@InCollection{Rabelo2020,
  author    = {Ricardo J. Rabelo and Hernesto A. Ruiz and Maiara H. Cancian},
  booktitle = {Boosting Collaborative Networks 4.0},
  publisher = {Springer International Publishing},
  title     = {For a Dynamic Web Services Discovery Model for Open Ecosystems of Software Providers},
  year      = {2020},
  pages     = {83--97},
  doi       = {10.1007/978-3-030-62412-5_7},
  url       = {https://doi.org/10.1007%2F978-3-030-62412-5_7},
}

@Book{Chaubey2020,
  editor    = {Nirbhay Chaubey and Satyen Parikh and Kiran Amin},
  publisher = {Springer Singapore},
  title     = {Computing Science, Communication and Security},
  year      = {2020},
  doi       = {10.1007/978-981-15-6648-6},
  url       = {https://doi.org/10.1007%2F978-981-15-6648-6},
}

@InCollection{Shahin2015,
  author    = {Mojtaba Shahin and Muhammad Ali Babar},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Improving the Quality of Architecture Design Through Peer-Reviews and Recombination},
  year      = {2015},
  pages     = {70--86},
  doi       = {10.1007/978-3-319-23727-5_6},
  url       = {https://doi.org/10.1007%2F978-3-319-23727-5_6},
}

@InCollection{AdjeponYamoah2016,
  author    = {David Ebo Adjepon-Yamoah},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {cloud-{ATAM}: Method for Analysing Resilient Attributes of Cloud-Based Architectures},
  year      = {2016},
  pages     = {105--114},
  doi       = {10.1007/978-3-319-45892-2_8},
  url       = {https://doi.org/10.1007%2F978-3-319-45892-2_8},
}

@Book{Murayama2021,
  editor    = {Yuko Murayama and Dimiter Velev and Plamena Zlateva},
  publisher = {Springer International Publishing},
  title     = {Information Technology in Disaster Risk Reduction},
  year      = {2021},
  doi       = {10.1007/978-3-030-81469-4},
  url       = {https://doi.org/10.1007%2F978-3-030-81469-4},
}

@InCollection{Buchgeher2015,
  author    = {Georg Buchgeher and Rainer Weinreich and Thomas Kriechbaum},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {Making the Case for Centralized Software Architecture Management},
  year      = {2015},
  month     = {dec},
  pages     = {109--121},
  doi       = {10.1007/978-3-319-27033-3_8},
  url       = {https://doi.org/10.1007%2F978-3-319-27033-3_8},
}

@Book{Raschke2021,
  editor    = {Alexander Raschke and Elvinia Riccobene and Klaus-Dieter Schewe},
  publisher = {Springer International Publishing},
  title     = {Logic, Computation and Rigorous Methods},
  year      = {2021},
  doi       = {10.1007/978-3-030-76020-5},
  url       = {https://doi.org/10.1007%2F978-3-030-76020-5},
}

@Article{Nogueira2016,
  author    = {Elias Nogueira and Ana Moreira and Daniel Lucr{\'{e}}dio and Vin{\'{\i}}cius Garcia and Renata Fortes},
  journal   = {Journal of Software Engineering Research and Development},
  title     = {Issues on developing interoperable cloud applications: definitions, concepts, approaches, requirements, characteristics and evaluation models},
  year      = {2016},
  month     = {dec},
  number    = {1},
  volume    = {4},
  doi       = {10.1186/s40411-016-0033-6},
  publisher = {Sociedade Brasileira de Computacao - {SB}},
  url       = {https://doi.org/10.1186%2Fs40411-016-0033-6},
}

@Book{Luo2021,
  editor    = {Yuhua Luo},
  publisher = {Springer International Publishing},
  title     = {Cooperative Design, Visualization, and Engineering},
  year      = {2021},
  doi       = {10.1007/978-3-030-88207-5},
  url       = {https://doi.org/10.1007%2F978-3-030-88207-5},
}

@InCollection{Ardagna2014,
  author    = {Danilo Ardagna and Giovanni Paolo Gibilisco and Michele Ciavotta and Alexander Lavrentev},
  booktitle = {Search-Based Software Engineering},
  publisher = {Springer International Publishing},
  title     = {A Multi-model Optimization Framework for the Model Driven Design of Cloud Applications},
  year      = {2014},
  pages     = {61--76},
  doi       = {10.1007/978-3-319-09940-8_5},
  url       = {https://doi.org/10.1007%2F978-3-319-09940-8_5},
}

@InCollection{Schermann2020,
  author    = {Gerald Schermann and F{\'{a}}bio Oliveira and Erik Wittern and Philipp Leitner},
  booktitle = {Service-Oriented Computing},
  publisher = {Springer International Publishing},
  title     = {Topology-Aware Continuous Experimentation in Microservice-Based Applications},
  year      = {2020},
  pages     = {19--35},
  doi       = {10.1007/978-3-030-65310-1_2},
  url       = {https://doi.org/10.1007%2F978-3-030-65310-1_2},
}

@Book{Badhwar2021,
  author    = {Raj Badhwar},
  publisher = {Springer International Publishing},
  title     = {The {CISO}'s Next Frontier},
  year      = {2021},
  doi       = {10.1007/978-3-030-75354-2},
  url       = {https://doi.org/10.1007%2F978-3-030-75354-2},
}

@Book{Bisong2019,
  author    = {Ekaba Bisong},
  publisher = {Apress},
  title     = {Building Machine Learning and Deep Learning Models on Google Cloud Platform},
  year      = {2019},
  doi       = {10.1007/978-1-4842-4470-8},
  url       = {https://doi.org/10.1007%2F978-1-4842-4470-8},
}

@Book{Edelmann2021,
  editor    = {Noella Edelmann and Csaba Cs{\'{a}}ki and Sara Hofmann and Thomas J. Lampoltshammer and Laura Alcaide Mu{\~{n}}oz and Peter Parycek and Gerhard Schwabe and Efthimios Tambouris},
  publisher = {Springer International Publishing},
  title     = {Electronic Participation},
  year      = {2021},
  doi       = {10.1007/978-3-030-82824-0},
  url       = {https://doi.org/10.1007%2F978-3-030-82824-0},
}

@InCollection{Muccini2018,
  author    = {Henry Muccini and Mahyar Tourchi Moghaddam},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {{IoT} Architectural Styles},
  year      = {2018},
  pages     = {68--85},
  doi       = {10.1007/978-3-030-00761-4_5},
  url       = {https://doi.org/10.1007%2F978-3-030-00761-4_5},
}

@Article{Ciancone2013,
  author    = {Andrea Ciancone and Mauro Luigi Drago and Antonio Filieri and Vincenzo Grassi and Heiko Koziolek and Raffaela Mirandola},
  journal   = {Software {\&} Systems Modeling},
  title     = {The {KlaperSuite} framework for model-driven reliability analysis of component-based systems},
  year      = {2013},
  month     = {mar},
  number    = {4},
  pages     = {1269--1290},
  volume    = {13},
  doi       = {10.1007/s10270-013-0334-8},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10270-013-0334-8},
}

@Article{Yan2019,
  author    = {Biwei Yan and Jiguo Yu and Meihong Yang and Honglu Jiang and Zhiguo Wan and Lina Ni},
  journal   = {Personal and Ubiquitous Computing},
  title     = {A novel distributed Social Internet of Things service recommendation scheme based on {LSH} forest},
  year      = {2019},
  month     = {oct},
  doi       = {10.1007/s00779-019-01283-4},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00779-019-01283-4},
}

@Book{He2021,
  editor    = {Xin He and En Shao and Guangming Tan},
  publisher = {Springer International Publishing},
  title     = {Network and Parallel Computing},
  year      = {2021},
  doi       = {10.1007/978-3-030-79478-1},
  url       = {https://doi.org/10.1007%2F978-3-030-79478-1},
}

@Article{Pang2020,
  author    = {Beibei Pang and Fei Hao and Yixuan Yang and Doo-Soon Park},
  journal   = {The Journal of Supercomputing},
  title     = {An efficient approach for multi-user multi-cloud service composition in human{\textendash}land sustainable computational systems},
  year      = {2020},
  month     = {jan},
  number    = {7},
  pages     = {5442--5459},
  volume    = {76},
  doi       = {10.1007/s11227-019-03140-w},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11227-019-03140-w},
}

@Article{Liu2014,
  author    = {Lin Liu and Chen Yang and JianMin Wang and XiaoJun Ye and YingBo Liu and HongJi Yang and XiaoDong Liu},
  journal   = {Science China Information Sciences},
  title     = {Requirements model driven adaption and evolution of Internetware},
  year      = {2014},
  month     = {jan},
  number    = {6},
  pages     = {1--19},
  volume    = {57},
  doi       = {10.1007/s11432-014-5064-1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11432-014-5064-1},
}

@Book{Hammoudi2021,
  editor    = {Slimane Hammoudi and Christoph Quix and Jorge Bernardino},
  publisher = {Springer International Publishing},
  title     = {Data Management Technologies and Applications},
  year      = {2021},
  doi       = {10.1007/978-3-030-83014-4},
  url       = {https://doi.org/10.1007%2F978-3-030-83014-4},
}

@Article{Zhao2015,
  author    = {Pinghua Zhao and Yanwei Liu and Jinxia Liu and Ruixiao Yao and Song Ci},
  journal   = {Multimedia Tools and Applications},
  title     = {Perceptual rate-distortion optimization for H.264/{AVC} video coding from both signal and vision perspectives},
  year      = {2015},
  month     = {mar},
  number    = {5},
  pages     = {2781--2800},
  volume    = {75},
  doi       = {10.1007/s11042-015-2533-5},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11042-015-2533-5},
}

@InCollection{Sanctis2020,
  author    = {Martina De Sanctis and Ludovico Iovino and Maria Teresa Rossi and Manuel Wimmer},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {A Flexible Architecture for Key Performance Indicators Assessment in Smart Cities},
  year      = {2020},
  pages     = {118--135},
  doi       = {10.1007/978-3-030-58923-3_8},
  url       = {https://doi.org/10.1007%2F978-3-030-58923-3_8},
}

@InCollection{Kopp2021,
  author    = {Andrii Kopp and Dmytro Orlovskyi},
  booktitle = {Information and Communication Technologies in Education, Research, and Industrial Applications},
  publisher = {Springer International Publishing},
  title     = {Towards the Method and Information Technology for Evaluation of Business Process Model Quality},
  year      = {2021},
  pages     = {93--118},
  doi       = {10.1007/978-3-030-77592-6_5},
  url       = {https://doi.org/10.1007%2F978-3-030-77592-6_5},
}

@InCollection{Dahanayake2015,
  author    = {Ajantha Dahanayake and Bernhard Thalheim},
  booktitle = {Correct Software in Web Applications and Web Services},
  publisher = {Springer International Publishing},
  title     = {W$\ast$H: The Conceptual Model for Services},
  year      = {2015},
  pages     = {145--176},
  doi       = {10.1007/978-3-319-17112-8_5},
  url       = {https://doi.org/10.1007%2F978-3-319-17112-8_5},
}

@Article{Gerpheide2015,
  author    = {Christine M. Gerpheide and Ramon R. H. Schiffelers and Alexander Serebrenik},
  journal   = {Software Quality Journal},
  title     = {Assessing and improving quality of {QVTo} model transformations},
  year      = {2015},
  month     = {jun},
  number    = {3},
  pages     = {797--834},
  volume    = {24},
  doi       = {10.1007/s11219-015-9280-8},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11219-015-9280-8},
}

@Book{Enriquez2021,
  editor    = {Jos{\'{e}} Gonz{\'{a}}lez Enr{\'{\i}}quez and S{\o}ren Debois and Peter Fettke and Pierluigi Plebani and Inge van de Weerd and Ingo Weber},
  publisher = {Springer International Publishing},
  title     = {Business Process Management: Blockchain and Robotic Process Automation Forum},
  year      = {2021},
  doi       = {10.1007/978-3-030-85867-4},
  url       = {https://doi.org/10.1007%2F978-3-030-85867-4},
}

@Article{Bento2021,
  author    = {Andre Bento and Jaime Correia and Ricardo Filipe and Filipe Araujo and Jorge Cardoso},
  journal   = {Journal of Grid Computing},
  title     = {Automated Analysis of Distributed Tracing: Challenges and Research Directions},
  year      = {2021},
  month     = {feb},
  number    = {1},
  volume    = {19},
  doi       = {10.1007/s10723-021-09551-5},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10723-021-09551-5},
}

@Book{Broy2021,
  author    = {Manfred Broy and Marco Kuhrmann},
  publisher = {Springer Berlin Heidelberg},
  title     = {Einführung in die Softwaretechnik},
  year      = {2021},
  doi       = {10.1007/978-3-662-50263-1},
  url       = {https://doi.org/10.1007%2F978-3-662-50263-1},
}

@InCollection{Elberzhager2017,
  author    = {Frank Elberzhager and Matthias Naab},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {High Quality at Short Time-to-Market: Challenges Towards This Goal and Guidelines for the Realization},
  year      = {2017},
  month     = {nov},
  pages     = {121--132},
  doi       = {10.1007/978-3-319-71440-0_7},
  url       = {https://doi.org/10.1007%2F978-3-319-71440-0_7},
}

@Article{DeFranco2017,
  author    = {J. DeFranco and M. Kassab and P. Laplante and N. Laplante},
  journal   = {Innovations in Systems and Software Engineering},
  title     = {The nonfunctional requirement focus in medical device software: a systematic mapping study and taxonomy},
  year      = {2017},
  month     = {aug},
  number    = {2-3},
  pages     = {81--100},
  volume    = {13},
  doi       = {10.1007/s11334-017-0301-6},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11334-017-0301-6},
}

@Article{Bertolino2017,
  author    = {Antonia Bertolino and Antonello Calabro' and Felicita Di Giandomenico and Giuseppe Lami and Francesca Lonetti and Eda Marchetti and Fabio Martinelli and Ilaria Matteucci and Paolo Mori},
  journal   = {Software Quality Journal},
  title     = {A tour of secure software engineering solutions for connected vehicles},
  year      = {2017},
  month     = {nov},
  number    = {4},
  pages     = {1223--1256},
  volume    = {26},
  doi       = {10.1007/s11219-017-9393-3},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11219-017-9393-3},
}

@Article{Kosinska2020,
  author    = {Joanna Kosi{\'{n}}ska and Krzysztof Zieli{\'{n}}ski},
  journal   = {Journal of Grid Computing},
  title     = {Autonomic Management Framework for Cloud-Native Applications},
  year      = {2020},
  month     = {sep},
  number    = {4},
  pages     = {779--796},
  volume    = {18},
  doi       = {10.1007/s10723-020-09532-0},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10723-020-09532-0},
}

@Book{Gleb2021,
  author    = {Taras Gleb},
  publisher = {Apress},
  title     = {Systematic Cloud Migration},
  year      = {2021},
  doi       = {10.1007/978-1-4842-7252-7},
  url       = {https://doi.org/10.1007%2F978-1-4842-7252-7},
}

@InCollection{Spillner2018,
  author    = {Josef Spillner and Giovanni Toffetti and Manuel Ram{\'{\i}}rez L{\'{o}}pez},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Cloud-Native Databases: An Application Perspective},
  year      = {2018},
  pages     = {102--116},
  doi       = {10.1007/978-3-319-79090-9_7},
  url       = {https://doi.org/10.1007%2F978-3-319-79090-9_7},
}

@Book{Camposo2021,
  author    = {Guilherme Camposo},
  publisher = {Apress},
  title     = {Cloud Native Integration with Apache Camel},
  year      = {2021},
  doi       = {10.1007/978-1-4842-7211-4},
  url       = {https://doi.org/10.1007%2F978-1-4842-7211-4},
}

@Book{Adler2021,
  editor    = {Rasmus Adler and Amel Bennaceur and Simon Burton and Amleto Di Salle and Nicola Nostro and Rasmus L{\o}venstein Olsen and Selma Saidi and Philipp Schleiss and Daniel Schneider and Hans-Peter Schwefel},
  publisher = {Springer International Publishing},
  title     = {Dependable Computing - {EDCC} 2021 Workshops},
  year      = {2021},
  doi       = {10.1007/978-3-030-86507-8},
  url       = {https://doi.org/10.1007%2F978-3-030-86507-8},
}

@InCollection{Bryzgalov2021,
  author    = {Anton Bryzgalov and Sergey Stupnikov},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {A Cloud-Native Serverless Approach for Implementation of Batch Extract-Load Processes in Data Lakes},
  year      = {2021},
  pages     = {27--42},
  doi       = {10.1007/978-3-030-81200-3_3},
  url       = {https://doi.org/10.1007%2F978-3-030-81200-3_3},
}

@Book{Karlapalem2021,
  editor    = {Kamal Karlapalem and Hong Cheng and Naren Ramakrishnan and R. K. Agrawal and P. Krishna Reddy and Jaideep Srivastava and Tanmoy Chakraborty},
  publisher = {Springer International Publishing},
  title     = {Advances in Knowledge Discovery and Data Mining},
  year      = {2021},
  doi       = {10.1007/978-3-030-75762-5},
  url       = {https://doi.org/10.1007%2F978-3-030-75762-5},
}

@Book{Balandin2021,
  editor    = {Dmitry Balandin and Konstantin Barkalov and Victor Gergel and Iosif Meyerov},
  publisher = {Springer International Publishing},
  title     = {Mathematical Modeling and Supercomputer Technologies},
  year      = {2021},
  doi       = {10.1007/978-3-030-78759-2},
  url       = {https://doi.org/10.1007%2F978-3-030-78759-2},
}

@Book{Sychev2021,
  editor    = {Alexander Sychev and Sergey Makhortov and Bernhard Thalheim},
  publisher = {Springer International Publishing},
  title     = {Data Analytics and Management in Data Intensive Domains},
  year      = {2021},
  doi       = {10.1007/978-3-030-81200-3},
  url       = {https://doi.org/10.1007%2F978-3-030-81200-3},
}

@InCollection{Gleb2021a,
  author    = {Taras Gleb},
  booktitle = {Systematic Cloud Migration},
  publisher = {Apress},
  title     = {Develop Target Architecture},
  year      = {2021},
  pages     = {41--56},
  doi       = {10.1007/978-1-4842-7252-7_3},
  url       = {https://doi.org/10.1007%2F978-1-4842-7252-7_3},
}

@Article{Kehrer2019,
  author    = {Stefan Kehrer and Wolfgang Blochinger},
  journal   = {{SICS} Software-Intensive Cyber-Physical Systems},
  title     = {Migrating parallel applications to the cloud: assessing cloud readiness based on parallel design decisions},
  year      = {2019},
  month     = {feb},
  number    = {2-3},
  pages     = {73--84},
  volume    = {34},
  doi       = {10.1007/s00450-019-00396-8},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00450-019-00396-8},
}

@Book{Badhwar2021a,
  author    = {Raj Badhwar},
  publisher = {Springer International Publishing},
  title     = {The {CISO}'s Next Frontier},
  year      = {2021},
  doi       = {10.1007/978-3-030-75354-2},
  url       = {https://doi.org/10.1007%2F978-3-030-75354-2},
}

@InCollection{Chakraborty2021,
  author    = {Mainak Chakraborty and Ajit Pratap Kundan},
  booktitle = {Monitoring Cloud-Native Applications},
  publisher = {Apress},
  title     = {Observability},
  year      = {2021},
  pages     = {25--54},
  doi       = {10.1007/978-1-4842-6888-9_2},
  url       = {https://doi.org/10.1007%2F978-1-4842-6888-9_2},
}

@InCollection{Zeeshan2020,
  author    = {Afzaal Ahmad Zeeshan},
  booktitle = {{DevSecOps} for .{NET} Core},
  publisher = {Apress},
  title     = {Writing Secure Apps},
  year      = {2020},
  pages     = {57--108},
  doi       = {10.1007/978-1-4842-5850-7_3},
  url       = {https://doi.org/10.1007%2F978-1-4842-5850-7_3},
}

@InCollection{Fehling2013,
  author    = {Christoph Fehling and Frank Leymann and Ralph Retter and Walter Schupeck and Peter Arbitter},
  booktitle = {Cloud Computing Patterns},
  publisher = {Springer Vienna},
  title     = {Introduction},
  year      = {2013},
  month     = {may},
  pages     = {1--20},
  doi       = {10.1007/978-3-7091-1568-8_1},
  url       = {https://doi.org/10.1007%2F978-3-7091-1568-8_1},
}

@InCollection{Chakraborty2021a,
  author    = {Mainak Chakraborty and Ajit Pratap Kundan},
  booktitle = {Monitoring Cloud-Native Applications},
  publisher = {Apress},
  title     = {Architecture of a Modern Monitoring System},
  year      = {2021},
  pages     = {55--96},
  doi       = {10.1007/978-1-4842-6888-9_3},
  url       = {https://doi.org/10.1007%2F978-1-4842-6888-9_3},
}

@Book{Maglogiannis2021,
  editor    = {Ilias Maglogiannis and John Macintyre and Lazaros Iliadis},
  publisher = {Springer International Publishing},
  title     = {Artificial Intelligence Applications and Innovations. {AIAI} 2021 {IFIP} {WG} 12.5 International Workshops},
  year      = {2021},
  doi       = {10.1007/978-3-030-79157-5},
  url       = {https://doi.org/10.1007%2F978-3-030-79157-5},
}

@InCollection{Shivakumar2019,
  author    = {Shailesh Kumar Shivakumar},
  booktitle = {Build a Next-Generation Digital Workplace},
  publisher = {Apress},
  title     = {Digital Transformation to Next-Generation Workplaces},
  year      = {2019},
  month     = {dec},
  pages     = {199--226},
  doi       = {10.1007/978-1-4842-5512-4_8},
  url       = {https://doi.org/10.1007%2F978-1-4842-5512-4_8},
}
Resource not found.

@InCollection{Shetty2021,
  author    = {Rajaneesh Sudhakar Shetty},
  booktitle = {5G Mobile Core Network},
  publisher = {Apress},
  title     = {5G Packet Core Testing Strategies},
  year      = {2021},
  pages     = {235--275},
  doi       = {10.1007/978-1-4842-6473-7_5},
  url       = {https://doi.org/10.1007%2F978-1-4842-6473-7_5},
}

@Book{Brambilla2021a,
  editor    = {Marco Brambilla and Richard Chbeir and Flavius Frasincar and Ioana Manolescu},
  publisher = {Springer International Publishing},
  title     = {Web Engineering},
  year      = {2021},
  doi       = {10.1007/978-3-030-74296-6},
  url       = {https://doi.org/10.1007%2F978-3-030-74296-6},
}

@Book{Copeland2021,
  author    = {Marshall Copeland},
  publisher = {Apress},
  title     = {Cloud Defense Strategies with Azure Sentinel},
  year      = {2021},
  doi       = {10.1007/978-1-4842-7132-2},
  url       = {https://doi.org/10.1007%2F978-1-4842-7132-2},
}

@InCollection{Gole2020,
  author    = {Vinayak Gole and Shreekant Shiralkar},
  booktitle = {Empower Decision Makers with {SAP} Analytics Cloud},
  publisher = {Apress},
  title     = {{SAC} for Enabling {\textquotedblleft}Single Version of Truth{\textquotedblright}},
  year      = {2020},
  pages     = {31--78},
  doi       = {10.1007/978-1-4842-6097-5_3},
  url       = {https://doi.org/10.1007%2F978-1-4842-6097-5_3},
}

@InCollection{Chakraborty2021b,
  author    = {Mainak Chakraborty and Ajit Pratap Kundan},
  booktitle = {Monitoring Cloud-Native Applications},
  publisher = {Apress},
  title     = {Introduction to Modern Monitoring},
  year      = {2021},
  pages     = {3--24},
  doi       = {10.1007/978-1-4842-6888-9_1},
  url       = {https://doi.org/10.1007%2F978-1-4842-6888-9_1},
}

@Book{Gatev2021,
  author    = {Radoslav Gatev},
  publisher = {Apress},
  title     = {Introducing Distributed Application Runtime (Dapr)},
  year      = {2021},
  doi       = {10.1007/978-1-4842-6998-5},
  url       = {https://doi.org/10.1007%2F978-1-4842-6998-5},
}

@Book{Taulli2020,
  author    = {Tom Taulli},
  publisher = {Apress},
  title     = {The Robotic Process Automation Handbook},
  year      = {2020},
  doi       = {10.1007/978-1-4842-5729-6},
  url       = {https://doi.org/10.1007%2F978-1-4842-5729-6},
}

@Book{Lamouchi2021,
  author    = {Nebrass Lamouchi},
  publisher = {Apress},
  title     = {Pro Java Microservices with Quarkus and Kubernetes},
  year      = {2021},
  doi       = {10.1007/978-1-4842-7170-4},
  url       = {https://doi.org/10.1007%2F978-1-4842-7170-4},
}

@Article{Pecorelli2021,
  author    = {Fabiano Pecorelli and Fabio Palomba and Andrea De Lucia},
  journal   = {Empirical Software Engineering},
  title     = {The Relation of Test-Related Factors to Software Quality: A Case Study on Apache Systems},
  year      = {2021},
  month     = {feb},
  number    = {2},
  volume    = {26},
  doi       = {10.1007/s10664-020-09891-y},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10664-020-09891-y},
}

@Article{Wei2021,
  author    = {Hao Wei and Joaquin Salvachua Rodriguez and Octavio Nieto-Taladriz Garcia},
  journal   = {Journal of Grid Computing},
  title     = {Deployment Management and Topology Discovery of Microservice Applications in the Multicloud Environment},
  year      = {2021},
  month     = {jan},
  number    = {1},
  volume    = {19},
  doi       = {10.1007/s10723-021-09539-1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10723-021-09539-1},
}

@Book{Matos2021,
  editor    = {Miguel Matos and Fab{\'{\i}}ola Greve},
  publisher = {Springer International Publishing},
  title     = {Distributed Applications and Interoperable Systems},
  year      = {2021},
  doi       = {10.1007/978-3-030-78198-9},
  url       = {https://doi.org/10.1007%2F978-3-030-78198-9},
}

@Book{Stephanidis2021,
  editor    = {Constantine Stephanidis and Margherita Antona and Stavroula Ntoa},
  publisher = {Springer International Publishing},
  title     = {{HCI} International 2021 - Posters},
  year      = {2021},
  doi       = {10.1007/978-3-030-78645-8},
  url       = {https://doi.org/10.1007%2F978-3-030-78645-8},
}

@InCollection{Gleb2021b,
  author    = {Taras Gleb},
  booktitle = {Systematic Cloud Migration},
  publisher = {Apress},
  title     = {Cloud Migration Fundamentals},
  year      = {2021},
  pages     = {19--35},
  doi       = {10.1007/978-1-4842-7252-7_2},
  url       = {https://doi.org/10.1007%2F978-1-4842-7252-7_2},
}

@InCollection{Haff2021,
  author    = {Gordon Haff},
  booktitle = {How Open Source Ate Software},
  publisher = {Apress},
  title     = {The Challenges Facing Open Source Today},
  year      = {2021},
  pages     = {173--199},
  doi       = {10.1007/978-1-4842-6800-1_7},
  url       = {https://doi.org/10.1007%2F978-1-4842-6800-1_7},
}

@InCollection{Lin2018,
  author    = {Jinjin Lin and Pengfei Chen and Zibin Zheng},
  booktitle = {Service-Oriented Computing},
  publisher = {Springer International Publishing},
  title     = {Microscope: Pinpoint Performance Issues with Causal Graphs in Micro-service Environments},
  year      = {2018},
  pages     = {3--20},
  doi       = {10.1007/978-3-030-03596-9_1},
  url       = {https://doi.org/10.1007%2F978-3-030-03596-9_1},
}

@InCollection{Gole2020a,
  author    = {Vinayak Gole and Shreekant Shiralkar},
  booktitle = {Empower Decision Makers with {SAP} Analytics Cloud},
  publisher = {Apress},
  title     = {Exploit {\textquotedblleft}Augmented Analytics{\textquotedblright} Capability of {SAC}},
  year      = {2020},
  pages     = {123--153},
  doi       = {10.1007/978-1-4842-6097-5_5},
  url       = {https://doi.org/10.1007%2F978-1-4842-6097-5_5},
}

@Article{Toka2021,
  author    = {L{\'{a}}szl{\'{o}} Toka},
  journal   = {Journal of Grid Computing},
  title     = {Ultra-Reliable and Low-Latency Computing in the Edge with Kubernetes},
  year      = {2021},
  month     = {jul},
  number    = {3},
  volume    = {19},
  doi       = {10.1007/s10723-021-09573-z},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10723-021-09573-z},
}

@InCollection{Jindal2021,
  author    = {Anshul Jindal and Michael Gerndt},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {From {DevOps} to {NoOps}: Is It Worth It?},
  year      = {2021},
  pages     = {178--202},
  doi       = {10.1007/978-3-030-72369-9_8},
  url       = {https://doi.org/10.1007%2F978-3-030-72369-9_8},
}

@Book{Pothecary2021,
  author    = {Ryan Pothecary},
  publisher = {Apress},
  title     = {Running Microsoft Workloads on {AWS}},
  year      = {2021},
  doi       = {10.1007/978-1-4842-6628-1},
  url       = {https://doi.org/10.1007%2F978-1-4842-6628-1},
}

@InCollection{Fehling2013a,
  author    = {Christoph Fehling and Frank Leymann and Ralph Retter and Walter Schupeck and Peter Arbitter},
  booktitle = {Cloud Computing Patterns},
  publisher = {Springer Vienna},
  title     = {Composite Cloud Application Patterns},
  year      = {2013},
  month     = {may},
  pages     = {287--330},
  doi       = {10.1007/978-3-7091-1568-8_6},
  url       = {https://doi.org/10.1007%2F978-3-7091-1568-8_6},
}

@InCollection{Zeeshan2020a,
  author    = {Afzaal Ahmad Zeeshan},
  booktitle = {{DevSecOps} for .{NET} Core},
  publisher = {Apress},
  title     = {Automating Production Environments for Quality},
  year      = {2020},
  pages     = {215--264},
  doi       = {10.1007/978-1-4842-5850-7_6},
  url       = {https://doi.org/10.1007%2F978-1-4842-5850-7_6},
}

@Article{Zimmermann2016,
  author    = {Olaf Zimmermann},
  journal   = {Computing},
  title     = {Architectural refactoring for the cloud: a decision-centric view on cloud migration},
  year      = {2016},
  month     = {oct},
  number    = {2},
  pages     = {129--145},
  volume    = {99},
  doi       = {10.1007/s00607-016-0520-y},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00607-016-0520-y},
}

@InCollection{Fowley2018,
  author    = {Frank Fowley and Claus Pahl},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Cloud Migration Architecture and Pricing {\textendash} Mapping a Licensing Business Model for Software Vendors to a {SaaS} Business Model},
  year      = {2018},
  pages     = {91--103},
  doi       = {10.1007/978-3-319-72125-5_7},
  url       = {https://doi.org/10.1007%2F978-3-319-72125-5_7},
}

@InCollection{Gutierrez2020,
  author    = {Felipe Gutierrez},
  booktitle = {Spring Cloud Data Flow},
  publisher = {Apress},
  title     = {Spring Cloud},
  year      = {2020},
  month     = {dec},
  pages     = {89--127},
  doi       = {10.1007/978-1-4842-1239-4_5},
  url       = {https://doi.org/10.1007%2F978-1-4842-1239-4_5},
}

@InCollection{Chatterjee2020,
  author    = {Rithik Chatterjee},
  booktitle = {Red Hat and {IT} Security},
  publisher = {Apress},
  title     = {Security in {DevOps} and Automation},
  year      = {2020},
  month     = {nov},
  pages     = {65--104},
  doi       = {10.1007/978-1-4842-6434-8_3},
  url       = {https://doi.org/10.1007%2F978-1-4842-6434-8_3},
}

@Article{Sfaxi2021,
  author    = {Lilia Sfaxi and Mohamed Mehdi Ben Aissa},
  journal   = {Computing},
  title     = {Designing and implementing a Big Data benchmark in a financial context: application to a cash management use case},
  year      = {2021},
  month     = {apr},
  number    = {9},
  pages     = {1983--2005},
  volume    = {103},
  doi       = {10.1007/s00607-021-00933-x},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00607-021-00933-x},
}

@InCollection{Raj2018a,
  author    = {Pethuru Raj and Anupama Raman},
  booktitle = {Software-Defined Cloud Centers},
  publisher = {Springer International Publishing},
  title     = {Automated Multi-cloud Operations and Container Orchestration},
  year      = {2018},
  pages     = {185--218},
  doi       = {10.1007/978-3-319-78637-7_9},
  url       = {https://doi.org/10.1007%2F978-3-319-78637-7_9},
}

@InCollection{Gkikopoulos2021,
  author    = {Panagiotis Gkikopoulos and Valerio Schiavoni and Josef Spillner},
  booktitle = {Distributed Applications and Interoperable Systems},
  publisher = {Springer International Publishing},
  title     = {Analysis and Improvement of Heterogeneous Hardware Support in Docker Images},
  year      = {2021},
  pages     = {125--142},
  doi       = {10.1007/978-3-030-78198-9_9},
  url       = {https://doi.org/10.1007%2F978-3-030-78198-9_9},
}

@Article{Moghadam2021,
  author    = {Mahshid Helali Moghadam and Mehrdad Saadatmand and Markus Borg and Markus Bohlin and Björn Lisper},
  journal   = {Software Quality Journal},
  title     = {An autonomous performance testing framework using self-adaptive fuzzy reinforcement learning},
  year      = {2021},
  month     = {mar},
  doi       = {10.1007/s11219-020-09532-z},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11219-020-09532-z},
}

@Article{Casale2019,
  author    = {G. Casale and M. Arta{\v{c}} and W.-J. van den Heuvel and A. van Hoorn and P. Jakovits and F. Leymann and M. Long and V. Papanikolaou and D. Presenza and A. Russo and S. N. Srirama and D. A. Tamburri and M. Wurster and L. Zhu},
  journal   = {{SICS} Software-Intensive Cyber-Physical Systems},
  title     = {{RADON}: rational decomposition and orchestration for serverless computing},
  year      = {2019},
  month     = {aug},
  number    = {1-2},
  pages     = {77--87},
  volume    = {35},
  doi       = {10.1007/s00450-019-00413-w},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00450-019-00413-w},
}

@Article{Mondal2021,
  author    = {Subrota Kumar Mondal and Rui Pan and H M Dipu Kabir and Tan Tian and Hong-Ning Dai},
  journal   = {The Journal of Supercomputing},
  title     = {Kubernetes in {IT} administration and serverless computing: An empirical study and research challenges},
  year      = {2021},
  month     = {jul},
  doi       = {10.1007/s11227-021-03982-3},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11227-021-03982-3},
}

@InCollection{Raj2017,
  author    = {Pethuru Raj and Parvathy Arulmozhi and Nithya Chidambaram},
  booktitle = {Requirements Engineering for Service and Cloud Computing},
  publisher = {Springer International Publishing},
  title     = {The Requirements Elicitation Approaches for Software-Defined Cloud Environments},
  year      = {2017},
  pages     = {89--117},
  doi       = {10.1007/978-3-319-51310-2_5},
  url       = {https://doi.org/10.1007%2F978-3-319-51310-2_5},
}

@Article{Wang2021,
  author    = {Yingying Wang and Harshavardhan Kadiyala and Julia Rubin},
  journal   = {Empirical Software Engineering},
  title     = {Promises and challenges of microservices: an exploratory study},
  year      = {2021},
  month     = {may},
  number    = {4},
  volume    = {26},
  doi       = {10.1007/s10664-020-09910-y},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10664-020-09910-y},
}

@InCollection{Tender2019,
  author    = {Peter De Tender and David Rendon and Samuel Erskine},
  booktitle = {Pro Azure Governance and Security},
  publisher = {Apress},
  title     = {Optimizing {IT} Operations Using Azure Monitor and Log Analytics},
  year      = {2019},
  pages     = {181--228},
  doi       = {10.1007/978-1-4842-4910-9_6},
  url       = {https://doi.org/10.1007%2F978-1-4842-4910-9_6},
}

@InCollection{Raj2018b,
  author    = {Pethuru Raj and Anupama Raman},
  booktitle = {Software-Defined Cloud Centers},
  publisher = {Springer International Publishing},
  title     = {The Distinct Trends and Transitions in the Information Technology ({IT}) Space},
  year      = {2018},
  pages     = {1--12},
  doi       = {10.1007/978-3-319-78637-7_1},
  url       = {https://doi.org/10.1007%2F978-3-319-78637-7_1},
}

@InCollection{Haff2021a,
  author    = {Gordon Haff},
  booktitle = {How Open Source Ate Software},
  publisher = {Apress},
  title     = {From {\textquotedblleft}Free{\textquotedblright} to {\textquotedblleft}Open Source{\textquotedblright} to Products},
  year      = {2021},
  pages     = {27--45},
  doi       = {10.1007/978-1-4842-6800-1_2},
  url       = {https://doi.org/10.1007%2F978-1-4842-6800-1_2},
}

@Article{Monteiro2020a,
  author    = {Davi Monteiro and Paulo Henrique M. Maia and Lincoln S. Rocha and Nabor C. Mendon{\c{c}}a},
  journal   = {Service Oriented Computing and Applications},
  title     = {Building orchestrated microservice systems using declarative business processes},
  year      = {2020},
  month     = {aug},
  number    = {4},
  pages     = {243--268},
  volume    = {14},
  doi       = {10.1007/s11761-020-00300-2},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11761-020-00300-2},
}

@InCollection{Andreou2021,
  author    = {Andreas S. Andreou and Andreas Christoforou},
  booktitle = {Next-Gen Digital Services. A Retrospective and Roadmap for Service Computing of the Future},
  publisher = {Springer International Publishing},
  title     = {On the Migration to and Synthesis of (Micro-)services: The Use of Intelligent Techniques},
  year      = {2021},
  pages     = {48--66},
  doi       = {10.1007/978-3-030-73203-5_4},
  url       = {https://doi.org/10.1007%2F978-3-030-73203-5_4},
}

@InCollection{Kousalya2017,
  author    = {G. Kousalya and P. Balakrishnan and C. Pethuru Raj},
  booktitle = {Computer Communications and Networks},
  publisher = {Springer International Publishing},
  title     = {Demystifying the Traits of Software-Defined Cloud Environments ({SDCEs})},
  year      = {2017},
  pages     = {23--53},
  doi       = {10.1007/978-3-319-56982-6_2},
  url       = {https://doi.org/10.1007%2F978-3-319-56982-6_2},
}

@Article{Wang2020b,
  author    = {Sa Wang and Yan-Hai Zhu and Shan-Pei Chen and Tian-Ze Wu and Wen-Jie Li and Xu-Sheng Zhan and Hai-Yang Ding and Wei-Song Shi and Yun-Gang Bao},
  journal   = {Journal of Computer Science and Technology},
  title     = {A Case for Adaptive Resource Management in Alibaba Datacenter Using Neural Networks},
  year      = {2020},
  month     = {jan},
  number    = {1},
  pages     = {209--220},
  volume    = {35},
  doi       = {10.1007/s11390-020-9732-x},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11390-020-9732-x},
}

@Article{OparaMartins2016,
  author    = {Justice Opara-Martins and Reza Sahandi and Feng Tian},
  journal   = {Journal of Cloud Computing},
  title     = {Critical analysis of vendor lock-in and its impact on cloud computing migration: a business perspective},
  year      = {2016},
  month     = {apr},
  number    = {1},
  volume    = {5},
  doi       = {10.1186/s13677-016-0054-z},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1186%2Fs13677-016-0054-z},
}

@InCollection{Andrikopoulos2018,
  author    = {Vasilios Andrikopoulos},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Engineering Cloud-Based Applications: Towards an Application Lifecycle},
  year      = {2018},
  pages     = {57--72},
  doi       = {10.1007/978-3-319-79090-9_4},
  url       = {https://doi.org/10.1007%2F978-3-319-79090-9_4},
}

@InCollection{Fehling2013b,
  author    = {Christoph Fehling and Frank Leymann and Ralph Retter and Walter Schupeck and Peter Arbitter},
  booktitle = {Cloud Computing Patterns},
  publisher = {Springer Vienna},
  title     = {Cloud Offering Patterns},
  year      = {2013},
  month     = {may},
  pages     = {79--150},
  doi       = {10.1007/978-3-7091-1568-8_3},
  url       = {https://doi.org/10.1007%2F978-3-7091-1568-8_3},
}

@Article{Falkenthal2019,
  author    = {Michael Falkenthal and Uwe Breitenbücher and Johanna Barzen and Frank Leymann},
  journal   = {{SICS} Software-Intensive Cyber-Physical Systems},
  title     = {On the algebraic properties of concrete solution aggregation},
  year      = {2019},
  month     = {feb},
  number    = {2-3},
  pages     = {117--128},
  volume    = {34},
  doi       = {10.1007/s00450-019-00400-1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00450-019-00400-1},
}

@Article{Rahman2020,
  author    = {Sabidur Rahman and Tanjila Ahmed and Sifat Ferdousi and Partha Bhaumik and Pulak Chowdhury and Massimo Tornatore and Goutam Das and Biswanath Mukherjee},
  journal   = {Photonic Network Communications},
  title     = {Virtualized controller placement for multi-domain optical transport networks using machine learning},
  year      = {2020},
  month     = {jul},
  number    = {3},
  pages     = {126--136},
  volume    = {40},
  doi       = {10.1007/s11107-020-00895-8},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11107-020-00895-8},
}

@Article{ParviziMosaed2014,
  author    = {Alireza Parvizi-Mosaed and Shahrouz Moaven and Jafar Habibi and Ghazaleh Beigi and Mahdieh Naser-Shariat},
  journal   = {Frontiers of Information Technology {\&} Electronic Engineering},
  title     = {Towards a self-adaptive service-oriented methodology based on extended {SOMA}},
  year      = {2014},
  month     = {dec},
  number    = {1},
  pages     = {43--69},
  volume    = {16},
  doi       = {10.1631/fitee.1400040},
  publisher = {Zhejiang University Press},
  url       = {https://doi.org/10.1631%2Ffitee.1400040},
}

@InCollection{Apel2019,
  author    = {Sebastian Apel and Florian Hertrampf and Steffen Späthe},
  booktitle = {Innovations for Community Services},
  publisher = {Springer International Publishing},
  title     = {Towards a Metrics-Based Software Quality Rating for a Microservice Architecture},
  year      = {2019},
  pages     = {205--220},
  doi       = {10.1007/978-3-030-22482-0_15},
  url       = {https://doi.org/10.1007%2F978-3-030-22482-0_15},
}

@InCollection{Avritzer2018,
  author    = {Alberto Avritzer and Vincenzo Ferme and Andrea Janes and Barbara Russo and Henning Schulz and Andr{\'{e}} van Hoorn},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {A Quantitative Approach for the Assessment of Microservice Architecture Deployment Alternatives by Automated Performance Testing},
  year      = {2018},
  pages     = {159--174},
  doi       = {10.1007/978-3-030-00761-4_11},
  url       = {https://doi.org/10.1007%2F978-3-030-00761-4_11},
}

@InCollection{Oliveira2014,
  author    = {Lucas Bueno Ruas Oliveira and Elena Leroux and Katia Romero Felizardo and Flavio Oquendo and Elisa Yumi Nakagawa},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Towards a Process to Design Architectures of Service-Oriented Robotic Systems},
  year      = {2014},
  pages     = {218--225},
  doi       = {10.1007/978-3-319-09970-5_20},
  url       = {https://doi.org/10.1007%2F978-3-319-09970-5_20},
}

@InCollection{Athanasopoulos2021,
  author    = {Dionysis Athanasopoulos and Daniel Keenan},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {Stability Metrics for Continuous Integration of Service-Oriented Systems},
  year      = {2021},
  pages     = {139--147},
  doi       = {10.1007/978-3-030-74296-6_11},
  url       = {https://doi.org/10.1007%2F978-3-030-74296-6_11},
}

@Book{Jung2021,
  author    = {Jürgen Jung and Bardo Fraunholz},
  publisher = {Springer International Publishing},
  title     = {Masterclass Enterprise Architecture Management},
  year      = {2021},
  doi       = {10.1007/978-3-030-78495-9},
  url       = {https://doi.org/10.1007%2F978-3-030-78495-9},
}

@Article{Mohsin2018,
  author    = {Ahmad Mohsin and Naeem Khalid Janjua},
  journal   = {Service Oriented Computing and Applications},
  title     = {A review and future directions of {SOA}-based software architecture modeling approaches for System of Systems},
  year      = {2018},
  month     = {oct},
  number    = {3-4},
  pages     = {183--200},
  volume    = {12},
  doi       = {10.1007/s11761-018-0245-1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11761-018-0245-1},
}

@InCollection{Ntentos2020a,
  author    = {Evangelos Ntentos and Uwe Zdun and Konstantinos Plakidas and Sebastian Meixner and Sebastian Geiger},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Assessing Architecture Conformance to Coupling-Related Patterns and Practices in Microservices},
  year      = {2020},
  pages     = {3--20},
  doi       = {10.1007/978-3-030-58923-3_1},
  url       = {https://doi.org/10.1007%2F978-3-030-58923-3_1},
}

@InCollection{Salgado2016,
  author    = {Carlos E. Salgado and Ricardo J. Machado and Rita S. P. Maciel},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {A Three-Dimensional Approach for a Quality-Based Alignment Between Requirements and Architecture},
  year      = {2016},
  pages     = {112--125},
  doi       = {10.1007/978-3-319-32689-4_9},
  url       = {https://doi.org/10.1007%2F978-3-319-32689-4_9},
}

@InCollection{Manuali2015,
  author    = {Carlo Manuali and Antonio Lagan{\`{a}}},
  booktitle = {Computational Science and Its Applications -- {ICCSA} 2015},
  publisher = {Springer International Publishing},
  title     = {A Trial User, Resources and Services Quality Evaluation for Grid Communities Sustainability},
  year      = {2015},
  pages     = {324--338},
  doi       = {10.1007/978-3-319-21407-8_24},
  url       = {https://doi.org/10.1007%2F978-3-319-21407-8_24},
}

@InCollection{Bogner2020,
  author    = {Justus Bogner and Stefan Wagner and Alfred Zimmermann},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Collecting Service-Based Maintainability Metrics from {RESTful} {API} Descriptions: Static Analysis and Threshold Derivation},
  year      = {2020},
  pages     = {215--227},
  doi       = {10.1007/978-3-030-59155-7_16},
  url       = {https://doi.org/10.1007%2F978-3-030-59155-7_16},
}

@InCollection{Wu2018a,
  author    = {Wensheng Wu and Yuanfang Cai and Rick Kazman and Ran Mo and Zhipeng Liu and Rongbiao Chen and Yingan Ge and Weicai Liu and Junhui Zhang},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Software Architecture Measurement{\textemdash}Experiences from a Multinational Company},
  year      = {2018},
  pages     = {303--319},
  doi       = {10.1007/978-3-030-00761-4_20},
  url       = {https://doi.org/10.1007%2F978-3-030-00761-4_20},
}

@InCollection{BaniIsmail2018,
  author    = {Basel Bani-Ismail and Youcef Baghdadi},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {A Survey of Existing Evaluation Frameworks for Service Identification Methods: Towards a Comprehensive Evaluation Framework},
  year      = {2018},
  pages     = {191--202},
  doi       = {10.1007/978-3-319-95204-8_17},
  url       = {https://doi.org/10.1007%2F978-3-319-95204-8_17},
}

@InCollection{Ntentos2019,
  author    = {Evangelos Ntentos and Uwe Zdun and Konstantinos Plakidas and Daniel Schall and Fei Li and Sebastian Meixner},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Supporting Architectural Decision Making on Data Management in Microservice Architectures},
  year      = {2019},
  pages     = {20--36},
  doi       = {10.1007/978-3-030-29983-5_2},
  url       = {https://doi.org/10.1007%2F978-3-030-29983-5_2},
}

@InCollection{Kirstein2020,
  author    = {Fabian Kirstein and Kyriakos Stefanidis and Benjamin Dittwald and Simon Dutkowski and Sebastian Urbanek and Manfred Hauswirth},
  booktitle = {The Semantic Web},
  publisher = {Springer International Publishing},
  title     = {Piveau: A Large-Scale Open Data Management Platform Based on Semantic Web Technologies},
  year      = {2020},
  pages     = {648--664},
  doi       = {10.1007/978-3-030-49461-2_38},
  url       = {https://doi.org/10.1007%2F978-3-030-49461-2_38},
}

@InCollection{Zdun2017,
  author    = {Uwe Zdun and Elena Navarro and Frank Leymann},
  booktitle = {Service-Oriented Computing},
  publisher = {Springer International Publishing},
  title     = {Ensuring and Assessing Architecture Conformance to Microservice Decomposition Patterns},
  year      = {2017},
  pages     = {411--429},
  doi       = {10.1007/978-3-319-69035-3_29},
  url       = {https://doi.org/10.1007%2F978-3-319-69035-3_29},
}

@Article{Detten2013,
  author    = {Markus von Detten and Marie Christin Platenius and Steffen Becker},
  journal   = {Software {\&} Systems Modeling},
  title     = {Reengineering component-based software systems with Archimetrix},
  year      = {2013},
  month     = {apr},
  number    = {4},
  pages     = {1239--1268},
  volume    = {13},
  doi       = {10.1007/s10270-013-0341-9},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10270-013-0341-9},
}

@InCollection{Salgado2015,
  author    = {Carlos E. Salgado and Ricardo J. Machado and Rita S. P. Maciel},
  booktitle = {New Contributions in Information Systems and Technologies},
  publisher = {Springer International Publishing},
  title     = {Aligning Business Requirements with Services Quality Characteristics by Using Logical Architectures},
  year      = {2015},
  pages     = {593--602},
  doi       = {10.1007/978-3-319-16486-1_58},
  url       = {https://doi.org/10.1007%2F978-3-319-16486-1_58},
}

@Article{Rajesh2017,
  author    = {Sudha Rajesh and A. Chandrasekar},
  journal   = {Cluster Computing},
  title     = {Esteemed software patterns for banking system},
  year      = {2017},
  month     = {nov},
  number    = {S5},
  pages     = {11087--11099},
  volume    = {22},
  doi       = {10.1007/s10586-017-1304-7},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10586-017-1304-7},
}

@InCollection{Ye2021,
  author    = {Kailing Ye and Huiqun Yu and Guisheng Fan and Liqiong Chen},
  booktitle = {Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering},
  publisher = {Springer International Publishing},
  title     = {{EFMLP}: A Novel Model for Web Service {QoS} Prediction},
  year      = {2021},
  pages     = {375--385},
  doi       = {10.1007/978-3-030-67540-0_22},
  url       = {https://doi.org/10.1007%2F978-3-030-67540-0_22},
}

@InCollection{Wen2017,
  author    = {Bin Wen and Ziqiang Luo and Song Lin},
  booktitle = {Collaborate Computing: Networking, Applications and Worksharing},
  publisher = {Springer International Publishing},
  title     = {Runtime Exceptions Handling for Collaborative {SOA} Applications},
  year      = {2017},
  pages     = {252--261},
  doi       = {10.1007/978-3-319-59288-6_23},
  url       = {https://doi.org/10.1007%2F978-3-319-59288-6_23},
}

@InCollection{Mohammadi2019a,
  author    = {Nazila Gol Mohammadi},
  booktitle = {Trustworthy Cyber-Physical Systems},
  publisher = {Springer Fachmedien Wiesbaden},
  title     = {Computational Approach towards End-to-End Trustworthiness Evaluation},
  year      = {2019},
  pages     = {185--213},
  doi       = {10.1007/978-3-658-27488-7_10},
  url       = {https://doi.org/10.1007%2F978-3-658-27488-7_10},
}

@InCollection{Schnoor2019,
  author    = {Henning Schnoor and Wilhelm Hasselbring},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Comparing Static and Dynamic Weighted Software Coupling Metrics},
  year      = {2019},
  pages     = {285--298},
  doi       = {10.1007/978-3-030-30275-7_22},
  url       = {https://doi.org/10.1007%2F978-3-030-30275-7_22},
}

@InCollection{ColomoPalacios2016,
  author    = {Ricardo Colomo-Palacios and Luis Omar Colombo-Mendoza and Rafael Valencia-Garc{\'{\i}}a},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Towards Supporting International Standard-Based Software Engineering Approaches Using Semantic Web Technologies: A Systematic Literature Review},
  year      = {2016},
  pages     = {169--183},
  doi       = {10.1007/978-3-319-48024-4_14},
  url       = {https://doi.org/10.1007%2F978-3-319-48024-4_14},
}

@Article{Haupt2017,
  author    = {Florian Haupt and Frank Leymann and Karolina Vukojevic-Haupt},
  journal   = {Computer Science - Research and Development},
  title     = {{API} governance support through the structural analysis of {REST} {APIs}},
  year      = {2017},
  month     = {sep},
  number    = {3-4},
  pages     = {291--303},
  volume    = {33},
  doi       = {10.1007/s00450-017-0384-1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00450-017-0384-1},
}

@InCollection{Sabou2018,
  author    = {Marta Sabou and Fajar J. Ekaputra and Tudor Ionescu and Juergen Musil and Daniel Schall and Kevin Haller and Armin Friedl and Stefan Biffl},
  booktitle = {The Semantic Web},
  publisher = {Springer International Publishing},
  title     = {Exploring Enterprise Knowledge Graphs: A Use Case in Software Engineering},
  year      = {2018},
  pages     = {560--575},
  doi       = {10.1007/978-3-319-93417-4_36},
  url       = {https://doi.org/10.1007%2F978-3-319-93417-4_36},
}

@Article{Oliveira2015a,
  author    = {Nuno Oliveira and Lu{\'{\i}}s S Barbosa},
  journal   = {Journal of Software Engineering Research and Development},
  title     = {Self-adaptation by coordination-targeted reconfigurations},
  year      = {2015},
  month     = {may},
  number    = {1},
  volume    = {3},
  doi       = {10.1186/s40411-015-0021-2},
  publisher = {Sociedade Brasileira de Computacao - {SB}},
  url       = {https://doi.org/10.1186%2Fs40411-015-0021-2},
}

@InCollection{Orlowski2018,
  author    = {Cezary Orlowski and Arkadiusz Sarzy{\'{n}}ski and Kostas Karatzas and Nikos Katsifarakis},
  booktitle = {Transactions on Computational Collective Intelligence {XXXI}},
  publisher = {Springer Berlin Heidelberg},
  title     = {Decision Processes Based on {IoT} Data for Sustainable Smart Cities},
  year      = {2018},
  pages     = {136--146},
  doi       = {10.1007/978-3-662-58464-4_12},
  url       = {https://doi.org/10.1007%2F978-3-662-58464-4_12},
}

@InCollection{Abrahao2016,
  author    = {Silvia Abrahao and Maria Teresa Baldassarre and Danilo Caivano and Yvonne Dittrich and Rosa Lanzilotti and Antonio Piccinno},
  booktitle = {Product-Focused Software Process Improvement},
  publisher = {Springer International Publishing},
  title     = {Human Factors in Software Development Processes: Measuring System Quality},
  year      = {2016},
  pages     = {691--696},
  doi       = {10.1007/978-3-319-49094-6_57},
  url       = {https://doi.org/10.1007%2F978-3-319-49094-6_57},
}

@InCollection{Komarek2015,
  author    = {Ales Komarek and Jakub Pavlik and Vladimir Sobeslav},
  booktitle = {Computational Collective Intelligence},
  publisher = {Springer International Publishing},
  title     = {Network Visualization Survey},
  year      = {2015},
  pages     = {275--284},
  doi       = {10.1007/978-3-319-24306-1_27},
  url       = {https://doi.org/10.1007%2F978-3-319-24306-1_27},
}

@Article{Anastasopoulos2015,
  author    = {Markos P. Anastasopoulos and Anna Tzanakaki and Bijan Rahimzadeh Rofoee and Shuping Peng and Yan Yan and Dimitra Simeonidou and Giada Landi and Giacomo Bernini and Nicola Ciulli and Jordi Ferrer Riera and Eduard Escalona and Kostas Katsalis and Thanasis Korakis},
  journal   = {Photonic Network Communications},
  title     = {Optical wireless network convergence in support of energy-efficient mobile cloud services},
  year      = {2015},
  month     = {apr},
  number    = {3},
  pages     = {269--281},
  volume    = {29},
  doi       = {10.1007/s11107-015-0494-2},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11107-015-0494-2},
}

@InCollection{Tsoumas2020,
  author    = {Ilias Tsoumas and Chrysostomos Symvoulidis and Dimosthenis Kyriazis and Panagiotis Gouvas and Anastasios Zafeiropoulos and Javier Melian and Janez Sterle},
  booktitle = {Information Systems},
  publisher = {Springer International Publishing},
  title     = {Modelling 5G Cloud-Native Applications by Exploiting the Service Mesh Paradigm},
  year      = {2020},
  pages     = {151--162},
  doi       = {10.1007/978-3-030-44322-1_12},
  url       = {https://doi.org/10.1007%2F978-3-030-44322-1_12},
}

@InCollection{Kratzke2017,
  author    = {Nane Kratzke and Peter-Christian Quint},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Investigation of Impacts on Network Performance in the Advance of a Microservice Design},
  year      = {2017},
  pages     = {187--208},
  doi       = {10.1007/978-3-319-62594-2_10},
  url       = {https://doi.org/10.1007%2F978-3-319-62594-2_10},
}

@InCollection{Mercl2019,
  author    = {Lubos Mercl and Jakub Pavlik},
  booktitle = {Computational Collective Intelligence},
  publisher = {Springer International Publishing},
  title     = {Public Cloud Kubernetes Storage Performance Analysis},
  year      = {2019},
  pages     = {649--660},
  doi       = {10.1007/978-3-030-28374-2_56},
  url       = {https://doi.org/10.1007%2F978-3-030-28374-2_56},
}

@InCollection{Mandal2021,
  author    = {Atri Mandal and Saranya Gupta and Shivali Agarwal and Prateeti Mohapatra},
  booktitle = {Advances in Knowledge Discovery and Data Mining},
  publisher = {Springer International Publishing},
  title     = {Improved Topology Extraction Using Discriminative Parameter Mining of Logs},
  year      = {2021},
  pages     = {333--345},
  doi       = {10.1007/978-3-030-75762-5_27},
  url       = {https://doi.org/10.1007%2F978-3-030-75762-5_27},
}

@InCollection{Laszkowski2020,
  author    = {Juan Francisco Ribera Laszkowski and Andy Edmonds and Piyush Harsh and Francisco Gortazar and Thomas Michael Bohnert},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {{ElasTest}: An Elastic Platform for E2E Testing Complex Distributed Large Software Systems},
  year      = {2020},
  pages     = {210--218},
  doi       = {10.1007/978-3-030-63161-1_20},
  url       = {https://doi.org/10.1007%2F978-3-030-63161-1_20},
}

@InCollection{Poth2018,
  author    = {Alexander Poth and Mark Werner and Xinyan Lei},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {How to Deliver Faster with {CI}/{CD} Integrated Testing Services?},
  year      = {2018},
  pages     = {401--409},
  doi       = {10.1007/978-3-319-97925-0_33},
  url       = {https://doi.org/10.1007%2F978-3-319-97925-0_33},
}

@InCollection{Rosati2019,
  author    = {Pierangelo Rosati and Frank Fowley and Claus Pahl and Davide Taibi and Theo Lynn},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Right Scaling for Right Pricing: A Case Study on Total Cost of Ownership Measurement for Cloud Migration},
  year      = {2019},
  pages     = {190--214},
  doi       = {10.1007/978-3-030-29193-8_10},
  url       = {https://doi.org/10.1007%2F978-3-030-29193-8_10},
}

@InCollection{Gutierrez2017,
  author    = {Felipe Gutierrez},
  booktitle = {Spring Boot Messaging},
  publisher = {Apress},
  title     = {Microservices},
  year      = {2017},
  pages     = {179--192},
  doi       = {10.1007/978-1-4842-1224-0_11},
  url       = {https://doi.org/10.1007%2F978-1-4842-1224-0_11},
}

@Book{Daimi2021,
  editor    = {Kevin Daimi and Cathryn Peoples},
  publisher = {Springer International Publishing},
  title     = {Advances in Cybersecurity Management},
  year      = {2021},
  doi       = {10.1007/978-3-030-71381-2},
  url       = {https://doi.org/10.1007%2F978-3-030-71381-2},
}

@InCollection{Palma2021,
  author    = {Stefano Dalla Palma and Martin Garriga and Dario Di Nucci and Damian Andrew Tamburri and Willem-Jan Van Den Heuvel},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {{DevOps} and Quality Management in Serverless Computing: The {RADON} Approach},
  year      = {2021},
  pages     = {155--160},
  doi       = {10.1007/978-3-030-71906-7_13},
  url       = {https://doi.org/10.1007%2F978-3-030-71906-7_13},
}

@InCollection{Banijamali2020,
  author    = {Ahmad Banijamali and Pasi Kuvaja and Markku Oivo and Pooyan Jamshidi},
  booktitle = {Product-Focused Software Process Improvement},
  publisher = {Springer International Publishing},
  title     = {Kuksa{\textdollar}{\textdollar}{\^{}}$\lbrace${\ast}$\rbrace${\textdollar}{\textdollar}: Self-adaptive Microservices in Automotive Systems},
  year      = {2020},
  pages     = {367--384},
  doi       = {10.1007/978-3-030-64148-1_23},
  url       = {https://doi.org/10.1007%2F978-3-030-64148-1_23},
}

@InCollection{Gutierrez2018,
  author    = {Felipe Gutierrez},
  booktitle = {Pro Spring Boot 2},
  publisher = {Apress},
  title     = {Spring Boot in the Cloud},
  year      = {2018},
  month     = {dec},
  pages     = {433--454},
  doi       = {10.1007/978-1-4842-3676-5_12},
  url       = {https://doi.org/10.1007%2F978-1-4842-3676-5_12},
}

@InCollection{Bagnato2019,
  author    = {Alessandra Bagnato and Davide Fucci},
  booktitle = {Product-Focused Software Process Improvement},
  publisher = {Springer International Publishing},
  title     = {European Project Space Papers for the {PROFES} 2019 - Summary},
  year      = {2019},
  pages     = {573--576},
  doi       = {10.1007/978-3-030-35333-9_40},
  url       = {https://doi.org/10.1007%2F978-3-030-35333-9_40},
}

@InCollection{Floerecke2019,
  author    = {Sebastian Floerecke and Franz Lehner},
  booktitle = {Economics of Grids, Clouds, Systems, and Services},
  publisher = {Springer International Publishing},
  title     = {Business Model Characteristics for Local {IaaS} Providers for Counteracting the Dominance of the Hyperscalers},
  year      = {2019},
  pages     = {137--150},
  doi       = {10.1007/978-3-030-13342-9_12},
  url       = {https://doi.org/10.1007%2F978-3-030-13342-9_12},
}

@InCollection{Gole2020b,
  author    = {Vinayak Gole and Shreekant Shiralkar},
  booktitle = {Empower Decision Makers with {SAP} Analytics Cloud},
  publisher = {Apress},
  title     = {Product Road Map and Future Direction for {SAC}},
  year      = {2020},
  pages     = {265--301},
  doi       = {10.1007/978-1-4842-6097-5_10},
  url       = {https://doi.org/10.1007%2F978-1-4842-6097-5_10},
}

@InCollection{Raj2015,
  author    = {Pethuru Raj and Anupama Raman and Dhivya Nagaraj and Siddhartha Duggirala},
  booktitle = {Computer Communications and Networks},
  publisher = {Springer International Publishing},
  title     = {Big and Fast Data Analytics Yearning for High-Performance Computing},
  year      = {2015},
  pages     = {67--99},
  doi       = {10.1007/978-3-319-20744-5_3},
  url       = {https://doi.org/10.1007%2F978-3-319-20744-5_3},
}

@InCollection{Millward2021,
  author    = {Douglas J. Millward and Nkaepe Olaniyi and Cathryn Peoples},
  booktitle = {Advances in Cybersecurity Management},
  publisher = {Springer International Publishing},
  title     = {The New Normal: Cybersecurity and Associated Drivers for a Post-{COVID}-19 Cloud},
  year      = {2021},
  pages     = {397--417},
  doi       = {10.1007/978-3-030-71381-2_18},
  url       = {https://doi.org/10.1007%2F978-3-030-71381-2_18},
}

@InCollection{Shabelnyk2021,
  author    = {Oleksandr Shabelnyk and Pantelis A. Frangoudis and Schahram Dustdar and Christos Tsigkanos},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Updating Service-Based Software Systems in Air-Gapped Environments},
  year      = {2021},
  pages     = {147--163},
  doi       = {10.1007/978-3-030-86044-8_10},
  url       = {https://doi.org/10.1007%2F978-3-030-86044-8_10},
}

@InCollection{Floerecke2018,
  author    = {Sebastian Floerecke},
  booktitle = {Exploring Service Science},
  publisher = {Springer International Publishing},
  title     = {Success Factors of {SaaS} Providers' Business Models {\textendash} An Exploratory Multiple-Case Study},
  year      = {2018},
  pages     = {193--207},
  doi       = {10.1007/978-3-030-00713-3_15},
  url       = {https://doi.org/10.1007%2F978-3-030-00713-3_15},
}

@InProceedings{Ntentos2020,
  author       = {Ntentos, Evangelos and Zdun, Uwe and Plakidas, Konstantinos and Meixner, Sebastian and Geiger, Sebastian},
  booktitle    = {International Conference on Service-Oriented Computing},
  title        = {Metrics for Assessing Architecture Conformance to Microservice Architecture Patterns and Practices},
  year         = {2020},
  organization = {Springer},
  pages        = {580--596},
  doi          = {10.1007/978-3-030-65310-1_42},
  url          = {https://doi.org/10.1007/978-3-030-65310-1_42},
}

@Comment{jabref-meta: databaseType:bibtex;}
