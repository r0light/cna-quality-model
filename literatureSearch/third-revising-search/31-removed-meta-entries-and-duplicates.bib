@inproceedings{10.1145/2661829.2661991,
author = {Anchuri, Pranay and Sumbaly, Roshan and Shah, Sam},
title = {Hotspot Detection in a Service-Oriented Architecture},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661991},
doi = {10.1145/2661829.2661991},
abstract = {Large-scale websites are predominantly built as a service-oriented architecture. Here, services are specialized for a certain task, run on multiple machines, and communicate with each other to serve a user's request. Reducing latency and improving the cost to serve is quite important, but optimizing this service call graph is particularly challenging due to the volume of data and the graph's non-uniform and dynamic nature.In this paper, we present a framework to detect hotspots in a service-oriented architecture. The framework is general, in that it can handle arbitrary objective functions. We show that finding the optimal set of hotspots for a metric, such as latency, is NP-complete and propose a greedy algorithm by relaxing some constraints. We use a pattern mining algorithm to rank hotspots based on the impact and consistency. Experiments on real world service call graphs from LinkedIn, the largest online professional social network, show that our algorithm consistently outperforms baseline methods.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {1749–1758},
numpages = {10},
keywords = {service-oriented architecture, monitoring, call graph, hotspots},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/2593501.2593505,
author = {Miranda, Breno and Bertolino, Antonia},
title = {Social Coverage for Customized Test Adequacy and Selection Criteria},
year = {2014},
isbn = {9781450328586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593501.2593505},
doi = {10.1145/2593501.2593505},
abstract = {Test coverage information can be very useful for guiding testers in enhancing their test suites to exercise possible uncovered entities and in deciding when to stop testing. However, for complex applications that are reused in different contexts and for emerging paradigms (e.g., component-based development, service-oriented architecture, and cloud computing), traditional coverage metrics may no longer provide meaningful information to help testers on these tasks. Various proposals are advocating to leverage information that come from the testing community in a collaborative testing approach. In this work we introduce a coverage metric, the Social Coverage, that customizes coverage information in a given context based on coverage data collected from similar users. To evaluate the potential of our proposed approach, we instantiated the social coverage metric in the context of a real world service oriented application. In this exploratory study, we were able to predict the entities that would be of interest for a given user with an average precision of 97\% and average recall of 75\%. Our results suggest that, in similar environments, social coverage can provide a better support to testers than traditional coverage.},
booktitle = {Proceedings of the 9th International Workshop on Automation of Software Test},
pages = {22–28},
numpages = {7},
keywords = {User Similarity, Coverage Testing, Service-Oriented Application, Relative Coverage},
location = {Hyderabad, India},
series = {AST 2014}
}

@article{10.1145/2579281.2579294,
author = {Castelluccia, Daniela and Boffoli, Nicola},
title = {Service-Oriented Product Lines: A Systematic Mapping Study},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579294},
doi = {10.1145/2579281.2579294},
abstract = {Software product line engineering and service-oriented architectures both enable organizations to capitalize on reuse of existing software assets and capabilities and improve competitive advantage in terms of development savings, product flexibility, time-to-market. Both approaches accommodate variation of assets, including services, by changing the software being reused or composing services according a new orchestration. Therefore, variability management in Service-oriented Product Lines (SoPL) is one of the main challenges today. In order to highlight the emerging evidence-based results from the research community, we apply the well-defined method of systematic mapping in order to populate a classification scheme for the SoPL field of interest. The analysis of results throws light on the current open issues. Moreover, different facets of the scheme can be combined to answer more specific research questions. The report reveals the need for more empirical research able to provide new metrics measuring efficiency and efficacy of the proposed models, new methods and tools supporting variability management in SoPL, especially during maintenance and verification and validation. The mapping study about SoPL opens further investigations by means of a complete systematic review to select and validate the most efficient solutions to variability management in SoPL.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–6},
numpages = {6},
keywords = {empirical study, service-oriented architecture, product line development, mapping study, variability management, service-oriented computing, software product line}
}

@inproceedings{10.1145/3018896.3018961,
author = {Lehmann, Martin and Sandnes, Frode Eika},
title = {A Framework for Evaluating Continuous Microservice Delivery Strategies},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018961},
doi = {10.1145/3018896.3018961},
abstract = {The emergence of service-oriented computing, and in particular microservice architecture, has introduced a new layer of complexity to the already challenging task of continuously delivering changes to the end users. Cloud computing has turned scalable hardware into a commodity, but also imposes some requirements on the software development process. Yet, the literature mainly focuses on quantifiable metrics such as number of manual steps and lines of code required to make a change. The industry, on the other hand, appears to focus more on qualitative metrics such as increasing the productivity of their developers. These are common goals, but must be measured using different approaches. Therefore, based on interviews of industry stakeholders a framework for evaluating and comparing approaches to continuous microservice delivery is proposed. We show that it is possible to efficiently evaluate and compare strategies for continuously delivering microservices.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {64},
numpages = {9},
keywords = {continuous deployment, deployment strategy, cloud computing, microservices, evaluation framework, microservice architectures},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3207677.3277935,
author = {Luo, Ning and Zhao, Jun},
title = {Storage System Based on Norm and BPMN Mapping Research on Business Process Modeling},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3277935},
doi = {10.1145/3207677.3277935},
abstract = {Aiming1 at the problems of large variety of goods, large quantities of materials, and non-standard procurement and transportation in warehousing enterprises, a modeling method based on the specification and BPMN2.0 mapping was proposed. Through the analysis of the actual warehousing business process management, the service-oriented system modeling based on the warehouse management requirements, the study of the warehousing management system architecture based on the specification, the implementation of the warehousing system specification to BPMN mapping, a better implementation the purpose of warehousing service flexibility.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {163},
numpages = {5},
keywords = {business process modeling, flexibility semantic loss, BPMN2, Norm},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/3034950.3034975,
author = {Ke, Weimao},
title = {Distributed Search Efficiency and Robustness in Service Oriented Multi-Agent Networks},
year = {2017},
isbn = {9781450348348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3034950.3034975},
doi = {10.1145/3034950.3034975},
abstract = {We study decentralized searches in a large service-oriented agent network and investigate the influences of multiple factors on search efficiency. In this study we focus on overall system robustness and examine search performance in unstable environments where individual agents may fail or a system-wide attack may occur. Experimental results show that searches continue to be efficient when a large number of service agents become unavailable. Surprisingly, overall system performance in terms of a search path length metric improves with an increasing number of unavailable agents. Service unavailability also has an impact on the load balance of service agents. We plan to conduct further research to verify observed patterns and to understand related implications on system architecture design.},
booktitle = {Proceedings of the 2017 International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {9–18},
numpages = {10},
keywords = {robustness, service agents, distributed search, information network},
location = {Wuhan, China},
series = {ICMSS '17}
}

@inproceedings{10.1145/3229345.3229419,
author = {Oliveira, Joyce Aline and Vargas, Matheus and Rodrigues, Roni},
title = {SOA Reuse: Systematic Literature Review Updating and Research Directions},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229419},
doi = {10.1145/3229345.3229419},
abstract = {Service Oriented Architecture (SOA) reuse has been used strategically in organizations to reduce development costs and increase the quality of applications. This article analyzes a systematic literature review in order to identify concepts, goals, strategies, and metrics of SOA reuse. The results show that the main goal of SOA reuse is to decrease development costs. The factor that most negatively influences SOA reuse is the existence of legacy systems. The strategy used most to potentialize SOA reuse is business process management. Metrics proposed by studies to measure SOA reuse are related to modularity and adaptability indicators. The study is relevant because it increases the body of knowledge of the area. Additionally, a set of gaps to be addressed by researchers and reuse practitioners was identified.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {71},
numpages = {8},
keywords = {systematic literature review, Service Oriented Architecture, SOA reuse},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@inproceedings{10.1145/3284557.3284713,
author = {Rudorfer, Martin and Pannen, Tessa J. and Kr\"{u}ger, J\"{o}rg},
title = {A Case Study on Granularity of Industrial Vision Services},
year = {2018},
isbn = {9781450366281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284557.3284713},
doi = {10.1145/3284557.3284713},
abstract = {Software engineering paradigms such as service-oriented architectures are increasingly often applied in the field of factory automation. Functions like robot motion planning or object recognition are provided by cloud services. A crucial architectural aspect is the granularity, i.e. the scope and size of individual services. In our case study, we examine a service-based object recognition application for a robotic assembly use case. We implement three different granularity levels, measure their communication and computation times and discuss further architectural features. The fine-granular approach encapsulates individual image processing operations as services, which have high reusability but impose large communication overheads. The medium granularity approach is object-wise and offers best reuse efficiency and cohesion. The coarse solution offers the best performance.},
booktitle = {Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control},
articleno = {59},
numpages = {6},
keywords = {Service-Oriented Architecture, Object Recognition, Granularity},
location = {Stockholm, Sweden},
series = {ISCSIC '18}
}

@inproceedings{10.1145/2568225.2568230,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Integrating Adaptive User Interface Capabilities in Enterprise Applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568230},
doi = {10.1145/2568225.2568230},
abstract = {Many existing enterprise applications are at a mature stage in their development and are unable to easily benefit from the usability gains offered by adaptive user interfaces (UIs). Therefore, a method is needed for integrating adaptive UI capabilities into these systems without incurring a high cost or significantly disrupting the way they function. This paper presents a method for integrating adaptive UI behavior in enterprise applications based on CEDAR, a model-driven, service-oriented, and tool-supported architecture for devising adaptive enterprise application UIs. The proposed integration method is evaluated with a case study, which includes establishing and applying technical metrics to measure several of the method’s properties using the open-source enterprise application OFBiz as a test-case. The generality and flexibility of the integration method are also evaluated based on an interview and discussions with practitioners about their real-life projects.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {712–723},
numpages = {12},
keywords = {model-driven engineering, software architectures, Adaptive user interfaces, enterprise systems, integration, software metrics},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3143434.3143457,
author = {Ilin, I. and Levina, A. and Abran, A. and Iliashenko, O.},
title = {Measurement of Enterprise Architecture (EA) from an IT Perspective: Research Gaps and Measurement Avenues},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143457},
doi = {10.1145/3143434.3143457},
abstract = {Reorganizational projects in general and software-related projects in particular, are often implemented with a focus only on the reorganized components within an organizational management system, not taking into account relationships with the other components of an enterprise architecture (EA). This paper first looks at the current state of EA measurement to identify weaknesses and gaps in aligning and measuring EA components, EA structures and EA interrelationships from an IT perspective. It then identifies from related works available innovative measurement concepts that could contribute for aligning, measuring and monitoring software-related projects within an EA strategy. This includes measurement avenues within a Balanced Scorecard (BSC), contributions of functional size measurement to the BSC, and measurement of software structures and functionality within a service-oriented architecture (SOA).},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {232–243},
numpages = {12},
keywords = {functional size measurement (FSM), service-oriented architecture (SOA), function points (FP), enterprise architecture (EA), enterprise architecture measurement, balanced scorecard (BSC)},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@inproceedings{10.1145/3194164.3194166,
author = {Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred},
title = {Limiting Technical Debt with Maintainability Assurance: An Industry Survey on Used Techniques and Differences with Service- and Microservice-Based Systems},
year = {2018},
isbn = {9781450357135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194164.3194166},
doi = {10.1145/3194164.3194166},
abstract = {Maintainability assurance techniques are used to control this quality attribute and limit the accumulation of potentially unknown technical debt. Since the industry state of practice and especially the handling of Service- and Microservice-Based Systems in this regard are not well covered in scientific literature, we created a survey to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related treatment of systems based on service-orientation, and c) influences on developer satisfaction w.r.t. maintainability. 60 software professionals responded to our online questionnaire. The results indicate that using explicit and systematic techniques has benefits for maintainability. The more sophisticated the applied methods the more satisfied participants were with the maintainability of their software while no link to a hindrance in productivity could be established. Other important findings were the absence of architecture-level evolvability control mechanisms as well as a significant neglect of service-oriented particularities for quality assurance. The results suggest that industry has to improve its quality control in these regards to avoid problems with long-living service-based software systems.},
booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
pages = {125–133},
numpages = {9},
keywords = {service-based systems, microservice-based systems, maintainability, industry, survey, software quality control},
location = {Gothenburg, Sweden},
series = {TechDebt '18}
}

@inproceedings{10.1145/3018896.3036365,
author = {Ashrafi, Tasnia H. and Arefin, Sayed E. and Das, Kowshik D. J. and Hossain, Md. A. and Chakrabarty, Amitabha},
title = {FOG Based Distributed IoT Infrastructure},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3036365},
doi = {10.1145/3018896.3036365},
abstract = {The Internet of Things(IoT) can be defined as a network connectivity bridge between people, systems and physical world. With the increasing number of IoT devices and networks, dealing with enormous number of data efficiently is becoming more and more challenging for the present infrastructure which is a very big matter of concern. In this paper, we depicted the current infrastructure and proposed another model of IoT infrastructure to surpass the difficulties of the existing infrastructure, which will be a coordinated effort of Fog computing amalgamation with Machine-to-Machine(M2M) intelligent communication protocol followed by incorporation of Service Oriented Architecture(SOA) and finally integration of Agent based SOA. This model will have the capacity to exchange data by breaking down dependably and methodically with low latency, less bandwidth, heterogeneity in less measure of time maintaining the Quality of Service(QoS) precisely.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {124},
numpages = {13},
keywords = {M2M communication, quality of service (QoS), service oriented architecture(SOA), heterogeneous devices, internet of things (IoT), agent based SOA, fog computing},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/2577036.2577038,
author = {V\"{o}gele, Christian and Brunnert, Andreas and Danciu, Alexandru and Tertilt, Daniel and Krcmar, Helmut},
title = {Using Performance Models to Support Load Testing in a Large SOA Environment},
year = {2014},
isbn = {9781450327626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577036.2577038},
doi = {10.1145/2577036.2577038},
abstract = {Load testing in large service-oriented architecture (SOA) environments is especially challenging when services are under the control of different teams. It gets even more difficult if services need to be scaled before a load test starts. It is thus important to estimate workloads for services involved in a load test. Service workloads can be specified by the amount of service operation invocations distributed over time. We propose the use of performance models to derive this information for SOA-based applications before executing load tests. In a first step, we use these models to select usage scenarios. Afterwards, these models are transformed in a way that each scenario can be simulated separately from each other. These simulations can predict service workloads for selected usage scenarios and different user counts.},
booktitle = {Proceedings of the Third International Workshop on Large Scale Testing},
pages = {5–6},
numpages = {2},
keywords = {performance models, service workload, usage scenario, service-oriented architecture, load testing},
location = {Dublin, Ireland},
series = {LT '14}
}

@inproceedings{10.1145/2642937.2653471,
author = {Miranda, Breno},
title = {A Proposal for Revisiting Coverage Testing Metrics},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2653471},
doi = {10.1145/2642937.2653471},
abstract = {Test coverage information can be very useful for guiding testers in enhancing their test suites to exercise possible uncovered entities and in deciding when to stop testing. Since the concept of test criterion was born, several contributions have been made by both academia and industry in the definition and adaptation of adequacy criteria aiming at ensuring the discovery of more failures. Numerous contributions have also been done in the development of coverage tools. However, for complex applications that are reused in different contexts and for emerging paradigms (e.g., component-based development, service-oriented architecture, and cloud computing), traditional coverage metrics may no longer provide meaningful information to help testers on these tasks. Inspired by the idea of relative coverage this research focuses on the introduction of meaningful coverage metrics to cope with the challenges imposed by the current programming paradigms as well as on the definition of a theoretical framework for the development of relative coverage metrics.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {899–902},
numpages = {4},
keywords = {relative coverage, coverage testing, traditional coverage},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3068126.3068128,
author = {Filelis-Papadopoulos, C. K. and Gravvanis, G. A. and Morrison, J. P.},
title = {CloudLightning Simulation and Evaluation Roadmap},
year = {2017},
isbn = {9781450349369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3068126.3068128},
doi = {10.1145/3068126.3068128},
abstract = {The CloudLightning (CL) system, designed in the frame of the CloudLightning project, is a service-oriented architecture for the emerging large scale heterogeneous cloud. It facilitates a clear distinction between service-lifecyle management and resource-lifecycle management. This separation of concerns is used to make resource management issues tractable at scale and to enable functionality that is currently not naturally covered by the cloud paradigm. In particular, the CL project seeks to maximize computational efficiency of the cloud in a number of specific ways; by exploiting prebuilt HPC environments, by dynamically building HPC instances, by improving server utilization, by reducing power consumption and by improving service delivery. Given the scale and complexity of this project, its utility can presently only be measured through simulation. This paper outlines the parameters, constraints and limitation being considered as part of the design and construction of that simulation environment.},
booktitle = {Proceedings of the 1st International Workshop on Next Generation of Cloud Architectures},
articleno = {2},
numpages = {6},
keywords = {Self - Management, Evaluation, Self - Organisation, CloudLightning},
location = {Belgrade, Serbia},
series = {CloudNG:17}
}

@inproceedings{10.1145/3019612.3019805,
author = {Abid, Ahmed and Messai, Nizar and Rouached, Mohsen and Abid, Mohamed and Devogele, Thomas},
title = {Semantic Similarity Based Web Services Composition Framework},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019805},
doi = {10.1145/3019612.3019805},
abstract = {Computing similarities between Web services is a main concern in Service Oriented Architecture as it allows to decide which services are likely to be matched into a composite workflow, or in other cases, which services can be substituted in order to ensure continuous service availability. With the high maturity achieved by the standards, tools and frameworks in the Semantic Web domain, measuring Web services similarities relies more than ever on semantic descriptions of services as well as on semantic relationships these descriptions may hold. In this paper we present a Framework for Web services composition based on computing semantic similarity between Web services. We particularly focus on Services Matching engine which uses the considered similarity measure first to classify Web services into classes of functionally similar Web services and then to propose a composite sequence of services that matches a requested goal. In both tasks, the presented framework appeals for best known techniques of similarity computing and data and knowledge extraction, respectively.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1319–1325},
numpages = {7},
keywords = {semantic similarity, discovery and composition, web services, matching},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/3127479.3132020,
author = {Suresh, Lalith and Bodik, Peter and Menache, Ishai and Canini, Marco and Ciucu, Florin},
title = {Distributed Resource Management across Process Boundaries},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3132020},
doi = {10.1145/3127479.3132020},
abstract = {Multi-tenant distributed systems composed of small services, such as Service-oriented Architectures (SOAs) and Micro-services, raise new challenges in attaining high performance and efficient resource utilization. In these systems, a request execution spans tens to thousands of processes, and the execution paths and resource demands on different services are generally not known when a request first enters the system. In this paper, we highlight the fundamental challenges of regulating load and scheduling in SOAs while meeting end-to-end performance objectives on metrics of concern to both tenants and operators. We design Wisp, a framework for building SOAs that transparently adapts rate limiters and request schedulers system-wide according to operator policies to satisfy end-to-end goals while responding to changing system conditions. In evaluations against production as well as synthetic workloads, Wisp successfully enforces a range of end-to-end performance objectives, such as reducing average latencies, meeting deadlines, providing fairness and isolation, and avoiding system overload.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {611–623},
numpages = {13},
keywords = {scheduling, resource management, rate limiting, service-oriented architectures, microservices},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/2739480.2754724,
author = {Ouni, Ali and Gaikovina Kula, Raula and Kessentini, Marouane and Inoue, Katsuro},
title = {Web Service Antipatterns Detection Using Genetic Programming},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754724},
doi = {10.1145/2739480.2754724},
abstract = {Service-Oriented Architecture (SOA) is an emerging paradigm that has radically changed the way software applications are architected, designed and implemented. SOA allows developers to structure their systems as a set of ready-made, reusable and compostable services. The leading technology used today for implementing SOA is Web Services. Indeed, like all software, Web services are prone to change constantly to add new user requirements or to adapt to environment changes. Poorly planned changes may risk introducing antipatterns into the system. Consequently, this may ultimately leads to a degradation of software quality, evident by poor quality of service (QoS). In this paper, we introduce an automated approach to detect Web service antipatterns using genetic programming. Our approach consists of using knowledge from real-world examples of Web service antipatterns to generate detection rules based on combinations of metrics and threshold values. We evaluate our approach on a benchmark of 310 Web services and a variety of five types of Web service antipatterns. The statistical analysis of the obtained results provides evidence that our approach is efficient to detect most of the existing antipatterns with a score of 85\% of precision and 87\% of recall.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1351–1358},
numpages = {8},
keywords = {search-based software engineering, antipatterns, web services},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/2996913.2997007,
author = {Chondrogiannis, Theodoros and Gamper, Johann and Cavaliere, Roberto and Ohnewein, Patrick},
title = {MoTrIS: A Framework for Route Planning on Multimodal Transportation Networks},
year = {2016},
isbn = {9781450345897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996913.2997007},
doi = {10.1145/2996913.2997007},
abstract = {In this paper, we present MoTrIS, a service-oriented framework which enables spatio-temporal query processing on multimodal networks that are composed of a road network and one or more schedule-based transportation networks. MoTrIS provides a remote access API, which allows for the development of applications that require the processing of routing queries on multimodal networks. We discuss the architecture of MoTrIS and we elaborate on each of its individual components. The data input module allows for the import of data from various sources into a spatial-enabled relational database. The network module builds a multimodal network by combining a road network with one or more transportation networks. The timetable module stores and queries the schedule for each transportation mode. The query processing module enables the execution of queries over the multimodal network. The visualization module exports the results into a visualizable format. Finally, we present a web application which allows users to create, modify and test advanced spatio-temporal services, and we demonstrate all the necessary steps for a user to build such a new service.},
booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {82},
numpages = {4},
keywords = {multimodal networks, route planning, query services},
location = {Burlingame, California},
series = {SIGSPACIAL '16}
}

@inproceedings{10.1145/3241403.3241430,
author = {de Amorim Silva, Rafael and Braga, Rosana T. V.},
title = {An Acknowledged System of Systems for Educational Internet of Everything Ecosystems},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241430},
doi = {10.1145/3241403.3241430},
abstract = {Internet of Everything (IoE) is the next evolutionary step of Internet of Things (IoT). This paradigm evolves IoT by transforming it into an accessed, monitored and controlled global pervasive network for all the Internet. The IoE network must integrate embedded devices, people, processes and data in order to provide relevant information for various application areas such as medical, industry, aerospace, agrobusiness, automation, education, among others. In this paper, we propose an acknowledged System of Systems (SoS) to integrate IoE ecosystems that operate into educational environments. This SoS implements a service-oriented architecture to provide services as interfaces between constituent systems (CS). A CS is mediated by a negotiator agent that can be requested by the SoS coordinator agent to identify the availability of specific resources demanded by other CS. This coordinator agent is responsible for recommending the CS with the best available resource for the requester CS and providing information about how access it. All the IoE components also may be mediated by intelligent agents thus increasing the robustness of an IoE ecosystem.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {25},
numpages = {7},
keywords = {multi-agent systems, internet of everything, cyber-physical ecosystems, system of systems, service-oriented architecture},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1145/3230833.3233278,
author = {Mathas, Christos M. and Segou, Olga E. and Xylouris, Georgios and Christinakis, Dimitris and Kourtis, Michail-Alexandros and Vassilakis, Costas and Kourtis, Anastasios},
title = {Evaluation of Apache Spot's Machine Learning Capabilities in an SDN/NFV Enabled Environment},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233278},
doi = {10.1145/3230833.3233278},
abstract = {Software Defined Networking (SDN) and Network Function Virtualisation (NFV) are transforming modern networks towards a service-oriented architecture. At the same time, the cybersecurity industry is rapidly adopting Machine Learning (ML) algorithms to improve detection and mitigation of complex attacks. Traditional intrusion detection systems perform signature-based detection, based on well-known malicious traffic patterns that signify potential attacks. The main drawback of this method is that attack patterns need to be known in advance and signatures must be preconfigured. Hence, typical systems fail to detect a zero-day attack or an attack with unknown signature. This work considers the use of machine learning for advanced anomaly detection, and specifically deploys the Apache Spot ML framework on an SDN/NFV-enabled testbed running cybersecurity services as Virtual Network Functions (VNFs). VNFs are used to capture traffic for ingestion by the ML algorithm and apply mitigation measures in case of a detected anomaly. Apache Spot utilises Latent Dirichlet Allocation to identify anomalous traffic patterns in Netflow, DNS and proxy data. The overall performance of Apache Spot is evaluated by deploying Denial of Service (Slowloris, BoNeSi) and a Data Exfiltration attack (iodine).},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {52},
numpages = {10},
keywords = {Apache Spot, Software Defined Networking, SHIELD Project, Machine Learning, Penetration Testing, Latent Dirichlet Allocation, Network Function Virtualisation},
location = {Hamburg, Germany},
series = {ARES '18}
}

@inproceedings{10.1145/3330204.3330259,
author = {Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel},
title = {Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330259},
doi = {10.1145/3330204.3330259},
abstract = {In the last decade the (big) data-driven science paradigm became a wide-spread reality. However, this approach has some limitations such as a performance dependency on the quality of the data and the lack of reproducibility of the results. In order to enable this reproducibility, many tools such as Workflow Management Systems were developed to formalize process pipelines and capture execution traces. However, interoperating data generated by these solutions became a problem, since most systems adopted proprietary data models. To support interoperability across heterogeneous provenance data, we propose a Service Oriented Architecture with a polystore storage design in which provenance is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover, we propose a query layer that provides location and access transparency to users. Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios. Firstly, we illustrate how two research groups can compare their processes and results. Secondly, we show how our architecture can be used as a queriable provenance repository. We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology. Finally, we show our solution usability and extensibility appeal by comparing it to similar approaches.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {49},
numpages = {8},
keywords = {Workflows interoperability, polystore, heterogeneous provenance data integration},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/2691195.2691236,
author = {Dash, Shefali S. and Sethi, I. P. S. and Maurya, Ashutosh P.},
title = {Government Initiative for Automation of Co-Operative Banks Structure through Core Banking Solution},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691236},
doi = {10.1145/2691195.2691236},
abstract = {National Informatics Centre (NIC) has developed a core banking software with name Co-operative Core Banking Solution (CCBS), considering requirement of Indian co-operative banking sector. Along with software development, NIC also provide implementation and hosting service of this software for co-operative sector under Software as a Service Model. Due to latest Service Oriented Architecture of software, NIC-CCBS is able to integrate other central / state government software/applications and can generate meaningful MIS for decision / policy making for management level. This can also provide a good platform for other e-governance projects by establishing network connectivity, computer awareness and capacity building.NIC-CCBS is implemented at more than 100 co-operative bank locations across two states of India i.e. Chhattisgarh \&amp; Meghalaya and serving more than 20000 banking transactions on daily basis. NIC -- CCBS implementation has been implemented in step wise process which includes Business understanding, Process engineering, Data digitization, Data quality, Data Migration, hands hold support. On completion of CCBS application for State Co-operative Bank, District Central Co-operative Bank, Primary Agriculture Co-operative societies, NIC is moving ahead to serve to Agricultural Land Development Banks, Treasury Banks and State financial corporations.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {478–479},
numpages = {2},
keywords = {cooperative, change process, banking solution, software, implementation},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@inproceedings{10.5555/2819009.2819091,
author = {Lyons, Kelly and Oh, Christie},
title = {SOA4DM: Applying an SOA Paradigm to Coordination in Humanitarian Disaster Response},
year = {2015},
publisher = {IEEE Press},
abstract = {Despite efforts to achieve a sustainable state of control over the management of global crises, disasters are occurring with greater frequency, intensity, and affecting many more people than ever before while the resources to deal with them do not grow apace. As we enter 2015, with continued concerns that mega-crises may become the new normal, we need to develop novel methods to improve the efficiency and effectiveness of our management of disasters. Software engineering as a discipline has long had an impact on society beyond its role in the development of software systems. In fact, software engineers have been described as the developers of prototypes for future knowledge workers; tools such as Github and Stack Overflow have demonstrated applications beyond the domain of software engineering. In this paper, we take the potential influence of software engineering one-step further and propose using the software service engineering paradigm as a new approach to managing disasters. Specifically, we show how the underlying principles of service-oriented architectures (SOA) can be applied to the coordination of disaster response operations. We describe key challenges in coordinating disaster response and discuss how an SOA approach can address those challenges.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {519–522},
numpages = {4},
keywords = {SOA, disaster response},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.14778/2733004.2733009,
author = {Poess, Meikel and Rabl, Tilmann and Jacobsen, Hans-Arno and Caufield, Brian},
title = {TPC-DI: The First Industry Benchmark for Data Integration},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733009},
doi = {10.14778/2733004.2733009},
abstract = {Historically, the process of synchronizing a decision support system with data from operational systems has been referred to as Extract, Transform, Load (ETL) and the tools supporting such process have been referred to as ETL tools. Recently, ETL was replaced by the more comprehensive acronym, data integration (DI). DI describes the process of extracting and combining data from a variety of data source formats, transforming that data into a unified data model representation and loading it into a data store. This is done in the context of a variety of scenarios, such as data acquisition for business intelligence, analytics and data warehousing, but also synchronization of data between operational applications, data migrations and conversions, master data management, enterprise data sharing and delivery of data services in a service-oriented architecture context, amongst others. With these scenarios relying on up-to-date information it is critical to implement a highly performing, scalable and easy to maintain data integration system. This is especially important as the complexity, variety and volume of data is constantly increasing and performance of data integration systems is becoming very critical. Despite the significance of having a highly performing DI system, there has been no industry standard for measuring and comparing their performance. The TPC, acknowledging this void, has released TPC-DI, an innovative benchmark for data integration. This paper motivates the reasons behind its development, describes its main characteristics including workload, run rules, metric, and explains key decisions.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1367–1378},
numpages = {12}
}

@inproceedings{10.1145/2642668.2642679,
author = {Hassam, Mickael and Kara, Nadjia and Belqasmi, Fatna and Glitho, Roch},
title = {Virtualized Infrastructure for Video Game Applications in Cloud Environments},
year = {2014},
isbn = {9781450330268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642668.2642679},
doi = {10.1145/2642668.2642679},
abstract = {Mobile video games are fast-growing and fast-evolving. Cloud computing's paradigm can bring several benefits to mobile video games, like cost reduction through an efficient usage of resources, or an easier and faster on-demand deployment of new applications. This paper focuses on the architecture aspects of mobile cloud-based video gaming and proposes a new service oriented and virtualized paradigm. The idea is to provide reusable game engines sub modules like the rendering or physics engines as cloud computing services. We call these sub modules substrates. Offered by different substrates providers as services, they can be dynamically discovered, used and composed. There are several motivations for substrates virtualization, including the rapid introduction of new video game applications and cost efficiency through resource sharing. This paper also describes the implementation of a prototype and the measurements performed to validate some aspects of our paradigm. As a preliminary validation of this solution, we analyze the effects of different parameters like virtualization or inner latency on the QoS. The performance analysis shows that the overhead introduced by substrate virtualization is acceptable, and reveals how the low-latency connectivity between substrates that compose a video game application and the limitation of the amount of these substrates are crucial to achieve a satisfactory level of QoS.},
booktitle = {Proceedings of the 12th ACM International Symposium on Mobility Management and Wireless Access},
pages = {109–114},
numpages = {6},
keywords = {infrastructure and platform as services, mobile video game applications, substrate, virtualization, cloud computing},
location = {Montreal, QC, Canada},
series = {MobiWac '14}
}

@article{10.1145/3386041,
author = {Shi, Min and Tang, Yufei and Zhu, Xingquan and Liu, Jianxun},
title = {Topic-Aware Web Service Representation Learning},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3386041},
doi = {10.1145/3386041},
abstract = {The advent of Service-Oriented Architecture (SOA) has brought a fundamental shift in the way in which distributed applications are implemented. An overwhelming number of Web-based services (e.g., APIs and Mashups) have leveraged this shift and furthered development. Applications designed with SOA principles are typically characterized by frequent dependencies with one another in the form of heterogeneous networks, i.e., annotation relations between tags and services, and composition relations between Mashups and APIs. Although prior work has shown the utility gained by exploring these networks, their analysis is still in its infancy. This article develops an approach to learning representations of the Web service network, which seeks to embed Web services in low-dimensional continuous vectors with preserved information of the network structure, functional tags, and service descriptions, such that services with similar functional properties and network structures are mapped together in the learned latent space. We first propose a topic generative model for constructing two topic distribution networks (Mashup-Topic and API-Topic) from the service content. Then, we present an efficient optimization process to derive low-dimensional vector representations of Web services from a tri-layer bipartite network with the Mashup-Topic and API-Topic networks on two ends and the Mashup-API composition network in the middle. Experiments on real-word datasets have verified that our approach is effective to learn robust low-rank service representations, i.e., 25\% F1-measure gain over the state-of-the-art in Web service recommendation task.},
journal = {ACM Trans. Web},
month = {apr},
articleno = {9},
numpages = {23},
keywords = {Web services, Mashups, probabilistic topic model, network embedding, service representation}
}

@inproceedings{10.1145/3183519.3183537,
author = {Au\'{e}, Joop and Aniche, Maur\'{\i}cio and Lobbezoo, Maikel and van Deursen, Arie},
title = {An Exploratory Study on Faults in Web API Integration in a Large-Scale Payment Company},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183537},
doi = {10.1145/3183519.3183537},
abstract = {Service-oriented architectures are more popular than ever, and increasingly companies and organizations depend on services offered through Web APIs. The capabilities and complexity of Web APIs differ from service to service, and therefore the impact of API errors varies. API problem cases related to Adyen's payment service were found to have direct considerable impact on API consumer applications. With more than 60,000 daily API errors, the potential impact is enormous. In an effort to reduce the impact of API related problems, we analyze 2.43 million API error responses to identify the underlying faults. We quantify the occurrence of faults in terms of the frequency and impacted API consumers. We also challenge our quantitative results by means of a survey with 40 API consumers. Our results show that 1) faults in API integration can be grouped into 11 general causes: invalid user input, missing user input, expired request data, invalid request data, missing request data, insufficient permissions, double processing, configuration, missing server data, internal and third party, 2) most faults can be attributed to the invalid or missing request data, and most API consumers seem to be impacted by faults caused by invalid request data and third party integration; and 3) insufficient guidance on certain aspects of the integration and on how to recover from errors is an important challenge to developers.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {13–22},
numpages = {10},
keywords = {web engineering, web API integration, webservices},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@inproceedings{10.1145/3385032.3385042,
author = {Tummalapalli, Sahithi and Kumar, Lov and Murthy, N. L. Bhanu},
title = {Prediction of Web Service Anti-Patterns Using Aggregate Software Metrics and Machine Learning Techniques},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385042},
doi = {10.1145/3385032.3385042},
abstract = {Service-Oriented Architecture(SOA) can be characterized as an approximately coupled engineering intended to meet the business needs of an association/organization. Service-Based Systems (SBSs) are inclined to continually change to enjoy new client necessities and adjust the execution settings, similar to some other huge and complex frameworks. These changes may lead to the evolution of designs/products with poor Quality of Service (QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns. Anti-patterns makes the evolution and maintenance of the software systems hard and complex. Early identification of modules, classes, or source code regions where anti-patterns are more likely to occur can help in amending and maneuvering testing efforts leading to the improvement of software quality. In this work, we investigate the application of three sampling techniques, three feature selection techniques, and sixteen different classification techniques to develop the models for web service anti-pattern detection. We report the results of an empirical study by evaluating the approach proposed, on a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of five types of web-service anti-patterns. Experimental results demonstrated that SMOTE is the best performing data sampling techniques. The experimental results also reveal that the model developed by considering Uncorrelated Significant Predictors(SUCP) as the input obtained better performance compared to the model developed by other metrics. Experimental results also show that the Least Square Support Vector Machine with Linear(LSLIN) function has outperformed all other classifier techniques.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
articleno = {8},
numpages = {11},
keywords = {Feature Selection, Classifiers, Service-Based Systems(SBS), Aggregation measures, Web-Services, Class imbalance distribution, Machine Learning, WSDL, Source Code Metrics, Anti-pattern},
location = {Jabalpur, India},
series = {ISEC 2020}
}

@inproceedings{10.1145/3030207.3030215,
author = {Bause, Falko and Buchholz, Peter and May, Johannes},
title = {A Tool Supporting the Analytical Evaluation of Service Level Agreements},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030215},
doi = {10.1145/3030207.3030215},
abstract = {Quantitative aspects of modern IT systems are often specified by service level agreements (SLAs) which relate the maximal load of a system with guaranteed bounds for response times and delays. These quantities are specified for single services which are combined in a service oriented architecture (SOA) to composed services offered to potential users or other service providers. To derive SLAs for composed services and to plan the required capacity to guarantee SLAs, appropriate methods and tools have to be used that compute results based on information given in SLAs. In this paper it is argued that most available approaches are not sufficient to analyze systems based on SLA information. A new method and a tool are presented that support the efficient calculation of bounds for delays in composed systems based on bounds for the load and the delay of the individual components which are specified in the SLAs of the components. Furthermore, the presented tool can be used to generate bounds for the required processing capacity which a provider has to provide in order to guarantee the quality of service defined in the SLAs.The presented approach is in some sense a counterpart to mean value analysis for queueing networks but rather than mean values, worst case bounds for different quantities like response times or departure processes are computed. Analysis is based on min/+ algebra but the mathematical approach is hidden from the user by a graphical interface allowing a simple graphical specification and result representation for networks of composed services.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {233–244},
numpages = {12},
keywords = {analytical evaluation, performance modeling tools, service level agreements},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/2792838.2799499,
author = {N\'{e}meth, Botty\'{a}n},
title = {Scaling Up Recommendation Services in Many Dimensions},
year = {2015},
isbn = {9781450336925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2792838.2799499},
doi = {10.1145/2792838.2799499},
abstract = {Gravity R&amp;D has been providing recommendation engines as SaaS solutions since 2009. The company has a strong research focus and recommendation quality has always been their primary differentiating factor. Widely used or open source recommendation algorithms are of little use to our technology team as a result of the superiority of our in-house developed, proprietary algorithms. Gravity R&amp;D experienced many challenges while scaling up their services. The sheer quantity of data handled on a daily basis increased exponentially. This presentation will cover how overcoming these challenges permanently shaped our algorithms and system architecture used to generate these recommendations. Serving personalized recommendations requires real-time computation and data access for every single request. To generate responses in real-time, current user inputs have to be compared against their history in order to deliver accurate recommendations. We then combine this user information with specific details about available items as the next step in the recommendation process. It becomes more difficult to provide accurate recommendations as the number of transactions and items increase. It also becomes difficult because this type of analysis requires the combination of multiple heterogeneous algorithms that all require different inputs. Initially, the architecture was designed for MF based models and serving huge numbers of requests but with a limited number of items. Now, Gravity is using MF, neighborhood based models and metadata based models to generate recommendations for millions of items within their databases. This required a shift from a monolithic architecture with in-process caching to a more service oriented architecture with multi-layer caching. As a result of an increase in the number of components and number of clients, managing the infrastructure can be quite difficult. Even with these challenges, we don't believe that it is worthwhile to use a fully distributed system. It adds unneeded complexity, resources, and overhead to the system. We prefer an approach of firstly optimizing current algorithms and architecture and only moving to a distributed system when no other options are left.},
booktitle = {Proceedings of the 9th ACM Conference on Recommender Systems},
pages = {233},
numpages = {1},
keywords = {scalability, performance, recommendation engine},
location = {Vienna, Austria},
series = {RecSys '15}
}

@inproceedings{10.1145/3457913.3457939,
author = {Wei, Yuyang and Yu, Yijun and Pan, Minxue and Zhang, Tian},
title = {A Feature Table Approach to Decomposing Monolithic Applications into Microservices},
year = {2021},
isbn = {9781450388191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457913.3457939},
doi = {10.1145/3457913.3457939},
abstract = {Microservice architecture refers to the use of numerous small-scale and independently deployed services, instead of encapsulating all functions into one monolith. It has been a challenge in software engineering to decompose a monolithic system into smaller parts. In this paper, we propose the Feature Table approach, a structured approach to service decomposition based on the correlation between functional features and microservices: (1) we defined the concept of Feature Cards and 12 instances of such cards; (2) we formulated Decomposition Rules to decompose monolithic applications; (3) we designed the Feature Table Analysis Tool to provide semi-automatic analysis for identification of microservices; and (4) we formulated Mapping Rules to help developers implement microservice candidates. We performed a case study on Cargo Tracking System to validate our microservice-oriented decomposition approach. Cargo Tracking System is a typical case that has been decomposed by other related methods (dataflow-driven approach, Service Cutter, and API Analysis). Through comparison with the related methods in terms of specific coupling and cohesion metrics, the results show that the proposed Feature Table approach can deliver more reasonable microservice candidates, which are feasible in implementation with semi-automatic support.},
booktitle = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
pages = {21–30},
numpages = {10},
keywords = {monolith decomposition, Microservices, microservice architecture},
location = {Singapore, Singapore},
series = {Internetware '20}
}

@inproceedings{10.1145/3425269.3425273,
author = {de Freitas Apolin\'{a}rio, Daniel Rodrigo and de Fran\c{c}a, Breno Bernard Nicolau},
title = {Towards a Method for Monitoring the Coupling Evolution of Microservice-Based Architectures},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425273},
doi = {10.1145/3425269.3425273},
abstract = {The microservice architecture is claimed to satisfy ongoing software development demands, such as resilience, flexibility, and velocity. However, developing applications based on microservices also brings some drawbacks, such as the increased software operational complexity. Recent studies have also pointed out the lack of methods to prevent problems related to the maintainability of these solutions. Disregarding established design principles during the software evolution may lead to the so-called architectural erosion, which can end up in a condition of unfeasible maintenance. As microservices can be considered a new architecture style, there are few initiatives to monitoring the evolution of software microservice-based architectures. In this paper, we introduce the SYMBIOTE method for monitoring the coupling evolution of microservice-based systems. More specifically, this method collects coupling metrics during runtime (staging or production environments) and monitors them throughout software evolution. The longitudinal analysis of the collected measures allows detecting an upward trend in coupling metrics that could be signs of architectural erosion. To develop the proposed method, we performed an experimental analysis of the coupling metrics behavior using artificially-generated data.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {71–80},
numpages = {10},
keywords = {software evolution, microservices, maintainability, coupling metrics},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3412841.3442016,
author = {Brito, Miguel and Cunha, J\'{a}come and Saraiva, Jo\~{a}o},
title = {Identification of Microservices from Monolithic Applications through Topic Modelling},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442016},
doi = {10.1145/3412841.3442016},
abstract = {Microservices emerged as one of the most popular architectural patterns in the recent years given the increased need to scale, grow and flexibilize software projects accompanied by the growth in cloud computing and DevOps. Many software applications are being submitted to a process of migration from its monolithic architecture to a more modular, scalable and flexible architecture of microservices. This process is slow and, depending on the project's complexity, it may take months or even years to complete.This paper proposes a new approach on microservice identification by resorting to topic modelling in order to identify services according to domain terms. This approach in combination with clustering techniques produces a set of services based on the original software. The proposed methodology is implemented as an open-source tool for exploration of monolithic architectures and identification of microservices. A quantitative analysis using the state of the art metrics on independence of functionality and modularity of services was conducted on 200 open-source projects collected from GitHub. Cohesion at message and domain level metrics' showed medians of roughly 0.6. Interfaces per service exhibited a median of 1.5 with a compact interquartile range. Structural and conceptual modularity revealed medians of 0.2 and 0.4 respectively.Our first results are positive demonstrating beneficial identification of services due to overall metrics' results.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1409–1418},
numpages = {10},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3147234.3148111,
author = {L\'{o}pez, Manuel Ram\'{\i}rez and Spillner, Josef},
title = {Towards Quantifiable Boundaries for Elastic Horizontal Scaling of Microservices},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3148111},
doi = {10.1145/3147234.3148111},
abstract = {One of the most useful features of a microservices architecture is its versatility to scale horizontally. However, not all services scale in or out uniformly. The performance of an application composed of microservices depends largely on a suitable combination of replica count and resource capacity. In practice, this implies limitations to the efficiency of autoscalers which often overscale based on an isolated consideration of single service metrics. Consequently, application providers pay more than necessary despite zero gain in overall performance. Solving this issue requires an application-specific determination of scaling limits due to the general infeasibility of an application-agnostic solution. In this paper, we study microservices scalability, the auto-scaling of containers as microservice implementations and the relation between the number of replicas and the resulting application task performance. We contribute a replica count determination solution with a mathematical approach. Furthermore, we offer a calibration software tool which places scalability boundaries into declarative composition descriptions of applications ready to be consumed by cloud platforms.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {35–40},
numpages = {6},
keywords = {scalability, microservices, replication, optimization},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@inproceedings{10.1145/3011141.3011179,
author = {de Camargo, Andr\'{e} and Salvadori, Ivan and Mello, Ronaldo dos Santos and Siqueira, Frank},
title = {An Architecture to Automate Performance Tests on Microservices},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011179},
doi = {10.1145/3011141.3011179},
abstract = {The microservices architecture provides a new approach to develop applications. As opposed to monolithic applications, in which the application comprises a single software artifact, an application based on the microservices architecture is composed by a set of services, each one designed to perform a single and well-defined task. These services allow the development team to decouple several parts of the application using different frameworks, languages and hardware for each part of the system. One of the drawbacks for adopting the microservices architecture to develop applications is testability. In a single application test boundaries can be more easily established and tend to be more stable as the application evolves, while with microservices we can have a set of hundreds of services that operate together and are prone to change more rapidly. Each one of these services needs to be tested and updated as the service changes. In addition, the different characteristics of these services such as languages, frameworks or the used infrastructure have to be considered in the testing phase. Performance tests are applied to assure that a particular software complies with a set of non-functional requirements such as throughput and response time. These metrics are important to ensure that business constraints are respected and to help finding performance bottlenecks. In this paper, we present a new approach to allow the performance tests to be executed in an automated way, with each microservice providing a test specification that is used to perform tests. Along with the architecture, we also provide a framework that implements some key concepts of this architecture. This framework is available as an open source project1.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {422–429},
numpages = {8},
keywords = {test automation, performance test, microservices},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@inproceedings{10.1145/3308558.3313653,
author = {Shan, Huasong and Chen, Yuan and Liu, Haifeng and Zhang, Yunpeng and Xiao, Xiao and He, Xiaofeng and Li, Min and Ding, Wei},
title = {??-Diagnosis: Unsupervised and Real-Time Diagnosis of Small- Window Long-Tail Latency in Large-Scale Microservice Platforms},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313653},
doi = {10.1145/3308558.3313653},
abstract = {Microservice architectures and container technologies are broadly adopted by giant internet companies to support their web services, which typically have a strict service-level objective (SLO), tail latency, rather than average latency. However, diagnosing SLO violations, e.g., long tail latency problem, is non-trivial for large-scale web applications in shared microservice platforms due to million-level operational data and complex operational environments. We identify a new type of tail latency problem for web services, small-window long-tail latency (SWLT), which is typically aggregated during a small statistical window (e.g., 1-minute or 1-second). We observe SWLT usually occurs in a small number of containers in microservice clusters and sharply shifts among different containers at different time points. To diagnose root-causes of SWLT, we propose an unsupervised and low-cost diagnosis algorithm-?-Diagnosis, using two-sample test algorithm and ?-statistics for measuring similarity of time series to identify root-cause metrics from millions of metrics. We implement and deploy a real-time diagnosis system in our real-production microservice platforms. The evaluation using real web application datasets demonstrates that ?-Diagnosis can identify all the actual root-causes at runtime and significantly reduce the candidate problem space, outperforming other time-series distance based root-cause analysis algorithms.},
booktitle = {The World Wide Web Conference},
pages = {3215–3222},
numpages = {8},
keywords = {Root-cause analysis, tail latency, time series similarity},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{10.14778/3484224.3484232,
author = {Laigner, Rodrigo and Zhou, Yongluan and Salles, Marcos Antonio Vaz and Liu, Yijian and Kalinowski, Marcos},
title = {Data Management in Microservices: State of the Practice, Challenges, and Research Directions},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484232},
doi = {10.14778/3484224.3484232},
abstract = {Microservices have become a popular architectural style for data-driven applications, given their ability to functionally decompose an application into small and autonomous services to achieve scalability, strong isolation, and specialization of database systems to the workloads and data formats of each service. Despite the accelerating industrial adoption of this architectural style, an investigation of the state of the practice and challenges practitioners face regarding data management in microservices is lacking. To bridge this gap, we conducted a systematic literature review of representative articles reporting the adoption of microservices, we analyzed a set of popular open-source microservice applications, and we conducted an online survey to cross-validate the findings of the previous steps with the perceptions and experiences of over 120 experienced practitioners and researchers.Through this process, we were able to categorize the state of practice of data management in microservices and observe several foundational challenges that cannot be solved by software engineering practices alone, but rather require system-level support to alleviate the burden imposed on practitioners. We discuss the shortcomings of state-of-the-art database systems regarding microservices and we conclude by devising a set of features for microservice-oriented database systems.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3348–3361},
numpages = {14}
}

@inproceedings{10.1145/3126858.3126873,
author = {Brilhante, Jonathan and Costa, Rostand and Maritan, Tiago},
title = {Asynchronous Queue Based Approach for Building Reactive Microservices},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126873},
doi = {10.1145/3126858.3126873},
abstract = {To achieve scalability and flexibility in larger applications a new approach arises, named by Microservices (MS). However MS architectures are at their inception and are even more a concept than a fully mature design pattern. One of the hardest topics in this approach is how to properly migrate or develop a single microservice, in terms of scope, efficiency and dependability. In this sense, this work proposes a new architectural model based on high-level architecture pattern of reactive programming to the internal structure of a new microservice. The new model of microservices are internally coordinated by asynchronous queues, which allowed to preserve compatibility with most monolithic components and provide an encapsulation process to enable its continuity. A comparative study between the standard approach and the proposed architecture was carried out to measure the eventual performance improvement of the new strategy.},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {373–380},
numpages = {8},
keywords = {reactive approach, micro services, refactoring, asynchronous queues},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.1109/ECASE.2019.00013,
author = {Yuan, Eric},
title = {Architecture Interoperability and Repeatability with Microservices: An Industry Perspective},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ECASE.2019.00013},
doi = {10.1109/ECASE.2019.00013},
abstract = {Microservices, along with supporting technologies such as containers, have become a prevalent architecture approach for today's software systems, especially in enterprise environments. They represent the latest evolutionary step in the decades-old journey towards service- and component-based software architectures. Along with virtualization technologies, microservices have enabled the loose-coupling of both service interfaces (message passing) and service integration (form and fit). This paper attempts to explore the impact of microservices on software architecture interoperability and repeatability, based on our experiences in developing two microservice-based systems. Our central thesis is that, if we view software architecture as a set of principal design decisions, the microservices approach enable us to more elegantly separate these decisions from non-architectural, domain-specific ones, and thus make these decisions more interoperable, reusable, and repeatable across disparate problem domains. We therefore propose that a microservices based reference architecture (RA) and reference implementation (RI) be created for the community-wide infrastructure for software engineering and software architecture research, along with a set of detailed considerations.},
booktitle = {Proceedings of the 2nd International Workshop on Establishing a Community-Wide Infrastructure for Architecture-Based Software Engineering},
pages = {26–33},
numpages = {8},
keywords = {software architecture, DevOps, microservice, cloud computing},
location = {Montreal, Quebec, Canada},
series = {ECASE '19}
}

@inproceedings{10.1145/3297280.3297400,
author = {Cardarelli, Mario and Iovino, Ludovico and Di Francesco, Paolo and Di Salle, Amleto and Malavolta, Ivano and Lago, Patricia},
title = {An Extensible Data-Driven Approach for Evaluating the Quality of Microservice Architectures},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297400},
doi = {10.1145/3297280.3297400},
abstract = {Microservice architecture (MSA) is defined as an architectural style where the software system is developed as a suite of small services, each running in its own process and communicating with lightweight mechanisms. The benefits of MSA are many, ranging from an increase in development productivity, to better business-IT alignment, agility, scalability, and technology flexibility. The high degree of microservices distribution and decoupling is, however, imposing a number of relevant challenges from an architectural perspective. In this context, measuring, controlling, and keeping a satisfactory level of quality of the system architecture is of paramount importance.In this paper we propose an approach for the specification, aggregation, and evaluation of software quality attributes for the architecture of microservice-based systems. The proposed approach allows developers to (i) produce architecture models of the system, either manually or automatically via recovering techniques, (ii) contribute to an ecosystem of well-specified and automatically-computable software quality attributes for MSAs, and (iii) continuously measure and evaluate the architecture of their systems by (re-)using the software quality attributes defined in the ecosystem. The approach is implemented by using Model-Driven Engineering techniques.The current implementation of the approach has been validated by assessing the maintainability of a third-party, publicly available benchmark system.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1225–1234},
numpages = {10},
keywords = {microservices, architecture recovery, model-driven, software quality},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3460319.3464805,
author = {Pan, Yicheng and Ma, Meng and Jiang, Xinrui and Wang, Ping},
title = {Faster, Deeper, Easier: Crowdsourcing Diagnosis of Microservice Kernel Failure from User Space},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464805},
doi = {10.1145/3460319.3464805},
abstract = {With the widespread use of cloud-native architecture, increasing web applications (apps) choose to build on microservices. Simultaneously, troubleshooting becomes full of challenges owing to the high dynamics and complexity of anomaly propagation. Existing diagnostic methods rely heavily on monitoring metrics collected from the kernel side of microservice systems. Without a comprehensive monitoring infrastructure, application owners and even cloud operators cannot resort to these kernel-space solutions. This paper summarizes several insights on operating a top commercial cloud platform. Then, for the first time, we put forward the idea of user-space diagnosis for microservice kernel failures. To this end, we develop a crowdsourcing solution - DyCause, to resolve the asymmetric diagnostic information problem. DyCause deploys on the application side in a distributed manner. Through lightweight API log sharing, apps collect the operational status of kernel services collaboratively and initiate diagnosis on demand. Deploying DyCause is fast and lightweight as we do not have any architectural and functional requirements for the kernel. To reveal more accurate correlations from asymmetric diagnostic information, we design a novel statistical algorithm that can efficiently discover the time-varying causalities between services. This algorithm also helps us build the temporal order of the anomaly propagation. Therefore, by using DyCause, we can obtain more in-depth and interpretable diagnostic clues with limited indicators. We apply and evaluate DyCause on both a simulated test-bed and a real-world cloud system. Experimental results verify that DyCause running in the user-space outperforms several state-of-the-art algorithms running in the kernel on accuracy. Besides, DyCause shows superior advantages in terms of algorithmic efficiency and data sensitivity. Simply put, DyCause produces a significantly better result than other baselines when analyzing much fewer or sparser metrics. To conclude, DyCause is faster to act, deeper in analysis, and easier to deploy.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {646–657},
numpages = {12},
keywords = {granger causal intervals, dynamic service dependency, microservice system, root cause analysis},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3465480.3467838,
author = {Das, Prangshuman and Laigner, Rodrigo and Zhou, Yongluan},
title = {HawkEDA: A Tool for Quantifying Data Integrity Violations in Event-Driven Microservices},
year = {2021},
isbn = {9781450385558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465480.3467838},
doi = {10.1145/3465480.3467838},
abstract = {A microservice architecture advocates for subdividing an application into small and independent components, each communicating via well-defined APIs or asynchronous events, to allow for higher scalability, availability, and fault isolation. However, the implementation of substantial amount of data management logic at the application-tier and the existence of functional dependencies cutting across microservices create a great barrier for developers to reason about application safety and performance trade-offs.To fill this gap, this work presents HawkEDA, the first data management tool that allows practitioners to experiment their microservice applications with different real-world workloads to quantify the amount of data integrity anomalies. In our demonstration, we present a case study of a popular open-source event-driven microservice to showcase the interface through which developers specify application semantics and the flexibility of HawkEDA.},
booktitle = {Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
pages = {176–179},
numpages = {4},
keywords = {microservice, event-driven architecture, data integrity},
location = {Virtual Event, Italy},
series = {DEBS '21}
}

@article{10.1145/3418899,
author = {Brondolin, Rolando and Santambrogio, Marco D.},
title = {A Black-Box Monitoring Approach to Measure Microservices Runtime Performance},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3418899},
doi = {10.1145/3418899},
abstract = {Microservices changed cloud computing by moving the applications’ complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers’ power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {34},
numpages = {26},
keywords = {cloud computing, power attribution, Microservices, docker, kubernetes, performance monitoring, network performance monitoring}
}

@inproceedings{10.1145/3439231.3440604,
author = {Mercier, Julien and Whissell-Turner, Kathleen and Paradis, Ariane and Avaca, Ivan},
title = {Good Vibrations: Tuning a Systems Dynamics Model of Affect and Cognition in Learning to the Appropriate Frequency Bands of Fine-Grained Temporal Sequences of Data: Frequency Bands of Affect and Cognition},
year = {2021},
isbn = {9781450389372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439231.3440604},
doi = {10.1145/3439231.3440604},
abstract = {Process-oriented studies of cooperative learning from an educational neuroscience perspective has not been firmly quantified experimentally. Within a modeling approach aimed at the development of a systems dynamics model of affect and cognition, the goal of this exploratory study is to identify typical timescales of variation for continuous metrics of affect (Frontal Alpha Asymmetry (FAA): valence) and cognition (Cognitive Load (CL); Index of Cognitive Engagement (ICE); Frontal Midline Theta (FMT): attention). These metrics were obtained from 72 participants paired in dyads (player and watcher) from whom electroencephalography (EEG) was recorded for 2 hours while one participant was playing a serious game to learn Physics, and the other one was watching passively. The results show rather slow cyclical variation for every metric tested, accompanied in certain cases by short bursts of faster variations. This result converges with [Newell 1990] cognitive architecture assuming that psychophysiological measures capture activity at higher levels such as operation tasks and operations. Theoretical, methodological and applied implications are discussed. Also, the need for further fine-grained analyses of the context and other atypical analyses are expressed.},
booktitle = {Proceedings of the 9th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {194–202},
numpages = {9},
keywords = {online measures of learning, game-based learning, educational neuroscience},
location = {Online, Portugal},
series = {DSAI '20}
}

@inproceedings{10.1145/3344948.3344991,
author = {Santos, Nuno and Salgado, Carlos E. and Morais, Francisco and Melo, M\'{o}nica and Silva, Sara and Martins, Raquel and Pereira, Marco and Rodrigues, Helena and Machado, Ricardo J. and Ferreira, Nuno and Pereira, Manuel},
title = {A Logical Architecture Design Method for Microservices Architectures},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344991},
doi = {10.1145/3344948.3344991},
abstract = {The use of microservices architectures has been widely adopted in software development, especially for cloud-based solutions. Developing such solutions faces several challenges beyond typical architecture and service design concerns, including service exposition (API), inter-service communication, and infrastructure deployment, among others. Although model-driven approaches allow abstracting microservices behavior from the business domain, there is a lack of proper methods for addressing the referred challenges. In this paper, the elicitation of microservices, their identification uses using functionally decomposed UML use cases as input within a logical architecture derivation method, namely an adapted version of the Four Step Rule Set (4SRS), using SoaML diagrams, that responds to microservices specific characteristics. We demonstrate the approach using a scenario within a live industrial project.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {145–151},
numpages = {7},
keywords = {logical architectures, service participants, decomposition, SoaML, UML, microservices},
location = {Paris, France},
series = {ECSA '19}
}

@inproceedings{10.1145/3415088.3415097,
author = {Mlotshwa, Likhwa Lothar and Makura, Sheunesu M. and Karie, Nickson M. and Kebande, Victor R.},
title = {Opportunistic Security Architecture for Osmotic Computing Paradigm in Dynamic IoT-Edge's Resource Diffusion},
year = {2020},
isbn = {9781450375580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415088.3415097},
doi = {10.1145/3415088.3415097},
abstract = {Increased heterogeneity of physical resources has had positive and negative effects in Internet of Things (IoT) through the existence of edge computing. As a result, there has been a need for effective dynamic management of IoT, cloud and edge resources, in order to address the existence of low-level constraints during resource migration. Nevertheless, the explosion of IoT devices and data has allowed orchestration of microservices to adopt an opportunistic approach to how applications and services are deployed in the edge in IoT platform. A notable approach has been osmotic computing that allows resources from a federated cloud to be able to diffuse from an ecosystem of higher solute (network properties and entities) concentration to solvent (applications, layered interfaces and services). We posit that, while computing resources and applications are able to move from the federated environment, to the cloud deployable models, to the edge, then to IoT ecosystem, there is a higher chance of susceptibility of threats and attacks that may be directed to the emerging edge applications/data due to dynamic emergent configurations. This paper proposes a 5-layer opportunistic architecture that adds security metrics across different levels of osmotic computing paradigm. The proposed 5-layer security architecture addresses the need for autonomously securing resources-edge computation, edge storage and emerging edge configurations as the computing resources move to a higher solute in heterogenous edge and cloud datacenters across IoT devices. This has been achieved by proposing security metrics that address the prevailing challenge with a degree of certainty.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent and Innovative Computing Applications},
articleno = {9},
numpages = {7},
keywords = {opportunistic, osmotic, IoT, edge, security architecture},
location = {Plaine Magnien, Mauritius},
series = {ICONIC '20}
}

@inproceedings{10.5555/3172795.3172823,
author = {Khazaei, Hamzeh and Ravichandiran, Rajsimman and Park, Byungchul and Bannazadeh, Hadi and Tizghadam, Ali and Leon-Garcia, Alberto},
title = {Elascale: Autoscaling and Monitoring as a Service},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Auto-scalability has become an evident feature for cloud software systems including but not limited to big data and IoT applications. Cloud application providers now are in full control over their applications' microservices and macroservices; virtual machines and containers can be provisioned or deprovisioned on demand at run-time. Elascale strives to adjust both micro/macro resources with respect to workload and changes in the internal state of the whole application stack. Elascale leverages Elasticsearch stack for collection, analysis and storage of performance metrics. Elascale then uses its default scaling engine to elastically adapt the managed application. Extendibility is guaranteed through provider, schema, plug-in and policy elements in the Elascale by which flexible scalability algorithms, including both reactive and proactive techniques, can be designed and implemented for various technologies, infrastructures and software stacks. In this paper, we present the architecture and initial implementation of Elascale; an instance will be leveraged to add auto-scalability to a generic IoT application. Due to zero dependency to the target software system, Elascale can be leveraged to provide auto-scalability and monitoring as-a-service for any type of cloud software system.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {234–240},
numpages = {7},
keywords = {microservices, cloud application, scalability as a service, docker, macroservices, monitoring, elasticsearch, containers, auto-scalability},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/2747470.2747474,
author = {Toffetti, Giovanni and Brunner, Sandro and Bl\"{o}chlinger, Martin and Dudouet, Florian and Edmonds, Andrew},
title = {An Architecture for Self-Managing Microservices},
year = {2015},
isbn = {9781450334761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2747470.2747474},
doi = {10.1145/2747470.2747474},
abstract = {Running applications in the cloud efficiently requires much more than deploying software in virtual machines. Cloud applications have to be continuously managed: 1) to adjust their resources to the incoming load and 2) to face transient failures replicating and restarting components to provide resiliency on unreliable infrastructure. Continuous management monitors application and infrastructural metrics to provide automated and responsive reactions to failures (health management) and changing environmental conditions (auto-scaling) minimizing human intervention.In the current practice, management functionalities are provided as infrastructural or third party services. In both cases they are external to the application deployment. We claim that this approach has intrinsic limits, namely that separating management functionalities from the application prevents them from naturally scaling with the application and requires additional management code and human intervention. Moreover, using infrastructure provider services for management functionalities results in vendor lock-in effectively preventing cloud applications to adapt and run on the most effective cloud for the job.In this position paper we propose a novel architecture that enables scalable and resilient self-management of microservices applications on cloud.},
booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
pages = {19–24},
numpages = {6},
location = {Bordeaux, France},
series = {AIMC '15}
}

@inproceedings{10.1145/3219819.3219834,
author = {Staar, Peter W J and Dolfi, Michele and Auer, Christoph and Bekas, Costas},
title = {Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219834},
doi = {10.1145/3219819.3219834},
abstract = {Over the past few decades, the amount of scientific articles and technical literature has increased exponentially in size. Consequently, there is a great need for systems that can ingest these documents at scale and make the contained knowledge discoverable. Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images) as well as the presentation of the data (e.g. complex tables) make the extraction of qualitative and quantitive data extremely challenging. In this paper, we present a modular, cloud-based platform to ingest documents at scale. This platform, called the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse and annotate documents (i.e. collect ground-truth), train machine-learning classification algorithms and ultimately convert any type of PDF or bitmap-documents to a structured content representation format. We will show that each of the modules is scalable due to an asynchronous microservice architecture and can therefore handle massive amounts of documents. Furthermore, we will show that our capability to gather groundtruth is accelerated by machine-learning algorithms by at least one order of magnitude. This allows us to both gather large amounts of ground-truth in very little time and obtain very good precision/recall metrics in the range of 99\% with regard to content conversion to structured output. The CCS platform is currently deployed on IBM internal infrastructure and serving more than 250 active users for knowledge-engineering project engagements.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {774–782},
numpages = {9},
keywords = {asynchronous architecture, ibm research, artificial intelligence, machine learning, deep learning, cloud computing, ibm, knowledge ingestion, cloud architecture, ai, table processing, pdf, document conversion},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/3293455,
author = {Arcuri, Andrea},
title = {RESTful API Automated Test Case Generation with EvoMaster},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3293455},
doi = {10.1145/3293455},
abstract = {RESTful APIs are widespread in industry, especially in enterprise applications developed with a microservice architecture. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, because often the tested API is a remote service whose code is not available. In this article, we consider testing from the point of view of the developers, who have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding metrics. However, REST is not a protocol but rather a set of guidelines on how to design resources accessed over HTTP endpoints. For example, there are guidelines on how related resources should be structured with hierarchical URIs and how the different HTTP verbs should be used to represent well-defined actions on those resources. Test-case generation for RESTful APIs that only rely on white-box information of the source code might not be able to identify how to create prerequisite resources needed before being able to test some of the REST endpoints. Smart sampling techniques that exploit the knowledge of best practices in RESTful API design are needed to generate tests with predefined structures to speed up the search. We implemented our technique in a tool called EvoMaster, which is open source. Experiments on five open-source, yet non-trivial, RESTful services show that our novel technique automatically found 80 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such an approach are therefore discussed, such as the handling of SQL databases.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
articleno = {3},
numpages = {37},
keywords = {REST, Software engineering, web service, testing}
}

@inproceedings{10.5555/3101282.3101285,
author = {Aderaldo, Carlos M. and Mendon\c{c}a, Nabor C. and Pahl, Claus and Jamshidi, Pooyan},
title = {Benchmark Requirements for Microservices Architecture Research},
year = {2017},
isbn = {9781538604175},
publisher = {IEEE Press},
abstract = {Microservices have recently emerged as a new architectural style in which distributed applications are broken up into small independently deployable services, each running in its own process and communicating via lightweight mechanisms. However, there is still a lack of repeatable empirical research on the design, development and evaluation of microservices applications. As a first step towards filling this gap, this paper proposes, discusses and illustrates the use of an initial set of requirements that may be useful in selecting a community-owned architecture benchmark to support repeatable microservices research.},
booktitle = {Proceedings of the 1st International Workshop on Establishing the Community-Wide Infrastructure for Architecture-Based Software Engineering},
pages = {8–13},
numpages = {6},
keywords = {software architecture, microservices, research benchmark},
location = {Buenos Aires, Argentina},
series = {ECASE '17}
}

@article{10.1145/2829950,
author = {Tomusk, Erik and Dubach, Christophe and O’boyle, Michael},
title = {Four Metrics to Evaluate Heterogeneous Multicores},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2829950},
doi = {10.1145/2829950},
abstract = {Semiconductor device scaling has made single-ISA heterogeneous processors a reality. Heterogeneous processors contain a number of different CPU cores that all implement the same Instruction Set Architecture (ISA). This enables greater flexibility and specialization, as runtime constraints and workload characteristics can influence which core a given workload is run on. A major roadblock to the further development of heterogeneous processors is the lack of appropriate evaluation metrics. Existing metrics can be used to evaluate individual cores, but to evaluate a heterogeneous processor, the cores must be considered as a collective. Without appropriate metrics, it is impossible to establish design goals for processors, and it is difficult to accurately compare two different heterogeneous processors.We present four new metrics to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity, gap overhead, set overhead, and generality. The metrics consider sets rather than individual cores. We use examples to demonstrate each metric, and show that the metrics can be used to quantify intuitions about heterogeneous cores.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {37},
numpages = {25},
keywords = {single-ISA, gap overhead, set overhead, effective speed, generality, Localized nonuniformity}
}

@inproceedings{10.1145/3195546.3195549,
author = {Schmidts, Oliver and Kraft, Bodo and Schreiber, Marc and Z\"{u}ndorf, Albert},
title = {Continuously Evaluated Research Projects in Collaborative Decoupled Environments},
year = {2018},
isbn = {9781450357449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195546.3195549},
doi = {10.1145/3195546.3195549},
abstract = {Often, research results from collaboration projects are not transferred into productive environments even though approaches are proven to work in demonstration prototypes. These demonstration prototypes are usually too fragile and error-prone to be transferred easily into productive environments. A lot of additional work is required.Inspired by the idea of an incremental delivery process, we introduce an architecture pattern, which combines the approach of Metrics Driven Research Collaboration with microservices for the ease of integration. It enables keeping track of project goals over the course of the collaboration while every party may focus on their expert skills: researchers may focus on complex algorithms, practitioners may focus on their business goals.Through the simplified integration (intermediate) research results can be introduced into a productive environment which enables getting an early user feedback and allows for the early evaluation of different approaches. The practitioners' business model benefits throughout the full project duration.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice},
pages = {2–9},
numpages = {8},
keywords = {research collaboration management, metrics, lean software development, software architecture, research best practices},
location = {Gothenburg, Sweden},
series = {SER&amp;IP '18}
}

@inproceedings{10.1145/2973839.2973846,
author = {Aniche, Maur\'{\i}cio and Gerosa, Marco Aur\'{e}lio and Treude, Christoph},
title = {Developers' Perceptions on Object-Oriented Design and Architectural Roles},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973846},
doi = {10.1145/2973839.2973846},
abstract = {Software developers commonly rely on well-known software architecture patterns, such as MVC, to build their applications. In many of these patterns, classes play specific roles in the system, such as Controllers or Entities, which means that each of these classes has specific characteristics in terms of object-oriented class design and implementation. Indeed, as we have shown in a previous study, architectural roles are different from each other in terms of code metrics. In this paper, we present a study in a software development company in which we captured developers' perceptions on object-oriented design aspects of the architectural roles in their system and whether these perceptions match the source code metric analysis. We found that their developers do not have a common perception of how their architectural roles behave in terms of object-oriented design aspects, and that their perceptions also do not match the results of the source code metric analysis. This phenomenon also does not seem to be related to developers' experience. We find these results alarming, and thus, we suggest software development teams to invest in education and knowledge sharing about how their system's architectural roles behave.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {software architecture, object-oriented design, code metrics},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1145/3011077.3011078,
author = {Mellouk, Abdelhamid},
title = {New Provider Services for Convergence Technologies Based on Quality of Experience and Quality of Service Paradigms},
year = {2016},
isbn = {9781450348157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011077.3011078},
doi = {10.1145/3011077.3011078},
abstract = {Based on a convergence of network technologies, the Next Generation Network (NGN) is being deployed to carry high quality video and voice data. In fact, the convergence of network technologies has been driven by the converging needs of end-users. The perceived end-to-end quality is becoming one of the main goals required by users that must be guaranteed by the network operators and the Internet Service Providers, through manufacturer equipment. This is referred to as the notion of Quality of Experience (QoE) and is becoming commonly used to represent user perception. The QoE is not a technical metric, but rather a concept consisting of all elements of a user's perception of the network services. In this talk, we focus on the idea of how to integrate the QoE into a control- command chain in order to construct an adaptive network system. More precisely, in the context of Content-Oriented Networks that is used to redesign the current Internet architecture to accommodate content-oriented applications and services, the talk aim to describe an end-to-end QoE model applied to a Content Distribution Network architecture and see relationships between Quality of service and Quality of Experience.},
booktitle = {Proceedings of the 7th Symposium on Information and Communication Technology},
pages = {1},
numpages = {1},
location = {Ho Chi Minh City, Vietnam},
series = {SoICT '16}
}

@inproceedings{10.1145/3411029.3411032,
author = {Kogias, Marios and Bugnion, Edouard},
title = {Tail-Tolerance as a Systems Principle Not a Metric},
year = {2020},
isbn = {9781450388764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411029.3411032},
doi = {10.1145/3411029.3411032},
abstract = {Tail-latency tolerance (or just simply tail-tolerance) is the ability for a system to deliver a response with low-latency nearly all the time. It it typically expressed as a system metric (e.g., the 99th or 99.99th percentile latency) or as a service-level objective (e.g., the maximum throughput so that the tail latency is below a desired threshold). We advocate instead that modern datacenter systems should incorporate tail-tolerance as a core systems design principle and not a metric to be observed, and that tail-tolerant systems can be built out of large and complex applications whose individual components may suffer from latency deviations. This is analogous to fault-tolerance, where a fault-tolerant system can be built out of unreliable components. The general solution is for the system to control the applied load and keep it under the threshold that violates the latency SLO. We propose to augment RPC semantics with an architectural layer that measures the observed tail latency and probabilistically rejects RPC requests maintaining throughput under the threshold that violates the SLO. Our design is application-independent, and does not make any assumptions about the request service time distribution. We implemented a proof of concept for such a tail-tolerant layer using programmable switches, called SVEN. We demonstrate that the approach is suitable even for microsecond-scale RPCs with variable service times. Moreover, our approach does not induce measurable overheads, and can maintain the maximum achieved throughput very close to the load level that would violate the SLO without SVEN.},
booktitle = {Proceedings of the 4th Asia-Pacific Workshop on Networking},
pages = {16–22},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {APNet '20}
}

@inproceedings{10.5555/2821327.2821338,
author = {L\"{u}bke, Daniel},
title = {Using Metric Time-Lines for Identifying Architecture Shortcomings in Process Execution Architectures},
year = {2015},
publisher = {IEEE Press},
abstract = {Process Execution with Service Orchestrations is an emerging architectural style for developing business software systems. However, few special metrics for guiding software architecture decisions have been proposed and no existing business process metrics have been evaluated for their suitability. By following static code metrics over time, architects can gain a better understanding, how processes and the whole system evolve and whether the metrics evolve as expected. This allows architects to recogniize when to intervene in the development and make architecture adjustments or refactorings. This paper presents an explatory study that uses time-lines of static process size metrics for constant feedback to software architects that deal with process-oriented architectures.},
booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
pages = {55–58},
numpages = {4},
location = {Florence, Italy},
series = {SAM '15}
}

@inproceedings{10.1145/3401025.3401740,
author = {Scrocca, Mario and Tommasini, Riccardo and Margara, Alessandro and Valle, Emanuele Della and Sakr, Sherif},
title = {The Kaiju Project: Enabling Event-Driven Observability},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3401740},
doi = {10.1145/3401025.3401740},
abstract = {Microservices architectures are getting momentum. Even small and medium-size companies are migrating towards cloud-based distributed solutions supported by lightweight virtualization techniques, containers, and orchestration systems. In this context, understanding the system behavior at runtime is critical to promptly react to errors. Unfortunately, traditional monitoring techniques are not adequate for such complex and dynamic environments. Therefore, a new challenge, namely observability, emerged from precise industrial needs: expose and make sense of the system behavior at runtime. In this paper, we investigate observability as a research problem. We discuss the benefits of events as a unified abstraction for metrics, logs, and trace data, and the advantages of employing event stream processing techniques and tools in this context. We show that an event-based approach enables understanding the system behavior in near real-time more effectively than state-of-the-art solutions in the field. We implement our model in the Kaiju system and we validate it against a realistic deployment supported by a software company.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {85–96},
numpages = {12},
keywords = {orchestration systems, observability, event-based systems, event stream processing},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3164541.3164576,
author = {Kim, Heejin and Jeon, Seil and Raza, Syed M. and Lee, Joohyun and Choo, Hyunseung},
title = {Service-Aware Split Point Selection for User-Centric Mobility Enhancement in SDN},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164576},
doi = {10.1145/3164541.3164576},
abstract = {IP mobility anchor works as the redirection/split point of the packet destined to the mobile terminal (MT), as well as IP address/prefix assignment and mobility binding management in the legacy mobility management protocols. In software-defined networking (SDN), the split point can be managed by the SDN controller or controller application, as the control of the network is separated from the forwarding entities. The demand of user QoE is ever increasing and they always want to get the best service continuity served in mobility management. Differentiated split point selection per service type could be one of the effective measures to enhance user QoE in a mobility management environment. In this paper, we propose a service-aware split point selection mechanism for user-centric mobility management enhancement in SDN. Specifically, we propose the mobility control architecture, which can classify service flow type and determine advantageous split point depending on a service flow type. We analyze the performance of the proposed split point selection mechanism compared to target mechanisms. We also measure the performance metrics on an ONOS-based SDN testbed to identify the superiority of the proposed mechanism.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {95},
numpages = {8},
keywords = {split point selection, mobility management, software-defined networking},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@inproceedings{10.1145/3098954.3098977,
author = {Boukoros, Spyros and Katzenbeisser, Stefan},
title = {Measuring Privacy in High Dimensional Microdata Collections},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3098977},
doi = {10.1145/3098954.3098977},
abstract = {Microdata is collected by companies in order to enhance their quality of service as well as the accuracy of their recommendation systems. These data often become publicly available after they have been sanitized. Recent reidentification attacks on publicly available, sanitized datasets illustrate the privacy risks involved in microdata collections. Currently, users have to trust the provider that their data will be safe in case data is published or if a privacy breach occurs. In this work, we empower users by developing a novel, user-centric tool for privacy measurement and a new lightweight privacy metric. The goal of our tool is to estimate users' privacy level prior to sharing their data with a provider. Hence, users can consciously decide whether to contribute their data. Our tool estimates an individuals' privacy level based on published popularity statistics regarding the items in the provider's database, and the users' microdata. In this work, we describe the architecture of our tool as well as a novel privacy metric, which is necessary for our setting where we do not have access to the provider's database. Our tool is user friendly, relying on smart visual results that raise privacy awareness. We evaluate our tool using three real world datasets, collected from major providers. We demonstrate strong correlations between the average anonymity set per user and the privacy score obtained by our metric. Our results illustrate that our tool which uses minimal information from the provider, estimates users' privacy levels comparably well, as if it had access to the actual database.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {15},
numpages = {8},
keywords = {user empowerment, privacy metrics, privacy, microdata},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/3258045,
author = {Savola, Reijo and Abie, Habtamu and Kanstr\'{e}n, Teemu},
title = {Session Details: Fourth International Workshop on Measurability of Security in Software Architectures (MeSSa 2017)},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3258045},
doi = {10.1145/3258045},
abstract = {Cybersecurity incidents are increasing, and at the same time, our society depends more and more on cyber-physical systems. Systematic approaches to measure cybersecurity are needed in order to support efficient construction and maintenance of secure software systems. Security measurement of software architectures is needed to produce sufficient evidence of security level as early as in the design phase. Design-time security measuring should support "security by design" approach. Moreover, software architectures have to support runtime security measurement to obtain up-to-date security information from an online software system, service or product. Security metrics and measurements are exploited in situational awareness monitoring and self-adaptive security solutions. The area of security metrics and security assurance metrics research is evolving, but still lacks widely accepted metrics definitions and applicable measuring techniques. Strong collaboration between security experts, software architects and system developers is needed to address this. MeSSa2017 workshop addresses these and other related topics to increase the importance of the overall picture, requiring sets of design patterns, measurements, metrics, best practices, and means to integrate this cost-effectively in the overall design and operational profiles.The outcome of the workshop will be an increased shared understanding of challenges and opportunities in systematic approaches to measure cybersecurity, which are needed in order to support efficient construction and maintenance of secure software systems.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/3368691.3368704,
author = {Albataineh, Abdallah and Al-Qassas, Raad S. and Qasaimeh, Malik},
title = {A New Architecture for Voice Interconnection Using Packet Switched Network},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368704},
doi = {10.1145/3368691.3368704},
abstract = {Interconnecting voice service providers require a mutual trust between communicating entities, which are built either using bilateral agreements or intermediary service provider. To achieve such relationship between Anonymous Service Providers we should have an automated mechanism. In this paper, we propose a conceptual architecture that can build such relationship between communicating Anonymous Service Providers. By applying this architecture, we argue that we can increase efficiency, security, and performance of service provider's networks. The impact of internet speed on the interconnection network is measured using key metrics including ACD, ASR, PDD, NER, and MOS.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {13},
numpages = {7},
keywords = {PDD, ACD, SIP, ASR, SS7, voice network architecture},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3359789.3359843,
author = {Nagendra, Vasudevan and Yegneswaran, Vinod and Porras, Phillip and Das, Samir R},
title = {Coordinated Dataflow Protection for Ultra-High Bandwidth Science Networks},
year = {2019},
isbn = {9781450376280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359789.3359843},
doi = {10.1145/3359789.3359843},
abstract = {The Science DMZ (SDMZ) is a special purpose network architecture proposed by ESnet (Energy Sciences Network) to facilitate distributed science experimentation on terabyte- (or petabyte-) scale data, exchanged over ultra-high bandwidth WAN links. Critical security challenges faced by these networks include: (i) network monitoring at high bandwidths, (ii) reconciling site-specific policies with project-level policies for conflict-free policy enforcement, (iii) dealing with geographically-distributed datasets with varying levels of sensitivity, and (iv) dynamically enforcing appropriate security rules. To address these challenges, we develop a fine-grained dataflow-based security enforcement system, called CoordiNetZ (CNZ), that provides coordinated situational awareness, i.e., the use of context-aware tagging for policy enforcement using the dynamic contextual information derived from hosts and network elements. We also developed tag and IP-based security microservices that incur minimal overheads in enforcing security to data flows exchanged across geographically-distributed SDMZ sites. We evaluate our prototype implementation across two geographically distributed SDMZ sites with SDN-based case studies, and present performance measurements that respectively highlight the utility of our framework and demonstrate efficient implementation of security policies across distributed SDMZ networks.},
booktitle = {Proceedings of the 35th Annual Computer Security Applications Conference},
pages = {568–583},
numpages = {16},
keywords = {usability and human-centric aspects of security, SDN, distributed systems security, software-defined programmable security, big data security, network security, NFV},
location = {San Juan, Puerto Rico, USA},
series = {ACSAC '19}
}

@inproceedings{10.1145/3338466.3358917,
author = {Heinl, Michael P. and Giehl, Alexander and Wiedermann, Norbert and Plaga, Sven and Kargl, Frank},
title = {MERCAT: A Metric for the Evaluation and Reconsideration of Certificate Authority Trustworthiness},
year = {2019},
isbn = {9781450368261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338466.3358917},
doi = {10.1145/3338466.3358917},
abstract = {Public key infrastructures (PKIs) build the foundation for secure communication of a vast majority of cloud services. In the recent past, there has been a series of security incidents leading to increasing concern regarding the trust model currently employed by PKIs. One of the key criticisms is the architecture's implicit assumption that certificate authorities (CAs) are trustworthy a priori.This work proposes a holistic metric to compensate this assumption by a differentiating assessment of a CA's individual trustworthiness based on objective criteria. The metric utilizes a wide range of technical and non-technical factors derived from existing policies, technical guidelines, and research. It consists of self-contained submetrics allowing the simple extension of the existing set of criteria. The focus is thereby on aspects which can be assessed by employing practically applicable methods of independent data collection.The metric is meant to help organizations, individuals, and service providers deciding which CAs to trust or distrust. For this, the modularized submetrics are clustered into coherent submetric groups covering a CA's different properties and responsibilities. By applying individually chosen weightings to these submetric groups, the metric's outcomes can be adapted to tailored protection requirements according to an exemplifying attacker model.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
pages = {1–15},
numpages = {15},
keywords = {cloud security, ca, metric, digital certificate, x.509, trustworthiness assessment, pki},
location = {London, United Kingdom},
series = {CCSW'19}
}

@inproceedings{10.5555/3191835.3191902,
author = {Lin, Chih-Lu and Chen, Ying-Liang and Kao, Hung-Yu},
title = {Question Difficulty Evaluation by Knowledge Gap Analysis in Question Answer Communities},
year = {2014},
isbn = {9781479958764},
publisher = {IEEE Press},
abstract = {The Community Question Answer (CQA) service is a typical forum of Web 2.0 that shares knowledge among people. There are thousands of questions that are posted and solved every day. Because of the various users of the CQA service, question search and ranking are the most important topics of research in the CQA portal. In this study, we addressed the problem of identifying questions as being hard or easy by means of a probability model. In addition, we observed the phenomenon called knowledge gap that is related to the habit of users and used a knowledge gap diagram to illustrate how much of a knowledge gap exists in different categories. To this end, we proposed an approach called the knowledge-gap-based difficulty rank (KG-DRank) algorithm, which combines the user-user network and the architecture of the CQA service to find hard questions. We used f-measure, AUC, MAP, NDCG, precision@Top5 and concordance analysis to evaluate the experimental results. Our results show that our approach leads to better performance than other baseline approaches across all evaluation metrics.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {336–339},
numpages = {4},
keywords = {social network, link analysis, knowledge gap, expert finding, difficulty, CQA portal},
location = {Beijing, China},
series = {ASONAM '14}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Software Development and Research with the HADAS Toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {software product line, CVL, energy efficiency, optimisation, clafer, metrics, repository, variability},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@article{10.1145/3351278,
author = {Yu, Tuo and Nahrstedt, Klara},
title = {ShoesHacker: Indoor Corridor Map and User Location Leakage through Force Sensors in Smart Shoes},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351278},
doi = {10.1145/3351278},
abstract = {The past few years have witnessed the rise of smart shoes, the wearable devices that measure foot force or track foot motion. However, people are not aware of the possible privacy leakage from in-shoe force sensors. In this paper, we explore the possibility of locating an indoor victim based on the force signals leaked from smart shoes. We present ShoesHacker, an attack scheme that reconstructs the corridor map of the building that the victim walks in based on force data only. The corridor map enables the attacker to recognize the building, and thus locate the victim on a global map. To handle the lack of training data, we design the stair landing detection algorithm, based on which we extract training data when victims are walking in stairwells. We estimate the trajectory of each walk, and propose the path merging algorithm to merge the trajectories. Moreover, we design a metric to quantify the similarity between corridor maps, which makes building recognition possible. Our experimental results show that, the building recognition accuracy reaches 77.5\% in a 40-building dataset, and the victim can be located with an average error lower than 6 m, which reveals the danger of privacy leakage through smart shoes. CCS Concepts: • Information systems~Mobile information processing systems; Location based services; • Human-centered computing~Mobile devices; Ubiquitous and mobile computing systems and tools; • Security and privacy~Domain-specific security and privacy architectures.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {120},
numpages = {29},
keywords = {Corridor map reconstruction, force sensors, smart shoes}
}

@inproceedings{10.1145/2745802.2745822,
author = {Stevanetic, Srdjan and Zdun, Uwe},
title = {Software Metrics for Measuring the Understandability of Architectural Structures: A Systematic Mapping Study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745822},
doi = {10.1145/2745802.2745822},
abstract = {The main idea of software architecture is to concentrate on the "big picture" of a software system. In the context of object-oriented software systems higher-level architectural structures or views above the level of classes are frequently used to capture the "big picture" of the system. One of the critical aspects of these higher-level views is understandability, as one of their main purposes is to enable designers to abstract away fine-grained details. In this article we present a systematic mapping study on software metrics related to the understandability concepts of such higher-level software structures with regard to their relations to the system implementation. In our systematic mapping study, we started from 3951 studies obtained using an electronic search in the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our inclusion/exclusion criteria as well as the snowballing technique we selected 268 studies for in-depth study. From those, we selected 25 studies that contain relevant metrics. We classify the identified studies and metrics with regard to the measured artefacts, attributes, quality characteristics, and representation model used for the metrics definitions. Additionally, we present the assessment of the maturity level of the identified studies. Overall, there is a lack of maturity in the studies. We discuss possible techniques how to mitigate the identified problems. From the academic point of view we believe that our study is a good starting point for future studies aiming at improving the existing works. From a practitioner's point of view, the results of our study can be used as a catalogue and an indication of the maturity of the existing research results.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {21},
numpages = {14},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1145/2601248.2601264,
author = {Stevanetic, Srdjan and Zdun, Uwe},
title = {Exploring the Relationships between the Understandability of Components in Architectural Component Models and Component Level Metrics},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601264},
doi = {10.1145/2601248.2601264},
abstract = {Architectural component models represent high level designs and are frequently used as a central view of architectural descriptions of software systems. The components in those models represent important high level organization units that group other components and classes in object-oriented design views. Hence, understandability of components and their interactions plays a key role in supporting the architectural understanding of a software system. In this paper we present a study we carried out to examine the relationships between the effort required to understand a component, measured through the time that participants spent on studying a component, and component level metrics that describe component's size, complexity and coupling in terms of the number of classes in a component and the classes' relationships. The participants were 49 master students, and they had to fully understand the components' functionalities in order to answer 4 true/false questions for each of the 7 components in the architecture of the Soomla Android store system. Correlation, collinearity and multivariate regression analysis were performed. The results of the analysis show a statistically significant correlation between three of the metrics, number of classes, number of incoming dependencies, and number of internal dependencies, on one side, and the effort required to understand a component, on the other side. In a multivariate regression analysis we obtained 3 reasonably well-fitting models that can be used to estimate the effort required to understand a component. In our future work we plan to study more components and investigate more metrics and their relationships to the understandability of components and architectural component models.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {32},
numpages = {10},
keywords = {software metrics, empirical evaluation, architectural component models, understandability},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1145/3267955.3267965,
author = {Khan, Junaid Ahmed and Westphal, Cedric and Garcia-Luna-Aceves, J. J. and Ghamri-Doudane, Yacine},
title = {NICE: Network-Oriented Information-Centric Centrality for Efficiency in Cache Management},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3267965},
doi = {10.1145/3267955.3267965},
abstract = {All Information-Centric Networking (ICN) architectures proposed to date aim at connecting users to content directly, rather than connecting clients to servers. Surprisingly, however, although content caching is an integral of any information-Centric Network, limited work has been reported on information-centric management of caches in the context of an ICN. Indeed, approaches to cache management in networks of caches have focused on network connectivity rather than proximity to content.We introduce the Network-oriented Information-centric Centrality for Efficiency (NICE) as a new metric for cache management in information-centric networks. We propose a method to compute information-centric centrality that scales with the number of caches in a network rather than the number of content objects, which is many orders of magnitude larger. Furthermore, it can be pre-processed offline and ahead of time. We apply the NICE metric to a content replacement policy in caches, and show that a content replacement based on NICE exhibits better performances than LRU and other policies based on topology-oriented definitions of centrality.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {31–42},
numpages = {12},
keywords = {content offloading, graph centrality, ICN, cache management},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@inproceedings{10.1145/3375555.3384936,
author = {Avritzer, Alberto},
title = {Automated Scalability Assessment in DevOps Environments},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3384936},
doi = {10.1145/3375555.3384936},
abstract = {In this extended abstract, we provide an outline of the presentation planned for WOSP-C 2020. The goal of the presentation is to provide an overview of the challenges and approaches for automated scalability assessment in the context of DevOps and microservices. The focus of this presentation is on approaches that employ automated identification of performance problems because these approaches can leverage performance anti-pattern[5] detection technology. In addition, we envision extending the approach to recommend component refactoring. In our previous work[1,2] we have designed a methodology and associated tool support for the automated scalability assessment of micro-service architectures, which included the automation of all the steps required for scalability assessment. The presentation starts with an introduction to dependability, operational Profile Data, and DevOps. Specifically, we provide an overview of the state of the art in continuous performance monitoring technologies[4] that are used for obtaining operational profile data using APM tools. We then present an overview of selected approaches for production and performance testing based on the application monitoring tool (PPTAM) as introduced in [1,2]. The presentation concludes by outlining a vision for automated performance anti-pattern[5] detection. Specifically, we present the approach introduced for automated anti-pattern detection based on load testing results and profiling introduced in[6] and provide recommendations for future research.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {10},
numpages = {1},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3204949.3204956,
author = {Mangla, Tarun and Zegura, Ellen and Ammar, Mostafa and Halepovic, Emir and Hwang, Kyung-Wook and Jana, Rittwik and Platania, Marco},
title = {VideoNOC: Assessing Video QoE for Network Operators Using Passive Measurements},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204956},
doi = {10.1145/3204949.3204956},
abstract = {Video streaming traffic is rapidly growing in mobile networks. Mobile Network Operators (MNOs) are expected to keep up with this growing demand, while maintaining a high video Quality of Experience (QoE). This makes it critical for MNOs to have a solid understanding of users' video QoE with a goal to help with network planning, provisioning and traffic management. However, designing a system to measure video QoE has several challenges: i) large scale of video traffic data and diversity of video streaming services, ii) cross-layer constraints due to complex cellular network architecture, and iii) extracting QoE metrics from network traffic. In this paper, we present VideoNOC, a prototype of a flexible and scalable platform to infer objective video QoE metrics (e.g., bitrate, rebuffering) for MNOs. We describe the design and architecture of VideoNOC, and outline the methodology to generate a novel data source for fine-grained video QoE monitoring. We then demonstrate some of the use cases of such a monitoring system. VideoNOC reveals video demand across the entire network, provides valuable insights on a number of design choices by content providers (e.g., OS-dependent performance, video player parameters like buffer size, range of encoding bitrates, etc.) and helps analyze the impact of network conditions on video QoE (e.g., mobility and high demand).},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {101–112},
numpages = {12},
keywords = {passive measurement, cellular network, video streaming, QoE},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/3234944.3234952,
author = {Kim, Yubin and Callan, Jamie},
title = {Measuring the Effectiveness of Selective Search Index Partitions without Supervision},
year = {2018},
isbn = {9781450356565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234944.3234952},
doi = {10.1145/3234944.3234952},
abstract = {Selective search architectures partition a document collection into topic-oriented index shards, usually using algorithms that have random components. Different mappings of documents into index shards (shard maps) produce different search accuracy and consistency, however identifying which shard maps will deliver the highest average effectiveness is an open problem. This paper presents a new metric, Area Under Recall Curve (AUReC), to evaluate and compare shard maps. AUReC is the first such metric that is independent of resource selection and shard cut-off estimation. It does not require an end-to-end evaluation or manual gold-standard judgements. Experiments show that its predictions are highly-correlated with evaluating end-to-end systems of various configurations, while being easier to implement and computationally inexpensive.},
booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {91–98},
numpages = {8},
keywords = {distributed search, selective search, clustering, evaluation, cluster-based retrieval},
location = {Tianjin, China},
series = {ICTIR '18}
}

@inproceedings{10.1145/3357384.3357956,
author = {Boiarov, Andrei and Tyantov, Eduard},
title = {Large Scale Landmark Recognition via Deep Metric Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357956},
doi = {10.1145/3357384.3357956},
abstract = {This paper presents a novel approach for landmark recognition in images that we've successfully deployed at Mail.ru. This method enables us to recognize famous places, buildings, monuments, and other landmarks in user photos. The main challenge lies in the fact that it's very complicated to give a precise definition of what is and what is not a landmark. Some buildings, statues and natural objects are landmarks; others are not. There's also no database with a fairly large number of landmarks to train a recognition model. A key feature of using landmark recognition in a production environment is that the number of photos containing landmarks is extremely small. This is why the model should have a very low false positive rate as well as high recognition accuracy. We propose a metric learning-based approach that successfully deals with existing challenges and efficiently handles a large number of landmarks. Our method uses a deep neural network and requires a single pass inference that makes it fast to use in production. We also describe an algorithm for cleaning landmarks database which is essential for training a metric learning model. We provide an in-depth description of basic components of our method like neural network architecture, the learning strategy, and the features of our metric learning approach. We show the results of proposed solutions in tests that emulate the distribution of photos with and without landmarks from a user collection. We compare our method with others during these tests. The described system has been deployed as a part of a photo recognition solution at Cloud Mail.ru, which is the photo sharing and storage service at Mail.ru Group.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {169–178},
numpages = {10},
keywords = {metric learning, landmark recognition, deep learning},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3368235.3370269,
author = {Spillner, Josef},
title = {Serverless Computing and Cloud Function-Based Applications},
year = {2019},
isbn = {9781450370448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368235.3370269},
doi = {10.1145/3368235.3370269},
abstract = {Serverless computing is a growing industry trend with corresponding rise in interest by scholars and tinkerers. Increasingly, open source and academic system prototypes are being proposed especially in relation with cloud, edge and fog computing among other distributed computing specialisations. Due to the strict separation between elastically scalable stateless microservices bound to stateful backend services prevalent in this computing paradigm, the resulting applications are inherently distributed with favourable characteristics such as elastic scalability and disposability. Still, software application developers are confronted with a multitude of different methods and tools to build, test and deploy their function-based applications in today's serverless ecosystems. The logical next step is therefore a methodical development approach with key enablers based on a classification of languages, tools, systems, system behaviours, patterns, pitfalls, application architectures, compositions and cloud services around the serverless application development process.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
pages = {177–178},
numpages = {2},
keywords = {tutorial, artefact quality, serverless computing, cloud functions},
location = {Auckland, New Zealand},
series = {UCC '19 Companion}
}

@inproceedings{10.1145/3131151.3131153,
author = {Durelli, Rafael S. and Viana, Matheus C. and de S. Landi, Andr\'{e} and Durelli, Vinicius H. S. and Delamaro, Marcio E. and de Camargo, Valter V.},
title = {Improving the Structure of KDM Instances via Refactorings: An Experimental Study Using KDM-RE},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131153},
doi = {10.1145/3131151.3131153},
abstract = {Architecture-Driven Modernization (ADM) is an initiative of the Object Management Group (OMG) whose main purpose is to provide standard metamodels for software modernization activities. The most important metamodel is the Knowledge Discovery Metamodel (KDM), which represents software artifacts in a language-agnostic fashion. A fundamental step in software modernization is refactoring. However, there is a lack of tools that address how refactoring can be applied in conjunction with ADM. We developed a tool, called KDM-RE, that supports refactorings in KDM instances through: (i) a set of wizards that aid the software modernization engineer during refactoring activities; (ii) a change propagation module that keeps the internal metamodels synchronized; and (iii) the selection and application of refactorings available in its repository. This paper evaluates the application of refactorings to KDM instances in an experiment involving seven systems implemented in Java. We compared the pre-refactoring versions of these systems with the refactored ones using the Quality Model for Object-Oriented Design (QMOOD) metric set. The results from this evaluation suggest that KDM-RE provides advantages to software modernization engineers refactoring systems represented as KDMs.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {174–183},
numpages = {10},
keywords = {Model-Driven Development, Knowledge-Discovery Metamodel, Architecture-Driven Modernization, Refactoring},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@inproceedings{10.1145/3209978.3210005,
author = {Mohammad, Hafeezul Rahman and Xu, Keyang and Callan, Jamie and Culpepper, J. Shane},
title = {Dynamic Shard Cutoff Prediction for Selective Search},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210005},
doi = {10.1145/3209978.3210005},
abstract = {Selective search architectures use resource selection algorithms such as Rank-S or Taily to rank index shards and determine how many to search for a given query. Most prior research evaluated solutions by their ability to improve efficiency without significantly reducing early-precision metrics such as P@5 and NDCG@10. This paper recasts selective search as an early stage of a multi-stage retrieval architecture, which makes recall-oriented metrics more appropriate. A new algorithm is presented that predicts the number of shards that must be searched for a given query in order to meet recall-oriented goals. Decoupling shard ranking from deciding how many shards to search clarifies efficiency vs. effectiveness trade-offs, and enables them to be optimized independently. Experiments on two corpora demonstrate the value of this approach.},
booktitle = {The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
pages = {85–94},
numpages = {10},
keywords = {distributed search, resource selection, selective search},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3178461.3178462,
author = {Meryem, Amar and Samira, Douzi and Bouabid, El Ouahidi},
title = {Enhancing Cloud Security Using Advanced MapReduce K-Means on Log Files},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178462},
doi = {10.1145/3178461.3178462},
abstract = {Many customers ranked cloud security as a major challenge that threaten their work and reduces their trust on cloud service's provider. Hence, a significant improvement is required to establish better adaptations of security measures that suit recent technologies and especially distributed architectures.Considering the meaningful recorded data in cloud generated log files, making analysis on them, mines insightful value about hacker's activities. It identifies malicious user behaviors and predicts new suspected events. Not only that, but centralizing log files, prevents insiders from causing damage to system. In this paper, we proposed to take away sensitive log files into a single server provider and combining both MapReduce programming and k-means on the same algorithm to cluster observed events into classes having similar features. To label unknown user behaviors and predict new suspected activities this approach considers cosine distances and deviation metrics.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {63–67},
numpages = {5},
keywords = {MapReduce, Cloud Security, log files, K-means, Deviation metric},
location = {Casablanca, Morocco},
series = {ICSIM '18}
}

@article{10.1145/2659118.2659135,
author = {Tiwari, Umesh and Kumar, Santosh},
title = {In-out Interaction Complexity Metrics for Component-Based Software},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2659118.2659135},
doi = {10.1145/2659118.2659135},
abstract = {In the current state of software engineering, component-based software development is one of the most alluring paradigms for developing large and complex software products. In this software engineering methodology pre-engineered, pre-tested, context-based, adaptable, deployable software components are assembled according to a predefined architecture. Rather than developing a system from scratch, component-based software development emphasizes the integration of these components according to the user's requirements and specifications. In component-based software, the components interact to access and provide services and functionality to each other. Currently, the emphasis of industry and researchers is on developing impressive and efficient metrics and measurement tools to analyze the interaction complexity among these components. To represent the request and the response of services among components, we have used outgoing edges and incoming edges respectively. In this paper we have defined these interactions as In-Interactions and Out-Interactions. The metrics proposed in this paper are solely based on the interactions among the components. In this work some simple methods and metrics for computing the complexity of composable components are suggested. The metrics discussed in this paper include the computation of interaction complexities as Total-Interactions of a component, Total- Interactions of component-based software, Interaction-Ratio of a component, Interaction-Ratio of component-based software, Average- Interaction among components and Interaction-Percentage of components.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–4},
numpages = {4},
keywords = {component-based software development, pre-engineered, adaptable, in-interactions, context-based, out-interactions, metrics}
}

@inproceedings{10.1145/3102304.3102334,
author = {Calcina-Ccori, Pablo and Costa, Laisa and Fedrecheski, Geovane and Esquiagola, John and Zuffo, Marcelo and da Silva, Fl\'{a}vio Corr\^{e}a},
title = {Agile Servient Integration with the Swarm: Automatic Code Generation for Nodes in the Internet of Things},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102334},
doi = {10.1145/3102304.3102334},
abstract = {Swarm vision, consists in an organic ecosystem of heterogeneous devices that communicate and collaborate to achieve complex results. In previous work, we have proposed an architecture to implement this vision based on web technologies. In this paper, we have proposed a framework that makes the creation of Swarm-ready servients (devices that acts both as server and client) easier, by generating a ready-to-run project from a high-level description of the service. The project generated contains all dependencies and libraries needed to integrate an IoT device into the Swarm, thus saving development and configuration time. We compared the development effort of creating a servient by hand and by using our framework, having the number of lines of code as a metric. Our results show a reduction of 500\% in the development effort to connect a device to the Swarm. The next steps include a semantic high-level description for participating services and support for resource-constrained devices.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {30},
numpages = {6},
keywords = {Servient, automatic code generation, Internet of Things, Swarm},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/3167486.3167534,
author = {Seraoui, Youssef and Belmekki, Mostafa and Bellafkih, Mostafa and Raouyane, Brahim},
title = {ETOM Mapping onto NFV Framework: IMS Use Case},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167534},
doi = {10.1145/3167486.3167534},
abstract = {Telecom professionals have a strong interest in the proposition and adaptation of innovate network management models and frameworks to help mobile network operators (MNOs) to improve their business processes and get more agile in the telecoms industry that evolves with great speed. The model being established by the TeleManagement Forum (TM Forum) is the Enhanced Telecom Operations MAP (eTOM) business process framework on which we rely in this work to propose a mapping of the eTOM model onto the network functions virtualization (NFV) framework with the projection of this function mapping onto the IP Multimedia Subsystem (IMS) use case. This mapping covers essentially four main components playing important rules in the MNO's business processes, including customers, services, infrastructure resources, and also service providers. The main goal, thereby, is to design a combined architecture in a virtualized environment for dynamic delivery of services with quality of service (QoS) and improved resource performance so as to meet the purposes of the 5G network in terms of a proposed, virtual telecom environment managed and orchestrated by the conjunction of the aforementioned paradigms. Indeed, we conducted simulations to evaluate part of this function mapping in an IMS setting for static service chain provisioning. Thus, results showed possible provisioning of services in this context in measuring SIP related key performance indicators and performance metrics. Results showed the feasibility of our approach. In addition, resource performance improved obviously in the NFV context in accordance with eTOM business processes.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {45},
numpages = {8},
keywords = {five generation (5G), service chain provisioning, network functions virtualization (NFV), IP Multimedia Subsystem (IMS), Business process, quality of service (QoS), New Generation Operations Systems and Software (NGOSS), resource performance, Enhanced Telecom Operations Map (eTOM)},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@inproceedings{10.1145/3465481.3470091,
author = {Komisarek, Miko\l{}aj and Pawlicki, Marek and Kowalski, Miko\l{}aj and Marzecki, Adrian and Kozik, Rafa\l{} and Chora\'{s}, Micha\l{}},
title = {Network Intrusion Detection in the Wild - the Orange Use Case in the SIMARGL Project},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470091},
doi = {10.1145/3465481.3470091},
abstract = {There is a profuse abundance of network security incidents around the world every day. Increasingly, services and data stored on servers fall victim to sophisticated techniques that cause all sorts of damage. Hackers invent new ways to bypass security measures and modify the existing viruses in order to deceive defense systems. Therefore, in response to these illegal procedures, new ways to defend against them are being developed. In this paper, a method for anomaly detection based on machine learning technique is presented and a near real-time processing system architecture is proposed. The main contribution is a test-run of ML algorithms on real-world data coming from a world-class telecom operator. This work investigates the effectiveness of detecting malicious behaviour in network packets using several machine learning techniques. The results achieved are expressed with a set of metrics. For better clarity on the classifier performance, 10-fold cross-validation was used.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {65},
numpages = {7},
keywords = {machine learning, network intrusion detection},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1145/3238147.3240467,
author = {Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin},
title = {Experiences Applying Automated Architecture Analysis Tool Suites},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240467},
doi = {10.1145/3238147.3240467},
abstract = {In this paper, we report our experiences of applying three complementary automated software architecture analysis techniques, supported by a tool suite, called DV8, to 8 industrial projects within a large company. DV8 includes two state-of-the-art architecture-level maintainability metrics—Decoupling Level and Propagation Cost, an architecture flaw detection tool, and an architecture root detection tool. We collected development process data from the project teams as input to these tools, reported the results back to the practitioners, and followed up with telephone conferences and interviews. Our experiences revealed that the metrics scores, quantitative debt analysis, and architecture flaw visualization can effectively bridge the gap between management and development, help them decide if, when, and where to refactor. In particular, the metrics scores, compared against industrial benchmarks, faithfully reflected the practitioners’ intuitions about the maintainability of their projects, and enabled them to better understand the maintainability relative to other projects internal to their company, and to other industrial products. The automatically detected architecture flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify the “hotspots" within the systems that are responsible for high maintenance costs. Except for the two smallest projects for which both architecture metrics indicated high maintainability, all other projects are planning or have already begun refactorings to address the problems detected by our analyses. We are working on further automating the tool chain, and transforming the analysis suite into deployable services accessible by all projects within the company.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {779–789},
numpages = {11},
keywords = {Software Architecture, Software Maintenance, Software Quality},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1145/2983637,
author = {Miao, Wang and Min, Geyong and Wu, Yulei and Wang, Haozhe and Hu, Jia},
title = {Performance Modelling and Analysis of Software-Defined Networking under Bursty Multimedia Traffic},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2983637},
doi = {10.1145/2983637},
abstract = {Software-Defined Networking (SDN) is an emerging architecture for the next-generation Internet, providing unprecedented network programmability to handle the explosive growth of big data driven by the popularisation of smart mobile devices and the pervasiveness of content-rich multimedia applications. In order to quantitatively investigate the performance characteristics of SDN networks, several research efforts from both simulation experiments and analytical modelling have been reported in the current literature. Among those studies, analytical modelling has demonstrated its superiority in terms of cost-effectiveness in the evaluation of large-scale networks. However, for analytical tractability and simplification, existing analytical models are derived based on the unrealistic assumptions that the network traffic follows the Poisson process, which is suitable to model nonbursty text data, and the data plane of SDN is modelled by one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies have shown that, due to the features of heavy volume and high velocity, the multimedia big data generated by real-world multimedia applications reveals the bursty and correlated nature in the network transmission. With the aim of capturing such features of realistic traffic patterns and obtaining a comprehensive and deeper understanding of the performance behaviour of SDN networks, this article presents a new analytical model to investigate the performance of SDN in the presence of the bursty and correlated arrivals modelled by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance metrics in terms of the average latency and average network throughput of the SDN networks are derived based on the developed analytical model. To consider a realistic multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted to model the SDN data plane. To address the challenging problem of obtaining the key performance metrics, for example, queue-length distribution of a PQ system with a given service capacity, a versatile methodology extending the Empty Buffer Approximation (EBA) method is proposed to facilitate the decomposition of such a PQ system to two SSSQ systems. The validity of the proposed model is demonstrated through extensive simulation experiments. To illustrate its application, the developed model is then utilised to study the strategy of the network configuration and resource allocation in SDN networks.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {77},
numpages = {19},
keywords = {performance modelling and analysis, resource allocation, multimedia big data, Software-defined networking, queueing decomposition}
}

@article{10.1145/3394956,
author = {Sahoo, Kshira Sagar and Puthal, Deepak},
title = {SDN-Assisted DDoS Defense Framework for the Internet of Multimedia Things},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3394956},
doi = {10.1145/3394956},
abstract = {The Internet of Things is visualized as a fundamental networking model that bridges the gap between the cyber and real-world entity. Uniting the real-world object with virtualization technology is opening further opportunities for innovation in nearly every individual’s life. Moreover, the usage of smart heterogeneous multimedia devices is growing extensively. These multimedia devices that communicate among each other through the Internet form a unique paradigm called the Internet of Multimedia Things (IoMT). As the volume of the collected data in multimedia application increases, the security, reliability of communications, and overall quality of service need to be maintained. Primarily, distributed denial of service attacks unveil the pervasiveness of vulnerabilities in IoMT systems. However, the Software Defined Network (SDN) is a new network architecture that has the central visibility of the entire network, which helps to detect any attack effectively. In this regard, the combination of SDN and IoMT, termed SD-IoMT, has the immense ability to improve the network management and security capabilities of the IoT system. This article proposes an SDN-assisted two-phase detection framework, namely SD-IoMT-Protector, in which the first phase utilizes the entropy technique as the detection metric to verify and alert about the malicious traffic. The second phase has trained with an optimized machine learning technique for classifying different attacks. The outcomes of the experimental results signify the usefulness and effectiveness of the proposed framework for addressing distributed denial of service issues of the SD-IoMT system.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {dec},
articleno = {98},
numpages = {18},
keywords = {entropy, SDN, Control plane, IoMT, machine learning, security}
}

@article{10.1145/3319498,
author = {Izadpanah, Ramin and Allan, Benjamin A. and Dechev, Damian and Brandt, Jim},
title = {Production Application Performance Data Streaming for System Monitoring},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3319498},
doi = {10.1145/3319498},
abstract = {In this article, we present an approach to streaming collection of application performance data. Practical application performance tuning and troubleshooting in production high-performance computing (HPC) environments requires an understanding of how applications interact with the platform, including (but not limited to) parallel programming libraries such as Message Passing Interface (MPI). Several profiling and tracing tools exist that collect heavy runtime data traces either in memory (released only at application exit) or on a file system (imposing an I/O load that may interfere with the performance being measured). Although these approaches are beneficial in development stages and post-run analysis, a systemwide and low-overhead method is required to monitor deployed applications continuously. This method must be able to collect information at both the application and system levels to yield a complete performance picture.In our approach, an application profiler collects application event counters. A sampler uses an efficient inter-process communication method to periodically extract the application counters and stream them into an infrastructure for performance data collection. We implement a tool-set based on our approach and integrate it with the Lightweight Distributed Metric Service (LDMS) system, a monitoring system used on large-scale computational platforms. LDMS provides the infrastructure to create and gather streams of performance data in a low overhead manner. We demonstrate our approach using applications implemented with MPI, as it is one of the most common standards for the development of large-scale scientific applications.We utilize our tool-set to study the impact of our approach on an open source HPC application, Nalu. Our tool-set enables us to efficiently identify patterns in the behavior of the application without source-level knowledge. We leverage LDMS to collect system-level performance data and explore the correlation between the system and application events. Also, we demonstrate how our tool-set can help detect anomalies with a low latency. We run tests on two different architectures: a system enabled with Intel Xeon Phi and another system equipped with Intel Xeon processor. Our overhead study shows our method imposes at most 0.5\% CPU usage overhead on the application in realistic deployment scenarios.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {apr},
articleno = {8},
numpages = {25},
keywords = {Application and system monitoring, application profiling, performance data streaming}
}

@inproceedings{10.1145/3130218.3130225,
author = {Wang, Zicong and Chen, Xiaowen and Li, Chen and Guo, Yang},
title = {Fairness-Oriented and Location-Aware NUCA for Many-Core SoC},
year = {2017},
isbn = {9781450349840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3130218.3130225},
doi = {10.1145/3130218.3130225},
abstract = {Non-uniform cache architecture (NUCA) is often employed to organize the last level cache (LLC) by Networks-on-Chip (NoC). However, along with the scaling up for network size of Systems-on-Chip (SoC), two trends gradually begin to emerge. First, the network latency is becoming the major source of the cache access latency. Second, the communication distance and latency gap between different cores is increasing. Such gap can seriously cause the network latency imbalance problem, aggravate the degree of non-uniform for cache access latencies, and then worsen the system performance.In this paper, we propose a novel NUCA-based scheme, named fairness-oriented and location-aware NUCA (FL-NUCA), to alleviate the network latency imbalance problem and achieve more uniform cache access. We strive to equalize network latencies which are measured by three metrics: average latency (AL), latency standard deviation (LSD), and maximum latency (ML). In FL-NUCA, the memory-to-LLC mapping and links are both non-uniform distributed to better fit the network topology and traffics, thereby equalizing network latencies from two aspects, i.e., non-contention latencies and contention latencies, respectively. The experimental results show that FL-NUCA can effectively improve the fairness of network latencies. Compared with the traditional static NUCA (S-NUCA), in simulation with synthetic traffics, the average improvements for AL, LSD, and ML are 20.9\%, 36.3\%, and 35.0\%, respectively. In simulation with PARSEC benchmarks, the average improvements for AL, LSD, and ML are 6.3\%, 3.6\%, and 11.2\%, respectively.},
booktitle = {Proceedings of the Eleventh IEEE/ACM International Symposium on Networks-on-Chip},
articleno = {13},
numpages = {8},
keywords = {non-uniform cache architecture, memory mapping, Networks-on-chip},
location = {Seoul, Republic of Korea},
series = {NOCS '17}
}

@inproceedings{10.1145/3374664.3375750,
author = {Nguyen, Hoai Viet and Lo Iacono, Luigi},
title = {CREHMA: Cache-Aware REST-Ful HTTP Message Authentication},
year = {2020},
isbn = {9781450371070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374664.3375750},
doi = {10.1145/3374664.3375750},
abstract = {Scalability and security are two important elements of contemporary distributed software systems. The Web vividly shows that while complying with the constraints defined by the architectural style REST, the layered design of software with intermediate systems enables to scale at large. Intermediaries such as caches, however, interfere with the security guarantees of the industry standard for protecting data in transit on the Web, TLS, as in these circumstances the TLS channel already terminates at the intermediate system's server. For more in-depth defense strategies, service providers require message-oriented security means in addition to TLS. These are hardly available and only in the form of HTTP signature schemes that do not take caches into account either. In this paper we introduce CREHMA, a REST-ful HTTP message signature scheme that guarantees the integrity and authenticity of Web assets from end-to-end while simultaneous allowing service providers to enjoy the benefits of Web caches. Decisively, CREHMA achieves these guarantees without having to trust on the integrity of the cache and without requiring making changes to existing Web caching systems. In extensive experiments we evaluated CREHMA and found that it only introduces marginal impacts on metrics such as latency and data expansion while providing integrity protection from end to end. CREHMA thus extends the possibilities of service providers to achieve an appropriate balance between scalability and security.},
booktitle = {Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy},
pages = {49–60},
numpages = {12},
keywords = {rest, web caching, http, end-to-end security, signature},
location = {New Orleans, LA, USA},
series = {CODASPY '20}
}

@inproceedings{10.1145/2568088.2568098,
author = {Ewing, John M. and Menasc\'{e}, Daniel A.},
title = {A Meta-Controller Method for Improving Run-Time Self-Architecting in SOA Systems},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2568098},
doi = {10.1145/2568088.2568098},
abstract = {This paper builds on SASSY, a system for automatically generating SOA software architectures that optimize a given utility function of multiple QoS metrics. In SASSY, SOA software systems are automatically re-architected when services fail or degrade. Optimizing both architecture and service provider selection presents a pair of nested NP-hard problems. Here we adapt hill-climbing, beam search, simulated annealing, and evolutionary programming to both architecture optimization and service provider selection. Each of these techniques has several parameters that influence their efficiency. We introduce in this paper a meta-controller that automates the run-time selection of heuristic search techniques and their parameters. We examine two different meta-controller implementations that each use online learning. The first implementation identifies the best heuristic search combination from various prepared combinations. The second implementation analyzes the current self-architecting problem (e.g. changes in performance metrics, service degradations/failures) and looks for similar, previously encountered re-architecting problems to find an effective heuristic search combination for the current problem. A large set of experiments demonstrates the effectiveness of the first meta-controller implementation and indicates opportunities for improving the second meta-controller implementation.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {173–184},
numpages = {12},
keywords = {combinatorial search techniques, heuristic search, soa, metaheuristics, meta-controlled qos optimization, autonomic computing, automated run-time software architecting},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1145/2661714.2661726,
author = {Stohr, Denny and Wilk, Stefan and Effelsberg, Wolfgang},
title = {Monitoring of User Generated Video Broadcasting Services},
year = {2014},
isbn = {9781450331579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661714.2661726},
doi = {10.1145/2661714.2661726},
abstract = {Mobile video broadcasting services offer users the opportunity to instantly share content from their mobile handhelds to a large audience over the Internet. However, existing data caps in cellular network contracts and limitations in their upload capabilities restrict the adoption of mobile video broadcasting services. Additionally, the quality of those video streams is often reduced by the lack of skills of recording users and the technical limitations of the video capturing devices. Our research focuses on large-scale events that attract dozens of users to record video in parallel. In many cases, available network infrastructure is not capable to upload all video streams in parallel. To make decisions on how to appropriately transmit those video streams, a suitable monitoring of the video generation process is required. For this scenario, a measurement framework is proposed that allows Internet-scale mobile broadcasting services to deliver samples in an optimized way. Our framework architecture analyzes three zones for effectively monitoring user-generated video. Besides classical Quality of Service metrics on the network state, video quality indicators and additional auxiliary sensor information is gathered. Aim of this framework is an efficient coordination of devices and their uploads based on the currently observed system state.},
booktitle = {Proceedings of the First International Workshop on Internet-Scale Multimedia Management},
pages = {39–42},
numpages = {4},
keywords = {video composition, network monitoring, mobile, measurement, mix, cellular networks, video broadcast},
location = {Orlando, Florida, USA},
series = {WISMM '14}
}

@inproceedings{10.5555/2821357.2821365,
author = {Nikravesh, Ali Yadavar and Ajila, Samuel A. and Lung, Chung-Horng},
title = {Towards an Autonomic Auto-Scaling Prediction System for Cloud Resource Provisioning},
year = {2015},
publisher = {IEEE Press},
abstract = {This paper investigates the accuracy of predictive auto-scaling systems in the Infrastructure as a Service (IaaS) layer of cloud computing. The hypothesis in this research is that prediction accuracy of auto-scaling systems can be increased by choosing an appropriate time-series prediction algorithm based on the performance pattern over time. To prove this hypothesis, an experiment has been conducted to compare the accuracy of time-series prediction algorithms for different performance patterns. In the experiment, workload was considered as the performance metric, and Support Vector Machine (SVM) and Neural Networks (NN) were utilized as time-series prediction techniques. In addition, we used Amazon EC2 as the experimental infrastructure and TPC-W as the benchmark to generate different workload patterns. The results of the experiment show that prediction accuracy of SVM and NN depends on the incoming workload pattern of the system under study. Specifically, the results show that SVM has better prediction accuracy in the environments with periodic and growing workload patterns, while NN outperforms SVM in forecasting unpredicted workload pattern. Based on these experimental results, this paper proposes an architecture for a self-adaptive prediction suite using an autonomic system approach. This suite can choose the most suitable prediction technique based on the performance pattern, which leads to more accurate prediction results.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {35–45},
numpages = {11},
keywords = {resource provisioning, auto-scaling, workload pattern, neural networks, cloud computing, support vector machine, autonomic},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/2619239.2631456,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
title = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2631456},
doi = {10.1145/2619239.2631456},
abstract = {After more than two decades of evolution, TCP and its end host based modifications can still suffer from severely degraded performance under real-world challenging network conditions. The reason, as we observe, is due to TCP family's fundamental architectural deficiency, which hardwires packet-level events to control responses and ignores emprical performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented Congestion Control (PCC), a new congestion control architecture in which each sender controls its sending strategy based on empirically observed performance metrics. We show through preliminary experimental results that PCC achieves consistently high performance under various challenging network conditions.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {365–366},
numpages = {2},
keywords = {congestion control},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@article{10.1145/2740070.2631456,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
title = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2631456},
doi = {10.1145/2740070.2631456},
abstract = {After more than two decades of evolution, TCP and its end host based modifications can still suffer from severely degraded performance under real-world challenging network conditions. The reason, as we observe, is due to TCP family's fundamental architectural deficiency, which hardwires packet-level events to control responses and ignores emprical performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented Congestion Control (PCC), a new congestion control architecture in which each sender controls its sending strategy based on empirically observed performance metrics. We show through preliminary experimental results that PCC achieves consistently high performance under various challenging network conditions.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {365–366},
numpages = {2},
keywords = {congestion control}
}

@inproceedings{10.1145/2970276.2970338,
author = {Peldszus, Sven and Kulcs\'{a}r, G\'{e}za and Lochau, Malte and Schulze, Sandro},
title = {Continuous Detection of Design Flaws in Evolving Object-Oriented Programs Using Incremental Multi-Pattern Matching},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970338},
doi = {10.1145/2970276.2970338},
abstract = {Design flaws in object-oriented programs may seriously corrupt code quality thus increasing the risk for introducing subtle errors during software maintenance and evolution. Most recent approaches identify design flaws in an ad-hoc manner, either focusing on software metrics, locally restricted code smells, or on coarse-grained architectural anti-patterns. In this paper, we utilize an abstract program model capturing high-level object-oriented code entities, further augmented with qualitative and quantitative design-related information such as coupling/cohesion. Based on this model, we propose a comprehensive methodology for specifying object-oriented design flaws by means of compound rules integrating code metrics, code smells and anti-patterns in a modular way. This approach allows for efficient, automated design-flaw detection through incremental multi-pattern matching, by facilitating systematic information reuse among multiple detection rules as well as between subsequent detection runs on continuously evolving programs. Our tool implementation comprises well-known anti-patterns for Java programs. The results of our experimental evaluation show high detection precision, scalability to real-size programs, as well as a remarkable gain in efficiency due to information reuse.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {578–589},
numpages = {12},
keywords = {object-oriented software architecture, continuous software evolution, design-flaw detection},
location = {Singapore, Singapore},
series = {ASE '16}
}

@article{10.1145/2975161,
author = {Bouraoui, Hasna and Jerad, Chadlia and Chattopadhyay, Anupam and Hadj-Alouane, Nejib Ben},
title = {Hardware Architectures for Embedded Speaker Recognition Applications: A Survey},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/2975161},
doi = {10.1145/2975161},
abstract = {Authentication technologies based on biometrics, such as speaker recognition, are attracting more and more interest thanks to the elevated level of security offered by these technologies. Despite offering many advantages, such as remote use and low vulnerability, speaker recognition applications are constrained by the heavy computational effort and the hard real-time constraints. When such applications are run on an embedded platform, the problem becomes more challenging, as additional constraints inherent to this specific domain are added. In the literature, different hardware architectures were used/designed for implementing a process with a focus on a given particular metric. In this article, we give a survey of the state-of-the-art works on implementations of embedded speaker recognition applications. Our aim is to provide an overview of the different approaches dealing with acceleration techniques oriented towards speaker and speech recognition applications and attempt to identify the past, current, and future research trends in the area. Indeed, on the one hand, many flexible solutions were implemented, using either General Purpose Processors or Digital Signal Processors. In general, these types of solutions suffer from low area and energy efficiency. On the other hand, high-performance solutions were implemented on Application Specific Integrated Circuits or Field Programmable Gate Arrays but at the expense of flexibility. Based on the available results, we compare the application requirements vis-\`{a}-vis the performance achieved by the systems. This leads to the projection of new research trends that can be undertaken in the future.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {apr},
articleno = {78},
numpages = {28},
keywords = {acceleration, speaker recognition, Embedded hardware, classification algorithms and implementations}
}

@inproceedings{10.1145/2668322.2668327,
author = {Papalambrou, Andreas and Stefanidis, Kyriakos and Gialelis, John and Serpanos, Dimitrios},
title = {Detection, Traceback and Filtering of Denial of Service Attacks in Networked Embedded Systems},
year = {2014},
isbn = {9781450329323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668322.2668327},
doi = {10.1145/2668322.2668327},
abstract = {This work presents a composite scheme for detection, traceback and filtering of distributed denial of service (DDoS) attacks in networked embedded systems. A method based on algorithmic analysis of various node and network parameters is used to detect attacks while a packet marking method is used to mitigate the effects of the attack by filtering the incoming traffic that is part of this attack and trace back to the origin of the attack. The combination of the detection and mitigation methods provide an increased level of security in comparison to approaches based on a single method. Furthermore, the scheme is developed in a way to comply with the novel SHIELD secure architecture being developed, which aims at providing interoperability with other secure components as well as metrics to quantify their security properties.},
booktitle = {Proceedings of the 9th Workshop on Embedded Systems Security},
articleno = {5},
numpages = {8},
keywords = {denial of service attacks, embedded systems security},
location = {New Delhi, India},
series = {WESS '14}
}

@inproceedings{10.1145/3297663.3310307,
author = {van der Sar, Jerom and Donkervliet, Jesse and Iosup, Alexandru},
title = {Yardstick: A Benchmark for Minecraft-like Services},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3310307},
doi = {10.1145/3297663.3310307},
abstract = {Online gaming applications entertain hundreds of millions of daily active players and often feature vastly complex architecture. Among online games, Minecraft-like games simulate unique (e.g., modifiable) environments, are virally popular, and are increasingly provided as a service. However, the performance of Minecraft-like services, and in particular their scalability, is not well understood. Moreover, currently no benchmark exists for Minecraft-like games. Addressing this knowledge gap, in this work we design and use the Yardstick benchmark to analyze the performance of Minecraft-like services. Yardstick is based on an operational model that captures salient characteristics of Minecraft-like services. As input workload, Yardstick captures important features, such as the most-popular maps used within the Minecraft community. Yardstick captures system- and application-level metrics, and derives from them service-level metrics such as frequency of game-updates under scalable workload. We implement Yardstick, and, through real-world experiments in our clusters, we explore the performance and scalability of popular Minecraft-like servers, including the official vanilla server, and the community-developed servers Spigot and Glowstone. Our findings indicate the scalability limits of these servers, that Minecraft-like services are poorly parallelized, and that Glowstone is the least viable option among those tested.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {243–253},
numpages = {11},
keywords = {minecraft, as a service, distributed systems, online gaming, yardstick, benchmark},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/2747470.2747471,
author = {Dudouet, Florian and Edmonds, Andrew and Erne, Michael},
title = {Reliable Cloud-Applications: An Implementation through Service Orchestration},
year = {2015},
isbn = {9781450334761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2747470.2747471},
doi = {10.1145/2747470.2747471},
abstract = {As cloud-deployed applications became more and more mainstream, continuously more complex services started to be deployed; indeed where initially monolithic applications were simply ported to the cloud, applications are now more and more often composed of micro-services. This improves the flexibility of an application but also makes it more complex due to the sheer number of services comprising it.As deployment and runtime management becomes more complex, orchestration software are becoming necessary to completely manage the lifecycle of cloud applications. One crucial problem remaining is how these applications can be made reliable in the cloud, a naturally unreliable environment.In this paper we propose concepts and architectures which were implemented in our orchestration software to guarantee reliability. Our initial implementation also relies on Monasca, a well-known monitoring software for Open-Stack, to gather proper metric and execute threshold-based actions. This allows us to show how service reliability can be ensured using orchestration and how a proper incident-management software feeding decisions to the orchestration engine ensures high-availability of all components of managed applications.},
booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
pages = {1–6},
numpages = {6},
keywords = {incident management, orchestration, cloud computing, reliability},
location = {Bordeaux, France},
series = {AIMC '15}
}

@inproceedings{10.1145/3377813.3381349,
author = {Diamantopoulos, Nikos and Wong, Jeffrey and Mattos, David Issa and Gerostathopoulos, Ilias and Wardrop, Matthew and Mao, Tobias and McFarland, Colin},
title = {Engineering for a Science-Centric Experimentation Platform},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381349},
doi = {10.1145/3377813.3381349},
abstract = {Netflix is an internet entertainment service that routinely employs experimentation to guide strategy around product innovations. As Netflix grew, it had the opportunity to explore increasingly specialized improvements to its service, which generated demand for deeper analyses supported by richer metrics and powered by more diverse statistical methodologies. To facilitate this, and more fully harness the skill sets of both engineering and data science, Netflix engineers created a science-centric experimentation platform that leverages the expertise of scientists from a wide range of backgrounds working on data science tasks by allowing them to make direct code contributions in the languages used by them (Python and R). Moreover, the same code that runs in production is able to be run locally, making it straightforward to explore and graduate both metrics and causal inference methodologies directly into production services.In this paper, we provide two main contributions. Firstly, we report on the architecture of this platform, with a special emphasis on its novel aspects: how it supports science-centric end-to-end workflows without compromising engineering requirements. Secondly, we describe its approach to causal inference, which leverages the potential outcomes conceptual framework to provide a unified abstraction layer for arbitrary statistical models and methodologies.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {191–200},
numpages = {10},
keywords = {software architecture, causal inference, science-centric, experimentation, A/B testing},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@article{10.1145/3233182,
author = {Ji, Kecheng and Ling, Ming and Shi, Longxing and Pan, Jianping},
title = {An Analytical Cache Performance Evaluation Framework for Embedded Out-of-Order Processors Using Software Characteristics},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3233182},
doi = {10.1145/3233182},
abstract = {Utilizing analytical models to evaluate proposals or provide guidance in high-level architecture decisions is been becoming more and more attractive. A certain number of methods have emerged regarding cache behaviors and quantified insights in the last decade, such as the stack distance theory and the memory level parallelism (MLP) estimations. However, prior research normally oversimplified the factors that need to be considered in out-of-order processors, such as the effects triggered by reordered memory instructions, and multiple dependences among memory instructions, along with the merged accesses in the same MSHR entry. These ignored influences actually result in low and unstable precisions of recent analytical models.By quantifying the aforementioned effects, this article proposes a cache performance evaluation framework equipped with three analytical models, which can more accurately predict cache misses, MLPs, and the average cache miss service time, respectively. Similar to prior studies, these analytical models are all fed with profiled software characteristics in which case the architecture evaluation process can be accelerated significantly when compared with cycle-accurate simulations.We evaluate the accuracy of proposed models compared with gem5 cycle-accurate simulations with 16 benchmarks chosen from Mobybench Suite 2.0, Mibench 1.0, and Mediabench II. The average root mean square errors for predicting cache misses, MLPs, and the average cache miss service time are around 4\%, 5\%, and 8\%, respectively. Meanwhile, the average error of predicting the stall time due to cache misses by our framework is as low as 8\%. The whole cache performance estimation can be sped by about 15 times versus gem5 cycle-accurate simulations and 4 times when compared with recent studies. Furthermore, we have shown and studied the insights between different performance metrics and the reorder buffer sizes by using our models. As an application case of the framework, we also demonstrate how to use our framework combined with McPAT to find out Pareto optimal configurations for cache design space explorations.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {aug},
articleno = {79},
numpages = {25},
keywords = {Analytical models, cache misses, cache miss service time, software characteristics, memory level parallelism}
}

@inproceedings{10.1145/3452296.3472922,
author = {Fayed, Marwan and Bauer, Lorenz and Giotsas, Vasileios and Kerola, Sami and Majkowski, Marek and Odintsov, Pavel and Sitnicki, Jakub and Chung, Taejoong and Levin, Dave and Mislove, Alan and Wood, Christopher A. and Sullivan, Nick},
title = {The Ties That Un-Bind: Decoupling IP from Web Services and Sockets for Robust Addressing Agility at CDN-Scale},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472922},
doi = {10.1145/3452296.3472922},
abstract = {The couplings between IP addresses, names of content or services, and socket interfaces, are too tight. This impedes system manageability, growth, and overall provisioning. In turn, large-scale content providers are forced to use staggering numbers of addresses, ultimately leading to address exhaustion (IPv4) and inefficiency (IPv6).In this paper, we revisit IP bindings, entirely. We attempt to evolve addressing conventions by decoupling IP in DNS and from network sockets. Alongside technologies such as SNI and ECMP, a new architecture emerges that ``unbinds'' IP from services and servers, thereby returning IP's role to merely that of reachability. The architecture is under evaluation at a major CDN in multiple datacenters. We show that addresses can be generated randomly emph{per-query}, for 20M+ domains and services, from as few as ~4K addresses, 256 addresses, and even emph{one} IP address. We explain why this approach is transparent to routing, L4/L7 load-balancers, distributed caching, and all surrounding systems -- and is emph{highly desirable}. Our experience suggests that many network-oriented systems and services (e.g., route leak mitigation, denial of service, measurement) could be improved, and new ones designed, if built with addressing agility.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {433–446},
numpages = {14},
keywords = {provisioning, programmable sockets, addressing, content distribution},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up Architectural SW Health Builds in a New Product Line Generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start from scratch but takes into consideration the architecture and assets of the former product line generation. Being able to accommodate legacy and 3rd party code is one of the major product line qualities to be met. On the other side, product line qualities like reusability, maintainability and alterability, i.e. being able to cope up with a large amount of variability, with configurability and fast integratability are major drivers.While setting up a new product line generation and thus a new corresponding architecture, we this time focused on architectural software (SW) health and tracking of architectural metrics from the very beginning. Taking the definition of "architecture being a set of design decisions" [18] literally, we attempt to implement an architectural check for every design decision taken. Architectural design decisions in our understanding do not only - and even not mainly - deal with the definition of components and their interaction but with patterns and rules or anti-patterns. The rules and anti-patterns, "what not to do" or more often also "what not to do &lt;u&gt;any more&lt;/u&gt;", is even more important in setting up a new product line generation because developers are not only used to the old style of developing and the old architecture, but also still have to develop assets for both generations.In this article we describe selected architectural checks that we have implemented, the layered architecture check and the check for usage of obsolete services. Additionally we discuss selected architectural metrics: the coupling coefficient metrics and the instability metrics. In the summary and outlook we describe our experiences and still open topics in setting up architectural SW health checks for a large-scale product line.The real-world examples are taken from the domain of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {embedded software, architectural checks, technical debt, software architecture, software erosion, architectural technical debt, product line development},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.1109/ICSE-NIER.2019.00037,
author = {Aniche, Maur\'{\i}cio and Yoder, Joseph W. and Kon, Fabio},
title = {Current Challenges in Practical Object-Oriented Software Design},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00037},
doi = {10.1109/ICSE-NIER.2019.00037},
abstract = {According to the extensive 50-year-old body of knowledge in object-oriented programming and design, good software designs are, among other characteristics, lowly coupled, highly cohesive, extensible, comprehensible, and not fragile. However, with the increased complexity and heterogeneity of contemporary software, this might not be enough.This paper discusses the practical challenges of object-oriented design in modern software development. We focus on three main challenges: (1) how technologies, frameworks, and architectures pressure developers to make design decisions that they would not take in an ideal scenario, (2) the complexity of current real-world problems require developers to devise not only a single, but several models for the same problem that live and interact together, and (3) how existing quality assessment techniques for object-oriented design should go beyond high-level metrics.Finally, we propose an agenda for future research that should be tackled by both scientists and practitioners soon. This paper is a call for arms for more reality-oriented research on the object-oriented software design field.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {113–116},
numpages = {4},
keywords = {software architecture, domain modeling, software design, object-oriented design, class design, software engineering, object-oriented programming},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/3238147.3240463,
author = {Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin},
title = {Achieving Test Automation with Testers without Coding Skills: An Industrial Report},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240463},
doi = {10.1145/3238147.3240463},
abstract = {We present a process driven test automation solution which enables delegating (part of) automation tasks from test automation engineer (expensive resource) to test analyst (non-developer, less expensive). In our approach, a test automation engineer implements test steps (or actions) which are executed automatically. Such automated test steps represent user actions in the system under test and specified by a natural language which is understandable by a non-technical person. Then, a test analyst with a domain knowledge organizes automated steps combined with test input to create an automated test case. It should be emphasized that the test analyst does not need to possess programming skills to create, modify or execute automated test cases. We refine benchmark test automation architecture to be better suitable for an effective separation and sharing of responsibilities between the test automation engineer (with coding skills) and test analyst (with a domain knowledge). In addition, we propose a metric to empirically estimate cooperation between test automation engineer and test analyst's works. The proposed automation solution has been defined based on our experience in the development and maintenance of Helsenorg, the national electronic health services in Norway which has had over one million of visits per month past year, and we still use it to automate the execution of regression tests.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {749–756},
numpages = {8},
keywords = {process-driven test automation, DSL for test automation, keyword-driven test automation, Helsenorge, Test automation},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3343031.3350592,
author = {Galteri, Leonardo and Seidenari, Lorenzo and Bertini, Marco and Uricchio, Tiberio and Del Bimbo, Alberto},
title = {Fast Video Quality Enhancement Using GANs},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350592},
doi = {10.1145/3343031.3350592},
abstract = {Video compression algorithms result in a reduction of image quality, because of their lossy approach to reduce the required bandwidth. This affects commercial streaming services such as Netflix, or Amazon Prime Video, but affects also video conferencing and video surveillance systems. In all these cases it is possible to improve the video quality, both for human view and for automatic video analysis, without changing the compression pipeline, through a post-processing that eliminates the visual artifacts created by the compression algorithms. Generative Adversarial Networks have obtained extremely high quality results in image enhancement tasks; however, to obtain such results large generators are usually employed, resulting in high computational costs and processing time. In this work we present an architecture that can be used to reduce the computational cost and that has been implemented on mobile devices. A possible application is to improve video conferencing, or live streaming. In these cases there is no original uncompressed video stream available. Therefore, we report results using no-reference video quality metric showing high naturalness and quality even for efficient networks.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1065–1067},
numpages = {3},
keywords = {video compression, real-time enhancement, video streaming, video quality enhancement, gans},
location = {Nice, France},
series = {MM '19}
}

@article{10.1145/3308897.3308943,
author = {Luong, Doanh Kim and Ali, Muhammad and Benamrane, Fouad and Ammar, Ibrahim and Hu, Yim-Fun},
title = {Seamless Handover for Video Streaming over an SDN-Based Aeronautical Communications Network},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3308897.3308943},
doi = {10.1145/3308897.3308943},
abstract = {There have been increasing interests in applying Software Defined Networking (SDN) to aeronautical communications primarily for air traffic management purposes. From the service passenger communications' point of view, a major goal is to improve passengers' perception of quality of experience on the infotainment services being provided for them. Due to the high speed of aircrafts and the use of multiple radio technologies during different flight phases and across different areas, vertical handovers between these different radio technologies are envisaged. This poses a challenge to maintain the quality of service during such handovers, especially for high bandwidth applications such as video streaming. This paper proposes an SDN-based aeronautical communications architecture consisting of both satellite and terrestrial-based radio technology. In addition, an experimental implementation of the Locator ID Separation Protocol (LISP) protocol with built-in multi-homing capability over the SDN-based architecture was proposed to handle vertical handovers between the satellite and other radio technologies onboard the aircraft. By using both objective and subjective Quality of Experience (QoE) metrics, the simulation experiments show the benefit of combining LISP with SDN to improve the video streaming quality during the handover in the aeronautical communication environment.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {98–99},
numpages = {2},
keywords = {sdn, aeronautical communications, lisp mobility, vertical handovers, multi-homing}
}

@inproceedings{10.1145/2815317.2815321,
author = {da Silva, Madalena P. and Dantas, Mario A.R. and Gon\c{c}alves, Alexandre L. and Pinto, Alex R.},
title = {A Managing QoE Approach for Provisioning User Experience Aware Services Using SDN},
year = {2015},
isbn = {9781450337571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815317.2815321},
doi = {10.1145/2815317.2815321},
abstract = {Provision and delivery of services with quality is a classic research problem, however the computational resources available in the network infrastructure of providers are, usually, managed with conventional Quality of Service (QoS) parameters. This paper presents an approach of Quality of Experience (QoE) management for providing services aware of the user experience. QoE modeling and architecture are proposed, with a semantic engine able to learn the user's experience during the use of a service, detecting violations of QoS metrics and providing information, allowing the controller to perform actions in the elements of the Software Defined Networking. The experimental results demonstrate that the proposal is feasible and functional and that the time spent between QoE detection and adaptation of policies in network resources do not influence the quality perceived by the user.},
booktitle = {Proceedings of the 11th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {51–58},
numpages = {8},
keywords = {semantic engine, software-defined network, quality of service, quality of experience},
location = {Cancun, Mexico},
series = {Q2SWinet '15}
}

@inproceedings{10.1145/3364641.3364680,
author = {Couto, Christian Marlon Souza and Terra, Ricardo},
title = {A Quality-Oriented Approach to Recommend Move Method Refactorings},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364680},
doi = {10.1145/3364641.3364680},
abstract = {Refactoring processes are common in large software systems, especially when developers neglect architectural erosion process for long periods. Even though there are many refactoring approaches, very few consider the refactoring impact on the software quality.Given this scenario, we propose a refactoring approach to software systems oriented to software quality metrics. Based on the QMOOD (Quality Model for Object Oriented Design), the main idea is to move methods between classes in order to maximize the values of the quality metrics. Using a formal notation, we describe the problem as follows. Given a software system S, our approach recommends a sequence of refactorings R1, R2,..., Rn that result in system versions S1, S2,..., Sn, where quality(Si+1) &gt; quality(Si).We performed three types of evaluation to verify the usefulness of our implemented tool, called QMove. First, we applied our approach on 13 open-source systems that we modified by randomly moving a subset of its methods to other classes, then checking if our approach would recommend the moved methods to return to their original place, and we achieve 84\% recall, on average. Second, we compared QMove against two state-of-art refactoring tools (JMove and JDeodorant) on the 13 previously evaluated systems, and QMove showed better recall value (84\%) than the other two (30\% and 29\%, respectively). Third, we conducted the same comparison among QMove, JMove, and JDeodorant applied in two proprietary systems where experts evaluated the quality of the recommendations. QMove obtained eight positively evaluated recommendations from the experts, against two and none of JMove and JDeodorant, respectively.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {315},
numpages = {1},
keywords = {software architecture, refactoring, quality metrics},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.1145/3386367.3431306,
author = {Marques, Jonatas and Levchenko, Kirill and Gaspary, Luciano},
title = {IntSight: Diagnosing SLO Violations with in-Band Network Telemetry},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431306},
doi = {10.1145/3386367.3431306},
abstract = {Performance requirements for many of today's high-perfor-mance networks are expressed as service-level objectives (SLOs), i.e., precise guarantees, typically on latency and bandwidth, that a user can expect from the network. For network operators, monitoring their own SLO compliance, and quickly diagnosing any violations, is a critical element for effective operations. Unfortunately, existing network architectures are not engineered for this purpose; there is no mechanism, for example, for the operator to monitor the 95th per-centile latency experienced by a customer. Data plane programmability has made per-packet measurements possible but brings the challenge of keeping the monitoring overhead low and practical. In this paper, we present IntSight, a system for highly accurate and fine-grained detection and diagnosis of SLO violations. The main contribution of IntSight is, building upon in-band telemetry, introducing path-wise computation of network metrics and selective generation of reports. We show the effectiveness of IntSight by way of two use cases. Our evaluation using real networks also shows that IntSight generates up to two orders of magnitude less monitoring traffic than state-of-the-art approaches. Furthermore, its processing and memory requirements are low and therefore compatible with currently existing programmable platforms.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {421–434},
numpages = {14},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@inproceedings{10.1145/3437120.3437292,
author = {Maikantis, Theodoros and Tsintzira, Angeliki-Agathi and Ampatzoglou, Apostolos and Arvanitou, Elvira-Maria and Chatzigeorgiou, Alexander and Stamelos, Ioannis and Bibi, Stamatia and Deligiannis, Ignatios},
title = {Software Architecture Reconstruction via a Genetic Algorithm: Applying the Move Class Refactoring},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437292},
doi = {10.1145/3437120.3437292},
abstract = {Modularity is one of the four key principles of software design and architecture. According to this principle, software should be organized into modules that are tightly linked internally (high cohesion), whereas at the same time as independent from other modules as possible (low coupling). However, in practice, this principle is violated due to poor architecting design decisions, lack of time, or coding shortcuts, leading to a phenomenon termed as architectural technical debt (ATD). To alleviate this problem (lack of architectural modularity), the most common solution is the application of a software refactoring, namely Move Class—i.e., moving classes (the core artifact in object-oriented systems) from one module to another. To identify Move Class refactoring opportunities, we employ a search-based optimization process, relying on optimization metrics, through which optimal moves are derived. Given the extensive search space required for applying a brute-force search strategy, in this paper, we propose the use of a genetic algorithm that re-arranges existing software classes into existing or new modules (software packages in Java, or folders in C++). To validate the usefulness of the proposed refactorings, we performed an industrial case study on three projects (from the Aviation, Healthcare, and Manufacturing application domains). The results of the study indicate that the proposed architecture reconstruction is able to improve modularity, improving both coupling and cohesion. The obtained results can be useful to practitioners through an open source tool; whereas at the same point, they open interesting future work directions.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {135–139},
numpages = {5},
location = {Athens, Greece},
series = {PCI '20}
}

@inproceedings{10.1145/2841113.2841114,
author = {Forget, Alain and Chiasson, Sonia and Biddle, Robert},
title = {Choose Your Own Authentication},
year = {2015},
isbn = {9781450337540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2841113.2841114},
doi = {10.1145/2841113.2841114},
abstract = {To solve the long-standing problems users have in creating and remembering text passwords, a wide variety of alternative authentication schemes have been proposed. Some of these schemes outperform others by various metrics in various contexts. However, none unilaterally outperform all others, and so text passwords persist as the main scheme applications depend upon. In this paper, we challenge the long-standing assumption that only one authentication scheme can be offered by an application service. We propose Choose Your Own Authentication (CYOA): a novel authentication architecture that enables users to choose a scheme amongst several available alternatives. CYOA would enable users to select whichever scheme best suits their preferences, abilities, and usage context. Existing text password systems could easily be replaced. Furthermore, the three-party architecture would enable delegating the management of authentication systems to trusted-third parties. The architecture allows rapid deployment and testing of novel authentication technologies. Our two-week usability study suggests that participants were willing to leverage alternative schemes. Participants were confident that CYOA could keep their financial information secure.},
booktitle = {Proceedings of the 2015 New Security Paradigms Workshop},
pages = {1–15},
numpages = {15},
keywords = {survey, user study, Authentication, usable security},
location = {Twente, Netherlands},
series = {NSPW '15}
}

@inproceedings{10.1145/3291533.3291540,
author = {Dalgkitsis, Anestis and Louta, Malamati and Karetsos, George T.},
title = {Traffic Forecasting in Cellular Networks Using the LSTM RNN},
year = {2018},
isbn = {9781450366106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291533.3291540},
doi = {10.1145/3291533.3291540},
abstract = {In this work we design and implement a Neural Network that can identify recurrent patterns in various metrics which can be then used for cellular network traffic forecasting. Based on a custom architecture and memory, this Neural Network can handle prediction tasks faster and more accurately in real life scenarios. This approach offers a solution for service providers to enhance cellular network performance, by utilizing effectively the available resources. In order to provide a robust conclusion about the performance and precision of the proposed Neural Network, multiple predictions were made using the same data-set and the results were compared against other similar algorithms from the literature.},
booktitle = {Proceedings of the 22nd Pan-Hellenic Conference on Informatics},
pages = {28–33},
numpages = {6},
keywords = {recurrent neural networks, cellular networks, traffic forecasting, long-short term memory},
location = {Athens, Greece},
series = {PCI '18}
}

@inproceedings{10.1145/3323716.3323729,
author = {Berba, Elizalde M. and Palaoag, Thelma D.},
title = {Improving Customer Satisfaction on Internet Services in L-NU Using Virtualized AAA Network Architecture},
year = {2019},
isbn = {9781450361040},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323716.3323729},
doi = {10.1145/3323716.3323729},
abstract = {This study mainly aims to improve the satisfaction level on internet services in Lyceum-Northwestern University (L-NU). From a traditional network architecture, the researchers made use of a virtualized Authentication, Authorization and Accounting (AAA) network architecture to improve the internet services provided to the students of L-NU. For the methodology of this study, the researchers had to make use of a network lifecycle called Prepare, Plan, Design, Implement, Operate, and Optimize (PPDIOO) and there was also a need to combine both quantitative and qualitative research approach. To make this happen, the researchers had to fulfill the following objectives: a) determine the current network setup of L-NU, b) measure the current satisfaction level of the users, c) design, develop and implement a virtualized AAA network architecture, d) measure the satisfaction level of the users who have used the AAA network setup, and e) compare the measured satisfaction level from the users who used the internet facilities using the traditional network architecture and satisfaction level from users who have used the internet facilities after the implementation of AAA network architecture. As a result, it was found out that the implementation of the new network architecture has significantly improved the internet service level of L-NU which is reflected by a higher customer satisfaction rating. Therefore, the researchers conclude that it is most essential that AAA network architecture be implemented to enterprise type of network setup such as but not limited to education institutions in managing their internet services. Consequently, this kind of network architecture lead to a more effective and more efficient way of managing network resources of an institution or an organization while further improving the satisfaction level. In order to optimize the AAA network architecture and gain more implementation advantages, virtualization technology was used to contain and run numerous operating system instances such as four physical servers into one single physical server which favors to saving resources such as energy, space, money and of which also leads to simplified administration.},
booktitle = {Proceedings of the 8th International Conference on Informatics, Environment, Energy and Applications},
pages = {178–183},
numpages = {6},
keywords = {authorization, hypervisor, AAA, virtualization, accounting, PPDIOO, authentication, customer satisfaction},
location = {Osaka, Japan},
series = {IEEA '19}
}

@article{10.1109/TASLP.2019.2915922,
author = {Xu, Zhen and Sun, Chengjie and Long, Yinong and Liu, Bingquan and Wang, Baoxun and Wang, Mingjiang and Zhang, Min and Wang, Xiaolong},
title = {Dynamic Working Memory for Context-Aware Response Generation},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2915922},
doi = {10.1109/TASLP.2019.2915922},
abstract = {In human-to-human conversations, the context generally provides several backgrounds and strategic points for the following response. Therefore, many response generation approaches have explored the methodologies to incorporate the context into the encoder–decoder architecture, to generate context-aware responses that are remarkably relevant and cohesive to the given context. However, most approaches pay less attention to semantic interactions implicitly existing within contextual utterances, which are of great importance to capture semantic clues of the given dialog context, indeed. This paper proposes a dynamic working memory mechanism to model long-term semantic hints in the conversation context, by performing semantic interactions between utterances and updating context representation dynamically. Then, the outputs of the dynamic working memory are employed to provide helpful clues for the encoder–decoder architecture to generate responses to the given dialog. We have evaluated the proposed approach on Twitter Customer Service Corpus and OpenSubtitles Corpus, with several automatic evaluation metrics and the human evaluation, and the empirical results show the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {1419–1431},
numpages = {13}
}

@article{10.1145/3377138,
author = {Wu, Hao and Liu, Weizhi and Lin, Huanxin and Wang, Cho-Li},
title = {A Model-Based Software Solution for Simultaneous Multiple Kernels on GPUs},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3377138},
doi = {10.1145/3377138},
abstract = {As a critical computing resource in multiuser systems such as supercomputers, data centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking is an intuitive solution to underutilization in GPGPU computing. Recently proposed solutions of multitasking GPUs can be classified into two categories: (1) spatially partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels simultaneously within a CU. Compared to SPS, SMK can improve resource utilization even further due to the interleaving of instructions from kernels with low dynamic resource contentions.However, it is hard to implement SMK on current GPU architecture, because (1) techniques for applying SMK on top of GPU hardware scheduling policy are scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences of concurrently executed kernels. In this article, we propose a lightweight and effective performance model to evaluate the complex interferences of SMK. Based on the probability of independent events, our performance model is built from a totally new angle and contains limited parameters. Then, we propose a metric, symbiotic factor, which can evaluate an SMK scheme so that kernels with complementary resource utilization can corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing and kernel stretching techniques and integrate them to apply SMK on GPUs instead of simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based concurrent kernel execution whose kernel launching order brings fast execution time, the results of corunning kernel pairs show 11\%, 18\%, and 12\% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the results show 29\%, 18\%, and 51\% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average.},
journal = {ACM Trans. Archit. Code Optim.},
month = {mar},
articleno = {7},
numpages = {26},
keywords = {concurrent kernel execution, GPGPU}
}

@inproceedings{10.1145/3459104.3459148,
author = {Pradhan, Ayush and Joy, Eldhose and Jawagal, Harsha and Prasad Jayaraman, Sundar},
title = {A Framework for Leveraging Contextual Information in Automated Domain Specific Comprehension},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459148},
doi = {10.1145/3459104.3459148},
abstract = {When it comes to information, Enterprises today are seen as a black hole, a mass of it goes in but gets difficult to extract the practical knowledge out of it. An automated system that has the ability to consume this large mass of information and provide specific, knowledgeable, domain-oriented responses back, will go a long way in unlocking the value of this large-scale unstructured information. In a bid to enrich the answering system's accuracy in Machine Reading Comprehension (MRC), we propose a domain-specific Question Answers (QuAns) framework that specifically aims to auto-generate questions from a domain-based document using an improvised Sequence to Sequence (Seq2Seq) technique equipped with Attention and Copy mechanism. The generated questions are conditioned on a set of candidate answers, derived using a combination of heuristic-driven and graph-based techniques. Further, it also leverages the contextual information by pooling strategy to build an automated response system using a deep custom fine-tuned Bidirectional Encoder Representations from Transformers (BERT) framework and retrieving the top-k contexts for a user query. The evaluation of the QuAns architecture is performed in combination with human supervision as at times, the automated metrics like BLEU, Exact Match (EM), F1 score, etc. fail to gauge the diverse semantic and structural aspects of a generated response. Primarily, the proffered ensemble technique has leveraged the augmented domain knowledge to enrich the answering response efficacy and improving the EM and F1 score by 14.86\% and 12.76\% respectively over Vanilla BERT architecture. To enhance the user experience, the conversational system is equipped with Natural Language Generation (NLG) to present a human-readable response. Our architectural pipeline aims to provide a one-stop solution for the organizations in processing huge volumes of multidisciplinary data by significantly reducing the human introspection and the overhead cost.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {263–270},
numpages = {8},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3422392.3422501,
author = {Barros, Daniel D. R. and Horita, Fl\'{a}vio and Fantinato, Denis G.},
title = {Data Mining Tool to Discover DevOps Trends from Public Repositories: Predicting Release Candidates with Gthbmining.Rc},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422501},
doi = {10.1145/3422392.3422501},
abstract = {Public repositories have been performing an essential role in bringing software and services to technical communities and general users. Most of the cases, public repositories have a DevOps tool, with a live and historical database behind it, to support delivering and all steps this software or service should adopt before going to production. This paper introduces gthbmining, a data mining set of tools to discover DevOps trends from public repositories, and presents the module gthbmining.rc. Considering the premise of a GitHub public repository, the main contribution here is predicting release candidates, an important label a software release has. The methodology, architecture, components and interfaces are explained, as well as potential users. The results show a reliable and flexible tool, as classifiers metrics and graphics are provided, along with the possibility to add new data mining algorithms in the open source module presented. Related works are also supplied, and a conclusion shows the outcomes gthbmining.rc can provide.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {658–663},
numpages = {6},
keywords = {Data Mining, Release Candidate, DevOps, GitHub Mining Tool},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2851613.2851874,
author = {Abderrahim, Wiem and Choukair, Zied},
title = {PaaS Dependability Integration Architecture Based on Cloud Brokering},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851874},
doi = {10.1145/2851613.2851874},
abstract = {Cloud computing has revolutionized the way IT is provisioned nowadays since it exposes computing capabilities as rental resources to consumers. The emergence of cloud computing services hasn't though prevented outages in these environments even among high profile ranked cloud providers. Tremendous efforts concentrated on fault management measures have been applied for these environments. But they have been focused mainly on the IaaS service model and have been operated on the cloud provider side alone. In this context, this paper proposes an architecture for cloud brokering that implements dependability properties in an end to end way involving different cloud actors and all over the cloud service models SaaS, PaaS and IaaS.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {484–487},
numpages = {4},
keywords = {cloud provider, fault tolerance, fault forecasting, PaaS, IaaS, cloud broker, SaaS, fault management, dependability},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2856636.2876471,
author = {Zodik, Gabi},
title = {Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2876471},
doi = {10.1145/2856636.2876471},
abstract = {The second wave of change presented by the age of mobility, wearables, and IoT focuses on how organizations and enterprises, from a wide variety of commercial areas and industries, will use and leverage the new technologies available. Businesses and industries that don't change with the times will simply cease to exist.Applications need to be powered by cognitive and contextual technologies to support real-time proactive decisions. These decisions will be based on the mobile context of a specific user or group of users, incorporating location, time of day, current user task, and more. Driven by the huge amounts of data produced by mobile and wearables devices, and influenced by privacy concerns, the next wave in computing will need to exploit data and computing at the edge of the network. Future mobile apps will have to be cognitive to 'understand' user intentions based on all the available interactions and unstructured data.Mobile applications are becoming increasingly ubiquitous, going beyond what end users can easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business (B2B) apps, only about 30\% of the development efforts appear in the interface of the mobile app. For example, areas such as the collaborative nature of the software or the shortened development cycle and time-to-market are not apparent to end users. The other 70\% of the effort invested is dedicated to integrating the applications with back-office systems and developing those aspects of the application that operate behind the scenes.An important, yet often complex, part of the solution and mobile app takes place far from the public eye-in the back-office environment. It is there that various aspects of customer relationship management must be addressed: tracking usage data, pushing out messaging as needed, distributing apps to employees within the enterprise, and handling the wide variety of operational and management tasks-often involving the collection and monitoring of data from sensors and wearable devices. All this must be carried out while addressing security concerns that range from verifying user identities, to data protection, to blocking attempted breaches of the organization, and activation of malicious code. Of course, these tasks must be augmented by a systematic approach and vigilant maintenance of user privacy.The first wave of the mobile revolution focused on development platforms, run-time platforms, deployment, activation, and management tools for multi-platform environments, including comprehensive mobile device management (MDM). To realize the full potential of this revolution, we must capitalize on information about the context within which mobile devices are used. With both employees and customers, this context could be a simple piece of information such as the user location or time of use, the hour of the day, or the day of the week. The context could also be represented by more complex data, such as the amount of time used, type of activity performed, or user preferences. Further insight could include the relationship history with the user and the user's behavior as part of that relationship, as well as a long list of variables to be considered in various scenarios. Today, with the new wave of wearables, the definition of context is being further extended to include environmental factors such as temperature, weather, or pollution, as well as personal factors such as heart rate, movement, or even clothing worn.In both B2E and B2C situations, a context-dependent approach, based on the appropriate context for each specific user, offers a superior tool for working with both employees and clients alike. This mode of operation does not start and end with the individual user. Rather, it takes into account the people surrounding the user, the events taking place nearby, appliances or equipment activated, the user's daily schedule, as well as other, more general information, such as the environment and weather.Developing enterprise-wide, context-dependent, mobile solutions is still a complex challenge. A system of real added-value services must be developed, as well as a comprehensive architecture. These four-tier architectures comprise end-user devices like wearables and smartphones, connected to systems of engagement (SoEs), and systems of record (SoRs). All this is needed to enable data analytics and collection in the context where it is created. The data collected will allow further interaction with employees or customers, analytics, and follow-up actions based on the results of that analysis. We also need to ensure end-to-end (E2E) security across these four tiers, and to keep the data and application contexts in sync. These are just some of the challenges being addressed by IBM Research.As an example, these technologies could be deployed in the retail space, especially in brick-and-mortar stores. Identifying a customer entering a store, detecting her location among the aisles, and cross-referencing that data with the customer's transaction history, could lead to special offers tailor-made for that specific customer or suggestions relevant to her purchasing process. This technology enables real-world implementation of metrics, analytics, and other tools familiar to us from the online realm. We can now measure visits to physical stores in the same way we measure web page hits: analyze time spent in the store, the areas visited by the customer, and the results of those visits. In this way, we can also identify shoppers wandering around the store and understand when they are having trouble finding the product they want to purchase. We can also gain insight into the standard traffic patterns of shoppers and how they navigate a store's floors and departments. We might even consider redesigning the store layout to take advantage of this insight to enhance sales.In healthcare, the context can refer to insight extracted from data received from sensors on the patient, from either his mobile device or wearable technology, and information about the patient's environment and location at that moment in time. This data can help determine if any assistance is required. For example, if a patient is discharged from the hospital for continued at-home care, doctors can continue to remotely monitor his condition via a system of sensors and analytic tools that interpret the sensor readings.This approach can also be applied to the area of safety. Scientists at IBM Research are developing a platform that collects and analyzes data from wearable technology to protect the safety of employees working in construction, heavy industry, manufacturing, or out in the field. This solution can serve as a real-time warning system by analyzing information gathered from wearable sensors embedded in personal protective equipment, such as smart safety helmets and protective vests, and in the workers' individual smartphones. These sensors can continuously monitor a worker's pulse rate, movements, body temperature, and hydration level, as well as environmental factors such as noise level, and other parameters. The system can provide immediate alerts to the worker about any dangers in the work environment to prevent possible injury. It can also be used to prevent accidents before they happen or detect accidents once they occur. For example, with sophisticated algorithms, we can detect if a worker falls based on a sudden difference in elevations detected by an accelerometer, and then send an alert to notify her peers and supervisor or call for help. Monitoring can also help ensure safety in areas where continuous exposure to heat or dangerous materials must be limited based on regulated time periods.Mobile technologies can also help manage events with massive numbers of participants, such as professional soccer games, music festivals, and even large-scale public demonstrations, by sending alerts concerning long and growing lines or specific high-traffic areas. These technologies can be used to detect accidents typical of large-scale gatherings, send warnings about overcrowding, and alert the event organizers. In the same way, they can alleviate parking problems or guide public transportation operators- all via analysis and predictive analytics.IBM Research - Haifa is currently involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers have a special expertise in time- and location-based intelligent applications, including visual maps that display activity contexts and predictive analytics systems for mobile data and users. In another area, IBM researchers in Haifa are developing new cognitive services driven from the unique data available on mobile and wearable devices. Looking to the future, the IBM Research team is further advancing the integration of wearable technology, augmented reality systems, and biometric tools for mobile user identity validation.Managing contextual data and analyzing the interaction between the different kinds of data presents fascinating challenges for the development of next-generation programming. For example, we need to rethink when and where data processing and computations should occur: Is it best to leave them at the user-device level, or perhaps they should be moved to the back-office systems, servers, and/or the cloud infrastructures with which the user device is connected? New-age applications are becoming more and more distributed. They operate on a wide range of devices, such as wearable technologies, use a variety of sensors, and depend on cloud-based systems.As a result, a new distributed programming paradigm is emerging to meet the needs of these use-cases and real-time scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and data in business systems, and must be able to shift computation from the cloud to the edge, based on context in close to real-time. By processing data at the edge of the network, close to where the interactions and processing are happening, we can help reduce latency and offer new opportunities for improved privacy and security.Despite all these interactions, data collection, and the analytic insights based upon them-we cannot forget the issues of privacy. Without a proper and reliable solution that offers more control over what personal data is shared and how it is used, people will refrain from sharing information. Such sharing is necessary for developing and understanding the context in which people are carrying out various actions, and to offer them tools and services to enhance their actions.In the not-so-distant future, we anticipate the appearance of ad-hoc networks for wearable technology systems that will interact with one another to further expand the scope and value of available context-dependent data.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {11–12},
numpages = {2},
location = {Goa, India},
series = {ISEC '16}
}

@inproceedings{10.1145/3466933.3466945,
author = {Oliveira, Breno Silva and Ara\'{u}jo, \'{I}talo L. and Paiva, Joseane O. V. and Junior, Evilasio C. and Andrade, Rossana M. C.},
title = {Refactoring Decision Based on Measurements for IoHT Apps},
year = {2021},
isbn = {9781450384919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466933.3466945},
doi = {10.1145/3466933.3466945},
abstract = {Internet of Things (IoT) provides smart objects with the ability to connect to the Internet, allowing the exchange of information among them to provide a certain service and the development of innovative applications in several domains, including e-Health, in which it is called Internet of Health Things (IoHT). This domain can be critical specially when the application deals with the monitoring of the user health in real-time, what demands software quality assurance, even more than in other applications. Measures can be used to support that, for example, measures can suggest which components need refactoring to improve the software code, thus improving the application. In this work, we report how to do that with two existing measures that guide the refactoring process of an IoHT application for fall detection, called WatchAlert. These measures indicate that changes in both the architecture and the algorithms for fall detection should occur. After the refactoring, the app accuracy was improved from 73.3\% to 92.7\%. We believe that this work can contribute to other studies focusing on developing applications on the IoHT domain using a methodology, a set of refactoring techniques, and lessons learned that could be replicated to improve the quality of this type of application.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Information Systems},
articleno = {12},
numpages = {9},
keywords = {Refactoring, Internet of Things, e-Health, Measures, Fall detection},
location = {Uberl\^{a}ndia, Brazil},
series = {SBSI '21}
}

@inproceedings{10.5555/2821327.2821330,
author = {Zimmermann, Olaf},
title = {Metrics for Architectural Synthesis and Evaluation: Requirements and Compilation by Viewpoint: An Industrial Experience Report},
year = {2015},
publisher = {IEEE Press},
abstract = {During architectural analysis and synthesis, architectural metrics are established tacitly or explicitly. In architectural evaluation, these metrics are then consulted to assess whether architectures are fit for purpose and in line with recommended practices and published architectural knowledge. This experience report presents a personal retrospective of the author's use of architectural metrics during 20 years in IT architect roles in professional services as well as research and development. This reflection drives the identification of use cases, critical success factors and elements of risk for architectural metrics management. An initial catalog of architectural metrics is compiled next, which is organized by viewpoints and domains. The report concludes with a discussion of practical impact of architectural metrics and potential research topics in this area.},
booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
pages = {8–14},
numpages = {7},
keywords = {architectural metrics management, viewpoints, architectural reviews, integration, architectural metrics, enterprise information systems, patterns},
location = {Florence, Italy},
series = {SAM '15}
}

@inproceedings{10.1145/3341105.3374026,
author = {Araldo, Andrea and Stefano, Alessandro Di and Stefano, Antonella Di},
title = {Resource Allocation for Edge Computing with Multiple Tenant Configurations},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374026},
doi = {10.1145/3341105.3374026},
abstract = {Edge Computing (EC) consists in deploying computational resources, e.g., memory, CPUs, at the Edge of the network, e.g., base stations, access points, and run there a part of the computation currently running on the Cloud. This approach promises to reduce latency, inter-domain traffic and enhance user experience. Since resources at the Edge are scarce, resource allocation is crucial for EC. While most of the studies assume users interact directly with the Edge submitting a sequence of tasks, we instead consider that users will interact with different Service Providers (SPs), as they currently do in the Web. We therefore consider the case of a Network Operator (NO) that owns the resources at the Edge and must decide how much resource to allocate to the different tenants (SPs).We propose MORA, a polynomial time strategy which allows the NO to maximize its utility, which can be inter-domain traffic savings, improved users' QoE or other metrics of interest. The core of MORA is that (i) it exploits service elasticity, i.e., the fact that services can adapt to the resources allocated by the NO and rely on a remote Cloud for the excess of computation, (ii) it is suitable for micro-services architecture, which decomposes a single service in a set of components, which MORA places in the different computational nodes of the Edge and (iii) it copes with multi-dimensional resources, e.g., memory and CPUs. After analyzing the properties of the algorithm, we show numerically that it performs close to the optimum. To guarantee reproducibility, the numerical evaluation is performed on publicly available traces from Google and Alibaba clusters and in synthetic scenarios and our code is open source.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1190–1199},
numpages = {10},
keywords = {container systems, resource allocation, edge computing, network optimization, cloud computing},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.5555/2820518.2820566,
author = {Mirakhorli, Mehdi and Cleland-Huang, Jane},
title = {Modifications, Tweaks, and Bug Fixes in Architectural Tactics},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Architectural qualities such as reliability, performance, and security, are often realized in a software system through the adoption of tactical design decisions such as the decision to use redundant processes, a heartbeat monitor, or a specific authentication mechanism. Such decisions are critical for delivering a system that meets its quality requirements. Despite the stability of high-level decisions, our analysis has shown that tactic-related classes tend to be modified more frequently than other classes and are therefore stronger predictors of change than traditional Object-Oriented coupling and cohesion metrics. In this paper we present the results from this initial study, including an analysis of why tactic-related classes are changed, and a discussion of the implications of these findings for maintaining architectural quality over the lifetime of a software system.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {377–380},
numpages = {4},
keywords = {modifications, architectural decisions, metrics, tactics, bugs},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.5555/2840819.2840915,
author = {Kim, Yeseong and Imani, Mohsen and Patil, Shruti and Rosing, Tajana S.},
title = {CAUSE: Critical Application Usage-Aware Memory System Using Non-Volatile Memory for Mobile Devices},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {Mobile devices are severely limited in memory, which affects critical user-experience metrics such as application service time. Emerging non-volatile memory (NVM) technologies such as STT-RAM and PCM are ideal candidates to provide higher memory capacity with negligible energy overhead. However, existing memory management systems overlook mobile users application usage which provides crucial cues for improving user experience. In this paper, we propose CAUSE, a novel memory system based on DRAM-NVM hybrid memory architecture. CAUSE takes explicit account of the application usage patterns to distinguish data criticality and identify suitable swap candidates. We also devise NVM hardware design optimized for the access characteristics of the swapped pages. We evaluate CAUSE on a real Android smartphone and NVSim simulator using user application usage logs. Our experimental results show that the proposed technique achieves 32\% faster launch time for mobile applications while reducing energy cost by 90\% and 44\% on average over non-optimized STT-RAM and PCM, respectively.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {690–696},
numpages = {7},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1145/2628588.2628598,
author = {Sharakhov, Nikita and Marojevic, Vuk and Romano, Ferdinando and Polys, Nicholas and Dietrich, Carl},
title = {Visualizing Real-Time Radio Spectrum Access with CORNET3D},
year = {2014},
isbn = {9781450330152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628588.2628598},
doi = {10.1145/2628588.2628598},
abstract = {Modern web technology enables the 3D portrayal of real-time data. WebSocket connections provide data over the web without the time-consuming overhead of HTTP requests. The server-side "push" paradigm is particularly useful for creating novel tools such as CORNET3D, where real-time 3D visualization is required. CORNET3D is an innovative Web3D interface to a research and education test bed for Dynamic Spectrum Access (DSA). Our system can drive several 2D and 3D portrayals of spectral data and radio performance metrics from a live, online system. The testbed can further integrate the data portrayals into a multi-user "serious game" to teach students about strategies for the optimal use of spectrum resources by providing them with real-time scoring based on their choices of radio transmission parameters. This paper describes the web service architecture and Webd3D front end for our DSA testbed, detailing new methods for spectrum visualization and the applications they enable.},
booktitle = {Proceedings of the 19th International ACM Conference on 3D Web Technologies},
pages = {109–116},
numpages = {8},
keywords = {web applications, computer graphics, WebSockets, WebGL, HTML5},
location = {Vancouver, British Columbia, Canada},
series = {Web3D '14}
}

@inproceedings{10.1145/3366424.3382670,
author = {Tiwary, Mayank and Mishra, Pritish and Jain, Shashank and Puthal, Deepak},
title = {Data Aware Web-Assembly Function Placement},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382670},
doi = {10.1145/3366424.3382670},
abstract = {Existing container based serverless computing systems are limited by cold-start problems and complex architecture for stateful services, multi-tenancy, etc. This paper presents serverless functions to be placed as per data locality and executed as a web-assembly sandbox, which results better execution latency and reduced network usage as compared to the existing architectures. The designed serverless runtime features resource isolation in terms of CPU, Memory, and file-system isolation and falicitates multi-tenancy executions. The proposed architecture is evaluated using IoT workloads with different performance metrics.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {4–5},
numpages = {2},
keywords = {Servelress, Data Locality, Web-Assembly, Multi-Tenancy},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3338466.3358916,
author = {Alder, Fritz and Asokan, N. and Kurnikov, Arseny and Paverd, Andrew and Steiner, Michael},
title = {S-FaaS: Trustworthy and Accountable Function-as-a-Service Using Intel SGX},
year = {2019},
isbn = {9781450368261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338466.3358916},
doi = {10.1145/3338466.3358916},
abstract = {Function-as-a-Service (FaaS) is a recent and popular cloud computing paradigm in which the function provider specifies a function to be run and is billed only for the computational resources used by that function. Compared to other cloud paradigms, FaaS requires significantly more fine-grained measurement of functions' compute time and memory usage. Since functions are short and stateless, small ephemeral entities (e.g. individuals or underutilized data centers) can become FaaS service providers. However, this exacerbates the already substantial challenges of 1) ensuring integrity of computation, 2) minimizing information revealed to the service provider, and 3) accurately measuring computational resource usage.To address these challenges, we introduce S-FaaS, the first architecture and implementation of FaaS to provide strong security and accountability guarantees using Intel SGX. To match the dynamic event-driven nature of FaaS, we introduce a new key distribution enclave and a novel transitive attestation protocol. A core contribution of S-FaaS is our set of reusable resource measurement mechanisms that securely measure compute time and memory usage inside an enclave. We have integrated S-FaaS into the OpenWhisk FaaS framework and provide this as open source software.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
pages = {185–199},
numpages = {15},
keywords = {function-as-a-service, resource measurement, intel sgx},
location = {London, United Kingdom},
series = {CCSW'19}
}

@inproceedings{10.1145/3209914.3209918,
author = {Zou, Luyao and Rui, Xuhua and Nguyen, Tuan Anh and Min, Dugki and Choi, Eunmi and Thang, Tran Duc and Son, Nguyen Nhu},
title = {A Scalable Network Area Storage with Virtualization: Modelling and Evaluation Using Stochastic Reward Nets},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3209918},
doi = {10.1145/3209914.3209918},
abstract = {Modelling and analysis of storage system in data centers for availability prediction is of paramount importance. Many studies in literature proposed different architectures and techniques to enhance availability of the storage system. In this paper, we proposed to incorporate virtualization techniques on a network area storage. We used stochastic reward nets to model the system's architecture and operational scenarios. Furthermore, we investigated various measures of interests including steady state availability, downtime and downtime cost, and sensitivity of the system availability with respect to impacting parameters. The analysis results show that the proposed storage system with virtualization can obtain an acceptable level of service availability. Furthermore, the sensitivity analysis also points out complicated dependences of service availability upon system parameters. This paper presents a preliminary study to help guide the development of a scalable network area storage with virtualization in practice.},
booktitle = {Proceedings of the 1st International Conference on Information Science and Systems},
pages = {225–233},
numpages = {9},
keywords = {Stochastic Reward Nets, Network Attached Storage, Availability, Reliability},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@inproceedings{10.1145/3276774.3276777,
author = {Coffman, Austin R. and Bu\v{s}i\'{c}, Ana and Barooah, Prabir},
title = {Virtual Energy Storage from TCLs Using QoS Preserving Local Randomized Control},
year = {2018},
isbn = {9781450359511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276774.3276777},
doi = {10.1145/3276774.3276777},
abstract = {We propose a control architecture for distributed coordination of a collection of on/off TCLs (thermostatically controlled loads), such as residential air conditioners, to provide the same service to the power grid as a large battery. This involves a collection of loads to coordinate their on/off decisions so that the aggregate power consumption profile tracks a grid-supplied reference. A key constraint is to maintain each consumer's quality of service (QoS). Recent works have proposed randomization at the loads. Thermostats at the loads are replaced by a randomized controller, and the grid broadcasts a scalar to all loads, which tunes the probability of turning on or off at each load depending on its state. In this paper we propose a modification of a previous design by Meyn and Bu\v{s}i\'{c}. The previous design by Meyn and Bu\v{s}i\'{c} ensures that the indoor temperature remains within a pre-specified bound, but other QoS metrics, especially the frequency of turning on and off was not limited. The controller we propose can be tuned to reduce the cycling rate of a TCL to any desired degree. The proposed design is compared against the design by Meyn and Bu\v{s}i\'{c} and another well cited design in the literature on control of TCL populations, by Mathieu et al. We show through simulations that the proposed controller is able to reduce the cycling of individual ACs compared to the previous designs with little loss in tracking of the grid-supplied reference signal.},
booktitle = {Proceedings of the 5th Conference on Systems for Built Environments},
pages = {93–102},
numpages = {10},
keywords = {virtual energy storage, demand response, distributed control, randomized control},
location = {Shenzen, China},
series = {BuildSys '18}
}

@inproceedings{10.1145/3379310.3379320,
author = {Lumba, Ester and Waworuntu, Alexander},
title = {Application of Lecturer Performance Report in Indonesia with Model View Controller (MVC) Architecture},
year = {2020},
isbn = {9781450376853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379310.3379320},
doi = {10.1145/3379310.3379320},
abstract = {Lecturers in Indonesia have a fundamental obligation to conduct Tri Dharma activities consisting of teaching, research and community service. Most higher education institutions use Tri Dharma as a measure of lecturer's performance. In addition, lecturer activity data related to Tri Dharma is needed by the head of study program and department related to research, publication and community service to be stored which will be used as a source of data during the accreditation process. This paper discusses the application development of lecturer performance reports using the Model View Controller (MVC) architecture with Java programming language. The result is a desktop-based application that will be used by the head of the study program and the lecturers.},
booktitle = {Proceedings of the 2020 2nd Asia Pacific Information Technology Conference},
pages = {23–28},
numpages = {6},
keywords = {Indonesia higher-education, MVC architecture, application development, desktop-based application},
location = {Bali Island, Indonesia},
series = {APIT '20}
}

@inproceedings{10.1109/UCC.2014.167,
author = {Wagle, Shyam S.},
title = {SLA Assured Brokering (SAB) and CSP Certification in Cloud Computing},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.167},
doi = {10.1109/UCC.2014.167},
abstract = {Due to lack of information of the cloud service providers (CSPs), customers can not easily choose services according to their requirement and due to vendor lock-in and lack of interoperability standards among cloud service providers, customers cannot switch the providers once services are subscribed from CSPs. Recently proposed third party architecture which is called cloud broker can access inter-cloud and provides services to the customers according to their requirement but providing SLA based cloud services as per their requirement is still missing in current researches. In our work, we propose the SLA assured brokering framework which matches the requirements of the customer with SLA offered by CSPs using similarity matching algorithm and willingness to pay capacity for the services. It also measures the services offered by CSPs for certifying and ranking the CSPs.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {1016–1017},
numpages = {2},
keywords = {Certifying, Similarity Matching, SLA, Cloud Brokering},
series = {UCC '14}
}

@inproceedings{10.1109/MODELS-C.2019.00032,
author = {Burdusel, Alexandru and Zschaler, Steffen},
title = {Towards Scalable Search-Based Model Engineering with MDEOptimiser Scale},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00032},
doi = {10.1109/MODELS-C.2019.00032},
abstract = {Running scientific experiments using search-based model engineering (SBME) tools is a complex task, that poses a number of challenges, starting from defining an experiment workflow, to parameter tuning, finding optimal computational resources to run on, collecting and interpreting metrics and making the entire process easily reproducible.Despite the proliferation of easily accessible hardware, as a result of the increased availability of infrastructure-as-a-service providers, many SBME tools are rarely using this technology for accelerating experimentation. Running many experiments on a single machine implies much longer waiting times and reduces the ability to increase the speed of iterations when doing SBME research, thus, slowing down the entire process.In this paper, we introduce a domain-specific language (DSL) and a framework that can be used to configure and run experiments at scale, on cloud infrastructure, in a reproducible way. We will describe our DSL and framework architecture along with an example to showcase how a case study can be evaluated using two different model optimisation tools.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {189–195},
numpages = {7},
keywords = {search based model engineering, reproducible research, middleware, model driven engineering, cloud, workflow, evolutionary search},
location = {Munich, Germany},
series = {MODELS '19}
}

@inproceedings{10.1145/2933349.2933359,
author = {Sirin, Utku and Appuswamy, Raja and Ailamaki, Anastasia},
title = {OLTP on a Server-Grade ARM: Power, Throughput and Latency Comparison},
year = {2016},
isbn = {9781450343190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933349.2933359},
doi = {10.1145/2933349.2933359},
abstract = {Although scaling out of low-power cores is an alternative to power-hungry Intel Xeon processors for reducing the power overheads, they have proven inadequate for complex, non-parallelizable workloads. On the other hand, by the introduction of the 64-bit ARMv8 architecture, traditionally low power ARM processors have become powerful enough to run computationally intensive server-class applications.In this study, we compare a high-performance Intel x86 processor with a commercial implementation of the ARM Cortex-A57. We measure the power used, throughput delivered and latency quantified when running OLTP workloads. Our results show that the ARM processor consumes 3 to 15 times less power than the x86, while penalizing OLTP throughput by a much lower factor (1.7 to 3). As a result, the significant power savings deliver up to 9 times higher energy efficiency. The x86's heavily optimized power-hungry micro-architectural structures contribute to throughput only marginally. As a result, the x86 wastes power when utilization is low, while lightweight ARM processor consumes only as much power as it is utilized, achieving energy proportionality. On the other hand, ARM's quantified latency can be up to 11x higher than x86 towards to the tail of latency distribution, making x86 more suitable for certain type of service-level agreements.},
booktitle = {Proceedings of the 12th International Workshop on Data Management on New Hardware},
articleno = {10},
numpages = {7},
location = {San Francisco, California},
series = {DaMoN '16}
}

@inproceedings{10.1145/3286685.3286686,
author = {Saurez, Enrique and Balasubramanian, Bharath and Schlichting, Richard and Tschaen, Brendan and Huang, Zhe and Narayanan, Shankaranarayanan Puzhavakath and Ramachandran, Umakishore},
title = {METRIC: A Middleware for Entry Transactional Database Clustering at the Edge},
year = {2018},
isbn = {9781450361170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286685.3286686},
doi = {10.1145/3286685.3286686},
abstract = {A geo-distributed database for edge architectures spanning thousands of sites needs to assure efficient local updates while replicating sufficient state across sites to enable global management and support mobility, failover etc. To address this requirement, a new paradigm for database clustering that achieves a better balance than existing solutions between performance and strength of semantics called entry transactionality is introduced. Inspired by entry consistency in shared memory systems, entry transactionality guarantees that only a client that owns a range of keys in the database has a sequentially consistent value of the keys and can perform local and, hence, efficient transactions across these keys. Important use cases enabled by entry transactionality such as federated controllers and state management for edge applications are identified. The semantics of entry transactionality incorporating the complex failure modes in geo-distributed services are defined, and the difficult challenges in realizing these semantics are outlined. Then, a novel Middleware for Entry Transactional Clustering (METRIC) that combines existing SQL databases with an underlying geo-distributed entry consistent store to realize entry transactionality is described. This paper provides initial findings from an on-going effort.},
booktitle = {Proceedings of the 3rd Workshop on Middleware for Edge Clouds \&amp; Cloudlets},
pages = {2–7},
numpages = {6},
location = {Rennes, France},
series = {MECC'18}
}

@inproceedings{10.1145/3447545.3451180,
author = {Klaver, Luuk and van der Knaap, Thijs and van der Geest, Johan and Harmsma, Edwin and van der Waaij, Bram and Pileggi, Paolo},
title = {Towards Independent Run-Time Cloud Monitoring},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451180},
doi = {10.1145/3447545.3451180},
abstract = {Cloud computing services are integral to the digital transformation. They deliver greater connectivity, tremendous savings, and lower total cost of ownership. Despite such benefits and benchmarking advances, costs are still quite unpredictable, performance is unclear, security is inconsistent, and there is minimal control over aspects like data and service locality. Estimating performance of cloud environments is very hard for cloud consumers. They would like to make informed decisions about which provider better suits their needs using specialized evaluation mechanisms. Providers have their own tools reporting specific metrics, but they are potentially biased and often incomparable across providers. Current benchmarking tools allow comparison but consumers need more flexibility to evaluate environments under actual operating conditions for specialized applications. Ours is early stage work and a step towards a monitoring solution that enables independent evaluation of clouds for very specific application needs. In this paper, we present our initial architecture of the Cloud Monitor that aims to integrate existing and new benchmarks in a flexible and extensible way. By way of a simplistic demonstrator, we illustrate the concept. We report some preliminary monitoring results after a brief time of monitoring and are able to observe unexpected anomalies. The results suggest an independent monitoring solution is a powerful enabler of next generation cloud computing, not only for the consumer but potentially the whole ecosystem.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {21–26},
numpages = {6},
keywords = {performance evaluation, cloud computing, run-time monitoring, benchmarking},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3358695.3361753,
author = {Mohammadi, Farnaz and Panou, Angeliki and Ntantogian, Christoforos and Karapistoli, Eirini and Panaousis, Emmanouil and Xenakis, Christos},
title = {CUREX: SeCUre and PRivate HEalth Data EXchange},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3361753},
doi = {10.1145/3358695.3361753},
abstract = {The Health sector's increasing dependence on digital information and communication infrastructures renders it vulnerable to privacy and cybersecurity threats, especially as the theft of health data has become lucrative for cyber criminals. CUREX comprehensively addresses the protection of the confidentiality and integrity of health data by producing a novel, flexible and scalable situational awareness-oriented platform. It allows a healthcare provider to assess cybersecurity and privacy risks that are exposed to and suggest optimal strategies for addressing these risks with safeguards tailored to each business case and application. CUREX is fully GDPR compliant by design. At its core, a decentralised architecture enhanced by a private blockchain infrastructure ensures the integrity of the data and –most importantly- the patient safety. Crucially, CUREX expands beyond technical measures and improves cyber hygiene through training and awareness activities for healthcare personnel. Its validation focuses on highly challenging cases of health data exchange, spanning patient cross-border mobility, remote healthcare, and data exchange for research.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {263–268},
numpages = {6},
keywords = {Cybersecurity, Risk assessment, eHealth, Blockchain, Cyber hygiene},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.1145/3298689.3346961,
author = {Panteli, Maria},
title = {Recommendation Systems Compliant with Legal and Editorial Policies: The BBC+ App Journey},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346961},
doi = {10.1145/3298689.3346961},
abstract = {The BBC produces thousands of pieces of content every day and numerous BBC products deliver this content to millions of users. For many years the content has been manually curated (this is evident in the selection of stories on the front page of the BBC News website and app for example). To support content creation and curation, a set of editorial guidelines have been developed to build quality and trust in the BBC. As personalisation becomes more important for audience engagement, we have been exploring how algorithmically-driven recommendations could be integrated in our products. In this talk we describe how we developed recommendation systems for the BBC+ app that comply with legal and editorial policies and promote the values of the organisation. We also discuss the challenges we face moving forward, extending the use of recommendation systems for a public service media organisation like the BBC.The BBC+ app is the first product to host in-house recommendations in a fully algorithmically-driven application. The app surfaces short video clips and is targeted at younger audiences. The first challenge we dealt with was content metadata. Content metadata are created for different purposes and managed by different teams across the organisation making it difficult to have reliable and consistent information. Metadata enrichment strategies have been applied to identify content that is considered to be editorially sensitive, such as political content, current legal cases, archived news, commercial content, and content unsuitable for an under 16 audience. Metadata enrichment is also applied to identify content that due care has not been taken such as poor titles, and spelling and grammar mistakes. The first versions of recommendation algorithms exclude all editorially risky content from the recommendations, the most serious of which is avoiding contempt of court. In other cases we exclude content that could undermine our quality and trustworthiness.The General Data Protection Regulation (GDPR) that recently came into effect had strong implications on the design of our system architecture, the choice of the recommendation models, and the implementation of specific product features. For example, the user should be able to delete their data or switch off personalisation at any time. Our system architecture should allow us to trace down and delete all data from that user and switch to non-personalised content. The recommendations should also be explainable and this led us to sometimes choosing a simpler model so that it is possible to more easily explain why a user was recommended a particular type of content. Specific product features were also added to enhance transparency and explainability. For example, the user could view their history of watched items, delete any item, and get an explanation of why a piece of content was recommended to them.At the BBC we aim to not only entertain our audiences but also to inform and educate. These BBC values are also reflected in our evaluation strategies and metrics. While we aim to increase audience engagement we are also responsible for providing recent and diverse content that meets the needs of all our audiences. Accuracy metrics such as Hit Rate and Normalized Discounted Cumulative Gain (NDCG) can give a good estimate of the predictive performance of the model. However, recency and diversity metrics have sometimes more weight in our products, especially in applications delivering news content. What is more, qualitative evaluation is very important before releasing any new model into production. We work closely with editorial teams who provide feedback on the quality of the recommendations and flag content not adhering to the BBC's values or the legal and editorial policies.The development of the BBC+ app has been a great journey. We learned a lot about our content metadata, the implications of GDPR in our system, and our evaluation strategies. We created a minimum viable product that is compliant with legal and editorial policies. However, a lot needs to be done to ensure the recommendations meet the quality standards of the BBC. While excluding editorially sensitive content has limited the risk of contempt of court, algorithmic fairness and impartiality still need to be addressed. We encourage the community to look more into these topics and help us create the way forward towards applications with responsible machine learning.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {529},
numpages = {1},
keywords = {technology policy, recommendations, public service},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{10.1145/2816839.2816875,
author = {Ilyas, Bambrik and Fedoua, Didi},
title = {A Load Management Algorithm For Wireless Mesh Networks},
year = {2015},
isbn = {9781450334587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2816839.2816875},
doi = {10.1145/2816839.2816875},
abstract = {The WMN (Wireless Mesh Network) is a new emerging technology that can render the field of industrial network more efficient and profitable. Due to its versatility that allows a flexible configuration, this kind of network is commonly considered as a very suitable architecture for mobile clients. The difference between the WMNs and other dynamic networks, such as the MANET (Mobile Ad-hoc Network), is that the Mesh network contains static wireless nodes called MR (Mesh Routers). Consequently, the presence of this infrastructure makes the WMN more suitable to provide QoS (Quality of Service). However, the guarantee of QoS in a dynamic topology is a difficult task by comparison with static networks. These difficulties are caused by the random movement of the clients, the shared nature of the wireless channel, the complexity of multi-hop communications and most importantly the management of the traffic load forwarded through the MRs. In this paper, we propose a new algorithm for load balancing in WMN that can search for alternative paths in order to deviate from the loaded MRs. The proposed algorithm can operate with different metrics at the same time and applies the Genetic Algorithm in case there is a large population of possible solutions.},
booktitle = {Proceedings of the International Conference on Intelligent Information Processing, Security and Advanced Communication},
articleno = {46},
numpages = {6},
keywords = {traffic load, Genetic Algorithm, WMN, Mesh Routers, mobile clients, QoS},
location = {Batna, Algeria},
series = {IPAC '15}
}

@inproceedings{10.1145/3453688.3461512,
author = {Gao, Chengsi and Li, Bing and Wang, Ying and Chen, Weiwei and Zhang, Lei},
title = {Tenet: A Neural Network Model Extraction Attack in Multi-Core Architecture},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453688.3461512},
doi = {10.1145/3453688.3461512},
abstract = {As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4\%.},
booktitle = {Proceedings of the 2021 on Great Lakes Symposium on VLSI},
pages = {21–26},
numpages = {6},
keywords = {memory timing channel, machine learning, deep learning security, multi-core},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@inproceedings{10.1145/3383313.3412248,
author = {Hansen, Casper and Hansen, Christian and Maystre, Lucas and Mehrotra, Rishabh and Brost, Brian and Tomasi, Federico and Lalmas, Mounia},
title = {Contextual and Sequential User Embeddings for Large-Scale Music Recommendation},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412248},
doi = {10.1145/3383313.3412248},
abstract = {Recommender systems play an important role in providing an engaging experience on online music streaming services. However, the musical domain presents distinctive challenges to recommender systems: tracks are short, listened to multiple times, typically consumed in sessions with other tracks, and relevance is highly context-dependent. In this paper, we argue that modeling users’ preferences at the beginning of a session is a practical and effective way to address these challenges. Using a dataset from Spotify, a popular music streaming service, we observe that a) consumption from the recent past and b) session-level contextual variables (such as the time of the day or the type of device used) are indeed predictive of the tracks a user will stream—much more so than static, average preferences. Driven by these findings, we propose CoSeRNN, a neural network architecture that models users’ preferences as a sequence of embeddings, one for each session. CoSeRNN predicts, at the beginning of a session, a preference vector, based on past consumption history and current context. This preference vector can then be used in downstream tasks to generate contextually relevant just-in-time recommendations efficiently, by using approximate nearest-neighbour search algorithms. We evaluate CoSeRNN on session and track ranking tasks, and find that it outperforms the current state of the art by upwards of 10\% on different ranking metrics. Dissecting the performance of our approach, we find that sequential and contextual information are both crucial.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {53–62},
numpages = {10},
keywords = {Sequence, Music Recommendation, User Embeddings, Context},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3405656.3418718,
author = {G\"{u}ndo\u{g}an, Cenk and Ams\"{u}ss, Christian and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
title = {Toward a RESTful Information-Centric Web of Things: A Deeper Look at Data Orientation in CoAP},
year = {2020},
isbn = {9781450380409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405656.3418718},
doi = {10.1145/3405656.3418718},
abstract = {The information-centric networking (ICN) paradigm offers replication of autonomously verifiable content throughout a network, in which content is bound to names instead of hosts. This has proven beneficial in particular for the constrained IoT. Several approaches, the most prominent of which being Named Data Networking, propose access to named content directly on the network layer. Independently, the IETF CoAP protocol group started to develop mechanisms that support autonomous content processing and in-network storage.In this paper, we explore the emerging CoAP protocol building blocks and how they contribute to an information-centric network architecture for a data-oriented RESTful Web of Things. We discuss design options and measure characteristic performances of different network configurations, which deploy CoAP proxies and OSCORE content object security, and compare with NDN. Our findings indicate an almost continuous design space ranging from plain CoAP at the one end to NDN on the other. On both ends---ICN and CoAP---we identify protocol features and aspects whose mutual transfer potentially improves design and operation of the other.},
booktitle = {Proceedings of the 7th ACM Conference on Information-Centric Networking},
pages = {77–88},
numpages = {12},
keywords = {CoAP Proxy, Internet of Things, ICN, OSCORE, content object security, protocol evaluation},
location = {Virtual Event, Canada},
series = {ICN '20}
}

@article{10.1145/3340290,
author = {Wang, Ji and Bao, Weidong and Zheng, Lei and Zhu, Xiaomin and Yu, Philip S.},
title = {An Attention-Augmented Deep Architecture for Hard Drive Status Monitoring in Large-Scale Storage Systems},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/3340290},
doi = {10.1145/3340290},
abstract = {Data centers equipped with large-scale storage systems are critical infrastructures in the era of big data. The enormous amount of hard drives in storage systems magnify the failure probability, which may cause tremendous loss for both data service users and providers. Despite a set of reactive fault-tolerant measures such as RAID, it is still a tough issue to enhance the reliability of large-scale storage systems. Proactive prediction is an effective method to avoid possible hard-drive failures in advance. A series of models based on the SMART statistics have been proposed to predict impending hard-drive failures. Nonetheless, there remain some serious yet unsolved challenges like the lack of explainability of prediction results. To address these issues, we carefully analyze a dataset collected from a real-world large-scale storage system and then design an attention-augmented deep architecture for hard-drive health status assessment and failure prediction. The deep architecture, composed of a feature integration layer, a temporal dependency extraction layer, an attention layer, and a classification layer, cannot only monitor the status of hard drives but also assist in failure cause diagnoses. The experiments based on real-world datasets show that the proposed deep architecture is able to assess the hard-drive status and predict the impending failures accurately. In addition, the experimental results demonstrate that the attention-augmented deep architecture can reveal the degradation progression of hard drives automatically and assist administrators in tracing the cause of hard drive failures.},
journal = {ACM Trans. Storage},
month = {aug},
articleno = {21},
numpages = {26},
keywords = {recurrent neural network, SMART, attention mechanism, deep neural network, Hard drive failure}
}

@inproceedings{10.1145/3446999.3447004,
author = {Jiang, Dongming and Jiang, Yuan and Li, Jinzi},
title = {A Session-Based Interaction Model for Cloud Service},
year = {2021},
isbn = {9781450388559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446999.3447004},
doi = {10.1145/3446999.3447004},
abstract = {Being executed in the Internet space, cloud services provide elastic computing resource to satisfy customers’ demand. Since single service usually can't suit their complex need, customers need to assemble multiple services into one service workflow which coordinates individual service to fulfill the specific task by means of service interaction. Obviously, the interaction mechanism of cloud service is the glue of workflow and plays an important role in service workflow. However, owing to the dynamic characteristic of the Internet, cloud service interaction is complicated and volatile. Hence, the question how to describe and formalize the complex interaction of cloud service workflow is a non-trivial work, which has directly influence on the overall design of cloud service architecture engineering and its performance.Consider the question above, this article put forward a session-based interaction model of cloud service. Regarding the complexity of service workflow, this paper applies the divide-and-conquer strategy to decompose the interaction of cloud services into session, then constructs the interaction model of cloud service. In the first step, we introduce and formalize the notion of session, so the complex interaction of cloud service workflow can be decomposed into hierarchical session pattern, which is easy to formalize by the guard automata. Next, using the session as the abstract data type, the session-based interaction model is proposed in order to facilitate the formalization of cloud service workflow. In addition, the model incorporates the notion of role and business protocol to strength the model flexibility. Like the role of object in object-oriented programming language, this model provides a new perspective for modelling the cloud service workflow and lay the solid foundation for its formal verification.},
booktitle = {Proceedings of the 2020 8th International Conference on Information Technology: IoT and Smart City},
pages = {25–28},
numpages = {4},
keywords = {role, interface, session, service workflow, cloud service},
location = {Xi'an, China},
series = {ICIT '20}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-Grained Test Case Prioritization for Integration Testing of Delta-Oriented Software Product Lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3054977.3057290,
author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
title = {Simulating Conflict Detection in Heterogeneous Services of a Smart City: Demo Abstract},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3057290},
doi = {10.1145/3054977.3057290},
abstract = {Despite the increasing intelligence of smart services and sophistication of IoT platforms, the safety issues in smart cities are not addressed adequately, especially the safety issues arising from the integration of smart services. Therefore, in this demo abstract, we present CityGuard, a safety-aware watchdog architecture to detect conflicts among actions of heterogeneous services considering both safety and performance requirements. This demo simulates parts of New York City to depict how CityGuard identifies unsafe actions and thus helps to prevent the city from safety hazards, detects two major types of conflicts, i.e., device and environmental conflicts, and improves the overall city performance in terms of multiple performance metrics. This demo complements the full paper on CityGuard that appears in this conference [2].},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {275–276},
numpages = {2},
keywords = {Smart City, City Safety, Conflict Detection, City Simulation},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@article{10.1109/TNET.2017.2746011,
author = {Sapountzis, Nikolaos and Spyropoulos, Thrasyvoulos and Nikaein, Navid and Salim, Umer},
title = {User Association in HetNets: Impact of Traffic Differentiation and Backhaul Limitations},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2017.2746011},
doi = {10.1109/TNET.2017.2746011},
abstract = {Operators, struggling to continuously add capacity and upgrade their architecture to keep up with data traffic increase, are turning their attention to denser deployments that improve spectral efficiency. Denser deployments make the problem of user association challenging, and much work has been devoted to finding algorithms that strike a tradeoff between user quality of service, and network-wide performance load-balancing. Nevertheless, the majority of these algorithms typically consider simple setups with a single type of traffic, usually elastic non-guaranteed bit rate GBR. They also focus on the radio access part, ignoring the backhaul topology and potential capacity limitations. Backhaul constraints are emerging as a key performance bottleneck in future networks, partly due to the continuous improvement of the radio interface, and partly due to the need for inexpensive backhaul links to reduce capital and operational expenditures. To this end, we propose an analytical framework for user association that jointly considers radio access and backhaul network performance. Specifically, we derive an algorithm that takes into account spectral efficiency, base station load, backhaul link capacities and topology, and two traffic classes GBR and non-GBR in both the uplink and downlink directions. We prove analytically an optimal user association rule that ends up maximizing either an arithmetic or a weighted harmonic mean of the achieved performance along different dimensions e.g., uplink and downlink performances or GBR and non-GBR performances. We then use extensive simulations to study the impact of: 1 traffic differentiation; and 2 backhaul capacity limitations and topology on key performance metrics.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {3396–3410},
numpages = {15}
}

@inproceedings{10.1145/2684822.2697043,
author = {Lattanzi, Silvio and Mirrokni, Vahab},
title = {Distributed Graph Algorithmics: Theory and Practice},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2697043},
doi = {10.1145/2684822.2697043},
abstract = {As a fundamental tool in modeling and analyzing social, and information networks, large-scale graph mining is an important component of any tool set for big data analysis. Processing graphs with hundreds of billions of edges is only possible via developing distributed algorithms under distributed graph mining frameworks such as MapReduce, Pregel, Gigraph, and alike. For these distributed algorithms to work well in practice, we need to take into account several metrics such as the number of rounds of computation and the communication complexity of each round. For example, given the popularity and ease-of-use of MapReduce framework, developing practical algorithms with good theoretical guarantees for basic graph algorithms is a problem of great importance.In this tutorial, we first discuss how to design and implement algorithms based on traditional MapReduce architecture. In this regard, we discuss various basic graph theoretic problems such as computing connected components, maximum matching, MST, counting triangle and overlapping or balanced clustering. We discuss a computation model for MapReduce and describe the sampling, filtering, local random walk, and core-set techniques to develop efficient algorithms in this framework. At the end, we explore the possibility of employing other distributed graph processing frameworks. In particular, we study the effect of augmenting MapReduce with a distributed hash table (DHT) service and also discuss the use of a new graph processing framework called ASYMP based on asynchronous message-passing method. In particular, we will show that using ASyMP, one can improve the CPU usage, and achieve significantly improved running time.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {419–420},
numpages = {2},
keywords = {mapreduce algorithms, large scale data-mining, parallel computing},
location = {Shanghai, China},
series = {WSDM '15}
}

@article{10.1145/3183517,
author = {Floris, Alessandro and Ahmad, Arslan and Atzori, Luigi},
title = {QoE-Aware OTT-ISP Collaboration in Service Management: Architecture and Approaches},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3183517},
doi = {10.1145/3183517},
abstract = {It is a matter of fact that quality of experience (QoE) has become one of the key factors determining whether a new multimedia service will be successfully accepted by the final users. Accordingly, several QoE models have been developed with the aim of capturing the perception of the user by considering as many influencing factors as possible. However, when it comes to adopting these models in the management of the services and networks, it frequently happens that no single provider has access to all of the tools to either measure all influencing factors parameters or control over the delivered quality. In particular, it often happens to the over-the-top (OTT) and Internet service providers (ISPs), which act with complementary roles in the service delivery over the Internet. On the basis of this consideration, in this article we first highlight the importance of a possible OTT-ISP collaboration for a joint service management in terms of technical and economic aspects. Then we propose a general reference architecture for a possible collaboration and information exchange among them. Finally, we define three different approaches, namely joint venture, customer lifetime value based, and QoE fairness based. The first aims to maximize the revenue by providing better QoE to customers paying more. The second aims to maximize the profit by providing better QoE to the most profitable customers (MPCs). The third aims to maximize QoE fairness among all customers. Finally, we conduct simulations to compare the three approaches in terms of QoE provided to the users, profit generated for the providers, and QoE fairness.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {apr},
articleno = {36},
numpages = {24},
keywords = {ISP, QoE, Over The Top service providers, Internet service providers, QoE management, quality of experience, OTT, OTT-ISP collaboration}
}

@inproceedings{10.1145/3357141.3357149,
author = {Seabra, Matheus and Naz\'{a}rio, Marcos Felipe and Pinto, Gustavo},
title = {REST or GraphQL? A Performance Comparative Study},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357149},
doi = {10.1145/3357141.3357149},
abstract = {Given the variety of architectural models that can be used, a frequent questioning among software development practitioners is: which architectural model to use? To respond this question regarding performance issues, three target applications have been studied, each written using two models web services architectures: REST and GraphQL. Through research of performance metrics of response time and the average transfer rate between the requests, it was possible to deduce the particularities of each architectural model in terms of performance metrics. It was observed that migrating to GraphQL. resulted in an increase in performance in two-thirds of the tested applications, with respect to average number of requests per second and transfer rate of data. However, it was noticed that services after migration for GraphQL performed below its REST counterpart for workloads above 3000 requests, ranging from 98 to 2159 Kbytes per second after the migration study. On the other hand, for more trivial workloads, services on both REST and GraphQL architectures presented similar performances, where values between REST and GraphQL services ranged from 6.34 to 7.68 requests per second for workloads of 100 requests.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {123–132},
numpages = {10},
keywords = {GraphQL, Teste de desempenho, REST, Modelo arquitetural},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/3167132.3167178,
author = {Fernandes, Rodrigo and Sim\~{a}o, Jos\'{e} and Veiga, Lu\'{\i}s},
title = {EcoVMbroker: Energy-Aware Scheduling for Multi-Layer Datacenters},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167178},
doi = {10.1145/3167132.3167178},
abstract = {The cloud relies on efficient algorithms to find resources for jobs by fulfilling the job's requirements and at the same time optimise an objective function. Utility is a measure of the client satisfaction that can be seen as an objective function maximised by schedulers based on the agreed service level agreement (SLA). We propose EcoVM-Broker which can reduce energy consumption by using dynamic voltage frequency scaling (DVFS) and applying reductions of utility, different for classes of users and across ranges of resource allocations. Using efficient data structures and a hierarchical architecture, we created a scalable solution for the fast growing heterogeneous cloud. EcoVMBroker proved that we can delegate work in a hierarchical datacenter, make decisions based on summaries of resource usage collected from several nodes and still be efficient.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {403–410},
numpages = {8},
keywords = {energy efficiency, virtual machice scheduling, DVFS, partial utility},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1145/2641361.2641374,
author = {Ohkawa, Takeshi and Uetake, Daichi and Yokota, Takashi and Ootsu, Kanemitsu and Baba, Takanobu},
title = {Reconfigurable and Hardwired ORB Engine on FPGA by Java-to-HDL Synthesizer for Realtime Application},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {5},
issn = {0163-5964},
url = {https://doi.org/10.1145/2641361.2641374},
doi = {10.1145/2641361.2641374},
abstract = {A platform for networked FPGA system design, which is named "ORB Engine", is proposed to add more controllability and design productivity on FPGA-based systems composed of software and hardwired IPs. A developer can define an object-oriented interface for the circuit IP in FPGA, and implement the control sequence part using Java. The circuit IP in FPGA can be handled through object-oriented interface from variety of programing languages like C++, Java, Python, Ruby and so on. Application specific and high-efficiency circuit for ORB (Object Request Broker) protocol processing is synthesized from easy-handling Java code using JavaRock Java-to-HDL synthesizer within the de-facto standard CORBA (Common Object Request Broker Architecture). The measurement result shows a very low latency as low as 200us of UDP/IP packet in/out and exhibits a fluctuation free delay performance, which is desirable for real-time applications.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {77–82},
numpages = {6}
}

@inproceedings{10.5555/2821481.2821486,
author = {Van Landuyt, Dimitri and Joosen, Wouter},
title = {On the Role of Early Architectural Assumptions in Quality Attribute Scenarios: A Qualitative and Quantitative Study},
year = {2015},
publisher = {IEEE Press},
abstract = {Architectural assumptions are fundamentally different from architectural decisions because they can not be traced directly to requirements, nor to domain, technical or environmental constraints; they represent conditions under which the designed solution is expected to be valid. Early architectural assumptions are similar in nature, with the key difference that they are not made during architectural design but during requirement elicitation, not by the software architect (a solution-oriented stakeholder), but by the requirements engineer (a problem-oriented stakeholder). They represent initial assumptions about the system's architecture, and allow the requirements engineer to be more precise in documenting the requirements of the system.The role of early architectural assumptions in the current practice of quality attribute scenario elicitation and related development activities in the transition to architecture is unknown and under-investigated. In this paper, we present the results of an exploratory study that focuses on the role and nature of these assumptions in the early development stages. We studied a reasonably large set of quality attribute scenarios for a realistic industrial case, a smart metering system. Our study (i) confirms that quality attribute scenario elicitation in practice does rely heavily on early architectural assumptions, and (ii) shows that they do influence the perceived quality of the requirements body as a whole, in some cases positively, in other cases negatively.These findings provide empirical arguments in favor of making such assumptions explicit already during the requirements elicitation activities. Especially in the context of iterative software development methodologies such as the Twin Peaks model, a well-defined and -documented set of assumptions could smoothen the transition between successive development iterations.},
booktitle = {Proceedings of the Fifth International Workshop on Twin Peaks of Requirements and Architecture},
pages = {9–15},
numpages = {7},
location = {Florence, Italy},
series = {TwinPeaks '15}
}

@inproceedings{10.1145/3220267.3220280,
author = {Odema, Mohanad and Adly, Ihab and El-Baz, Ahmed and Amin, Hani},
title = {A RESTful Architecture for Portable Remote Online Experimentation Services},
year = {2018},
isbn = {9781450364690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220267.3220280},
doi = {10.1145/3220267.3220280},
abstract = {In this paper, an architecture is proposed to deliver portable remote online experimentation services. This can benefit the educational and academic sectors in terms of providing remote online accessibility to real experiment setups. Thus, the users can be relieved from geographical and time dependence for the experiment to be conducted. Nowadays, almost all web services leverage the efficiency and prevalence of the REST (Representational State Transfer) architecture. Hence, this proposed remote online service has been implemented in compliance with the RESTful architectural style.Web-based experiments require compatibility with any of the users' portable devices and accessibility at any time. A RESTful architecture can fulfill these requirements. In addition, different experiments can be made available online based on this architecture while sharing the same infrastructure. A case study has been selected to obtain measurements of different force components existing inside wind tunnels. The complete implementation of this system is provided starting from the embedded controller retrieving sensor measurements to the web server development and user interface design.},
booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
pages = {102–105},
numpages = {4},
keywords = {Online testing and experimentation, RESTful architecture, Remote testing facilities},
location = {Cairo, Egypt},
series = {ICSIE '18}
}

@inproceedings{10.1145/3465481.3470018,
author = {Eckel, Michael and Riemann, Tim},
title = {Userspace Software Integrity Measurement},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470018},
doi = {10.1145/3465481.3470018},
abstract = {Todays computing systems are more interconnected and sophisticated than ever before. Especially in healthcare 4.0, services and infrastructures rely on cyber-physical systemss (CPSess) and Internet of Things (IoT) devices. This adds to the complexity of these highly connected systems and their manageability. Even worse, the variety of emerging cyber attacks is becoming more severe and sophisticated, making healthcare one of the most important sectors with major security risks. The development of appropriate countermeasures constitutes one of the most complex and difficult challenges in cyber security research. Research areas include, among others, anomaly detection, network security, multi-layer event detection, cyber resiliency, and integrity protection. Securing the integrity of software running on a device is a desirable protection goal in the context of systems security. With a Trusted Platform Module (TPM), measured boot, and remote attestation there exist technologies to ensure that a system has booted up correctly and runs only authentic software. The Linux Integrity Measurement Architecture (IMA) extends these principles into the operating systems (OSes), measuring native binaries before they are loaded. However, interpreted language files, such as Java classes and Python scripts, are not considered executables and are not measured as such. Contemporary OSess ship with many of these and it is vital to consider them as security-critical as native binaries. In this paper, we introduce Userspace Software Integrity Measurement (USIM) for the Linux OSes. Userspace Software Integrity Measurement (USIM) enables interpreters to measure, log, and irrevocably anchor critical events in the TPM. We develop a software library in C which provides TPM-based measurement functionality as well as the USIM service, which provides concurrent access handling to the TPM based event logging. Further, we develop and implement a concept to realize highly frequent event logging on the slow TPM. We integrate this library into the Java Virtual Machine (JVM) to measure Java classes and show that it can be easily integrated into other interpreters. With performance measurements we demonstrate that our contribution is feasible and that overhead is negligible.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {138},
numpages = {11},
keywords = {Systems security, integrity verification, Trusted Computing},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1145/2742580.2742810,
author = {Karedla, Rama},
title = {Programming for the Intel Xeon Processor},
year = {2015},
isbn = {9781450335270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742580.2742810},
doi = {10.1145/2742580.2742810},
abstract = {Software programmers tend to focus on the software layer leaving performance on the table by not taking advantage of the underlying hardware. This talk will help the programmer take advantage of the underlying Intel Xeon server architecture to write more efficient programs. We broadly cover topics such as time measurement, memory ordering, making efficient use of the multi level caches, NUMA aware programming and the use of the many compute cores available in the Xeon architecture via multi-threading.We hope to show the benefit to both, latency and throughput oriented applications. The talk will also address using the new AVX vector registers to achieve higher performance, and briefly touch upon the recently announced Transactional Synchronization Extensions (TSX) features. Examples of application profiling will demonstrate the benefit of optimizing for performance in parallel with code development.},
booktitle = {Applicative 2015},
location = {New York, NY, USA},
series = {Applicative 2015}
}

@inproceedings{10.1145/2766498.2774989,
author = {Podiyan, Pradeep and Butakov, Sergey and Zavarsky, Pavol},
title = {Study of Compliance of Android Location APIs with Geopriv},
year = {2015},
isbn = {9781450336239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766498.2774989},
doi = {10.1145/2766498.2774989},
abstract = {This paper carefully examines the location APIs of Android OS as well as the Geopriv standard architecture to study measures that are being taken by Android OS to protect the location privacy of a user. Android offers various location APIs in its architecture for the app developers to work on location based services (LBS). The results of this evaluation will be compared with Geopriv standard architecture and its ways to enhance location information privacy on mobile platforms. The review of functionality of location APIs shows that Android has limited features such as Geofencing to have some extent of location privacy for a typical user. Only few of the recommendation in distribution segment of Geopriv with slightly different approach are similar to the protection mechanisms offered by location APIs in Android. The paper proposes general steps that can be taken to address location privacy issues on mobile devices.},
booktitle = {Proceedings of the 8th ACM Conference on Security \&amp; Privacy in Wireless and Mobile Networks},
articleno = {30},
numpages = {2},
keywords = {Geopriv, Android, location privacy},
location = {New York, New York},
series = {WiSec '15}
}

@inproceedings{10.1145/3139531.3139537,
author = {Liu, Ling},
title = {Keynote: Privacy and Trust: Friend or Foe},
year = {2017},
isbn = {9781450353939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139531.3139537},
doi = {10.1145/3139531.3139537},
abstract = {Internet of Things (IoT) and Big Data have fueled the development of fully distributed computational architectures for future cyber systems from data analytics, machine learning (ML) to artificial intelligence (AI). Trust and Privacy become two vital and necessary measures for distributed management of IoT powered big data learning systems and services. However, these two measures have been studied independently in computer science, social science and law.Trust is widely considered as a critical measure for the correctness, predictability, and resiliency (with respect to reliability and security) of software systems, be it big data systems, IoT systems, machine learning systems, or Artificial Intelligence systems. Privacy on the other hand is commonly recognized as a personalization measure for imposing control on the ways of how data is captured, accessed and analyzed, and the ways of how data analytic results from ML models and AI systems should be released and shared.Broadly speaking, in human society, we rely on three types of trust in our everyday work and life to achieve a peaceful mind: (1) verifiable belief-driven trust, (2) statistical evidence based trust, and (3) complex systemwide cognitive trust. Interestingly, privacy has been a more controversial subject. On one hand, privacy is an important built-in dimension of trust, which is deep rooted in human society, and a highly valued virtue in Western civilization. Even though different human beings may have diverse levels of privacy sensitivity, we all trust that our privacy is respected in our social and professional environments, including at home, at work and in social commons. Thus, Privacy is a perfect example of three-fold trust: belief-driven, statistical evident, and complex cognitive trust. On the other hand, many view privacy (and privacy protection) as an antagonistic measure of trust and one is often asked to show trust at the cost of giving up on privacy.Are Privacy and Trust friend or foe? This keynote will share my view to this question from multiple perspectives. I conjecture that the answer to this question can fundamentally change the ways we conduct research in privacy and trust in the next generation of big data enhanced cyber learning systems from data mining, machine learning to artificial intelligence.},
booktitle = {Proceedings of the 2017 Workshop on Women in Cyber Security},
pages = {11},
numpages = {1},
keywords = {privacy, big data, internet of things, trust, deep learning},
location = {Dallas, Texas, USA},
series = {CyberW '17}
}

@inproceedings{10.1145/3288599.3295582,
author = {Shah, Ryan and Nagaraja, Shishir},
title = {Do We Have the Time for IRM? Service Denial Attacks and SDN-Based Defences},
year = {2019},
isbn = {9781450360944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288599.3295582},
doi = {10.1145/3288599.3295582},
abstract = {Distributed sensor networks such as IoT deployments generate large quantities of measurement data. Often, the analytics that runs on this data is available as a web service which can be purchased for a fee. A major concern in the analytics ecosystem is ensuring the security of the data. Often, companies offer Information Rights Management (IRM) as a solution to the problem of managing usage and access rights of the data that transits administrative boundaries. IRM enables individuals and corporations to create restricted IoT data, which can have its flow from organisation to individual control - disabling copying, forwarding, and allowing timed expiry. We describe our investigations into this functionality and uncover a weak-spot in the architecture - its dependence upon the accurate global availability of time. We present an amplified denial-of-service attack which attacks time synchronisation and could prevent all the users in an organisation from reading any sort of restricted data until their software has been re-installed and re-configured. We argue that IRM systems built on current technology will be too fragile for businesses to risk widespread use. We also present defences that leverage the capabilities of Software-Defined Networks to apply a simple filter-based approach to detect and isolate attack traffic.},
booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
pages = {496–501},
numpages = {6},
location = {Bangalore, India},
series = {ICDCN '19}
}

@inproceedings{10.1145/3409390.3409402,
author = {Monfared, Saleh Khalaj and Hajihassani, Omid and Kiarostami, Mohammad Sina and Zanjani, Soroush Meghdadi and Rahmati, Dara and Gorgin, Saeid},
title = {BSRNG: A High Throughput Parallel BitSliced Approach for Random Number Generators},
year = {2020},
isbn = {9781450388689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409390.3409402},
doi = {10.1145/3409390.3409402},
abstract = {In this work, a high throughput method for generating high-quality Pseudo-Random Numbers using the bitslicing technique is proposed. In such a technique, instead of the conventional row-major data representation, column-major data representation is employed, which allows the bitslicing implementation to take full advantage of all the available datapath of the hardware platform. By employing this data representation as building blocks of algorithms, we showcase the capability and scalability of our proposed method in various PRNG methods in the category of block and stream ciphers. The LFSR-based (Linear Feedback Shift Register) nature of the PRNG in our implementation perfectly suits the GPU’s many-core structure due to its register oriented architecture. In the proposed SIMD vectorized GPU implementation, each GPU thread can generate several 32 pseudo-random bits in each LFSR clock cycle. We then compare our implementation with some of the most significant PRNGs that display a satisfactory performance throughput and randomness criteria. The proposed implementation successfully passes the NIST test for statistical randomness and bit-wise correlation criteria. For computer-based PRNG and the optical solutions in terms of performance and performance per cost, this technique is efficient while maintaining an acceptable randomness measure. Our highest performance among all of the implemented CPRNGs with the proposed method is achieved by the MICKEY 2.0 algorithm, which shows 40\% improvement over state of the art NVIDIA’s proprietary high-performance PRNG, cuRAND library, achieving 2.72 Tb/s of throughput on the affordable NVIDIA GTX 2080 Ti.},
booktitle = {Workshop Proceedings of the 49th International Conference on Parallel Processing},
articleno = {12},
numpages = {10},
keywords = {CUDA, PRNG, High-performance, cuRAND, Stream cipher, Bitslicing, Cryptography},
location = {Edmonton, AB, Canada},
series = {ICPP Workshops '20}
}

@inproceedings{10.1145/2609248.2609264,
author = {Lazarescu, Mihai T. and Cohen, Albert and Guatto, Adrien and L\^{e}, Nhat Minn and Lavagno, Luciano and Pop, Antoniu and Prieto, Manuel and Terechko, Andrei and Sutii, Alexandru},
title = {Energy-Aware Parallelization Flow and Toolset for C Code},
year = {2014},
isbn = {9781450329415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609248.2609264},
doi = {10.1145/2609248.2609264},
abstract = {Multicore architectures are increasingly used in embedded systems to achieve higher throughput with lower energy consumption. This trend accentuates the need to convert existing sequential code to effectively exploit the resources of these architectures. We present a parallelization flow and toolset for legacy C code that includes a performance estimation tool, a parallelization tool, and a streaming-oriented parallelization framework. These are part of the work-in-progress EU FP7 PHARAON project that aims to develop a complete set of techniques and tools to guide and assist software development for heterogeneous parallel architectures. We demonstrate the effectiveness of the use of the toolset in an experiment where we measure the parallelization quality and time for inexperienced users, and the parallelization flow and performance results for the parallelization of a practical example of a stereo vision application.},
booktitle = {Proceedings of the 17th International Workshop on Software and Compilers for Embedded Systems},
pages = {79–88},
numpages = {10},
keywords = {program parallelization, execution profiling, energy estimation, data dependency analysis},
location = {Sankt Goar, Germany},
series = {SCOPES '14}
}

@inproceedings{10.1145/2642668.2642673,
author = {Hatoum, Rima and Hatoum, Abbas and Ghaith, Alaa and Pujolle, Guy},
title = {Qos-Based Joint Resource Allocation with Link Adaptation for SC-FDMA Uplink in Heterogeneous Networks},
year = {2014},
isbn = {9781450330268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642668.2642673},
doi = {10.1145/2642668.2642673},
abstract = {The LTE-based femtocell network is a promising solution adopted today to cope with the huge cellular traffic requirements. In particular, the Uplink communication becomes an attractive issue especially with the emerging of the interactive services and large uploaded data volume. Intelligent allocation of the resources and interference management are the main challenges in such context. In this paper, we propose a linear optimization model for the SC-FDMA Uplink transmission aiming to adaptively allocate resources with respect to the link quality. Both power and modulation and coding schemes are independently assigned to each user over each allocated sub-channel. The cluster architecture is adopted as a hybrid centralized/distributed network. The user differentiation strategy ensures the QoS guarantee with respect to a priority level of each user. Taking into account the specifications of the uplink communication, we confirm through comparative simulations the outperformance of our proposal considering several metrics such as throughput satisfaction rate, transmitted power, outage probability, special spectrum reuse and others.},
booktitle = {Proceedings of the 12th ACM International Symposium on Mobility Management and Wireless Access},
pages = {59–66},
numpages = {8},
keywords = {link adaptation, interference mitigation, resource allocation, SC-FDMA-femtocell, uplink, QoS},
location = {Montreal, QC, Canada},
series = {MobiWac '14}
}

@inproceedings{10.1145/3384217.3386393,
author = {Petz, Adam},
title = {An Infrastructure for Faithful Execution of Remote Attestation Protocols},
year = {2020},
isbn = {9781450375610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384217.3386393},
doi = {10.1145/3384217.3386393},
abstract = {Experience shows that even with a well-intentioned user at the keyboard, a motivated attacker can compromise a computer system at a layer below or adjacent to the shallow forms of authentication that are now accepted as commonplace[3]. Therefore, rather than asking "Can we trust the person behind the keyboard", a still better question might be: "Can we trust the computer system underneath?". An emerging technology for gaining trust in a remote computing system is remote attestation. Remote attestation is the activity of making a claim about properties of a target by supplying evidence to an appraiser over a network[2]. Although many existing approaches to remote attestation wisely adopt a layered architecture-where the bottom layers measure layers above-the dependencies between components remain static and measurement orderings fixed. For modern computing environments with diverse topologies, we can no longer fix a target architecture any more than we can fix a protocol to measure that architecture.Copland [1] is a domain-specific language and formal framework that provides a vocabulary for specifying the goals of layered attestation protocols. It also provides a reference semantics that characterizes system measurement events and evidence handling; a foundation for comparing protocol alternatives. The aim of this work is to refine the Copland semantics to a more fine-grained notion of attestation manager execution-a high-privilege thread of control responsible for invoking attestation services and bundling evidence results. This refinement consists of two cooperating components called the Copland Compiler and the Attestation Virtual Machine (AVM). The Copland Compiler translates a Copland protocol description into a sequence of primitive attestation instructions to be executed in the AVM. When considered in combination with advances in virtualization, trusted hardware, and high-assurance system software components-like compilers, file-systems, and OS kernels-a formally verified remote attestation infrastructure creates exciting opportunities for building system-level security arguments.},
booktitle = {Proceedings of the 7th Symposium on Hot Topics in the Science of Security},
articleno = {17},
numpages = {1},
location = {Lawrence, Kansas},
series = {HotSoS '20}
}

@inproceedings{10.1145/3318265.3318280,
author = {Hang, Zijun and Shi, Yang and Wen, Mei and Quan, Wei and Zhang, Chunyuan},
title = {SWAP: A Sliding Window Algorithm for in-Network Packet Measurement},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318280},
doi = {10.1145/3318265.3318280},
abstract = {Network traffic measurement is a fundamental part of many network applications, such as DDOS detection, capacity planning, and quality-of-service improvement. To achieve this, we need to count the number of packets passed during a past time interval. Traditionally, switches sample the packets and send them to the CPU for analysis. It is unavoidable that the sampling will sacrifice the measuring accuracy. Nowadays, programmable switches can keep the counters in the data plane. However, they still rely on the CPU to drain and clear the records periodically, which brings in too much communication latency. To overcome these disadvantages, we propose a metering mechanism under the RMT architectural model called SWAP. SWAP is carefully designed to count the number of packets during an interval accurately with little hardware resource usage. We prototype it using P4 and simulation results show SWAP achieves high efficiency and moderate accuracy at line speed.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {84–89},
numpages = {6},
keywords = {P4, network algorithm, software-defined networks, programmable switches},
location = {Xi'an, China},
series = {HP3C '19}
}

@inproceedings{10.1145/2985766.2985772,
author = {Edoh, Thierry Oscar C. and Atchome, Athanase and Alahassa, Bidossessi R.U. and Pawar, Pravin},
title = {Evaluation of a Multi-Tier Heterogeneous Sensor Network for Patient Monitoring: The Case of Benin},
year = {2016},
isbn = {9781450345187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2985766.2985772},
doi = {10.1145/2985766.2985772},
abstract = {In this paper we propose and evaluate a wireless sensor network (WSN) system in order to improve an existing patient-monitoring and surveillance system at the cardiologic intensive care unit (CICU) of a large university clinic (Centre Hospitalier Universitaire Hubert Koutoukou Maga - CHU-HKM) in Cotonou city of Benin (a West-African country). We have designed a multi-tier architecture and simulated a heterogeneous, autonomous, and energy efficient wireless sensor network system to overcome issues faced by existing patient monitoring system in CICU such as manual collection and processing of data. The improvement of the patient monitoring system has the objectives of providing affordable and better health care service provision as well as autonomous and automatic collection and processing of patient's bio-signals and environmental data. The proposed Wireless Sensor Network consists of wireless heterogeneous nodes which sense patient bio-signals, measure environmental parameters in the hospitalization rooms such as ambient temperature, quality of air and send collected data to a gateway (central node) for processing and storage. The conducted simulation experiments show that the proposed sensor network architecture which uses ZigBee wireless standard and protocol highly improves the patience monitoring and surveillance experience at CICU. It promotes collection and autonomous processing of patient physiological data and room ambient temperature data. Incorporating such system in CICU will be highly beneficial for taking a correct decision during treatment. Beyond the accuracy and quality of the collected medical data, proposed WSN is also designed to reduce the energy consumption within the sensor network system.},
booktitle = {Proceedings of the 2016 ACM Workshop on Multimedia for Personal Health and Health Care},
pages = {23–29},
numpages = {7},
keywords = {cardiology, patient monitorin, intensive care unit, zigbee standards, cooperative sensors, wireless sensors network},
location = {Amsterdam, The Netherlands},
series = {MMHealth '16}
}

@article{10.1109/TNET.2018.2793892,
author = {Araldo, Andrea and Dan, Gyorgy and Rossi, Dario},
title = {Caching Encrypted Content Via Stochastic Cache Partitioning},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2793892},
doi = {10.1109/TNET.2018.2793892},
abstract = {In-network caching is an appealing solution to cope with the increasing bandwidth demand of video, audio, and data transfer over the Internet. Nonetheless, in order to protect consumer privacy and their own business, content providers CPs increasingly deliver encrypted content, thereby preventing Internet service providers ISPs from employing traditional caching strategies, which require the knowledge of the objects being transmitted. To overcome this emerging tussle between security and efficiency, in this paper we propose an architecture in which the ISP partitions the cache space into slices, assigns each slice to a different CP, and lets the CPs remotely manage their slices. This architecture enables transparent caching of encrypted content and can be deployed in the very edge of the ISP’s network i.e., base stations and femtocells, while allowing CPs to maintain exclusive control over their content. We propose an algorithm, called SDCP, for partitioning the cache storage into slices so as to maximize the bandwidth savings provided by the cache. A distinctive feature of our algorithm is that ISPs only need to measure the aggregated miss rates of each CP, but they need not know the individual objects that are requested. We prove that the SDCP algorithm converges to a partitioning that is close to the optimal, and we bound its optimality gap. We use simulations to evaluate SDCP’s convergence rate under stationary and nonstationary content popularity. Finally, we show that SDCP significantly outperforms traditional reactive caching techniques, considering both CPs with perfect and with imperfect knowledge of their content popularity.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {548–561},
numpages = {14}
}

@article{10.1145/3372136,
author = {Lao, Laphou and Li, Zecheng and Hou, Songlin and Xiao, Bin and Guo, Songtao and Yang, Yuanyuan},
title = {A Survey of IoT Applications in Blockchain Systems: Architecture, Consensus, and Traffic Modeling},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3372136},
doi = {10.1145/3372136},
abstract = {Blockchain technology can be extensively applied in diverse services, including online micro-payments, supply chain tracking, digital forensics, health-care record sharing, and insurance payments. Extending the technology to the Internet of things (IoT), we can obtain a verifiable and traceable IoT network. Emerging research in IoT applications exploits blockchain technology to record transaction data, optimize current system performance, or construct next-generation systems, which can provide additional security, automatic transaction management, decentralized platforms, offline-to-online data verification, and so on. In this article, we conduct a systematic survey of the key components of IoT blockchain and examine a number of popular blockchain applications.In particular, we first give an architecture overview of popular IoT-blockchain systems by analyzing their network structures and protocols. Then, we discuss variant consensus protocols for IoT blockchains, and make comparisons among different consensus algorithms. Finally, we analyze the traffic model for P2P and blockchain systems and provide several metrics. We also provide a suitable traffic model for IoT-blockchain systems to illustrate network traffic distribution.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {18},
numpages = {32},
keywords = {traffic modeling, IoT, consensus, architecture, Blockchain}
}

@inproceedings{10.1145/3405837.3411375,
author = {Khooi, Xin Zhe and Csikor, Levente and Kang, Min Suk and Divakaran, Dinil Mon},
title = {In-Network Defense against AR-DDoS Attacks},
year = {2021},
isbn = {9781450380485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405837.3411375},
doi = {10.1145/3405837.3411375},
abstract = {The prevalence of the disruptive amplified reflection DDoS (AR-DDoS) attacks is one of the biggest concerns of all network operators today. The increasing magnitude of new attacks are rendering existing measures (e.g., scrubbing services) inefficient. This work demonstrates DIDA, an efficient, topology independent, in-line AR-DDoS detection and mitigation architecture that operates entirely in the data plane.},
booktitle = {Proceedings of the SIGCOMM '20 Poster and Demo Sessions},
pages = {18–20},
numpages = {3},
keywords = {in-network, detection and mitigation, amplification attacks, denial-of-service attacks, reflection attacks, programmable switches},
location = {Virtual event},
series = {SIGCOMM '20}
}

@inproceedings{10.5555/2893711.2893715,
author = {Rauter, Tobias and H\"{o}ller, Andrea and Iber, Johannes and Kreiner, Christian},
title = {Thingtegrity: A Scalable Trusted Computing Architecture for the Internet of Things},
year = {2016},
isbn = {9780994988607},
publisher = {Junction Publishing},
address = {USA},
abstract = {Remote attestation is used to prove the integrity of one system (prover) to another (challenger). The prover measures its configuration and transmits the result to the challenger for verification. Common attestation methods lead to complex configuration measurements (e.g., hash of all executables), which are updated every time one of the software modules changes. The updated configuration has to be distributed to all possible challengers since they need a reference to enable the verification. Recently, an idea of reducing the complexity of the configuration measurement by taking into account privileges of software modules has been presented. However, this approach has not been exhaustively analyzed since, as yet, no implementation exists. Especially in the Internet of Things (IoT) domain, where resources are constrained strictly while devices are potentially physically exposed to adversaries, attestation methodologies with reduced overhead are desireable. In this work we combine binary-, property- and privilege-based remote attestation to integrate a trusted computing architecture transparently into iotivity, an existing IoT middleware. As a first step, we aim to enable to attestation of the integrity of complex devices with different services to constrained devices. With the help of an illustrative simulated environment, we show that our architecture reduces the effort of bootstrapping trusted relations, as well as updating single modules in the whole system, even if software and devices from different vendors are combined.},
booktitle = {Proceedings of the 2016 International Conference on Embedded Wireless Systems and Networks},
pages = {23–34},
numpages = {12},
location = {Graz, Austria},
series = {EWSN '16}
}

@inproceedings{10.1145/3210259.3210262,
author = {Erb, Benjamin and Mei\ss{}ner, Dominik and Kargl, Frank and Steer, Benjamin A. and Cuadrado, Felix and Margan, Domagoj and Pietzuch, Peter},
title = {Graphtides: A Framework for Evaluating Stream-Based Graph Processing Platforms},
year = {2018},
isbn = {9781450356954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210259.3210262},
doi = {10.1145/3210259.3210262},
abstract = {Stream-based graph systems continuously ingest graph-changing events via an established input stream, performing the required computation on the corresponding graph. While there are various benchmarking and evaluation approaches for traditional, batch-oriented graph processing systems, there are no common procedures for evaluating stream-based graph systems. We, therefore, present GraphTides, a generic framework which includes the definition of an appropriate system model, an exploration of the parameter space, suitable workloads, and computations required for evaluating such systems. Furthermore, we propose a methodology and provide an architecture for running experimental evaluations. With our framework, we hope to systematically support system development, performance measurements, engineering, and comparisons of stream-based graph systems.},
booktitle = {Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences \&amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {3},
numpages = {10},
keywords = {evolving graphs, evaluation, temporal graphs, stream-based graphs, measurements, graph processing, graph analytics},
location = {Houston, Texas},
series = {GRADES-NDA '18}
}

@inproceedings{10.1145/3414045.3415941,
author = {Wazid, Mohammad and Bera, Basudeb and Mitra, Ankush and Das, Ashok Kumar and Ali, Rashid},
title = {Private Blockchain-Envisioned Security Framework for AI-Enabled IoT-Based Drone-Aided Healthcare Services},
year = {2020},
isbn = {9781450381055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414045.3415941},
doi = {10.1145/3414045.3415941},
abstract = {Internet of Drones (IoD) architecture is designed to support a co-ordinated access for the airspace using the unmanned aerial vehicles (UAVs) known as drones. Recently, IoD communication environment is extremely useful for various applications in our daily activities. Artificial intelligence (AI)-enabled Internet of Things (IoT)-based drone-aided healthcare service is a specialized environment which can be used for different types of tasks, for instance, blood and urine samples collections, medicine delivery and for the delivery of other medical needs including the current pandemic of COVID-19. Due to wireless nature of communication among the deployed drones and their ground station server, several attacks (for example, replay, man-in-the-middle, impersonation and privileged-insider attacks) can be easily mounted by malicious attackers. To protect such attacks, the deployment of effective authentication, access control and key management schemes are extremely important in the IoD environment. Furthermore, combining the blockchain mechanism with deployed authentication make it more robust against various types of attacks. To mitigate such issues, we propose a private-blockchain based framework for secure communication in an IoT-enabled drone-aided healthcare environment. The blockchain-based simulation of the proposed framework has been carried out to measure its impact on various performance parameters.},
booktitle = {Proceedings of the 2nd ACM MobiCom Workshop on Drone Assisted Wireless Communications for 5G and Beyond},
pages = {37–42},
numpages = {6},
keywords = {healthcare, authentication, security, blockchain, privacy, internet of drones (IoD)},
location = {London, United Kingdom},
series = {DroneCom '20}
}

@inproceedings{10.1145/2676723.2691890,
author = {Hoffman, Mark E.},
title = {Student Board-Writing to Integrate Communication Skills and Content to Enhance Student Learning (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2691890},
doi = {10.1145/2676723.2691890},
abstract = {Students frequently use a whiteboard to individually demonstrate understanding or interactively develop understanding in groups. The practice is employed to develop content knowledge; however, an opportunity to intentionally develop communication skills is overlooked. On the other hand, instructors carefully integrate instructional organization and communication to maximize student content learning. Taken together, this presents an opportunity for students to intentionally improve their communication skills in the service of content learning. This poster details a "work in progress" project where students follow organizational guidelines for written homework and board-writing to facilitate in-class, problem solution presentation. Problem solution presentations occur during one class period each week. Students are given colored pencils for written homework and colored markers for board-writing. Student work including written homework and board-writing was gathered from the 2013 and 2014 iterations of a sophomore-level computer architecture course. Preliminary analysis of student work shows that students either adopt the guidelines from the start or learn to use them through feedback and practice. On the semester-end survey, students report that adopting guidelines for written homework, board-writing, and color scheme improve presentation, and board-writing improves student learning. Future work includes gathering data from more students including recorded student presentations, developing quantitative scores to analyze student work, and developing measures of student learning.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {684},
numpages = {1},
keywords = {student board-writing, content learning, communication skills},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1145/2641798.2641814,
author = {Xu, Yi and Helal, Sumi},
title = {Application Caching for Cloud-Sensor Systems},
year = {2014},
isbn = {9781450330305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641798.2641814},
doi = {10.1145/2641798.2641814},
abstract = {Driven by critical and pressing smart city applications, accessing massive numbers of sensors by cloud-hosted services is becoming an emerging and inevitable situation. Na\"{\i}vely connecting massive numbers of sensors to the cloud raises major scalability and energy challenges. An architecture embodying distributed optimization is needed to manage the scale and to allow limited energy sensors to last longer in such a dynamic and high-velocity big data system. We developed a multi-tier architecture which we call Cloud, Edge and Beneath (CEB). Based on CEB, we propose an Application Fragment Caching Algorithm (AFCA) which selectively caches application fragments from the cloud to lower layers of CEB to improve cloud scalability. Through experiments, we show and measure the effect of AFCA on cloud scalability.},
booktitle = {Proceedings of the 17th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {303–306},
numpages = {4},
keywords = {cloud-sensor systems, application caching, cloud computing},
location = {Montreal, QC, Canada},
series = {MSWiM '14}
}

@inproceedings{10.1145/3308558.3313551,
author = {Yoon, Changhoon and Kim, Kwanwoo and Kim, Yongdae and Shin, Seungwon and Son, Sooel},
title = {Doppelg\"{a}ngers on the Dark Web: A Large-Scale Assessment on Phishing Hidden Web Services},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313551},
doi = {10.1145/3308558.3313551},
abstract = {Anonymous network services on the World Wide Web have emerged as a new web architecture, called the Dark Web. The Dark Web has been notorious for harboring cybercriminals abusing anonymity. At the same time, the Dark Web has been a last resort for people who seek freedom of the press as well as avoid censorship. This anonymous nature allows website operators to conceal their identity and thereby leads users to have difficulties in determining the authenticity of websites. Phishers abuse this perplexing authenticity to lure victims; however, only a little is known about the prevalence of phishing attacks on the Dark Web. We conducted an in-depth measurement study to demystify the prevalent phishing websites on the Dark Web. We analyzed the text content of 28,928 HTTP Tor hidden services hosting 21 million dark webpages and confirmed 901 phishing domains. We also discovered a trend on the Dark Web in which service providers perceive dark web domains as their service brands. This trend exacerbates the risk of phishing for their service users who remember only a partial Tor hidden service address. Our work facilitates a better understanding of the phishing risks on the Dark Web and encourages further research on establishing an authentic and reliable service on the Dark Web.},
booktitle = {The World Wide Web Conference},
pages = {2225–2235},
numpages = {11},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3286062.3286086,
author = {Sharma, Puneet and Raghuramu, Arun and Lee, David and Saxena, Vinay and Chuah, Chen-Nee},
title = {We Don't Need No Licensing Server},
year = {2018},
isbn = {9781450361200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286062.3286086},
doi = {10.1145/3286062.3286086},
abstract = {Cloudification of edge to core infrastructure has led to new and rich application and service deployment and operational models. These ecosystems have complex relationships between the application vendors, infrastructure operators and application users. Traditional licensing and compliance enforcement methods such as those based on in person audits and dynamic issuing of license keys inhibit the resource provisioning and consumption flexibility offered by cloudified services due to scalability and management overheads. In this work, we argue the need for a trusted framework for application usage rights compliance. This new architecture named "Metered Boot" provides a way to realize trusted, capacity/usage based rights compliance for service deployments that allows decoupling of usage rights governed by application vendors from the resource provisioning by the infrastructure provider. We have built a Metered Boot prototype for a particular usecase of NFV usage rights compliance.},
booktitle = {Proceedings of the 17th ACM Workshop on Hot Topics in Networks},
pages = {162–168},
numpages = {7},
location = {Redmond, WA, USA},
series = {HotNets '18}
}

@inproceedings{10.1145/3319535.3363279,
author = {Mo, Fan and Shahin Shamsabadi, Ali and Katevas, Kleomenis and Cavallaro, Andrea and Haddadi, Hamed},
title = {Poster: Towards Characterizing and Limiting Information Exposure in DNN Layers},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363279},
doi = {10.1145/3319535.3363279},
abstract = {Pre-trained Deep Neural Network (DNN) models are increasingly used in smartphones and other user devices to enable prediction services, leading to potential disclosures of (sensitive) information from training data captured inside these models. Based on the concept of generalization error, we propose a framework to measure the amount of sensitive information memorized in each layer of a DNN. Our results show that, when considered individually, the last layers encode a larger amount of information from the training data compared to the first layers. We find that the same DNN architecture trained with different datasets has similar exposure per layer. We evaluate an architecture to protect the most sensitive layers within an on-device Trusted Execution Environment (TEE) against potential white-box membership inference attacks without the significant computational overhead.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2653–2655},
numpages = {3},
keywords = {sensitive information exposure, privacy, deep learning, trusted execution environment, training data},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1145/3014431,
author = {Hu, Han and Wen, Yonggang and Chua, Tat-Seng and Li, Xuelong},
title = {Cost-Optimized Microblog Distribution over Geo-Distributed Data Centers: Insights from Cross-Media Analysis},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3014431},
doi = {10.1145/3014431},
abstract = {The unprecedent growth of microblog services poses significant challenges on network traffic and service latency to the underlay infrastructure (i.e., geo-distributed data centers). Furthermore, the dynamic evolution in microblog status generates a huge workload on data consistence maintenance. In this article, motivated by insights of cross-media analysis-based propagation patterns, we propose a novel cache strategy for microblog service systems to reduce the inter-data center traffic and consistence maintenance cost, while achieving low service latency. Specifically, we first present a microblog classification method, which utilizes the external knowledge from correlated domains, to categorize microblogs. Then we conduct a large-scale measurement on a representative online social network system to study the category-based propagation diversity on region and time scales. These insights illustrate social common habits on creating and consuming microblogs and further motivate our architecture design. Finally, we formulate the content cache problem as a constrained optimization problem. By jointly using the Lyapunov optimization framework and simplex gradient method, we find the optimal online control strategy. Extensive trace-driven experiments further demonstrate that our algorithm reduces the system cost by 24.5\% against traditional approaches with the same service latency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {40},
numpages = {18},
keywords = {Social media analytics, data center, cross-media analysis, performance optimization}
}

@inproceedings{10.1145/3019612.3019724,
author = {Lautner, Douglas and Hua, Xiayu and Debates, Scott and Song, Miao and Shah, Jagat and Ren, Shangping},
title = {BaaS (Bluetooth-as-a-Sensor): Conception, Design and Implementation on Mobile Platforms},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019724},
doi = {10.1145/3019612.3019724},
abstract = {As network connectivity becomes more capable, mobile devices are evolving into sensory data accumulators. Bluetooth (BT) components, which are widely used for communication purposes, also have the potential to become contextual sensors by constantly listening to information broadcast by nearby Bluetooth Low Energy (BLE) beacons. Compared to traditional Micro-Electro-Mechanical (MEMs) based contextual sensors, Bluetooth-as-a-Sensor (BaaS) provides a wider sensing spectrum and more comprehensive environmental information. However, current implementations of BT are optimized as a data transmitter, therefore deploying BaaS on a traditional mobile platform would cause an unacceptably high current drain and hence a significant reduction in battery life. Our objective is to conquer the current drain problem associated with having continuous wireless BT sensing. We provide a novel BaaS-based architecture which utilizes an energy-efficient sensor fusion core (SFC) to execute heavy-duty and long-standing tasks. We also present an optimized duty cycle algorithm that minimizes the duty cycle while guaranteeing an application's QoS requirements. Both BaaS architecture and algorithm are implemented and deployed on a Moto X platform and then applied to an indoor location service for consumer use validation. The performance of the BaaS-based architecture is evaluated for both average current drain and location accuracy. Data measured on Moto X shows that when using the BaaS architecture, the battery life is 5 times longer than using the traditional BT architecture.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {550–556},
numpages = {7},
keywords = {embedded system, cellphone development, mobile sensing, mobile device, energy efficiency},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/3219104.3219148,
author = {Ruan, Guangchen and Wernert, Eric and Gniady, Tassie and Tuna, Esen and Sherman, William},
title = {High Performance Photogrammetry for Academic Research},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3219148},
doi = {10.1145/3219104.3219148},
abstract = {Photogrammetry is the process of computationally extracting a three-dimensional surface model from a set of two-dimensional photographs of an object or environment. It is used to build models of everything from terrains to statues to ancient artifacts. In the past, the computational process was done on powerful PCs and could take weeks for large datasets. Even relatively small objects often required many hours of compute time to stitch together. With the availability of parallel processing options in the latest release of state-of-the-art photogrammetry software, it is possible to leverage the power of high performance computing systems on large datasets. In this paper we present a particular implementation of a high performance photogrammetry service. Though the service is currently based on a specific software package (Agisoft's PhotoScan), our system architecture is designed around a general photogrammetry process that can be easily adapted to leverage other photogrammetry tools. In addition, we report on an extensive performance study that measured the relative impacts of dataset size, software quality settings, and processing cluster size. Furthermore, we share lessons learned that are useful to system administrators looking to establish a similar service, and we describe the user-facing support components that are crucial for the success of the service.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {45},
numpages = {8},
keywords = {HPC, scalability, distributed processing, benchmarking, photogrammetry, performance evaluation},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@article{10.1145/2699503,
author = {Patrignani, Marco and Agten, Pieter and Strackx, Raoul and Jacobs, Bart and Clarke, Dave and Piessens, Frank},
title = {Secure Compilation to Protected Module Architectures},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/2699503},
doi = {10.1145/2699503},
abstract = {A fully abstract compiler prevents security features of the source language from being bypassed by an attacker operating at the target language level. Unfortunately, developing fully abstract compilers is very complex, and it is even more so when the target language is an untyped assembly language. To provide a fully abstract compiler that targets untyped assembly, it has been suggested to extend the target language with a protected module architecture—an assembly-level isolation mechanism which can be found in next-generation processors. This article provides a fully abstract compilation scheme whose source language is an object-oriented, high-level language and whose target language is such an extended assembly language. The source language enjoys features such as dynamic memory allocation and exceptions. Secure compilation of first-order method references, cross-package inheritance, and inner classes is also presented. Moreover, this article contains the formal proof of full abstraction of the compilation scheme. Measurements of the overhead introduced by the compilation scheme indicate that it is negligible.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {apr},
articleno = {6},
numpages = {50},
keywords = {protected module architecture, Fully abstract compilation}
}

@inproceedings{10.1145/3286719.3286727,
author = {Coroller, Stevan and Chabridon, Sophie and Laurent, Maryline and Conan, Denis and Leneutre, Jean},
title = {Position Paper: Towards End-to-End Privacy for Publish/Subscribe Architectures in the Internet of Things},
year = {2018},
isbn = {9781450361187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286719.3286727},
doi = {10.1145/3286719.3286727},
abstract = {The Internet of Things paradigm lacks end-to-end privacy solutions to consider its full adoption in real life scenarios in the near future. The recent enactment of the EU General Data Protection Regulation (GDPR) indeed emphasises the need for stronger security and privacy measures for personal data processing and free movement, including consent management and accountability by the data controller and processor. In this paper, we suggest an architecture to enforce end-to-end data usage control in Distributed Event-Based Systems (DEBS), from data producers to consumer services, taking into account some of the GDPR requirements concerning consent management and data processing transparency. Our architecture proposal is based on UCONABC usage control models, which we overlap with a distributed hash table overlay for scalability and fault-tolerance concerns, and across and within systems data usage control. Our proposal highlights the benefits of combining both DEBS and end-user usage control architectures. To complete our approach, we quickly survey existing encryption models that ensure data confidentiality in topic-based Publish/Subscribe systems and highlight the remaining obstacles to transpose them to content-based DEBS with an overlay of brokers.},
booktitle = {Proceedings of the 5th Workshop on Middleware and Applications for the Internet of Things},
pages = {35–40},
numpages = {6},
keywords = {Content-based Distributed Event-Based Systems, Usage Control, Privacy, IoT},
location = {Rennes, France},
series = {M4IoT'18}
}

@inproceedings{10.5555/3195638.3195647,
author = {Caulfield, Adrian M. and Chung, Eric S. and Putnam, Andrew and Angepat, Hari and Fowers, Jeremy and Haselman, Michael and Heil, Stephen and Humphrey, Matt and Kaur, Puneet and Kim, Joo-Young and Lo, Daniel and Massengill, Todd and Ovtcharov, Kalin and Papamichael, Michael and Woods, Lisa and Lanka, Sitaram and Chiou, Derek and Burger, Doug},
title = {A Cloud-Scale Acceleration Architecture},
year = {2016},
publisher = {IEEE Press},
abstract = {Hyperscale datacenter providers have struggled to balance the growing need for specialized hardware (efficiency) with the economic benefits of homogeneity (manageability). In this paper we propose a new cloud architecture that uses reconfigurable logic to accelerate both network plane functions and applications. This Configurable Cloud architecture places a layer of reconfigurable logic (FPGAs) between the network switches and the servers, enabling network flows to be programmably transformed at line rate, enabling acceleration of local applications running on the server, and enabling the FPGAs to communicate directly, at datacenter scale, to harvest remote FPGAs unused by their local servers. We deployed this design over a production server bed, and show how it can be used for both service acceleration (Web search ranking) and network acceleration (encryption of data in transit at high-speeds). This architecture is much more scalable than prior work which used secondary rack-scale networks for inter-FPGA communication. By coupling to the network plane, direct FPGA-to-FPGA messages can be achieved at comparable latency to previous work, without the secondary network. Additionally, the scale of direct inter-FPGA messaging is much larger. The average round-trip latencies observed in our measurements among 24, 1000, and 250,000 machines are under 3, 9, and 20 microseconds, respectively. The Configurable Cloud architecture has been deployed at hyperscale in Microsoft's production datacenters worldwide.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {7},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.1145/3424978.3425039,
author = {Li, Hailing and Zhang, Xiaohang and Cao, Shoufeng and He, Longtao and Zhang, Hui},
title = {Active Measurement of Open Resolver Service Nodes},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425039},
doi = {10.1145/3424978.3425039},
abstract = {Driven by the growing number of DNS requests on the Internet, the architecture of the recursive resolver has become more huge and complex, especially for open resolvers that provide resolution services to the public. There are many service nodes with different roles in the open resolver, and the nodes that directly communicate with the authoritative server are called recursive egress nodes. This paper proposed a distributed measurement system and performed active measurement and analysis on the characteristics of the egress node of open resolvers collected from passive DNS traffic and third party active scanning. The results from 65 vantage points show that (1) most open resolvers have dozens of recursive egress nodes, and (2) most open resolvers have deployed at least one IPv6 address egress node, while IPv4 address still dominates the service node configuration. (3) A small amount of recursive egress nodes is reused by a large number of open resolvers, so that a large amount of DNS request traffic on the Internet is concentrated on limited recursive egress nodes, which will reduce the redundancy of DNS and cause cyber security risks. (4) The median distances between most open resolvers with multiple egress nodes and the users usually exceed 1000 kilometers, which will bring negative effect on the scheduling accuracy of CDN.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {61},
numpages = {8},
keywords = {Open resolver, Distributed active measurement, Recursive egress node, DNS redirection},
location = {Sanya, China},
series = {CSAE '20}
}

@inproceedings{10.1145/3364544.3364824,
author = {Faye, S\'{e}bastien and Melakessou, Foued and Mtalaa, Wassila and Gautier, Prune and AlNaffakh, Neamah and Khadraoui, Djamel},
title = {SWAM: A Novel Smart Waste Management Approach for Businesses Using IoT},
year = {2019},
isbn = {9781450370158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364544.3364824},
doi = {10.1145/3364544.3364824},
abstract = {The waste recycling industry has grown considerably in the recent years and many solutions have become democratized around smart waste collection. However, existing decision support systems generally rely on a limited flow of information and offer an often static or statistically based approach, focusing on specific use-cases (e.g., individuals, municipalities). This paper introduces SWAM, a new smart waste collection platform currently being elaborated in Luxembourg that targets businesses and large entities (e.g., restaurants, shopping centers). SWAM aims to consider multiple sources of combined information in its decision-making process and go further in the routing optimization process. The platform notably uses ultrasonic sensors to measure the filling level of containers, and smartphones with embedded intelligence to understand a driver's actions. This paper presents our experience on the development and deployment of this platform in Luxembourg, as well as the relevant options on the choice of sensing and communication technologies available in the market. It also presents the system architecture and two fundamental components. Firstly, a data management layer, which implements models for analyzing and predicting the filling patterns of the containers. Secondly, a multi-objective optimization layer, which compiles the collection routes that minimize the impact on the environment and maximize the service quality. This paper is intended to serve as a practical reference for the deployment of waste management systems, which have many technological components and can greatly fluctuate depending on the use cases to be covered.},
booktitle = {Proceedings of the 1st ACM International Workshop on Technology Enablers and Innovative Applications for Smart Cities and Communities},
pages = {38–45},
numpages = {8},
keywords = {WSN, Smart Waste Collection, Multi-Objective Optimization, IoT},
location = {New York, NY, USA},
series = {TESCA'19}
}

@inproceedings{10.5555/2665671.2665678,
author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
title = {A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services},
year = {2014},
isbn = {9781479943944},
publisher = {IEEE Press},
abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurablefabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6x8 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cablesIn this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system when ranking candidate documents. Under high load, the largescale reconfigurable fabric improves the ranking throughput of each server by a factor of 95\% for a fixed latency distribution--- or, while maintaining equivalent throughput, reduces the tail latency by 29\%},
booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
pages = {13–24},
numpages = {12},
location = {Minneapolis, Minnesota, USA},
series = {ISCA '14}
}

@article{10.1145/2678373.2665678,
author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
title = {A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2678373.2665678},
doi = {10.1145/2678373.2665678},
abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurablefabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6x8 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cablesIn this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system when ranking candidate documents. Under high load, the largescale reconfigurable fabric improves the ranking throughput of each server by a factor of 95\% for a fixed latency distribution--- or, while maintaining equivalent throughput, reduces the tail latency by 29\%},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {13–24},
numpages = {12}
}

@article{10.1145/3151123.3151125,
author = {Zeinalipour-Yazti, Demetrios and Laoudias, Christos},
title = {The Anatomy of the Anyplace Indoor Navigation Service},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3151123.3151125},
doi = {10.1145/3151123.3151125},
abstract = {The pervasiveness of smartphones is leading to the uptake of a new class of Internet-based Indoor Navigation (IIN) services, which might soon diminish the need of Satellite-based localization technologies in urban environments. These services rely on geo-location databases that store spatial models along with wireless, light and magnetic signals used to localize users and provide better power efficiency and wider coverage than predominant approaches. In this article we overview Anyplace, an open, modular, extensible and scalable navigation architecture that exploits crowdsourced Wi-Fi data to develop a novel navigation service that won several international research awards for its utility and accuracy (i.e., less than 2 meters). Our MIT-licenced open-source software stack has to this date been used by thaousands of researchers and practitioners around the globe, with the public Anyplace service reaching over 100,000 real user interactions.},
journal = {SIGSPATIAL Special},
month = {oct},
pages = {3–10},
numpages = {8}
}

@inproceedings{10.5555/2665049.2665054,
author = {Kofler, Klaus and Davis, Gregory and Gesing, Sandra},
title = {SAMPO: An Agent-Based Mosquito Point Model in OpenCL},
year = {2014},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Agent-based modeling and simulations are applied for problems where population-level patterns arise from the interaction of many autonomous individuals. These problems are compute-intensive and excellent candidates for the use of parallel algorithms and architectures. As a cross-platform software development framework for parallel architectures, OpenCL appears as an ideal tool to implement such algorithms. However, OpenCL does not natively support object-oriented development, which most of the toolkits and frameworks used to build agent-based models require.The present work describes an OpenCL implementation of an existing agent-based model, simulating populations of the Anopheles gambiae mosquito, one of the most important vectors of malaria in Africa. Discussed are the methods and techniques used to overcome the design challenges, which arise when transitioning from an object-oriented program to an efficient OpenCL implementation. In particular, the parallelism inside the program has been maximized, dynamic divergent branching was reduced, and the number of data transfers between the OpenCL host and device has been minimized as far as possible.Even though our implementation was designed for this specific use case, the approach can be generalized to other contexts, as most agent-based point models would benefit from the same basic design decisions that we took for our implementation. Comparisons between the object-oriented and the OpenCL implementation illustrate that using an OpenCL approach offers two important performance benefits: an overall simulation time speedup of up to 576 with no measurable loss of accuracy, and better scalability as the agent-population size increases. The tradeoffs necessary to achieve these performance benefits and the implications for future agent-based software development frameworks are discussed.},
booktitle = {Proceedings of the 2014 Symposium on Agent Directed Simulation},
articleno = {5},
numpages = {10},
keywords = {OpenCL, GPGPU, agent-based modelling},
location = {Tampa, Florida},
series = {ADS '14}
}

@inproceedings{10.1145/3123878.3131999,
author = {Szabo, Marton and Majdan, Andras and Pongracz, Gergely and Toka, Laszlo and Sonkoly, Balazs},
title = {Making the Data Plane Ready for NFV: An Effective Way of Handling Resources},
year = {2017},
isbn = {9781450350570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123878.3131999},
doi = {10.1145/3123878.3131999},
abstract = {In order to enable carrier grade network services constructed from software-based network functions, we need a novel data plane supporting high performance packet processing, low latency and flexible, fine granular programmability and control. The network functions implemented as virtual machines or containers use the same hardware resources (cpu, memory) as the elements responsible for networking, therefore, a low-level resource orchestrator which is capable of jointly controlling these resources is an indispensable component. In this demonstration, we showcase our novel resource orchestrator (FERO) on top of a data plane making use of open-source components such as, Docker, DPDK and OVS. It is capable of i) generating an abstract model of the underlying hardware architecture during the bootstrap process, ii) mapping the incoming network service requests to available resources based on our recently proposed Service Graph embedding engine and the generated graph model. The impact of the orchestration decision is shown on-the-fly by real-time performance measurements on a graphical dashboard.},
booktitle = {Proceedings of the SIGCOMM Posters and Demos},
pages = {97–99},
numpages = {3},
keywords = {SDN, SFC, DPDK, Docker, NFV},
location = {Los Angeles, CA, USA},
series = {SIGCOMM Posters and Demos '17}
}

@inproceedings{10.1145/3320326.3320391,
author = {El Mrabet, Zakaria and Ezzari, Mehdi and Elghazi, Hassan and El Majd, Badr Abou},
title = {Deep Learning-Based Intrusion Detection System for Advanced Metering Infrastructure},
year = {2019},
isbn = {9781450366458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320326.3320391},
doi = {10.1145/3320326.3320391},
abstract = {Smart grid is an alternative solution of the conventional power grid which harnesses the power of the information technology to save the energy and meet todays' environment requirements. Due to the inherent vulnerabilities in the information technology, the smart grid is exposed to wide variety of threats that could be translated into cyber-attacks. In this paper, we develop a deep learning-based intrusion detection system to defend against cyber-attacks in the advanced metering infrastructure network. The proposed machine learning approach is trained and tested extensively on an empirical industrial dataset which is composed of several attack' categories including the scanning, buffer overflow, and denial of service attacks. Then, an experimental comparison in terms of detection accuracy is conducted to evaluate the performance of the proposed approach with Na\"{\i}ve Bayes, Support Vector Machine, and Random Forest. The obtained results suggest that the proposed approaches produce optimal results comparing to the other algorithms. Finally, we propose a network architecture to deploy the proposed anomaly-based intrusion detection system across the Advanced metering infrastructure network. In addition, we propose a network security architecture composed of two types of Intrusion detection system types, Host and Network based, deployed across the Advanced Metering Infrastructure network to inspect the traffic and detect the malicious one at all the levels.},
booktitle = {Proceedings of the 2nd International Conference on Networking, Information Systems \&amp; Security},
articleno = {58},
numpages = {7},
keywords = {Advanced Metering Infrastructure, Intrusion detection system, Deep learning, cross entropy loss, detection accuracy},
location = {Rabat, Morocco},
series = {NISS '19}
}

@inproceedings{10.1145/2949550.2949652,
author = {Hu, Hao and Hong, Xingchen and Terstriep, Jeff and Liu, Yan Y. and Finn, Michael P. and Rush, Johnathan and Wendel, Jeffrey and Wang, Shaowen},
title = {TopoLens: Building a CyberGIS Community Data Service for Enhancing the Usability of High-Resolution National Topographic Datasets},
year = {2016},
isbn = {9781450347556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2949550.2949652},
doi = {10.1145/2949550.2949652},
abstract = {Geospatial data, often embedded with geographic references, are important to many application and science domains, and represent a major type of big data. The increased volume and diversity of geospatial data have caused serious usability issues for researchers in various scientific domains, which call for innovative cyberGIS solutions. To address these issues, this paper describes a cyberGIS community data service framework to facilitate geospatial big data access, processing, and sharing based on a hybrid supercomputer architecture. Through the collaboration between the CyberGIS Center at the University of Illinois at Urbana-Champaign (UIUC) and the U.S. Geological Survey (USGS), a community data service for accessing, customizing, and sharing digital elevation model (DEM) and its derived datasets from the 10-meter national elevation dataset, namely TopoLens, is created to demonstrate the workflow integration of geospatial big data sources, computation, analysis needed for customizing the original dataset for end user needs, and a friendly online user environment. TopoLens provides online access to precomputed and on-demand computed high-resolution elevation data by exploiting the ROGER supercomputer. The usability of this prototype service has been acknowledged in community evaluation.},
booktitle = {Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},
articleno = {39},
numpages = {8},
keywords = {microservices, web-based gateway environment, CyberGIS, data sharing, elevation data, geospatial big data},
location = {Miami, USA},
series = {XSEDE16}
}

@inproceedings{10.1145/2996913.2996917,
author = {Chatterjee, Abhranil and Anjaria, Janit and Roy, Sourav and Ganguli, Arnab and Seal, Krishanu},
title = {SAGEL: Smart Address Geocoding Engine for Supply-Chain Logistics},
year = {2016},
isbn = {9781450345897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996913.2996917},
doi = {10.1145/2996913.2996917},
abstract = {With the recent explosion of e-commerce industry in India, the problem of address geocoding, that is, transforming textual address descriptions to geographic reference, such as latitude, longitude coordinates, has emerged as a core problem for supply chain management. Some of the major areas that rely on precise and accurate address geocoding are supply chain fulfilment, supply chain analytics and logistics. In this paper, we present some of the challenges faced in practice while building an address geocoding engine as a core capability at Flipkart. We discuss the unique challenges of building a geocoding engine for a rapidly developing country like India, such as, fuzzy region boundaries, dynamic topography and lack of convention in spellings of toponyms, to name a few. We motivate the need for building a reliable and precise address geocoding system from a business perspective and argue why some of the commercially available solutions do not suffice for our requirements. SAGEL has evolved through 3 cycles of solution prototypes and pilot experiments. We describe the learnings from each of these phases and how we incorporated them to get to the first production-ready version. We describe how we store and index map data on a SolrCloud cluster of Apache Solr, an open-source search platform, and the core algorithm for geocoding which works post-retrieval in order to determine the best matches among a set of candidate results. We give a brief description of the system architecture and provide accuracy results of our geocoding engine by measuring deviations of geocoded customer addresses across India, from verified latitude, longitude coordinates of those addresses, for a sizeable address set. We also measure and report our system's ability to geocode up to different region levels, like city, locality or building. We compare our results with those of the geocoding service provided by Google against a set of addresses for which we have verified latitude-longitude coordinates and show that our geocoding engine is almost as accurate as Google's, while having a higher coverage.},
booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {42},
numpages = {10},
keywords = {spatio-textual searching, geographic information retrieval, spatial data mining and knowledge discovery, storage and indexing},
location = {Burlingame, California},
series = {SIGSPACIAL '16}
}

@inproceedings{10.1145/2775088.2775100,
author = {You, Taewan and Martinez-Julia, Pedro and Skarmeta, Antonio and Jung, Heeyoung},
title = {Design and Deployment of Federation Testbed in EU-KR for Identifier-Based Communications},
year = {2015},
isbn = {9781450335645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2775088.2775100},
doi = {10.1145/2775088.2775100},
abstract = {SmartFIRE is the first intercontinental testbed, federating multiple small-scale testbeds in South Korea and Europe, which exploits the benefits and building blocks of an OpenFlow-based infrastructure. As a part of SmartFIRE, both ETRI and UMU designs and develops a federation testbed for Identifier-based communications that all of communication services are achieved by Identifier not by IP address. In order to manage and control the testbed, we deploy a Measurement and Management Framework (OMF), further we will deploy SFA aggregate manager to federate with other SmartFIRE testbed. In this paper we introduce the federation testbed for ID-based communications including network connectivity, architecture configuration, and federation architecture. Moreover, to exploit the testbed, we design and implement two mobility use cases that we show seamless network connection service under host's mobility, such as intra-domain handover and inter-domain handover. Thus we can show result of the experimentation that the communication session wouldn't be cut off even though communication entity moves to a different network. Finally we refer future works for federation to cooperate with other SmartFIRE testbeds and additional ID-based communication scenario.},
booktitle = {The 10th International Conference on Future Internet},
pages = {13–16},
numpages = {4},
keywords = {KR, EU, SmartFire, Federation Testbed, Deployment, Identifier-based communications},
location = {Seoul, Republic of Korea},
series = {CFI '15}
}

@inproceedings{10.1145/3154273.3154316,
author = {Talasila, Prasad and Kakrambe, Mihir and Rai, Anurag and Santy, Sebastin and Goveas, Neena and Deshpande, Bharat M.},
title = {BITS Darshini: A Modular, Concurrent Protocol Analyzer Workbench},
year = {2018},
isbn = {9781450363723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3154273.3154316},
doi = {10.1145/3154273.3154316},
abstract = {Network measurements are essential for troubleshooting and active management of networks. Protocol analysis of captured network packet traffic is an important passive network measurement technique used by researchers and network operations engineers. In this work, we present a measurement workbench tool named BITS Darshini (Darshini in short) to enable scientific network measurements.We have created Darshini as a modular, concurrent web application that stores experimental meta-data and allows users to specify protocol parse graphs. Darshini performs protocol analysis on a concurrent pipeline architecture, persists the analysis to a database and provides the analysis results via a REST API service. We formulate the problem of mapping protocol parse graph to a concurrent pipeline as a graph embedding problem. Our tool, Darshini, performs protocol analysis up to transport layer and is suitable for the study of small and medium-sized networks. Darshini enables secure collaboration and consultations with experts.},
booktitle = {Proceedings of the 19th International Conference on Distributed Computing and Networking},
articleno = {54},
numpages = {10},
keywords = {measurement workbench, concurrent packet analysis, Network measurements, packet analyzer, graph embedding, protocol parse graph, collaborative analysis},
location = {Varanasi, India},
series = {ICDCN '18}
}

@inproceedings{10.1145/2996890.2996903,
author = {Sukhoroslov, Oleg and Volkov, Sergey and Afanasiev, Alexander},
title = {Program Autotuning as a Service: Opportunities and Challenges},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.2996903},
doi = {10.1145/2996890.2996903},
abstract = {Program autotuning is becoming an increasingly valuable tool for improving performance portability across diverse target architectures, exploring trade-offs between several criteria, or meeting quality of service requirements. Recent work on general autotuning frameworks enabled rapid development of domain-specific autotuners reusing common libraries of parameter types and search techniques. In this work we explore the use of such frameworks to develop general-purpose online services for program autotuning using the Software as a Service model. Beyond the common benefits of this model, the proposed approach opens up a number of unique opportunities, such as collecting performance data and utilizing it to improve further runs, or enabling remote online autotuning. However, the proposed autotuning as a service approach also brings in several challenges, such as accessing target systems, dealing with measurement latency, and supporting execution of user-provided code. This paper presents the first step towards implementing the proposed approach and addressing these challenges. We describe an implementation of generic autotuning service that can be used for tuning arbitrary programs on user-provided computing systems. The service is based on OpenTuner autotuning framework and runs on Everest platform that enables rapid development of computational web services. In contrast to OpenTuner, the service doesn't require installation of the framework, allows users to avoid writing code and supports efficient parallel execution of measurement tasks across multiple machines. The performance of the service is evaluated by using it for tuning synthetic and real programs.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {148–155},
numpages = {8},
keywords = {software as a service, program autotuning, web services, distributed computing},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/3426744.3431328,
author = {V\"{o}r\"{o}s, P\'{e}ter and Pongr\'{a}cz, Gergely and Laki, S\'{a}ndor},
title = {Towards a Hybrid Next Generation NodeB},
year = {2020},
isbn = {9781450381819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426744.3431328},
doi = {10.1145/3426744.3431328},
abstract = {5G Radio Access Networks consists of two key services: User Plane Function (UPF) and next generation NodeB (gNB). Though several papers have recently demonstrated that the high-level UPF can be described in P4, for the lowest-level gNB service it is more challenging and cannot purely be done with existing programmable switches. In this paper, we show that gNB requires functionalities such as Automatic Repeat Request (ARQ) and ciphering/deciphering that are not supported by the high speed P4-programmable switches available in the market. To overcome these limitations, we propose a hybrid approach where the majority of packet processing is done by a high speed P4-programmable switch while the additional functionalities are solved by external services implemented in DPDK. The coordination of packets among the services is also handled by the P4-switch. Our preliminary results include the identification of functionalities required by a gNB node for delivering user data, the design of a hybrid architecture, and the performance evaluation of the buffering and re-transmission service. Finally, our measurements demonstrate that the proposed hybrid approach is scalable and could be an alternative to existing gNB solutions in the future.},
booktitle = {Proceedings of the 3rd P4 Workshop in Europe},
pages = {56–58},
numpages = {3},
location = {Barcelona, Spain},
series = {EuroP4'20}
}

@article{10.1145/2660768,
author = {Kim, Lok-Won and Lee, Dong-U and Villasenor, John},
title = {Automated Iterative Pipelining for ASIC Design},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/2660768},
doi = {10.1145/2660768},
abstract = {We describe an automated pipelining approach for optimally balanced pipeline implementation that achieves low area cost as well as meeting timing requirements. Most previous automatic pipelining methods have focused on Instruction Set Architecture (ISA)-based designs and the main goal of such methods generally has been maximizing performance as measured in terms of instructions per clock (IPC). By contrast, we focus on datapath-oriented designs (e.g., DSP filters for image or communication processing applications) in ASIC design flows. The goal of the proposed pipelining approach is to find the optimally pipelined design that not only meets the user-specified target clock frequency, but also seeks to minimize area cost of a given design. Unlike most previous approaches, the proposed methods incorporate the use of accurate area and timing information (iteratively achieved by synthesizing every interim pipelined design) to achieve higher accuracy during design exploration. When compared with exhaustive design exploration that considers all possible pipeline patterns, the two heuristic pipelining methods presented here involve only a small area penalty (typically under 5\%) while offering dramatically reduced computational complexity. Experimental validation is performed with commercial ASIC design tools and described for applications including polynomial function evaluation, FIR filters, matrix multiplication, and discrete wavelet transform filter designs with a 90nm standard cell library.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {mar},
articleno = {28},
numpages = {24},
keywords = {pipelined hardware architecture, ASIC designs, Timing error resolution, pipelining, design area optimization}
}

@article{10.1145/2845082,
author = {\"{A}ij\"{o}, Tomi and J\"{a}\"{a}skel\"{a}inen, Pekka and Elomaa, Tapio and Kultala, Heikki and Takala, Jarmo},
title = {Integer Linear Programming-Based Scheduling for Transport Triggered Architectures},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2845082},
doi = {10.1145/2845082},
abstract = {Static multi-issue machines, such as traditional Very Long Instructional Word (VLIW) architectures, move complexity from the hardware to the compiler. This is motivated by the ability to support high degrees of instruction-level parallelism without requiring complicated scheduling logic in the processor hardware. The simpler-control hardware results in reduced area and power consumption, but leads to a challenge of engineering a compiler with good code-generation quality.Transport triggered architectures (TTA), and other so-called exposed datapath architectures, take the compiler-oriented philosophy even further by pushing more details of the datapath under software control. The main benefit of this is the reduced register file pressure, with a drawback of adding even more complexity to the compiler side.In this article, we propose an Integer Linear Programming (ILP)-based instruction scheduling model for TTAs. The model describes the architecture characteristics, the particular processor resource constraints, and the operation dependencies of the scheduled program. The model is validated and measured by compiling application kernels to various TTAs with a different number of datapath components and connectivity. In the best case, the cycle count is reduced to 52\% when compared to a heuristic scheduler. In addition to producing shorter schedules, the number of register accesses in the compiled programs is generally notably less than those with the heuristic scheduler; in the best case, the ILP scheduler reduced the number of register file reads to 33\% of the heuristic results and register file writes to 18\%. On the other hand, as expected, the ILP-based scheduler uses distinctly more time to produce a schedule than the heuristic scheduler, but the compilation time is within tolerable limits for production-code generation.},
journal = {ACM Trans. Archit. Code Optim.},
month = {dec},
articleno = {59},
numpages = {22},
keywords = {transport triggered architectures, exposed datapath, instruction-level parallelism, integer linear programming, Code generation}
}

@inproceedings{10.1145/3052973.3053028,
author = {Inci, Mehmet Sinan and Eisenbarth, Thomas and Sunar, Berk},
title = {Hit by the Bus: QoS Degradation Attack on Android},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3053028},
doi = {10.1145/3052973.3053028},
abstract = {Mobile apps need optimal performance and responsiveness to rise amongst numerous rivals on the market. Further, some apps like media streaming or gaming apps cannot even function properly with a performance below a certain threshold. In this work, we present the first performance degradation attack on Android OS that can target rival apps using a combination of logical channel leakages and low-level architectural bottlenecks in the underlying hardware. To show the viability of the attack, we design a proof-of-concept app and test it on various mobile platforms. The attack runs covertly and brings the target to the level of unresponsiveness. With less than 10\% CPU time in the worst case, it requires minimal computational effort to run as a background service, and requires only the UsageStats permission from the user. We quantify the impact of our attack using 11 popular benchmark apps, running 44 different tests. The measured QoS degradation varies across platforms and applications, reaching a maximum of 90\% in some cases. The attack combines the leakage from logical channels with low-level architectural bottlenecks to design a malicious app that can covertly degrade Quality of Service (QoS) of any targeted app. Furthermore, our attack code has a small footprint and is not detected by the Android system as malicious. Finally, our app can pass the Google Play Store malware scanner, Google Bouncer, as well as the top malware scanners in the Play Store.},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {716–727},
numpages = {12},
keywords = {performance degradation, mobile security, mobile malware, QoS attack},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@inproceedings{10.1145/3378679.3394536,
author = {Liang, Yilei and O'Keeffe, Dan and Sastry, Nishanth},
title = {PAIGE: Towards a Hybrid-Edge Design for Privacy-Preserving Intelligent Personal Assistants},
year = {2020},
isbn = {9781450371322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378679.3394536},
doi = {10.1145/3378679.3394536},
abstract = {Intelligent Personal Assistants (IPAs) such as Apple's Siri, Google Now, and Amazon Alexa are becoming an increasingly important class of web application. In contrast to previous keyword-oriented search applications, IPAs support a rich query interface that allows user interaction through images, audio, and natural language queries. However, modern IPAs rely heavily on compute-intensive machine-learning inference. To achieve acceptable performance, ML-driven IPAs increasingly depend on specialized hardware accelerators (e.g. GPUs, FPGAs or TPUs), increasing costs for IPA service providers. For end-users, IPAs also present considerable privacy risks given the sensitive nature of the data they capture.We present PAIGE, a hybrid edge-cloud architecture for privacy-preserving Intelligent Personal Assistants. PAIGE's design is founded on the assumption that recent advances in low-cost hardware for machine-learning inference offer an opportunity to offload compute-intensive IPA ML tasks to the network edge. To allow privacy-preserving access to large IPA databases for less compute-intensive pre-processed queries, PAIGE leverages trusted execution environments at the server side. PAIGE's hybrid design allows privacy-preserving hardware acceleration of compute-intensive tasks, while avoiding the need to move potentially large IPA question-answering databases to the edge. As a step towards realising PAIGE, we present a first systematic performance evaluation of existing edge accelerator hardware platforms for a subset of IPA workloads, and show they offer a competitive alternative to existing data-center alternatives.},
booktitle = {Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking},
pages = {55–60},
numpages = {6},
keywords = {intelligent personal assistants, edge computing, trusted execution environments},
location = {Heraklion, Greece},
series = {EdgeSys '20}
}

@inproceedings{10.1145/2749469.2750422,
author = {Zhang, Tianwei and Lee, Ruby B.},
title = {CloudMonatt: An Architecture for Security Health Monitoring and Attestation of Virtual Machines in Cloud Computing},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750422},
doi = {10.1145/2749469.2750422},
abstract = {Cloud customers need guarantees regarding the security of their virtual machines (VMs), operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated by the customer not knowing where his VM is executing, and on the semantic gap between what the customer wants to know versus what can be measured in the cloud. We present an architecture for monitoring a VM's security health, with the ability to attest this to the customer in an unforgeable manner. We show a concrete implementation of property-based attestation and a full prototype based on the OpenStack open source cloud software.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {362–374},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@article{10.1145/2872887.2750422,
author = {Zhang, Tianwei and Lee, Ruby B.},
title = {CloudMonatt: An Architecture for Security Health Monitoring and Attestation of Virtual Machines in Cloud Computing},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3S},
issn = {0163-5964},
url = {https://doi.org/10.1145/2872887.2750422},
doi = {10.1145/2872887.2750422},
abstract = {Cloud customers need guarantees regarding the security of their virtual machines (VMs), operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated by the customer not knowing where his VM is executing, and on the semantic gap between what the customer wants to know versus what can be measured in the cloud. We present an architecture for monitoring a VM's security health, with the ability to attest this to the customer in an unforgeable manner. We show a concrete implementation of property-based attestation and a full prototype based on the OpenStack open source cloud software.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {362–374},
numpages = {13}
}

@inproceedings{10.1145/3281411.3281426,
author = {Xhonneux, Mathieu and Duchene, Fabien and Bonaventure, Olivier},
title = {Leveraging EBPF for Programmable Network Functions with IPv6 Segment Routing},
year = {2018},
isbn = {9781450360807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281411.3281426},
doi = {10.1145/3281411.3281426},
abstract = {With the advent of Software Defined Networks (SDN), Network Function Virtualisation (NFV) or Service Function Chaining (SFC), operators expect networks to support flexible services beyond the mere forwarding of packets. The network programmability framework which is being developed within the IETF by leveraging IPv6 Segment Routing enables the realisation of in-network functions.In this paper, we demonstrate that this vision of in-network programmability can be realised. By leveraging the eBPF support in the Linux kernel, we implement a flexible framework that allows network operators to encode their own network functions as eBPF code that is automatically executed while processing specific packets. Our lab measurements indicate that the overhead of calling such eBPF functions remains acceptable. Thanks to eBPF, operators can implement a variety of network functions. We describe the architecture of our implementation in the Linux kernel. This extension has been released with Linux 4.18. We illustrate the flexibility of our approach with three different use cases: delay measurements, hybrid networks and network discovery. Our lab measurements also indicate that the performance penalty of running eBPF network functions on Linux routers does not incur a significant overhead.},
booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
pages = {67–72},
numpages = {6},
location = {Heraklion, Greece},
series = {CoNEXT '18}
}

@inproceedings{10.1145/2959100.2959122,
author = {Celma, Oscar},
title = {The Exploit-Explore Dilemma in Music Recommendation},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959100.2959122},
doi = {10.1145/2959100.2959122},
abstract = {Were The Rolling Stones right when they said, "You can't always get what you want; but if you try sometime you get what you need"? Recommendation systems are the crystal ball of the Internet: predicting user intentions, making sense of big data, and delivering what people are looking for before they even know they want it. Pandora radio is best known for the Music Genome Project; the most unique and richly labeled music catalog of 1.5 million+ tracks. While this content-based approach to music recommendation is extremely effective and still used today as the foundation to the leading online radio service, Pandora has also collected more than a decade of contextual listener feedback in the form of more than 65 billion thumbs from 79M+ monthly active users who have created more than 9 billion stations. This session will look at how the interdisciplinary team at Pandora goes about making sense of these massive data sets to successfully make large scale music recommendations to our listeners.As opposed to more traditional recommender systems which need only to recommend a single item or set of items, Pandora's recommenders must provide an evolving set of sequential items, which constantly keep the experience new and exciting. In this talk I will present a dynamic ensemble learning system that combines musicological data and machine learning models to provide a truly personalized experience. This approach allows us to switch from a lean back experience (exploitation) to a more exploration mode to discover new music tailored specifically to users individual tastes. To exemplify this, I will present a recently launched product led by the research team, Thumbprint Radio.Following this session the audience will have an in-depth understanding of how Pandora uses science to determine the perfect balance of familiarity, discovery, repetition and relevance for each individual listener, measures and evaluates user satisfaction, and how our online and offline architecture stack plays a critical role in our success.},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
pages = {377},
numpages = {1},
keywords = {exploit-explore dilemma, ensemble learning, thumbprint radio, content-based recommendation, A/B online testing, offline evaluation, machine listening},
location = {Boston, Massachusetts, USA},
series = {RecSys '16}
}

@inproceedings{10.1145/2619239.2631461,
author = {Fiadino, Pierdomenico and Schiavone, Mirko and Casas, Pedro},
title = {Vivisecting Whatsapp through Large-Scale Measurements in Mobile Networks},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2631461},
doi = {10.1145/2619239.2631461},
abstract = {WhatsApp, the new giant in instant multimedia messaging in mobile networks is rapidly increasing its popularity, taking over the traditional SMS/MMS messaging. In this paper we present the first large-scale characterization of WhatsApp, useful among others to ISPs willing to understand the impacts of this and similar applications on their networks. Through the combined analysis of passive measurements at the core of a national mobile network, worldwide geo-distributed active measurements, and traffic analysis at end devices, we show that: (i) the WhatsApp hosting architecture is highly centralized and exclusively located in the US; (ii) video sharing covers almost 40\% of the total WhatsApp traffic volume; (iii) flow characteristics depend on the OS of the end device; (iv) despite the big latencies to US servers, download throughputs are as high as 1.5 Mbps; (v) users react immediately and negatively to service outages through social networks feedbacks.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {133–134},
numpages = {2},
keywords = {service outages, mobile networks, whatsapp, large-scale measurements, instant multimedia messaging},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@article{10.1145/2740070.2631461,
author = {Fiadino, Pierdomenico and Schiavone, Mirko and Casas, Pedro},
title = {Vivisecting Whatsapp through Large-Scale Measurements in Mobile Networks},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2631461},
doi = {10.1145/2740070.2631461},
abstract = {WhatsApp, the new giant in instant multimedia messaging in mobile networks is rapidly increasing its popularity, taking over the traditional SMS/MMS messaging. In this paper we present the first large-scale characterization of WhatsApp, useful among others to ISPs willing to understand the impacts of this and similar applications on their networks. Through the combined analysis of passive measurements at the core of a national mobile network, worldwide geo-distributed active measurements, and traffic analysis at end devices, we show that: (i) the WhatsApp hosting architecture is highly centralized and exclusively located in the US; (ii) video sharing covers almost 40\% of the total WhatsApp traffic volume; (iii) flow characteristics depend on the OS of the end device; (iv) despite the big latencies to US servers, download throughputs are as high as 1.5 Mbps; (v) users react immediately and negatively to service outages through social networks feedbacks.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {133–134},
numpages = {2},
keywords = {large-scale measurements, mobile networks, instant multimedia messaging, whatsapp, service outages}
}

@inproceedings{10.1145/2984393.2984398,
author = {Tomtsis, Dimitrios and Kontogiannis, Sotirios and Kokkonis, George and Zinas, Nicholas},
title = {IoT Architecture for Monitoring Wine Fermentation Process of Debina Variety Semi-Sparkling Wine},
year = {2016},
isbn = {9781450348102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984393.2984398},
doi = {10.1145/2984393.2984398},
abstract = {This paper proposes a new system architecture and HTTP communication mechanism called Smart Barrel System (Wine-SBS) for the process of monitoring Debina varietal sparkling wine fermenting conditions, produced at the area of Zitsa Epirus, Greece. The system includes microcontroller equipment with sensors that monitor wine attributes and storage conditions, called CBS-sensor transceivers, which are distributed among the debina fermentation vessels. The transmission of measurements, which occur periodically, are sent to a central cloud system application service. The CBS-sensor data are collected by a CBS-sensor collector and then follows an HTTP/2 request of multiplexed HTTP flows to a remote application server.},
booktitle = {Proceedings of the SouthEast European Design Automation, Computer Engineering, Computer Networks and Social Media Conference},
pages = {42–47},
numpages = {6},
keywords = {wine fermentation monitoring system, Precision enology, wireless sensor network},
location = {Kastoria, Greece},
series = {SEEDA-CECNSM '16}
}

@inproceedings{10.1145/3331076.3331108,
author = {Buccafurri, Francesco and Musarella, Lorenzo and Nardone, Roberto},
title = {Enabling Propagation in Web of Trust by Ethereum},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331108},
doi = {10.1145/3331076.3331108},
abstract = {Web of Trust offers a way to bind identities with the corresponding public keys. It relies on a distributed architecture, where each user could play the role of certificate signer. With the widespread diffusion of social networks, the trust propagation is a matter of growing interest. This paper proposes an approach enabling the propagation in Web of Trust by means of Ethereum. The usage of Ethereum eliminates the necessity of single-organization trusted services, which is, in general, not realistic. Although the information stored on Ethereum is public, the privacy of users is protected because trust chains involve only Ethereum addresses and strong measures are implemented to contrast their malicious de-anonymization. The approach relies on the usage of a smart contract for storing the status of certificate signatures and to manage revocations. When a user u wants to trust another user v, the smart contract checks the presence of trust chains originating from root nodes of u.},
booktitle = {Proceedings of the 23rd International Database Applications \&amp; Engineering Symposium},
articleno = {9},
numpages = {6},
keywords = {social network, trust propagation, smart contract, pretty good privacy, Ethereum, blockchain},
location = {Athens, Greece},
series = {IDEAS '19}
}

@inproceedings{10.1145/3297663.3309668,
author = {Talreja, Disha and Lahiri, Kanishka and Kalambur, Subramaniam and Raghavendra, Prakash},
title = {Performance Scaling of Cassandra on High-Thread Count Servers},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3309668},
doi = {10.1145/3297663.3309668},
abstract = {NoSQL databases are commonly used today in cloud deployments due to their ability to "scale-out" and effectively use distributed computing resources in a data center. At the same time, cloud servers are also witnessing rapid growth in CPU core counts, memory bandwidth, and memory capacity. Hence, apart from scaling out effectively, it's important to consider how such workloads "scale-up" within a single system, so that they can make the best use of available resources. In this paper, we describe our experiences studying the performance scaling characteristics of Cassandra, a popular open-source, column-oriented database, on a single high-thread count dual socket server. We demonstrate that using commonly used benchmarking practices, Cassandra does not scale well on such systems. Next, we show how by taking into account specific knowledge of the underlying topology of the server architecture, we can achieve substantial improvements in performance scalability. We report on how, during the course of our work, we uncovered an area for performance improvement in the official open-source implementation of the Java platform with respect to NUMA awareness. We show how optimizing this resulted in 27\% throughput gain for Cassandra under studied configurations. As a result of these optimizations, using standard workload generators, we obtained up to 1.44x and 2.55x improvements in Cassandra throughput over baseline single and dual-socket performance measurements respectively. On wider testing across a variety of workloads, we achieved excellent performance scaling, averaging 98\% efficiency within a socket and 90\% efficiency at the system-level.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {179–187},
numpages = {9},
keywords = {cassandra, performance benchmarking, nosql databases, performance scalability},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3241539.3241567,
author = {Marquez, Cristina and Gramaglia, Marco and Fiore, Marco and Banchs, Albert and Costa-Perez, Xavier},
title = {How Should I Slice My Network? A Multi-Service Empirical Evaluation of Resource Sharing Efficiency},
year = {2018},
isbn = {9781450359030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241539.3241567},
doi = {10.1145/3241539.3241567},
abstract = {By providing especially tailored instances of a virtual network,network slicing allows for a strong specialization of the offered services on the same shared infrastructure. Network slicing has profound implications on resource management, as it entails an inherent trade-off between: (i) the need for fully dedicated resources to support service customization, and (ii) the dynamic resource sharing among services to increase resource efficiency and cost-effectiveness of the system. In this paper, we provide a first investigation of this trade-off via an empirical study of resource management efficiency in network slicing. Building on substantial measurement data collected in an operational mobile network (i) we quantify the efficiency gap introduced by non-reconfigurable allocation strategies of different kinds of resources, from radio access to the core of the network, and (ii) we quantify the advantages of their dynamic orchestration at different timescales. Our results provide insights on the achievable efficiency of network slicing architectures, their dimensioning, and their interplay with resource management algorithms.},
booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
pages = {191–206},
numpages = {16},
keywords = {network efficiency, network slicing, resource management},
location = {New Delhi, India},
series = {MobiCom '18}
}

@article{10.1145/2968216,
author = {Maqsood, Tahir and Khalid, Osman and Irfan, Rizwana and Madani, Sajjad A. and Khan, Samee U.},
title = {Scalability Issues in Online Social Networks},
year = {2016},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/2968216},
doi = {10.1145/2968216},
abstract = {The last decade witnessed a tremendous increase in popularity and usage of social network services, such as Facebook, Twitter, and YouTube. Moreover, advances in Web technologies coupled with social networks has enabled users to not only access, but also generate, content in many forms. The overwhelming amount of produced content and resulting network traffic gives rise to precarious scalability issues for social networks, such as handling a large number of users, infrastructure management, internal network traffic, content dissemination, and data storage. There are few surveys conducted to explore the different dimensions of social networks, such as security, privacy, and data acquisition. Most of the surveys focus on privacy or security-related issues and do not specifically address scalability challenges faced by social networks. In this survey, we provide a comprehensive study of social networks along with their significant characteristics and categorize social network architectures into three broad categories: (a) centralized, (b) decentralized, and (c) hybrid. We also highlight various scalability issues faced by social network architectures. Finally, a qualitative comparison of presented architectures is provided, which is based on various scalability metrics, such as availability, latency, interserver communication, cost of resources, and energy consumption, just to name a few.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {40},
numpages = {42},
keywords = {decentralized social networks, social network, centralized social networks, hybrid social networks, Scalability}
}

@inproceedings{10.1145/3003733.3003756,
author = {Efthymiopoulos, Nikolaos and Efthymiopoulou, Maria and Christakidis, Athanasios},
title = {Experimentation on Low Delay and Stable Congestion Control for P2P Video Streaming},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003756},
doi = {10.1145/3003733.3003756},
abstract = {In recent years, a number of research efforts have focused on using peer-to-peer (P2P) systems in order to provide live streaming (LS) and Video-on-Demand (VoD) services. Most of them focused on the development of distributed P2P block schedulers for content exchange among the participating peers and on the architecture of the overlay graph (P2P overlay) that interconnects the set of these peers. Currently, the effort has shifted towards the combination of P2P systems with cloud infrastructures. By deploying monitoring and control architectures they use resources from the cloud in order to enhance the QoS, thus achieving an attractive trade-off between stability and low cost operation. However, there is a lack of research effort on the congestion control layer of these systems while the existing congestion control architectures in use are not suited for P2P traffic. This paper proposes a P2P congestion control protocol suitable for LS and VoD that: i) is capable to manage sequential traffic to multiple network destinations, ii) efficiently exploits the available bandwidth, iii) accurately measures the idle peers' resources, iv) it avoids network congestion, and v) is friendly to other TCP generated traffic. Our proposed algorithms and protocol have been implemented, tested and evaluated through a series of real experiments in the context of STEER [20].},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {48},
numpages = {6},
keywords = {video streaming, congestion control, P2P},
location = {Patras, Greece},
series = {PCI '16}
}

@inproceedings{10.5555/2667510.2667516,
author = {Seneviratne, Janaka and Parampalli, Udaya and Kulik, Lars},
title = {An Authorised Pseudonym System for Privacy Preserving Location Proof Architectures},
year = {2014},
isbn = {9781921770326},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {An emerging class of Location Based Services (LBSs) needs verified mobile device locations for service provision. For example, an automated car park billing system requires verified locations of cars to confirm the place and the duration of parked cars. Location Proof Architectures (LPAs) allow a user (or a device on behalf of its user) to obtain a proof of its presence at a location from a trusted third party. A major concern in LPAs is to preserve user location privacy. To achieve this a user's identity and location data should be maintained separately with additional measures that prevent leaking sensitive identity and location data. In this paper, we present a privacy preserving LPA in which users appear under pseudonyms. Our main contribution is a third party free pseudonym registering protocol based on blind signature schemes. We show that our protocol allows to build a pseudonym system with a guaranteed degree of privacy agreed at the time of pseudonym registration. We also demonstrate that a pseudonym can be authenticated across different organizations in an LPA. Our system ensures that (i) only authenticated users can register their pseudonyms, (ii) the pseudonyms have a consistent degree of privacy at the point of registration and (iii) a user cannot take another user's pseudonym.},
booktitle = {Proceedings of the Twelfth Australasian Information Security Conference - Volume 149},
pages = {47–56},
numpages = {10},
keywords = {privacy, pseudonym, location proof architecture},
location = {Auckland, New Zealand},
series = {AISC '14}
}

@inproceedings{10.1145/2695664.2695835,
author = {Rrushi, Julian L. and Farhangi, Hassan and Nikolic, Radina and Howey, Clay and Carmichael, Kelly and Palizban, Ali},
title = {By-Design Vulnerabilities in the ANSI C12.22 Protocol Specification},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695835},
doi = {10.1145/2695664.2695835},
abstract = {The ANSI C12.22 is a standard that specifies interfaces to data communication networks in the smart grid. In this paper we discuss several vulnerabilities by design that we discovered in the ANSI C12.22 protocol specification during an analysis of the overall protocol architecture. The consequences of an exploitation of those vulnerabilities consist of denial of service conditions and disruptions to ANSI C12.22 nodes and relays. We developed attack code to experiment with exploitations of most of the vulnerabilities that we discuss in this paper. Our research testbed consisted of meters that we emulated via the Trilliant TstBench software. The emulated meters were running on virtual Windows machines on a virtual network. In the paper, we provide details of the vulnerabilities by design that we identified, and thus propose a series of revisions of the ANSI C12.22 protocol specification with the objective of mitigating those vulnerabilities.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {2231–2236},
numpages = {6},
keywords = {ANSI C12.22 protocol, smartgrid security, vulnerabilities},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3197768.3201560,
author = {Tzallas, Alexandros T. and Katertsidis, Nikolaos and Glykos, Konstantinos and Segkouli, Sofia and Votis, Konstantinos and Tzovaras, Dimitrios and Barru\'{e}, Cristian and Paliokas, Ioannis and Cort\'{e}s, Ulises},
title = {Designing a Gamified Social Platform for People Living with Dementia and Their Live-in Family Caregivers},
year = {2018},
isbn = {9781450363907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197768.3201560},
doi = {10.1145/3197768.3201560},
abstract = {In the current paper, a social gamified platform for people living with dementia and their live-in family caregivers, integrating a broader diagnostic approach and interactive interventions is presented. The CAREGIVERSPRO-MMD (C-MMD) platform constitutes a support tool for the patient and the informal caregiver - also referred to as the dyad - that strengthens self-care, and builds community capacity and engagement at the point of care. The platform is implemented to improve social collaboration, adherence to treatment guidelines through gamification, recognition of progress indicators and measures to guide management of patients with dementia, and strategies and tools to improve treatment interventions and medication adherence. Moreover, particular attention was provided on guidelines, considerations and user requirements for the design of a User-Centered Design (UCD) platform. The design of the platform has been based on a deep understanding of users, tasks and contexts in order to improve platform usability, and provide adaptive and intuitive User Interfaces with high accessibility. In this paper, the architecture and services of the C-MMD platform are presented, and specifically the gamification aspects.},
booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
pages = {476–481},
numpages = {6},
keywords = {Interventions, Cloud Platform, Caregivers, Gamification, Social Networking, Self-management, Dementia},
location = {Corfu, Greece},
series = {PETRA '18}
}

@inproceedings{10.1145/3018896.3018934,
author = {Kokkonis, George and Kontogiannis, Sotirios and Tomtsis, Dimitrios},
title = {FITRA: A Neuro-Fuzzy Computational Algorithm Approach Based on an Embedded Water Planting System},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018934},
doi = {10.1145/3018896.3018934},
abstract = {This paper proposes a novel neuro-fuzzy computational algorithm for embedded irrigation systems called FITRA. It presents a new system architecture for the process of continuously monitoring environmental conditions and efficient irrigation of arable areas. The system includes microcontroller equipment with multiple sensors interspersed all over the field. Transmissions of measurements, which occur periodically, send to a central cloud system Application Service (AS) assisted by a 3G network. The decision for irrigation or not is made by a neuro-fuzzy algorithm. As an input for that algorithm are the values taken from the interspersed sensors. As an output, this algorithm controls the central solenoid water valve of the water planting system. The irrigation system automatically adjusts to changing environmental conditions.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {39},
numpages = {8},
keywords = {soil sensor, agriculture, water planting systems, IoT, smart irrigation, smart farming, neuro-fuzzy algorithms},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3297280.3297648,
author = {Koupaee, Mahnaz},
title = {Mortality Prediction Using Medical Notes: Student Research Abstract},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297648},
doi = {10.1145/3297280.3297648},
abstract = {Mortality prediction is a critical task for assessing patients' conditions in Intensive Care Units (ICU) of hospitals to improve decision-making and quality of care. Measurements taken and recorded at different time points are the main source of information to be used for tasks related to healthcare. However, the notes written by medical service providers during patients' stay in hospital as a rich source of detailed information is not sufficiently exploited. In this work, we propose a Convolutional Neural Network (CNN) architecture to utilize the unstructured texts to predict the pre-discharge and post-discharge mortality of ICU patients. Evaluations show high performance of the proposed method in terms of precision and recall. Moreover, our method outperforms the state of the art method by achieving a higher AUC.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {778–781},
numpages = {4},
keywords = {medical notes, convolutional neural network, MIMIC, mortality prediction},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/2699343.2699348,
author = {Achtzehn, Andreas and Riihihj\"{a}rvi, Janne and Barri\'{\i}a Castillo, Irving Antonio and Petrova, Marina and M\"{a}h\"{o}nen, Petri},
title = {CrowdREM: Harnessing the Power of the Mobile Crowd for Flexible Wireless Network Monitoring},
year = {2015},
isbn = {9781450333917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2699343.2699348},
doi = {10.1145/2699343.2699348},
abstract = {High-speed mobile broadband connections have opened exciting new opportunities to collect sensor data from thousands or even millions of distributed mobile devices for the purpose of crowdsourced decision making. In this paper, we propose CrowdREM (crowdsourced radio environment mapping), a framework with the specific aim of monitoring and modelling wireless cellular networks. CrowdREM enables operator-independent and highly efficient collection of network performance data along all layers of the communications protocol stack. Such extensive information on network load, spectrum usage, or local coverage can help operators to optimize their networks and service quality and enable improved consumer decision making. In this paper, we introduce the mbox{CrowdREM} mobile architecture and show first results from a prototype implementation on open-source mobile phones. We demonstrate the versatility of using commodity devices for network and spectrum monitoring, and present the challenges originating from the use of uncalibrated and low-precision measurement equipment. We have acquired an extensive data set from using our prototype implementation in a 21-day measurement campaign covering more than 1,000 hours of measurement data. From this we present and discuss the potential derivation of tangible and relevant network performance and signal quality indicators, which could, e.g., be conducted by independent parties.},
booktitle = {Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications},
pages = {63–68},
numpages = {6},
keywords = {cellular networks, drive testing, crowdsourcing, mobile},
location = {Santa Fe, New Mexico, USA},
series = {HotMobile '15}
}

@inproceedings{10.1145/2578153.2583037,
author = {Chrobot, Nina},
title = {The Role of Processing Fluency in Online Consumer Behavior: Evaluating Fluency by Tracking Eye Movements},
year = {2014},
isbn = {9781450327510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578153.2583037},
doi = {10.1145/2578153.2583037},
abstract = {The Internet enables people to extensively research products or services, and also easily compare prices between offers [e.g. Baker et al. 2001]. Taking into account the amount of information available on the Internet, acquisition of new information can face some difficulties, especially when one wants to make a purchase decision. Therefore, the ability to process relevant information fluently enables a user to create a better experience and to become more efficient in gathering information related to the purpose of the visit. This ability might be connected to the cognitive task that can either be effortless or effortful, and may lead to a metacognitive experience of either fluency or disfluency [Alter and Oppenheimer 2009]. Nevertheless, some e-commerce websites are preferred over others and this preference varies between individuals. This variation can be influenced by user's prior experience, cognitive sources but also graphics or information architecture on the web page. Presented project aims at applying the fluency concept to consumer behavior in online environment by studying eye movements and promoting eye tracking as an objective measure.},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
pages = {387–388},
numpages = {2},
location = {Safety Harbor, Florida},
series = {ETRA '14}
}

@article{10.1145/3457143,
author = {Burny, Nicolas and Vanderdonckt, Jean},
title = {UiLab, a Workbench for Conducting and Reproducing Experiments in GUI Visual Design},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3457143},
doi = {10.1145/3457143},
abstract = {With the continuously increasing number and variety of devices, the study of visual design of their Graphical User Interfaces grows in importance and scope, particularly for new devices, including smartphones, tablets, and large screens. Conducting a visual design experiment typically requires defining and building a GUI dataset with different resolutions for different devices, computing visual design measures for the various configurations, and analyzing their results. This workflow is very time- and resource-consuming, therefore limiting its reproducibility. To address this problem, we present UiLab, a cloud-based workbench that parameterizes the settings for conducting an experiment on visual design of Graphical User Interfaces, for facilitating the design of such experiments by automating some workflow stages, and for fostering their reproduction by automating their deployment. Based on requirements elicited for UiLab, we define its conceptual model to delineate the borders of services of the software architecture to support the new workflow. We exemplify it by demonstrating a system walkthrough and we assess its impact on experiment reproducibility in terms of design and development time saved with respect to a classical workflow. Finally, we discuss potential benefits brought by this workbench with respect to reproducing experiments in GUI visual design and existing shortcomings to initiate future avenues. We publicly release UiLab source code on a GitHub repository.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {may},
articleno = {196},
numpages = {31},
keywords = {aesthetics, usability evaluation, visual design, user interface evaluation}
}

@inproceedings{10.1145/3452918.3467815,
author = {Dijkstra-Soudarissanane, Sylvie and Klunder, Tessa and Brandt, Aschwin and Niamut, Omar},
title = {Towards XR Communication for Visiting Elderly at Nursing Homes},
year = {2021},
isbn = {9781450383899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452918.3467815},
doi = {10.1145/3452918.3467815},
abstract = {Due to the current pandemic, the elderly in care homes are greatly affected by the lack of contact with their families, resulting in various mental conditions (e.g., depression, feelings of loneliness) and deterioration of mental health for dementia patients. In response, residents and family members increasingly resorted to mediated communication to maintain social contact. To facilitate high-quality mediated social contact between residents in nursing homes and remote family members, we developed an Augmented Reality (AR)-based communication tool. The proposed demonstrator improved this situation by providing a working communication tool that enables the elderly to feel being together with their family by means of AR techniques. A complete end-to-end-chain architecture is defined, where the aspects of capture, transmission, and rendering are thoroughly investigated to fit the purpose of the use case. Based on an extensive user study comprising user experience (UX) and quality of service (QoS) measurements, each module is presented with the improvements made and the resulting higher quality AR communication platform.},
booktitle = {Proceedings of the 2021 ACM International Conference on Interactive Media Experiences},
pages = {319–321},
numpages = {3},
keywords = {Communication, Immersive Media, Augmented Reality, WebRTC, Conferencing, AR, Social XR, Volumetric video},
location = {Virtual Event, USA},
series = {IMX '21}
}

@inproceedings{10.1145/3075564.3075584,
author = {Xu, Dawen and Liao, Yi and Wang, Ying and Li, Huawei and Li, Xiaowei},
title = {Selective Off-Loading to Memory: Task Partitioning and Mapping for PIM-Enabled Heterogeneous Systems},
year = {2017},
isbn = {9781450344876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3075564.3075584},
doi = {10.1145/3075564.3075584},
abstract = {Processing-in-Memory (PIM) is returning as a promising solution to address the issue of memory wall as computing systems gradually step into the big data era. Researchers continually proposed various PIM architecture combined with novel memory device or 3D integration technology, but it is still a lack of universal task scheduling method in terms of the new heterogeneous platform. In this paper, we propose a formalized model to quantify the performance and energy of the PIM+CPU heterogeneous parallel system. In addition, we are the first to build a task partitioning and mapping framework to exploit different PIM engines. In this framework, an application is divided into subtasks and mapped onto appropriate execution units based on the proposed PIM-oriented Earliest-Finish-Time (PEFT) algorithm to maximize the performance gains brought by PIM. Experimental evaluations show our PIM-aware framework significantly improves the system performance compared to conventional processor architectures.},
booktitle = {Proceedings of the Computing Frontiers Conference},
pages = {255–258},
numpages = {4},
keywords = {Mapping, Architecture, Memory Wall, PIM},
location = {Siena, Italy},
series = {CF'17}
}

@inproceedings{10.1145/2976767.2987689,
author = {Falkner, Katrina and Szabo, Claudia and Chiprianov, Vanea},
title = {Model-Driven Performance Prediction of Systems of Systems},
year = {2016},
isbn = {9781450343213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976767.2987689},
doi = {10.1145/2976767.2987689},
abstract = {Systems of Systems exhibit characteristics that pose difficulty in modelling and predicting their overall performance capabilities, including the presence of operational independence, emergent behaviour, and evolutionary development. When considering Systems of Systems within the autonomous defence systems context, these aspects become increasingly critical, as performance constraints are typically driven by hard constraints on space, weight and power.System execution modelling languages and tools permit early prediction of the performance of model-driven systems, however the focus to date has been on understanding the performance of a model rather than determining if it meets performance requirements, and only subsequently carrying out analysis to reveal the causes of any requirement violations. Such an analysis is even more difficult when applied to several systems cooperating to achieve a common goal - a System of Systems (SoS).The successful integration of systems within a SoS context has been identified as one of the most substantial challenges facing military systems development [2]. Accordingly, there is a critical need to understand the non-functional aspects of the SoS (such as quality of service, power, size, cost and scalable management of communications), and to explore how these non-functional aspects evolve under new conditions and deployment scenarios. It is crucial that we develop methodologies for modelling and understanding non-functional properties early in the development and integration cycle to better inform our understanding of the impact of emergent behaviour and evolution within the SoS.We propose an integrated approach to performance prediction of model-driven real time embedded defence systems and systems of systems [1]. Our architectural prototyping system supports a scenario-driven experimental platform for evaluating model suitability within a set of deployment and real-time performance constraints. We present an overview of our performance prediction system, demonstrating the integration of modelling, execution and performance analysis, and discuss a case study to illustrate our approach. Our work employs state-of-the-art model-driven engineering techniques to facilitate SoS performance prediction and analysis at design time, either before the SoS is built and deployed, or during its lifetime when required to evolve.Our model-driven performance prediction platform supports a scenario-driven experimental environment for evaluating a SoS within the context of a specific deployment (modelling geographical distribution) and integration constraints. The main contributions of our work are: (a) a modeling methodology that captures diverse perspectives of the performance modeling of Systems of Systems; (b) a performance analysis engine that captures metrics associated with these perspectives and (c) a case study showing the performance evaluaton of a system of systems and its evolution as a result of the performance analysis. We discuss how our approach to modelling supports the specific characteristics of an SoS, and illustrate this through a case study, based on a "Blue Ocean" scenario, demonstrating how we may obtain performance predictions within a SoS with emergent and evolutionary properties. Within the context of our environment, we define models for the individual systems within our System of Systems, defined for representative workload to predict execution costs, i.e. CPU, memory usage and network usage, within a generic situation. Our modelling environment supports the generation of executable forms of these models, which may then be executed above realistic deployment scenarios in order to obtain predictions of System of System performance.},
booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
pages = {44},
numpages = {1},
location = {Saint-malo, France},
series = {MODELS '16}
}

@inproceedings{10.1145/3018981.3018986,
author = {Mell, Peter and Shook, James and Harang, Richard},
title = {Measuring and Improving the Effectiveness of Defense-in-Depth Postures},
year = {2016},
isbn = {9781450347884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018981.3018986},
doi = {10.1145/3018981.3018986},
abstract = {Defense-in-depth is an important security architecture principle that has significant application to industrial control systems (ICS), cloud services, storehouses of sensitive data, and many other areas. We claim that an ideal defense-in-depth posture is 'deep', containing many layers of security, and 'narrow', the number of node independent attack paths is minimized. Unfortunately, accurately calculating both depth and width is difficult using standard graph algorithms because of a lack of independence between multiple vulnerability instances (i.e., if an attacker can penetrate a particular vulnerability on one host then they can likely penetrate the same vulnerability on another host). To address this, we represent known weaknesses and vulnerabilities as a type of colored attack graph. We measure depth and width through solving the shortest color path and minimum color cut problems. We prove both of these to be NP-Hard and thus for our solution we provide a suite of greedy heuristics. We then empirically apply our approach to large randomly generated networks as well as to ICS networks generated from a published ICS attack template. Lastly, we discuss how to use these results to help guide improvements to defense-in-depth postures.},
booktitle = {Proceedings of the 2nd Annual Industrial Control System Security Workshop},
pages = {15–22},
numpages = {8},
keywords = {attack graph, measurement, defense in depth, security},
location = {Los Angeles, CA, USA},
series = {ICSS '16}
}

@article{10.1145/3428151,
author = {Bibi, Iram and Akhunzada, Adnan and Malik, Jahanzaib and Khan, Muhammad Khurram and Dawood, Muhammad},
title = {Secure Distributed Mobile Volunteer Computing with Android},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3428151},
doi = {10.1145/3428151},
abstract = {Volunteer Computing provision of seamless connectivity that enables convenient and rapid deployment of greener and cheaper computing infrastructure is extremely promising to complement next-generation distributed computing systems. Undoubtedly, without tactile Internet and secure VC ecosystems, harnessing its full potentials and making it an alternative viable and reliable computing infrastructure is next to impossible. Android-enabled smart devices, applications, and services are inevitable for Volunteer computing. Contrarily, the progressive developments of sophisticated Android malware may reduce its exponential growth. Besides, Android malwares are considered the most potential and persistent cyber threat to mobile VC systems. To secure Android-based mobile volunteer computing, the authors proposed MulDroid, an efficient and self-learning autonomous hybrid (Long-Short-Term Memory, Convolutional Neural Network, Deep Neural Network) multi-vector Android malware threat detection framework. The proposed mechanism is highly scalable with well-coordinated infrastructure and self-optimizing capabilities to proficiently tackle fast-growing dynamic variants of sophisticated malware threats and attacks with 99.01\% detection accuracy. For a comprehensive evaluation, the authors employed current state-of-the-art malware datasets (Android Malware Dataset, Androzoo) with standard performance evaluation metrics. Moreover, MulDroid is compared with our constructed contemporary hybrid DL-driven architectures and benchmark algorithms. Our proposed mechanism outperforms in terms of detection accuracy with a trivial tradeoff speed efficiency. Additionally, a 10-fold cross-validation is performed to explicitly show unbiased results.},
journal = {ACM Trans. Internet Technol.},
month = {sep},
articleno = {2},
numpages = {21},
keywords = {android malware, Volunteer computing (VC), tactile internet, deep learning (DL)}
}

@inproceedings{10.1145/2815675.2815677,
author = {Gracia-Tinedo, Ra\'{u}l and Tian, Yongchao and Samp\'{e}, Josep and Harkous, Hamza and Lenton, John and Garc\'{\i}a-L\'{o}pez, Pedro and S\'{a}nchez-Artigas, Marc and Vukolic, Marko},
title = {Dissecting UbuntuOne: Autopsy of a Global-Scale Personal Cloud Back-End},
year = {2015},
isbn = {9781450338486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815675.2815677},
doi = {10.1145/2815675.2815677},
abstract = {Personal Cloud services, such as Dropbox or Box, have been widely adopted by users. Unfortunately, very little is known about the internal operation and general characteristics of Personal Clouds since they are proprietary services.In this paper, we focus on understanding the nature of Personal Clouds by presenting the internal structure and a measurement study of UbuntuOne (U1). We first detail the U$1$ architecture, core components involved in the U1 metadata service hosted in the datacenter of Canonical, as well as the interactions of U$1$ with Amazon S3 to outsource data storage. To our knowledge, this is the first research work to describe the internals of a large-scale Personal Cloud.Second, by means of tracing the U$1$ servers, we provide an extensive analysis of its back-end activity for one month. Our analysis includes the study of the storage workload, the user behavior and the performance of the U1 metadata store. Moreover, based on our analysis, we suggest improvements to U1 that can also benefit similar Personal Cloud systems.Finally, we contribute our dataset to the community, which is the first to contain the back-end activity of a large-scale Personal Cloud. We believe that our dataset provides unique opportunities for extending research in the field.},
booktitle = {Proceedings of the 2015 Internet Measurement Conference},
pages = {155–168},
numpages = {14},
keywords = {performance analysis, measurement, personal cloud},
location = {Tokyo, Japan},
series = {IMC '15}
}

@inproceedings{10.1145/3372224.3419195,
author = {Zhang, Chaoyun and Fiore, Marco and Ziemlicki, Cezary and Patras, Paul},
title = {Microscope: Mobile Service Traffic Decomposition for Network Slicing as a Service},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3419195},
doi = {10.1145/3372224.3419195},
abstract = {The growing diversification of mobile services imposes requirements on network performance that are ever more stringent and heterogeneous. Network slicing aligns mobile network operation to this context, by enabling operators to isolate and customize network resources on a per-service basis. A key input for provisioning resources to slices is real-time information about the traffic demands generated by individual services. Acquiring such knowledge is however challenging, as legacy approaches based on in-depth inspection of traffic streams have high computational costs, which inflate with the widening adoption of encryption over data and control traffic. In this paper, we present a new approach to service-level demand estimation for slicing, which hinges on decomposition, i.e., the inference of per-service demands from traffic aggregates. By operating on total traffic volumes only, our approach overcomes the complexity and limitations of legacy traffic classification techniques, and provides a suitable input to recent 'Network Slice as a Service' (NSaaS) models. We implement decomposition through Microscope, a novel framework that uses deep learning to infer individual service demands from complex spatiotemporal features hidden in traffic aggregates. Microscope (i) transforms traffic data collected in irregular radio access deployments in a format suitable for convolutional learning, and (ii) can accommodate a variety of neural network architectures, including original 3D Deformable Convolutional Neural Networks (3D-DefCNNs) that we explicitly design for decomposition. Experiments with measurement data collected in an operational network demonstrate that Microscope accurately estimates per-service traffic demands with relative errors below 1.2\%. Further, tests in practical NSaaS management use cases show that resource allocations informed by decomposition yield affordable costs for the mobile network operator.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {38},
numpages = {14},
keywords = {traffic decomposition, service demand estimation, network slicing, neural networks, mobile network data traffic, deep learning},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@article{10.1145/3412821.3412823,
author = {Cerina, L. and Santambrogio, M. D.},
title = {SAGe: A Configurable Code Generator for Efficient Symbolic Analysis of Time-Series},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
url = {https://doi.org/10.1145/3412821.3412823},
doi = {10.1145/3412821.3412823},
abstract = {Some of the most recent applications and services revolve around the analysis of time-series, which generally exhibits chaotic characteristics. This behavior brought back the necessity to simplify their representation to discover meaningful patterns and extract information efficiently. Furthermore, recent trends show how computation is moving back from the Cloud to the Edge of network, meaning that algorithms should be compatible with low-power embedded devices. A family of methods called Symbolic Analysis (SA) tries to solve this issue, reducing the dimensionality of the original data in a set of symbolic words and providing distance metrics for the obtained symbols. However, SA is usually implemented using application-specific tools, which are not easily adaptable, or mathematical environments (e.g. R, Julia) that do not ensure portability, or that require additional work to maximize computing performance. We propose here SAGe: a code generation tool that helps the user to prototype efficient and portable code, starting from a high-level representation of SA requirements. Other than exploiting similarities between SA pipelines, SAGe employs general code templates to build and deploy the code on different architectures, such as embedded devices, microcontrollers, and FPGAs. Preliminary results show a speedup up to 223x against Python implementations running on an x86 desktop machine and a notable increase in computational efficiency on a reconfigurable device.},
journal = {SIGBED Rev.},
month = {jul},
pages = {12–17},
numpages = {6}
}

@inproceedings{10.1145/3459104.3459136,
author = {A. Panayiotou, Nikolaos and P. Stavrou, Vasileios and E. Stergiou, Konstantinos},
title = {Applying the Industry 4.0 in a Smart Gas Grid: The Greek Gas Distribution Network Case},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459136},
doi = {10.1145/3459104.3459136},
abstract = {The aim of this paper is to design and implement a series of actions regarding the operation of DEDA S.A. (Natural Gas Distribution Networks), based on principles of Industry 4.0. The Natural Gas Distribution sector is one of the most critical and innovative areas where Industry 4.0 can be applied, being part of critical infrastructure management. At first, company's business process architecture was developed, with the aim to export DEDA's business process and functional specifications related to the required information systems. Subsequently, company's communication network is implemented alongside the company's gas network, in coordination with the company's control room.In addition, modernization of metering system is taking place in order to exchange information between smart meters and the control room. A number of Information Systems, such as the pipeline surveillance system and the Business Intelligence system will also be installed in order to ensure communication at different levels using Cloud technologies. The implementation is expected to improve DEDA's organization, increasing customers' service level. As a result, there will be an expected increase in the operational efficiency of DEDA's network through the use of advanced technologies, in cooperation with business process modelling techniques. The effort should be continued in this direction in order to achieve even greater improvement in business processes, information systems and pipeline automation.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {180–184},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3449301.3449322,
author = {Wen, Yana and Wei, Tingyue and Cui, Kewei and Ling, Bai and Zhang, Yahao and Huang, Meng},
title = {Research on Belt and Road Big Data Visualization Based on Text Clustering Algorithm},
year = {2021},
isbn = {9781450388597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449301.3449322},
doi = {10.1145/3449301.3449322},
abstract = {In the era of big data, people's visual needs for data expression are increasing. In order to achieve better big data display effects, this article introduced the use of text clustering algorithms to achieve data crawling and Echarts technology to realize big data visualization. This system used mvvm's architecture and vue framework development platform, ThinkPHP was used as the background framework, and ES6 related technologies and specifications were used for application development. This system used Echarts, IView, GIS technology and JavaScript development methods to demonstrate economic big data module functions on the web side; Applied CSS3, HTML5, GIS technology to implement project achievement module and university alliance module; Applied Echarts, HTML5, JS function library technology to achieve national information module. This system used stored procedure, database index optimization technology to achieve rapid screening of massive data, and dynamically update and displayed related data through two-way data binding. This system combined real-time location technology with GIS technology to measure the distance between the user and the destination, and automatically plan the tour route to provide related services. This system can provide feasibility suggestions for strategic researchers or experts in related areas of the “Belt and Road”, and provide theoretical basis and technical support.},
booktitle = {Proceedings of the 6th International Conference on Robotics and Artificial Intelligence},
pages = {121–125},
numpages = {5},
keywords = {One Belt One Road, big data visualization, Keywords-component, Text clustering algorithm},
location = {Singapore, Singapore},
series = {ICRAI '20}
}

@inproceedings{10.1145/2809730.2809749,
author = {Walter, Nadine and Altm\"{u}ller, Tobias and Bengler, Klaus},
title = {Concept of a Reference Architecture for an Extendable In-Vehicle Adaptive Recommendation Service},
year = {2015},
isbn = {9781450338585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809730.2809749},
doi = {10.1145/2809730.2809749},
abstract = {An adaptive recommendation service can reduce driver distraction through reducing the amount of operation steps needed to call a function. It learns the routine user behavior of the driver related to a situation and supports the driver with this knowledge by giving proactive recommendations for preconfigured functions. An adaptive recommendation service is a complex system and the development includes several challenges. One is the development of an architecture which needs to be modular, extendible in regard to the support of different functions and integrated in an overall in-vehicle HMI architecture. This architecture describes the components and interfaces of an adaptive recommendation service which need to be researched and developed. It is a starting point for the implementation of realistic prototypes in a real vehicle or driving simulator which enables an extensive evaluation of the whole adaptive recommendation service.},
booktitle = {Adjunct Proceedings of the 7th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {88–93},
numpages = {6},
keywords = {software architecture, prototyping, driver distraction, recommendation system, adaptive system},
location = {Nottingham, United Kingdom},
series = {AutomotiveUI '15}
}

@inproceedings{10.1145/2883851.2883876,
author = {Renz, Jan and Hoffmann, Daniel and Staubitz, Thomas and Meinel, Christoph},
title = {Using A/B Testing in MOOC Environments},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883876},
doi = {10.1145/2883851.2883876},
abstract = {In recent years, Massive Open Online Courses (MOOCs) have become a phenomenon offering the possibility to teach thousands of participants simultaneously. In the same time the platforms used to deliver these courses are still in their fledgling stages. While course content and didactics of those massive courses are the primary key factors for the success of courses, still a smart platform may increase or decrease the learners experience and his learning outcome. The paper at hand proposes the usage of an A/B testing framework that is able to be used within an micro-service architecture to validate hypotheses about how learners use the platform and to enable data-driven decisions about new features and settings. To evaluate this framework three new features (Onboarding Tour, Reminder Mails and a Pinboard Digest) have been identified based on a user survey. They have been implemented and introduced on two large MOOC platforms and their influence on the learners behavior have been measured. Finally this paper proposes a data driven decision workflow for the introduction of new features and settings on e-learning platforms.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics \&amp; Knowledge},
pages = {304–313},
numpages = {10},
keywords = {A/B testing, MOOC, controlled online tests, e-learning, microservice},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2642687.2642690,
author = {Migault, Daniel and Palomares, Daniel and Hendrik, Hendrik and Laurent, Maryline},
title = {Secure IPsec Based Offload Architectures for Mobile Data},
year = {2014},
isbn = {9781450330275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642687.2642690},
doi = {10.1145/2642687.2642690},
abstract = {Radio Access Network (RAN) are likely to be overloaded, and some places will not be able to provide the necessary requested bandwidth. In order to respond to the demand of bandwidth, overloaded RAN are currently offloading their traffic on WLAN. WLAN Access Points like (ISP provided xDSL boxes) are untrusted, unreliable and do not handle mobility. As a result, mobility, multihoming, and security cannot be handled by the network anymore, and must be handled by the terminal. This paper positions offload architectures based on IPsec and shows that IPsec can provide end-to-end security, as well as seamless connectivity across IP networks. Then, the remaining of the paper evaluates how mobility on these IPsec based architectures impacts the Quality of Service (QoS) for real time applications such as an audio streaming service. QoS is measured using network interruption time and POLQA. Measurements compare TCP/HLS and UDP/RTSP over various IPsec configurations.},
booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {95–104},
numpages = {10},
keywords = {IPsec multiple interfaces, IPsec mobility, wlan offload architecture, terminal mobility, quality of service},
location = {Montreal, QC, Canada},
series = {Q2SWinet '14}
}

@inproceedings{10.1145/2568088.2576760,
author = {Ghaith, Shadi and Wang, Miao and Perry, Philip and Murphy, Liam},
title = {Software Contention Aware Queueing Network Model of Three-Tier Web Systems},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2576760},
doi = {10.1145/2568088.2576760},
abstract = {Using modelling to predict the performance characteristics of software applications typically uses Queueing Network Models representing the various system hardware resources. Leaving out the software resources, such as the limited number of threads, in such models leads to a reduced prediction accuracy. Accounting for Software Contention is a challenging task as existing techniques to model software components are complex and require deep knowledge of the software architecture. Furthermore, they also require complex measurement processes to obtain the model's service demands. In addition, solving the resultant model usually require simulation solvers which are often time consuming.In this work, we aim to provide a simpler model for three-tier web software systems which accounts for Software Contention that can be solved by time efficient analytical solvers. We achieve this by expanding the existing "Two-Level Iterative Queuing Modelling of Software Contention" method to handle the number of threads at the Application Server tier and the number of Data Sources at the Database Server tier. This is done in a generic manner to allow for extending the solution to other software components like memory and critical sections. Initial results show that our technique clearly outperforms existing techniques.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {273–276},
numpages = {4},
keywords = {performance prediction, software contention, performance models, web applications},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1145/3152434.3152450,
author = {Singhvi, Arjun and Banerjee, Sujata and Harchol, Yotam and Akella, Aditya and Peek, Mark and Rydin, Pontus},
title = {Granular Computing and Network Intensive Applications: Friends or Foes?},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152450},
doi = {10.1145/3152434.3152450},
abstract = {Computing/infrastructure as a service continues to evolve with bare metal, virtual machines, containers and now serverless granular computing service offerings. Granular computing enables developers to decompose their applications into smaller logical units or functions, and run them on small, low cost and short lived computation containers without having to worry about setting up servers - hence the term serverless computing. While serverless environments can be used very cost effectively for large scale parallel processing data analytics applications, it is less clear if network intensive packet processing applications can also benefit from these new computing services as they do not share the same characteristics. This paper examines the architectural constraints as well as current serverless implementations to develop a position on this topic and influence the next generation of computing services. We support our position through measurement and experimentation on Amazon's AWS Lambda service with a few popular network functions.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {157–163},
numpages = {7},
location = {<conf-loc>, <city>Palo Alto</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {HotNets '17}
}

@inproceedings{10.1145/2976749.2978414,
author = {Jia, Yaoqi and Chua, Zheng Leong and Hu, Hong and Chen, Shuo and Saxena, Prateek and Liang, Zhenkai},
title = {"The Web/Local" Boundary Is Fuzzy: A Security Study of Chrome's Process-Based Sandboxing},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978414},
doi = {10.1145/2976749.2978414},
abstract = {Process-based isolation, suggested by several research prototypes, is a cornerstone of modern browser security architectures. Google Chrome is the first commercial browser that adopts this architecture. Unlike several research prototypes, Chrome's process-based design does not isolate different web origins, but primarily promises to protect "the local system" from "the web". However, as billions of users now use web-based cloud services (e.g., Dropbox and Google Drive), which are integrated into the local system, the premise that browsers can effectively isolate the web from the local system has become questionable. In this paper, we argue that, if the process-based isolation disregards the same-origin policy as one of its goals, then its promise of maintaining the "web/local system (local)" separation is doubtful. Specifically, we show that existing memory vulnerabilities in Chrome's renderer can be used as a stepping-stone to drop executables/scripts in the local file system, install unwanted applications and misuse system sensors. These attacks are purely data-oriented and do not alter any control flow or import foreign code. Thus, such attacks bypass binary-level protection mechanisms, including ASLR and in-memory partitioning. Finally, we discuss various full defenses and present a possible way to mitigate the attacks presented.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {791–804},
numpages = {14},
keywords = {data-oriented attacks, browser design, browser security},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/2973750.2973766,
author = {Yu, Der-Yeuan and Ranganathan, Aanjhan and Masti, Ramya Jayaram and Soriente, Claudio and Capkun, Srdjan},
title = {SALVE: Server Authentication with Location Verification},
year = {2016},
isbn = {9781450342261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973750.2973766},
doi = {10.1145/2973750.2973766},
abstract = {The Location Service (LCS) proposed by the telecommunication industry is an architecture that allows the location of mobile devices to be accessed in various applications. We explore the use of LCS in location-enhanced server authentication, which traditionally relies on certificates. Given recent incidents involving certificate authorities, various techniques to strengthen server authentication were proposed. They focus on improving the certificate validation process, such as pinning, revocation, or multi-path probing. In this paper, we propose using the server's geographic location as a second factor of its authenticity. Our solution, SALVE, achieves location-based server authentication by using secure DNS resolution and by leveraging LCS for location measurements. We develop a TLS extension that enables the client to verify the server's location in addition to its certificate. Successful server authentication therefore requires a valid certificate and the server's presence at a legitimate geographic location, e.g., on the premises of a data center. SALVE prevents server impersonation by remote adversaries with mis-issued certificates or stolen private keys of the legitimate server. We develop a prototype implementation and our evaluation in real-world settings shows that it incurs minimal impact to the average server throughput. Our solution is backward compatible and can be integrated with existing approaches for improving server authentication in TLS.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking},
pages = {401–414},
numpages = {14},
keywords = {location service, TLS, server authentication, location-based authentication},
location = {New York City, New York},
series = {MobiCom '16}
}

@inproceedings{10.1145/3131365.3131373,
author = {Chung, Taejoong and van Rijswijk-Deij, Roland and Choffnes, David and Levin, Dave and Maggs, Bruce M. and Mislove, Alan and Wilson, Christo},
title = {Understanding the Role of Registrars in DNSSEC Deployment},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131373},
doi = {10.1145/3131365.3131373},
abstract = {The Domain Name System (DNS) provides a scalable, flexible name resolution service. Unfortunately, its unauthenticated architecture has become the basis for many security attacks. To address this, DNS Security Extensions (DNSSEC) were introduced in 1997. DNSSEC's deployment requires support from the top-level domain (TLD) registries and registrars, as well as participation by the organization that serves as the DNS operator. Unfortunately, DNSSEC has seen poor deployment thus far: despite being proposed nearly two decades ago, only 1\% of .com, .net, and .org domains are properly signed.In this paper, we investigate the underlying reasons why DNSSEC adoption has been remarkably slow. We focus on registrars, as most TLD registries already support DNSSEC and registrars often serve as DNS operators for their customers. Our study uses large-scale, longitudinal DNS measurements to study DNSSEC adoption, coupled with experiences collected by trying to deploy DNSSEC on domains we purchased from leading domain name registrars and resellers. Overall, we find that a select few registrars are responsible for the (small) DNSSEC deployment today, and that many leading registrars do not support DNSSEC at all, or require customers to take cumbersome steps to deploy DNSSEC. Further frustrating deployment, many of the mechanisms for conveying DNSSEC information to registrars are error-prone or present security vulnerabilities. Finally, we find that using DNSSEC with third-party DNS operators such as Cloudflare requires the domain owner to take a number of steps that 40\% of domain owners do not complete. Having identified several operational challenges for full DNSSEC deployment, we make recommendations to improve adoption.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {369–383},
numpages = {15},
keywords = {DNS, public key infrastructure, DNS security extension, PKI, DNS operator, registrar, DNSSEC},
location = {London, United Kingdom},
series = {IMC '17}
}

@inproceedings{10.1145/3446434.3446523,
author = {Daroshka, Vitali and Evgrafov, Arkady and Nikiforova, Jeanne and Chargazia, Grigory and Parshukov, Aleksey},
title = {The Specific of Business Reputation Value Measurement in Transformational Economy (on Example of Economy of the Republic of Belarus)},
year = {2021},
isbn = {9781450388900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446434.3446523},
doi = {10.1145/3446434.3446523},
abstract = {Globalization of economy and overcoming of the world economic crisis consequences increases the value of evidence-based suggestions to improve management in domestic industrial organizations. The increase of the competition from foreign business defines the need for justification of the adequate, complete concept of ensuring the stability of industrial enterprises' functioning that combines effective management of tangible and intangible assets. Looking at current trends of the world economic development, it's objectively seen that intangible competitive production components play an increasingly important role to preserve market leadership both for one organization and the entire national economy.Global trends in globalization dictate fundamentally new rules for the formation of the architecture of the business model of an industrial organization, which determine the transition from the production of products and services for the impersonal mass of consumers to the global scale of personalized service for each client. In other words, the current business environment requires the simultaneous realization of two trends that are opposite in nature: the global scale of activity (globalization) and the personal approach to each consumer (customization and personalization of products and services), which makes it necessary to create a system of criteria for assessing the technological transition.The main attention is given to an element of intangible assets that is one of the most difficult for value measurement and management, the business reputation of an organization. According to literature, there is no common opinion among the scientists as to what business reputation means and how it is linked to another subjective intangible asset, goodwill.},
booktitle = {Proceedings of the International Scientific Conference - Digital Transformation on Manufacturing, Infrastructure and Service},
articleno = {89},
numpages = {8},
keywords = {transformational economy, risks, Business reputation, value-based management},
location = {Saint Petersburg, Russian Federation},
series = {DTMIS '20}
}

@inproceedings{10.1145/2736084.2736091,
author = {Zhang, Cong and Liu, Jiangchuan},
title = {On Crowdsourced Interactive Live Streaming: A Twitch.Tv-Based Measurement Study},
year = {2015},
isbn = {9781450333528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2736084.2736091},
doi = {10.1145/2736084.2736091},
abstract = {Empowered by today's rich tools for media generation and collaborative production, the multimedia service paradigm is shifting from the conventional single source, to multi-source, to many sources, and now toward crowdsource. Such crowdsourced live streaming platforms as Twitch.tv allow general users to broadcast their content to massive viewers, thereby greatly expanding the content and user bases. The resources available for these non-professional broadcasters however are limited and unstable, which potentially impair the streaming quality and viewers' experience. The diverse live interactions among the broadcasters and viewers can further aggravate the problem.In this paper, we present an initial investigation on the modern crowdsourced live streaming systems. Taking Twitch as a representative, we outline their inside architecture using both crawled data and captured traffic of local broadcasters/viewers. Closely examining the access data collected in a two-month period, we reveal that the view patterns are determined by both events and broadcasters' sources. Our measurements explore the unique source- and event-driven views, showing that the current delay strategy on the viewer's side substantially impacts the viewers' interactive experience, and there is significant disparity between the long broadcast latency and the short live messaging latency. On the broadcaster's side, the dynamic uploading capacity is a critical challenge, which noticeably affects the smoothness of live streaming for viewers.},
booktitle = {Proceedings of the 25th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {55–60},
numpages = {6},
keywords = {view statistics, crowdsourced live streaming, Twitch.tv, interactive latency},
location = {Portland, Oregon},
series = {NOSSDAV '15}
}

@inproceedings{10.1145/2602458.2602476,
author = {Spacek, Petr and Dony, Christophe and Tibermacine, Chouki},
title = {A Component-Based Meta-Level Architecture and Prototypical Implementation of a Reflective Component-Based Programming and Modeling Language},
year = {2014},
isbn = {9781450325776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602458.2602476},
doi = {10.1145/2602458.2602476},
abstract = {Component-based Software Engineering studies the design, development and maintenance of software constructed upon sets of connected components. Using existing standard solutions, component-based models are frequently transformed into non-component-based programs, most of the time object-oriented, for run-time execution. As a consequence many component-level descriptions (part of code), e.g. explicit architectures or ports declarations, vanish at the implementation stage, making debugging, transformations or reverse-engineering difficult. It has been shown that component-based programming languages contribute to bridge this gap between design and implementation and to provide a conceptual and practical continuum to fully develop applications with components. In this paper we go one step further in this direction by making a component-oriented programming and modeling language truly reflective, thus making verification, evolution or transformation stages of software development part of this new continuum. The gained reflection capabilities indeed make it possible to perform architecture checking, code refactoring, model transformations or even to implement new languages constructs with and for components. The paper presents an original executable meta-level architecture achieving the vision that "everything is a component" and an operational implementation demonstrating its feasibility and effectiveness. Our system revisits some standard solutions for reification in the component's context and also handles new cases, such as ports reification, to allow for runtime introspection and intercession on components and on their descriptors. We validate these ideas in the context of an executable prototypical and minimal component-based language, named Compo, whose first goal is to help imagining the future.},
booktitle = {Proceedings of the 17th International ACM Sigsoft Symposium on Component-Based Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {component, reflection, architecture, programming, transformations, reflexive meta-model, modeling, constraints},
location = {Marcq-en-Bareul, France},
series = {CBSE '14}
}

@inproceedings{10.1145/2666620.2666630,
author = {Vidas, Timothy and Tan, Jiaqi and Nahata, Jay and Tan, Chaur Lih and Christin, Nicolas and Tague, Patrick},
title = {A5: Automated Analysis of Adversarial Android Applications},
year = {2014},
isbn = {9781450331555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666620.2666630},
doi = {10.1145/2666620.2666630},
abstract = {Mobile malware is growing - both in overall volume and in number of existing variants - at a pace rapid enough that systematic manual, human analysis is becoming increasingly difficult. As a result, there is a pressing need for techniques and tools that provide automated analysis of mobile malware samples. We present A5, an open source automated system to process Android malware. A5 is a hybrid system combining static and dynamic malware analysis techniques. Android's architecture permits many different paths for malware to react to system events, any of which may result in malicious behavior. Key innovations in A5 consist of novel methods of interacting with mobile malware to better coerce malicious behavior, and in combining both virtual and physical pools of Android platforms to capture behavior that could otherwise be missed. The primary output of A5 is a set of network threat indicators and intrusion detection system signatures that can be used to detect and prevent malicious network activity. We detail A5's distributed design and demonstrate applicability of our interaction techniques using examples from real malware. Additionally, we compare A5 with other automated systems and provide performance measurements of an implementation, using a published dataset of 1,260 unique malware samples, showing that A5 can quickly process large amounts of malware. We provide a public web interface to our implementation of A5 that allows third parties to use A5 as a web service.},
booktitle = {Proceedings of the 4th ACM Workshop on Security and Privacy in Smartphones \&amp; Mobile Devices},
pages = {39–50},
numpages = {12},
keywords = {virtualization, static analysis, sandbox, dynamic analysis, mobile malware, malicious behavior},
location = {Scottsdale, Arizona, USA},
series = {SPSM '14}
}

@inproceedings{10.1145/3030207.3044531,
author = {Jun, Tae Joon and Yoo, Myong Hwan and Kim, Daeyoung and Cho, Kyu Tae and Lee, Seung Young and Yeun, Kyuoke},
title = {HPC Supported Mission-Critical Cloud Architecture},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3044531},
doi = {10.1145/3030207.3044531},
abstract = {Tactical Operations Center (TOC) system in military field is an advanced computer system composed of multiple servers and desktops to interlock internal/external weapon systems processing mission-critical applications in combat situation. However, the current TOC system has several limitations such as difficulty of integrating tactical weapon systems including missile launch system and radar system into the single TOC system due to the heterogeneity of HW and SW between systems, and an inefficient computing resource management for the weapon systems.In this paper, we proposed a novel HPC supported mission-critical Cloud architecture as TOC for Surface-to-Air-Missile (SAM) system with OpenStack Cloud OS, Data Distribution Service (DDS), and GPU virtualization techniques. With this approach, our system provides elastic resource management over the weapon systems with virtual machines, integration of heterogeneous systems with different kinds of guest OS, real-time, reliable, and high-speed communication between the virtual machines and virtualized GPU resource over the virtual machines. Evaluation of our TOC system includes DDS performance measurement over 10Gbps Ethernet and QDR InfiniBand networks on the virtualized environment with OpenStack Cloud OS, and GPU virtualization performance evaluation with two different methods, PCI pass-through and remote-API. With the evaluation results, we conclude that our system provides reasonable performance in the combat situation compared to the previous TOC system while additionally supports scalable and elastic use of computing resource through the virtual machines.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {223–232},
numpages = {10},
keywords = {tactical operations center, cloud computing, data distribution service, gpgpu},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/3120895.3120916,
author = {Arndt, Oliver Jakob and Spindeldreier, Christian and Wohnrade, Kevin and Pfefferkorn, Daniel and Neuenhahn, Martin and Blume, Holger},
title = {FPGA Accelerated NoC-Simulation: A Case Study on the Intel Xeon Phi Ringbus Topology},
year = {2017},
isbn = {9781450353168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3120895.3120916},
doi = {10.1145/3120895.3120916},
abstract = {Complex signal processing algorithms targeted on architectures with increasingly high numbers of parallel processing units require high performance core-interconnections (i.e., low latencies, high throughput, no pinch-offs or bottlenecks). Therefore, assisting techniques, exploring characteristics of diverse topologies of common as well as innovative Network-on-Chips (NoCs), are necessary for the development of chips with massive parallel processing cores. In contrast to analytic NoC models, event driven NoC simulations can handle even complex task graphs, but however feature long simulation times. Enabling the simulation of even complex task graphs, in this work, we propose to use FPGA accelerated simulation. While we extend such a simulator in order to imitate cache coherence communication-behavior, we also present a translation of real measured profiles to task graphs for in-depth simulation of the communication behavior of an existing NoC-based manycore. Therefore, this approach is able to not only deal with synthetic scenarios, but analyse the communication behavior of real world applications. Additionally, a simulation of the Histograms of Oriented Gradients algorithm, running on the Intel Xeon Phi manycore, exhibiting a 70-stop ring-bus, exemplifies this approach.},
booktitle = {Proceedings of the 8th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
articleno = {21},
numpages = {6},
location = {Bochum, Germany},
series = {HEART '17}
}

@inproceedings{10.1145/2684746.2689095,
author = {Ben Fakih, Hichem and Elhossini, Ahmed and Juurlink, Ben},
title = {An Efficient and Flexible FPGA Implementation of a Face Detection System (Abstract Only)},
year = {2015},
isbn = {9781450333153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684746.2689095},
doi = {10.1145/2684746.2689095},
abstract = {Robust and rapid face detection systems are constantly gaining more interest, since they represent the first stone for many challenging tasks in the field of computer vision. In this paper a software-hardware co-design approach is presented, that enables the detection of frontal faces in real time. A complete hardware implementation of all components taking part of the face detection is introduced. This work is based on the object detection framework of Viola and Jones, which makes use of a cascade of classifiers to reduce the computation time. The proposed architecture is flexible, as it allows the use of multiple instances of the face detector. This makes developers free to choose the speed range and reserved resources for this task. The current implementation runs on the Zynq SoC and receives images over IP network, which allows exposing the face detection task as a remote service that can be consumed from any device connected to the network. We performed several measurements for the final detector and the software equivalent. Using three Evaluator cores, the ZedBoard system achieves a maximal average frame rate of 13.4 FPS when analysing an image containing 640x480 pixels. This stands for an improvement of 5.25 times compared to the software solution and represents acceptable results for most real-time systems. On the ZC706 system, a higher frame rate of 16.58 FPS is achieved. The proposed hardware solution achieved 92\% accuracy, which is low compared to the software solution (97\%) due to different scaling algorithm. The proposed solution achieved higher frame rate compared to other solutions found in the literature.},
booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {261},
numpages = {1},
keywords = {copmuter visioin, fpga, viola and jones, zynq, face detection},
location = {Monterey, California, USA},
series = {FPGA '15}
}

@inproceedings{10.1145/3437120.3437284,
author = {Psilias, Dimitrios and Milidonis, Athanasios and Voyiatzis, Ioannis},
title = {Architecture for Secure UAV Systems},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437284},
doi = {10.1145/3437120.3437284},
abstract = {UAV applications are providing an extended range of services in society's needs. These applications require high execution speed and security to all transmitted data. In this paper an architecture is proposed for secure UAV applications. The architecture consists of a microcontroller to execute the flight controller tasks and a FPGA for implementing the security related tasks. The microcontroller is an Arduino which is widely used in UAVs. Arduino communicates with all sensors and generates outputs needed for controlling the UAV's motors. The circuit inside the FPGA encrypts/decrypts data related to transmission. Measurements taken concerning the execution time and power consumption, reveal the benefits of the extra hardware added for encryption/decryption in comparison with those of a single microcontroller.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {99–102},
numpages = {4},
location = {Athens, Greece},
series = {PCI '20}
}

@inproceedings{10.1145/3230833.3233251,
author = {Blanc, Gregory and Kheir, Nizar and Ayed, Dhouha and Lefebvre, Vincent and de Oca, Edgardo Montes and Bisson, Pascal},
title = {Towards a 5G Security Architecture: Articulating Software-Defined Security and Security as a Service},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233251},
doi = {10.1145/3230833.3233251},
abstract = {5G is envisioned as a transformation of the communications architecture towards multi-tenant, scalable and flexible infrastructure, which heavily relies on virtualised network functions and programmable networks. In particular, orchestration will advance one step further in blending both compute and data resources, usually dedicated to virtualisation technologies, and network resources into so-called slices. Although 5G security is being developed in current working groups, slice security is seldom addressed.In this work, we propose to integrate security in the slice life cycle, impacting its management and orchestration that relies on the virtualization/softwarisation infrastructure. The proposed security architecture connects the demands specified by the tenants through as-a-service mechanisms with built-in security functions relying on the ability to combine enforcement and monitoring functions within the software-defined network infrastructure. The architecture exhibits desirable properties such as isolating slices down to the hardware resources or monitoring service-level performance.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {47},
numpages = {8},
keywords = {Security as a Service, Network Slicing, Software-Defined Security},
location = {Hamburg, Germany},
series = {ARES '18}
}

@inproceedings{10.4108/icst.pervasivehealth.2014.255331,
author = {Weiss, Patrick and Heldmann, Marcus and Gabrecht, Alexander and Schweikard, Achim and M\"{u}nte, Thomas M. and Maehle, Erik},
title = {A Low Cost Tele-Rehabilitation Device for Training of Wrist and Finger Functions after Stroke},
year = {2014},
isbn = {9781631900112},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.pervasivehealth.2014.255331},
doi = {10.4108/icst.pervasivehealth.2014.255331},
abstract = {There is a need for robotic rehabilitation devices that improve the outcome while reducing the cost of therapy. This paper presents a device for training of supination/pronation, dorsal wrist extension, and finger manipulation after stroke. The system exhibits modularity in terms of the communication architecture and different optional components. User interfaces (UI) can be implemented on different kinds of devices including a Rasperry Pi single-board computer on which a Qt-based graphical UI was run in this instance. Tele-rehabilitation functionality is included using SSL-encrypted RESTful web services on a three-tier architecture. Expensive sensors were omitted in order to have a cost-effective system which is a requirement for home-based rehabilitation. The current-based torque sensing is evaluated by comparing current measurements to force-torque sensor values. After canceling out the static friction, the low error justified the omission of an additional sensor.},
booktitle = {Proceedings of the 8th International Conference on Pervasive Computing Technologies for Healthcare},
pages = {422–425},
numpages = {4},
keywords = {robotic rehabilitation, stroke, wrist and finger functions, home health care, tele-rehabilitation},
location = {Oldenburg, Germany},
series = {PervasiveHealth '14}
}

@article{10.1145/3399742,
author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
title = {Spectre Attacks: Exploiting Speculative Execution},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3399742},
doi = {10.1145/3399742},
abstract = {Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try to guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access the victim's memory and registers, and can perform operations with measurable side effects.Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side-channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, such as operating system process separation, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing and side-channel attacks. These attacks represent a serious threat to actual systems because vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices.Although makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.},
journal = {Commun. ACM},
month = {jun},
pages = {93–101},
numpages = {9}
}

@article{10.1145/2935634.2935640,
author = {Orwat, Carsten and Bless, Roland},
title = {Values and Networks: Steps Toward Exploring Their Relationships},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2935634.2935640},
doi = {10.1145/2935634.2935640},
abstract = {Many technical systems of the Information and Communication Technology (ICT) sector enable, structure and/or constrain social interactions. Thereby, they influence or implement certain values, including human rights, and affect or raise conflicts among values. The ongoing developments toward an "Internet of everything'' is likely to lead to further value conflicts. This trend illustrates that a better understanding of the relationships between social values and networks is urgently needed because it is largely unknown what values lie behind protocols, design principles, or technical and organizational options of the Internet. This paper focuses on the complex steps of realizing human rights in Internet architectures and protocols as well as in Internet-based products and services. Besides direct implementation of values in Internet protocols, there are several other options that can indirectly contribute to realizing human rights via political processes and market choices. Eventually, a better understanding of what values can be realized by networks in general, what technical measures may affect certain values, and where complementary institutional developments are needed may lead toward a methodology for considering technical and institutional systems together.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {25–31},
numpages = {7},
keywords = {governance, rules, human rights, values, institutions, communication protocols, network design}
}

@inproceedings{10.1145/2785592.2785611,
author = {Adjepon-Yamoah, David and Romanovsky, Alexander and Iliasov, Alexei},
title = {A Reactive Architecture for Cloud-Based System Engineering},
year = {2015},
isbn = {9781450333467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785592.2785611},
doi = {10.1145/2785592.2785611},
abstract = {The paper introduces an architecture to support system engineering on the cloud. It employs the main benefits of the cloud: scalability, parallelism, cost-effectiveness, multi-user access and flexibility. The architecture includes an open toolbox which provides tools as a service to support various phases of system engineering. The architecture uses the Open Services for Life-cycle Collaboration (OSLC) technology to create a reactive middleware that informs all stakeholders about any changes in the development artefacts. It facilitates the interoperability of tools and enables the workflow of tools to support complex engineering steps. Another component of the architecture is a shared repository of artefacts. All the artefacts generated during a system engineering process are stored in the repository, and can be accessed by relevant stakeholders. The shared repository also serves as a platform to support a protocol for formal model decomposition and group work on the decomposed models. Finally, the architecture includes components for ensuring the dependability of the system engineering process.},
booktitle = {Proceedings of the 2015 International Conference on Software and System Process},
pages = {77–81},
numpages = {5},
keywords = {dependability, artefact repository, reactive architecture, cloud computing, system engineering},
location = {Tallinn, Estonia},
series = {ICSSP 2015}
}

@inproceedings{10.1145/2789168.2790089,
author = {Kurose, James F.},
title = {Research Challenges and Opportunities in a Mobility-Centric World},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2790089},
doi = {10.1145/2789168.2790089},
abstract = {The Internet recently passed an historic inflection point, with the number of broadband mobile devices surpassing the number of wired PCs and servers connected to the Internet. Mobility now profoundly affects the architecture, services and applications in both the wireless and wired domains. In this "bottom up" talk, we begin by discussing several specific mobility-related challenges and recent results in areas including mobility measurement (including privacy considerations) and modeling, and context-sensitive services. We then take a broader look at current and future challenges, and conclude by discussing several NSF investments in programs and projects in area of mobile networking.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {290},
numpages = {1},
keywords = {measurement modeling, architecture, mobility, computer networks},
location = {Paris, France},
series = {MobiCom '15}
}

@inproceedings{10.1145/2987443.2987482,
author = {Orsini, Chiara and King, Alistair and Giordano, Danilo and Giotsas, Vasileios and Dainotti, Alberto},
title = {BGPStream: A Software Framework for Live and Historical BGP Data Analysis},
year = {2016},
isbn = {9781450345262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987443.2987482},
doi = {10.1145/2987443.2987482},
abstract = {We present BGPStream, an open-source software framework for the analysis of both historical and real-time Border Gateway Protocol (BGP) measurement data. Although BGP is a crucial operational component of the Internet infrastructure, and is the subject of research in the areas of Internet performance, security, topology, protocols, economics, etc., there is no efficient way of processing large amounts of distributed and/or live BGP measurement data. BGPStream fills this gap, enabling efficient investigation of events, rapid prototyping, and building complex tools and large-scale monitoring applications (e.g., detection of connectivity disruptions or BGP hijacking attacks). We discuss the goals and architecture of BGPStream. We apply the components of the framework to different scenarios, and we describe the development and deployment of complex services for global Internet monitoring that we built on top of it.},
booktitle = {Proceedings of the 2016 Internet Measurement Conference},
pages = {429–444},
numpages = {16},
keywords = {bgp monitoring, internet routing, network measurement, network monitoring, bgp measurement, real-time monitoring, internet measurement},
location = {Santa Monica, California, USA},
series = {IMC '16}
}

@inproceedings{10.1145/2789168.2790094,
author = {Cui, Yong and Lai, Zeqi and Wang, Xin and Dai, Ningwei and Miao, Congcong},
title = {QuickSync: Improving Synchronization Efficiency for Mobile Cloud Storage Services},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2790094},
doi = {10.1145/2789168.2790094},
abstract = {Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstance even though the incremental sync is implemented. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 52.9\% sync time in our experiment settings.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {592–603},
numpages = {12},
keywords = {mobile cloud storage, measurement, performance},
location = {Paris, France},
series = {MobiCom '15}
}

@article{10.1145/2998573,
author = {Fernandes, Fernando and Weigel, Lucas and Jung, Claudio and Navaux, Philippe and Carro, Luigi and Rech, Paolo},
title = {Evaluation of Histogram of Oriented Gradients Soft Errors Criticality for Automotive Applications},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2998573},
doi = {10.1145/2998573},
abstract = {Pedestrian detection reliability is a key problem for autonomous or aided driving, and methods that use Histogram of Oriented Gradients (HOG) are very popular. Embedded Graphics Processing Units (GPUs) are exploited to run HOG in a very efficient manner. Unfortunately, GPUs architecture has been shown to be particularly vulnerable to radiation-induced failures. This article presents an experimental evaluation and analytical study of HOG reliability. We aim at quantifying and qualifying the radiation-induced errors on pedestrian detection applications executed in embedded GPUs.We analyze experimental results obtained executing HOG on embedded GPUs from two different vendors, exposed for about 100 hours to a controlled neutron beam at Los Alamos National Laboratory. We consider the number and position of detected objects as well as precision and recall to discriminate critical erroneous computations. The reported analysis shows that, while being intrinsically resilient (65\% to 85\% of output errors only slightly impact detection), HOG experienced some particularly critical errors that could result in undetected pedestrians or unnecessary vehicle stops.Additionally, we perform a fault-injection campaign to identify HOG critical procedures. We observe that Resize and Normalize are the most sensitive and critical phases, as about 20\% of injections generate an output error that significantly impacts HOG detection. With our insights, we are able to find those limited portions of HOG that, if hardened, are more likely to increase reliability without introducing unnecessary overhead.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {38},
numpages = {25},
keywords = {pedestrian detection, HOG}
}

@inproceedings{10.1145/2656075.2656086,
author = {Hsieh, Chih-Ming and Samie, Farzad and Srouji, M. Sammer and Wang, Manyi and Wang, Zhonglei and Henkel, J\"{o}rg},
title = {Hardware/Software Co-Design for a Wireless Sensor Network Platform},
year = {2014},
isbn = {9781450330510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656075.2656086},
doi = {10.1145/2656075.2656086},
abstract = {Wireless sensor networks have become shared resources providing sensing services to monitor ambient environment. The tasks performed by the sensor nodes and the network structure are becoming more and more complex so that they cannot be handled efficiently by traditional sensor nodes any more. The traditional sensor node architecture, which has software implementation running on a fixed hardware design, is no longer fit to the changing requirements when new applications with complex computation are added to this shared infrastructure due to several reasons. First, the operation behavior changes because of the application requirements and the environmental conditions which makes a fixed architecture not efficient all the time. Second, to collaborate with other already deployed sensor networks and to maintain an efficient network structure, the sensor nodes require flexible communication capabilities. Furthermore, the information required to determine an efficient hardware/software co-design under the system constraints cannot be known a priori. Therefore a platform which can adapt to run-time situations will play an important role in wireless sensor networks. In this paper, we present a hardware/software co-design framework for a wireless sensor platform, which can adaptively change its hardware/software configuration to accelerate complex operations and provides a flexible communication mechanism to deal with complex network structures. We perform real-world measurements on our prototype to analyze its capabilities. In addition, our case studies with prototype implementation and network simulations show the energy savings of the sensor network application by using the proposed design with run-time adaptivity.},
booktitle = {Proceedings of the 2014 International Conference on Hardware/Software Codesign and System Synthesis},
articleno = {1},
numpages = {10},
keywords = {sensor networks, FPGA, multi-radio, reconfiguration, hardware accelerator, low power},
location = {New Delhi, India},
series = {CODES '14}
}

@inproceedings{10.1145/3132340.3132358,
author = {Olariu, Stephan and Florin, Ryan},
title = {Vehicular Clouds Research: What is Missing?},
year = {2017},
isbn = {9781450351645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132340.3132358},
doi = {10.1145/3132340.3132358},
abstract = {Vehicular Clouds (VCs) have become an active research topic. However, even a cursory look reveals that the VC literature of recent years is full of papers discussing fanciful VC architectures and services that often seem too good to be true. And many of them are. It seems to us that promoting VC models without any regard to their practical feasibility is apt to discredit the VC concept altogether. Part of the problem stems from the fact that some authors do not seem to be concerned with the obvious fact that moving vehicles' residency times in the VC may, indeed, be very short and, therefore, so is their contribution to the amount of useful work performed. Should a vehicle running a user job leave the VC prematurely, the amount of work performed by that vehicle may be lost, unless special precautions are taken. Such precautionary measures involve either some flavor of checkpointing or some form of redundant job assignment. Both approaches have consequences in terms of overhead and impact job completion time. The success of conventional cloud computing (CC) is attributable to the ability to provide quantifiable functional characteristics such as scalability, reliability and availability. By the same token, if the VCs are to see a widespread adoption, the same quantitative aspects have to be addressed here, too. Feasibility issues in terms of sufficient compute power, communication bandwidth, reliability, availability, and job duration time are all fundamental quantitative aspects of VCs that need to be studied and understood before one can claim with any degree of certainty that they can support the workload for which they are intended. The first contribution of this paper is to make a case for the stringent need to address quantitatively the performance characteristics of VC architectures and proposed services. Our second contribution is to point out directions and challenges facing the VC community.},
booktitle = {Proceedings of the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {77–84},
numpages = {8},
keywords = {vehicular clouds, reliability, redundancy, availability, acm proceedings, cloud computing},
location = {Miami, Florida, USA},
series = {DIVANet '17}
}

@article{10.1109/TNET.2020.2981514,
author = {Li, Yang and Zheng, Jianwei and Li, Zhenhua and Liu, Yunhao and Qian, Feng and Bai, Sen and Liu, Yao and Xin, Xianlong},
title = {Understanding the Ecosystem and Addressing the Fundamental Concerns of Commercial MVNO},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2981514},
doi = {10.1109/TNET.2020.2981514},
abstract = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs), which operate on top of existing cellular infrastructures of base carriers, while offering cheaper or more flexible data plans compared to those of the base carriers. In this paper, we present a two-year measurement study towards understanding various fundamental aspects of today's MVNO ecosystem, including its architecture, customers, performance, economics, and the complex interplay with the base carrier. Our study focuses on a large commercial MVNO with one million customers, operating atop a nation-wide base carrier. Our measurements clarify several key concerns raised by MVNO customers, such as inaccurate billing and potential performance discrimination with the base carrier. We also leverage big data analytics, statistical modeling, and machine learning to address the MVNO's key concerns with regard to data usage prediction, data plan reselling, customer churn mitigation, and billing delay reduction. Our proposed techniques can help achieve higher revenues and improved services for commercial MVNOs.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1364–1377},
numpages = {14}
}

@inproceedings{10.1145/3307334.3326070,
author = {Xiao, Ao and Liu, Yunhao and Li, Yang and Qian, Feng and Li, Zhenhua and Bai, Sen and Liu, Yao and Xu, Tianyin and Xin, Xianlong},
title = {An In-Depth Study of Commercial MVNO: Measurement and Optimization},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326070},
doi = {10.1145/3307334.3326070},
abstract = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs), which operate on top of the existing cellular infrastructures of base carriers while offering cheaper or more flexible data plans compared to those of the base carriers. In this paper, we present a nearly two-year measurement study towards understanding various key aspects of today's MVNO ecosystem, including its architecture, performance, economics, customers, and the complex interplay with the base carrier. Our study focuses on a large commercial MVNO with reviseabout 1 million customers, operating atop a nation-wide base carrier. Our measurements clarify several key concerns raised by MVNO customers, such as inaccurate billing and potential performance discrimination with the base carrier. We also leverage big data analytics and machine learning to optimize an MVNO's key businesses such as data plan reselling and customer churn mitigation. Our proposed techniques can help achieve \%will lead to higher revenues and improved services for commercial MVNOs.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {457–468},
numpages = {12},
keywords = {network performance, mvno, machine learning, churn mitigation, data prediction},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@inproceedings{10.1109/CCGRID.2017.27,
author = {Colmant, Maxime and Felber, Pascal and Rouvoy, Romain and Seinturier, Lionel},
title = {WattsKit: Software-Defined Power Monitoring of Distributed Systems},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.27},
doi = {10.1109/CCGRID.2017.27},
abstract = {The design and the deployment of energy-efficient distributed systems is a challenging task, which requires software engineers to consider all the layers of a system, from hardware to software. In particular, monitoring and analyzing the power consumption of a distributed system spanning several---potentially heterogeneous---nodes becomes particularly tedious when aiming at a finer granularity than observing the power consumption of hosting nodes. While the state-of-the-art in software-defined power meters fails to deliver adaptive solutions to offer such service-level perspective and to cope with the diversity of hardware CPU architectures, this paper proposes to automatically learn the power models of the nodes supporting a distributed system, and then to use these inferred power models to better understand how the power consumption of the system's processes is distributed across nodes at runtime.Our solution, named WattsKit, offers a modular toolkit to build software-defined power meters "\`{a} la carte", thus dealing with the diversity of user and hardware requirements. Beyond the demonstrated capability of covering a wide diversity of CPU architectures with high accuracy, we illustrate the benefits of adopting software-defined power meters to analyze the power consumption of complex layered and distributed systems. In particular, we illustrate the capability of our approach to monitor the power consumption of a system composed of Docker Swarm, Weave,Elasticsearch, and Apache ZooKeeper. Thanks to WattsKit, developers and administrators are now able to identify potential power leaks in their software infrastructure.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {514–523},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3404555.3404632,
author = {Lin, Huang and Diangang, Wang and Xiao, Liu and Yongning, Zhuo and Yong, Zeng},
title = {A Predictor Based on Parallel LSTM for Burst Network Traffic Flow},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404632},
doi = {10.1145/3404555.3404632},
abstract = {The network traffic prediction is a key step for service quality control in computer network. Aimed at the problem that the performance of the traditional prediction method significantly degrades for the burst short-term flow, this paper proposed a double LSTM architecture, one of which acts as the main flow predictor, another as the detector for the moment the burst flow starts. The two LSTM unit can exchange their internal state's information, and the predictor uses the detector's information to improve the accuracy of the prediction. To train the offline double LSTM architecture, a Depth-Backstep algorithm is put forward. To use the architecture to perform the online prediction, a pulse series is used as a simulant of the burst event. A simulation experiment is designed to test performance of the predictor. The results of the experiment show that the prediction accuracy of the double LSTM architecture is significantly improved, compared with the traditional single LSTM architecture.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {476–480},
numpages = {5},
keywords = {computer network, Traffic prediction, machine learning, LSTM},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1145/3053600.3053634,
author = {Walter, J\"{u}rgen and Stier, Christian and Koziolek, Heiko and Kounev, Samuel},
title = {An Expandable Extraction Framework for Architectural Performance Models},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053634},
doi = {10.1145/3053600.3053634},
abstract = {Providing users with Quality of Service (QoS) guarantees and the prevention of performance problems are challenging tasks for software systems. Architectural performance models can be applied to explore performance properties of a software system at design time and run time. At design time, architectural performance models support reasoning on effects of design decisions. At run time, they enable automatic reconfigurations by reasoning on the effects of changing user behavior. In this paper, we present a framework for the extraction of architectural performance models based on monitoring log files generalizing over the targeted architectural modeling language. Using the presented framework, the creation of a performance model extraction tool for a specific modeling formalism requires only the implementation of a key set of object creation routines specific to the formalism. Our framework integrates them with extraction techniques that apply to many architectural performance models, e.g., resource demand estimation techniques. This lowers the effort to implement performance model extraction tools tremendously through a high level of reuse. We evaluate our framework presenting builders for the Descartes Modeling Language (DML) and the Palladio Component Model(PCM). For the extracted models we compare simulation results with measurements receiving accurate results.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {165–170},
numpages = {6},
keywords = {builder pattern, descartes modeling language, palladio component model, automated performance model extraction},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1109/ASE.2015.93,
author = {Salama, Maria},
title = {Stability of Self-Adaptive Software Architectures},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.93},
doi = {10.1109/ASE.2015.93},
abstract = {Stakeholders and organisations are increasingly looking for long-lived software. As architectures have a profound effect on the operational life-time of the software and the quality of the service provision, architectural stability could be considered a primary criterion towards achieving the long-livety of the software. Architectural stability is envisioned as the next step in quality attributes, combining many inter-related qualities. This research suggests the notion of behavioural stability as a primary criterion for evaluating whether the architecture maintains achieving the expected quality attributes, maintaining architecture robustness, and evaluating how well the architecture accommodates run-time evolutionary changes. The research investigates the notion of architecture stability at run-time in the context of self-adaptive software architectures. We expect to define, characterise and analyse this intuitive concept, as well as identify the consequent trade-offs to be dynamically managed and enhance the self-adaptation process for a long-lived software.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {886–889},
numpages = {4},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.5555/2685048.2685073,
author = {Zhai, Ennan and Chen, Ruichuan and Wolinsky, David Isaac and Ford, Bryan},
title = {Heading off Correlated Failures through Independence-as-a-Service},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {Today's systems pervasively rely on redundancy to ensure reliability. In complex multi-layered hardware/software stacks, however - especially in the clouds where many independent businesses deploy interacting services on common infrastructure - seemingly independent systems may share deep, hidden dependencies, undermining redundancy efforts and introducing unanticipated correlated failures. Complementing existing post-failure forensics, we propose Independence-as-a-Service (or INDaaS), an architecture to audit the independence of redundant systems proactively, thus avoiding correlated failures. INDaaS first utilizes pluggable dependency acquisition modules to collect the structural dependency information (including network, hardware, and software dependencies) from a variety of sources. With this information, INDaaS then quantifies the independence of systems of interest using pluggable auditing modules, offering various performance, precision, and data secrecy tradeoffs. While the most general and efficient auditing modules assume the auditor is able to obtain all required information, INDaaS can employ private set intersection cardinality protocols to quantify the independence even across businesses unwilling to share their full structural information with anyone. We evaluate the practicality of INDaaS with three case studies via auditing realistic network, hardware, and software dependency structures.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {317–334},
numpages = {18},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{10.1145/2627566.2627575,
author = {Antonescu, Alexandru-Florian and Braun, Torsten},
title = {Modeling and Simulation of Concurrent Workload Processing in Cloud-Distributed Enterprise Information Systems},
year = {2014},
isbn = {9781450329927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627566.2627575},
doi = {10.1145/2627566.2627575},
abstract = {Cloud Computing enables provisioning and distribution of highly scalable services in a reliable, on-demand and sustainable manner. Distributed Enterprise Information Systems (dEIS) are a class of applications with important economic value and with strong requirements in terms of performance and reliability. In order to validate dEIS architectures, stability, scaling and SLA compliance, large testing deployments are necessary, adding complexity to the design and testing of such systems. To fill this gap, we present and validate a methodology for modeling and simulating such complex distributed systems using the CloudSim cloud computing simulator, based on measurement data from an actual distributed system. We present an approach for creating a performance-based model of a distributed cloud application using recorded service performance traces. We then show how to integrate the created model into CloudSim. We validate the CloudSim simulation model by comparing performance traces gathered during distributed concurrent experiments with simulation results using different VM configurations. We demonstrate the usefulness of using a cloud simulator for modeling properties of real cloud-distributed applications.},
booktitle = {Proceedings of the 2014 ACM SIGCOMM Workshop on Distributed Cloud Computing},
pages = {11–16},
numpages = {6},
keywords = {cloud computing, distributed applications, modelling and simulation, performance profiling},
location = {Chicago, Illinois, USA},
series = {DCC '14}
}

@inproceedings{10.5555/3395101.3395117,
author = {Patan\'{e}, Giancarlo M. M. and Valastro, Gianluca C. and Sambo, Yusuf A. and Ozturk, Metin and Hussain, Sajjad and Imran, Muhammad A. and Panno, Daniela},
title = {Flexible SDN/NFV-Based SON Testbed for 5G Mobile Networks},
year = {2020},
isbn = {9781728129235},
publisher = {IEEE Press},
abstract = {In the next few years, a considerable innovation concerning the design of the future 5G mobile networks will be a concrete step towards enabling effective high throughput and low latency services. Software Defined Networking (SDN), Network Function Virtualization (NFV) and Self Organizing Network (SON) are considered the enabling technologies to achieve these goals. In this paper, assuming a Control-Data Separation Architecture (CDSA), we propose a flexible SDN/NFV-based SON testbed, for future 5G mobile networks. The main contribution of our work is to cover the need for a CDSA based testbed, enabling the investigation of the NG-SON capabilities for practical implementations. We implement two different testbed setups, a real one and a virtualized one, both based on the FlexRAN and OpenAirInterface software tools. First, we implement a specific case study, i.e., the RAN entities activation/deactivation procedures. Next, we carry out time measurements, concerning the aforementioned procedures, in order to prove proper testbed functioning. Finally, we validate the C-SON and D-SON capabilities of our testbed, considering the features of the results.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Symposium on Distributed Simulation and Real Time Applications},
pages = {79–86},
numpages = {8},
keywords = {5G, FlexRAN, NFV, Cloud-RAN, SDN, OpenAirInterface},
location = {Cosenza, Italy},
series = {DS-RT '19}
}

@article{10.1109/TNET.2020.2976129,
author = {Xie, Kun and Chen, Yuxiang and Wang, Xin and Xie, Gaogang and Cao, Jiannong and Wen, Jigang},
title = {Accurate and Fast Recovery of Network Monitoring Data: A GPU Accelerated Matrix Completion},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2976129},
doi = {10.1109/TNET.2020.2976129},
abstract = {Gaining a full knowledge of end-to-end network performance is important for some advanced network management and services. Although it becomes increasingly critical, end-to-end network monitoring usually needs active probing of the path and the overhead will increase quadratically with the number of network nodes. To reduce the measurement overhead, matrix completion is proposed recently to predict the end-to-end network performance among all node pairs by only measuring a small set of paths. Despite its potential, applying matrix completion to recover the missing data suffers from low recovery accuracy and long recovery time. To address the issues, we propose MC-GPU to exploit Graphics Processing Units (GPUs) to enable parallel matrix factorization for high-speed and highly accurate Matrix Completion. To well exploit the special architecture features of GPUs for both task independent and data-independent parallel task execution, we propose several novel techniques: similar OD (origin and destination) pairs reordering taking advantage of the locality-sensitive hash (LSH) functions, balanced matrix partition, and parallel matrix completion. We implement the proposed MC-GPU on the GPU platform and evaluate the performance using real trace data. We compare the proposed MC-GPU with the state of the art matrix completion algorithms, and our results demonstrate that MC-GPU can achieve significantly faster speed with high data recovery accuracy.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {958–971},
numpages = {14}
}

@inproceedings{10.1145/2980258.2980430,
author = {Muthuraman, Sangeetha and Venkatesan, V. Prasanna},
title = {Design of QOS Based Web Service Selection/Composition Hyper-Heuristic Model},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2980430},
doi = {10.1145/2980258.2980430},
abstract = {A web service selection/composition problem is a NP-complete problem that cannot be solved in polynomial time. An efficient solution is essential to solve this problem. This solution may be attained by following hyper-heuristic strategies. As a first step in addressing the problem, this paper presents a new web services selection/composition model which enables such a hyper-heuristic notion. Various parts of this proposed model can be implemented by using different algorithms thus enabling many hybrid implementations. In this paper the proposed model has been implemented by using a reference score and trust based service selection algorithm and a strategic tree based service composition algorithm. To realize this implementation agent based architecture has been proposed. A well defined QOS model has been used to accurately receive customer's request and update service specific quality values. The algorithms implemented are efficient as the computational complexities of these algorithms have been greatly reduced and also a fault tolerant approach has been adopted. The experimental results illustrate that the proposed model and algorithms have effectively solved the web services selection/composition problem.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {80},
numpages = {10},
keywords = {service composition, QOS based web service selection and composition, web services selection/composition model, Service selection, hyper-heuristic model},
location = {Pondicherry, India},
series = {ICIA-16}
}

@inproceedings{10.1145/3473714.3473742,
author = {Xin, An},
title = {Research on Multi-Sensor Fusion Perception Method of Vehicle-Infrastructure Collaboration for Smart Automobiles},
year = {2021},
isbn = {9781450390231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473714.3473742},
doi = {10.1145/3473714.3473742},
abstract = {Traffic congestion should be solved, it reduce traffic safety potential risks, and improve people's travel efficiency. The paper based on intelligent car's own operation characteristics (Smart car perception is generally only 300 ~ 500 meters, there is a perceived problem of super-visual distance and vision blind zone), In order to effectively improve the safety operation level of smart cars, and combined with intelligent non-smart cars on the road may encounter cars a smart car trip perception highway outside the scope of frequently asked questions, such as over-site sensations, the whole scene and area's perception, the blind area, the emergency corner, tunnel, bridge, other highways travel common scenes and so on. This paper is based on new infrastructure transformations or newly build's research and practical results such as road traffic intelligence infrastructure, it deployed the current road traffic to intelligently infrastructure, especially the technical difficulties existing in the process of common perceptual equipment (smart cameras, radar) are synonymous with multiple sensor information fusion perceptions, it proposed a multi-sensor fusion algorithm based on error variance, and designed a multi-object multi-sensor data processing system architecture. This paper also proposes traffic operation scheduling architecture based on the game theory of car road synergies on the basis of multi-sensor data fusion. Finally, these architectures were analyzed using computer simulation techniques. The results show that the traffic operation schedule for multi-sensor fusion algorithm based on error variance and game theory based on the study proposed this study can be more obvious. The safety and efficiency of the road traffic environment of smart vehicles. Optimization, smart vehicles equipped with smart vehicles are also more than 25\% higher than traditional common vehicles in terms of vehicle safety. All in all, this study proposed to synergistic multi-sensor convergence method for smart cars, that based on smart car a smart car perception, compared to non-smart roads after intelligent infrastructure construction and transformation of road traffic intelligent transportation systems, Higher efficiency and more intelligent can better solve the common problems in road traffic environment, providing people with safer, efficient and high-quality traffic travel services.},
booktitle = {Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics},
pages = {164–175},
numpages = {12},
keywords = {smart vehicle, perceptual method research, multi-sensor fusion, vehicle-infrastructure collaboration},
location = {Guangzhou, China},
series = {ICCIR '21}
}

@article{10.1109/TNET.2013.2253797,
author = {De Cicco, Luca and Mascolo, Saverio},
title = {An Adaptive Video Streaming Control System: Modeling, Validation, and Performance Evaluation},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2013.2253797},
doi = {10.1109/TNET.2013.2253797},
abstract = {Adaptive video streaming is a relevant advancement with respect to classic progressive download streaming a la YouTube. Among the different approaches, the video stream-switching technique is getting wide acceptance, being adopted by Microsoft, Apple, and popular video streaming services such as Akamai, Netflix, Hulu, Vudu, and Livestream. In this paper, we present a model of the automatic video stream-switching employed by one of these leading video streaming services along with a description of the client-side communication and control protocol. From the control architecture point of view, the automatic adaptation is achieved by means of two interacting control loops having the controllers at the client and the actuators at the server: One loop is the buffer controller, which aims at steering the client playout buffer to a target length by regulating the server sending rate; the other one implements the stream-switching controller and aims at selecting the video level. A detailed validation of the proposed model has been carried out through experimental measurements in an emulated scenario.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {526–539},
numpages = {14},
keywords = {performance evaluation, modeling, stream-switching, adaptive video streaming}
}

@inproceedings{10.1145/2664591.2664611,
author = {Poulo, Lebeko and Phiri, Lighton and Suleman, Hussein},
title = {Fine-Grained Scalability of Digital Library Services in the Cloud},
year = {2014},
isbn = {9781450332460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2664591.2664611},
doi = {10.1145/2664591.2664611},
abstract = {Modern digital library systems are increasingly handling massive data volumes; this content needs to be stored, indexed and made easily accessible to end users. Cloud computing promises to address some of these needs through a set of services that arguably support scalability of service provision. This paper discusses a set of experiments to assess the scalability of typical digital library services that use cloud computing facilities for core processing and storage. Horizontal scalability experiments were performed to benchmark the overall performance of the architecture with increasing load. The results of the experiments indicate that stable response times and some degree of variability are attainable due to multiple middleware servers when browsing and/or searching a collection of a fixed size. There is minimal variation in response times when varying collection sizes and equally after the caching phases. Most importantly, request sequencing proved that the quantity and age of requests have no impact on response times. The experimental results thus provide evidence to support the feasibility of building and deploying cloud-based Digital Libraries.},
booktitle = {Proceedings of the Southern African Institute for Computer Scientist and Information Technologists Annual Conference 2014 on SAICSIT 2014 Empowered by Technology},
pages = {157–165},
numpages = {9},
keywords = {Scalability, Amazon AWS, Cloud Computing},
location = {Centurion, South Africa},
series = {SAICSIT '14}
}

@inproceedings{10.5555/3042094.3042231,
author = {Valadares, Arthur and Lopes, Cristina V. and Achar, Rohan and Bowman, Mic},
title = {CADIS: Aspect-Oriented Architecture for Collaborative Modeling and Simulation},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {The development of large and complex simulated models often requires teams to collaborate. One approach is to break a large model into independently developed partial models that, when combined, capture the overall behavior. However, maintaining consistent world state across independently developed simulations is a challenge.In this paper, we introduce the Collaborative Aspect-Oriented Distributed Interactive Simulation (CADIS) architecture and development platform. CADIS embodies a new paradigm for integrating independently developed time-discrete partial models and simulations, focusing on transparently maintaining synchronized shared state. Data is pulled and instantiated in the beginning of each time step, and pushed at the end of each time step. An urban simulation is used to demonstrate CADIS capabilities and performance. We show how simple optimizations can bring the performance of the framework to acceptable levels, making CADIS a viable modeling and simulation methodology supporting separation of concerns.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {1024–1035},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

@inproceedings{10.1145/3277868.3277880,
author = {Klugman, Noah and Dutta, Prabal},
title = {Set and Forget Sensing with Applets on IFTTT},
year = {2018},
isbn = {9781450360494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277868.3277880},
doi = {10.1145/3277868.3277880},
abstract = {Rich data sets can be collected trivially by bootstrapping off mobile phones and cloud services. We describe an end-to-end system built with IFTTT that requires no code to collect arrival and departure times from a geographic area on the campus of the University of California, Berkeley. This system was configured and deployed in less than one half hour, cost nothing to deploy or run, and functioned without interruption for seven months, taking 463 measurements of a single participant. Along with providing the data set, which provides some insight into the working life of a graduate student, we describe each part of the system architecture and discuss how a model of sensing-as-an-applet enables data streams with de-facto standardized, high reliability, and close-to-no-barrier of entry.},
booktitle = {Proceedings of the First Workshop on Data Acquisition To Analysis},
pages = {23–24},
numpages = {2},
keywords = {trigger-action programming, sensing at scale, IFTTT},
location = {Shenzhen, China},
series = {DATA '18}
}

@inproceedings{10.1145/2785956.2787495,
author = {Hartert, Renaud and Vissicchio, Stefano and Schaus, Pierre and Bonaventure, Olivier and Filsfils, Clarence and Telkamp, Thomas and Francois, Pierre},
title = {A Declarative and Expressive Approach to Control Forwarding Paths in Carrier-Grade Networks},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787495},
doi = {10.1145/2785956.2787495},
abstract = {SDN simplifies network management by relying on declarativity (high-level interface) and expressiveness (network flexibility). We propose a solution to support those features while preserving high robustness and scalability as needed in carrier-grade networks. Our solution is based on (i) a two-layer architecture separating connectivity and optimization tasks; and (ii) a centralized optimizer called framework, which translates high-level goals expressed almost in natural language into compliant network configurations. Our evaluation on real and synthetic topologies shows that framework improves the state of the art by (i) achieving better trade-offs for classic goals covered by previous works, (ii) supporting a larger set of goals (refined traffic engineering and service chaining), and (iii) optimizing large ISP networks in few seconds. We also quantify the gains of our implementation, running Segment Routing on top of IS-IS, over possible alternatives (RSVP-TE and OpenFlow).},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {15–28},
numpages = {14},
keywords = {optimization, service chaining, mpls, isp, segment routing (sr), traffic engineering, software defined networking (sdn)},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@article{10.1145/2829988.2787495,
author = {Hartert, Renaud and Vissicchio, Stefano and Schaus, Pierre and Bonaventure, Olivier and Filsfils, Clarence and Telkamp, Thomas and Francois, Pierre},
title = {A Declarative and Expressive Approach to Control Forwarding Paths in Carrier-Grade Networks},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2829988.2787495},
doi = {10.1145/2829988.2787495},
abstract = {SDN simplifies network management by relying on declarativity (high-level interface) and expressiveness (network flexibility). We propose a solution to support those features while preserving high robustness and scalability as needed in carrier-grade networks. Our solution is based on (i) a two-layer architecture separating connectivity and optimization tasks; and (ii) a centralized optimizer called framework, which translates high-level goals expressed almost in natural language into compliant network configurations. Our evaluation on real and synthetic topologies shows that framework improves the state of the art by (i) achieving better trade-offs for classic goals covered by previous works, (ii) supporting a larger set of goals (refined traffic engineering and service chaining), and (iii) optimizing large ISP networks in few seconds. We also quantify the gains of our implementation, running Segment Routing on top of IS-IS, over possible alternatives (RSVP-TE and OpenFlow).},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {15–28},
numpages = {14},
keywords = {optimization, mpls, service chaining, isp, segment routing (sr), traffic engineering, software defined networking (sdn)}
}

@inproceedings{10.1145/3368235.3368838,
author = {Harsh, Piyush and Ribera Laszkowski, Juan Francisco and Edmonds, Andy and Quang Thanh, Tran and Pauls, Michael and Vlaskovski, Radoslav and Avila-Garc\'{\i}a, Orlando and Pages, Enric and Gort\'{a}zar Bellas, Francisco and Gallego Carrillo, Micael},
title = {Cloud Enablers For Testing Large-Scale Distributed Applications},
year = {2019},
isbn = {9781450370448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368235.3368838},
doi = {10.1145/3368235.3368838},
abstract = {Testing large-scale distributed systems (also known as testing in the large) is a challenge that spreads across different technical domains and areas of expertise. Current methods and tools provide some minimal guarantees in relation to the correctness of their functional properties and have serious limitations when evaluating their extra-functional properties in realistic conditions, such as scalability, availability and performance efficiency. Cloud Testing and more specifically "testing in the cloud'' has arisen to tackle those challenges. In this new paradigm, cloud-based environment and infrastructure are used to run realistic end-to-end and/or system-level tests, collect test data and analyse them. In this paper we present a set of cloud-native services to take from the tester the responsibility of managing the resources and complementary services required to simulate realistic operational conditions and production environments. Specifically, they provide cloud testing capabilities such as logs and measurements collection from both testing jobs and system under test; test data analytics and visualization; provisioning and operation of additional services and processes to replicate realistic production ecosystems; support to scalability and diversity of underlying testing infrastructure; and replication of the operational conditions of the software under test through its instrumentation. We present the architecture of the cloud testing solution and the detailed design of each of the services; we also evaluate their relative contribution to satisfy different needs in the context of test execution.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
pages = {35–42},
numpages = {8},
keywords = {cloud testing, testing, reliability, scalability, continuous testing, large-scale distributed systems, continuous integration},
location = {Auckland, New Zealand},
series = {UCC '19 Companion}
}

@inproceedings{10.1145/3229556.3229563,
author = {Kastanakis, Savvas and Sermpezis, Pavlos and Kotronis, Vasileios and Dimitropoulos, Xenofontas},
title = {CABaRet: Leveraging Recommendation Systems for Mobile Edge Caching},
year = {2018},
isbn = {9781450359061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229556.3229563},
doi = {10.1145/3229556.3229563},
abstract = {Joint caching and recommendation has been recently proposed for increasing the efficiency of mobile edge caching. While previous works assume collaboration between mobile network operators and content providers (who control the recommendation systems), this might be challenging in today's economic ecosystem, with existing protocols and architectures. In this paper, we propose an approach that enables cache-aware recommendations without requiring a network and content provider collaboration. We leverage information provided publicly by the recommendation system, and build a system that provides cache-friendly and high-quality recommendations. We apply our approach to the YouTube service, and conduct measurements on YouTube video recommendations and experiments with video requests, to evaluate the potential gains in the cache hit ratio. Finally, we analytically study the problem of caching optimization under our approach. Our results show that significant caching gains can be achieved in practice; 8 to 10 times increase in the cache hit ratio from cache-aware recommendations, and an extra 2 times increase from caching optimization.},
booktitle = {Proceedings of the 2018 Workshop on Mobile Edge Communications},
pages = {19–24},
numpages = {6},
keywords = {Mobile Edge Networks, Recommendation Systems, Joint Caching and Recommendation},
location = {Budapest, Hungary},
series = {MECOMM'18}
}

@inproceedings{10.1145/3314058.3318167,
author = {Aghaei, Ehsan and Al-shaer, Ehab},
title = {ThreatZoom: Neural Network for Automated Vulnerability Mitigation},
year = {2019},
isbn = {9781450371476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314058.3318167},
doi = {10.1145/3314058.3318167},
abstract = {Increasing the variety and quantity of cyber threats becoming the evident that traditional human-in-loop approaches are no longer sufficient to keep systems safe. To address this momentous moot point, forward-thinking pioneers propose new cyber security strategy using automation to build a more efficient and cheaper defense. Associating large number of unpatchable CVEs (vulnerability descriptions) generated everyday to appropriate CWE (weakness) and CAPEC (attack pattern) can be used to automatically infer the expected impact and corresponding mitigation course of actions for that new CVE. Routinely, adversary exploits a vulnerability to trigger a cyber attack where this vulnerability results from a product or system weakness. Hence, finding a common system weakness associated with a vulnerability within a particular product can help to identifying the software, system, or architecture flaw and the potential attack impacts. This identification leads to prevent, detect, and mitigate those flaws. On the other hand, after recognizing the cause and the effect of a vulnerability, discovering the procedural-oriented description of the attack to create behavioral observables for detection and mitigation is necessary that can be derived from CAPEC and ATTCK. Mapping the CWE to CAPEC and ATTCK which provides pre-TTP and post-TTP respectively where TTP stands for Tactics, Techniques, and Procedures. Having all CWE, CAPEC, and ATTCK in one hand enables us to find corresponding mitigation for each one. On the other hand, extracting threat actions provided by each of these concepts leads to find another type of mitigation coming from Critical Security Controls (CSC).In this proposal, the target is to do mapping all the way from CVE to CAPEC and ATTCk automatically using machine learning, deep learning, and natural language processing and find the appropriate mitigation for each one and then find a proper patch as course of action defense. So far, we have introduced a neural network model which successfully classifies CVE to CWE automatically and as working on a deep learning model to classify CWEs to CAPEC.},
booktitle = {Proceedings of the 6th Annual Symposium on Hot Topics in the Science of Security},
articleno = {24},
numpages = {3},
keywords = {CVE, CWE, cyber security, CAPEC},
location = {Nashville, Tennessee, USA},
series = {HotSoS '19}
}

@inproceedings{10.1145/2645884.2645890,
author = {Irish, Andrew T. and Iland, Daniel and Isaacs, Jason T. and Hespanha, Jo\~{a}o P. and Belding, Elizabeth M. and Madhow, Upamanyu},
title = {Using Crowdsourced Satellite SNR Measurements for 3D Mapping and Real-Time GNSS Positioning Improvement},
year = {2014},
isbn = {9781450330732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645884.2645890},
doi = {10.1145/2645884.2645890},
abstract = {Geopositioning using Global Navigation Satellite Systems (GNSS), such as the Global Positioning System (GPS), is inaccurate in urban environments due to frequent non-line-of-sight (NLOS) signal reception. This poses a major problem for mobile services that benefit from accurate urban localization, such as navigation, hyperlocal advertising, and geofencing applications. However, urban NLOS signal reception can be exploited in two ways. First, one can use satellite signal-to-noise ratio (SNR) measurements crowdsourced from mobile devices to create 3D environment maps. This is possible because, for example, the SNR of signals obstructed by buildings is lower on average than that of line-of-sight (LOS) signals. Second, in a sort of reverse process called Shadow Matching, SNR readings from a particular device at an instant in time can be compared to 3D maps to provide real-time localization improvement. In this paper we give a brief overview of how such a system works and describe a scalable, low-cost, software-only architecture that implements it.},
booktitle = {Proceedings of the 6th Annual Workshop on Wireless of the Students, by the Students, for the Students},
pages = {5–8},
numpages = {4},
keywords = {localization improvement, gps, gnss, crowdsourcing, 3d mapping, shadow matching},
location = {Maui, Hawaii, USA},
series = {S3 '14}
}

@inproceedings{10.1145/2968219.2968315,
author = {Alissandrakis, Aris and Nake, Isabella},
title = {A New Approach for Visualizing Quantified Self Data Using Avatars},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968315},
doi = {10.1145/2968219.2968315},
abstract = {In recent years, it is becoming more common for people to use applications or devices that keep track of their life and activities, such as physical fitness, places they visited, the music they listen to, or pictures they took. This generates data that are used by the service providers for a variety of (usually analytics) purposes, but commonly there are limitations on how the users themselves can also explore or interact with these data. Our position paper describes a new approach of visualizing such Quantified Self data, in a meaningful and enjoyable way that can give the users personal insights into their own data. The visualization of the information is proposed as an avatar that maps the different activities the user is engaged with, along with each such activity level, as graphical features. An initial prototype (both in terms of graphical design and software architecture) as well as possible future extensions are discussed.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {522–527},
numpages = {6},
keywords = {quantified self, avatars, data visualization},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3090354.3090463,
author = {Rafii, Fadoua and Hassani, Badr Dine Rossi and Kbir, M'hamed A\"{\i}t},
title = {New Approach for Microarray Data Decision Making with Respect to Multiple Sources},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090463},
doi = {10.1145/3090354.3090463},
abstract = {Microarray technology is an innovative technology, which has brought changes to the biological fields. It is considered as an interesting advent for worthwhile researches. It has permitted simultaneous measurements of the hundreds of activities of genes. However, most of users and specifically researchers and biologists find difficulties while extracting and interpreting this kind of data, also the results of Microarray experiments are stored in multiple and different databases. The present paper focuses on providing a global architecture for making decisions on Microarray data, by taking advantages from the semantic web technologies and the data mining techniques. The major goal consists on getting decisions about a given disease from many experiment data distributed on many sources over the net. The input dataset, real elements array form, is retrieved from the integrated experiments designed for cancer studies. This work is interested to two huge Microarray databases: GEO and ArrayExpress. The integration was based on semantic web technologies used to integrate data from several Web sites and Microarray data sources. This can be done by a user to combine several experiments that treat the same disease or phenomenon in order to have more significant results. Also a user can upload a specific dataset, via Web services provided by a laboratory, that can be combined with other data, containing the same genes and treating the same disease, and receive results of data mining techniques proposed by this laboratory. We suppose that each laboratory has its own Web services that can receive data which respects a predefined format.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {106},
numpages = {5},
keywords = {Microarray, Data mining, Semantic web},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/2744769.2744897,
author = {Wang, Ying and Han, Yinhe and Wang, Cheng and Li, Huawei and Li, Xiaowei},
title = {RADAR: A Case for Retention-Aware DRAM Assembly and Repair in Future FGR DRAM Memory},
year = {2015},
isbn = {9781450335201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2744769.2744897},
doi = {10.1145/2744769.2744897},
abstract = {Refresh operations consume substantial energy and bandwidth in high-density DRAM memory. To cope with this issue, Fine-Grained Refresh (FGR) is recently proposed to eliminate unnecessary refresh operations caused by minor weak cells. Even JEDEC's DDR4 DRAM specification announces the support of FGR to make DRAM refresh more scalable. Unfortunately, we observe that the effectiveness of FGR is greatly confined by the procedure of refresh-oblivious device integration because all memory devices within a module have to be controlled and refreshed in a lockstep way after the step of assembly. In this work, we firstly propose to intelligently integrate the "compatible" devices through a pre-assembly testing and retention-aware matching method. Second, we reuse the reconfiguration structure from yield-oriented remapping mechanism in memory chips and propose Microfix to create a balanced distribution of retention time in memory banks through fine-grained row-address tuning. With this optimization architecture, RADAR, we can eliminate the refresh overhead of produced memory modules by 28\% on average.},
booktitle = {Proceedings of the 52nd Annual Design Automation Conference},
articleno = {19},
numpages = {6},
location = {San Francisco, California},
series = {DAC '15}
}

@inproceedings{10.1145/3027063.3053237,
author = {Savic, Selena and B\"{u}hlmann, Vera},
title = {Digital Literacy in Architecture: How Space is Organized by Computation},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3053237},
doi = {10.1145/3027063.3053237},
abstract = {The integration of architecture and digital technologies happens on an instrumental level, where digital is associated with making the design process more efficient. Architects commonly report on interaction with computers describing the service software has provided. Computational procedures remain obscured by design outputs. In this project, we propose to critically study the relationship of architecture and technology from a perspective of interaction with digital tools. We propose the use of text-mining on a corpus of architectural discourse in social media. With concepts extracted from this initial step, we will conduct a series of experiments on collaborative qualification using a mobile application. We will show how the challenge of organizing a discourse on computational process in architectural design could involve computation in productive new ways. Finally, we will discuss how these insights could enrich the future development of computer-based tools for design.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2909–2914},
numpages = {6},
keywords = {collaborative qualification, digital literacy, computer-aided design, meaning-making, text-mining},
location = {<conf-loc>, <city>Denver</city>, <state>Colorado</state>, <country>USA</country>, </conf-loc>},
series = {CHI EA '17}
}

@inproceedings{10.1145/3366623.3368137,
author = {Barcelona-Pons, Daniel and Garc\'{\i}a-L\'{o}pez, Pedro and Ruiz, \'{A}lvaro and G\'{o}mez-G\'{o}mez, Amanda and Par\'{\i}s, Gerard and S\'{a}nchez-Artigas, Marc},
title = {FaaS Orchestration of Parallel Workloads},
year = {2019},
isbn = {9781450370387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366623.3368137},
doi = {10.1145/3366623.3368137},
abstract = {Function as a Service (FaaS) is based on a reactive programming model where functions are activated by triggers in response to cloud events (e.g., objects added to an object store). The inherent elasticity and the pay-per-use model of serverless functions make them very appropriate for embarrassingly parallel tasks like data preprocessing, or even the execution of MapReduce jobs in the cloud.But current Serverless orchestration systems are not designed for managing parallel fork-join workflows in a scalable and efficient way. We demonstrate in this paper that existing services like AWS Step Functions or Azure Durable Functions incur in considerable overheads, and only Composer at IBM Cloud provides suitable performance.Successively, we analyze the architecture of OpenWhisk as an open-source FaaS systems and its orchestration features (Composer). We outline its architecture problems and propose guidelines for orchestrating massively parallel workloads using serverless functions.},
booktitle = {Proceedings of the 5th International Workshop on Serverless Computing},
pages = {25–30},
numpages = {6},
keywords = {orchestration, FaaS, Serverless, event-based},
location = {Davis, CA, USA},
series = {WOSC '19}
}

@article{10.1145/3057857,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Visual Simple Transformations: Empowering End-Users to Wire Internet of Things Objects},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3057857},
doi = {10.1145/3057857},
abstract = {Empowering end-users to wire Internet of Things (IoT) objects (things and services) together would allow them to more easily conceive and realize interesting IoT solutions. A challenge lies in devising a simple end-user development approach to support the specification of transformations, which can bridge the mismatch in the data being exchanged among IoT objects. To tackle this challenge, we present Visual Simple Transformations (ViSiT) as an approach that allows end-users to use a jigsaw puzzle metaphor for specifying transformations that are automatically converted into underlying executable workflows. ViSiT is explained by presenting meta-models and an architecture for implementing a system of connected IoT objects. A tool is provided for supporting end-users in visually developing and testing transformations. Another tool is also provided for allowing software developers to modify, if they wish, a transformation's underlying implementation. This work was evaluated from a technical perspective by developing transformations and measuring ViSiT's efficiency and scalability and by constructing an example application to show ViSiT's practicality. A study was conducted to evaluate this work from an end-user perspective, and its results showed positive indications of perceived usability, learnability, and the ability to conceive real-life scenarios for ViSiT.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {apr},
articleno = {10},
numpages = {43},
keywords = {End-user development, internet of things, transformations}
}

@inproceedings{10.1145/3199902.3199904,
author = {\v{S}ljivo, Amina and Kerkhove, Dwight and Moerman, Ingrid and De Poorter, Eli and Hoebeke, Jeroen},
title = {Interactive Web Visualizer for IEEE 802.11ah Ns-3 Module},
year = {2018},
isbn = {9781450364133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199902.3199904},
doi = {10.1145/3199902.3199904},
abstract = {The main purpose of running ns-3 simulations is to generate relevant data sets for further study. There are two strategies to generate output from ns-3, either using generic predefined bulk output mechanisms or using the ns-3's Tracing system. Both require parsing the raw output data to extract and process the data of interest to obtain meaningful information. However, parsing such output is in most cases time consuming and prone to mistakes. Post-processing is even harder when a large number of simulations needs to be analyzed and even the tracing system cannot simplify this task. Moreover, results obtained this way are only available once the simulation is finished.Therefore, we developed a user-friendly interactive visualization and post-processing tool for IEEE 802.11ah called ahVisualizer. Beside the topology and MAC configuration, ahVisualizer also plots our traces for each node over time during the simulation, as well as averages and standard deviations for each traced parameter. It can compare all the measured values across different simulations. Users can easily download figures and data in various formats. Moreover, it includes a post-processing tool which plots desired series, with desired fixed parameters, from a large set of simulations. This paper presents the ahVisualizer, its services and its architecture and shows how this tool enables much faster and easier data analysis and monitoring of ns-3 simulations with 802.11ah.},
booktitle = {Proceedings of the 2018 Workshop on Ns-3},
pages = {23–29},
numpages = {7},
keywords = {distributed simulations, IEEE 802.11ah, visualization, post-processing, ns-3, Wi-Fi HaLow, analysis},
location = {Surathkal, India},
series = {WNS3 '18}
}

@inproceedings{10.1145/3195970.3196056,
author = {Yang, Haoyu and Li, Shuhe and Ma, Yuzhe and Yu, Bei and Young, Evangeline F. Y.},
title = {GAN-OPC: Mask Optimization with Lithography-Guided Generative Adversarial Nets},
year = {2018},
isbn = {9781450357005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195970.3196056},
doi = {10.1145/3195970.3196056},
abstract = {Mask optimization has been a critical problem in the VLSI design flow due to the mismatch between the lithography system and the continuously shrinking feature sizes. Optical proximity correction (OPC) is one of the prevailing resolution enhancement techniques (RETs) that can significantly improve mask printability. However, in advanced technology nodes, the mask optimization process consumes more and more computational resources. In this paper, we develop a generative adversarial network (GAN) model to achieve better mask optimization performance. We first develop an OPC-oriented GAN flow that can learn target-mask mapping from the improved architecture and objectives, which leads to satisfactory mask optimization results. To facilitate the training process and ensure better convergence, we also propose a pre-training procedure that jointly trains the neural network with inverse lithography technique (ILT). At convergence, the generative network is able to create quasi-optimal masks for given target circuit patterns and fewer normal OPC steps are required to generate high quality masks. Experimental results show that our flow can facilitate the mask optimization process as well as ensure a better printability.},
booktitle = {Proceedings of the 55th Annual Design Automation Conference},
articleno = {131},
numpages = {6},
location = {San Francisco, California},
series = {DAC '18}
}

@article{10.1109/TNET.2016.2518712,
author = {Afek, Yehuda and Bremler-Barr, Anat and Harchol, Yotam and Hay, David and Koral, Yaron},
title = {Making DPI Engines Resilient to Algorithmic Complexity Attacks},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2518712},
doi = {10.1109/TNET.2016.2518712},
abstract = {This paper starts by demonstrating the vulnerability of Deep Packet Inspection DPI mechanisms, which are at the core of security devices, to algorithmic complexity denial of service attacks, thus exposing a weakness in the first line of defense of enterprise networks and clouds. A system and a multi-core architecture to defend from these algorithmic complexity attacks is presented in the second part of the paper. The integration of this system with two different DPI engines is demonstrated and discussed. The vulnerability is exposed by showing how a simple low bandwidth cache-miss attack takes down the Aho-Corasick AC pattern matching algorithm that lies at the heart of most DPI engines. As a first step in the mitigation of the attack, we have developed a compressed variant of the AC algorithm that improves the worst case performance under an attack. Still, under normal traffic its running-time is worse than classical AC implementations. To overcome this problem, we introduce  ${rm MCA}^{2}$—Multi-Core Architecture to Mitigate Complexity Attacks, which dynamically combines the classical AC algorithm with our compressed implementation, to provide a robust solution to mitigate this cache-miss attack. We demonstrate the effectiveness of our architecture by examining cache-miss algorithmic complexity attacks against DPI engines and show a goodput boost of up to 73\%. Finally, we show that our architecture may be generalized to provide a principal solution to a wide variety of algorithmic complexity attacks.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {3262–3275},
numpages = {14}
}

@article{10.1145/3059149,
author = {Ying, Xuhang and Zhang, Jincheng and Yan, Lichao and Chen, Yu and Zhang, Guanglin and Chen, Minghua and Chandra, Ranveer},
title = {Exploring Indoor White Spaces in Metropolises},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3059149},
doi = {10.1145/3059149},
abstract = {It is a promising vision to exploit white spaces, that is, vacant VHF and UHF TV channels, to meet the rapidly growing demand for wireless data services in both outdoor and indoor scenarios. While most prior works have focused on outdoor white space, the indoor story is largely open for investigation. Motivated by this observation and discovering that 70\% of the spectrum demand comes from indoor environment, we carry out a comprehensive study to explore indoor white spaces. We first conduct a large-scale measurement study and compare outdoor and indoor TV spectrum occupancy at 30+ diverse locations in a typical metropolis—Hong Kong. Our results show that abundant white spaces are available in different areas in Hong Kong, which account for more than 50\% and 70\% of the entire TV spectrum in outdoor and indoor scenarios, respectively. Although there are substantially more white spaces indoors than outdoors, there have been very few solutions for identifying indoor white space. To fill in this gap, we develop the first data-driven, low-cost indoor white space identification system for White-space Indoor Spectrum EnhanceR (WISER), to allow secondary users to identify white spaces for communication without sensing the spectrum themselves. We design the architecture and algorithms to address the inherent challenges. We build a WISER prototype and carry out real-world experiments to evaluate its performance. Our results show that WISER can identify 30\%--40\% more indoor white spaces with negligible false alarms, as compared to alternative baseline approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {9},
numpages = {25},
keywords = {sensor placement, clustering algorithms, TV white spaces}
}

@article{10.1145/3448738,
author = {Shi, Cong and Liu, Jian and Liu, Hongbo and Chen, Yingying},
title = {WiFi-Enabled User Authentication through Deep Learning in Daily Activities},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3448738},
doi = {10.1145/3448738},
abstract = {User authentication is a critical process in both corporate and home environments due to the ever-growing security and privacy concerns. With the advancement of smart cities and home environments, the concept of user authentication is evolved with a broader implication by not only preventing unauthorized users from accessing confidential information but also providing the opportunities for customized services corresponding to a specific user. Traditional approaches of user authentication either require specialized device installation or inconvenient wearable sensor attachment. This article supports the extended concept of user authentication with a device-free approach by leveraging the prevalent WiFi signals made available by IoT devices, such as smart refrigerator, smart TV, and smart thermostat, and so on. The proposed system utilizes the WiFi signals to capture unique human physiological and behavioral characteristics inherited from their daily activities, including both walking and stationary ones. Particularly, we extract representative features from channel state information (CSI) measurements of WiFi signals, and develop a deep-learning-based user authentication scheme to accurately identify each individual user. To mitigate the signal distortion caused by surrounding people’s movements, our deep learning model exploits a CNN-based architecture that constructively combines features from multiple receiving antennas and derives more reliable feature abstractions. Furthermore, a transfer-learning-based mechanism is developed to reduce the training cost for new users and environments. Extensive experiments in various indoor environments are conducted to demonstrate the effectiveness of the proposed authentication system. In particular, our system can achieve over 94\% authentication accuracy with 11 subjects through different activities.},
journal = {ACM Trans. Internet Things},
month = {may},
articleno = {13},
numpages = {25},
keywords = {WiFi signals, User authentication, IoT}
}

@inproceedings{10.1145/3301551.3301582,
author = {Ruangvanich, Supparang and Nilsook, Prachyanun},
title = {Personality Learning Analytics System in Intelligent Virtual Learning Environment},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301582},
doi = {10.1145/3301551.3301582},
abstract = {In this paper, the researchers propose a conceptual for system architecture of learning analytics process in the intelligent learning environment. Within this concept, today's competitive business environment need for businesses in order to implement the monitor and analyze the user-generated data on their own and their competitors. The achievement of competitive advantage is often necessary to listen to and understand what customers are saying about competitors' products and services. Not only personality analytics but also the conceptual description can capture an intelligent learning environment, and it is the analytic tools that are used to improve learning and education. The researchers also discuss how learning analytics is developed in different fields. It closely tied to, a series of other fields of study including business intelligence, web analytics, academic analytics, educational data mining, and action analytics. The researchers believe that conceptual of personality analytics in the intelligent learning environment can play an essential role in managing and analyzing personality and contribute to the concept of personality analytics in the intelligent learning environment. The results of this research could be summarized as follows: learning analytics process should be used as measuring and collecting data about learners and learning with the aim of improving teaching and learning practice through analysis of the data. By achieving this process, it should collect data to report or analyze the happening about the learner. Then, instructors monitor learning what is happening now, while as learning analytics should get what is going to happen in the future for learners. Finally, instructors take action to feedback learners.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {245–250},
numpages = {6},
keywords = {Intelligent Environment, Personal Analytics, Virtual Learning Environment, Learning Analytics, System Architecture},
location = {Hong Kong, Hong Kong},
series = {ICIT '18}
}

@inproceedings{10.1145/2701973.2702098,
author = {de Silva, Lavindra and Yan, Rongjie and Ingrand, Felix and Alami, Rachid and Bensalem, Saddek},
title = {A Verifiable and Correct-by-Construction Controller for Robots in Human Environments},
year = {2015},
isbn = {9781450333184},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701973.2702098},
doi = {10.1145/2701973.2702098},
abstract = {With the increasing use of domestic and service robots alongside humans, it is now becoming crucial to be able to verify whether robot-software is safe, dependable, and correct. Indeed, in the near future it may well be necessary for robot-software developers to provide safety certifications guaranteeing, e.g. that a hospital nursebot will not move too fast while a person is leaning on it, that the arm of a service robot will not unexpectedly open its gripper while holding a glass, or that there will never be a software deadlock while a robot is navigating in an office. To this end, we have provided a framework and software engineering methodology for developing safe and dependable real-world robotic architectures, with a focus on the functional level--the lowest level of a typical layered robotic architecture--which has all the basic action and perception capabilities such as image processing, obstacle avoidance, and motion control. Unlike past work we address the formal verification of the functional level, which allows providing guarantees that it will not do steps leading to undesirable/disastrous outcomes.},
booktitle = {Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts},
pages = {281},
numpages = {1},
keywords = {verification, reliability, human factors},
location = {Portland, Oregon, USA},
series = {HRI'15 Extended Abstracts}
}

@inproceedings{10.1145/2723372.2735363,
author = {Chevalier, Jules and Subercaze, Julien and Gravier, Christophe and Laforest, Fr\'{e}d\'{e}rique},
title = {Slider: An Efficient Incremental Reasoner},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2735363},
doi = {10.1145/2723372.2735363},
abstract = {The Semantic Web has gained substantial momentum over the last decade. It contributes to the manifestation of knowledge from data, and leverages implicit knowledge through reasoning algorithms. The main drawbacks of current reasoning methods over ontologies are two-fold: first they struggle to provide scalability for large datasets, and second, the batch processing reasoners who provide the best scalability so far are unable to infer knowledge from evolving data. We contribute to solving these problems by introducing Slider, an efficient incremental reasoner. Slider goes a significant step beyond existing system, including i) performance, by more than a 70\% improvement in average compared to the fastest reasoner available to the best of our knowledge, and ii) inferences on streams of semantic data, by using intrinsic features that are themselves streams-oriented. Slider is fragment agnostic and conceived to handle expanding data with a growing background knowledge base. It natively supports pdf and RDFS, and its architecture allows to extend it to more complex fragments with a minimal effort. In this demo a web-based interface allows the users to visualize the internal behaviour of Slider during the inference, to better understand its design and principles.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1081–1086},
numpages = {6},
keywords = {web of data, streamed reasoning, incremental reasoning},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3167020.3167061,
author = {Pacheco, Fannia and Exposito, Ernesto and Gineste, Mathieu and Budoin, Cedric},
title = {An Autonomic Traffic Analysis Proposal Using Machine Learning Techniques},
year = {2017},
isbn = {9781450348959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167020.3167061},
doi = {10.1145/3167020.3167061},
abstract = {Network analysis has recently become in one of the most challenging tasks to handle due to the rapid growth of communication technologies. For network management, accurate identification and classification of network traffic is a key task. For example, identifying traffic from different applications is critical to manage bandwidth resources and to ensure Quality of Service objectives. Machine learning emerges as a suitable tool for traffic classification; however, it requires several steps that must be followed adequately in order to achieve the goals. In this paper, we proposed an architecture to perform traffic analysis based on Machine Learning techniques and autonomic computing. We analyze the procedures to perform Machine Learning over traffic network classification, and at the same time we give guidelines to introduce all these procedures into the architecture proposed. The main contribution of our proposal is the reconfiguration of the traffic classifier that will change according to the knowledge acquired from the traffic analysis process.},
booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
pages = {273–280},
numpages = {8},
keywords = {traffic analysis, quality of service, autonomic computing, Machine Learning},
location = {Bangkok, Thailand},
series = {MEDES '17}
}

@inproceedings{10.1145/3391614.3399389,
author = {Taguchi, Shuhei and Yamamura, Chigusa and Ohmata, Hisayuki and Sekine, Daisuke and Kajita, Kaisei and Fujii, Arisa},
title = {Sharing Same Elements in User Viewing History Data Securely Through Private Set Intersection Under User-Centric Data Control},
year = {2020},
isbn = {9781450379762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391614.3399389},
doi = {10.1145/3391614.3399389},
abstract = {Recently, various over-the-top (OTT) streaming services as well as traditional broadcasts distribute numerous content every day, allowing users to watch their favorite content at any time. While users can choose from a quantity of content, they often view the same content mainly because most OTT streaming services implement recommendation systems. However, it is often difficult for users to realize the same content they viewed and enjoy sharing their opinions or feelings because they do not know each other when and which content they viewed. For the purpose of encouraging such enjoyable experiences, we propose a system architecture that allows users to share the same content they viewed by using the user's viewing history data. Such viewing history data is currently collected and stored by hundreds of different services and companies. Therefore, our system architecture adopts a user-centric data control model that allows users to collect and store their data on their own online storages, and use it for their purposes. If users are asked to disclose all of the raw data of their viewing history to each other or to third party when sharing, most of them will feel anxiety because the data often contains sensitive personal information. Therefore, we introduce a method using private set intersection (PSI), a cryptographic technique that allows users to share the same elements in the users’ viewing history data without revealing anything to each other except the elements at the intersection. We also demonstrate the feasibility of the architecture through use cases.},
booktitle = {Proceedings of the 2020 ACM International Conference on Interactive Media Experiences},
pages = {138–142},
numpages = {5},
keywords = {Viewing history, Private set intersection, Personal data, Decentralization, User-centric, GDPR;},
location = {Cornella, Barcelona, Spain},
series = {IMX '20}
}

@inproceedings{10.1145/3426462.3426467,
author = {Ibrahim, Seif and Harrison, Cyrus and Larsen, Matthew},
title = {JIT’s Complicated: A Comprehensive System For Derived Field Generation},
year = {2020},
isbn = {9781450388122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426462.3426467},
doi = {10.1145/3426462.3426467},
abstract = {Derived field calculations are a vital part of the visualization and analysis workflow. These calculations allow simulation users to create important quantities of interest that are not generated by the simulation, and systems that calculate derived quantities must be flexible enough to accommodate a wide variety of user requests. In situ analysis imposes additional constraints on the system, and derived field calculations must be able to leverage the same resources as the simulation to minimize the runtime and memory usage. Just-in-time (JIT) compilation defers code creation until runtime, and a JIT based system is capable of fusing a complex expression into a single kernel invocation (i.e., kernel fusion). Without kernel fusion, the system would be forced to evaluate each piece of the expression (e.g., an operator or function call) as separate kernel invocations, which increases both runtime and memory pressure on the host simulation. In this paper, we present a production-oriented in situ derived field system that leverages JIT compilation to target heterogeneous HPC architectures. Additionally, we explore the runtime costs of using this system to calculate three expressions in three simulation codes.},
booktitle = {ISAV'20 In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization},
pages = {27–31},
numpages = {5},
location = {Atlanta, GA, USA},
series = {ISAV'20}
}

@inproceedings{10.1145/2976749.2978361,
author = {Holzinger, Philipp and Triller, Stefan and Bartel, Alexandre and Bodden, Eric},
title = {An In-Depth Study of More Than Ten Years of Java Exploitation},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978361},
doi = {10.1145/2976749.2978361},
abstract = {When created, the Java platform was among the first runtimes designed with security in mind. Yet, numerous Java versions were shown to contain far-reaching vulnerabilities, permitting denial-of-service attacks or even worse allowing intruders to bypass the runtime's sandbox mechanisms, opening the host system up to many kinds of further attacks.This paper presents a systematic in-depth study of 87 publicly available Java exploits found in the wild. By collecting, minimizing and categorizing those exploits, we identify their commonalities and root causes, with the goal of determining the weak spots in the Java security architecture and possible countermeasures.Our findings reveal that the exploits heavily rely on a set of nine weaknesses, including unauthorized use of restricted classes and confused deputies in combination with caller-sensitive methods. We further show that all attack vectors implemented by the exploits belong to one of three categories: single-step attacks, restricted-class attacks, and information hiding attacks.The analysis allows us to propose ideas for improving the security architecture to spawn further research in this area.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {779–790},
numpages = {12},
keywords = {java security, exploits, access control, security analysis},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/2590651.2590682,
author = {Urra, Enrique and Cabrera-Paniagua, Daniel and Cubillos, Claudio},
title = {Towards a Distributed Hyperheuristic Deploy Architecture},
year = {2014},
isbn = {9781450324359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590651.2590682},
doi = {10.1145/2590651.2590682},
abstract = {The hyperheuristic term is known in the optimization field as an automated methodology for selecting or generating heuristics to solve hard computational search problems. From the design perspective, it is based on decoupling the solving intelligence from the domain expertise, allowing to reuse the same solver for multiple, usually related problem domains. There are few works in which hyperheuristics have been designed and evaluated in distributed environments. In this paper, we propose a conceptual design of a distributed hyperheuristic architecture, from the problem domain deploying perspective, which allows to communicate different optimization environments (such as solver and domain) and to offering a "solving service". Different problems domains could be addressed using an encapsulated hyperheuristic solver, and through well defined interfaces, users can provide different heuristic components to perform the optimization process. The proposed architecture is only an initial step for which different modeling, design and implementation issues must be addressed. Such research should be focused on defining how conceptual design contributions must be leveraged to implement well defined interfaces, capable of connecting hyperheuristic solvers and problem domains within distributed environments.},
booktitle = {Proceedings of the 7th Euro American Conference on Telematics and Information Systems},
articleno = {31},
numpages = {4},
keywords = {hyperheuristics, optimization, distributed architecture},
location = {Valparaiso, Chile},
series = {EATIS '14}
}

@inproceedings{10.5555/3437539.3437714,
author = {Dev, Sundar and Lo, David and Cheng, Liqun and Ranganathan, Parthasarathy},
title = {Autonomous Warehouse-Scale Computers},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Modern Warehouse-Scale Computers (WSCs), composed of many generations of servers and a myriad of domain specific accelerators, are becoming increasingly heterogeneous. Meanwhile, WSC workloads are also becoming incredibly diverse with different communication patterns, latency requirements, and service level objectives (SLOs). Insufficient understanding of the interactions between workload characteristics and the underlying machine architecture leads to resource over-provisioning, thereby significantly impacting the utilization of WSCs.We present Autonomous Warehouse-Scale Computers, a new WSC design that leverages machine learning techniques and automation to improve job scheduling, resource management, and hardware-software co-optimization to address the increasing heterogeneity in WSC hardware and workloads. Our new design introduces two new layers in the WSC stack, namely: (a) a Software-Defined Server (SDS) Abstraction Layer which redefines the hardware-software boundary and provides greater control of the hardware to higher layers of the software stack through stable abstractions; and (b) a WSC Efficiency Layer which regularly monitors the resource usage of workloads on different hardware types, autonomously quantifies the performance sensitivity of workloads to key system configurations, and continuously improves scheduling decisions and hardware resource QoS policies to maximize cluster level performance. Our new WSC design has been successfully deployed across all WSCs at Google for several years now. The new WSC design improves throughput of workloads (by 7--10\%, on average), increases utilization of hardware resources (up to 2x), and reduces performance variance for critical workloads (up to 25\%).},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {175},
numpages = {6},
keywords = {machine learning, heterogeneity, WSC, automation},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.1109/TCBB.2017.2665542,
author = {Chai, Guoshi and Yu, Min and Jiang, Lixu and Duan, Yaocong and Huang, Jian},
title = {HMMCAS: A Web Tool for the Identification and Domain Annotations of CAS Proteins},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2017.2665542},
doi = {10.1109/TCBB.2017.2665542},
abstract = {The CRISPR-Cas clustered regularly interspaced short palindromic repeats-CRISPR-associated proteins adaptive immune systems are discovered in many bacteria and most archaea. These systems are encoded by cas CRISPR-associated operons that have an extremely diverse architecture. The most crucial step in the depiction of cas operons composition is the identification of cas genes or Cas proteins. With the continuous increase of the newly sequenced archaeal and bacterial genomes, the recognition of new Cas proteins is becoming possible, which not only provides candidates for novel genome editing tools but also helps to understand the prokaryotic immune system better. Here, we describe HMMCAS, a web service for the detection of CRISPR-associated structural and functional domains in protein sequences. HMMCAS uses hmmscan similarity search algorithm in HMMER3.1 to provide a fast, interactive service based on a comprehensive collection of hidden Markov models of Cas protein family. It can accurately identify the Cas proteins including those fusion proteins, for example the Cas1-Cas4 fusion protein in Candidatus Chloracidobacterium thermophilum B Cab. thermophilum B. HMMCAS can also find putative cas operon and determine which type it belongs to. HMMCAS is freely available at http://i.uestc.edu.cn/hmmcas.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {1313–1315},
numpages = {3}
}

@inproceedings{10.1145/3109761.3109783,
author = {Bacciu, Davide and Chessa, Stefano and Gallicchio, Claudio and Micheli, Alessio},
title = {On the Need of Machine Learning as a Service for the Internet of Things},
year = {2017},
isbn = {9781450352437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109761.3109783},
doi = {10.1145/3109761.3109783},
abstract = {In recent years we are witnessing a rapid increase in the diffusion of the Internet of Things (IoT) technology, with a large scale adoption of interconnected heterogeneous devices that are pervasively collecting information through the interaction with humans in their environment. The adoption of Machine Learning (ML) methodologies can play a fundamental role, allowing smarter IoT applications to continuously adapt to evolving environmental conditions and user's needs. In this context, the time is now ripe for a decisive step forward in the direction of a systematic integration of ML functionalities within the IoT platform.In this paper, we outline the principles that should guide the realization of a ML service for the IoT, proposing a conceptual architecture of such a learning service, integrated within the IoT reference model. Our proposal leverages on the experience of recent successful European initiatives that led to the realization of intelligent sensor networks built on the synergy between resource efficient ML models for temporal data processing and wireless sensor networks. The relevant impact of ML in applicative domains of interest for the IoT is also enucleated through a brief summary of recent results.},
booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
articleno = {22},
numpages = {8},
keywords = {machine learning service, adaptive IoT applications, intelligent sensor networks, distributed learning service, internet of things},
location = {Liverpool, United Kingdom},
series = {IML '17}
}

@inproceedings{10.1145/2611765.2611773,
author = {Dhawan, Udit and Vasilakis, Nikos and Rubin, Raphael and Chiricescu, Silviu and Smith, Jonathan M. and Knight, Thomas F. and Pierce, Benjamin C. and DeHon, Andr\'{e}},
title = {PUMP: A Programmable Unit for Metadata Processing},
year = {2014},
isbn = {9781450327770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611765.2611773},
doi = {10.1145/2611765.2611773},
abstract = {We introduce the Programmable Unit for Metadata Processing (PUMP), a novel software-hardware element that allows flexible computation with uninterpreted metadata alongside the main computation with modest impact on runtime performance (typically 10--40\% for single policies, compared to metadata-free computation on 28 SPEC CPU2006 C, C++, and Fortran programs). While a host of prior work has illustrated the value of ad hoc metadata processing for specific policies, we introduce an architectural model for extensible, programmable metadata processing that can handle arbitrary metadata and arbitrary sets of software-defined rules in the spirit of the time-honored 0-1-∞ rule. Our results show that we can match or exceed the performance of dedicated hardware solutions that use metadata to enforce a single policy, while adding the ability to enforce multiple policies simultaneously and achieving flexibility comparable to software solutions for metadata processing. We demonstrate the PUMP by using it to support four diverse safety and security policies---spatial and temporal memory safety, code and data taint tracking, control-flow integrity including return-oriented-programming protection, and instruction/data separation---and quantify the performance they achieve, both singly and in combination.},
booktitle = {Proceedings of the Third Workshop on Hardware and Architectural Support for Security and Privacy},
articleno = {8},
numpages = {8},
keywords = {memory safety, taint tracking, tagged architecture, metadata, control-flow integrity, security},
location = {Minneapolis, Minnesota, USA},
series = {HASP '14}
}

@inproceedings{10.1145/3394885.3431616,
author = {Chakaravarthy, Ravikumar V. and Kwon, Hyun and Jiang, Hua},
title = {Vision Control Unit in Fully Self Driving Vehicles Using Xilinx MPSoC and Opensource Stack},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431616},
doi = {10.1145/3394885.3431616},
abstract = {Fully self-driving (FSD) vehicles are becoming increasing popular over the last few years and companies are investing significantly into its research and development. In the recent years, FSD technology innovators like Tesla, Google etc. have been working on proprietary autonomous driving stacks and have been able to successfully bring the vehicle to the roads. On the other end, organizations like Autoware Foundation and Baidu are fueling the growth of self-driving mobility using open source stacks. These organizations firmly believe in enabling autonomous driving technology for everyone and support developing software stacks through the open source community that is SoC vendor agnostic. In this proposed solution we describe a vision control unit for a fully self-driving vehicle developed on Xilinx MPSoC platform using open source software components.The vision control unit of an FSD vehicle is responsible for camera video capture, image processing and rendering, AI algorithm processing, data and meta-data transfer to next stage of the FSD pipeline. In this proposed solution we have used many open source stacks and frameworks for video and AI processing. The processing of the video pipeline and algorithms take full advantage of the pipelining and parallelism using all the heterogenous cores of the Xilinx MPSoC. In addition, we have developed an extensible, scalable, adaptable and configurable AI backend framework, XTA, for acceleration purposes that is derived from a popular, open source AI backend framework, TVM-VTA. XTA uses all the MPSoC cores for its computation in a parallel and pipelined fashion. XTA also adapts to the compute and memory parameters of the system and can scale to achieve optimal performance for any given AI problem. The FSD system design is based on a distributed system architecture and uses open source components like Autoware for autonomous driving algorithms, ROS and Distributed Data Services as a messaging middleware between the functional nodes and a real-time kernel to coordinate the actions. The details of image capture, rendering and AI processing of the vision perception pipeline will be presented along with the performance measurements of the vision pipeline.In this proposed solution we will demonstrate some of the key use cases of vision perception unit like surround vision and object detection. In addition, we will also show the capability of Xilinx MPSoC technology to handle multiple channels of real time camera and the integration with the Lidar/Radar point cloud data to feed into the decision-making unit of the overall system. The system is also designed with the capability to update the vision control unit through Over the Air Update (OTA). It is also envisioned that the core AI engine will require regular updates with the latest training values; hence a built-in platform level mechanism supporting such capability is essential for real world deployment.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {311–317},
numpages = {7},
keywords = {MPSoC, Vision Pipeline, AI, ROS, FSD, Autoware, XTA, Heterogenous Processing},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@article{10.1145/2894750,
author = {To, Quoc-Cuong and Nguyen, Benjamin and Pucheral, Philippe},
title = {Private and Scalable Execution of SQL Aggregates on a Secure Decentralized Architecture},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/2894750},
doi = {10.1145/2894750},
abstract = {Current applications, from complex sensor systems (e.g., quantified self) to online e-markets, acquire vast quantities of personal information that usually end up on central servers where they are exposed to prying eyes. Conversely, decentralized architectures that help individuals keep full control of their data complexify global treatments and queries, impeding the development of innovative services. This article aims precisely at reconciling individual's privacy on one side and global benefits for the community and business perspectives on the other. It promotes the idea of pushing the security to secure hardware devices controlling the data at the place of their acquisition. Thanks to these tangible physical elements of trust, secure distributed querying protocols can reestablish the capacity to perform global computations, such as Structured Query Language (SQL) aggregates, without revealing any sensitive information to central servers. This article studies how to secure the execution of such queries in the presence of honest-but-curious and malicious attackers. It also discusses how the resulting querying protocols can be integrated in a concrete decentralized architecture. Cost models and experiments on SQL/Asymmetric Architecture (AA), our distributed prototype running on real tamper-resistant hardware, demonstrate that this approach can scale to nationwide applications.},
journal = {ACM Trans. Database Syst.},
month = {aug},
articleno = {16},
numpages = {43},
keywords = {parallel computing, decentralized architecture, Trusted hardware}
}

@article{10.1109/TNET.2016.2627005,
author = {Habibi Gharakheili, Hassan and Sivaraman, Vijay and Moors, Tim and Vishwanath, Arun and Matthews, John and Russell, Craig},
title = {Enabling Fast and Slow Lanes for Content Providers Using Software Defined Networking},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2627005},
doi = {10.1109/TNET.2016.2627005},
abstract = {Residential broadband consumption is growing rapidly, increasing the gap between Internet service provider ISP costs and revenues. Meanwhile, proliferation of Internet-enabled devices is congesting access networks, degrading end-user experience, and affecting content provider monetization. In this paper, we propose a new model whereby the content provider explicitly signals fast- and slow-lane requirements to the ISP on a per-flow basis, using open APIs supported through software defined networking SDN. Our first contribution is to develop an architecture that supports this model, presenting arguments on why this benefits consumers better user experience, ISPs two-sided revenue, and content providers fine-grained control over peering arrangement. Our second contribution is to evaluate our proposal using a real trace of over 10 million flows to show that video flow quality degradation can be nearly eliminated by the use of dynamic fast-lanes, and web-page load times can be hugely improved by the use of slow-lanes for bulk transfers. Our third contribution is to develop a fully functional prototype of our system using open-source SDN components Openflow switches and POX controller modules and instrumented video/file-transfer servers to demonstrate the feasibility and performance benefits of our approach. Our proposal is a first step towards the long-term goal of realizing open and agile access network service quality management that is acceptable to users, ISPs, and content providers alike.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1373–1385},
numpages = {13}
}

@inproceedings{10.1145/3127041.3131363,
author = {Grosch, Franz-Josef},
title = {Elevate Embedded Real-Time Programming with a Synchronous Language},
year = {2017},
isbn = {9781450350938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127041.3131363},
doi = {10.1145/3127041.3131363},
abstract = {Product development at companies such as Bosch requires systems engineering for digital hardware and mechatronic components as well as software engineering for resource-constrained real-time applications cooperating with distributed server applications. While many of the involved engineering disciplines greatly benefit from model-based approaches and from advances in software infrastructures, deeply embedded software still is written in C since the seventies and runs on platforms designed in the nineties (e.g. OSEK). Simulation tools like Simulink or Modelica are used to test discrete code against continuous plant models or to generate code for certain aspects, but they do not really provide modern implementation technologies to address software architecture and qualities or to make embedded programming "attractive" for software professionals.We regard synchronous languages as suitable to solve many of the issues in the integration (causality) and synchronisation (clocks) of time-triggered and event-triggered embedded functions that exhibit their behaviour over time steps and are coordinated according to their mode-switching in a structured synchronous control flow. Searching for an imperative synchronous language (with deterministic concurrent composition, and synchronous control flow), equipped with features for encapsulation and composition (objects, packages, separate compilation) and supporting programming parallel tasks deployed to separate cores (clock refinement and deterministic inter-task communication), we ended up in designing our own language, suitable for resource-constrained, real-time applications running on multi-core controllers.We will explain the main requirements and features of this language, how they integrate with the principles of a synchronous language, how they can be applied to typical everyday problems in embedded development, and how such locally synchronous services may integrate in a globally asynchronous service architecture.},
booktitle = {Proceedings of the 15th ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {156},
numpages = {1},
location = {Vienna, Austria},
series = {MEMOCODE '17}
}

@inproceedings{10.1145/3456415.3456416,
author = {Chen, Ken and He, Jiabei},
title = {Big-Data-Based Research on the Architecture Design of University Hydropower Intelligent Decision Service Platform},
year = {2021},
isbn = {9781450389174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456415.3456416},
doi = {10.1145/3456415.3456416},
abstract = {With the continuous development and wide application of big data and artificial intelligence technology, how to efficiently use and mine the whole process data of university hydropower models, perception, business and flows, and realize the transformation of informationization of hydropower management to intelligentialize and wisdom, it has become one of the main tasks in the construction of universitiy informatization under the strategy of advocating energy conservation, lowcarbon sustainable development. Combining with the actual demand of university hydropower management, managing and serving the whole process of hydropower data collection, storage, analysis, monitoring and decision-making assistance, this paper proposes the architecture of an intelligent decision-making service platform for university hydropower on big data, and sorts out the core and key technologies in the platform development process and the current mainstream development frameworks and tools to provide technical references for the realization of intelligent hydropower management and application services in universities, and promote the overall planning and step-by-step implementation of smart campuses.},
booktitle = {Proceedings of the 2021 9th International Conference on Communications and Broadband Networking},
pages = {1–5},
numpages = {5},
keywords = {hydropower information, big data, intelligentization},
location = {Shanghai, China},
series = {ICCBN '21}
}

@article{10.1145/2724942.2724952,
author = {Souza, Jeferson L. R. and Rufino, Jos\'{e}},
title = {The Wi-STARK Architecture for Resilient Real-Time Wireless Communications},
year = {2015},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
url = {https://doi.org/10.1145/2724942.2724952},
doi = {10.1145/2724942.2724952},
abstract = {Networking communications play an important role to secure a dependable and timely operation of distributed and real-time embedded system applications; however, an effective real-time support is not yet properly addressed in the wireless realm. This paper presents Wi-STARK, a novel architecture for resilient and real-time wireless communications within an one-hop communication domain. Low level reliable (frame) communications, node failure detection, membership management, and networking partition control are provided; since these low level services extend and build upon the exposed interface offered by networking technologies, Wi-STARK is in strict compliance with wireless communication standards, such as IEEE 802.15.4 and IEEE 802.11p. The Wi-STARK service interface is then offered as operating system primitives, helpful for building distributed control applications. The one-hop dependability and timeliness guarantees offered by Wi-STARK are a fundamental step towards an effective design of real-time wireless networks with multiple hops, including end-to-end schedulability analysis of networking operations.},
journal = {SIGBED Rev.},
month = {jan},
pages = {61–66},
numpages = {6},
keywords = {timeliness, Wi-STARK, fault tolerance, resilience, wireless communications, dependability, real-time}
}

@inproceedings{10.5555/3374138.3374174,
author = {Kaya, M. Cagri and Karamanlioglu, Alper and \c{C}etinta\c{s}, undefined. \c{C}a\u{g}lar and \c{C}ilden, Erkin and Canberi, Haluk and O\u{g}uzt\"{u}z\"{u}n, Halit},
title = {A Configurable Gateway for DDS-HLA Interoperability},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Interoperability is a challenge for constructing Live-Virtual-Constructive (LVC) systems. This study is a step toward LVC interoperability adhering to a gateway-based approach with a particular focus on two standard middleware, namely, Data Distribution Service for Real-Time Systems (DDS) and High-Level Architecture (HLA) for distributed simulation. A gateway is designed and implemented to achieve DDS-HLA interoperability. This gateway has the ability to realize two-way data transfer between DDS and HLA systems. The gateway design is based on the idea of a configurable connector that is equipped with variability capabilities. It can perform data-type conversions between these two systems according to the mappings specified by the user. In a case study, the gateway is integrated into a distributed tactical environment simulation system.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {36},
numpages = {11},
keywords = {high-level architecture, data distribution service, gateway, interoperability},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@inproceedings{10.1109/ICCPS.2014.6843740,
author = {Zhang, Jiaxing and Qiu, Hanjiao and Shamsabadi, Salar Shahini and Birken, Ralf and Schirner, Gunar},
title = {WiP Abstract: System-Level Integration of Mobile Multi-Modal Multi-Sensor Systems},
year = {2014},
isbn = {9781479949304},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCPS.2014.6843740},
doi = {10.1109/ICCPS.2014.6843740},
abstract = {Heterogeneous roaming sensor systems have gained significant importance in many domains of civil infrastructure performance inspection as they accelerate data collection and analysis. However, designing such systems is challenging due to the immense complexity in the heterogeneity and processing demands of the involved sensors. Unifying frameworks are needed to simplify development, deployment and operation of roaming sensors and computing units.To address the sensing needs, we propose SIROM3, a Scalable Intelligent Roaming Multi-Modal Multi-Sensor framework. SIROM3 incorporates a CPS approach for infrastructure performance monitoring to address the following challenges: 1. Scalability and expandability. It offers a scalable and expandable solution enabling diversity in sensing and the growth in processing platforms from sensors to control centers. 2. Fusion foundations. SIROM3 enables fusion of data collected by logically and geo-spatially distributed sensors. 3. Big data handling. Automatic collection, categorization, storage and manipulation of heterogeneous large volume of data streams. 4. Automation. SIROM3 minimizes human interaction through full automation from data acquisition to visualization of the fused results.Illustrated in Fig. 1, SIROM3 realizes scalability and expandability in a system-level design approach encapsulating common functionality across hierarchical components in a Run-Time Environment (RTE). The RTE deploys a layered design paradigm defining services in both software and hardware architectures. Equipped with multiple RTE-enabled Multi-Sensor Aggregators (MSA), an array of Roaming Sensor Systems (RSS) operate as mobile agents attached to vehicles to provide distributed computing services regulated by Fleet Control and Management (FCM) center via communication network. A series of foundational services including the Precise Timing Protocol (PTP), GPS timing systems, Distance Measurement Instruments (DMI) through middleware services (CORBA) embedded in the RTE build the fusion foundations for data correlation and analysis. A Heterogeneous Stream File-system Overlay (HSFO) alleviates the big data challenge. It facilitates storing, processing, categorizing and fusing large heterogeneous data stream collected by versatile sensors. A GIS visualization module is integrated for visual analysis and monitoring.SIROM3 enables coordination and collaboration across sensors, MSAs and RSSes, which produce high volume of heterogeneous data stored in HSFO. To fuse the data efficiently, SIROM3 contains an expandable plugin system (part of RTE) for rapid algorithm prototyping using data streams in the architectural hierarchy (i.e. from MSAs to FCM) via the HSFO API. This makes an ideal test-bed to develop new algorithms and methodologies expanding CPS principles to civil infrastructure performance monitoring. In result, SIROM3 simplifies the development, construction and operation of roaming multi-modal multi-sensor systems.We demonstrate the efficiency of SIROM3 by automating the assessment of road surface conditions at the city scale. We realized an RSS with 6 MSAs and 30 heterogeneous sensors, including radars, microphones, GPS and cameras, all deployed onto a van sponsored by the VOTERS (Versatile Onboard Traffic Embedded Roaming Sensors) project. Over 20 terabytes of data have been collected, aggregated, fused, analyzed and geo-spatially visualized using SIROM3 for studying the pavement conditions of the city of Brockton, MA covering 300 miles. The expandability of SIROM3 is shown by adding a millimeter-wave radar needing less than 50 lines of C++ code for system integration. SIROM3 offers a unified solution for comprehensive roadway assessment and evaluation. The integrated management of big data (from collection to automated processing) is an ideal research platform for automated assessment of civil infrastructure performance.},
booktitle = {ICCPS '14: ACM/IEEE 5th International Conference on Cyber-Physical Systems (with CPS Week 2014)},
pages = {227},
numpages = {1},
location = {Berlin, Germany},
series = {ICCPS '14}
}

@inproceedings{10.1145/3447545.3451195,
author = {Calzarossa, Maria Carla and Massari, Luisa and Tessera, Daniele},
title = {Performance Monitoring Guidelines},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451195},
doi = {10.1145/3447545.3451195},
abstract = {Monitoring, that is, the process of collecting measurements on infrastructures and services, is an important subject of performance engineering. Although monitoring is not a new education topic, nowadays its relevance is rapidly increasing and its application is particularly demanding due to the complex distributed architectures of new and emerging technologies. As a consequence, monitoring has become a "must have" skill for students majoring in computer science and in computing-related fields. In this paper, we present a set of guidelines and recommendations to plan, design and setup sound monitoring projects. Moreover, we investigate and discuss the main challenges to be faced to build confidence in the entire monitoring process and ensure measurement quality. Finally, we describe practical applications of these concepts in teaching activities.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {109–114},
numpages = {6},
keywords = {measurement platforms, performance engineering, performance monitoring, measurement quality, passive measurements, active measurements},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1145/3390605,
author = {Ismail, Leila and Materwala, Huned},
title = {Computing Server Power Modeling in a Data Center: Survey, Taxonomy, and Performance Evaluation},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3390605},
doi = {10.1145/3390605},
abstract = {Data centers are large-scale, energy-hungry infrastructure serving the increasing computational demands as the world is becoming more connected in smart cities. The emergence of advanced technologies such as cloud-based services, internet of things (IoT), and big data analytics has augmented the growth of global data centers, leading to high energy consumption. This upsurge in energy consumption of the data centers not only incurs the issue of surging high cost (operational and maintenance) but also has an adverse effect on the environment. Dynamic power management in a data center environment requires the cognizance of the correlation between the system and hardware-level performance counters and the power consumption. Power consumption modeling exhibits this correlation and is crucial in designing energy-efficient optimization strategies based on resource utilization. Several works in power modeling are proposed and used in the literature. However, these power models have been evaluated using different benchmarking applications, power-measurement techniques, and error-calculation formulas on different machines. In this work, we present a taxonomy and evaluation of 24 software-based power models using a unified environment, benchmarking applications, power-measurement techniques, and error formulas, with the aim of achieving an objective comparison. We use different server architectures to assess the impact of heterogeneity on the models’ comparison. The performance analysis of these models is elaborated in the article.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {58},
numpages = {34},
keywords = {machine learning, Data center, green computing, server power consumption modeling, resource utilization, energy-efficiency}
}

@inproceedings{10.1145/3404835.3462878,
author = {Li, Houyi and Chen, Zhihong and Li, Chenliang and Xiao, Rong and Deng, Hongbo and Zhang, Peng and Liu, Yongchao and Tang, Haihong},
title = {Path-Based Deep Network for Candidate Item Matching in Recommenders},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462878},
doi = {10.1145/3404835.3462878},
abstract = {The large-scale recommender system mainly consists of two stages: matching and ranking. The matching stage (also known as the retrieval step) identifies a small fraction of relevant items from billion-scale item corpus in low latency and computational cost. Item-to-item collaborative filtering (item-based CF) and embedding-based retrieval (EBR) have been long used in the industrial matching stage owing to its efficiency. However, item-based CF is hard to meet personalization, while EBR has difficulty in satisfying diversity. In this paper, we propose a novel matching architecture, Path-based Deep Network (named PDN), through incorporating both personalization and diversity to enhance matching performance. Specifically, PDN is comprised of two modules: Trigger Net and Similarity Net. PDN utilizes Trigger Net to capture the user's interest in each of his/her interacted item. Similarity Net is devised to evaluate the similarity between each interacted item and the target item based on these items' profile and CF information. The final relevance between the user and the target item is calculated by explicitly considering user's diverse interests, ie aggregating the relevance weights of the related two-hop paths (one hop of a path corresponds to user-item interaction and the other to item-item relevance). Furthermore, we describe the architecture design of the proposed PDN in a leading real-world E-Commerce service (Mobile Taobao App). Based on offline evaluations and online A/B test, we show that PDN outperforms the existing solutions for the same task. The online results also demonstrate that PDN can retrieve more personalized and more diverse items to significantly improve user engagement. Currently, PDN system has been successfully deployed at Mobile Taobao App and handling major online traffic.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1493–1502},
numpages = {10},
keywords = {recommendation systems, deep learning},
location = {<conf-loc>, <city>Virtual Event</city>, <country>Canada</country>, </conf-loc>},
series = {SIGIR '21}
}

@inproceedings{10.1145/3267955.3267972,
author = {Sardara, Mauro and Muscariello, Luca and Compagno, Alberto},
title = {A Transport Layer and Socket API for (h)ICN: Design, Implementation and Performance Analysis},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3267972},
doi = {10.1145/3267955.3267972},
abstract = {In this paper we present the design of a transport layer and socket API that can be used in several ICN architectures such as NDN, CCN and hICN. The current design makes it possible to expose an API that is simple to insert in current applications and easy to use to develop novel ones. The proliferation of connected applications for very different use cases and services with wide spectrum of requirements suggests that several transport services will coexist in the Internet. This is just about to happen with QUIC, MPTCP, LEDBAT as the most notable ones but is expected to grow and diversify with the advent of applications for 5G, IoT, MEC with heterogeneous connectivity. The advantages of ICN have to be measurable from the application, end-services and in the network, with relevant key performance indicators. We have implemented an high speed transport stack with most of the designed features that we present in this paper with extensive experiments and benchmarks to show the scalability of the current systems in different use cases.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {137–147},
numpages = {11},
keywords = {socket API, transport services, ICN},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@inproceedings{10.1145/3026937.3026938,
author = {Alonso, Pedro and Catalan, Sandra and Herrero, Jos\'{e} R. and Quintana-Ort\'{\i}, Enrique S. and Rodr\'{\i}guez-S\'{a}nchez, Rafael},
title = {Reduction to Tridiagonal Form for Symmetric Eigenproblems on Asymmetric Multicore Processors},
year = {2017},
isbn = {9781450348836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3026937.3026938},
doi = {10.1145/3026937.3026938},
abstract = {Asymmetric multicore processors (AMPs), as those present in ARM big.LITTLE technology, have been proposed as a means to address the end of Dennard power scaling law. The idea of these architectures is to activate only the type (and number) of cores that satisfy the quality of service requested by the application(s) in execution while delivering high energy efficiency.For dense linear algebra problems though, performance is of paramount importance, asking for an efficient use of all computational resources in the AMP. In response to this, we investigate how to exploit the asymmetric cores of an ARMv7 big.LITTLE AMP in order to attain high performance for the reduction to tridiagonal form, an essential step towards the solution of dense symmetric eigenvalue problems. The routine for this purpose in LAPACK is especially challenging, since half of its floating-point arithmetic operations (flops) are cast in terms of compute-bound kernels while the remaining half correspond to memory-bound kernels. To deal with this scenario: 1) we leverage a tuned implementation of the compute-bound kernels for AMPs; 2) we develop and parallelize new architecture-aware micro-kernels for the memory-bound kernels; 3) and we carefully adjust the type and number of cores to use at each step of the reduction procedure.},
booktitle = {Proceedings of the 8th International Workshop on Programming Models and Applications for Multicores and Manycores},
pages = {39–47},
numpages = {9},
keywords = {symmetric eigenvalue problem, workload balancing, Condensed forms, asymmetric multicore processors, basic linear algebra subprograms (BLAS), multi-threading},
location = {Austin, TX, USA},
series = {PMAM'17}
}

@article{10.1145/3191737,
author = {Classen, Jiska and Wegemer, Daniel and Patras, Paul and Spink, Tom and Hollick, Matthias},
title = {Anatomy of a Vulnerable Fitness Tracking System: Dissecting the Fitbit Cloud, App, and Firmware},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191737},
doi = {10.1145/3191737},
abstract = {Fitbit fitness trackers record sensitive personal information, including daily step counts, heart rate profiles, and locations visited. By design, these devices gather and upload activity data to a cloud service, which provides aggregate statistics to mobile app users. The same principles govern numerous other Internet-of-Things (IoT) services that target different applications. As a market leader, Fitbit has developed perhaps the most secure wearables architecture that guards communication with end-to-end encryption. In this article, we analyze the complete Fitbit ecosystem and, despite the brand's continuous efforts to harden its products, we demonstrate a series of vulnerabilities with potentially severe implications to user privacy and device security. We employ a range of techniques, such as protocol analysis, software decompiling, and both static and dynamic embedded code analysis, to reverse engineer previously undocumented communication semantics, the official smartphone app, and the tracker firmware. Through this interplay and in-depth analysis, we reveal how attackers can exploit the Fitbit protocol to extract private information from victims without leaving a trace, and wirelessly flash malware without user consent. We demonstrate that users can tamper with both the app and firmware to selfishly manipulate records or circumvent Fitbit's walled garden business model, making the case for an independent, user-controlled, and more secure ecosystem. Finally, based on the insights gained, we make specific design recommendations that can not only mitigate the identified vulnerabilities, but are also broadly applicable to securing future wearable system architectures.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {5},
numpages = {24},
keywords = {Wearables, firmware reverse engineering, Nexmon, health}
}

@inproceedings{10.1145/3094405.3094411,
author = {Avino, G. and Malinverno, M. and Malandrino, F. and Casetti, C. and Chiasserini, C. F.},
title = {Characterizing Docker Overhead in Mobile Edge Computing Scenarios},
year = {2017},
isbn = {9781450350587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094405.3094411},
doi = {10.1145/3094405.3094411},
abstract = {Mobile Edge Computing (MEC) is an emerging network paradigm that provides cloud and IT services at the point of access of the network. Such proximity to the end user translates into ultra-low latency and high bandwidth, while, at the same time, it alleviates traffic congestion in the network core. Due to the need to run servers on edge nodes (e.g., an LTE-A macro eNodeB), a key element of MEC architectures is to ensure server portability and low overhead. A possible tool that can be used for this purpose is Docker, a framework that allows easy, fast deployment of Linux containers. This paper addresses the suitability of Docker in MEC scenarios by quantifying the CPU consumed by Docker when running two different containerized services: multiplayer gaming and video streaming. Our tests, run with varying numbers of clients and servers, yield different results for the two case studies: for the gaming service, the overhead logged by Docker increases only with the number of servers; conversely, for the video streaming case, the overhead is not affected by the number of either clients or servers.},
booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
pages = {30–35},
numpages = {6},
keywords = {Mobile Edge Computing, 5G networks, Containers, Docker},
location = {Los Angeles, CA, USA},
series = {HotConNet '17}
}

@inproceedings{10.5555/3192424.3192553,
author = {Giovanetti, Romain and Lancieri, Luigi},
title = {Model of Computer Architecture for Online Social Networks Flexible Data Analysis: The Case of Twitter Data},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {Since several years, there is an increasing interest for new services based on the analysis of data coming from online social networks. Such services can, for example, provide the e-reputation of a product or a company, detect new trends in a commercial, social or political context, etc. The huge quantity of data is an opportunity in term of representativeness but is also difficult to manage. Within Twitter, for example, it appears that the huge stream of data is, most of the time, incompatible with a flexible analysis unless to have high computer resources. The only practical solution is often to observe in a static way a limited portion of a phenomenon in a limited time slot. This paper is devoted to the study of necessary conditions to provide an equilibrium between the computer architecture complexity and the analysis flexibility.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {677–684},
numpages = {8},
keywords = {computer architecture, platform, distributed database, flexible data analysis, online social network, Twitter},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1145/3387514.3405881,
author = {Schomp, Kyle and Bhardwaj, Onkar and Kurdoglu, Eymen and Muhaimen, Mashooq and Sitaraman, Ramesh K.},
title = {Akamai DNS: Providing Authoritative Answers to the World's Queries},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405881},
doi = {10.1145/3387514.3405881},
abstract = {We present Akamai DNS, one of the largest authoritative DNS infrastructures in the world, that supports the Akamai content delivery network (CDN) as well as authoritative DNS hosting and DNS-based load balancing services for many enterprises. As the starting point for a significant fraction of the world's Internet interactions, Akamai DNS serves millions of queries each second and must be resilient to avoid disrupting myriad online services, scalable to meet the ever increasing volume of DNS queries, performant to prevent user-perceivable performance degradation, and reconfigurable to react quickly to shifts in network conditions and attacks. We outline the design principles and architecture used to achieve Akamai DNS's goals, relating the design choices to the system workload and quantifying the effectiveness of those designs. Further, we convey insights from operating the production system that are of value to the broader research community.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {465–478},
numpages = {14},
keywords = {DNS, Distributed Systems},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3313294.3313384,
author = {Di Stefano, Alessandro and Scat\`{a}, Marialisa and La Corte, Aurelio and Das, Sajal K. and Li\`{o}, Pietro},
title = {Improving QoE in Multi-Layer Social Sensing: A Cognitive Architecture and Game Theoretic Model},
year = {2019},
isbn = {9781450367066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313294.3313384},
doi = {10.1145/3313294.3313384},
abstract = {This paper proposes a novel cognitive architecture and game-theoretic model for resource sharing among netizens, thus improving their quality of experience (QoE) in multi-layer social sensing environments. The underlying approach is to quantify micro-rewards and inequalities derived from social multi-layer interactions. Specifically, we model our society as a social multi-layer network of individuals or groups of individuals (nodes), where the layers represent multiple channels of interactions (on various services). The weighted edges correspond to the multiple social relationships between nodes participating in different services, reflecting the importance assigned to each of these edges and are defined based on the concepts of awareness and homophily. Heterogeneity, both interactions-wise on the multiple layers and related to homophily between individuals, on each node and layer of a weighted multiplex network produces a complex multi-scale interplay between nodes in the multi-layer structure. Applying game theory, we quantify the impact of heterogeneity on the evolutionary dynamics of social sensing through a data driven approach based on the propagation of individual-level micro-affirmations and micro-inequalities. The micro-packets of energy continuously exchanged between nodes may impact positively or negatively on their social behaviors, producing peaks of extreme dissatisfaction and in some cases a form of distress. Quantifying the evolutionary dynamics of human behaviors enables the detection of such peaks in the population and enable us design a targeted control mechanism, where social rewards and self-healing help improve the QoE of the netizens.},
booktitle = {Proceedings of the Fourth International Workshop on Social Sensing},
pages = {18–23},
numpages = {6},
keywords = {game theory, social sensing, Cognitive architecture, IoP, QoE, multi-layer networks},
location = {Montreal, QC, Canada},
series = {SocialSense'19}
}

@article{10.14778/3476249.3476287,
author = {Chiosa, Monica and Preu\ss{}er, Thomas B. and Alonso, Gustavo},
title = {SKT: A One-Pass Multi-Sketch Data Analytics Accelerator},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476287},
doi = {10.14778/3476249.3476287},
abstract = {Data analysts often need to characterize a data stream as a first step to its further processing. Some of the initial insights to be gained include, e.g., the cardinality of the data set and its frequency distribution. Such information is typically extracted by using sketch algorithms, now widely employed to process very large data sets in manageable space and in a single pass over the data. Often, analysts need more than one parameter to characterize the stream. However, computing multiple sketches becomes expensive even when using high-end CPUs. Exploiting the increasing adoption of hardware accelerators, this paper proposes SKT, an FPGA-based accelerator that can compute several sketches along with basic statistics (average, max, min, etc.) in a single pass over the data. SKT has been designed to characterize a data set by calculating its cardinality, its second frequency moment, and its frequency distribution. The design processes data streams coming either from PCIe or TCP/IP, and it is built to fit emerging cloud service architectures, such as Microsoft's Catapult or Amazon's AQUA. The paper explores the trade-offs of designing sketch algorithms on a spatial architecture and how to combine several sketch algorithms into a single design. The empirical evaluation shows how SKT on an FPGA offers a significant performance gain over high-end, server-class CPUs.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2369–2382},
numpages = {14}
}

@inproceedings{10.1145/2896387.2896447,
author = {Mansour, Ibrahim and Sahandi, Reza and Cooper, Kendra and Warman, Adrian},
title = {Interoperability in the Heterogeneous Cloud Environment: A Survey of Recent User-Centric Approaches},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2896447},
doi = {10.1145/2896387.2896447},
abstract = {Cloud computing provides users the ability to access shared, online computing resources. However, providers often offer their own proprietary applications, interfaces, APIs and infrastructures, resulting in a heterogeneous cloud environment. This heterogeneous environment makes it difficult for users to change cloud service providers; exploring capabilities to support the automated migration from one provider to another is an active, open research area. Many standards bodies (IEEE, NIST, DMTF and SNIA), industry (middleware) and academia have been pursuing approaches to reduce the impact of vendor lock-in by investigating the cloud migration problem at the level of the VM. However, the migration downtime, decoupling VM from underlying systems and security of live channels remain open issues. This paper focuses on analysing recently proposed live, cloud migration approaches for VMs at the infrastructure level in the cloud architecture. The analysis reveals issues with flexibility, performance, and security of the approaches, including additional loads to the CPU and disk I/O drivers of the physical machine where the VM initially resides. The next steps of this research are to develop and evaluate a new approach LibZam (Libya Zamzem) that will work towards addressing the identified limitations.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {62},
numpages = {7},
keywords = {Cloud Architecture, Software Defined Network, Cloud Computing, Cloud Migration, Network Function Virtualization, Cloud Interoperability, VM Live Migration, Cloud Infrastructure},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@article{10.5555/2910557.2910562,
author = {Formiga, Llu\'{\i}s and Barr\'{o}n-Cede\~{n}o, Alberto and M\`{a}rquez, Llu\'{\i}s and Henr\'{\i}quez, Carlos A. and Mari\~{n}o, Jos\'{e} B.},
title = {Leveraging Online User Feedback to Improve Statistical Machine Translation},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {In this article we present a three-step methodology for dynamically improving a statistical machine translation (SMT) system by incorporating human feedback in the form of free edits on the system translations. We target at feedback provided by casual users, which is typically error-prone. Thus, we first propose a filtering step to automatically identify the better user-edited translations and discard the useless ones. A second step produces a pivot-based alignment between source and user-edited sentences, focusing on the errors made by the system. Finally, a third step produces a new translation model and combines it linearly with the one from the original system. We perform a thorough evaluation on a real-world dataset collected from the Reverso.net translation service and show that every step in our methodology contributes significantly to improve a general purpose SMT system. Interestingly, the quality improvement is not only due to the increase of lexical coverage, but to a better lexical selection, reordering, and morphology. Finally, we show the robustness of the methodology by applying it to a different scenario, in which the new examples come from an automatically Web-crawled parallel corpus. Using exactly the same architecture and models provides again a significant improvement of the translation quality of a general purpose baseline SMT system.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {159–192},
numpages = {34}
}

@inproceedings{10.1145/3338103.3338106,
author = {Landwehr, Marvin and Borning, Alan and Wulf, Volker},
title = {The High Cost of Free Services: Problems with Surveillance Capitalism and Possible Alternatives for IT Infrastructure},
year = {2019},
isbn = {9781450372817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338103.3338106},
doi = {10.1145/3338103.3338106},
abstract = {A large portion of the software side of our information technology infrastructure, including web search, email, social media, transportation information, and much more, is provided "free" to the end users, although the corporations that provide this are often enormously profitable. The business model involves customized advertising and behavior manipulation, powered by intensive gathering and cross-correlation of personal information. Significant other parts of our IT infrastructure use fees-for-service but still involve intensive information gathering and behavior manipulation. There are significant indirect costs of these business models, including loss of privacy, supporting surveillance by both corporations and the state, automated manipulations of behavior, undermining the democratic process, and consumerism with its attendant environmental costs. In a recent book, Shoshana Zuboff terms this "surveillance capitalism." Our primary focus in this essay is how we could develop new models for providing these services. We describe some intermediate steps toward those models: education, regulation, and resistance. Following that, we discuss a partial solution, involving for-profit corporations that provide these services without tracking personal information. Finally, we describe desired characteristics for more comprehensive solutions, and outline a range of such solutions for different portions of the IT infrastructure that more truly return control to the end users. A common feature of several is the use of highly decentralized storage of information (either on the end user's own personal devices or on small servers), a modular architecture and interface to allow for customization of what information is to be shared, and a distributed ledger mechanism for authentication.},
booktitle = {Proceedings of the Fifth Workshop on Computing within Limits},
articleno = {3},
numpages = {10},
keywords = {economics, advertising, surveillance capitalism, political manipulation, manipulation of behavior, digital infrastructure, IT business models},
location = {Lappeenranta, Finland},
series = {LIMITS '19}
}

@inproceedings{10.1145/3079856.3080210,
author = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
title = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080210},
doi = {10.1145/3079856.3080210},
abstract = {With increasing deployment of virtual machines for cloud services and server applications, memory address translation overheads in virtualized environments have received great attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss necessitates up to 24 memory references for one guest to host translation. While dedicated page walk caches and such recent enhancements eliminate many of these memory references, our measurements on the Intel Skylake processors indicate that many programs in virtualized mode of execution still spend hundreds of cycles for translations that do not hit in the TLBs.This paper presents an innovative scheme to reduce the cost of address translations by using a very large Translation Lookaside Buffer that is part of memory, the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses required in commonly used 2D walks with radix-4 type of page tables. Even if many of the 24 accesses may hit in the page walk caches, the aggregated cost of the many hits plus the overhead of occasional misses from page walk caches still exceeds the cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space, TLB entries (as opposed to multiple page table entries) can be cached in large L2 and L3 data caches, yielding significant benefits. Through detailed evaluation running SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves performance by approximately 10\% on average. The improvement is more than 16\% for 5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate nearly all TLB misses in 8-core systems.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {469–480},
numpages = {12},
keywords = {Very Large TLB, Address Translation, Die-Stacked DRAM, Virtualization},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1145/3140659.3080210,
author = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
title = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080210},
doi = {10.1145/3140659.3080210},
abstract = {With increasing deployment of virtual machines for cloud services and server applications, memory address translation overheads in virtualized environments have received great attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss necessitates up to 24 memory references for one guest to host translation. While dedicated page walk caches and such recent enhancements eliminate many of these memory references, our measurements on the Intel Skylake processors indicate that many programs in virtualized mode of execution still spend hundreds of cycles for translations that do not hit in the TLBs.This paper presents an innovative scheme to reduce the cost of address translations by using a very large Translation Lookaside Buffer that is part of memory, the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses required in commonly used 2D walks with radix-4 type of page tables. Even if many of the 24 accesses may hit in the page walk caches, the aggregated cost of the many hits plus the overhead of occasional misses from page walk caches still exceeds the cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space, TLB entries (as opposed to multiple page table entries) can be cached in large L2 and L3 data caches, yielding significant benefits. Through detailed evaluation running SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves performance by approximately 10\% on average. The improvement is more than 16\% for 5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate nearly all TLB misses in 8-core systems.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {469–480},
numpages = {12},
keywords = {Die-Stacked DRAM, Very Large TLB, Address Translation, Virtualization}
}

@inproceedings{10.1145/3472727.3472798,
author = {Zhang, Zhi-Li and Dayalan, Udhaya Kumar and Ramadan, Eman and Salo, Timothy J.},
title = {Towards a Software-Defined, Fine-Grained QoS Framework for 5G and Beyond Networks},
year = {2021},
isbn = {9781450386333},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472727.3472798},
doi = {10.1145/3472727.3472798},
abstract = {5G offers a slew of new features and capabilities to support a whole gamut of new applications. On the other hand, 5G new radio (NR), especially, high-band mmWave radio, also poses new challenges, as shown by recent measurement studies of commercial 5G services. In order to effectively support new classes of application such as extra low-latency and/or high-bandwidth applications, we argue that truly cross-layer network-application integration that exposes application semantics to enable 5G and beyond 5G (B5G) networks to make intelligent decisions, e.g., for dynamic radio resource allocation, is needed. Unfortunately the existing 5G flow-based framework is inadequate to support such cross-layer integration. We therefore advocate a software-defined, fine-grained QoS framework. We use ultra-high resolution (UHR) volumetric video streaming as a use case and conduct very preliminary experiments to demonstrate the potential benefits of the proposed framework. This position paper serves as a strawman to call for new intelligent architectural designs for B5G networks and next-generation wireless systems.},
booktitle = {Proceedings of the ACM SIGCOMM 2021 Workshop on Network-Application Integration},
pages = {7–13},
numpages = {7},
keywords = {software -defined, application semantics, fine-grained, 5G and beyond, QoS framework},
location = {Virtual Event, USA},
series = {NAI'21}
}

@inproceedings{10.1145/2876019.2876023,
author = {Pan, Xiang and Yegneswaran, Vinod and Chen, Yan and Porras, Phillip and Shin, Seungwon},
title = {HogMap: Using SDNs to Incentivize Collaborative Security Monitoring},
year = {2016},
isbn = {9781450340786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876019.2876023},
doi = {10.1145/2876019.2876023},
abstract = {Cyber Threat Intelligence (CTI) sharing facilitates a comprehensive understanding of adversary activity and enables enterprise networks to prioritize their cyber defense technologies. To that end, we introduce HogMap, a novel software-defined infrastructure that simplifies and incentivizes collaborative measurement and monitoring of cyber-threat activity. HogMap proposes to transform the cyber-threat monitoring landscape by integrating several novel SDN-enabled capabilities: (i) intelligent in-place filtering of malicious traffic, (ii) dynamic migration of interesting and extraordinary traffic and (iii) a software-defined marketplace where various parties can opportunistically subscribe to and publish cyber-threat intelligence services in a flexible manner.We present the architectural vision and summarize our preliminary experience in developing and operating an SDN-based HoneyGrid, which spans three enterprises and implements several of the enabling capabilities (e.g., traffic filtering, traffic forwarding and connection migration). We find that SDN technologies greatly simplify the design and deployment of such globally distributed and elastic HoneyGrids.},
booktitle = {Proceedings of the 2016 ACM International Workshop on Security in Software Defined Networks \&amp; Network Function Virtualization},
pages = {7–12},
numpages = {6},
keywords = {cyber threat intelligence, honeynet, honeygrid, sdn, marketplace},
location = {New Orleans, Louisiana, USA},
series = {SDN-NFV Security '16}
}

@inproceedings{10.1145/3109859.3109861,
author = {Jaradat, Shatha},
title = {Deep Cross-Domain Fashion Recommendation},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109861},
doi = {10.1145/3109859.3109861},
abstract = {With the increasing number of online shopping services, the number of users and the quantity of visual and textual information on the Internet, there is a pressing need for intelligent recommendation systems that analyze the user's behavior amongst multiple domains and help them to find the desirable information without the burden of search. However, there is little research that has been done on complex recommendation scenarios that involve knowledge transfer across multiple domains. This problem is especially challenging when the involved data sources are complex in terms of the limitations on the quantity and quality of data that can be crawled. The goal of this paper is studying the connection between visual and textual inputs for better analysis of a certain domain, and to examine the possibility of knowledge transfer from complex domains for the purpose of efficient recommendations. The methods employed to achieve this study include both design of architecture and algorithms using deep learning technologies to analyze the effect of deep pixel-wise semantic segmentation and text integration on the quality of recommendations. We plan to develop a practical testing environment in a fashion domain.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {407–410},
numpages = {4},
keywords = {cnn, transfer learning, deep learning, fashion recommendation, domain adaptation, cross-domain knowledge transfer},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/3404868.3406662,
author = {Edeline, Korian and Donnet, Benoit},
title = {Evaluating the Impact of Path Brokenness on TCP Options},
year = {2020},
isbn = {9781450380393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404868.3406662},
doi = {10.1145/3404868.3406662},
abstract = {In-path network functions enforcing policies like firewalls, IDSes, NATs, and TCP enhancing proxies are ubiquitous. They are deployed in various types of networks and bring obvious value to the Internet.Unfortunately, they also break important architectural principles and, consequently, make the Internet less flexible by preventing the use of advanced protocols, features, or options. In some scenarios, feature-disabling middlebox policies can lead to a performance shortfall. Moreover, middleboxes are also prone to enforce policies that disrupt transport control mechanisms, which can also have direct consequences in term of Quality-of-Service (QoS).In this paper, we investigate the impact of the most prevalent in-path impairments on the TCP protocol and its features. Using network experiments in a controlled environment, we quantify the QoS decreases and shortfall induced by feature-breaking middleboxes, and show that even in the presence of a fallback mechanism, TCP QoS remains affected.},
booktitle = {Proceedings of the Applied Networking Research Workshop},
pages = {38–44},
numpages = {7},
location = {Virtual Event, Spain},
series = {ANRW '20}
}

@inproceedings{10.1145/3038450.3038452,
author = {Michalakis, Konstantinos and Aliprantis, John and Caridakis, George},
title = {Intelligent Visual Interface with the Internet of Things},
year = {2017},
isbn = {9781450349024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3038450.3038452},
doi = {10.1145/3038450.3038452},
abstract = {Communication between users and physical objects and sensors through the web within the Internet of Things framework, requires by definition the capability to perceive the sensors and the underlying information and services. Visualization of the Things in IoT is thus a requirement for natural interaction between users and IoT instances in the upcoming but steadily established computing paradigm. The immense quantity of sensors and variety of usable information introduces the need to intelligently filter and adapt the respective information sources and layers. Current work proposes an architecture that supports intelligent interaction between users and the IoT addressing the intelligent perception requirement described earlier. On the one hand, sensory visualization is tackled via Augmented Reality layers of sensors and information and on the other hand context and location awareness enhance the system by providing usable in the respective senses information.},
booktitle = {Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects},
pages = {27–30},
numpages = {4},
keywords = {context awareness, markerless tracking, natural interaction, augmented reality, internet of things},
location = {Limassol, Cyprus},
series = {SmartObject '17}
}

@article{10.1145/2699697,
author = {Yue, Tao and Briand, Lionel C. and Labiche, Yvan},
title = {AToucan: An Automated Framework to Derive UML Analysis Models from Use Case Models},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2699697},
doi = {10.1145/2699697},
abstract = {The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model comprising class, sequence and activity diagrams from a use case model and to automatically establish traceability links between model elements of the use case model and the generated analysis model. Note that our goal is to save effort through automated support, not to replace human abstraction and decision making.Seven (six) case studies were performed to compare class (sequence) diagrams generated by aToucan to the ones created by experts, Masters students, and trained, fourth-year undergraduate students. Results show that aToucan performs well regarding consistency (e.g., 88\% class diagram consistency) and completeness (e.g., 80\% class completeness) when comparing generated class diagrams with reference class diagrams created by experts and Masters students. Similarly, sequence diagrams automatically generated by aToucan are highly consistent with the ones devised by experts and are also rather complete, for instance, 91\% and 97\% message consistency and completeness, respectively. Further, statistical tests show that aToucan significantly outperforms fourth-year engineering students in this respect, thus demonstrating the value of automation. We also conducted two industrial case studies demonstrating the applicability of aToucan in two different industrial domains. Results showed that the vast majority of model elements generated by aToucan are correct and that therefore, in practice, such models would be good initial models to refine and augment so as to converge towards to correct and complete analysis models. A performance analysis shows that the execution time of aToucan (when generating class and sequence diagrams) is dependent on the number of simple sentences contained in the use case model and remains within a range of a few minutes. Five different software system descriptions (18 use cases altogether) were performed to evaluate the generation of activity diagrams. Results show that aToucan can generate 100\% complete and correct control flow information of activity diagrams and on average 85\% data flAow information completeness. Moreover, we show that aToucan outperforms three commercial tools in terms of activity diagram generation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {13},
numpages = {52},
keywords = {sequence diagram, traceability, analysis model, automation, Use case modeling, class diagram, activity diagram, transformation, UML}
}

@inproceedings{10.1145/3465481.3470030,
author = {Espinha Gasiba, Tiago and Andrei-Cristian, Iosif and Lechner, Ulrike and Pinto-Albuquerque, Maria},
title = {Raising Security Awareness of Cloud Deployments Using Infrastructure as Code through CyberSecurity Challenges},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470030},
doi = {10.1145/3465481.3470030},
abstract = {Improper deployment of software can have serious consequences, ranging from simple downtime to permanent data loss and data breaches. Infrastructure as Code tools serve to streamline delivery by promising consistency and speed, by abstracting away from the underlying actions. However, this simplicity may distract from architectural or configuration faults, potentially compromising the secure development lifecycle. One way to address this issue involves awareness training. Sifu is a platform that provides education on security through serious games, developed in the industry, for the industry. The presented work extends the Sifu platform with challenges addressing Terraform-aided cloud deployment on Amazon Web Services. This paper proposes an evaluation pipeline behind the challenges, and provides details of the vulnerability detection and feedback mechanisms, as well as a novel technique for detecting undesired differences between a given architecture and a target result. Furthermore, this paper quantifies the challenges’ perceived usefulness and impact, by evaluating the challenges among a total of twelve participants. Our preliminary results show that the challenges are suitable for education and the industry, with potential usage in internal training. A key finding is that, although the participants understand the importance of secure coding, their answers indicate that universities leave them unprepared in this area. Finally, our results are compared with related industry works, to extract and provide good practices and advice for practitioners.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {63},
numpages = {8},
keywords = {Serious Games, Secure Coding, Infrastructure as Code, Training, Awareness, DevSecOps, CyberSecurity Challenges},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1145/3075564.3076259,
author = {Llewellynn, Tim and Fern\'{a}ndez-Carrobles, M. Milagro and Deniz, Oscar and Fricker, Samuel and Storkey, Amos and Pazos, Nuria and Velikic, Gordana and Leufgen, Kirsten and Dahyot, Rozenn and Koller, Sebastian and Goumas, Georgios and Leitner, Peter and Dasika, Ganesh and Wang, Lei and Tutschku, Kurt},
title = {BONSEYES: Platform for Open Development of Systems of Artificial Intelligence: Invited Paper},
year = {2017},
isbn = {9781450344876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3075564.3076259},
doi = {10.1145/3075564.3076259},
abstract = {The Bonseyes EU H2020 collaborative project aims to develop a platform consisting of a Data Marketplace, a Deep Learning Toolbox, and Developer Reference Platforms for organizations wanting to adopt Artificial Intelligence. The project will be focused on using artificial intelligence in low power Internet of Things (IoT) devices ("edge computing"), embedded computing systems, and data center servers ("cloud computing"). It will bring about orders of magnitude improvements in efficiency, performance, reliability, security, and productivity in the design and programming of systems of artificial intelligence that incorporate Smart Cyber-Physical Systems (CPS). In addition, it will solve a causality problem for organizations who lack access to Data and Models. Its open software architecture will facilitate adoption of the whole concept on a wider scale. To evaluate the effectiveness, technical feasibility, and to quantify the real-world improvements in efficiency, security, performance, effort and cost of adding AI to products and services using the Bonseyes platform, four complementary demonstrators will be built. Bonseyes platform capabilities are aimed at being aligned with the European FI-PPP activities and take advantage of its flagship project FIWARE. This paper provides a description of the project motivation, goals and preliminary work.},
booktitle = {Proceedings of the Computing Frontiers Conference},
pages = {299–304},
numpages = {6},
keywords = {Internet of things, Deep Learning, Smart Cyber-Physical Systems, Data marketplace},
location = {Siena, Italy},
series = {CF'17}
}

@inproceedings{10.1145/2744769.2744912,
author = {Bokhari, Haseeb and Javaid, Haris and Shafique, Muhammad and Henkel, J\"{o}rg and Parameswaran, Sri},
title = {SuperNet: Multimode Interconnect Architecture for Manycore Chips},
year = {2015},
isbn = {9781450335201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2744769.2744912},
doi = {10.1145/2744769.2744912},
abstract = {Designers of the on-chip interconnect for manycore chips are faced with the dilemma of meeting performance, power and reliability requirements for different operational scenarios. In this paper, we propose a multimode on-chip interconnect called SuperNet. This interconnect can be configured to run in three different modes: energy efficient mode; performance mode; and, reliability mode. Our proposed interconnect is based on two parallel multi-vt optimized packet switched network-on-chip (NoC) meshes. We describe the circuit design techniques and architectural modifications required to realize such a multimode interconnect. Our evaluation with diverse set of applications show that the energy efficient mode can save on average 40\% NoC power, whereas the performance mode can improve the core IPC by up to 13\% on selected high MPKI applications. The reliability mode provides protection against soft errors in the router's data path through byte oriented SECDED codes that can correct up to 8 bit errors and detect up to 16 bit errors in a 64 bit flit, whereas the router's control path is protected through DMR lock step execution.},
booktitle = {Proceedings of the 52nd Annual Design Automation Conference},
articleno = {85},
numpages = {6},
keywords = {power optimization, fault tolerance, performance, network-on-chip, multimode},
location = {San Francisco, California},
series = {DAC '15}
}

@inproceedings{10.1145/3123939.3123983,
author = {Fang, Yuanwei and Zou, Chen and Elmore, Aaron J. and Chien, Andrew A.},
title = {UDP: A Programmable Accelerator for Extract-Transform-Load Workloads and More},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3123983},
doi = {10.1145/3123939.3123983},
abstract = {Big data analytic applications give rise to large-scale extract-transform-load (ETL) as a fundamental step to transform new data into a native representation. ETL workloads pose significant performance challenges on conventional architectures, so we propose the design of the unstructured data processor (UDP), a software programmable accelerator that includes multi-way dispatch, variable-size symbol support, Flexible-source dispatch (stream buffer and scalar registers), and memory addressing to accelerate ETL kernels both for current and novel future encoding and compression. Specifically, UDP excels at branch-intensive and symbol and pattern-oriented workloads, and can offload them from CPUs.To evaluate UDP, we use a broad set of data processing workloads inspired by ETL, but broad enough to also apply to query execution, stream processing, and intrusion detection/monitoring. A single UDP accelerates these data processing tasks 20-fold (geometric mean, largest increase from 0.4 GB/s to 40 GB/s) and performance per watt by a geomean of 1,900-fold. UDP ASIC implementation in 28nm CMOS shows UDP logic area of 3.82mm2 (8.69mm2 with 1MB local memory), and logic power of 0.149W (0.864W with 1MB local memory); both much smaller than a single core.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {55–68},
numpages = {14},
keywords = {control-flow accelerator, data analytics, parsing, data encoding and transformation, compression},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings{10.1145/3023956.3023958,
author = {Lachmann, Remo and Beddig, Simon and Lity, Sascha and Schulze, Sandro and Schaefer, Ina},
title = {Risk-Based Integration Testing of Software Product Lines},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023958},
doi = {10.1145/3023956.3023958},
abstract = {Software product lines (SPL) capture commonalities and variabilities of product families and, thus, enable mass customization of product variants according to customers desired configurations. However, they introduce new challenges to software testing due to a potentially large number of variants. While each variant should be tested, testing resources are limited and, thus, a retest of all, partially redundant, test cases for each variant is not feasible in SPL testing. Coping with these issues has been a major research focus in recent years, leading to different testing approaches. However, risk-based testing has not gained much attention in the SPL domain while being a successful approach for single-software systems. In this paper, we propose a novel risk-based testing approach for SPL integration testing. We incrementally test SPLs by stepping from one variant to the next. For each variant, we automatically compute failure probabilities and failure impacts for its architectural components. To avoid a computational overhead of generating and analyzing each variant, we exploit the variability between variants defined as deltas to focus on important changes. We evaluate our approach using an automotive case study, showing that the risk-based technique leads to positive results compared to random and delta-oriented testing.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {52–59},
numpages = {8},
keywords = {model-based testing, risk-based testing, software product lines, test case prioritization},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1145/3358696,
author = {Tang, Yibin and Wang, Ying and Li, Huawei and Li, Xiaowei},
title = {MV-Net: Toward Real-Time Deep Learning on Mobile GPGPU Systems},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3358696},
doi = {10.1145/3358696},
abstract = {Recently the development of deep learning has been propelling the sheer growth of vision and speech applications on lightweight embedded and mobile systems. However, the limitation of computation resource and power delivery capability in embedded platforms is recognized as a significant bottleneck that prevents the systems from providing real-time deep learning ability, since the inference of deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs) involves large quantities of weights and operations. Particularly, how to provide quality-of-services (QoS)-guaranteed neural network inference ability in the multitask execution environment of multicore SoCs is even more complicated due to the existence of resource contention. In this article, we present a novel deep neural network architecture, MV-Net, which provides performance elasticity and contention-aware self-scheduling ability for QoS enhancement in mobile computing systems. When the constraints of QoS, output accuracy, and resource contention status of the system change, MV-Net can dynamically reconfigure the corresponding neural network propagation paths and thus achieves an effective tradeoff between neural network computational complexity and prediction accuracy via approximate computing. The experimental results show that (1) MV-Net significantly improves the performance flexibility of current CNN models and makes it possible to provide always-guaranteed QoS in a multitask environment, and (2) it satisfies the quality-of-results (QoR) requirement, outperforming the baseline implementation significantly, and improves the system energy efficiency at the same time.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = {oct},
articleno = {35},
numpages = {25},
keywords = {deep learning, online scheduling, energy efficiency, approximate computing, Edge computing}
}

@article{10.14778/3484224.3484229,
author = {Koutsoukos, Dimitrios and M\"{u}ller, Ingo and Marroqu\'{\i}n, Renato and Klimovic, Ana and Alonso, Gustavo},
title = {Modularis: Modular Relational Analytics over Heterogeneous Distributed Platforms},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484229},
doi = {10.14778/3484224.3484229},
abstract = {The enormous quantity of data produced every day together with advances in data analytics has led to a proliferation of data management and analysis systems. Typically, these systems are built around highly specialized monolithic operators optimized for the underlying hardware. While effective in the short term, such an approach makes the operators cumbersome to port and adapt, which is increasingly required due to the speed at which algorithms and hardware evolve. To address this limitation, we present Modularis, an execution layer for data analytics based on sub-operators, i.e., composable building blocks resembling traditional database operators but at a finer granularity. To demonstrate the feasibility and advantages of our approach, we use Modularis to build a distributed query processing system supporting relational queries running on an RDMA cluster, a serverless cloud platform, and a smart storage engine. Modularis requires minimal code changes to execute queries across these three diverse hardware platforms, showing that the sub-operator approach reduces the amount and complexity of the code to maintain. In fact, changes in the platform affect only those sub-operators that depend on the underlying hardware (in our use cases, mainly the sub-operators related to network communication). We show the end-to-end performance of Modularis by comparing it with a framework for SQL processing (Presto), a commercial cluster database (SingleStore), as well as Query-as-a-Service systems (Athena, BigQuery). Modularis outperforms all these systems, proving that the design and architectural advantages of a modular design can be achieved without degrading performance. We also compare Modularis with a hand-optimized implementation of a join for RDMA clusters. We show that Modularis has the advantage of being easily extensible to a wider range of join variants and group by queries, all of which are not supported in the hand-tuned join.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3308–3321},
numpages = {14}
}

@inproceedings{10.1145/2811163.2811174,
author = {Park, Junseok and Choo, Sungji and Lee, Doheon},
title = {Citizen Organization System for Advanced MEDical Research (COSAMED)},
year = {2015},
isbn = {9781450337878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811163.2811174},
doi = {10.1145/2811163.2811174},
abstract = {Analyzing true effect of medicine or functional food is major issue in associated research area. In order to achieve that, gathering of massive clinical trial data is required. There are three major concepts for clinical trial data collection: Citizen Science, Health 2.0 and Crowdsourcing. Citizen Science, which uses web 2.0 technologies, is web-based service for health care. Health 2.0 uses non-professionally trained individuals to conduct science-related activities. Lastly, Crowdsourcing is an online distributed problem-solving and production model. Following systems have tried to process data based on above concepts. PatientsLikeme attempted to find potential benefits from clinical outcomes within longitudinal evaluation of online data-sharing platforms. ResearchKit is about to create apps that could revolutionize medical studies. However, these systems do not have reliable protocols to obtain credible results. In addition, they mainly focus on diseases with a medicine, not on effect with a functional food.Hereby, we are developing a novel system to solve the issues: Citizen Organization System for Advanced MEDical research(COSAMED). We are looking forward to find true effect information of a functional food with our new system. COSAMED is made of five steps to design a reliable protocol. (1) Target Item Selection, to select a target effect and effect related items. (2) Preparation of Research, to select designed clinical trial protocol on user demand with automated scientific criteria. (3) Recruiting Participants, to recruit participants from linked SNS friends or from other systems. (4) Data Collection, to collect effect information from various sources. (5) Analysis, to analyze the results by web-bases statistical tools, transfer results to a data warehouse and calculate credibility rate. Finally, the protocol is developed with product DB and clinical trial protocol snapshot DB on COSAMED. In future, we will integrate it with Openmhealth architecture to connect related systems easily and build it with user friendly interfaces to collect big data. COSAMED will be available at www.cosamed.org, and it will be a cornerstone of first citizen based clinical trial system.},
booktitle = {Proceedings of the ACM Ninth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {23},
numpages = {1},
keywords = {web based system, functional food, clinical trial protocol, citizen science, open source, health 2.0, crowdsourcing},
location = {Melbourne, Australia},
series = {DTMBIO '15}
}

@article{10.1109/TCBB.2014.2361348,
author = {Wang, Jianxin and Zhong, Jiancheng and Chen, Gang and Li, Min and Wu, Fang-xiang and Pan, Yi},
title = {ClusterViz: A Cytoscape APP for Cluster Analysis of Biological Network},
year = {2015},
issue_date = {July/August 2015},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {12},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2014.2361348},
doi = {10.1109/TCBB.2014.2361348},
abstract = {Cluster analysis of biological networks is one of the most important approaches for identifying functional modules and predicting protein functions. Furthermore, visualization of clustering results is crucial to uncover the structure of biological networks. In this paper, ClusterViz, an APP of Cytoscape 3 for cluster analysis and visualization, has been developed. In order to reduce complexity and enable extendibility for ClusterViz, we designed the architecture of ClusterViz based on the framework of Open Services Gateway Initiative. According to the architecture, the implementation of ClusterViz is partitioned into three modules including interface of ClusterViz, clustering algorithms and visualization and export. ClusterViz fascinates the comparison of the results of different algorithms to do further related analysis. Three commonly used clustering algorithms, FAG-EC, EAGLE and MCODE, are included in the current version. Due to adopting the abstract interface of algorithms in module of the clustering algorithms, more clustering algorithms can be included for the future use. To illustrate usability of ClusterViz, we provided three examples with detailed steps from the important scientific articles, which show that our tool has helped several research teams do their research work on the mechanism of the biological networks.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {815–822},
numpages = {8},
keywords = {cytoscape, biological networks, cluster, FAG-EC, EAGLE, visualization, MCODE}
}

@inproceedings{10.1145/2723372.2723719,
author = {Petraki, Eleni and Idreos, Stratos and Manegold, Stefan},
title = {Holistic Indexing in Main-Memory Column-Stores},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2723719},
doi = {10.1145/2723372.2723719},
abstract = {Great database systems performance relies heavily on index tuning, i.e., creating and utilizing the best indices depending on the workload. However, the complexity of the index tuning process has dramatically increased in recent years due to ad-hoc workloads and shortage of time and system resources to invest in tuning.This paper introduces holistic indexing, a new approach to automated index tuning in dynamic environments. Holistic indexing requires zero set-up and tuning effort, relying on adaptive index creation as a side-effect of query processing. Indices are created incrementally and partially;they are continuously refined as we process more and more queries. Holistic indexing takes the state-of-the-art adaptive indexing ideas a big step further by introducing the notion of a system which never stops refining the index space, taking educated decisions about which index we should incrementally refine next based on continuous knowledge acquisition about the running workload and resource utilization. When the system detects idle CPU cycles, it utilizes those extra cycles by refining the adaptive indices which are most likely to bring a benefit for future queries. Such idle CPU cycles occur when the system cannot exploit all available cores up to 100\%, i.e., either because the workload is not enough to saturate the CPUs or because the current tasks performed for query processing are not easy to parallelize to the point where all available CPU power is exploited.In this paper, we present the design of holistic indexing for column-oriented database architectures and we discuss a detailed analysis against parallel versions of state-of-the-art indexing and adaptive indexing approaches. Holistic indexing is implemented in an open-source column-store DBMS. Our detailed experiments on both synthetic and standard benchmarks (TPC-H) and workloads (SkyServer) demonstrate that holistic indexing brings significant performance gains by being able to continuously refine the physical design in parallel to query processing, exploiting any idle CPU resources.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1153–1166},
numpages = {14},
keywords = {self-organization, holistic indexing},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@article{10.1145/3464994.3464998,
author = {Balakrishnan, Hari and Banerjee, Sujata and Cidon, Israel and Culler, David and Estrin, Deborah and Katz-Bassett, Ethan and Krishnamurthy, Arvind and McCauley, Murphy and McKeown, Nick and Panda, Aurojit and Ratnasamy, Sylvia and Rexford, Jennifer and Schapira, Michael and Shenker, Scott and Stoica, Ion and Tennenhouse, David and Vahdat, Amin and Zegura, Ellen},
title = {Revitalizing the Public Internet by Making It Extensible},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/3464994.3464998},
doi = {10.1145/3464994.3464998},
abstract = {There is now a significant and growing functional gap between the public Internet, whose basic architecture has remained unchanged for several decades, and a new generation of more sophisticated private networks. To address this increasing divergence of functionality and overcome the Internet's architectural stagnation, we argue for the creation of an Extensible Internet (EI) that supports in-network services that go beyond best-effort packet delivery. To gain experience with this approach, we hope to soon deploy both an experimental version (for researchers) and a prototype version (for early adopters) of EI. In the longer term, making the Internet extensible will require a community to initiate and oversee the effort; this paper is the first step in creating such a community.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {18–24},
numpages = {7},
keywords = {internet architecture}
}

@inproceedings{10.1145/2702613.2732810,
author = {Park, Joongsin and Jeong, Beomtaek and Jeon, Seungjai and Han, Sehyung and Cho, Jun-Dong and Ko, JeongGil},
title = {Understanding Interactive Interface Design Requirements for the Visually Impaired},
year = {2015},
isbn = {9781450331463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702613.2732810},
doi = {10.1145/2702613.2732810},
abstract = {While taxies are widely considered as an easily accessible form of transportation to many, for the visually impaired, utilizing taxi services can be a significant challenge. In this paper we envision a system architecture where visually impaired people use GPS-enabled mobile computing devices to easily reserve and access taxi services. Specifically, as the first step in designing such a system, we try to understand the preferred interaction interface requirements of the visually impaired population using a set of interviews conducted over 28 visually impaired participants. Our results show that the smartphone usage rate of our interview participants is ~60\%; thus, smartphone-based applications should not be considered as the "everyone-will-use-platform" when interacting with the visually impaired. Results from an extensive set of questions reveal that interaction interfaces in the form of key chains and wrist watches can also be effective for various interactive applications.},
booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {881–886},
numpages = {6},
keywords = {visually impaired interfaces, wireless interaction systems},
location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {CHI EA '15}
}

@inproceedings{10.1145/2700171.2804454,
author = {Torre, Ilaria and Celik, Ilknur},
title = {User-Adapted Web of Things for Accessibility},
year = {2015},
isbn = {9781450333955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2700171.2804454},
doi = {10.1145/2700171.2804454},
abstract = {This paper describes a new wave of the Web that is the useradapted Web of Things. This is a new step in the evolution of the Web of Things and of adaptive web-based systems. The current proposals for the Web of Things focus on the augmentation of the physical objects in order to provide enhanced services. However, in our view, the Web of Things can also be a means to make physical objects accessible or more usable for people with special needs by exploiting adaptive and semantic techniques. The architecture presented in the paper describes the specific modules and components at the basis of this approach.},
booktitle = {Proceedings of the 26th ACM Conference on Hypertext \&amp; Social Media},
pages = {341–344},
numpages = {4},
keywords = {accessibility, user-adapted interaction, linked data, web of things, adaptation techniques, semantic web, special needs., adaptive web},
location = {Guzelyurt, Northern Cyprus},
series = {HT '15}
}

@inproceedings{10.1145/2602576.2602578,
author = {Johnson, Kenneth and Calinescu, Radu},
title = {Efficient Re-Resolution of SMT Specifications for Evolving Software Architectures},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602578},
doi = {10.1145/2602576.2602578},
abstract = {We present a generic method for the efficient constraint re-resolution of a component-based software architecture after changes such as addition, removal and modification of components. Given a formal description of an evolving system as a constraint-specification problem, our method identifies and executes the re-resolution steps required to verify the system's compliance with constraints after each change. At each step, satisfiability modulo theory (SMT) techniques determine the satisfiability of component constraints expressed as logical formulae over suitably chosen theories of arithmetic, reusing results obtained in previous steps. We illustrate the application of the approach on a constraint-satisfaction problem arising from cloud-deployed software services. The incremental method is shown to re-resolve system constraints in a fraction of the time taken by standard SMT resolution.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {93–102},
numpages = {10},
keywords = {incremental re-resolution, domain-specific languages, satisfiability modulo theory},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/3094405.3094412,
author = {Cozzolino, Vittorio and Ding, Aaron Yi and Ott, J\"{o}rg},
title = {FADES: Fine-Grained Edge Offloading with Unikernels},
year = {2017},
isbn = {9781450350587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094405.3094412},
doi = {10.1145/3094405.3094412},
abstract = {FADES is an edge offloading architecture that empowers us to run compact, single purpose tasks at the edge of the network to support a variety of IoT and cloud services. The design principle behind FADES is to efficiently exploit the resources of constrained edge devices through fine-grained computation offloading. FADES takes advantage of MirageOS unikernels to isolate and embed application logic in concise Xen-bootable images. We have implemented FADES and evaluated the system performance under various hardware and network conditions. Our results show that FADES can effectively strike a balance between running complex applications in the cloud and simple operations at the edge. As a solid step to enable fine-grained edge offloading, our experiments also reveal the limitation of existing IoT hardware and virtualization platforms, which shed light on future research to bring unikernel into IoT domain.},
booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
pages = {36–41},
numpages = {6},
keywords = {IoT, Edge Computing, Virtualization},
location = {Los Angeles, CA, USA},
series = {HotConNet '17}
}

@inproceedings{10.1145/3230833.3233248,
author = {Cabaj, Krzysztof and Gregorczyk, Marcin and Mazurczyk, Wojciech and Nowakowski, Piotr and \.{Z}\'{o}rawski, Piotr},
title = {SDN-Based Mitigation of Scanning Attacks for the 5G Internet of Radio Light System},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233248},
doi = {10.1145/3230833.3233248},
abstract = {Currently 5G communication networks are gaining on importance among industry, academia, and governments worldwide as they are envisioned to offer wide range of high-quality services and unfaltering user experiences. However, certain security, privacy and trust challenges need to be addressed in order for the 5G networks to be widely welcomed and accepted. That is why in this paper, we take a step towards these requirements and we introduce a dedicated SDN-based integrated security framework for the Internet of Radio Light (IoRL) system that is following 5G architecture design. In particular, we present how TCP SYN-based scanning activities which typically comprise the first phase of the attack chain can be detected and mitigated using such an approach. Enclosed experimental results prove that the proposed security framework has potential to become an effective defensive solution.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {49},
numpages = {10},
keywords = {5G System Architecture, Network Function Virtualization, Visible Light Communications, mm Wave Communications, Integrated Security Framework, Software Defined Networks},
location = {Hamburg, Germany},
series = {ARES '18}
}

@inproceedings{10.1145/3073763.3073770,
author = {de Dinechin, Beno\^{\i}t Dupont and Graillat, Amaury},
title = {Network-on-Chip Service Guarantees on the Kalray MPPA-256 Bostan Processor},
year = {2017},
isbn = {9781450352260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3073763.3073770},
doi = {10.1145/3073763.3073770},
abstract = {The Kalray MPPA-256 Bostan manycore processor implements a clustered architecture, where clusters of cores share a local memory, and a DMA-capable network-on-chip (NoC) connects the clusters. The NoC implements wormhole switching without virtual channels, with source routing, and can be configured for maximum flow rate and burstiness at ingress. We describe and illustrate the techniques used to configure the MPPA NoC for guaranteed services. Our approach is based on three steps: global selection of routes between end-points and computation of flow rates, by solving the max-min fairness with unsplittable path problem; configuration of the flow burstiness parameters at ingress, by solving an acyclic set of linear inequalities; and end-to-end latency upper bound computation, based on the principles of separated flow analysis (SFA). In this paper, we develop the two last steps, taking advantage of the effects of NoC link shaping on the leaky-bucket arrival curves of flows.},
booktitle = {Proceedings of the 2nd International Workshop on Advanced Interconnect Solutions and Technologies for Emerging Computing Systems},
pages = {35–40},
numpages = {6},
keywords = {network-on-chip, deterministic network calculus, separated flow analysis, link traffic shaping},
location = {Stockholm, Sweden},
series = {AISTECS '17}
}

@inproceedings{10.5555/3242181.3242227,
author = {Falcone, Alberto and Garro, Alfredo and Anagnostou, Anastasia and Taylor, Simon J. E.},
title = {An Introduction to Developing Federations with the High Level Architecture (HLA)},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {The IEEE 1516-2010 - High Level Architecture (HLA) for distributed simulation is growing in a variety of application domains due to its capabilities to enable the interoperability and reusability of distributed simulation components. However, the development of simulation models based on the HLA standard remains a challenging task that requires a considerable effort in terms of both time and cost. This paper provides an introduction tutorial on developing HLA-based simulations using the HLA Development Kit (DKF) framework. The tutorial guides developers through the necessary steps for defining and creating an HLA-based simulation, and explains how the HLA elements can be easily managed by using the DKF's services. The effectiveness of the DKF is proven by its concrete exploitation in the context of the Simulation Exploration Experience (SEE), a project led by NASA and which involves as partners several U.S. and European Institutions.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {43},
numpages = {15},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@article{10.14778/2733004.2733046,
author = {To, Quoc-Cuong and Nguyen, Benjamin and Pucheral, Philippe},
title = {SQL/AA: Executing SQL on an Asymmetric Architecture},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733046},
doi = {10.14778/2733004.2733046},
abstract = {Current applications, from complex sensor systems (e.g. quantified self) to online e-markets acquire vast quantities of personal information which usually end-up on central servers. This information represents an unprecedented potential for user customized applications and business (e.g., car insurance billing, carbon tax, traffic decongestion, resource optimization in smart grids, healthcare surveillance, participatory sensing). However, the PRISM affair has shown that public opinion is starting to wonder whether these new services are not bringing us closer to science fiction dystopias. It has become clear that centralizing and processing all one's data on a single server is a major problem with regards to privacy concerns. Conversely, decentralized architectures, devised to help individuals keep full control of their data, complexify global treatments and queries, often impeding the development of innovative services and applications.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1625–1628},
numpages = {4}
}

@inproceedings{10.1145/3318216.3363331,
author = {Romero, Eduardo and Stewart, Christopher and Morris, Nathaniel},
title = {Fast Inference Services for Alternative Deep Learning Structures},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363331},
doi = {10.1145/3318216.3363331},
abstract = {AI inference services receive requests, classify data and respond quickly. These services underlie AI-driven Internet of Things, recommendation engines and video analytics. Neural networks are widely used because they provide accurate results and fast inference, but it is hard to explain their classifications. Tree-based deep learning models can provide accuracy and are innately explainable. However, it is hard to achieve high inference rates because branch misprediction and cache misses produce inefficient executions. My research seeks to produce low latency inference services based on tree-based models. I will exploit the emergence of large L3 caches to convert tree-based model inference from sequential branching toward fast, in-cache lookups. Our approach begins with fully trained, accurate tree-based models, compiles them for inference on target processors and executes inference efficiently. If successful, our approach will enable qualitative advances in AI services. Tree-based models can report the most significant features in a classification in a single pass. In contrast, neural networks require iterative approaches to explain their results. Consider interactive AI recommendation services where users seek to explicitly order their instantaneous preferences to attract preferred content. Tree-based models can provide user feedback much more quickly than neural networks. Tree-based models also have less prediction variance than neural networks. Given the same training data, neural networks require many inferences to quantify variances of borderline classifications. Fast tree-based inference can explain variance in seconds (versus minutes). Our approach shows that competing machine learning approaches can provide comparable accuracy but desire wholly different architectural and platform support.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {329–331},
numpages = {3},
location = {Arlington, Virginia},
series = {SEC '19}
}

@inproceedings{10.1145/3278161.3278166,
author = {Ding, Aaron Yi and Janssen, Marijn},
title = {Opportunities for Applications Using 5G Networks: Requirements, Challenges, and Outlook},
year = {2018},
isbn = {9781450365802},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278161.3278166},
doi = {10.1145/3278161.3278166},
abstract = {The increasing demand for mobile network capacity driven by Internet of Things (IoT) applications results in the need for understanding better the potential and limitations of 5G networks. Vertical application areas like smart mobility, energy networks, industrial IoT applications, and AR/VR enhanced services all pose different requirements on the use of 5G networks. Some applications need low latency, whereas others need high bandwidth or security support. The goal of this paper is to identify the requirements and to understand the limitations for 5G driven applications. We review application areas and list the typical challenges and requirements posed on 5G networks. A main challenge will be to develop a network architecture being able to dynamically adapt to fluctuating traffic patterns and accommodating various technologies such as edge computing, blockchain based distributed ledger, software defined networking, and virtualization. To inspire future research, we reveal open problems and highlight the need for piloting with 5G applications, with tangible steps, to understand the configuration of 5G networks and the use of applications across multiple vertical industries.},
booktitle = {Proceedings of the Seventh International Conference on Telecommunications and Remote Sensing},
pages = {27–34},
numpages = {8},
keywords = {edge computing, smart city, 5G systems, pilot, IoT},
location = {Barcelona, Spain},
series = {ICTRS '18}
}

@inproceedings{10.1145/3095770.3095771,
author = {Maccabe, Arthur B.},
title = {Operating and Runtime Systems Challenges for HPC Systems},
year = {2017},
isbn = {9781450350860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3095770.3095771},
doi = {10.1145/3095770.3095771},
abstract = {Future HPC systems will be characterized by extreme heterogeneity. We will see increasing heterogeneity in virtually every aspect of node architecture from computational engines to memory systems. We will see increasing heterogeneity in applications, including heterogeneity within applications (as previously independent applications are composed to build new applications). We will see increasing heterogeneity in system usage models; in some cases, the HPC system is not the most precious resource being managed. We will also see increasing heterogeneity in the shared services (e.g., storage and visualization systems) that are connected to HPC systems.All of this increasing heterogeneity is certain to create new challenges in the design and implementation of operating and runtime systems. There will be new kinds of resources to manage and many resource management tactics will be invented (and some re-discovered and adapted) to address the new heterogeneity. In essence, we will tacitly agree that the operating and runtime systems need to adapt to enable the inevitable integration of new technologies, applications, usage models, and shared services. While this agreement is critical for our ability to make incremental progress, we, as a community, must step back and ask the relevant question: Does the OS or runtime system bear the brunt of the adaptation, or will we be able to insist on changes in the technologies, applications, and environment? In the past decade, we have seen a similar tradeoff play out between the application teams and the architects of computational engines: how much floating point precision is required and how is this precision implemented? How can we define similar tradeoffs that are important in the design and implementation of operating and runtime systems?},
booktitle = {Proceedings of the 7th International Workshop on Runtime and Operating Systems for Supercomputers ROSS 2017},
articleno = {1},
numpages = {1},
location = {Washingon, DC, USA},
series = {ROSS '17}
}

@inproceedings{10.5555/2772722.2772757,
author = {Bencha\"{\i}b, Yacine and Secci, Stefano and Phung, Chi-Dung},
title = {Transparent Cloud Access Performance Augmentation via an MPTCP-LISP Connection Proxy},
year = {2015},
isbn = {9781467366328},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The use by a growing number of users of Cloud-based services requires an adaptation of the network technologies used to access them. We propose to combine two novel protocols at the state of the art at Cloud access middle-boxes to better profit from spare unused network path diversity. The first protocol, Multipath TCP, allows creating multiple TCP/IP subflows, as much as needed. The second, the Locator/Identifier Separation Protocol (LISP), can be used to route the subflows on different wide-area network paths, possibly disjoint, and also allows native support for seamless virtual machine migrations. In this paper we specify how we can combine these two protocols to increase the bandwidth available to access applications run in multi-homed data-centers. We describe how these protocols can be integrated into a Cloud access middle-box. By means of a combined MPTCP-LISP access proxy, the acceleration is transparent to the user terminal that does not necessitate any upgrade. We provide the detailed system-level architecture based on open source code, and we document results from preliminary experimentations on one of two targeted use-cases. The evaluations conducted show that the overhead generated by our solution remains moderate despite the various system-level steps required to translate incoming TCP packets into MPTCP-LISP packets then routed over different IP paths.},
booktitle = {Proceedings of the Eleventh ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
pages = {201–202},
numpages = {2},
keywords = {data-center networking, cloud access protocol},
location = {Oakland, California, USA},
series = {ANCS '15}
}

@inproceedings{10.1145/3338840.3355655,
author = {Lu, Yung-Feng and Chen, Hung-Ming and Kuo, Chin-Fu and Tseng, Bo-Kai and Chou, Shih-Chun},
title = {Container-Based Load Balancing for WebRTC Applications},
year = {2019},
isbn = {9781450368438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338840.3355655},
doi = {10.1145/3338840.3355655},
abstract = {Nowadays, the progress of the communication technology is fast. With the popularity of smart phones, tablets and computers, social networking sites or social software have also developed rapidly, changing the user's habit of using network communication software. The demand for streaming audio and video communication has increased dramatically, resulting in the maturity of the Internet today. At present, we can know that there are a variety of applications that can be talked on the market, such as LINE, Skype, Hangouts, etc., which can make instant calls. In the era of the Internet, the communication software has shortened the dispersion in the world. The distance between people everywhere.This research implements a web-based instant messaging architecture of WebRTC (Web Real-Time Communication, WebRTC) built on a container. We solved the concatenation problem caused by constructing WebRTC services on the container and sought to improve the performance. WebRTC can directly provide instant video and audio communication technology, and cooperate with ICE mechanism to communicate on different domains. No additional Plug-in is needed, only web browser can realize instant messaging function through web browser. It saves a lot of complicated steps, such as: install the user user, and so on. Our system also implements a load balancing mechanism that distributes traffic across the TURN Server, improving overall system performance.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {20–26},
numpages = {7},
keywords = {web application, container, cloud, load balance},
location = {Chongqing, China},
series = {RACS '19}
}

@inproceedings{10.1145/3437359.3465576,
author = {Stubbs, Joe and Marru, Suresh and Mejia, Daniel and Navarro, John-Paul and Franz, Eric and Black, Steve and Wannipurage, Dimuthu and Pamidighantam, Sudhakar and Stirm, Claire and Dahan, Maytal and Pierce, Marlon and Zentner, Michael},
title = {Common Resource Descriptions for Interoperable Gateway Cyberinfrastructure},
year = {2021},
isbn = {9781450382922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437359.3465576},
doi = {10.1145/3437359.3465576},
abstract = {Science gateway projects face challenges utilizing the vast and heterogeneous landscape of powerful cyberinfrastructure available today, and interoperability across technologies remains poor. This interoperability issue leads to myriad problems: inability to bring multiple heterogeneous specialized resources together to solve problems where different resources are optimized for different facets of the problem; inability to choose from multiple resources on-the-fly as needed based on characteristics and available capacity; and ultimately a less than optimal application of nationally-funded resources toward advancing science. This paper presents version 1.0 of the Science Gateways Community Institute (SGCI) Resource Description Specification – a schema providing a common language for describing storage and computing resources utilized by science gateway technologies – as well as an Inventory API and software development kits for incorporating resource definitions into gateway projects. We discuss multiple gateway integration design options, with trade offs regarding robustness and availability. We detail the adoption to date of the SGCI Resource Specification by several prominent projects, including Apache Airavata, HUBzero®, Open OnDemand, Tapis, and XSEDE. The XSEDE adoption is worth highlighting explicitly as it has led to a new API within the XSEDE Information Services architecture which provides SGCI resource descriptions of all active XSEDE resources. Additionally, we show how the use of the SGCI Resource Specification provides interoperability across resource providers and projects that adopt it. Finally, as a proof of concept, we present a multi-step analysis that runs Quantum ESPRESSO and visualizes the energy band structures of a Gallium Arsenide (GaAs) crystal across multiple resource providers including the Halstead cluster at Purdue University and the Stampede2 supercomputer at TACC.},
booktitle = {Practice and Experience in Advanced Research Computing},
articleno = {20},
numpages = {9},
keywords = {science gateways community institute, Cyberinfrastructure, resource description, interoperability},
location = {Boston, MA, USA},
series = {PEARC '21}
}

@inproceedings{10.5555/3330299.3330308,
author = {Magnani, Antonio and D'Angelo, Gabriele and Ferretti, Stefano and Marzolla, Moreno},
title = {Anonymity and Confidentiality in Secure Distributed Simulation},
year = {2018},
isbn = {9781538650486},
publisher = {IEEE Press},
abstract = {Research on data confidentiality, integrity and availability is gaining momentum in the ICT community, due to the intrinsically insecure nature of the Internet. While many distributed systems and services are now based on secure communication protocols to avoid eavesdropping and protect confidentiality, the techniques usually employed in distributed simulations do not consider these issues at all. This is probably due to the fact that many real-world simulators rely on monolithic, offline approaches and therefore the issues above do not apply. However, the complexity of the systems to be simulated, and the rise of distributed and cloud based simulation, now impose the adoption of secure simulation architectures. This paper presents a solution to ensure both anonymity and confidentiality in distributed simulations. A performance evaluation based on an anonymized distributed simulator is used for quantifying the performance penalty for being anonymous. The obtained results show that this is a viable solution.},
booktitle = {Proceedings of the 22nd International Symposium on Distributed Simulation and Real Time Applications},
pages = {71–78},
numpages = {8},
keywords = {secure simulation, distributed simulation, anonymity, confidentiality},
location = {Madrid, Spain},
series = {DS-RT '18}
}

@inproceedings{10.1145/3180155.3182528,
author = {Segura, Sergio and Parejo, Jos\'{e} A. and Troya, Javier and Ruiz-Cort\'{e}s, Antonio},
title = {Metamorphic Testing of RESTful Web APIs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3182528},
doi = {10.1145/3180155.3182528},
abstract = {Web Application Programming Interfaces (APIs) specify how to access services and data over the network, typically using Web services. Web APIs are rapidly proliferating as a key element to foster reusability, integration, and innovation, enabling new consumption models such as mobile or smart TV apps. Companies such as Facebook, Twitter, Google, eBay or Netflix receive billions of API calls every day from thousands of different third-party applications and devices, which constitutes more than half of their total traffic.As Web APIs are progressively becoming the cornerstone of software integration, their validation is getting more critical. In this context, the fast detection of bugs is of utmost importance to increase the quality of internal products and third-party applications. However, testing Web APIs is challenging mainly due to the difficulty to assess whether the output of an API call is correct, i.e., the oracle problem. For instance, consider the Web API of the popular music streaming service Spotify. Suppose a search for albums with the query "redhouse" returning 21 total matches: Is this output correct? Do all the albums in the result set contain the keyword? Are there any albums containing the keyword not included in the result set? Answering these questions is difficult, even with small result sets, and often infeasible when the results are counted by thousands or millions.Metamorphic testing alleviates the oracle problem by providing an alternative when the expected output of a test execution is complex or unknown. Rather than checking the output of an individual program execution, metamorphic testing checks whether multiple executions of the program under test fulfil certain necessary properties called metamorphic relations. For instance, consider the following metamorphic relation in Spotify: two searches for albums with the same query should return the same number of total results regardless of the size of pagination. Suppose that a new Spotify search is performed using the exact same query as before and increasing the maximum number of results per page from 20 (default value) to 50: This search returns 27 total albums (6 more matches than in the previous search), which reveals a bug. This is an example of a real and reproducible fault detected using the approach presented in this paper and reported to Spotify. According to Spotify developers, it was a regression fault caused by a fix with undesired side effects.In this paper [1], we present a metamorphic testing approach for the automated detection of faults in RESTful Web APIs (henceforth also referred to as simply Web APIs). We introduce the concept of metamorphic relation output patterns. A Metamorphic Relation Output Pattern (MROP) defines an abstract output relation typically identified in Web APIs, regardless of their application domain. Each MROP is defined in terms of set operations among test outputs such as equality, union, subset, or intersection. MROPs provide a helpful guide for the identification of metamorphic relations, broadening the scope of our work beyond a particular Web API. Based on the notion of MROP, a methodology is proposed for the application of the approach to any Web API following the REST architectural pattern.The approach was evaluated in several steps. First, we used the proposed methodology to identify 33 metamorphic relations in four Web APIs developed by undergraduate students. All the relations are instances of the proposed MROPs. Then, we assessed the effectiveness of the identified relations at revealing 317 automatically seeded faults (i.e., mutants) in the APIs under test. As a result, 302 seeded faults were detected, achieving a mutation score of 95.3\%. Second, we evaluated the approach using real Web APIs and faults. In particular, we identified 20 metamorphic relations in the Web API of Spotify and 40 metamorphic relations in the Web API of YouTube. Each metamorphic relation was implemented and automatically executed using both random and manual test data. In total, 469K metamorphic tests were generated. As a result, 21 metamorphic relations were violated, and 11 issues revealed and reported (3 issues in Spotify and 8 issues in YouTube). To date, 10 of the reported issues have been either confirmed by the API developers or reproduced by other users supporting the effectiveness of our approach.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {882},
numpages = {1},
keywords = {metamorphic testing, RESTful web services, web API, REST},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2652524.2652585,
author = {Giacalone, Matteo and Paci, Federica and Mammoliti, Rocco and Perugino, Rodolfo and Massacci, Fabio and Selli, Claudio},
title = {Security Triage: An Industrial Case Study on the Effectiveness of a Lean Methodology to Identify Security Requirements},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652585},
doi = {10.1145/2652524.2652585},
abstract = {Context: Poste Italiane is a large corporation offering integrated services in banking and savings, postal services, and mobile communication. Every year, it receives thousands of change requests for its ICT services. Applying to each and every request a security assessment "by the book" is simply not possible. Goal: We report the experience by Poste Italiane of a lean methodology to identify security requirements that can be inserted in the production cycle of a normal company. Method: The process is based on surveying the overall IT architectures (Security Survey) and then a lean dynamic process (Security Triage) to evaluate individual change requests, so that important changes get the attention they need, minor changes can be quickly implemented, and compliance and security obligations are met. Results: The empirical evaluation conducted for over an year at Poste Italiane shows that the process significantly reduces the time to identify security requirements at the pace of change. Conclusions: The Security Survey and Triage process should thus be embedded in a company's production cycle as mandatory step to manage change requests so that security initiatives are prioritized based on the relevance of the assets and of the business objectives of the company.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {24},
numpages = {8},
keywords = {change requests, security requirements elicitation},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.1145/2820783.2820791,
author = {Magalh\~{a}es, Regis Pires and Coutinho, Gustavo and Mac\^{e}do, Jos\'{e} and Ferreira, Camila and Cruz, L\'{\i}via and Nascimento, Mario},
title = {Graphast: An Extensible Framework for Building Applications on Time-Dependent Networks},
year = {2015},
isbn = {9781450339674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2820783.2820791},
doi = {10.1145/2820783.2820791},
abstract = {Graphast is a framework tool that allows developers to compose a number of network models, data importing/exporting services as well as query services, in order to quickly build applications on time-dependent networks. The main goal is to allow developers to implement solutions to different types of problems on time-dependent networks using spatial queries, such as nearest neighbor queries, optimal sequenced routes, etc. Graphast allows the combination of facilities provided by the framework via a public API and/or the building of new facilities, e.g., a new query processing algorithm, and incorporate those into Graphast for others to use them as well. In this paper, we discuss Graphast's architectural components and how one can create/store instances of those components in order to build an application. The steps necessary for building a real world application are also presented.},
booktitle = {Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {93},
numpages = {4},
keywords = {query services, time-dependent networks, framework},
location = {Seattle, Washington},
series = {SIGSPATIAL '15}
}

@article{10.1145/3447868,
author = {Michel, Oliver and Bifulco, Roberto and R\'{e}tv\'{a}ri, G\'{a}bor and Schmid, Stefan},
title = {The Programmable Data Plane: Abstractions, Architectures, Algorithms, and Applications},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3447868},
doi = {10.1145/3447868},
abstract = {Programmable data plane technologies enable the systematic reconfiguration of the low-level processing steps applied to network packets and are key drivers toward realizing the next generation of network services and applications. This survey presents recent trends and issues in the design and implementation of programmable network devices, focusing on prominent abstractions, architectures, algorithms, and applications proposed, debated, and realized over the past years. We elaborate on the trends that led to the emergence of this technology and highlight the most important pointers from the literature, casting different taxonomies for the field, and identifying avenues for future research.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {82},
numpages = {36},
keywords = {Programmable data planes, programmable switches, packet processing, in-network computation, network programmability}
}

@inproceedings{10.1145/3180155.3182518,
author = {Bagherzadeh, Mojtaba and Kahani, Nafiseh and Bezemer, Cor-Paul and Hassan, Ahmed E. and Dingel, Juergen and Cordy, James R.},
title = {Analyzing a Decade of Linux System Calls},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3182518},
doi = {10.1145/3180155.3182518},
abstract = {The Linux kernel provides its services to the application layer using so-called system calls. All system calls combined form the Application Programming Interface (API) of the kernel. Hence, system calls provide us with a window into the development process and design decisions that are made for the Linux kernel. Our paper [1] presents the result of an empirical study of the changes (8,770) that were made to the system calls during the last decade (i.e., from April 2005 to December 2014). The main contributions and most important findings of our study are:(1) An overview of the Linux system calls. As of December 2014, 396 system calls existed in the Linux kernel. They can be categorized into 10 groups (process management, signal processing, and so on). 76 of the system calls were added over the last decade (new system calls). A new system call is usually not activated for all architectures at the same time. 40 out of 76 (53\%) new system calls and 102 of the 393 (26\%) existing system calls were sibling calls. A sibling call is a system call that is similar in functionality, and often in name, to another system call.(2) A study of the evolution of the Linux system calls over the last decade in terms of the size and type of changes that were made to the system calls. With an average growth of 25 LOC per day, the Linux system calls are relatively stable. The commits that are made to system calls are slightly more scattered than kernel commits. There exists a small group of very active system call developers. 8,288 of the 8,770 studied commits (95\%) were made to maintain, improve and fix bugs in system calls. 36\% of the system call-related commits were bug fixes. 4,498 (50\%) of the commits were made to only 25 (6\%) of the 393 system calls. 35\% of the system call-related commits were made to conduct code restructuring, and 36\% of the system call-related commits were made to fix bugs.(3) A study of the type of bug fixes that were made to the system calls over the last decade. Developers make mistakes in the seemingly trivial activation process of a system call. The steps that are required to activate a system call, such as assigning the unique number and updating the system call table, are performed manually. 58\% of the bug fix commits were made to fix semantic bugs. The proportion of bug fixes that fixed memory-related bugs remained constant throughout the last decade.(4) An analysis of the results and a discussion of their implications.Generalizability of results. We compared Linux system calls with FreeBSD system calls, to validate that our results are generalizable to other UNIX-based operating systems. Our findings for the FreeBSD operating system confirm that other UNIX-based operating systems use a system call mechanism that is similar to that of Linux. Therefore, we can safely assume that our findings are of value to other UNIX-based operating systems.Suggestion for automation. First, we suggest the automation of simple, reoccurring tasks in Linux, such as adding and removing system calls. Our study on FreeBSD shows that such tasks can successfully be automated. Second, we suggest that historical information about the evolution of a kernel API should be used to guide the testing process. Finally, we suggest that existing automated testing tools are extended to support testing system calls.Maintenance Effort. Compared to regular software systems, kernel APIs require an additional type of maintenance that involves adding and removing system calls. Also, approximately 11\% of the maintenance effort of a kernel API is assigned to the infrastructure for providing the API.Overall, the results of our study can be beneficial to practitioners, researchers, and more specifically kernel developers, by providing insights related to the challenges and problems that come with long term maintenance of a kernel API, such as the long-lived Linux kernel API. We have published our classification of 8,870 system call-related changes [1] so that it can be used to conduct further studies.The full paper is accepted for publication in the Empirical Software Engineering journal, and can be found at: https://link.springer.com/article/10.1007/s10664-017-9551-z.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {267},
numpages = {1},
keywords = {software evolution, system calls, linux kernel, API evolution},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3125486.3125492,
author = {Oramas, Sergio and Nieto, Oriol and Sordo, Mohamed and Serra, Xavier},
title = {A Deep Multimodal Approach for Cold-Start Music Recommendation},
year = {2017},
isbn = {9781450353533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3125486.3125492},
doi = {10.1145/3125486.3125492},
abstract = {An increasing amount of digital music is being published daily. Music streaming services often ingest all available music, but this poses a challenge: how to recommend new artists for which prior knowledge is scarce? In this work we aim to address this so-called cold-start problem by combining text and audio information with user feedback data using deep network architectures. Our method is divided into three steps. First, artist embeddings are learned from biographies by combining semantics, text features, and aggregated usage data. Second, track embeddings are learned from the audio signal and available feedback data. Finally, artist and track embeddings are combined in a multimodal network. Results suggest that both splitting the recommendation problem between feature levels (i.e., artist metadata and audio track), and merging feature embeddings in a multimodal approach improve the accuracy of the recommendations.},
booktitle = {Proceedings of the 2nd Workshop on Deep Learning for Recommender Systems},
pages = {32–37},
numpages = {6},
keywords = {semantics, multimodal, recommender systems, deep learning, music},
location = {Como, Italy},
series = {DLRS 2017}
}

@inproceedings{10.1145/3549823.3549824,
author = {Chen, Hui and Ye, Xingmao and Zhang, Libo and Bu, Qian},
title = {The Construction of Government Information Open Platform: Research and Practice of the Ministry of Natural Resources},
year = {2023},
isbn = {9781450397919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549823.3549824},
doi = {10.1145/3549823.3549824},
abstract = {The rapid development of information technology has provided strong technical support for China to promote the construction of service-oriented government. The government information open platform(GIOP) uses information technology to enable the public to obtain government information in the most convenient way and make government information better serve the public, which is an important measure to promote the construction of service-oriented and transparent government. The Ministry of Natural Resources (MNR), which was established in 2018, was studied to analyse the significance and construction requirements of the GIOP. The content system, functions, system architecture, key technical methods and achievements are described. Real-life cases are provided to illustrate the series of technological means and mechanism support for breaking the barriers between platforms, thereby promoting the innovation of government information services.},
booktitle = {Proceedings of the 9th International Conference on Management of E-Commerce and e-Government},
pages = {1–6},
numpages = {6},
keywords = {Ministry of Natural Resources, government information, open platform, policy document},
location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {ICMECG '22}
}

@inproceedings{10.1145/3444757.3485108,
author = {Morais, Gabriel and Bork, Dominik and Adda, Mehdi},
title = {Towards an Ontology-Driven Approach to Model and Analyze Microservices Architectures},
year = {2021},
isbn = {9781450383141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444757.3485108},
doi = {10.1145/3444757.3485108},
abstract = {Microservices Architectures (MSAs) are continuously replacing monolithic systems toward achieving more flexible and maintainable service-oriented software systems. However, the shift toward an MSA also requires a technological and managerial shift for its adopters. Architecting and managing MSAs represent unique challenges, including microservices' identification, interoperability, and reuse. To handle these challenges, we propose an Ontology-driven Conceptual Modelling approach, based on the Ontology of Microservices Architecture Concepts (OMSAC), for modelling and analyzing microservices-based systems. We show, how OMSAC-based conceptual models, stocked in a Stardog triple store, support Stakeholder-specific communication, documentation, and reuse. This paper reports on the application of our approach in three open-source MSA systems with a focus on microservices' discovery based on similarity metrics. Eventually, we compare the extracted similarity metrics derived from the application of machine learning techniques to the OMSAC models with a manual analysis performed by experts.},
booktitle = {Proceedings of the 13th International Conference on Management of Digital EcoSystems},
pages = {79–86},
numpages = {8},
keywords = {machine learning, Microservices, Stardog, ontology, OMSAC},
location = {Virtual Event, Tunisia},
series = {MEDES '21}
}

@article{10.14778/3583140.3583156,
author = {van Renen, Alexander and Leis, Viktor},
title = {Cloud Analytics Benchmark},
year = {2023},
issue_date = {February 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3583140.3583156},
doi = {10.14778/3583140.3583156},
abstract = {The cloud facilitates the transition to a service-oriented perspective. This affects cloud-native data management in general, and data analytics in particular. Instead of managing a multi-node database cluster on-premise, end users simply send queries to a managed cloud data warehouse and receive results. While this is obviously very attractive for end users, database system architects still have to engineer systems for this new service model. There are currently many competing architectures ranging from self-hosted (Presto, PostgreSQL), over managed (Snowflake, Amazon Redshift) to query-as-a-service (Amazon Athena, Google BigQuery) offerings. Benchmarking these architectural approaches is currently difficult, and it is not even clear what the metrics for a comparison should be.To overcome these challenges, we first analyze a real-world query trace from Snowflake and compare its properties to that of TPC-H and TPC-DS. Doing so, we identify important differences that distinguish traditional benchmarks from real-world cloud data warehouse workloads. Based on this analysis, we propose the Cloud Analytics Benchmark (CAB). By incorporating workload fluctuations and multi-tenancy, CAB allows evaluating different designs in terms of user-centered metrics such as cost and performance.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {1413–1425},
numpages = {13}
}

@inproceedings{10.1145/3543712.3543718,
author = {R\"{o}sch, Tobias and Sommer, Martin and Sax, Eric},
title = {Adaptive Application Development and Integration Process for Modern Automotive Software},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543718},
doi = {10.1145/3543712.3543718},
abstract = {Due to fast progress in information technologies and long lifecycles of vehicles, there are ever-increasing expectations in modern automotive software development regarding the flexibility to integrate updates and new functions quickly into already existing systems. This paper proposes a process, that is especially suitable for the development of new functions in higher programming languages and the usage of machine learning models. When developed in a tool like MATLAB, code generators can be used to integrate the function step-by-step into a service-oriented automotive E/E-architecture. It is based on a classic V-model process and uses integration steps according to the XiL approach. The key aspect is the frontloading of verification and validation into the steps as early as possible to keep iteration cycles fast. The proposed process is applied to the development of a Neural Network Model Predictive Control (NNMPC) for a Heating, Ventilation and Air-Conditioning (HVAC) unit of a city bus. The resulting NNMPC is then integrated into a system based on the AUTOSAR adaptive platform. That allowed the function to be developed and integrated quickly and seems to be a promising approach to bring new functions into already existing automotive E/E-architectures.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {85–90},
numpages = {6},
keywords = {software development process, AUTOSAR Adaptive, automotive software, service-oriented architecture},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@inproceedings{10.1145/3540250.3558951,
author = {Peng, Xin and Zhang, Chenxi and Zhao, Zhongyuan and Isami, Akasaka and Guo, Xiaofeng and Cui, Yunna},
title = {Trace Analysis Based Microservice Architecture Measurement},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558951},
doi = {10.1145/3540250.3558951},
abstract = {Microservice architecture design highly relies on expert experience and may often result in improper service decomposition. Moreover, a microservice architecture is likely to degrade with the continuous evolution of services. Architecture measurement is thus important for the long-term evolution of microservice architectures. Due to the independent and dynamic nature of services, source code analysis based approaches cannot well capture the interactions between services. In this paper, we propose a trace analysis based microservice architecture measurement approach. We define a trace data model for microservice architecture measurement, which enables fine-grained analysis of the execution processes of requests and the interactions between interfaces and services. Based on the data model, we define 14 architectural metrics to measure the service independence and invocation chain complexity of a microservice system. We implement the approach and conduct three case studies with a student course project, an open-source microservice benchmark system, and three industrial microservice systems. The results show that our approach can well characterize the independence and invocation chain complexity of microservice architectures and help developers to identify microservice architecture issues caused by improper service decomposition and architecture degradation.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1589–1599},
numpages = {11},
keywords = {Tracing, Architecture, Dynamic analysis, Microservice},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3483899.3483908,
author = {Santos, Ana and Paula, Hugo},
title = {Microservice Decomposition and Evaluation Using Dependency Graph and Silhouette Coefficient},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483908},
doi = {10.1145/3483899.3483908},
abstract = {The benefits provided by microservices architecture in some application scenarios are a motivating factor for organizations to migrate their monoliths to this architecture. Extracting microservices from existing monolithic code bases presents a key challenge in this context, and there is a lack of tools that automate not only the decomposition processes but also the evaluation of the resulting architecture. This work presents a new approach for microservice decomposition that analyzes source code of a monolithic application and, with the combined use of approaches in the literature, suggests parts to be extracted in microservices considering the artifacts: classes, methods and/or history of modifications. The quality of the microservices’ suggestions are assessed, quantitatively, through the silhouette coefficient, a quality metric used in clustering analysis, and the microservice granularity. A tool was developed to automate the process of microservice decomposition for Java repositories. As a result, it was observed that the tool generated clusters with satisfactory results and can be used as an auxiliary instrument by experts during the migration process from monolithic architecture to microservices.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {51–60},
numpages = {10},
keywords = {monolithic application, microservices, decomposition},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/3543507.3583338,
author = {Jiang, Xinrui and Pan, Yicheng and Ma, Meng and Wang, Ping},
title = {Look Deep into the Microservice System Anomaly through Very Sparse Logs},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583338},
doi = {10.1145/3543507.3583338},
abstract = {Intensive monitoring and anomaly diagnosis have become a knotty problem for modern microservice architecture due to the dynamics of service dependency. While most previous studies rely heavily on ample monitoring metrics, we raise a fundamental but always neglected issue - the diagnostic metric integrity problem. This paper solves the problem by proposing MicroCU – a novel approach to diagnose microservice systems using very sparse API logs. We design a structure named dynamic causal curves to portray time-varying service dependencies and a temporal dynamics discovery algorithm based on Granger causal intervals. Our algorithm generates a smoother space of causal curves and designs the concept of causal unimodalization to calibrate the causality infidelities brought by missing metrics. Finally, a path search algorithm on dynamic causality graphs is proposed to pinpoint the root cause. Experiments on commercial system cases show that MicroCU outperforms many state-of-the-art approaches and reflects the superiorities of causal unimodalization to raw metric imputation.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2970–2978},
numpages = {9},
keywords = {Dynamic Granger causality, Anomaly diagnosis, Root cause analysis, Microservice architecture},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3542929.3563477,
author = {Luo, Shutian and Xu, Huanle and Ye, Kejiang and Xu, Guoyao and Zhang, Liping and Yang, Guodong and Xu, Chengzhong},
title = {The Power of Prediction: Microservice Auto Scaling via Workload Learning},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3542929.3563477},
doi = {10.1145/3542929.3563477},
abstract = {When deploying microservices in production clusters, it is critical to automatically scale containers to improve cluster utilization and ensure service level agreements (SLA). Although reactive scaling approaches work well for monolithic architectures, they are not necessarily suitable for microservice frameworks due to the long delay caused by complex microservice call chains. In contrast, existing proactive approaches leverage end-to-end performance prediction for scaling, but cannot effectively handle microservice multiplexing and dynamic microservice dependencies.In this paper, we present Madu, a proactive microservice auto-scaler that scales containers based on predictions for individual microservices. Madu learns workload uncertainty to handle the highly dynamic dependency between microservices. Additionally, Madu adopts OS-level metrics to optimize resource usage while maintaining good control over scaling overhead. Experiments on large-scale deployments of microservices in Alibaba clusters show that the overall prediction accuracy of Madu can reach as high as 92.3\% on average, which is 13\% higher than the state-of-the-art approaches. Furthermore, experiments running real-world microservice benchmarks in a local cluster of 20 servers show that Madu can reduce the overall resource usage by 1.7X compared to reactive solutions, while reducing end-to-end service latency by 50\%.},
booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
pages = {355–369},
numpages = {15},
keywords = {microservices, proactive auto-scaler, workload uncertainty learning},
location = {San Francisco, California},
series = {SoCC '22}
}

@inproceedings{10.5555/3586210.3586383,
author = {Pearce, Glen and Pflaum, Alexis and Balasoiu, Dumitru Alin and Szabo, Claudia},
title = {Jeopardy Assessment for Dynamic Configuration of Collaborative Microservice Architectures},
year = {2023},
publisher = {IEEE Press},
abstract = {Microservice architectures, which are lightweight, flexible, and adapt easily to changes, have recently been considered for system development in military operations in contested and dynamic environments. However, in a military setting, the dynamic configuration of collaborative microservices execution becomes critical, and testing that microservice configurations behave as expected becomes paramount. In this paper, we propose a complex jeopardy metric and reconfiguration process that dynamically configures collaborative algorithms running on multiple nodes. Our metric and proposed scenarios will allow for the automated evaluation of microservice configurations and their re-configuration to suit operational needs. We evaluate our proposed scenario, metric, and various reconfiguration algorithms to show the benefits of this approach.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2070–2081},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3575693.3575710,
author = {Switzer, Jennifer and Marcano, Gabriel and Kastner, Ryan and Pannuto, Pat},
title = {Junkyard Computing: Repurposing Discarded Smartphones to Minimize Carbon},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575710},
doi = {10.1145/3575693.3575710},
abstract = {1.5 billion smartphones are sold annually, and most are decommissioned less than two years later. Most of these unwanted smartphones are neither discarded nor recycled but languish in junk drawers and storage units. This computational stockpile represents a substantial wasted potential: modern smartphones have increasingly high-performance and energy-efficient processors, extensive networking capabilities, and a reliable built-in power supply. This project studies the ability to reuse smartphones as "junkyard computers." Junkyard computers grow global computing capacity by extending device lifetimes, which supplants the manufacture of new devices. We show that the capabilities of even decade-old smartphones are within those demanded by modern cloud microservices and discuss how to combine phones to perform increasingly complex tasks. We describe how current operation-focused metrics do not capture the actual carbon costs of compute. We propose Computational Carbon Intensity---a performance metric that balances the continued service of older devices with the superlinear runtime improvements of newer machines. We use this metric to redefine device service lifetime in terms of carbon efficiency. We develop a cloudlet of reused Pixel 3A phones. We analyze the carbon benefits of deploying large, end-to-end microservice-based applications on these smartphones. Finally, we describe system architectures and associated challenges to scale to cloudlets with hundreds and thousands of smartphones.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {400–412},
numpages = {13},
keywords = {life cycle assessment, sustainability, cloud computing},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3578245.3585030,
author = {Belkhiri, Adel and Shahnejat Bushehri, Ahmad and Gohring de Magalhaes, Felipe and Nicolescu, Gabriela},
title = {Transparent Trace Annotation for Performance Debugging in Microservice-Oriented Systems (Work In Progress Paper)},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3585030},
doi = {10.1145/3578245.3585030},
abstract = {Microservices is a cloud-native architecture in which a single application is implemented as a collection of small, independent, and loosely-coupled services. This architecture is gaining popularity in the industry as it promises to make applications more scalable and easier to develop and deploy. Nonetheless, adopting this architecture in practice has raised many concerns, particularly regarding the difficulty of diagnosing performance bugs and explaining abnormal software behaviour. Fortunately, many tools based on distributed tracing were proposed to achieve observability in microservice-oriented systems and address these concerns (e.g., Jaeger). Distributed tracing is a method for tracking user requests as they flow between services. While these tools can identify slow services and detect latency-related problems, they mostly fail to pinpoint the root causes of these issues.This paper presents a new approach for enacting cross-layer tracing of microservice-based applications. It also proposes a framework for annotating traces generated by most distributed tracing tools with relevant tracing data and metrics collected from the kernel. The information added to the traces aims at helping the practitioner get a clear insight into the operations of the application executing user requests. The framework we present is notably efficient in diagnosing the causes of long tail latencies. Unlike other solutions, our approach for annotating traces is completely transparent as it does not require the modification of the application, the tracer, or the operating system. Furthermore, our evaluation shows that this approach incurs low overhead costs.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {25–32},
numpages = {8},
keywords = {microservices, performance analysis, distributed systems, software tracing},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3493649.3493656,
author = {Allen, Sadie and Toslali, Mert and Parthasarathy, Srinivasan and Oliveira, Fabio and Coskun, Ayse K.},
title = {Tritium: A Cross-Layer Analytics System for Enhancing Microservice Rollouts in the Cloud},
year = {2021},
isbn = {9781450391719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493649.3493656},
doi = {10.1145/3493649.3493656},
abstract = {Microservice architectures are widely used in cloud-native applications as their modularity allows for independent development and deployment of components. With the many complex interactions occurring in between components, it is difficult to determine the effects of a particular microservice rollout. Site Reliability Engineers must be able to determine with confidence whether a new rollout is at fault for a concurrent or subsequent performance problem in the system so they can quickly mitigate the issue. We present Tritium, a cross-layer analytics system that synthesizes several types of data to suggest possible causes for Service Level Objective (SLO) violations in microservice applications. It uses event data to identify new version rollouts, tracing data to build a topology graph for the cluster and determine services potentially affected by the rollout, and causal impact analysis applied to metric time-series to determine if the rollout is at fault. Tritium works based on the principle that if a rollout is not responsible for a change in an upstream or neighboring SLO metric, then the rollout's telemetry data will do a poor job predicting the behavior of that SLO metric. In this paper, we experimentally demonstrate that Tritium can accurately attribute SLO violations to downstream rollouts and outline the steps necessary to fully realize Tritium.},
booktitle = {Proceedings of the Seventh International Workshop on Container Technologies and Container Clouds},
pages = {19–24},
numpages = {6},
keywords = {version rollouts, container systems, microservices, Fault diagnosis},
location = {Virtual Event, Canada},
series = {WoC '21}
}

@article{10.1145/3532183,
author = {Zdun, Uwe and Queval, Pierre-Jean and Simhandl, Georg and Scandariato, Riccardo and Chakravarty, Somik and Jelic, Marjan and Jovanovic, Aleksandar},
title = {Microservice Security Metrics for Secure Communication, Identity Management, and Observability},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3532183},
doi = {10.1145/3532183},
abstract = {Microservice architectures are increasingly being used to develop application systems. Despite many guidelines and best practices being published, architecting microservice systems for security is challenging. Reasons are the size and complexity of microservice systems, their polyglot nature, and the demand for the continuous evolution of these systems. In this context, to manually validate that security architecture tactics are employed as intended throughout the system is a time-consuming and error-prone task. In this article, we present an approach to avoid such manual validation before each continuous evolution step in a microservice system, which we demonstrate using three widely used categories of security tactics: secure communication, identity management, and observability. Our approach is based on a review of existing security guidelines, the gray literature, and the scientific literature, from which we derived Architectural Design Decisions (ADDs) with the found security tactics as decision options. In our approach, we propose novel detectors to detect these decision options automatically and formally defined metrics to measure the conformance of a system to the different options of the ADDs. We apply the approach to a case study data set of 10 open source microservice systems, plus another 20 variants of these systems, for which we manually inspected the source code for security tactics. We demonstrate and assess the validity and appropriateness of our metrics by performing an assessment of their conformance to the ADDs in our systems’ dataset through statistical methods.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {16},
numpages = {34},
keywords = {microservice security, software architecture metrics, software architecture detectors, Microservice architecture}
}

@inproceedings{10.1145/3472883.3486999,
author = {Baarzi, Ataollah Fatahi and Kesidis, George},
title = {SHOWAR: Right-Sizing And Efficient Scheduling of Microservices},
year = {2021},
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472883.3486999},
doi = {10.1145/3472883.3486999},
abstract = {Microservices architecture have been widely adopted in designing distributed cloud applications where the application is decoupled into multiple small components (i.e. "microservices"). One of the challenges in deploying microservices is finding the optimal amount of resources (i.e. size) and the number of instances (i.e. replicas) for each microservice in order to maintain a good performance as well as prevent resource wastage and under-utilization which is not cost-effective. This paper presents SHOWAR, a framework that configures the resources by determining the number of replicas (horizontal scaling) and the amount of CPU and Memory for each microservice (vertical scaling). For vertical scaling, SHOWAR uses empirical variance in the historical resource usage to find the optimal size and mitigate resource wastage. For horizontal scaling, SHOWAR uses basic ideas from control theory along with kernel level performance metrics. Additionally, once the size for each microservice is found, SHOWAR bridges the gap between optimal resource allocation and scheduling by generating affinity rules (i.e. hints) for the scheduler to further improve the performance. Our experiments, using a variety of microservice applications and real-world workloads, show that, compared to the state-of-the-art autoscaling and scheduling systems, SHOWAR on average improves the resource allocation by up to 22\% while improving the 99th percentile end-to-end user request latency by 20\%.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {427–441},
numpages = {15},
keywords = {autoscaling, cloud computing, microservices},
location = {Seattle, WA, USA},
series = {SoCC '21}
}

@inproceedings{10.5555/3511065.3511076,
author = {Yoder, Joseph W. and Merson, Paulo},
title = {Strangler Patterns},
year = {2022},
isbn = {9781941652169},
publisher = {The Hillside Group},
address = {USA},
abstract = {Martin Fowler coined the term "Strangler Application" as a metaphor to describe a way of doing an evolutionary rewrite of a system, keeping it working while you evolve it. The main idea is to gradually create a new system around the edges of the old, letting it grow slowly over several years until the old system is strangled. The microservices architecture style has become very popular, and has been used to apply the strangler application to monolithic service-based systems. This paper describes different strategies (patterns) for applying the strangler application while evolving a monolith to use the microservices architecture style. The main ideas are: Wrap the monolith and protect services and system from change, Start Small and gradually evolve the system (baby steps), Pave the Road making microservices easier to create; Macroservice first then split to Microservice, Add new functionality as microservices, Extract Module / Component to Microservice, and Replace functionality with Microservice. As the system evolve it is common to Proxy Monolith Components and Add Fa\c{c}ade to the microservices},
booktitle = {Proceedings of the 27th Conference on Pattern Languages of Programs},
articleno = {8},
numpages = {25},
keywords = {pattern sequences, patterns, architecture, evolutionary architecture, continuous integration, DevOps, strangler, software development, sustainable delivery, microservices, monolith, pattern scenarios},
location = {Virtual Event},
series = {PLoP '20}
}

@inproceedings{10.1145/3559712.3559716,
author = {Moreira, Mateus Gabi and De Fran\c{c}a, Breno Bernard Nicolau},
title = {Analysis of Microservice Evolution Using Cohesion Metrics},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559712.3559716},
doi = {10.1145/3559712.3559716},
abstract = {The adoption of Microservices Architecture (MSA) has increased in recent years due to several claimed benefits, such as reducing deployment complexity, supporting technology diversity, and better scalability. However, MSA is not free from maintainability issues, especially the lack of cohesion, in which microservices possibly concentrate or miss responsibilities. Also, the lack of empirically-validated cohesion metrics for MSA makes the quantitative assessment even more challenging. In this paper, we empirically explore the practical applicability of service-level cohesion metrics in an open-source MSA application context. The qualitative results show the possibility of assessing MSA cohesion using these service-level metrics, the feasibility of tracking software evolution, and an indication of possible technical debts along the way.},
booktitle = {Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {40–49},
numpages = {10},
keywords = {Software evolution, Software architecture, Microservices, Cohesion Metrics},
location = {<conf-loc>, <city>Uberlandia</city>, <country>Brazil</country>, </conf-loc>},
series = {SBCARS '22}
}

@inproceedings{10.1145/3543507.3583274,
author = {Chakraborty, Sarthak and Garg, Shaddy and Agarwal, Shubham and Chauhan, Ayush and Saini, Shiv Kumar},
title = {CausIL: Causal Graph for Instance Level Microservice Data},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583274},
doi = {10.1145/3543507.3583274},
abstract = {AI-based monitoring has become crucial for cloud-based services due to its scale. A common approach to AI-based monitoring is to detect causal relationships among service components and build a causal graph. Availability of domain information makes cloud systems even better suited for such causal detection approaches. In modern cloud systems, however, auto-scalers dynamically change the number of microservice instances, and a load-balancer manages the load on each instance. This poses a challenge for off-the-shelf causal structure detection techniques as they neither incorporate the system architectural domain information nor provide a way to model distributed compute across varying numbers of service instances. To address this, we develop CausIL, which detects a causal structure among service metrics by considering compute distributed across dynamic instances and incorporating domain knowledge derived from system architecture. Towards the application in cloud systems, CausIL estimates a causal graph using instance-specific variations in performance metrics, modeling multiple instances of a service as independent, conditional on system assumptions. Simulation study shows the efficacy of CausIL over baselines by improving graph estimation accuracy by ∼ 25\% as measured by Structural Hamming Distance whereas the real-world dataset demonstrates CausIL’s applicability in deployment settings.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2905–2915},
numpages = {11},
keywords = {Microservices, Causal Graph, Causal Structure Detection, System Monitoring},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3530019.3530040,
author = {Sellami, Khaled and Saied, Mohamed Aymen and Ouni, Ali},
title = {A Hierarchical DBSCAN Method for Extracting Microservices from Monolithic Applications},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3530040},
doi = {10.1145/3530019.3530040},
abstract = {The microservices architectural style offers many advantages such as scalability, reusability and ease of maintainability. As such microservices has become a common architectural choice when developing new applications. Hence, to benefit from these advantages, monolithic applications need to be redesigned in order to migrate to a microservice based architecture. Due to the inherent complexity and high costs related to this process, it is crucial to automate this task. In this paper, we propose a method that can identify potential microservices from a given monolithic application. Our method takes as input the source code of the source application in order to measure the similarities and dependencies between all of the classes in the system using their interactions and the domain terminology employed within the code. These similarity values are then used with a variant of a density-based clustering algorithm to generate a hierarchical structure of the recommended microservices while identifying potential outlier classes. We provide an empirical evaluation of our approach through different experimental settings including a comparison with existing human-designed microservices and a comparison with 5 baselines. The results show that our method succeeds in generating microservices that are overall more cohesive and that have fewer interactions in-between them with up to 0.9 of precision score when compared to human-designed microservices.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {201–210},
numpages = {10},
keywords = {Legacy decomposition, Static Analysis, Clustering, Microservices},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3472883.3487003,
author = {Luo, Shutian and Xu, Huanle and Lu, Chengzhi and Ye, Kejiang and Xu, Guoyao and Zhang, Liping and Ding, Yu and He, Jian and Xu, Chengzhong},
title = {Characterizing Microservice Dependency and Performance: Alibaba Trace Analysis},
year = {2021},
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472883.3487003},
doi = {10.1145/3472883.3487003},
abstract = {Loosely-coupled and light-weight microservices running in containers are replacing monolithic applications gradually. Understanding the characteristics of microservices is critical to make good use of microservice architectures. However, there is no comprehensive study about microservice and its related systems in production environments so far. In this paper, we present a solid analysis of large-scale deployments of microservices at Alibaba clusters. Our study focuses on the characterization of microservice dependency as well as its runtime performance. We conduct an in-depth anatomy of microservice call graphs to quantify the difference between them and traditional DAGs of data-parallel jobs. In particular, we observe that microservice call graphs are heavy-tail distributed and their topology is similar to a tree and moreover, many microservices are hot-spots. We reveal three types of meaningful call dependency that can be utilized to optimize microservice designs. Our investigation on microservice runtime performance indicates most microservices are much more sensitive to CPU interference than memory interference. To synthesize more representative microservice traces, we build a mathematical model to simulate call graphs. Experimental results demonstrate our model can well preserve those graph properties observed from Alibaba traces.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {412–426},
numpages = {15},
location = {Seattle, WA, USA},
series = {SoCC '21}
}

@inproceedings{10.1145/3569902.3569916,
author = {Castro, Jessica and Laranjeiro, Nuno and Vieira, Marco},
title = {Detecting DoS Attacks in Microservice Applications: Approach&nbsp;and Case Study},
year = {2023},
isbn = {9781450397377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569902.3569916},
doi = {10.1145/3569902.3569916},
abstract = {A microservices-based architecture decreases the complexity of developing new systems, making them highly scalable and manageable. However, its distributed nature, the high granularity of services, and the large attack surface increase the need to secure those systems at runtime. This paper investigates the challenges of detecting low- and high-volume DoS attacks against microservices using application-level metrics. We conducted an exploratory study to evaluate how different services influence attack detection, the use of Machine Learning (ML) techniques to detect DoS attacks, and the application-level metrics that can be used to detect DoS attacks. The results show that, analysing the services in parallel improves the detection rate, ML models are promising in detecting DoS attacks, and the numbers of sockets and threads used by containers are valuable metrics to indicate high-volume DoS attacks.},
booktitle = {Proceedings of the 11th Latin-American Symposium on Dependable Computing},
pages = {73–78},
numpages = {6},
keywords = {attack detection, security, denial of service, microservices, machine learning, container},
location = {<conf-loc>, <city>Fortaleza/CE</city>, <country>Brazil</country>, </conf-loc>},
series = {LADC '22}
}

@inproceedings{10.1145/3531056.3542765,
author = {Peng, Xin},
title = {Large-Scale Trace Analysis for Microservice Anomaly Detection and Root Cause Localization},
year = {2022},
isbn = {9781450396639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531056.3542765},
doi = {10.1145/3531056.3542765},
abstract = {Distributed tracing traces requests as they flow between services. It has been widely accepted and practiced in industry as an important means to achieve observability in microservice architecture for various purposes such as anomaly detection and root cause localization. However, trace analysis in an industrial microservice system is often challenging due to the huge number of traces produced by the system and the difficulties in combining traces with other types of operation data such as logs and metrics. In this talk, I will first analyze the background and describe the industrial practice of distributed tracing and trace analysis. Then I will introduce our explorations on large-scale trace analysis for microservice anomaly detection and root cause localization.},
booktitle = {Proceedings of the Federated Africa and Middle East Conference on Software Engineering},
pages = {93–94},
numpages = {2},
keywords = {Log, Metrics, Root Cause Analysis, Trace, Anomaly Detection, Microservice Architecture, Observability},
location = {Cairo-Kampala, Egypt},
series = {FAMECSE '22}
}

@inproceedings{10.1145/3570748.3570756,
author = {Lee, Chunghan and Yoshitani, Reina and Hirotsu, Toshio},
title = {Enhancing Packet Tracing of Microservices in Container Overlay Networks Using EBPF},
year = {2022},
isbn = {9781450399814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570748.3570756},
doi = {10.1145/3570748.3570756},
abstract = {The microservices architecture has been rapidly adopted to latency-sensitive applications. The architecture of these applications and the container overlay networks on servers are also complex. Distributed tracing is widely adopted in microservice applications; it suffers from the drawback of only focusing on service discovery and latency-based monitoring at an application layer, making it still challenging to monitor container overlay networks using distributed tracing. In this paper, we present an extended Berkeley Packet Filter (eBPF)-based packet tracing method using distributed tracing for latency measurement in the container overlay network. To efficiently detect the trace context on a HTTP payload, we moved the trace context position just behind the HTTP request line using sidecar proxy. Our tracing method gathered the HTTP packets that had the trace context and measured the latency using eBPF. Our evaluation was conducted using open-source benchmarks on Kubernetes; the results showed that the proposed tracing header format reduced the HTTP payload search space by up to approximately 80, and there was no significant change in end-to-end latency. Moreover, our eBPF-based tracing method presented similar latency characteristics on the overlay network in comparison with the characteristics of the packet-level traces obtained under tcpdump.},
booktitle = {Proceedings of the 17th Asian Internet Engineering Conference},
pages = {53–61},
numpages = {9},
keywords = {Sidecar, ServiceMesh, Kubernetes, Latency, Distributed tracing, Microservices, eBPF},
location = {<conf-loc>, <city>Hiroshima</city>, <country>Japan</country>, </conf-loc>},
series = {AINTEC '22}
}

@inproceedings{10.1145/3611643.3613861,
author = {Xie, Zhe and Pei, Changhua and Li, Wanxue and Jiang, Huai and Su, Liangfei and Li, Jianhui and Xie, Gaogang and Pei, Dan},
title = {From Point-Wise to Group-Wise: A Fast and Accurate Microservice Trace Anomaly Detection Approach},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613861},
doi = {10.1145/3611643.3613861},
abstract = {As Internet applications continue to scale up, microservice architecture has become increasingly popular due to its flexibility and logical structure. Anomaly detection in traces that record inter-microservice invocations is essential for diagnosing system failures. Deep learning-based approaches allow for accurate modeling of structural features (i.e., call paths) and latency features (i.e., call response time), which can determine the anomaly of a particular trace sample. However, the point-wise manner employed by these methods results in substantial system detection overhead and impracticality, given the massive volume of traces (billion-level). Furthermore, the point-wise approach lacks high-level information, as identical sub-structures across multiple traces may be encoded differently. In this paper, we introduce the first Group-wise Trace anomaly detection algorithm, named GTrace. This method categorizes the traces into distinct groups based on their shared sub-structure, such as the entire tree or sub-tree structure. A group-wise Variational AutoEncoder (VAE) is then employed to obtain structural representations. Moreover, the innovative "predicting latency with structure" learning paradigm facilitates the association between the grouped structure and the latency distribution within each group. With the group-wise design, representation caching, and batched inference strategies can be implemented, which significantly reduces the burden of detection on the system. Our comprehensive evaluation reveals that GTrace outperforms state-of-the-art methods in both performances (2.64\% to 195.45\% improvement in AUC metrics and 2.31\% to 40.92\% improvement in best F-Score) and efficiency (21.9x to 28.2x speedup). We have deployed and assessed the proposed algorithm on eBay's microservices cluster, and our code is available at https://github.com/NetManAIOps/GTrace.git.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1739–1749},
numpages = {11},
keywords = {microservice trace, variational autoencoder, anomaly detection},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3580305.3599934,
author = {Wang, Lu and Zhang, Chaoyun and Ding, Ruomeng and Xu, Yong and Chen, Qihang and Zou, Wentao and Chen, Qingjun and Zhang, Meng and Gao, Xuedong and Fan, Hao and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Root Cause Analysis for Microservice Systems via Hierarchical Reinforcement Learning from Human Feedback},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599934},
doi = {10.1145/3580305.3599934},
abstract = {In microservice systems, the identification of root causes of anomalies is imperative for service reliability and business impact. This process is typically divided into two phases: (i)constructing a service dependency graph that outlines the sequence and structure of system components that are invoked, and (ii) localizing the root cause components using the graph, traces, logs, and Key Performance Indicators (KPIs) such as latency. However, both phases are not straightforward due to the highly dynamic and complex nature of the system, particularly in large-scale commercial architectures like Microsoft Exchange.In this paper, we propose a new framework that employs Hierarchical Reinforcement Learning from Human Feedback (HRLHF) to address these challenges. Our framework leverages the static topology of the microservice system and efficiently employs the feedback of engineers to reduce uncertainty in the discovery of the service dependency graph. The framework utilizes reinforcement learning to reduce the number of queries required from O(N2) to O(1), enabling the construction of the dependency graph with high accuracy and minimal human effort. Additionally, we extend the discovered dependency graphs to window causal graphs that capture the characteristics of time series over a specified time period, resulting in improved root cause analysis accuracy and robustness. Evaluations on both real datasets from Microsoft Exchange and synthetic datasets with injected anomalies demonstrate superior performance on various metrics compared to state-of-the-art methods. It is worth mentioning that, our framework has been integrated as a crucial component in Microsoft M365 Exchange service.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5116–5125},
numpages = {10},
keywords = {root cause analysis, causal discovery, reinforcement learning from human feedback},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@inproceedings{10.1145/3583780.3615195,
author = {Gong, Shengbo and Zhou, Jiajun and Xie, Chenxuan and Xuan, Qi},
title = {Neighborhood Homophily-Based Graph Convolutional Network},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615195},
doi = {10.1145/3583780.3615195},
abstract = {Graph neural networks (GNNs) have been proved powerful in graph-oriented tasks. However, many real-world graphs are heterophilous, challenging the homophily assumption of classical GNNs. To solve the universality problem, many studies deepen networks or concatenate intermediate representations, which does not inherently change neighbor aggregation and introduces noise. Recent studies propose new metrics to characterize the homophily, but rarely consider the correlation of the proposed metrics and models. In this paper, we first design a new metric, Neighborhood Homophily (NH), to measure the label complexity or purity in node neighborhoods. Furthermore, we incorporate the metric into the classical graph convolutional network (GCN) architecture and propose Neighborhood Homophily-based Graph Convolutional Network (NHGCN). In this framework, neighbors are grouped by estimated NH values and aggregated from different channels, and the resulting node predictions are then used in turn to estimate and update NH values. The two processes of metric estimation and model inference are alternately optimized to achieve better node classification. NHGCN achieves top overall performance on both homophilous and heterophilous benchmarks, with an improvement of up to 7.4\% compared to the current SOTA methods.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {3908–3912},
numpages = {5},
keywords = {graph neural networks, homophily, node classification},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{10.1145/3528229.3529382,
author = {Hrusto, Adha and Engstr\"{o}m, Emelie and Runeson, Per},
title = {Optimization of Anomaly Detection in a Microservice System through Continuous Feedback from Development},
year = {2022},
isbn = {9781450393348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528229.3529382},
doi = {10.1145/3528229.3529382},
abstract = {Monitoring a microservice system may bring a lot of benefits to development teams such as early detection of run-time errors and various performance anomalies. In this study, we explore deep learning (DL) solutions for detection of anomalous system's behavior based on collected monitoring data that consists of applications' and systems' performance metrics. The study is conducted in a collaboration with a Swedish company responsible for ticket and payment management in public transportation. Moreover, we specifically address a shortage of approaches for evaluating DL models without any ground truth data. Hence, we propose a solution design for anomaly detection and reporting alerts inspired by state-of-the-art DL solutions. Furthermore, we propose a plan for its in-context implementation and evaluation empowered by feedback from the development team. Through continuous feedback from development, the labeled data is generated and used for optimization of the DL model. In this way, a microservice system may leverage DL solutions to address rising challenges within its architecture.},
booktitle = {Proceedings of the 10th IEEE/ACM International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {13–20},
numpages = {8},
keywords = {DevOps, anomaly detection, deep learning, microservices},
location = {Pittsburgh, Pennsylvania},
series = {SESoS '22}
}

@inproceedings{10.1145/3575693.3575751,
author = {Liang, Mingyu and Gan, Yu and Li, Yueying and Torres, Carlos and Dhanotia, Abhishek and Ketkar, Mahesh and Delimitrou, Christina},
title = {Ditto: End-to-End Application Cloning for Networked Cloud Services},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575751},
doi = {10.1145/3575693.3575751},
abstract = {The lack of representative, publicly-available cloud services has been a recurring problem in the architecture and systems communities. While open-source benchmarks exist, they do not capture the full complexity of cloud services. Application cloning is a promising way to address this, however, prior work is limited to CPU-/cache-centric, single-node services, operating at user level.  

We present Ditto, an automated framework for cloning end-to-end cloud applications, both monolithic and microservices, which captures I/O and network activity, as well as kernel operations, in addition to application logic. Ditto takes a hierarchical approach to application cloning, starting with capturing the dependency graph across distributed services, to recreating each tier's control/data flow, and finally generating system calls and assembly that mimics the individual applications. Ditto does not reveal the logic of the original application, facilitating publicly sharing clones of production services with hardware vendors, cloud providers, and the research community.  

We show that across a diverse set of single- and multi-tier applications, Ditto accurately captures their CPU and memory characteristics as well as their high-level performance metrics, is portable across platforms, and facilitates a wide range of system studies.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {222–236},
numpages = {15},
keywords = {cloud computing, microservices, architecture, benchmarking and emulation, software reverse engineering},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3472883.3486986,
author = {Zhang, Jun and Ferydouni, Robert and Montana, Aldrin and Bittman, Daniel and Alvaro, Peter},
title = {3MileBeach: A Tracer with Teeth},
year = {2021},
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472883.3486986},
doi = {10.1145/3472883.3486986},
abstract = {We present 3MileBeach, a tracing and fault injection platform designed for microservice-based architectures. 3Mile-Beach interposes on the message serialization libraries that are ubiquitous in this environment, avoiding the application code instrumentation that tracing and fault injection infrastructures typically require. 3MileBeach provides message-level distributed tracing at less than 50\% of the overhead of the state-of-the-art tracing frameworks, and fault injection that allows higher precision experiments than existing solutions. We measure the overhead of 3MileBeach as a tracer and its efficacy as a fault injector. We qualitatively measure its promise as a platform for tuning and debugging by sharing concrete use cases in the context of bottleneck identification, performance tuning, and bug finding. Finally, we use 3MileBeach to perform a novel type of fault injection - Temporal Fault Injection (TFI), which more precisely controls individual inter-service message flow with temporal prerequisites, and makes it possible to catch an entirely new class of fault tolerance bugs.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {458–472},
numpages = {15},
keywords = {Temporal Fault Injection, Chaos Engineering, Application Tuning, Tracing, Bug Finding},
location = {Seattle, WA, USA},
series = {SoCC '21}
}

@article{10.1145/3582573,
author = {Chen, Jialuo and Wang, Jingyi and Ma, Xingjun and Sun, Youcheng and Sun, Jun and Zhang, Peixin and Cheng, Peng},
title = {QuoTe: Quality-Oriented Testing for Deep Learning Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3582573},
doi = {10.1145/3582573},
abstract = {Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing—that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality—that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {125},
numpages = {33},
keywords = {Deep learning, fairness, testing, robustness}
}

@inproceedings{10.1145/3578245.3585032,
author = {Somashekar, Gagan and Kumar, Rajat},
title = {Enhancing the Configuration Tuning Pipeline of Large-Scale Distributed Applications Using Large Language Models (Idea Paper)},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3585032},
doi = {10.1145/3578245.3585032},
abstract = {The performance of distributed applications implemented using microservice architecture depends heavily on the configuration of various parameters, which are hard to tune due to large configuration search space and inter-dependence of parameters. While the information in product manuals and technical documents guides the tuning process, manual collection of meta-data for all application parameters is laborious and not scalable. Prior works have largely overlooked the automated use of product manuals, technical documents and source code for extracting such meta-data. In the current work, we propose using large language models for automated meta-data extraction and enhancing the configuration tuning pipeline. We further ideate on building an in-house knowledge system using experimental data to learn important parameters in configuration tuning using historical data on parameter dependence, workload statistics, performance metrics and resource utilization. We expect productionizing the proposed system will reduce the total time and experimental iterations required for configuration tuning in new applications, saving an organization both developer time and money.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {39–44},
numpages = {6},
keywords = {microservice architecture, information retrieval, parameter tuning, large language models},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@article{10.1145/3611312,
author = {Maschi, Fabio and Alonso, Gustavo},
title = {Strega: An HTTP Server for FPGAs},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-7406},
url = {https://doi.org/10.1145/3611312},
doi = {10.1145/3611312},
abstract = {The computer architecture landscape is being reshaped by the new opportunities, challenges and constraints brought by the cloud. On the one hand, high-level applications profit from specialised hardware to boost their performance and reduce deployment costs. On the other hand, cloud providers maximise the CPU time allocated to client applications by offloading infrastructure tasks to hardware accelerators. While it is well understood how to do this for, e.g., network function virtualisation and protocols such as TCP/IP, support for higher networking layers is still largely missing, limiting the potential of accelerators. In this paper, we present Strega, an open-source1 light-weight HTTP server that enables crucial functionality such as FPGA-accelerated functions being called through a RESTful protocol (FPGA-as-a-Function). Our experimental analysis shows that a single Strega node sustains a throughput of 1.7&nbsp;M HTTP requests per second with an end-to-end latency as low as 16μs, outperforming nginx running on 32 vCPUs in both metrics, and can even be an alternative to the traditional OpenCL flow over the PCIe bus. Through this work, we pave the way for running microservices directly on FPGAs, bypassing CPU overhead and realising the full potential of FPGA acceleration in distributed cloud applications.},
note = {Just Accepted},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {oct},
keywords = {FPGA, distributed systems, HTTP, RESTful API, Network on chip, disaggregated accelerator, Webserver}
}

@article{10.1145/3583563,
author = {Camilli, Matteo and Colarusso, Carmine and Russo, Barbara and Zimeo, Eugenio},
title = {Actor-Driven Decomposition of Microservices through Multi-Level Scalability Assessment},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3583563},
doi = {10.1145/3583563},
abstract = {The microservices architectural style has gained widespread acceptance. However, designing applications according to this style is still challenging. Common difficulties concern finding clear boundaries that guide decomposition while ensuring performance and scalability. With the aim of providing software architects and engineers with a systematic methodology, we introduce a novel actor-driven decomposition strategy to complement the domain-driven design and overcome some of its limitations by reaching a finer modularization yet enforcing performance and scalability improvements. The methodology uses a multi-level scalability assessment framework that supports decision-making over iterative steps. At each iteration, architecture alternatives are quantitatively evaluated at multiple granularity levels. The assessment helps architects to understand the extent to which architecture alternatives increase or decrease performance and scalability. We applied the methodology to drive further decomposition of the core microservices of a real data-intensive smart mobility application and an existing open-source benchmark in the e-commerce domain. The results of an in-depth evaluation show that the approach can effectively support engineers in (i) decomposing monoliths or coarse-grained microservices into more scalable microservices and (ii) comparing among alternative architectures to guide decision-making for their deployment in modern infrastructures that orchestrate lightweight virtualized execution units.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {117},
numpages = {46},
keywords = {performance analysis, scalability assessment, architectural patterns, decomposition process, Microservices}
}

@inproceedings{10.1145/3628797.3628901,
author = {Le-Thanh, Phuc and Le-Anh, Tuan and Le-Trung, Quan},
title = {Research and Development of a Smart Solution for Runtime Web Application Self-Protection},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628901},
doi = {10.1145/3628797.3628901},
abstract = {In contemporary times, ensuring web application security is a critical concern for organizations due to the prevalence of numerous types of attacks that serve diverse purposes. While traditional security measures such as web application firewalls (WAF) and intrusion detection systems (IDS) can help mitigate attacks, there is still a possibility of them being circumvented or compromised. A more efficacious approach is to adopt runtime application self-protection (RASP) solutions integrated within the web application. This solution has demonstrated its effectiveness by aiding in early attack detection and rapid attack mitigation. In this research, we propose a smart solution for runtime web application self-protection (RASP) to protect against vulnerabilities, attacks, and common weaknesses that have been rated among the top ten web security risks in 2021 by the Open Web Application Security Project (OWASP). The proposed solution leverages convolutional neural network (CNN) and a family of recurrent neural network (RNN) techniques. It builds a deep learning model with deep neural network architectures that scrutinizes user requests, thereby detecting potential SQL injection (SQLi), Cross-Site scripting (XSS), command injection (CMDi), and other types of attacks. The solution is designed to dynamically adapt to the application’s behavior and traffic, with the goal of minimizing false positives and preventing the blocking of legitimate traffic. Furthermore, the proposed solution, based on a microservices architecture, enhances the flexibility of the prediction module during upgrades and automated deployment. It is integrated with MLOps and DevSecOps and is also designed to be compatible with RESTful API servers. Our results have validated the efficacy of this solution in providing real-time application protection.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {304–311},
numpages = {8},
keywords = {Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU)., Deep Learning, Convolutional Neural Network (CNN), Web Application Security, Runtime Application Self-Protection (RASP)},
location = {<conf-loc>, <city>Ho Chi Minh</city>, <country>Vietnam</country>, </conf-loc>},
series = {SOICT '23}
}

@inproceedings{10.1145/3491204.3527462,
author = {V, Thrivikraman and Dixit, Vishnu R. and S, Nikhil Ram and Gowda, Vikas K. and Vasudevan, Santhosh Kumar and Kalambur, Subramaniam},
title = {MiSeRTrace: Kernel-Level Request Tracing for Microservice Visibility},
year = {2022},
isbn = {9781450391597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491204.3527462},
doi = {10.1145/3491204.3527462},
abstract = {With the evolution of microservice applications, the underlying architectures have become increasingly complex compared to their monolith counterparts. This mainly brings in the challenge of observability. By providing a deeper understanding into the functioning of distributed applications, observability enables improving the performance of the system by obtaining a view of the bottlenecks in the implementation. The observability provided by currently existing tools that perform dynamic tracing on distributed applications is limited to the user-space and requires the application to be instrumented to track request flows. In this paper, we present a new open-source framework MiSeRTrace that can trace the end-to-end path of requests entering a microservice application at the kernel space without requiring instrumentation or modification of the application. Observability at the comprehensiveness of the kernel space allows breaking down of various steps in activities such as network transfers and IO tasks, thus enabling root cause based performance analysis and accurate identification of hotspots. MiSeRTrace supports tracing user-enabled kernel events provided by frameworks such as bpftrace or ftrace and isolates kernel activity associated with each application request with minimal overheads. We then demonstrate the working of the solution with results on a benchmark microservice application.},
booktitle = {Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
pages = {77–80},
numpages = {4},
keywords = {thread state model, microservice, request tracing, misertrace, kernel tracing},
location = {Bejing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3479239.3485679,
author = {Hanzo, Lajos},
title = {Space-Air-Ground Integrated Networking: From Single- to Multi-Component Pareto Optimization},
year = {2021},
isbn = {9781450390774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479239.3485679},
doi = {10.1145/3479239.3485679},
abstract = {Thanks to the spectacular advances in signal processing and nano-technology, five wireless generations have been conceived over the past five decades. Indeed, near-capacity operation at an infinitesimally low error-rate has become feasible and flawless multimedia communications is supported in areas of high traffic-density, but how do we fill the huge coverage holes existing across the globe? As a promising system-architecture, the SAGIN concept constituted by an integrated terrestrial, UAV-aided, airplane-assisted as well as satellite-based global coverage-solution will be highlighted to pave the way for seamless next-generation service provision. However, these links exhibit strongly heterogeneous properties, hence requiring different enabling techniques. The joint optimization of the associated conflicting performance metrics of throughput, transmit power, latency, error probability, hand-over probability and link-lifetime poses an extremely challenging problem. Explicitly, sophisticated multi-component system optimization is required for finding the Pareto-front of all optimal solutions, where none of the above-mentioned metric can be improved without degrading at least one of the others [1] - [5]....},
booktitle = {Proceedings of the 24th International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {1},
numpages = {1},
location = {Alicante, Spain},
series = {MSWiM '21}
}

@inproceedings{10.1145/3468737.3494104,
author = {Lanciano, Giacomo and Galli, Filippo and Cucinotta, Tommaso and Bacciu, Davide and Passarella, Andrea},
title = {Predictive Auto-Scaling with OpenStack Monasca},
year = {2021},
isbn = {9781450385640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468737.3494104},
doi = {10.1145/3468737.3494104},
abstract = {Cloud auto-scaling mechanisms are typically based on reactive automation rules that scale a cluster whenever some metric, e.g., the average CPU usage among instances, exceeds a predefined threshold. Tuning these rules becomes particularly cumbersome when scaling-up a cluster involves non-negligible times to bootstrap new instances, as it happens frequently in production cloud services.To deal with this problem, we propose an architecture for auto-scaling cloud services based on the status in which the system is expected to evolve in the near future. Our approach leverages on time-series forecasting techniques, like those based on machine learning and artificial neural networks, to predict the future dynamics of key metrics, e.g., resource consumption metrics, and apply a threshold-based scaling policy on them. The result is a predictive automation policy that is able, for instance, to automatically anticipate peaks in the load of a cloud application and trigger ahead of time appropriate scaling actions to accommodate the expected increase in traffic.We prototyped our approach as an open-source OpenStack component, which relies on, and extends, the monitoring capabilities offered by Monasca, resulting in the addition of predictive metrics that can be leveraged by orchestration components like Heat or Senlin. We show experimental results using a recurrent neural network and a multi-layer perceptron as predictor, which are compared with a simple linear regression and a traditional non-predictive auto-scaling policy. However, the proposed framework allows for the easy customization of the prediction policy as needed.},
booktitle = {Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing},
articleno = {20},
numpages = {10},
keywords = {elasticity auto-scaling, time-series forecasting, OpenStack, predictive operations},
location = {Leicester, United Kingdom},
series = {UCC '21}
}

@inproceedings{10.5555/3535850.3535942,
author = {Liu, Wencong and Liu, Jiamou and Zhang, Zijian and Liu, Yiwei and Zhu, Liehuang},
title = {Residual Entropy-Based Graph Generative Algorithms},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Classification and clustering are crucial tasks that recognize the identities and the communities of nodes in a graph. Several methods have been proposed to reduce the accuracy of node classification and clustering through graph neural networks (GNN). Existing defense methods usually modify the model architecture and adopt countermeasure training to enhance the robustness of the node classification and clustering. However, these defense methods are model-oriented and not robust.To alleviate the problem, this paper first proposes a robust node classification metric based on residual entropy. More concretely, we prove that maximizing the residual entropy helps to improve the robustness of the classification accuracy. We them propose two graph generative algorithms to resist against two kinds of GNN-based attacks, the untargeted and the targeted attacks. Finally, experimental analysis show that the proposed algorithms outperform the existing defense works under five classic datasets.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {816–824},
numpages = {9},
keywords = {robustness, graph generative algorithm, graph adversarial learning, residual entropy, node classification},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3477145.3477156,
author = {Balaji, Adarsha and Song, Shihao and Titirsha, Twisha and Das, Anup and Krichmar, Jeffrey and Dutt, Nikil and Shackleford, James and Kandasamy, Nagarajan and Catthoor, Francky},
title = {NeuroXplorer 1.0: An Extensible Framework for Architectural Exploration with Spiking Neural Networks},
year = {2021},
isbn = {9781450386913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477145.3477156},
doi = {10.1145/3477145.3477156},
abstract = {Recently, both industry and academia have proposed many different neuromorphic architectures to execute applications that are designed with Spiking Neural Network (SNN). Consequently, there is a growing need for an extensible simulation framework that can perform architectural explorations with SNNs, including both platform-based design of today’s hardware, and hardware-software co-design and design-technology co-optimization of the future. We present NeuroXplorer, a fast and extensible framework that is based on a generalized template for modeling a neuromorphic architecture that can be infused with the specific details of a given hardware and/or technology. NeuroXplorer can perform both low-level cycle-accurate architectural simulations and high-level analysis with data-flow abstractions. NeuroXplorer’s optimization engine can incorporate hardware-oriented metrics such as energy, throughput, and latency, as well as SNN-oriented metrics such as inter-spike interval distortion and spike disorder, which directly impact SNN performance. We demonstrate the architectural exploration capabilities of NeuroXplorer through case studies with many state-of-the-art machine learning models.},
booktitle = {International Conference on Neuromorphic Systems 2021},
articleno = {10},
numpages = {9},
keywords = {Spiking Neural Networks (SNN), Design-Technology Co-Optimization, Neuromorphic Computing, Platform-Based Design, Hardware-Software Co-Design, Non Volatile Memory (NVM)},
location = {Knoxville, TN, USA},
series = {ICONS 2021}
}

@inproceedings{10.1145/3579371.3589052,
author = {Ning, August and Tziantzioulis, Georgios and Wentzlaff, David},
title = {Supply Chain Aware Computer Architecture},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589052},
doi = {10.1145/3579371.3589052},
abstract = {Progressively and increasingly, our society has become more and more dependent on semiconductors and semiconductor-enabled products and services. The importance of chips and their supply chains has been highlighted during the 2020-present chip shortage caused by manufacturing disruptions and increased demand due to the COVID-19 pandemic. However, semiconductor supply chains are inherently vulnerable to disruptions and chip crises can easily recur in the future.We present the first work that elevates supply chain conditions to be a first-class design constraint for future computer architectures. We characterize and model the chip creation process from standard tapeout to packaging to provide a framework for architects to quickly assess the time-to-market of their chips depending on their architecture and the current market conditions. In addition, we propose a novel metric, the Chip Agility Score (CAS) - a way to quantify a chip architecture's resilience against production-side supply changes.We utilize our proposed time-to-market model, CAS, and chip design/manufacturing economic models to evaluate prominent architectures in the context of current and speculative supply chain changes. We find that using an older process node to re-release chips can decrease time-to-market by 73\%-116\% compared to using the most advanced processes. Also, mixed-process chiplet architectures can be 24\%-51\% more agile compared to equivalent single-process chiplet and monolithic designs respectively. Guided by our framework, we present an architectural design methodology that minimizes time-to-market and chip creation costs while maximizing agility for mass-produced legacy node chips.Our modeling framework and data sets are open-sourced to advance supply chain aware computer architecture research. https://github.com/PrincetonUniversity/ttm-cas},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {17},
numpages = {15},
keywords = {chip shortage, semiconductor supply chain, economics, modeling},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1109/DS-RT52167.2021.9576142,
author = {Victor, Carlos and Nguyen, Tuan Anh and Silva, Leonardo Augusto and Andrade, Ermeson and Santo, Guto Leoni and Min, Dugki and Lee, Jae Woo and Silva, Francisco Airton},
title = {Performability Assessment and Sensitivity Analysis of a Home Automation System},
year = {2022},
isbn = {9781665433266},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DS-RT52167.2021.9576142},
doi = {10.1109/DS-RT52167.2021.9576142},
abstract = {Home automation or domotics is a typical representative of everything as a service (XaaS). Individual houses are equipped with Internet of Things (IoT) sensors and home facilities capable of self-assessment to offer comfortable, secure, and high-quality home services to their residents. However, assessing such systems with a high level of diversity is paramount of importance and challenging to assimilate. Domotics XaaS requires a high quality of service (QoS) in service performance and operational availability. In that regard, we propose, in this paper, a modeling approach based on stochastic Petri nets (SPN) for the performability quantification of domotics architectures. SPN performability models are developed following the architecture of a home automation system consisting of several IoT sensors/devices to evaluate the trade-offs between performance and availability of home automation services. The inter-dependency between performance and availability metrics is evaluated. The metrics include, for example, the mean response time (MRT) and the number of discarded packets. Sensitivity analysis using the design of experiments (DoE) is performed to identify the system's impacting components and performability bottleneck. Simulation results highlight the useful aspects of the proposed performability models for architectural and operational optimization of home automation XaaS infrastructures.},
booktitle = {Proceedings of the 2021 IEEE/ACM 25th International Symposium on Distributed Simulation and Real Time Applications},
articleno = {18},
numpages = {4},
keywords = {stochastic models, home automation, domotics},
location = {Valencia, Spain},
series = {DS-RT '21}
}

@inproceedings{10.1145/3484266.3487376,
author = {Qazi, Ihsan Ayyub and Qazi, Zafar Ayyub and Ali, Ayesha and Abdullah, Muhammad and Habib, Rumaisa},
title = {Rethinking Web for Affordability and Inclusion},
year = {2021},
isbn = {9781450390873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3484266.3487376},
doi = {10.1145/3484266.3487376},
abstract = {Today's Web remains too expensive for many Internet users, especially in developing regions. Unfortunately, the rising complexity of the Web makes affordability an even bigger concern as it stands to limit users' access to Internet services. We propose a novel framework and fairness metric for rethinking Web architecture for affordability and inclusion. Our framework provides systematic guidelines for adapting Web complexity based on geographic variations in mobile broadband prices and income levels. Preliminary evaluation shows the resulting architecture can achieve a better balance between Web quality and affordability while preserving user privacy.},
booktitle = {Proceedings of the 20th ACM Workshop on Hot Topics in Networks},
pages = {9–15},
numpages = {7},
keywords = {Inclusion, Web, Affordability, User Privacy},
location = {<conf-loc>, <city>Virtual Event</city>, <country>United Kingdom</country>, </conf-loc>},
series = {HotNets '21}
}

@inproceedings{10.1145/3485983.3493356,
author = {Holzinger, Kilian and Stubbe, Henning and Biersack, Franz and Mari\~{n}o, Angela Gonzalez and Kane, Abdoul and Lluis, Francisco Fons and Haigang, Zhang and Wild, Thomas and Herkersdorf, Andreas and Carle, Georg},
title = {Precise Real-Time Monitoring of Time-Critical Flows},
year = {2021},
isbn = {9781450390989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485983.3493356},
doi = {10.1145/3485983.3493356},
abstract = {Ethernet is increasingly used in areas where time-critical and safety-relevant data are transported over the network along with best-effort flows, for example in intra vehicle networks or industrial networks. The resulting complex network architectures, time-sensitive networking configurations and system interactions are hard to foresee during the design phase. Therefore, it is hard to rule out any violations of flow specifications or timing and reliability requirements, especially in the presence of unpredictable failures.In this work, the design of a flow-oriented network monitoring system for time-sensitive applications is presented. It continuously supervises relevant performance metrics with high precision and short detection delay. Moreover, it allows to check compliance with flow specifications in real-time. Initial evaluations using intra vehicle network traffic yield a high measurement precision.},
booktitle = {Proceedings of the 17th International Conference on Emerging Networking EXperiments and Technologies},
pages = {489–490},
numpages = {2},
location = {Virtual Event, Germany},
series = {CoNEXT '21}
}

@inproceedings{10.1145/3546591.3547527,
author = {Shan, Yizhou and Lin, Will and Guo, Zhiyuan and Zhang, Yiying},
title = {Towards a Fully Disaggregated and Programmable Data Center},
year = {2022},
isbn = {9781450394413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546591.3547527},
doi = {10.1145/3546591.3547527},
abstract = {Today, we are seeing two trends in the data center. On the one hand, applications are becoming more fine-grained, driven by the recent trend of serverless computing and microservices. On the other hand, data-center hardware is becoming more heterogeneous and customized to different computing needs. Because of these trends and for better manageability, several major data centers are moving towards a disaggregated architecture, where different hardware resources like storage and accelerators are organized as independent, network-attached pools. However, data centers today are still server-centric and relies heavily on traditional CPU-based servers.In this paper, we take a step further and explore the possibility of building a fully disaggregated data center, where every type of resource is disaggregated. Moreover, we explore the requirements and implications of making each of the disaggregated device programmable. We present guidelines and initial solutions for data center designers to navigate design trade-offs. Specifically, we decompose the overarching problem into four sub-problems and propose solutions to each of them. At the top layer, we explore two types of abstractions and propose a disaggregation-native design methodology. At the bottom layer, we describe the hardware and key features required to build disaggregated devices as well as the networking infrastructure to connect them. To bridge these two layers, we propose a static-time component that compiles different user programs into heterogeneous disaggregated devices through a disaggregation-native intermediate representation. We also propose a run-time system that manages hardware resources and schedules compiler generated execution units. We hope our proposal can pave the way for future disaggregated and programmable data center deployment.},
booktitle = {Proceedings of the 13th ACM SIGOPS Asia-Pacific Workshop on Systems},
pages = {18–28},
numpages = {11},
keywords = {data-center network, resource disaggregation, data-center hardware architecture},
location = {Virtual Event, Singapore},
series = {APSys '22}
}

@inproceedings{10.1145/3616391.3622766,
author = {Makama, Aliyu and Kuladinithi, Koojana and Timm-Giel, Andreas},
title = {Evaluation of IEEE 802.11 Ad Hoc-Based Wireless Seismic Data Acquisition Networks},
year = {2023},
isbn = {9798400703683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616391.3622766},
doi = {10.1145/3616391.3622766},
abstract = {This paper investigates the performance of a proposed IEEE 802.11 ad hoc-based wireless seismic data acquisition (WSDA) network architecture. The study centers on examining the network performance in the 2.4 and 5 SI gigahertz bands with focus on addressing WSDA challenges such as scalability, reliability, self-configuration and organization, interference effects, and latency. Routing Protocol for Low-Power and Lossy Networks (RPL) is used as the enabling routing protocol employing the multiple Destination Oriented Directed Acyclic Graph (multi-DODAG) architecture. OMNeT++ discrete event simulator is used to evaluate the network performance, using metrics such as packet delivery ratio (PDR), end-to-end delay (E2ED), packet error rate (PER), retransmission ratio, packet dropped, etc. Results show that the proposed network architecture is scalable and reliable with large number of geophones in the network. In addition, Routing Protocol for Low-Power and Lossy Networks (RPL) proves to be a suitable candidate to enable self-configuration and organization in multi-hop ad hoc WSDA networks.},
booktitle = {Proceedings of the 19th ACM International Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {25–32},
numpages = {8},
keywords = {wireless gateway (gw), geophones, wireless geophone network (wgn), routing protocol for low-power and lossy networks (rpl), central control unit (ccu), wireless seismic data acquisition (wsda), destination oriented directed acyclic graph (dodag).},
location = {<conf-loc>, <city>Montreal</city>, <state>Quebec</state>, <country>Canada</country>, </conf-loc>},
series = {Q2SWinet '23}
}

@inproceedings{10.1145/3508397.3564828,
author = {Chancusig, Cristian and Tumbaco, Sergio and Alulema, Darwin and Iribarne, Luis and Criado, Javier},
title = {Binary Classification Architecture for Edge Computing Based on Cognitive Services and Deep Neural Networks},
year = {2022},
isbn = {9781450392198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508397.3564828},
doi = {10.1145/3508397.3564828},
abstract = {Systems based on computer vision and artificial intelligence are an alternative for repetitive inspection processes. However, it is possible to extend the learning capacity of these systems to classify multiple objects using edge computing. This allows combining local processing with cloud processing to expand the possibilities and reduce the response time. In this work, a classification architecture based on remote web services and local neural networks is proposed. To test this architecture, Microsoft Azure cognitive web services and its Computer Vision API have been used, combined with the use of transfer learning and ResNet 50. The cloud service allows the identification and labelling of image content, while the Edge service, based on the neural network, allows the generation of classification models for those objects not identified or incorrectly identified by the remote service. The architecture allows to extend the possibility of image recognition by integrating web services that combined with edge processing accelerate the identification process. The proposed architecture is composed of three layers; (a) a physical layer, for the mechanical and electronic structure; (b) a logical layer, which defines the interaction of the remote and local image recognition web services, and (c) an application layer, for the integration of the monitoring and control interfaces. Finally, the architecture was evaluated through functionality testing and performance metrics of classification models, as well as load and usability testing.},
booktitle = {Proceedings of the 14th International Conference on Management of Digital EcoSystems},
pages = {148–155},
numpages = {8},
keywords = {edge computing, computer vision, cyber-physical systems, neural network, microservices},
location = {Venice, Italy},
series = {MEDES '22}
}

@inproceedings{10.1145/3531056.3542769,
author = {Mwotil, Alex and Bainomugisha, Engineer and Araka, Stephen G.M.},
title = {Mira: An Application Containerisation Pipeline for Small Software Development Teams in Low Resource Settings},
year = {2022},
isbn = {9781450396639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531056.3542769},
doi = {10.1145/3531056.3542769},
abstract = {Cloud native applications leverage Development and Operation (DevOps), microservice architectures and containerisation for primarily availability, resilience and scalability reasons. Small developer teams in low resource settings have unique DevOps needs and harnessing its principles and practices is technically challenging and distinctly difficult in these contexts. We conducted a survey with professional developers, students and researchers situated and working in a low resource setting and the results indicate that these principles and practices are relatively new. In application containerisation, an operating system virtualisation method that can significantly optimize the use of computing resources, the respondents indicated challenges in the process steps. Particularly, small developer teams in low resource settings require custom tools and abstractions for software development and delivery automation. Informed by the developer needs, we designed and developed a custom automated containerisation pipeline, mira, for a managed cloud native platform situated in a low-resource setting. We validate mira against 6 major application frameworks, tools and/or languages and successful deployment of the resultant applications onto a cloud native platform.},
booktitle = {Proceedings of the Federated Africa and Middle East Conference on Software Engineering},
pages = {31–38},
numpages = {8},
keywords = {cloud, cloud native, orchestration, automation, containers, docker},
location = {Cairo-Kampala, Egypt},
series = {FAMECSE '22}
}

@inproceedings{10.1145/3603269.3604872,
author = {Habib, Rumaisa and Tanveer, Sarah and Inam, Aimen and Ahmed, Haseeb and Ali, Ayesha and Uzmi, Zartash Afzal and Qazi, Zafar Ayyub and Qazi, Ihsan Ayyub},
title = {A Framework for Improving Web Affordability and Inclusiveness},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603269.3604872},
doi = {10.1145/3603269.3604872},
abstract = {Today's Web remains too expensive for many Internet users, especially in developing regions. Unfortunately, the rising complexity of the Web makes affordability an even bigger concern as it stands to limit users' access to Internet services. We propose a novel framework and a fairness metric for rethinking Web architecture for affordability and inclusion. Our proposed framework systematically adapts Web complexity based on geographic variations in mobile broadband prices and income levels. We conduct a cross-country analysis of 99 countries, showing that our framework can better balance affordability and webpage quality while preserving user privacy. To adapt Web complexity, our framework solves an optimization problem to produce webpages that maximize page quality while reducing the webpage to a given target size.},
booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
pages = {592–607},
numpages = {16},
keywords = {inclusion, user privacy, transcoding service, web, affordability},
location = {New York, NY, USA},
series = {ACM SIGCOMM '23}
}

@inproceedings{10.1145/3511808.3557068,
author = {Li, Sen and Lv, Fuyu and Jin, Taiwei and Li, Guiyang and Zheng, Yukun and Zhuang, Tao and Liu, Qingwen and Zeng, Xiaoyi and Kwok, James and Ma, Qianli},
title = {Query Rewriting in TaoBao Search},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557068},
doi = {10.1145/3511808.3557068},
abstract = {In e-commerce search engines, query rewriting (QR) is a crucial technique that improves shopping experience by reducing the vocabulary gap between user queries and product catalog. Recent works have mainly adopted the generative paradigm. However, they hardly ensure high-quality generated rewrites and do not consider personalization, which leads to degraded search relevance. In this work, we present Contrastive Learning Enhanced Query Rewriting (CLE-QR), the solution used in Taobao product search. It uses a novel contrastive learning enhanced architecture based on "query retrieval-semantic relevance ranking-online ranking". It finds the rewrites from hundreds of millions of historical queries while considering relevance and personalization. Specifically, we first alleviate the representation degeneration problem during the query retrieval stage by using an unsupervised contrastive loss, and then further propose an interaction-aware matching method to find the beneficial and incremental candidates, thus improving the quality and relevance of candidate queries. We then present a relevance-oriented contrastive pre-training paradigm on the noisy user feedback data to improve semantic ranking performance. Finally, we rank these candidates online with the user profile to model personalization for the retrieval of more relevant products. We evaluate CLE-QR on Taobao Product Search, one of the largest e-commerce platforms in China. Significant metrics gains are observed in online A/B tests. CLE-QR has been deployed to our large-scale commercial retrieval system and serviced hundreds of millions of users since December 2021. We also introduce its online deployment scheme, and share practical lessons and optimization tricks of our lexical match system.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3262–3271},
numpages = {10},
keywords = {e-commerce search, query rewriting, lexical match},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3582197.3582240,
author = {Gao, Qihong and Wu, Yuxuan and Hao, Yi},
title = {Design and Implementation of an Edge Container Management Platform Based on Artificial Intelligence},
year = {2023},
isbn = {9781450397438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582197.3582240},
doi = {10.1145/3582197.3582240},
abstract = {The deployment and maintenance of IoT applications require a lot of manual work. To reduce the workload of the operation and maintenance personnel of the edge IoT system and improve the efficiency of edge applications, containerization technology based on K3s is more and more widely used. However, the existing edge container management platforms are not convenient in terms of application deployment and overall maintenance. This paper designs and implements an edge container management platform that supports AI operation and maintenance. In terms of design concepts, the abstract concepts involved in containerization technology are embodied as projects, which are easy to understand. In terms of management and control subsystems, the construction of the overall architecture and the interaction of various modules have been completed, thus, users can conveniently deploy and schedule edge applications and services. In terms of operation and maintenance subsystems, real-time collection, persistence, and analysis of logs, metric data, and trace data at all levels of the system are realized. In terms of visualization, the front-end display and monitoring of the system status are completed, which is convenient for project developers and platform operators to understand the running status of the project and platform in real-time, and provides a better solution for deployment and maintenance of IoT applications.},
booktitle = {Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City},
pages = {257–261},
numpages = {5},
keywords = {AIOps, Containerization technology, Edge computing, K3s},
location = {<conf-loc>, <city>Shanghai</city>, <country>China</country>, </conf-loc>},
series = {ICIT '22}
}

@article{10.1145/3480935,
author = {Anzt, Hartwig and Cojean, Terry and Flegar, Goran and G\"{o}bel, Fritz and Gr\"{u}tzmacher, Thomas and Nayak, Pratik and Ribizel, Tobias and Tsai, Yuhsiang Mike and Quintana-Ort\'{\i}, Enrique S.},
title = {Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3480935},
doi = {10.1145/3480935},
abstract = {In this article, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo’s design principle abstracts all functionality as “linear operators,” motivating the notation of a “linear operator algebra library.” Ginkgo’s current focus is oriented toward providing sparse linear algebra functionality for high performance graphics processing unit (GPU) architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific backends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo’s usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo’s high performance on state-of-the-art GPU architectures.},
journal = {ACM Trans. Math. Softw.},
month = {feb},
articleno = {2},
numpages = {33},
keywords = {High performance computing, multi-core and manycore architectures, healthy software lifecycle}
}

@inproceedings{10.1145/3617023.3617039,
author = {Viegas, Felipe and Canuto, Sergio and Cunha, Washington and Fran\c{c}a, Celso and Valiense, Claudio and Rocha, Leonardo and Gon\c{c}alves, Marcos Andr\'{e}},
title = {CluSent – Combining Semantic Expansion and De-Noising for Dataset-Oriented Sentiment Analysis of Short Texts},
year = {2023},
isbn = {9798400709081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617023.3617039},
doi = {10.1145/3617023.3617039},
abstract = {The lack of sufficient information, mainly in short texts, is a major challenge to building effective sentiment models. Short texts can be enriched with more complex semantic relationships that better capture affective information, with a potential undesired side effect of noise introduced into the data. This work proposes a new strategy for customized dataset-oriented sentiment analysis – CluSent – that exploits a powerful, recently proposed concept for representing semantically related words – CluWords. CluSent tackles the issues mentioned above of information shortage and noise by: (i) exploiting the semantic neighborhood of a given pre-trained word embedding to enrich document representation and (ii) introducing dataset-oriented filtering and weighting mechanisms to cope with noise, which takes advantage of the polarity and intensity information from lexicons. In our experimental evaluation, considering 19 datasets, five state-of-the-art baselines (including modern transformer architectures), and two metrics, CluSent was the best method in 30 out of 38 possibilities, with significant gains over the strongest baselines (over 14\%).},
booktitle = {Proceedings of the 29th Brazilian Symposium on Multimedia and the Web},
pages = {110–118},
numpages = {9},
keywords = {Sentiment Analysis, Classification, Natural Language Processing},
location = {<conf-loc>, <city>Ribeir\~{a}o Preto</city>, <country>Brazil</country>, </conf-loc>},
series = {WebMedia '23}
}

@inproceedings{10.1145/3534678.3539041,
author = {Li, Mingjie and Li, Zeyan and Yin, Kanglin and Nie, Xiaohui and Zhang, Wenchi and Sui, Kaixin and Pei, Dan},
title = {Causal Inference-Based Root Cause Analysis for Online Service Systems with Intervention Recognition},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539041},
doi = {10.1145/3534678.3539041},
abstract = {Fault diagnosis is critical in many domains, as faults may lead to safety threats or economic losses. In the field of online service systems, operators rely on enormous monitoring data to detect and mitigate failures. Quickly recognizing a small set of root cause indicators for the underlying fault can save much time for failure mitigation. In this paper, we formulate the root cause analysis problem as a new causal inference task namedintervention recognition. We proposed a novel unsupervised causal inference-based method namedCausal Inference-based Root Cause Analysis (CIRCA). The core idea is a sufficient condition for a monitoring variable to be a root cause indicator,i.e., the change of probability distribution conditioned on the parents in the Causal Bayesian Network (CBN). Towards the application in online service systems, CIRCA constructs a graph among monitoring metrics based on the knowledge of system architecture and a set of causal assumptions. The simulation study illustrates the theoretical reliability of CIRCA. The performance on a real-world dataset further shows that CIRCA can improve the recall of the top-1 recommendation by 25\% over the best baseline method.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3230–3240},
numpages = {11},
keywords = {online service systems, causal inference, intervention recognition, root cause analysis},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3569966.3571190,
author = {Hu, Wei},
title = {The Design and Implementation of Civil Aviation Meteorological Emergency Service Platform Based on 5G},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569966.3571190},
doi = {10.1145/3569966.3571190},
abstract = {In view of the problem that users cannot obtain meteorological data in time due to the interruption of Internet line in the current civil aviation meteorological external service system. Designing and implementing a civil aviation meteorological emergency service platform based on 5G. The platform is built based on C/S architecture. Firstly, designing and implementing the whole business module of the platform, and then establishing the corresponding protective measures for the security of the platform. Finally, deploying the 5G module between the data upload and the client, meanwhile, realizing the underlying switching logic between the Internet and 5G line. The application result shows that due to the use of 5G technology, The platform can provide users with meteorological emergency service in case of Internet interruption},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Software Engineering},
pages = {691–694},
numpages = {4},
keywords = {5G, meteorological service, emergency, civil aviation meteorological},
location = {<conf-loc>, <city>Guilin</city>, <country>China</country>, </conf-loc>},
series = {CSSE '22}
}

@article{10.1145/3587095,
author = {Lee, JunKyu and Mukhanov, Lev and Molahosseini, Amir Sabbagh and Minhas, Umar and Hua, Yang and Martinez del Rincon, Jesus and Dichev, Kiril and Hong, Cheol-Ho and Vandierendonck, Hans},
title = {Resource-Efficient Convolutional Networks: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3587095},
doi = {10.1145/3587095},
abstract = {Convolutional neural networks (CNNs) are used in our daily life, including self-driving cars, virtual assistants, social network services, healthcare services, and face recognition, among others. However, deep CNNs demand substantial compute resources during training and inference. The machine learning community has mainly focused on model-level optimizations such as architectural compression of CNNs, whereas the system community has focused on implementation-level optimization. In between, various arithmetic-level optimization techniques have been proposed in the arithmetic community. This article provides a survey on resource-efficient CNN techniques in terms of model-, arithmetic-, and implementation-level techniques, and identifies the research gaps for resource-efficient CNN techniques across the three different level techniques. Our survey clarifies the influence from higher- to lower-level techniques based on our resource efficiency metric definition and discusses the future trend for resource-efficient CNN research.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {276},
numpages = {36},
keywords = {neural networks, arithmetic utilization, Convolutional neural networks, resource efficiency}
}

@inproceedings{10.1145/3503161.3547883,
author = {Zhang, Jingjing and Fang, Shancheng and Mao, Zhendong and Zhang, Zhiwei and Zhang, Yongdong},
title = {Fine-Tuning with Multi-Modal Entity Prompts for News Image Captioning},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547883},
doi = {10.1145/3503161.3547883},
abstract = {News Image Captioning aims to generate descriptions for images embedded in news articles, including plentiful real-world concepts, especially about named entities. However, existing methods are limited in the entity-level template. Not only is it labor-intensive to craft the template, but it is error-prone due to local entity-aware, which solely constrains the prediction output at each language model decoding step with corrupted entity relationship. To overcome the problem, we investigate a concise and flexible paradigm to achieve global entity-aware by introducing a prompting mechanism with fine-tuning pre-trained models, named Fine-tuning with Multi-modal Entity Prompts for News Image Captioning (NewsMEP). Firstly, we incorporate two pre-trained models: (i) CLIP, translating the image with open-domain knowledge; (ii) BART, extended to encode article and image simultaneously. Moreover, leveraging the BART architecture, we can easily take the end-to-end fashion. Secondly, we prepend the target caption with two prompts to utilize entity-level lexical cohesion and inherent coherence in the pre-trained language model. Concretely, the visual prompts are obtained by mapping CLIP embeddings, and contextual vectors automatically construct the entity-oriented prompts. Thirdly, we provide an entity chain to control caption generation that focuses on entities of interest. Experiments results on two large-scale publicly available datasets, including detailed ablation studies, show that our NewsMEP not only outperforms state-of-the-art methods in general caption metrics but also achieves significant performance in precision and recall of various named entities.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4365–4373},
numpages = {9},
keywords = {fine-tuning, entity prompts, news image captioning, named entity},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3577065.3577094,
author = {Li, Dejian and Cui, Bingrong and Li, Kaixin and Shen, Tianjun and Sun, Yi and Chang, Shaonan},
title = {Energy Efficient Offloading Strategy Faced to Edge Computing-Enhanced Distributed Photovoltaic Smart Meter},
year = {2023},
isbn = {9781450397797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577065.3577094},
doi = {10.1145/3577065.3577094},
abstract = {With the rapid development of Power Internet of Things and the increase in the number of distributed photovoltaic smart meter (DPSM) technology, edge computing is required to support the low-latency computing service. Current works mainly focus on task offloading, the cooperation among smart meter devices is lack of attention. In order to reduce the task execution delay in of the network, a computing resource sharing architecture based on D2D is proposed. In order to reduce the processing delay and energy consumption of DPSM, we propose a task offloading model faced to DPSM. To minimize the total task execution delay, we formulate a Mixed-Integer Non-Linear Programming (MINLP) problem which optimizing task offloading and resource allocation jointly. Then, a generalized Benders decomposition algorithm combined with particle swarm optimization is proposed to solve the problem. Simulation results show that the proposed strategy can effectively improve the measurement efficiency and reduce the computational cost.},
booktitle = {Proceedings of the 2022 5th International Conference on Telecommunications and Communication Engineering},
pages = {161–164},
numpages = {4},
keywords = {Edge computing, Cost optimization, Task offloading, Smart meter},
location = {<conf-loc>, <city>Chengdu</city>, <country>China</country>, </conf-loc>},
series = {ICTCE '22}
}

@inproceedings{10.1145/3582016.3582031,
author = {Duraisamy, Padmapriya and Xu, Wei and Hare, Scott and Rajwar, Ravi and Culler, David and Xu, Zhiyi and Fan, Jianing and Kennelly, Christopher and McCloskey, Bill and Mijailovic, Danijela and Morris, Brian and Mukherjee, Chiranjit and Ren, Jingliang and Thelen, Greg and Turner, Paul and Villavieja, Carlos and Ranganathan, Parthasarathy and Vahdat, Amin},
title = {Towards an Adaptable Systems Architecture for Memory Tiering at Warehouse-Scale},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582031},
doi = {10.1145/3582016.3582031},
abstract = {Fast DRAM increasingly dominates infrastructure spend in large scale computing environments and this trend will likely worsen without an architectural shift. The cost of deployed memory can be reduced by replacing part of the conventional DRAM with lower cost albeit slower memory media, thus creating a tiered memory system where both tiers are directly addressable and cached. But, this poses numerous challenges in a highly multi-tenant warehouse-scale computing setting. The diversity and scale of its applications motivates an application-transparent solution in the general case, adaptable to specific workload demands.  

This paper presents TMTS(Transparent Memory Tiering System), an application-transparent memory tiering management system that implements an adaptive, hardware-guided architecture to dynamically optimize access to the various directly-addressed memory tiers without faults. TMTS has been deployed at scale for two years serving thousands of production services, successfully meeting service level objectives (SLOs) across diverse application classes in the fleet. The solution is developed in terms of system level metrics it seeks to optimize and evaluated across the diverse workload mix to guide advanced policies embodied in a user-level agent. It sustains less than 5\% overall performance degradation while replacing 25\% of DRAM with a much slower medium.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {727–741},
numpages = {15},
keywords = {Memory Tiering, Warehouse-Scale Computing, Memory Management},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3593434.3593442,
author = {Arcelli Fontana, Francesca and Camilli, Mateo and Rendina, Davide and Taraboi, Andrei Gabriel and Trubiani, Catia},
title = {Impact of Architectural Smells on Software Performance: An Exploratory Study},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593442},
doi = {10.1145/3593434.3593442},
abstract = {Architectural smells have been studied in the literature looking at several aspects, such as their impact on maintainability as a source of architectural debt, their correlations with code smells, and their evolution in the history of complex projects. The goal of this paper is to extend the study of architectural smells from a different perspective. We focus our attention on software performance, and we aim to quantify the impact of architectural smells as support to explain the root causes of system performance hindrances. Our method consists of a study design matching the occurrence of architectural smells with performance metrics. We exploit state-of-the-art tools for architectural smell detection, software performance profiling, and testing the systems under analysis. The removal of architectural smells generates new versions of systems from which we derive some observations on design changes improving/worsening performance metrics. Our experimentation considers two complex open-source projects, and results show that the detection and removal of two common types of architectural smells yield lower response time (up to ) with a large effect size, i.e., for - of the hotspot methods. The median memory consumption is also lower (up to ) with a large effect size for all the services.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {22–31},
numpages = {10},
keywords = {Software Architecture, Software Performance, Architectural Smells},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1145/3534678.3539132,
author = {Mangrulkar, Sourab and M S, Ankith and Sembium, Vivek},
title = {BE3R: BERT Based Early-Exit Using Expert Routing},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539132},
doi = {10.1145/3534678.3539132},
abstract = {Pre-trained language models like BERT have reported state-of-the-art performance on several Natural Language Processing (NLP) tasks, but high computational demands hinder its widespread adoption for large scale NLP tasks. In this work, we propose a novel routing based early exit model called BE3R (BERT based Early-Exit using Expert Routing), where we learn to dynamically exit in the earlier layers without needing to traverse through the entire model. Unlike the exiting early-exit methods, our approach can be extended to a batch inference setting. We consider the specific application of search relevance filtering in Amazon India marketplace services (a large e-commerce website). Our experimental results show that BE3R improves the batch inference throughput by 46.5\% over the BERT-Base model and 35.89\% over the DistilBERT-Base model on large dataset with 50 Million samples without any trade-off on the performance metric. We conduct thorough experimentation using various architectural choices, loss functions and perform qualitative analysis. We perform experiments on public GLUE Benchmark and demonstrate comparable performance to corresponding baseline models with 23\% average throughput improvement across tasks in batch inference setting.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3504–3512},
numpages = {9},
keywords = {attention models, transformers, relevance classification, natural language processing, deep learning, product search, e-commerce},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3583780.3615494,
author = {Tiady, Sambeet and Majumder, Anirban and Gupta, Deepak},
title = {PRODIGY: Product Design Guidance at Scale},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615494},
doi = {10.1145/3583780.3615494},
abstract = {Growth of e-commerce has enabled the creation of thousands of small-scale brands. However, these brands lack information on a) what new products to develop and b) how to refine existing products to improve on business metrics. We present a comprehensive Product Design Insights and Guidance service (named PRODIGY) that mines product attributes data available on e-commerce platforms and surface insights on a) new product development and b) product refinement. Our core contribution is a novel demand forecasting model for product designs based on a notable extension of the recently proposed FTTransformer architecture combined with a self-supervised pre-training task, akin to Masked Language Modeling (MLM) objective. For the product refinement use-case, we present a novel algorithm by embedding the design search in a data-density approximator, namely Conditional Variational Autoencoder. We run a thorough and comprehensive set of experiments and establish that PRODIGY achieves significant improvement in demand prediction as compared to state-of-the-art alternatives. Finally, we present our findings from an online experiment where PRODIGY helps to launch new products with +20\% lift in sales and +1.3\% lift in product ratings.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4836–4842},
numpages = {7},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{10.1145/3578245.3586012,
author = {Pouchard, Line C.},
title = {FAIR Enabling Re-Use of Data-Intensive Workflows and Scientific Reproducibility},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3586012},
doi = {10.1145/3578245.3586012},
abstract = {Scientific computing communities often run their experiments using complex data- and compute-intensive workflows that utilize high performance computing (HPC), distributed clusters and specialized architectures targeting machine learning and artificial intelligence. FAIR principles for data and software can be useful enablers for the reproducibility of performance (a key HPC metric) and that of scientific results (a crucial tenet of the scientific method) that are based in part on re-use, the R of FAIR principles. FAIR principles are under-used by HPC and data-intensive communities who have been slow to adopt them. This is due in part to the complexity of workflow life cycles, the numerous workflow management systems, the lack of integration of FAIR within existing technologies, and the specificity of managed systems that include rapidly evolving architectures and software stacks, and execution models that require resource managers and batch schedulers. Numerous challenges emerge for scientists attempting to publish FAIR datasets and software for the purpose of re-use and reproducibility, e.g. what data to publish and where due to sizes, how to "FAIRify" data subsetting, at what level of granularity to attribute persistent identifiers to software, what is the minimal amount of metadata needed to guarantee a certain level of reproducibility, what does reproducible AI actually mean? This talk will focus on such challenges and illustrate the negative impact of not applying FAIR on the reproducibility of experiments. We will introduce the notion of FAIR Digital Objects and present RECUP, a framework for data and metadata services for high performance workflows that proposes micro-solutions for adapting FAIR principles to HPC.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {329},
numpages = {1},
keywords = {HPC, data intensive, reproducibility, FAIR, RECUP, FDO, high performance computing},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3589608.3595081,
author = {Kaven, Sascha and Skwarek, Volker},
title = {Poster: Attribute Based Access Control for IoT Devices in 5G Networks},
year = {2023},
isbn = {9798400701733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589608.3595081},
doi = {10.1145/3589608.3595081},
abstract = {The deployment of 5G technology has the potential to usher in a new era for the internet of things (IoT). The introduction of new use cases, such as massive machine-type communications (mMTC), referring to a large number of IoT devices, resulting in the increasing importance of 5G as the basic communication infrastructure for IoT. However, the increasing connectivity of IoT devices coincides with a number of risks to security. Many IoT sensors have limited resources and, therefore, cannot perform the complex security measures required to protect them from attacks and data loss. Furthermore, IoT networks are very scattered, distributed and dynamic, so decentralised security measures are required. To address these challenges, this poster proposes the integration of attribute-based access control (ABAC) into the 5G service-based architecture. This approach aims to prevent unauthorized access to IoT devices at the network level, thereby alleviating the computational burden on resource-constrained IoT devices. By implementing ABAC, the proposed solution offers a more efficient method for managing access control within the IoT landscape in the context of 5G networks.},
booktitle = {Proceedings of the 28th ACM Symposium on Access Control Models and Technologies},
pages = {51–53},
numpages = {3},
keywords = {access control, abac, 5g},
location = {Trento, Italy},
series = {SACMAT '23}
}

@inproceedings{10.1145/3590777.3591405,
author = {Cali, Umit and Dynge, Marthe Fogstad and Idries, Ahmed and Mishra, Sambeet and Dmytro, Ivanko and Hashemipour, Naser and Kuzlu, Murat},
title = {Digital Energy Platforms Considering Digital Privacy and Security by Design Principles},
year = {2023},
isbn = {9781450398299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590777.3591405},
doi = {10.1145/3590777.3591405},
abstract = {The power system and markets have become increasingly complex, along with efforts to digitalize the energy sector. Accessing flexibility services, in particular, through digital energy platforms, has enabled communication between multiple entities within the energy system and streamlined flexibility market operations. However, digitalizing these vast and complex systems introduces new cybersecurity and privacy concerns, which must be properly addressed during the design of the digital energy platform ecosystems. More specifically, both privacy and cybersecurity measures should be embedded into all phases of the platform design and operation, based on the privacy and security by design principles. In this study, these principles are used to propose a holistic but generic architecture for digital energy platforms that are able to facilitate multiple use cases for flexibility services in the energy sector. A hybrid framework using both DLT and non-DLT solutions ensures trust throughout the layers of the platform architecture. Furthermore, an evaluation of numerous energy flexibility service use cases operating at various stages of the energy value chain is shown and graded in terms of digital energy platform technical maturity, privacy, and cybersecurity issues.},
booktitle = {Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference},
pages = {167–173},
numpages = {7},
keywords = {privacy., Flexibility markets, cybersecurity, distributed ledger technology, digitalization},
location = {Stavanger, Norway},
series = {EICC '23}
}

@article{10.1145/3588704,
author = {Chacko, Jeeta Ann and Mayer, Ruben and Jacobsen, Hans-Arno},
title = {How To Optimize My Blockchain? A Multi-Level Recommendation Approach},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588704},
doi = {10.1145/3588704},
abstract = {Aside from the conception of new blockchain architectures, existing blockchain optimizations in the literature primarily focus on system or data-oriented optimizations within prevailing blockchains. However, since blockchains handle multiple aspects ranging from organizational governance to smart contract design, a holistic approach that encompasses all the different layers of a given blockchain system is required to ensure that all optimization opportunities are taken into consideration. In this vein, we define a multi-level optimization recommendation approach that identifies optimization opportunities within a blockchain at the system, data, and user level. Multiple metrics and attributes are derived from a blockchain log and nine optimization recommendations are formalized. We implement an automated optimization recommendation tool, BlockOptR, based on these concepts. The system is extensively evaluated with a wide range of workloads covering multiple real-world scenarios. After implementing the recommended optimizations, we observe an average of 20\% improvement in the success rate of transactions and an average of 40\% improvement in latency.},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {24},
numpages = {27},
keywords = {process mining, performance optimization, blockchains}
}

@article{10.1145/3580815,
author = {Wang, Yanfei and Yu, Zhiwen and Liu, Sicong and Zhou, Zimu and Guo, Bin},
title = {Genie in the Model: Automatic Generation of Human-in-the-Loop Deep Neural Networks for Mobile Applications},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3580815},
doi = {10.1145/3580815},
abstract = {Advances in deep neural networks (DNNs) have fostered a wide spectrum of intelligent mobile applications ranging from voice assistants on smartphones to augmented reality with smart-glasses. To deliver high-quality services, these DNNs should operate on resource-constrained mobile platforms and yield consistent performance in open environments. However, DNNs are notoriously resource-intensive, and often suffer from performance degradation in real-world deployments. Existing research strives to optimize the resource-performance trade-off of DNNs by compressing the model without notably compromising its inference accuracy. Accordingly, the accuracy of these compressed DNNs is bounded by the original ones, leading to more severe accuracy drop in challenging yet common scenarios such as low-resolution, small-size, and motion-blur. In this paper, we propose to push forward the frontiers of the DNN performance-resource trade-off by introducing human intelligence as a new design dimension. To this end, we explore human-in-the-loop DNNs (H-DNNs) and their automatic performance-resource optimization. We present H-Gen, an automatic H-DNN compression framework that incorporates human participation as a new hyperparameter for accurate and efficient DNN generation. It involves novel hyperparameter formulation, metric calculation, and search strategy in the context of automatic H-DNN generation. We also propose human participation mechanisms for three common DNN architectures to showcase the feasibility of H-Gen. Extensive experiments on twelve categories of challenging samples with three common DNN structures demonstrate the superiority of H-Gen in terms of the overall trade-off between performance (accuracy, latency), and resource (storage, energy, human labour).},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {36},
numpages = {29},
keywords = {reinforcement Learning, model generation, neural networks, Human in the Loop}
}

@inproceedings{10.1145/3472813.3473181,
author = {Alahmadi, Rawan and Almimony, Shoroog and Bahakeem, Rahaf and Alnahdi, Amany},
title = {Health Records Retrieval System: A Web-Service Approach},
year = {2021},
isbn = {9781450389846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472813.3473181},
doi = {10.1145/3472813.3473181},
abstract = {Patients’ electronic health records are archived and collected on data repositories of medical data systems. This research paper proposes a system based on web service architecture that allows retrieving medical records by health care centers associated to the system. There will be no need to open a file in every health care center associated to the system. In addition, the doctor can see the lab radiological results and medications from any hospital by considering security and privacy of medical data. The system will provide patients with urgent medical treatment when transferred to any hospital, as the hospital can access the patient's electronic health records to be informed with health status and diagnostic history. For example, a person had an accident and was transferred to any hospital; the hospital can access the patient's own electronic health records and find out his health status and medication the patient's uses. The built web service-based system will maintain privacy and security measures.},
booktitle = {Proceedings of the 5th International Conference on Medical and Health Informatics},
pages = {145–149},
numpages = {5},
keywords = {Health record, Medical record, Web service},
location = {Kyoto, Japan},
series = {ICMHI '21}
}

@inproceedings{10.1145/3477495.3531942,
author = {Zou, Xinyu and Hu, Zhi and Zhao, Yiming and Ding, Xuchu and Liu, Zhongyi and Li, Chenliang and Sun, Aixin},
title = {Automatic Expert Selection for Multi-Scenario and Multi-Task Search},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531942},
doi = {10.1145/3477495.3531942},
abstract = {Multi-scenario learning (MSL) enables a service provider to cater for users' fine-grained demands by separating services for different user sectors, e.g., by user's geographical region. Under each scenario there is a need to optimize multiple task-specific targets e.g., click through rate and conversion rate, known as multi-task learning (MTL). Recent solutions for MSL and MTL are mostly based on the multi-gate mixture-of-experts (MMoE) architecture. MMoE structure is typically static and its design requires domain-specific knowledge, making it less effective in handling both MSL and MTL. In this paper, we propose a novel Automatic Expert Selection framework for Multi-scenario and Multi-task search, named AESM2. AESM2 integrates both MSL and MTL into a unified framework with an automatic structure learning. Specifically, AESM2 stacks multi-task layers over multi-scenario layers. This hierarchical design enables us to flexibly establish intrinsic connections between different scenarios, and at the same time also supports high-level feature extraction for different tasks. At each multi-scenario/multi-task layer, a novel expert selection algorithm is proposed to automatically identify scenario-/task-specific and shared experts for each input. Experiments over two real-world large-scale datasets demonstrate the effectiveness of AESM2 over a battery of strong baselines. Online A/B test also shows substantial performance gain on multiple metrics. Currently, AESM2 has been deployed online for serving major traffic.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1535–1544},
numpages = {10},
keywords = {search and ranking, multi-task learning, multi-scenario learning},
location = {<conf-loc>, <city>Madrid</city>, <country>Spain</country>, </conf-loc>},
series = {SIGIR '22}
}

@inproceedings{10.1145/3578245.3584725,
author = {Tocz\'{e}, Klervie and Abad, Cristina L. and Herbst, Nikolas and Iosup, Alexandru},
title = {HotCloudPerf'23 Workshop Chairs' Welcome},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3584725},
doi = {10.1145/3578245.3584725},
abstract = {It is our great pleasure to welcome you to the 2023 edition of the Workshop on Hot Topics in Cloud Computing Performance - HotCloudPerf 2023.Cloud computing is emerging as one of the most profound changes in the way we build and use IT. The use of global services in public clouds is increasing, and the lucrative and rapidly growing global cloud market already supports over 1 million IT-related jobs. However, it is currently challenging to make the IT services offered by public and private clouds performant (in an extended sense) and efficient. Emerging architectures, techniques, and real-world systems include interactions with the computing continuum, serverless operation, everything as a service, complex workflows, auto-scaling and -tiering, etc. It is unclear to which extent traditional performance engineering, software engineering, and system design and analysis tools can help with understanding and engineering these emerging technologies. The community needs practical tools and powerful methods to address hot topics in cloud computing performance.Responding to this need, the HotCloudPerf workshop proposes a meeting venue for academics and practitioners, from experts to trainees, in the field of cloud computing performance. The workshop aims to engage this community and to lead to the development of new methodological aspects for gaining a deeper understanding not only of cloud performance, but also of cloud operation and behavior, through diverse quantitative evaluation tools, including benchmarks, metrics, and workload generators. The workshop focuses on novel cloud properties such as elasticity, performance isolation, dependability, and other non-functional system properties, in addition to classical performance-related metrics such as response time, throughput, scalability, and efficiency.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {257–258},
numpages = {2},
keywords = {benchmarking, cloud/edge computing, performance},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3477495.3531965,
author = {Liu, Yuli and Walder, Christian and Xie, Lexing},
title = {Determinantal Point Process Likelihoods for Sequential Recommendation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531965},
doi = {10.1145/3477495.3531965},
abstract = {Sequential recommendation is a popular task in academic research and close to real-world application scenarios, where the goal is to predict the next action(s) of the user based on his/her previous sequence of actions. In the training process of recommender systems, the loss function plays an essential role in guiding the optimization of recommendation models to generate accurate suggestions for users. However, most existing sequential recommendation tech- niques focus on designing algorithms or neural network architectures, and few efforts have been made to tailor loss functions that fit naturally into the practical application scenario of sequential recommender systems.  Ranking-based losses, such as cross-entropy and Bayesian Personalized Ranking (BPR) are widely used in the sequential recommendation area. We argue that such objective functions suffer from two inherent drawbacks: i) the dependencies among elements of a sequence are overlooked in these loss formulations; ii) instead of balancing accuracy (quality) and diversity, only generating accurate results has been over emphasized. We therefore propose two new loss functions based on the Determinantal Point Process (DPP) likelihood, that can be adaptively applied to estimate the subsequent item or items. The DPP-distributed item set captures natural dependencies among temporal actions, and a quality vs. diversity decomposition of the DPP kernel pushes us to go beyond accuracy-oriented loss functions. Experimental results using the proposed loss functions on three real-world datasets show marked improvements over state-of-the-art sequential recommendation methods in both quality and diversity metrics.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1653–1663},
numpages = {11},
keywords = {sequential recommendation, diversity, determinantal point process, loss function},
location = {<conf-loc>, <city>Madrid</city>, <country>Spain</country>, </conf-loc>},
series = {SIGIR '22}
}

@inproceedings{10.1145/3590837.3590904,
author = {E V, Sandeepkumar and Jayavel, Kayalvizhi},
title = {Effective and Light Weight Security System for Highly Confidential Cloud Data Such as PHR},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590904},
doi = {10.1145/3590837.3590904},
abstract = {The server in a cloud storage system can hold a very large amount of Personal health records (PHR) data or information. The cloud platform's storage servers provide archival services for a lengthy time frame. The third party basically functions as an administrator for the efficiency of cloud storage. This is why we're starting up our cloud storage service. One of the biggest difficulties with the cloud is that it is vulnerable to hacking. Ordinary methods of encryption are used to safeguard the information from prying eyes. All of the secret messages' code words are kept in a system of varying symbols. Deletion coding is carried out in a manner analogous to that which is used to calculate the unequal code word cyphers required for a communication in a distributed setting. When the message symbols are stored in different servers in a dispersed environment, the cryptographic term signs are also calculated independently and stored. For this reason, we introduce and include a threshold proxy re-encryption scheme. Fully Homomorphic Encryption is a promising approach to securing sensitive PHR data by limiting who can view it. When sending encrypted PHR data, the proxy re-encryption mechanism re-encrypts the PHR data again before sending it on to the recipient or storage server. Allocation is completed when secure access control has maximised performance. In light of this, we expect to see the Schmidt-Samoa Public Key Encryption (SSPKE) method developed on the Enhanced v Boosting Algorithm (EBA) by PHR data Hiding Architecture. Additionally, in this initiative, we employ a procedure of multi-party protocol admission control to operate and access the user's PHR data without jeopardising the sensitive cloud PHR data privacy. The results of the experiments show the beneficial effect when various metrics, such as total processing time, server response time, and PHR data decomposition rate, are taken into account for the application of PHR.},
booktitle = {Proceedings of the 4th International Conference on Information Management \&amp; Machine Intelligence},
articleno = {67},
numpages = {7},
keywords = {EBA, Re-encryption,Security, PHR, SSPKE},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@inproceedings{10.1145/3528535.3533273,
author = {Rocha, Isabelly and Felber, Pascal and Schiavoni, Valerio and Chen, Lydia},
title = {EdgeTune: Inference-Aware Multi-Parameter Tuning},
year = {2022},
isbn = {9781450393409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528535.3533273},
doi = {10.1145/3528535.3533273},
abstract = {Deep Neural Networks (DNNs) have demonstrated impressive performance on many machine-learning tasks such as image recognition and language modeling, and are becoming prevalent even on mobile platforms. Despite so, designing neural architectures still remains a manual, time-consuming process that requires profound domain knowledge. Recently, Parameter Tuning Servers have gathered the attention o industry and academia. Those systems allow users from all domains to automatically achieve the desired model accuracy for their applications. However, although the entire process of tuning and training models is performed solely to be deployed for inference, state-of-the-art approaches typically ignore system-oriented and inference-related objectives such as runtime, memory usage, and power consumption. This is a challenging problem: besides adding one more dimension to an already complex problem, the information about edge devices available to the user is rarely known or complete. To accommodate all these objectives together, it is crucial for tuning system to take a holistic approach to parameter tuning and consider all levels of parameters simultaneously into account. We present EdgeTune, a novel inference-aware parameter tuning server. It considers the tuning of parameters in all levels backed by an optimization function capturing multiple objectives. Our approach relies on inference estimated metrics collected from our emulation server running asynchronously from the main tuning process. The latter can then leverage the inference performance while still tuning the model. We propose a novel one-fold tuning algorithm that employs the principle of multi-fidelity and simultaneously explores multiple tuning budgets, which the prior art can only handle as suboptimal case of single type of budget. EdgeTune outputs inference recommendations to the user while improving tuning time and energy by at least 18\% and 53\% when compared to the baseline.},
booktitle = {Proceedings of the 23rd ACM/IFIP International Middleware Conference},
pages = {1–14},
numpages = {14},
keywords = {tuning, deep neural networks, training, inference},
location = {Quebec, QC, Canada},
series = {Middleware '22}
}

@inproceedings{10.1145/3565387.3565413,
author = {Kong, Xiangying and Kong, Xinran},
title = {Design of Embedded Trust Root Based on Dual-Kernel Architecture},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565387.3565413},
doi = {10.1145/3565387.3565413},
abstract = {Given the characteristics and design constraints of the embedded system, a software trust root construction method based on dual kernel architecture and composed of bootloader and trusted kernel and a stem branch trust chain transmission model are proposed ,aiming at the requirements of the trusted environment of embedded applications, The Bootloader, solidified in the boot FLASH, embeds the SHA-1 engine, to measure and load the trusted kernel. Meanwhile, the trusted kernel realizes the protection of the Bootloader by prohibiting the user kernel and upper-layer applications from writing access to the FLASH. The interaction between them, as the root of trust, can resist non-physical attacks; the trusted kernel provides password service-related functions for the user kernel; the application system and the user kernel where it is lockated run as a process of the trusted kernel. Finally, based on predicate logic, a formal proof of trusted boot is given, and a prototype system is built to verify the availability of the scheme.},
booktitle = {Proceedings of the 6th International Conference on Computer Science and Application Engineering},
articleno = {26},
numpages = {6},
keywords = {Embedded system, Trust root, Dual-kernel, Predicate logic},
location = {Virtual Event, China},
series = {CSAE '22}
}

@inproceedings{10.1145/3532213.3532283,
author = {Mahenge, Shadrack Fred and Wambura, Stephen and Jiao, Licheng},
title = {A Modified U-Net Architecture for Road Surfaces Cracks Detection},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532283},
doi = {10.1145/3532213.3532283},
abstract = {Cracks on road surfaces causes inconveniences to drivers and passengers and may cause mechanical failure or even accidents. Good Road condition plays an important role in quick transportation of goods and services from one place to another and acts as a catalyst for the economic development. Road surfaces need to be maintained in good condition to ensure the safety of road users. Road damage detection is important for Structural Health Monitoring (SHM). Traditional manual inspection is normally performed through human visualization which is time consuming, expensive, dangerous because of the passing vehicles, suffers from subjective judgment of the inspector and pose difficulties in keeping records for future road maintenance and repair. The rapid emergency and development of AI has stimulated many experts to automate the process of crack detection through computer vision (CV) technology, though most of these studies faces challenge on getting good detection accuracy. In this study a novel modified U-Net Architecture for image classification and segmentation is proposed to detect cracks on the road surfaces by using detection and classification of the road images to determine whether they represent cracks or not. Extensive experiments are conducted on three publicly available road crack datasets to evaluate the performance of our proposed model, The performance of the proposed Modified U-Net architecture was verified with respect to different performance metrics such as accuracy, precision, recall and f1 score. Qualitative and Quantitative comparisons experimental results of the proposed approach were also compared with existing state of the art U-Net architectures. It can be inferred from results that the proposed approach achieves superior performance in terms of detection accuracy.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {464–471},
numpages = {8},
keywords = {object detection, deep learning, machine learning, Image processing, computer vision},
location = {Tianjin, China},
series = {ICCAI '22}
}

@article{10.1145/3626196,
author = {kumari, Rani and Sah, Dinesh Kumar and Cengiz, Korhan and Ivkovi\'{c}, Nikola and Balaji, Prasanalakshmi},
title = {Automatic Graph Construction and Exploring Different Types of LSTMs for Asian Hindi Languages for Medical Review Sentiment Analysis},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3626196},
doi = {10.1145/3626196},
abstract = {Sentiment Analysis (SA) of medical reviews is crucial for improving healthcare outcomes. However, analyzing sentiment in low-resource languages such as Asian Hindi presents significant challenges. In this study, we propose an automatic graph construction approach to extract relevant features from medical reviews in Asian Hindi languages. We explore different types of Long Short-Term Memory (LSTMs), including traditional LSTMs, bidirectional LSTMs, and attention-based LSTMs, to classify the sentiment of medical reviews. Our proposed approach uses attention-based LSTM architecture and pre-trained Word2Vec embeddings to achieve high accuracy. We compare the proposed approach with existing models using various evaluation metrics, including accuracy, precision, recall, and F1-score. The results demonstrate that our proposed approach outperforms all existing models in terms of accuracy, achieving an accuracy score of 81\%. These findings could have implications for improving healthcare outcomes by enabling better monitoring of patient feedback and identifying areas for improvement in medical services.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {oct},
keywords = {Long short-term memory, Graph construction, Deep learning, Hindi language}
}

@inproceedings{10.1145/3579856.3595793,
author = {Zohaib, Ali and Sheffey, Jade and Houmansadr, Amir},
title = {Investigating Traffic Analysis Attacks on Apple ICloud Private Relay},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579856.3595793},
doi = {10.1145/3579856.3595793},
abstract = {The iCloud Private Relay (PR) is a new feature introduced by Apple in June 2021 that aims to enhance online privacy by protecting a subset of web traffic from both local eavesdroppers and websites that use IP-based tracking. The service is integrated into Apple’s latest operating systems and uses a two-hop architecture where a user’s web traffic is relayed through two proxies run by disjoint entities. PR’s multi-hop architecture resembles traditional anonymity systems such as Tor and mix networks. Such systems, however, are known to be susceptible to a vulnerability known as traffic analysis: an intercepting adversary (e.g., a malicious router) can attempt to compromise the privacy promises of such systems by analyzing characteristics (e.g., packet timings and sizes) of their network traffic. In particular, previous works have widely studied the susceptibility of Tor to website fingerprinting and flow correlation, two major forms of traffic analysis. In this work, we are the first to investigate the threat of traffic analysis against the recently introduced PR. First, we explore PR’s current architecture to establish a comprehensive threat model of traffic analysis attacks against PR. Second, we quantify the potential likelihood of these attacks against PR by evaluating the risks imposed by real-world AS-level adversaries through empirical measurement of Internet routes. Our evaluations show that some autonomous systems are in a particularly strong position to perform traffic analysis on a large fraction of PR traffic. Finally, having demonstrated the potential for these attacks to occur, we evaluate the performance of several flow correlation and website fingerprinting attacks over PR traffic. Our evaluations show that PR is highly vulnerable to state-of-the-art website fingerprinting and flow correlation attacks, with both attacks achieving high success rates. We hope that our study will shed light on the significance of traffic analysis to the current PR deployment, convincing Apple to perform design adjustments to alleviate the risks.},
booktitle = {Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
pages = {773–784},
numpages = {12},
keywords = {Traffic Analysis, Anonymity Systems, iCloud Private Relay},
location = {Melbourne, VIC, Australia},
series = {ASIA CCS '23}
}

@inproceedings{10.1145/3563766.3564093,
author = {Anand, SVR and Arslan, Serhat and Chopra, Rajat and Katti, Sachin and Vaddiraju, Milind Kumar and Rana, Ranvir and Sheng, Peiyao and Tyagi, Himanshu and Viswanath, Pramod},
title = {Trust-Free Service Measurement and Payments for Decentralized Cellular Networks},
year = {2022},
isbn = {9781450398992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563766.3564093},
doi = {10.1145/3563766.3564093},
abstract = {Decentralized cellular networks have emerged to increase network accessibility by distributing infrastructure ownership over independent entities. Unlike the centralized setting, these architectures can allow users to connect to any untrusted base station without prior subscription. However, verification of the service is necessary in the absence of trust for commensurate payments by the user. Further, any method of verification must be non-intrusive and reliably agreed upon by the involved parties. To this end, we describe two-sided measurements where both the users and the providers independently assess the cellular service. We find that reconciling measurements from different layers of the cellular stack for a diverse set of matching observations is challenging but not impossible. Hence, new use cases such as a decentralized slicing marketplace, and contract-free roaming can be enabled by two-sided measurements. We envision applying two-sided measurements to real-time, on-demand network slicing and present an architecture that is capable of offering, as well as verifying, such slices in a scalable manner.},
booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
pages = {68–75},
numpages = {8},
keywords = {cellular architecture, decentralization},
location = {Austin, Texas},
series = {HotNets '22}
}

@article{10.1145/3476248,
author = {Ebrahimi, Maryam and Tadayon, Mohammad Hesam and Haghighi, Mohammad Sayad and Jolfaei, Alireza},
title = {A Quantitative Comparative Study of Data-Oriented Trust Management Schemes in Internet of Things},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3476248},
doi = {10.1145/3476248},
abstract = {In the Internet of Things (IoT) paradigm, all entities in the IoT network, whether home users or industrial things, receive data from other things to make decisions. However, in the decentralized, heterogeneous, and rapidly changing IoT network with billions of devices, deciding about where to get the services or information from is critical, especially because malicious entities can exist in such an unmanaged network. Security provisioning alone cannot solve the issue of service quality or reliability. One way to elevate security and reliability in the IoT network is to bridge the gap of trust between objects, and also between humans and objects, while taking into account the IoT network characteristics. Therefore, a proper trust management system must be established on top of the IoT network service architecture. Trust is related to the manner expected from objects in providing services and recommendations. Recommendations are the basis of decision making in every trust management system. Since trust management ideas in the IoT are still immature, the purpose of this article is to survey, analyze, and compare the approaches that have been taken in building trust management systems for the IoT. We break down the features of such systems by analysis and also do quantitative comparisons by simulation. This article is organized into two main parts. First, studies and approaches in this field are compared from four perspectives: (1) trust computation method, (2) resistance to attacks (3) adherence to the limitations of IoT networks and devices, and (4) performance of the trust management scheme. The second part is quantitative and simulates four major methods in this field and measures their performance. We also make extensive analytical comparisons to demonstrate the similarities and discrepancies of current IoT trust management schemes and extract the essence of a resilient trust management framework.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {apr},
articleno = {24},
numpages = {30},
keywords = {trust management, cyber security, data mining, recommender systems, Internet of things, decision making}
}

@inproceedings{10.1145/3557915.3560948,
author = {Cuza, Carlos Enrique Muniz and Ho, Nguyen and Zacharatou, Eleni Tzirita and Pedersen, Torben Bach and Yang, Bin},
title = {Spatio-Temporal Graph Convolutional Network for Stochastic Traffic Speed Imputation},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557915.3560948},
doi = {10.1145/3557915.3560948},
abstract = {The rapid increase of traffic data generated by different sensing systems opens many opportunities to improve transportation services. An important opportunity is to enable stochastic routing that computes the arrival time probabilities for each suggested route instead of only the expected travel time. However, traffic datasets typically have many missing values, which prevents the construction of stochastic speeds. To address this limitation, we propose the Stochastic Spatio-Temporal Graph Convolutional Network (SST-GCN) architecture that accurately imputes missing speed distributions in a road network. SST-GCN combines Temporal Convolutional Networks and Graph Convolutional Networks into a single framework to capture both spatial and temporal correlations between road segments and time intervals. Moreover, to cope with datasets with many missing values, we propose a novel self-adaptive context-aware diffusion process that regulates the propagated information around the network, avoiding the spread of false information. We extensively evaluate the effectiveness of SST-GCN on real-world datasets, showing that it achieves from 4.6\% to 50\% higher accuracy than state-of-the-art baselines using three different evaluation metrics. Furthermore, multiple ablation studies confirm our design choices and scalability to large road networks.},
booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
articleno = {14},
numpages = {12},
keywords = {graph convolutional networks, spatio-temporal, data imputation},
location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
series = {SIGSPATIAL '22}
}

@inproceedings{10.1145/3487553.3527149,
author = {Oraby, Shereen},
title = {Stylistic Control for Neural Natural Language Generation},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3527149},
doi = {10.1145/3487553.3527149},
abstract = {With the rise of conversational assistants, it has become more critical for dialog systems to keep users engaged by responding in a natural, interesting, and often personalized way, even in a task-oriented setting. Recent work has thus focused on stylistic control for natural language generation (NLG) systems in order to jointly control response semantics and style. In this talk, I will describe our work on automatic data curation and modeling approaches to facilitate style control for both personality-specific attributes of style (based on Big-Five personality traits), and other style attributes that are helpful for personalization, e.g., response length, descriptiveness, point-of-view, and sentiment. I will present work that incorporates these attributes into the training and generation pipelines for different NLG architectures, and will show how our data curation and modeling approaches are generalizable to new domains and style choices. Finally, I will describe how we use a combination of automatic and human evaluation methods to measure how well models successfully hit multiple style targets without sacrificing semantics.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1179},
numpages = {1},
keywords = {natural language generation, stylistic variation, dialog systems},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3614321.3614322,
author = {Peniche, Eduardo and Miranda, Leandro and Bernardini, Flavia and Viterbo, Jose},
title = {FGT-SAMK-NN: Impact of the Right to Be Forgotten Using a Lazy Algorithm in Data Stream Learning},
year = {2023},
isbn = {9798400707421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614321.3614322},
doi = {10.1145/3614321.3614322},
abstract = {“Right to Be Forgotten" is guaranteed by new international regulations on personal management data. This means that individuals can request the erasure of their data from third-party tools and services. However, this poses a challenge for machine learning estimators, who will need to forget parts of their knowledge. This paper examines the impact of learning and forgetting policies in Data Stream Learning. Storing data or retraining learning models from scratch in data stream mining is usually not feasible due to the large volume of instances. Therefore, more efficient solutions are necessary to deal with the dynamic nature of online machine learning. To address this issue, we implemented FGT-SAMK-NN, an incremental version of one of the most knowledgeable algorithms in Data Stream lazy algorithms: The SAMK-NN classifier. FGT-SAMK-NN can erase its past data, and we investigate the impact of data forgetting on predictive performance. Our proposal is compared to the original SAMK-NN algorithm using four non-stationary stream datasets. Our results demonstrate that evaluation metrics did not undergo significant changes, which may support the idea that has a good architecture for adaptations of the proposed nature. However, it was also noted that the processing time is very high for cases involving more forgettings, which may indicate that the high complexity of the model creates conflicts if the pattern of data streams, where the algorithm is used, involves a high forgetfulness rate.},
booktitle = {Proceedings of the 16th International Conference on Theory and Practice of Electronic Governance},
pages = {1–8},
numpages = {8},
keywords = {Right to Be Forgotten, Lazy Learning, SamK-NN, Data Stream, Data Stream Learning},
location = {<conf-loc>, <city>Belo Horizonte</city>, <country>Brazil</country>, </conf-loc>},
series = {ICEGOV '23}
}

@inproceedings{10.1145/3517745.3561426,
author = {Sattler, Patrick and Aulbach, Juliane and Zirngibl, Johannes and Carle, Georg},
title = {Towards a Tectonic Traffic Shift? Investigating Apple's New Relay Network},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561426},
doi = {10.1145/3517745.3561426},
abstract = {Apple recently published its first Beta of the iCloud Private Relay, a privacy protection service with promises resembling the ones of VPNs. The architecture consists of two layers (ingress and egress), operated by disjoint providers. The service is directly integrated into Apple's operating systems, providing a low entry-level barrier for a large user base. It seems to be set up for significant adoption with its relatively moderate entry-level price.This paper analyzes the iCloud Private Relay from a network perspective, its effect on the Internet, and future measurement-based research. We perform EDNS0 Client Subnet DNS queries to collect ingress relay addresses and find 1586 IPv4 addresses. Supplementary RIPE Atlas DNS measurements reveal 1575 IPv6 addresses. Knowing these addresses helps to detect clients communicating through the relay network passively. According to our scans, ingress addresses grew by 20\% from January through April. Moreover, according to our RIPE Atlas DNS measurements, 5.3\% of all probes use a resolver that blocks access to iCloud Private Relay.The analysis of our scans through the relay network verifies Apple's claim of rotating egress addresses. Nevertheless, it reveals that ingress and egress relays can be located in the same autonomous system, thus sharing similar routes, potentially allowing traffic correlation.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {449–457},
numpages = {9},
keywords = {relay networks, DNS ECS enumeration, overlay networks},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3544216.3544238,
author = {Miao, Rui and Zhu, Lingjun and Ma, Shu and Qian, Kun and Zhuang, Shujun and Li, Bo and Cheng, Shuguang and Gao, Jiaqi and Zhuang, Yan and Zhang, Pengcheng and Liu, Rong and Shi, Chao and Fu, Binzhang and Zhu, Jiaji and Wu, Jiesheng and Cai, Dennis and Liu, Hongqiang Harry},
title = {From Luna to Solar: The Evolutions of the Compute-to-Storage Networks in Alibaba Cloud},
year = {2022},
isbn = {9781450394208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544216.3544238},
doi = {10.1145/3544216.3544238},
abstract = {This paper presents the two generations of storage network stacks that reduced the average I/O latency of Alibaba Cloud's EBS service by 72\% in the last five years: Luna, a user-space TCP stack that corresponds the latency of network to the speed of SSD; and Solar, a storage-oriented UDP stack that enables both storage and network hardware accelerations.Luna is our first step towards a high-speed compute-to-storage network in the "storage disaggregation" architecture. Besides the tremendous performance gains and CPU savings compared with the legacy kernel TCP stack, more importantly, it teaches us the necessity of offloading both network and storage into hardware and the importance of recovering instantaneously from network failures.Solar provides a highly reliable and performant storage network running on hardware. For avoiding hardware's resource limitations and offloading storage's entire data path, Solar eliminates the superfluous complexity and the overfull states from the traditional architecture of the storage network. The core design of Solar is unifying the concepts of network packet and storage data block - each network packet is a self-contained storage data block. There are three remarkable advantages to doing so. First, it merges the packet processing and storage virtualization pipelines to bypass the CPU and PCIe; Second, since the storage processes data blocks independently, the packets in Solar become independent. Therefore, the storage (in hardware) does not need to maintain receiving buffers for assembling packets into blocks or handling packet reordering. Finally, due to the low resource requirement and the resilience to packet reordering, Solar inherently supports large-scale multi-path transport for fast failure recovery. Facing the future, Solar demonstrates that we can formalize the storage virtualization procedure into a P4-compatible packet processing pipeline. Hence, SOLAR's design perfectly applies to commodity DPUs (data processing units).},
booktitle = {Proceedings of the ACM SIGCOMM 2022 Conference},
pages = {753–766},
numpages = {14},
keywords = {storage network, data processing unit, in-network acceleration},
location = {Amsterdam, Netherlands},
series = {SIGCOMM '22}
}

@article{10.1145/3485129,
author = {Meneguette, Rodolfo and De Grande, Robson and Ueyama, Jo and Filho, Geraldo P. Rocha and Madeira, Edmundo},
title = {Vehicular Edge Computing: Architecture, Resource Management, Security, and Challenges},
year = {2021},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3485129},
doi = {10.1145/3485129},
abstract = {Vehicular Edge Computing (VEC), based on the Edge Computing motivation and fundamentals, is a promising technology supporting Intelligent Transport Systems services, smart city applications, and urban computing. VEC can provide and manage computational resources closer to vehicles and end-users, providing access to services at lower latency and meeting the minimum execution requirements for each service type. This survey describes VEC’s concepts and technologies; we also present an overview of existing VEC architectures, discussing them and exemplifying them through layered designs. Besides, we describe the underlying vehicular communication in supporting resource allocation mechanisms. With the intent to overview the risks, breaches, and measures in VEC, we review related security approaches and methods. Finally, we conclude this survey work with an overview and study of VEC’s main challenges. Unlike other surveys in which they are focused on content caching and data offloading, this work proposes a taxonomy based on the architectures in which VEC serves as the central element. VEC supports such architectures in capturing and disseminating data and resources to offer services aimed at a smart city through their aggregation and the allocation in a secure manner.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {4},
numpages = {46},
keywords = {Vehicular edge computer, architecture, resource management, security}
}

@inproceedings{10.1145/3488663.3493688,
author = {Ueda, Kazuaki and Tagami, Atsushi},
title = {Internet Flattening and Consolidation Considered Useful (for Deploying New Internet Architecture)},
year = {2021},
isbn = {9781450391382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488663.3493688},
doi = {10.1145/3488663.3493688},
abstract = {Several new Internet architectures have been proposed to fill the gap between the original design of Internet and its current usage. These new architectures have been studied for more than 15 years, and their technical benefits have been widely validated. However, to date, these architectures have not been deployed in commercial networks. One of the reasons is that current Internet involves multiple players such as content providers and Internet Service Providers (ISPs), which makes it difficult to make significant changes. On the other hand, several studies have shown two trends of the current Internet, consolidation in web content delivery and flattening of the Internet topology. Web content delivery is dominated by the large Content Delivery Network (CDN) providers. Moreover, to improve communication quality, such providers connect directly to the eyeball ISPs, and this results in the flat topology. In this paper, we focus on whether these two trends, i.e., Internet flattening and consolidation, can ease the hurdle for deploying new architecture. Based on the measurements of DNS and network path, we verified the current trend of flattening and consolidation of content delivery on the Internet. We also investigated the incremental deployment scenario of new architecture under this environment. The results showed that a significant amount of traffic can be handled by a new architecture, if only a small set of autonomous systems cooperatively deploy it.},
booktitle = {Proceedings of the Interdisciplinary Workshop on (de) Centralization in the Internet},
pages = {11–17},
numpages = {7},
keywords = {Future Internet Architecture, Internet consolidation},
location = {Virtual Event, Germany},
series = {IWCI'21}
}

@inproceedings{10.1145/3487075.3487146,
author = {Zhang, Qingjun and Hu, Dong and Lin, Qiang},
title = {Design of High-Precision Island WebGIS Based on Cesium},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487146},
doi = {10.1145/3487075.3487146},
abstract = {In this paper, the design of a high-precision WebGIS system for island application is presented. The system is developed based on Cesium to support 2D, 2.5D and 3D map capabilities, and provide networked comprehensive geographic information service. Concerning the practical requirements for complicated configuration of island surface, improved methods for interpolation correction, data structure optimization, visible analysis and island path planning are introduced to improve system accuracy and performance. The system adopts B/S architecture and modular development ideas for easier access and further updates. The main functional modules of the island WebGIS provide basic operations, including multi-dimensional scene browsing, base map switching, multi-control operation, layer plotting, contour line, intervisibility and terrain factors measurement. Besides, the characteristic functions of key techniques such as profile analysis, viewshed analysis, and island path planning are implemented. The test examples show that the overall functional performance of the system is satisfactory for island 3D GIS service.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
articleno = {71},
numpages = {7},
keywords = {WebGIS, Geographic information system, Cesium, Island},
location = {Sanya, China},
series = {CSAE '21}
}

@inproceedings{10.1145/3508072.3508114,
author = {Ofure Eichie, Julia and Oluwamayowa Agidi, Emmanuel and David Oyedum, Onyendi},
title = {Atmospheric Temperature Prediction across Nigeria Using Artificial Neural Network},
year = {2022},
isbn = {9781450387347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508072.3508114},
doi = {10.1145/3508072.3508114},
abstract = {Atmospheric temperature is one of the dominating atmospheric parameters that impact on the propagation of radio waves through the troposphere. Adequate knowledge of the atmospheric temperature of an environment is therefore essential for radio wave propagation planning. In this study, thirty-four (34)-year (1981-2014) atmospheric temperature data of 10 selected weather stations across the climatic zones of Nigeria, obtained from the Nigerian Meteorological Agency (NIMET) through the data bank of the West African Science Service Centre on Climate Change and Adaptive Land Use (WASCAL) of the Federal University of Technology Minna, Nigeria was used in Artificial Neural Network (ANN) for the prediction of mean monthly atmospheric temperature. The ANN architecture comprised of 2 inputs (the climatic zones and the corresponding month for the mean monthly atmospheric temperature), 1 hidden layer and 1 output (atmospheric temperature). Levenberg-Marquardt algorithm was used with 9 different pairs of activation functions formed from 3 activation functions (logsig, purelin and tansig). The number of neurons in the hidden layer was varied from 33-39 with an increasing steps of 2 (33, 35, 37 and 39). The network architecture of 2-37-1 (2 inputs, 37 neurons in the hidden layer and 1 output), with tansig/tansig pair of activation functions had the least mean square error value of 2.2280, and was used for the prediction process. The computed correlation values for measured and predicted atmospheric temperature ranged from 0.9733 to 0.8787, depicting strong positive correlation and good accuracy of the developed model. Comparisons of the measured and the ANN predicted atmospheric temperature across selected stations in the climatic zones of Nigeria, showed that the developed model can effectively predict mean monthly atmospheric temperature, using month and climatic zone as input parameters.},
booktitle = {The 5th International Conference on Future Networks \&amp; Distributed Systems},
pages = {280–286},
numpages = {7},
keywords = {Temperature, artificial neural network, refractive index, prediction},
location = {Dubai, United Arab Emirates},
series = {ICFNDS 2021}
}

@article{10.1145/3567826,
author = {Viola, Roberto and Mart\'{\i}n, \'{A}ngel and Zorrilla, Mikel and Montalb\'{a}n, Jon and Angueira, Pablo and Muntean, Gabriel-Miro},
title = {A Survey on Virtual Network Functions for Media Streaming: Solutions and Future Challenges},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3567826},
doi = {10.1145/3567826},
abstract = {Media services must ensure an enhanced user’s perceived quality during content playback to attract and retain audiences, especially while the streams are distributed remotely via networks. Thus, media streaming services rely heavily on good and predictable network performance when delivered to a large number of people. Furthermore, as the quality of media content gets high, the network performance demands are also increasing, and meeting them is challenging. Network functions devoted to improving media streaming services become essential to cope with the high dynamics of network performance and user mobility. Furthermore, new networking paradigms and architectures under the 5G networks umbrella are bringing new possibilities to deploy smart network functions, which monitor the media streaming services through live and objective metrics and boost them in real-time. This survey overviews the state-of-the-art technologies and solutions proposed to apply new network functions for enhancing media streaming.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {221},
numpages = {37},
keywords = {Media streaming, network virtualization, network functions}
}

@inproceedings{10.1145/3578245.3585328,
author = {Iosup, Alexandru and Prodan, Radu},
title = {ICPE'23 GraphSys Workshop Chairs Introduction (Welcome)},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3585328},
doi = {10.1145/3578245.3585328},
abstract = {It is our great pleasure to welcome you to the 2023 ACM/SPEC Workshop on Serverless, Extreme-Scale, and Sustainable Graph Processing Systems. This is the first such workshop, aiming to facilitate the exchange of ideas and expertise in the broad field of high-performance large-scale graph processing.Graphs and GraphSys - The use, interoperability, and analytical exploitation of graph data are essential for modern digital economies. Today, thousands of computational methods (algorithms) and findable, accessible, interoperable, and reusable (FAIR) graph datasets exist. However, current computational capabilities lag when faced with the complex workflows involved in graph processing, the extreme scale of existing graph datasets, and the need to consider sustainability metrics in graph-processing operations. Needs are emerging for graph-processing platforms to provide multilingual information processing and reasoning based on the massive graph representation of extreme data in the form of general graphs, knowledge graphs, and property graphs. Because graph workloads and graph datasets are strongly irregular, and involve one or several big data "Vs" (e.g., volume, velocity, variability, vicissitude), the community needs to reconsider traditional approaches in performance analysis and modeling, system architectures and techniques, serverless and "as a service" operation, real-world and simulation-driven experimentation, etc., and provide new tools and instruments to address emerging challenges in graph processing.Graphs or linked data are crucial to innovation, competition, and prosperity and establish a strategic investment in technical processing and ecosystem enablers. Graphs are universal abstractions that capture, combine, model, analyze, and process knowledge about real and digital worlds into actionable insights through item representation and interconnectedness. For societally relevant problems, graphs are extreme data that require further technological innovations to meet the needs of the European data economy. Digital graphs help pursue the United Nations Sustainable Development Goals (UN SDG) by enabling better value chains, products, and services for more profitable or green investments in the financial sector and deriving trustworthy insight for creating sustainable communities. All science, engineering, industry, economy, and society-at-large domains can leverage graph data for unique analysis and insight, but only if graph processing becomes easy to use, fast, scalable, and sustainable.GraphSys is a cross-disciplinary meeting venue focusing on state-of-the-art and the emerging (future) graph processing systems. We invite experts and trainees in the field, across academia, industry, governance, and society, to share experience and expertise leading to a shared body of knowledge, to formulate together a vision for the field, and to engage with the topics to foster new approaches, techniques, and solutions.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {207–208},
numpages = {2},
keywords = {workshop, graph processing, graphsys serverless, extreme-scale, and sustainable graph processing systems},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3600211.3604754,
author = {Narayanan Venkit, Pranav},
title = {Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models Using an Interdisciplinary Lens},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604754},
doi = {10.1145/3600211.3604754},
abstract = {The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP. The first facet focuses on identifying sociodemographic bias in various NLP architectures, emphasizing the importance of considering both the models themselves and human computation to comprehensively understand and identify bias. In the second facet, we delve into the significance of establishing a shared vocabulary across different fields and disciplines involved in NLP. By highlighting the potential bias stemming from a lack of shared understanding, this facet emphasizes the need for interdisciplinary collaboration to bridge the gap and foster a more inclusive and accurate analysis of bias. Finally, the third facet investigates the development of a holistic solution by integrating frameworks from social science disciplines. This approach recognizes the complexity of bias in NLP and advocates for an interdisciplinary framework that goes beyond purely technical considerations, involving social and ethical perspectives to address bias effectively. The first facet includes the following of my published works [6, 7, 8, 9] to provide results into how the importance of understanding the presence of bias in various minority group that has not been in focus in the prior works of bias in NLP. The work also shows the need to create a method that considers both human and AI indicators of bias, showcasing the importance of the first facet of my research. In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups. The work also demonstrates the utility of BITS as a model-independent method of identifying bias by focusing on social groups instead. Expanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias. The above two works look at both the implicit and explicit nature of bias in NLP, showcasing the need to distinguish the efforts placed in understanding them. The results also demonstrate the utility of identifying such biases as it provides context to the black-box nature of such public models. As the field of NLP evolved from embedding-based models to large language models, the way these models are constructed underwent significant changes [5]. However, the concern arises from the fact that these models often reflect a populist viewpoint [1] that perpetuates majority-held ideas rather than objective truths. This difference in perception can lead to biases perpetuated by the majority’s worldview. To explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives. Additionally, I explore potential debiasing methods such as adversarial triggering and prompt engineering, demonstrating their efficacy in mitigating stereotype propagation through LLM models. While prior work predominantly relies on automatic indicators like sentiment scores or vector distances to identify bias [3], the next phase of my research emphasizes the importance of understanding biases through the lens of human readers [7], bringing to light the need for a human lens in understanding bias through human-aided indicators and mixed-method identification. By incorporating concepts of social computation, using human evaluation, we gain a better understanding of biases’ potential societal impact within the context of language models. To achieve this, I conduct open-ended interviews and employ qualitative coding and thematic analysis to comprehend the implications of biases on human readers. The findings demonstrate that biased NLP models tend to replicate and amplify existing societal biases, posing potential harm when utilized in sociotechnical settings. The qualitative analysis from the interviews provides valuable insights into readers’ experiences when encountering biased articles, highlighting the capacity to shift a reader’s perception of a country. These findings emphasize the critical role of public perception in shaping AI’s impact on society and the need to correct biases in AI systems. The second facet of my research aims to bridge the disparity between AI research and society. This disparity has resulted in a lack of shared understanding between these domains, leading to potential biases and harm toward specific groups. Employing an interdisciplinary approach that combines social informatics, philosophy, and AI, I will investigate the similarities and disparities in the concepts utilized by machine learning models. Existing research [2] highlights the insufficient interdisciplinary effort and motivation in comprehending social aspects of NLP. To commence this exploration, I will delve into the shared taxonomy of sentiment and fairness in natural language processing, sociology, and humanities. This research will first delve into the interdisciplinary nature of sentiment and its application in sentiment analysis models. Sentiment analysis, a popular machine learning application for text classification based on sentiment, opinion, and subjectivity, holds significant influence as a sociotechnical system that impacts both social and technical actors within a network. Nevertheless, the definition and connotation of sentiment vary vastly across different research fields, potentially leading to misconceptions regarding the utility of such systems. To address this issue, this study will examine how diverse fields, including psychology, sociology, and technology, define the concept of sentiment. By unraveling the divergent perspectives on sentiment within different fields, the paper will uncover discrepancies and varying applications of this interdisciplinary concept. Additionally, the research will survey commonly utilized sentiment analysis models, aiming to comprehend their standardized definitions and associated issues. Ultimately, the study will pose critical questions that should be considered during the development of social models to mitigate potential biases and harm stemming from an insufficiently defined comprehension of fundamental social concepts. Similar efforts will be dedicated to comprehending the disparity in bias and fairness as an interdisciplinary concept, shedding light on the imperative for inclusive research to cultivate superior AI models as sociotechnical solutions. The third facet of my study embarks upon an exploration of the intricate interplay between human and AI actors, employing the formidable theoretical lens of actor-network theory (ANT). Through the presentation of a robust framework, this facet aims to engender the formation of efficacious development networks that foster collaboration among developers, practitioners, and other essential stakeholders. Such inclusive networks serve as crucibles for the cultivation of holistic solutions that transcend the discriminatory trappings afflicting specific populations. A tangible outcome of this endeavor entails the creation of an all-encompassing bias analysis platform, poised to guide the discernment and amelioration of an array of sociodemographic biases manifesting within any machine-learning system. By catalyzing the development of socially aware and less pernicious technology, this research makes a substantial contribution to the realms of NLP and AI. The significance of this proposed research reverberates beyond the confines of NLP, resonating throughout the broader domain of AI, wherein analogous challenges about social biases loom large. Leveraging the proposed framework, developers, practitioners, and policymakers are empowered to forge practical solutions that embody inclusivity and reliability, especially when used as a service (AIaaS). Moreover, the platform serves as a centralized locus for the identification and rectification of social biases, irrespective of the underlying model or architecture. By furnishing a cogent narrative that underscores the imperative for a comprehensive and interdisciplinary approach, my work strives to propel the ongoing endeavors to comprehend and mitigate biases within the realm of NLP. With its potential to augment the equity, inclusivity, and societal ramifications of NLP technologies, the proposed framework catapults the field towards responsible and ethical practices.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1004–1005},
numpages = {2},
location = {<conf-loc>, <city>Montr\'{e}al</city>, <state>QC</state>, <country>Canada</country>, </conf-loc>},
series = {AIES '23}
}

@inproceedings{10.1145/3581783.3613777,
author = {Fernandez, Sergi and Montagud, Mario and Rinc\'{o}n, David and Moragues, Juame and Cernigliaro, Gianluca},
title = {Addressing Scalability for Real-Time Multiuser Holo-Portation: Introducing and Assessing a Multipoint Control Unit (MCU) for Volumetric Video},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613777},
doi = {10.1145/3581783.3613777},
abstract = {Scalability, interoperability, and cost efficiency are key remaining challenges to successfully providing real-time holo-portation (and Metaverse-like) services. This paper, for the first time, presents the design and integration of a Multipoint Control Unit (MCU) in a pioneering real-time holo-portation platform, supporting realistic and volumetric user representations (i.e., 3D holograms), with the aim of overcoming such challenges. The feasibility and implications of adopting such an MCU, in comparison with state-of-the-art architectural approaches, are assessed through experimentation in two different deployment setups, by iteratively increasing the number of concurrent users in shared sessions. The obtained results are promising, as it is empirically proved that the newly adopted stream multiplexing together with the novel per-client and per-frame Volumetric Video (VV) processing optimization features provided by the MCU allow increasing the number of concurrent users, while: (i) significantly reducing resources consumption metrics (e.g., CPU, GPU, bandwidth) and frame rate degradation on the client side; and (ii) keeping the end-to-end latency within acceptable limits.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9243–9251},
numpages = {9},
keywords = {multipoint control unit (mcu), social vr, virtual reality (vr), volumetric video, cloud computing, holo-portation},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{10.1145/3426020.3426111,
author = {Rehman, Osama and Farrukh, Zaroon and Al-Busaidi, Asiya and Cha, Kyungjin and Park, Simon and Rahman, Ibrahim},
title = {IoT Powered Cancer Observation System.},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426111},
doi = {10.1145/3426020.3426111},
abstract = {Cancer is a global challenge and the second leading cause of death worldwide as reported by the World Health Organization. With the current global pandemic caused by the novel coronavirus, cancer patients are identified as having increased risk of mortality. With the growing number of cancer patients every year, the need for a continuous and round the clock observation system has become quite imperative. An Internet of Things (IoT) based system for monitoring cancer patients has the potential to timely detect cancer related symptoms in its early stages, to continuously monitor cancer diagnosed patients and to monitor those that got cured for post-treatment measures. This paper proposes a multi-layered architecture of an IoT-based cancer observation system that can be utilized as a platform to remotely diagnose and monitor cancer patients. An implementation framework of the proposed system is also presented is this work, along with a prototype design of a Patient Side Unit (PSU) represented by a wearable wrist band. The proposed system has the potential to be applied as a solution for reducing expensive and exhausting hospital visits, while gaining similar quality of medical services when residing at home.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {313–318},
numpages = {6},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@inproceedings{10.1145/3489525.3511678,
author = {Balsamo, Simonetta and Marin, Andrea and Mitrani, Isi},
title = {A Mixed PS-FCFS Policy for CPU Intensive Workloads},
year = {2022},
isbn = {9781450391436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489525.3511678},
doi = {10.1145/3489525.3511678},
abstract = {Round robin (RR) is a widely adopted scheduling policy in modern computer systems. The scheduler handles the concurrency by alternating the run processes in such a way that they can use the processor continuously for at most a quantum of time. When the processor is assigned to another process, a context switch occurs. Although modern architectures handle context switches quite efficiently, the processes may incur in some indirect costs mainly due to cache overwriting.RR is widely appreciated both in case of interactive and CPU intensive processes. In the latter case, with respect to the First-Come-First-Served approach (FCFS), RR does not penalise the small jobs.In this paper, we study a scheduling policy, namely PS-FCFS, that fixes a maximum level of parallelism N and leaves the remaining jobs in a FCFS queue. The idea is that of exploiting the advantages of RR without incurring in heavy slowdowns because of context switches.We propose a queueing model for PS-FCFS allowing us to: (i) find the optimal level of multiprogramming and (ii) study important properties of this policy such as the mean performance measures and results about its sensitivity to the moments of the jobs' service demands.},
booktitle = {Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering},
pages = {199–210},
numpages = {12},
keywords = {context switch, scheduling, response time optimisation, queueing systems},
location = {Beijing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3594739.3610725,
author = {Ma, Ling and Zhang, Runting and Shi, Xiaohua},
title = {Experience: Large Scale Indoor Location-Based Service in Libraries},
year = {2023},
isbn = {9798400702006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594739.3610725},
doi = {10.1145/3594739.3610725},
abstract = {This paper presents the experience in the implementation and application of a large-scale Indoor Location-Based Service (LBS) in an academic library. We deployed an indoor positioning system within the school library that leverages 550 Bluetooth beacons, covering an area of approximately 12,000 square meters. This system allows users to engage with location-aware book navigation services via a web interface on their mobile devices. Upon locating a book of interest, the system enters navigation mode, updating and guiding users based on their current location within the library. Utilizing the users’ positional data and borrowing history, the system is capable of recommending other potentially interesting books, exhibits, and library events. The system went live in February 2023, with a recorded usage sessions of 21,540 instances which includes 17,271 uses of the retrieval services and 2,065 indoor navigation services, as of May 2023. This paper outlines the system architecture of this large-scale indoor LBS, shares user engagement data, and after anonymization, makes this data publicly available for academic analysis and research. We are hoping that our experience can shed light on the understanding and future development of large-scale indoor LBS technologies.},
booktitle = {Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing \&amp; the 2023 ACM International Symposium on Wearable Computing},
pages = {391–395},
numpages = {5},
keywords = {location-based service, library},
location = {<conf-loc>, <city>Cancun, Quintana Roo</city>, <country>Mexico</country>, </conf-loc>},
series = {UbiComp/ISWC '23 Adjunct}
}

@inproceedings{10.1145/3447993.3483239,
author = {Wang, Sihan and Tu, Guan-Hua and Lei, Xinyu and Xie, Tian and Li, Chi-Yu and Chou, Po-Yi and Hsieh, Fucheng and Hu, Yiwen and Xiao, Li and Peng, Chunyi},
title = {Insecurity of Operational Cellular IoT Service: New Vulnerabilities, Attacks, and Countermeasures},
year = {2021},
isbn = {9781450383424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447993.3483239},
doi = {10.1145/3447993.3483239},
abstract = {More than 150 cellular networks worldwide have rolled out massive IoT services such as smart metering and environmental monitoring. Such cellular IoT services share the existing cellular network architecture with non-IoT (e.g., smartphone) ones. When they are newly integrated into the cellular network, new security vulnerabilities may happen from imprudent integration. In this work, we explore the security vulnerabilities of the cellular IoT from both system-integrated and service-integrated aspects. We discover five vulnerabilities spanning cellular standard design defects, network operation slips, and IoT device implementation flaws. Threateningly, they allow an adversary to remotely identify IP addresses and phone numbers assigned to cellular IoT devices and launch data/text spamming attacks against them. We experimentally validate these vulnerabilities and attacks with three major U.S. IoT carriers. The attack evaluation result shows that the adversary can raise an IoT data bill by up to $226 with less than 120 MB spam traffic and increase an IoT text bill at a rate of $5 per second; moreover, cellular IoT devices may suffer from denial of IoT services. We finally propose, prototype, and evaluate recommended solutions.},
booktitle = {Proceedings of the 27th Annual International Conference on Mobile Computing and Networking},
pages = {437–450},
numpages = {14},
keywords = {cellular IoT, security, service charging},
location = {New Orleans, Louisiana},
series = {MobiCom '21}
}

@inproceedings{10.1145/3492547.3492589,
author = {A. Alhamdi, Nada and S. Ahmeda, Shubat},
title = {Mobile WiMAX Network Optimization and Performance Analysis},
year = {2021},
isbn = {9781450390446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492547.3492589},
doi = {10.1145/3492547.3492589},
abstract = {Mobile WiMAX is a fast growing Broadband Wireless Access (BWA) technology that gives a flexible and cost-effective solution to fixed broadband access problems such as the lacking of support for terminal mobility. Mobile WiMAX enables low-cost mobile internet applications, realizes the convergence of mobile and fixed broadband access in single air interface and network architecture. Although Mobile WiMAX has a lot of features such as high data rate, quality of service, scalability, high security and of course mobility, there are some problems that can be identified within the network; two major problems include signal interference and network coverage. In this paper, Mobile WiMAX Network Optimization is implemented to improve the interference control, enable effective coverage and enhance user experience to meet new business development needs and maximize profits. The paper is supported by measured real data captured in an urban environment using vehicular drive tests and other tools. Analysis shows that Mobile WiMAX is able to achieve a high-level improvement in network coverage and signal quality.},
booktitle = {The 7th International Conference on Engineering \&amp; MIS 2021},
articleno = {18},
numpages = {6},
location = {Almaty, Kazakhstan},
series = {ICEMIS'21}
}

@inproceedings{10.1145/3485447.3512026,
author = {Balsebre, Pasquale and Yao, Dezhong and Cong, Gao and Hai, Zhen},
title = {Geospatial Entity Resolution},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512026},
doi = {10.1145/3485447.3512026},
abstract = {A geospatial database is today at the core of an ever increasing number of services. Building and maintaining it remains challenging due to the need to merge information from multiple providers. Entity Resolution (ER) consists of finding entity mentions from different sources that refer to the same real world entity. In geospatial ER, entities are often represented using different schemes and are subject to incomplete information and inaccurate location, making ER and deduplication daunting tasks. While tremendous advances have been made in traditional entity resolution and natural language processing, geospatial data integration approaches still heavily rely on static similarity measures and human-designed rules. In order to achieve automatic linking of geospatial data, a unified representation of entities with heterogeneous attributes and their geographical context, is needed. To this end, we propose Geo-ER1, a joint framework that combines Transformer-based language models, that have been successfully applied in ER, with a novel learning-based architecture to represent the geospatial character of the entity. Different from existing solutions, Geo-ER does not rely on pre-defined rules and is able to capture information from surrounding entities in order to make context-based, accurate predictions. Extensive experiments on eight real world datasets demonstrate the effectiveness of our solution over state-of-the-art methods. Moreover, Geo-ER proves to be robust in settings where there is no available training data for a specific city.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3061–3070},
numpages = {10},
keywords = {Entity resolution, neural networks, neighbourhood embedding, geospatial data, graph attention},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3563766.3564107,
author = {Birge-Lee, Henry and Apostolaki, Maria and Rexford, Jennifer},
title = {It Takes Two to Tango: Cooperative Edge-to-Edge Routing},
year = {2022},
isbn = {9781450398992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563766.3564107},
doi = {10.1145/3563766.3564107},
abstract = {In their unrelenting quest for lower latency, cloud providers are deploying servers closer to their customers and enterprises are adopting paid Network-as-a-Service (NaaS) offerings with performance guarantees. Unfortunately, these trends contribute to greater industry consolidation, benefiting larger companies and well-served regions while leaving little room for smaller cloud providers and enterprises to flourish. Instead, we argue that the public Internet could offer good enough performance, if only edge networks could work together to achieve better visibility and control over wide-area routing. We present Tango, a cooperative architecture where pairs of edge networks (e.g., access, enterprise, and data-center networks) collaborate to expose more wide-area paths, collect more accurate measurements, and split traffic more intelligently over the paths. Tango leverages programmable switches at the borders of the edge networks, coupled with techniques to coax BGP into exposing more paths, without requiring support from end hosts or intermediate ASes. Experiments with our preliminary Tango deployment (using IPv6 addresses and the Vultr cloud provider) show that Tango could offer much greater visibility and control over wide-area routing, allowing the public Internet to meet the needs of many modern networked applications.},
booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
pages = {174–180},
numpages = {7},
keywords = {SDN, BGP, network measurement, multipath routing},
location = {Austin, Texas},
series = {HotNets '22}
}

@inproceedings{10.1145/3524844.3528081,
author = {Quin, Federico and Weyns, Danny},
title = {SEAByTE: A Self-Adaptive Micro-Service System Artifact for Automating A/B Testing},
year = {2022},
isbn = {9781450393058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524844.3528081},
doi = {10.1145/3524844.3528081},
abstract = {Micro-services are a common architectural approach to software development today. An indispensable tool for evolving micro-service systems is A/B testing. In A/B testing, two variants, A and B, are applied in an experimental setting. By measuring the outcome of an evaluation criterion, developers can make evidence-based decisions to guide the evolution of their software. Recent studies highlight the need for enhancing the automation when such experiments are conducted in iterations. To that end, we contribute a novel artifact that aims at enhancing the automation of an experimentation pipeline of a micro-service system relying on the principles of self-adaptation. Concretely, we propose SEAByTE, an experimental framework for testing novel self-adaptation solutions to enhance the automation of continuous A/B testing of a micro-service based system. We illustrate the use of the SEAByTE artifact with a concrete example.},
booktitle = {Proceedings of the 17th Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {77–83},
numpages = {7},
location = {Pittsburgh, Pennsylvania},
series = {SEAMS '22}
}

@inproceedings{10.1145/3544793.3563400,
author = {Arhab, Nabil and Oussalah, Mourad and Jutila, Johannes and Outila, Tarja},
title = {Toward Car Parking Wellbeing Index.},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3563400},
doi = {10.1145/3544793.3563400},
abstract = {With the actual development in urban life by the emerging trends around the smart city, along with cars yet considered as the primary source for commuting by city visitors, the parking infrastructures and parking lots management recreates a role in providing an easy mobility service with the sustainability of land use for the city. This ultimately impacted the driver well being. This work provides a foundation for measuring the Parking Wellbeing Index as a tailored refocus of the sustainability society index that takes into account the car parking experience. The framework makes use of the Occupancy rate of the parking site and traffic flow in the vicinity of the parking site, as well as the environmental aspect, which includes both the business/economical dimension and the Green (nature) dimension. We demonstrated how the developed index could be applied in Oulu City center, where a set of experiments have been conducted and tested. Especially, software architecture has been devised to automatically retrieve traffic and parking occupancy using the City of Oulu open data platform. While land use classification has been used to assess the Green and Business aspects of the parking area. The results could help the municipality better manage the buildings and the landscape in a way to maximize community wellbeing.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {153–158},
numpages = {6},
keywords = {data analytic, wellbeing, car parking, land use},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@article{10.1145/3490754,
author = {Robol, Marco and Breaux, Travis D. and Paja, Elda and Giorgini, Paolo},
title = {Consent Verification Monitoring},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3490754},
doi = {10.1145/3490754},
abstract = {Advances in personalization of digital services are driven by low-cost data collection and processing, in addition to the wide variety of third-party frameworks for authentication, storage, and marketing. New privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, increasingly require organizations to explicitly state their data practices in privacy policies. When data practices change, a new version of the policy is released. This can occur a few times a year, when data collection or processing requirements are rapidly changing. Consent evolution raises specific challenges to ensuring GDPR compliance. We propose a formal consent framework to support organizations, data users, and data subjects in their understanding of policy evolution under a consent regime that supports both the retroactive and non-retroactive granting and withdrawal of consent. The contributions include (i) a formal framework to reason about data collection and access under multiple consent granting and revocation scenarios, (ii) a scripting language that implements the consent framework for encoding and executing different scenarios, (iii) five consent evolution use cases that illustrate how organizations would evolve their policies using this framework, and (iv) a scalability evaluation of the reasoning framework. The framework models are used to verify when user consent prevents or detects unauthorized data collection and access. The framework can be integrated into a runtime architecture to monitor policy violations as data practices evolve in real time. The framework was evaluated using the five use cases and a simulation to measure the framework scalability. The simulation results show that the approach is computationally scalable for use in runtime consent monitoring under a standard model of data collection and access and practice and policy evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {2},
numpages = {33},
keywords = {analysis, formal framework, retroactivity, logs, consent, verification, consent revocation, Privacy, GDPR, evolution, description logic}
}

@inproceedings{10.1145/3599957.3606221,
author = {Jeong, Bomi and Lee, Sungwon and Siddiqa, Ayesha and Ajmal, Mahnoor and Seo, Junho and Kim, Dongkyun},
title = {Experimental Analysis of Handover Process in Cell-Free Networks},
year = {2023},
isbn = {9798400702280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599957.3606221},
doi = {10.1145/3599957.3606221},
abstract = {With the emergence of services that require a high level of data usage, wireless networks are required to support higher data rates and greater capacity. To satisfy these requests, 5G architecture supports a high-frequency band. However, a high frequency occurs small range resulting in a need for numerous base stations (BSs). Deploying numerous BSs can be considered practically impossible due to high costs. To address the cost issue, instead of BSs, access points (APs) are used in Cell-free networks. That is, Cell-free networks that offer promising coverage gain and enhanced data rates consist of multiple APs and a single central process unit (CPU). This coverage of multiple APs' is much smaller than BS's. As a result, this small coverage can lead to frequent handovers. To overcome this frequent handover, an optimized handover scheme for the Cell-free network is required. Despite this optimization requirement, many studies in the Cell-free network field remain at the physical layer stage. No basic handover process is optimized for Cell-free networks, and no protocol for upper layers is defined. We consider that existing technologies will continue to be used in Cell-free networks, even if the protocols optimized for Cellfree networks are developed and used. For these reasons, we construct a Cell-free network architecture similar to the 5G structure for the handover process. In this paper, we investigate the handover process in 5G architecture and extend it to a Cellfree network environment. In our simulations, APs initiate and perform the handover process based on measurement reports from users. We analyzed and discussed the performance matrices of the handover in a Cell-free network environment, such as handover delay and throughput.},
booktitle = {Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems},
articleno = {11},
numpages = {7},
keywords = {Cell-free networks, 5G networks, Handover},
location = {Gdansk, Poland},
series = {RACS '23}
}

@inproceedings{10.1145/3551626.3564980,
author = {Lin, Jingyu and Yan, Yan and Wang, Hanzi},
title = {An End-to-End Scene Text Detector with Dynamic Attention},
year = {2022},
isbn = {9781450394789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551626.3564980},
doi = {10.1145/3551626.3564980},
abstract = {Detecting the arbitrarily oriented text in natural images is a challenging task in multimedia due to variations in text curvatures, orientations, and aspect ratios of natural scenes. Most previous scene text detectors often fail to locate the text instances which have a peculiar shape (an extreme aspect ratio) precisely. In this paper, we propose a dynamic end-to-end framework (DEF) which includes a convolution-based dynamic encoder (CDE) with various attention types to generate a deformable and dynamic view for multi-oriented text instances and curve ones. Different from previous methods that apply time-consuming post-processing steps like NMS, our method uses a Transformer-based decoder (TD) with a bipartite matching loss to model the relationship of corresponding queries and ground truths. As a result, by leveraging such a well-designed architecture, the receptive field will not be limited to a fixed shape, and a combination of global attention and local features provides a better representation for texts in natural scenes. We conduct extensive experiments qualitatively and quantitatively on several popular datasets. Experimental results show that the proposed method achieves superior performance compared with several state-of-the-art scene text detectors.},
booktitle = {Proceedings of the 4th ACM International Conference on Multimedia in Asia},
articleno = {9},
numpages = {7},
keywords = {scene text detection, transformer architecture, dynamic encoder},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {MMAsia '22}
}

@inproceedings{10.1145/3543829.3544525,
author = {Farah, Juan Carlos and Spaenlehauer, Basile and Ingram, Sandy and Gillet, Denis},
title = {A Blueprint for Integrating Task-Oriented Conversational Agents in Education},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543829.3544525},
doi = {10.1145/3543829.3544525},
abstract = {Over the past few years, there has been an increase in the use of chatbots for educational purposes. Nevertheless, the chatbot technologies and architectures that are often applied to educational contexts are not necessarily designed for such contexts. While general-purpose chatbot technologies can be used in educational contexts, there are some challenges specific to these contexts that need to be taken into consideration. Namely, chatbot technologies intended for education should, by design, integrate directly within online learning applications and focus on achieving learning goals by supporting learners with the task at hand. In this paper, we propose a blueprint for an architecture specifically aimed at integrating task-oriented chatbots to support learners in educational contexts. We then present a proof-of-concept implementation of our blueprint as a part of a code review application designed to teach programming best practices. Our blueprint could serve as a starting point for developers in education looking to build chatbot technologies targeting educational contexts and is a first step toward an open chatbot architecture explicitly tailored for learning applications.},
booktitle = {Proceedings of the 4th Conference on Conversational User Interfaces},
articleno = {34},
numpages = {8},
keywords = {digital education, software architecture, task-oriented interactions, chatbots, online learning, conversational agents},
location = {Glasgow, United Kingdom},
series = {CUI '22}
}

@inproceedings{10.1145/3460120.3484774,
author = {Xu, Fenghao and Shen, Siyu and Diao, Wenrui and Li, Zhou and Chen, Yi and Li, Rui and Zhang, Kehuan},
title = {Android on PC: On the Security of End-User Android Emulators},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484774},
doi = {10.1145/3460120.3484774},
abstract = {Android emulators today are not only acting as a debugging tool for developers but also serving the massive end-users. These end-user Android emulators have attracted millions of users due to their advantages of running mobile apps on desktops and are especially appealing for mobile game players who demand larger screens and better performance. Besides, they commonly provide some customized assistant functionalities to improve the user experience, such as keyboard mapping and app installation from the host. To implement these services, emulators inevitably introduce communication channels between host OS and Android OS (in the Virtual Machine), thus forming a unique architecture which mobile phone does not have. However, it is unknown whether this architecture brings any new security risks to emulators.This paper performed a systematic study on end-user Android emulators and discovered a series of security flaws on communication channel authentication, permission control, and open interfaces. Attackers could exploit these flaws to bypass Android security mechanisms and escalate their privileges inside emulators, ultimately invading users' privacy, such as stealing valuable game accounts and credentials. To understand the impact of our findings, we studied six popular emulators and measured their flaws. The results showed that the issues are pervasive and could cause severe security consequences. We believe our work just shows the tip of the iceberg, and further research can be done to improve the security of this ecosystem.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1566–1580},
numpages = {15},
keywords = {security assessment, android emulator},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3590777.3590778,
author = {Castillo-Fern\'{a}ndez, Elvira and D\'{\i}az-Verdejo, Jes\'{u}s and Estepa Alonso, Rafael and Estepa Alonso, Antonio and Mu\~{n}oz Calle, Javier and Mabinabeitia, Germ\'{a}n},
title = {Multistep Cyberattacks Detection Using a Flexible Multilevel System for Alerts and Events Correlation},
year = {2023},
isbn = {9781450398299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590777.3590778},
doi = {10.1145/3590777.3590778},
abstract = {Current network monitoring systems tend to generate several alerts per attack, especially in multistep attacks. However, Cybersecurity Officers (CSO) would rather receive a single alert summarizing the entire incident. Triggering a single alert per attack is a challenge that requires developing and evaluating advanced event correlation techniques and models to determine the relationships between the different observed events/alerts. In this work, we propose a flexible architecture oriented toward the correlation and aggregation of events and alerts in a multilevel iterative approach. In our scheme, sensors generate events and alerts that are stored in a non-relational database queried by modules that create knowledge structured as meta-alerts that are also stored in the database. These meta-alerts (also called hyperalerts) are, in turn, used iteratively to create new knowledge. This iterative approach can be used to aggregate information at multiple levels or steps in complex attack models. Our architecture also allows the incorporation of additional sensors and the evaluation of various correlation techniques and multistage attack models. The capabilities of the system are assessed through three case studies.},
booktitle = {Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference},
pages = {1–6},
numpages = {6},
keywords = {Intrusion Detection Systems, cyberattacks models, attack models, alert correlation, network security monitoring},
location = {Stavanger, Norway},
series = {EICC '23}
}

@inproceedings{10.1145/3587135.3592168,
author = {Sahita, Ravi and Shanbhogue, Vedvyas and Bresticker, Andrew and Khare, Atul and Patra, Atish and Ortiz, Samuel and Reid, Dylan and Kanwal, Rajnesh},
title = {CoVE: Towards Confidential Computing on RISC-V Platforms},
year = {2023},
isbn = {9798400701405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587135.3592168},
doi = {10.1145/3587135.3592168},
abstract = {Multi-tenant computing platforms are typically comprised of several software and hardware components including platform firmware, operating system, virtualization monitor, and tenant workloads (typically in a virtual machine, container, or application). This model is well established in large scale commercial deployments, but the downside is that all platform components and operators are in the Trusted Computing Base (TCB) of the tenant. This aspect is ill-suited for privacy-oriented workloads that aim to minimize the TCB. Confidential computing [1] presents a good stepping-stone towards providing a quantifiable TCB for computing. Confidential computing requires the use of a HW-attested Trusted Execution Environment for data-in-use protection. The RISC-V architecture presents a strong foundation for meeting the requirements for Confidential Computing in a clean slate manner. This paper describes a reference architecture and discusses ISA, non-ISA and System-on-Chip (SoC) requirements for confidential computing on RISC-V Platforms. It discusses proposed RISC-V ISA and non-ISA for Confidential Virtual-machine Extension, referred to as CoVE.},
booktitle = {Proceedings of the 20th ACM International Conference on Computing Frontiers},
pages = {315–321},
numpages = {7},
keywords = {Confidential Computing, Attestation, Virtualization, Security},
location = {Bologna, Italy},
series = {CF '23}
}

@inproceedings{10.1145/3576915.3616611,
author = {Kim, Jason and van Schaik, Stephan and Genkin, Daniel and Yarom, Yuval},
title = {ILeakage: Browser-Based Timerless Speculative Execution Attacks on Apple Devices},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3616611},
doi = {10.1145/3576915.3616611},
abstract = {Over the past few years, the high-end CPU market is undergoing a transformational change. Moving away from using x86 as the sole architecture for high performance devices, we have witnessed the introduction of heavy-weight Arm CPUs computing devices. Among these, perhaps the most influential was the introduction of Apple's M-series architecture, aimed at completely replacing Intel CPUs in the Apple ecosystem. However, while significant effort has been invested analyzing x86 CPUs, the Apple ecosystem remains largely unexplored.In this paper, we set out to investigate the resilience of the Apple ecosystem to speculative side-channel attacks. We first establish the basic toolkit needed for mounting side-channel attacks, such as the structure of caches and CPU speculation depth. We then tackle Apple's degradation of the timer resolution in both native and browser-based code. Remarkably, we show that distinguishing cache misses from cache hits can be done without time measurements, replacing timing based primitives with timerless counterparts based on race conditions. Finally, we use our distinguishing primitive to construct eviction sets and mount Spectre attacks, all while avoiding the use of timers.We then evaluate Safari's side-channel resilience. We bypass the compressed 35-bit addressing and the value poisoning countermeasures, creating a primitive that can speculatively read and leak any 64-bit address within Safari's rendering process. Combining this with a new method for consolidating websites from different domains into the same renderer process, we demonstrate end-to-end attacks leaking sensitive information, such as passwords, inbox content, and locations from popular services such as Google.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2038–2052},
numpages = {15},
keywords = {apple silicon, side-channel attacks, timerless channels, spectre},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@inproceedings{10.1145/3488042.3488053,
author = {Tedre, Matti and Denning, Peter and Toivonen, Tapani},
title = {CT 2.0},
year = {2021},
isbn = {9781450384889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488042.3488053},
doi = {10.1145/3488042.3488053},
abstract = {CT has been the central rallying point for K-12 computing education at least since the early 2010s. Many teachers, school administrators, and policymakers have joined the movement. A consensus has emerged over the conceptual landscape of CT. Meanwhile, machine learning (ML) has triggered some major changes in many sectors of computing. Children’s lives today are full of ML-driven services—take TikTok’s spot-on recommendations, social media’s automatic tagging of their friends in photos, and targeted personalized advertisement, just to mention a few. Children cannot learn to think about and design ML technology from learning classical programming. ML is poised to upend the CT consensus. Look at some of the changes ML has already triggered in computing. It has enabled greatly improved speech and image recognition, powerful recommendations on streaming services, autonomous navigation of cars, super-human performance in board and computer games, and even alternative-reality “deepfake” videos. Most advances in topics above are due to hardware evolution to non-traditional, special purpose architectures, new algorithms such as convolutional neural networks (CNN) or generative adversarial networks (GAN), and new objectives and measures of success. We will show that several key CT concepts, including debugging, problem-solving workflow, correctness, and notional machines, are insufficient for ML and need to be extended. Moreover, ML introduces new concepts including neural networks, curating and training data, and reinforcement learning that are not part of CT at all. All these changes challenge the traditional views related to teaching CT in K–12. ML is not the only emerging technology appearing in the computing landscape. Quantum computing and biological computing are not far behind. We need to start rethinking how CT must evolve to anticipate and meet these challenges.},
booktitle = {Proceedings of the 21st Koli Calling International Conference on Computing Education Research},
articleno = {3},
numpages = {8},
keywords = {Artificial intelligence, School, Machine learning, Computational thinking, K-12},
location = {Joensuu, Finland},
series = {Koli Calling '21}
}

@inproceedings{10.1145/3582016.3582025,
author = {Wang, Shuke and Zhang, Mingxing and Yang, Ke and Chen, Kang and Ma, Shaonan and Jiang, Jinlei and Wu, Yongwei},
title = {NosWalker: A Decoupled Architecture for Out-of-Core Random Walk Processing},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582025},
doi = {10.1145/3582016.3582025},
abstract = {Out-of-core random walk system has recently attracted a lot of attention as an economical way to run billions of walkers over large graphs. However, existing out-of-core random walk systems are all built upon general out-of-core graph processing frameworks, and hence do not take advantage of the unique properties of random walk applications. Different from traditional graph analysis algorithms, the sampling process of random walk can be decoupled from the processing of the walkers. It enables the system to reserve only pre-sample results in memory, which are typically much smaller than the entire edge set. Moreover, in random walk, it is not the number of walkers but the number of steps moved per second that dominates the overall performance. Thus, with independent walkers, there is no need to process all the walkers simultaneously. In this paper, we present NosWalker, an out-of-core random walk system that replaces the graph oriented scheduling with a decoupled system architecture that provides walker oriented scheduling. NosWalker is able to adaptively generate walkers and flexibly adjust the distribution of reserved pre-sample results in memory. Instead of processing all the walkers at once, NosWalker only tries its best to keep a few walkers able to continuously move forward. Experimental results show that NosWalker can achieve up to two orders of magnitude speedup compared to state-of-the-art out-of-core random walk systems. In particular, NosWalker demonstrates superior performance when the memory capacity can only hold about 10\%-50\% of the graph data, which can be a common case when the user needs to run billions of walkers over large graphs.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {466–482},
numpages = {17},
keywords = {out-of-core, graph processing, random walk},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.5555/3566055.3566086,
author = {El-Darieby, Mohamed and Daoud, George and Patel, Monil},
title = {Autonomous Vehicles Technology Stack \&amp; Generated Data},
year = {2022},
publisher = {IBM Corp.},
address = {USA},
abstract = {This workshop discusses, presents, and demos examples of how advances in Connected and Autonomous Vehicles (CAV) and in Information and Communications Technologies (ICT) will benefit Intelligent Transportation Systems (ITS). CAV technologies provide an enormous opportunity to generate and collect traffic datasets that directly impact Highway Traffic Management Operations (HTOps). CAV provides novel datasets in terms of types and pervasiveness of coverage of highway networks at a scale that we have not been exposed to before. This enables current HTOps for extensions in features and enhancements in accuracies. HTOps, as defined by Canadian ITS architecture [1], aims at managing traffic to enhance the efficiency of highways, avoid congestion, increase the safety of travelers, and enable more sustainable transportation. This workshop focuses on answering the following question “What (and How) are the features of CAV-generated data that can leverage (HTOps)?” We compare CAV-generated data to traditional traffic data (those collected from relatively “fixed location” sources such as CCTV, electronic loop detectors) and those collected from probe floating vehicles.Extensive research has been conducted into creating self-driving cars that can drive and maneuver on roads in a safe manner. This is embodied in the development of advanced safety features that include blind spot information, reversing/ parking/ lane change assistance, collision warning, and more. CAV can make movement actions based on knowledge of surroundings which relies on hundreds of thousands of data points collected from embedded sensors (LiDAR, cameras, radars, OBD unit, an IMU), (estimated at) a few Terabytes/Vehicle/Day. For example, Volvo’s Cirrus [2] and Lyft Level 5 [3] open-sourced datasets make use of high-resolution video cameras as well as LiDAR sensors to produce Gaussian points that monitor vehicle 3600 surroundings.As an example HTOPS service, we discuss in detail Variable Speed Limits (VSL) and how CAV data can be used to optimize maximum travel speed limit in a dynamic manner over different highway zones based on traffic, congestion, and weather. VSL helps achieve the objectives of HTOps, including devising premeditated plans for peak hours operations and controlling the travel speed in order to increase the throughput of vehicles. To increase safety and ensure compliance, gradual implementation through increments/ decrements of speed along highway zones is required. The extent, expressiveness and quality of CAV data can help achieve such advanced HTOps. For example, the nuScenes dataset [4] that uses linked data to break down and simplify data schema (with various objects such as Category, Ego Pose, Instance, Lidarseg, Map, Sample_annotation, and Sample Data.) With the help of distance measurements, space headings can be accurately defined for vehicles in front and behind the main car. In addition, current vehicle speed data can be obtained from onboard in real-time. This can also be extended to other HTOps services such as Dynamic Lane Management (DLM) which sets lane allocation rules based on the given traffic state, assigning a lane(s) to a prioritized class of vehicle or assigning variable travel speeds for each lane.CAV manufacturers and researchers have made significant progress in building a CAV technology stack that allows a vehicle to drive itself in a safe manner. The technology stack consists of sensing (described above), perception, prediction, and planning layers. The higher three layers use sophisticated deep learning machine learning to process collected data and produce further information on highway traffic conditions. For example, with camera footage, object (e.g. pedestrians, vehicles, traffic signs) detection software can be applied to identify, categorize and characterize surrounding objects. At the same time, LiDAR data can be mapped to provide a bird’s eye spatial view of the surrounding area. Such semantic object maps with binding boxes on objects and trajectory lines are valuable data that TMCs can use to understand the current roadway environment. This is important for understanding stopping time, and congestion levels between cars on the highway provided the current lane of the vehicle. However, due to how recent such developments are, the focus has been on developing the sensing and perception layers of that stack, with much less focus given to the prediction and planning layers.},
booktitle = {Proceedings of the 32nd Annual International Conference on Computer Science and Software Engineering},
pages = {227–228},
numpages = {2},
keywords = {microservices, cloud operations, Cloud computing, artificial intelligence},
location = {Toronto, Canada},
series = {CASCON '22}
}

@inproceedings{10.1145/3511808.3557081,
author = {Nie, Ping and Lu, Yujie and Zhang, Shengyu and Zhao, Ming and Xie, Ruobing and Wang, William Yang and Ren, Yi},
title = {MIC: Model-Agnostic Integrated Cross-Channel Recommender},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557081},
doi = {10.1145/3511808.3557081},
abstract = {Semantically connecting users and items is a fundamental problem for the matching stage of an industrial recommender system. Recent advances in this topic are based on multi-channel retrieval to efficiently measure users' interest on items from the massive candidate pool. However, existing studies are primarily built upon pre-defined retrieval channels, including User-CF (U2U), Item-CF (I2I), and Embedding-based Retrieval (U2I), thus access to the limited correlation between users and items which solely entail from partial information of latent interactions. In this paper, we propose a model-agnostic integrated cross-channel (MIC) approach for the large-scale recommendation, which maximally leverages the inherent multi-channel mutual information to enhance the matching performance. Specifically, MIC robustly models correlation within user-item, user-user, and item-item from latent interactions in a universal schema. For each channel, MIC naturally aligns pairs with semantic similarity and distinguishes them otherwise with more uniform anisotropic representation space. While state-of-the-art methods require specific architectural design, MIC intuitively considers them as a whole by enabling the complete information flow among users and items. Thus MIC can be easily plugged into other retrieval recommender systems. Extensive experiments show that our MIC helps several state-of-the-art models boost their performance on four real-world benchmarks. The satisfactory deployment of the proposed MIC on industrial online services empirically proves its scalability and flexibility.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3400–3409},
numpages = {10},
keywords = {model-agnostic, cross-channel contrastive, retrieval recommender},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3588444.3591030,
author = {Bentaleb, Abdelhak and Farahani, Reza and Tashtarian, Farzad and Hellwagner, Hermann and Zimmermann, Roger},
title = {Which CDN to Download From? A Client and Server Strategies},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588444.3591030},
doi = {10.1145/3588444.3591030},
abstract = {Content Delivery Networks (CDNs) has been evolved to enable different video streaming services to deliver media content over the Internet with less latency and improved quality. However, using only a single CDN is highly vulnerable to outages and crashes, resulting in a poor viewer experience. Regardless of viewers, traffic, or media content, a single CDN will be never sufficient to satisfy viewers' quality of experience (QoE) requirements. To avoid single CDN issues, leveraging multiple CDNs from multiple providers, refers to multi-CDN, helps in improving performance, increasing geographic coverage, and alleviating outages. An essential part in multi-CDN solutions is the decision to select the best performing CDN in real-time, depending on periodic measurements of CDNs and video players. While multi-CDN architecture provides tremendous benefits, it has not been well investigated and integrated with the industry. This paper highlights various decision strategies for real-time CDN selection that helps content providers select the right solution aligned with their goals and business.},
booktitle = {Proceedings of the 2nd Mile-High Video Conference},
pages = {135–136},
numpages = {2},
keywords = {multi-CDN, ABR, QoE, adaptive video streaming, HLS, DASH},
location = {Denver, CO, USA},
series = {MHV '23}
}

@inproceedings{10.1145/3485447.3511982,
author = {Zeng, Liekang and Huang, Peng and Luo, Ke and Zhang, Xiaoxi and Zhou, Zhi and Chen, Xu},
title = {Fograph: Enabling Real-Time Deep Graph Inference with Fog Computing},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511982},
doi = {10.1145/3485447.3511982},
abstract = {Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, the traditional model serving paradigm resorts to the cloud by fully uploading the geo-distributed input data to the remote datacenter. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN serving in fog environment. Prototype-based evaluation and case study demonstrate that Fograph significantly outperforms the state-of-the-art cloud serving and vanilla fog deployment by up to 5.39 \texttimes{} execution speedup and 6.84 \texttimes{} throughput improvement.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1774–1784},
numpages = {11},
keywords = {graph neural networks, Fog computing, model serving, distributed processing},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3543712.3543740,
author = {Chen, Yen-Jen and Lin, En-Cheng},
title = {Design and Implementation of Hardware and Peripheral System for IoT Gateway},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543740},
doi = {10.1145/3543712.3543740},
abstract = {This paper uses Linkit Smart 7688 Duo development board developed by MediaTek as the core of the overall IoT gateway system design, which compares other development boards and IoT gateways on the market. Smart 7688 Duo development board is about 40\% lower in price than Raspberry Pi, and the hardware CPU clock is higher than NEXCOM NIO 51 gateway. This implementation provides a low-cost and highly customizable solution that allows system integrators to more effectively provide their customers with the most appropriate IT services. The proposed IoT gateway design utilizes the Linkit Smart 7688 Duo with MIPS and MCU dual-core chip, Arduino development environment and industrial protocol Modbus to design the data transfer of each sensor in the peripheral system. The IoT gateway obtains the sensor data and transmits it to the server using Message Queuing Telemetry Transport (MQTT). The data acquisition accuracy of the developed MCU program was measured with 2 sensors of hydrogen sulfide (H2S) and methane (CH4). The overall system architecture and peripheral systems are designed to realize the IoT gateway taking into account the internal heat dissipation and module wiring, and also the appearance of the chassis is designed to carry the IoT gateway system, so as to achieve a high-quality product prototype that is accurate, economical, and customizable.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {268–274},
numpages = {7},
keywords = {MQTT, Modbus, MCU, Gateway, IoT},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@article{10.1145/3476839,
author = {Cucchiara, Rita and Fabbri, Matteo},
title = {Fine-Grained Human Analysis under Occlusions and Perspective Constraints in Multimedia Surveillance},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3476839},
doi = {10.1145/3476839},
abstract = {Human detection in the wild is a research topic of paramount importance in computer vision, and it is the starting step for designing intelligent systems oriented to human interaction that work in complete autonomy. To achieve this goal, computer vision and machine learning should aim at superhuman capabilities. In this work, we address the problem of fine-grained human analysis under occlusions and perspective constraints. More specifically, we discuss some issues and some possible solutions to effectively detect people using pose estimation methods and to detect humans under occlusions both in the two-dimensional (2D) image plane and in the 3D space exploiting single monocular cameras. Dealing with occlusion can be done at the joint level or pixel level: We discuss two different solutions, the former based on a supervised neural network architecture for detecting occluded joints and the latter based on a semi-supervised specialized GAN that exploits both appearance and human shape attributes to determine the missing parts of the visible shape. To deal with perspective constraints, we further discuss a neural approach based on a double architecture that learns to create an optimal neural representation, which is useful to reconstruct the 3D position of human keypoints starting with simple RGB images. All these approaches have a critical point in common: the need for large annotated datasets. To have large, fair, consistent, transparent, and ethical datasets, we propose the adoption of synthetic datasets as, for example, JTA and MOTSynth. In this article, we discuss the pros and cons of using synthetic datasets while tackling several human-centered AI issues with respect to European GDPR rules for privacy. We further explore and discuss an application in the field of risk assessment by space occupancy estimation during the COVID-19 pandemic called Inter-Homines.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {32},
numpages = {23},
keywords = {3D localization, synthetic dataset, tracking, human pose estimation, People detection}
}

@article{10.1109/TNET.2021.3115935,
author = {Shapira, Tal and Shavitt, Yuval},
title = {SASA: Source-Aware Self-Attention for IP Hijack Detection},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3115935},
doi = {10.1109/TNET.2021.3115935},
abstract = {IP hijack attacks deflect traffic between endpoints through the attacker network, leading to man-in-the-middle attacks. Current detection solutions are only based on AS-level path analysis, while attacks that include data-plane manipulations may exhibit only geographic anomalies and preserve the AS-level route, or hide the problematic AS in the path. Thus, there is a need to develop data-plane analysis frameworks that examine the actual route packets traverse. We introduce here a deep learning system that examines the geography of traceroute measurements to detect malicious routes. We use multiple geolocation services, with various levels of confidence; each also suffers from location errors. Moreover, identifying a hijacked route is not sufficient since an operator presented with a hijack alert needs an indication of the cause for flagging out the problematic route. Thus, we introduce a novel deep learning layer, called Source-Aware Self-Attention (&lt;italic&gt;SASA&lt;/italic&gt;), which is an extension of the attention mechanism. &lt;italic&gt;SASA&lt;/italic&gt; learns each data source’s confidence and combines this score with the attention of each router in the route to point out the most problematic one. We validate our IP hijacking classification method using two router data types: coordinates and country location, and show that &lt;italic&gt;SASA&lt;/italic&gt; outperforms the regular self-attention layer, using the same neural network architecture, and achieves extremely high accuracy.},
journal = {IEEE/ACM Trans. Netw.},
month = {oct},
pages = {437–449},
numpages = {13}
}

@article{10.1145/3628448,
author = {Cao, Yejun and Yu, Xiwen and Jiang, Fengling},
title = {Application of 3D Image Technology in Rural Planning},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3628448},
doi = {10.1145/3628448},
abstract = {The well-being of villages and villagers is directly related to the development of urban-rural relations. Rural development is an important part of poverty alleviation, as well as the main goal and means of rural rejuvenation, Because rural planning affects rural economic development and rural revitalization. Electronic imaging can improve the speed of rural area planning, and can also model the planned scheme. However, the current rural planning still lacks top-level design, which cannot improve the overall structural design of rural planning, and the data resources of rural planning are not perfect. Therefore, this paper studied the direction of rural planning and design by analyzing the focus, problems and external environment characteristics of rural planning, and then analyzed the application characteristics in rural planning according to the process of 3D image technology. By reducing the complex design links in rural planning, the office efficiency of planning and the 3D visualization of planning model can be promoted, so as to improve the effect of rural planning and the construction service of rural planning. The simulated annealing algorithm showed that the planning efficiency and average planning speed of 3D image application in rural planning were gradually increasing; the average planning efficiency was about 1.80, and the average planning speed was about 1.05. The planning efficiency increased by 1.20 in the whole process, while the average planning speed increased by 0.33 in the whole process. Through comparison, it can be seen that the planning rationality of rural planning under 3D image was 12.16\% higher than the original one, while the measurement error rate of some teachers was 47.28\% lower. In a word, 3D imaging and electronic imaging can improve the architectural design and layout planning of rural planning.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {oct},
keywords = {Electronic Imaging, 3D Imaging, Planning Process Analysis, Rural Planning}
}

@inproceedings{10.1145/3485832.3485836,
author = {R\"{o}ckl, Jonas and Protsenko, Mykolai and Huber, Monika and M\"{u}ller, Tilo and Freiling, Felix C.},
title = {Advanced System Resiliency Based on Virtualization Techniques for IoT Devices},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485836},
doi = {10.1145/3485832.3485836},
abstract = {An increasing number of powerful devices are equipped with network connectivity and are connected to the Internet of Things (IoT). Influenced by the steady growth of computing power of the devices, the paradigm of IoT-based service deployment is expected to change, following the example of cloud-based infrastructure: An embedded platform can be provided as-a-service to several independent application service suppliers. This fosters additional challenges concerning security and isolation. At the same time, recently revealed critical vulnerabilities like Ripple20 and Amnesia:33 show that embedded devices are not spared from wide-spread attacks. In this paper, we define new trusted computing concepts, focusing on privilege separation among several entities sharing one physical device. The concepts guarantee remote recovery capabilities within a bounded amount of time, even if notable portions of the software stack have been compromised. We derive a resilient system architecture suitable for the secure operation of multiple isolated services on one embedded device. We integrate an interface for detecting intrusions and anomalies to enable the automatic recovery of compromised devices and prototype our system on a Nitrogen8M development board. Our evaluation shows that the overhead in terms of network throughput and CPU performance is low so that we believe that our concept is a meaningful step towards more resilient future IoT devices.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {455–467},
numpages = {13},
keywords = {virtualization, trusted computing, recovery, cyber resilience},
location = {<conf-loc>, <city>Virtual Event</city>, <country>USA</country>, </conf-loc>},
series = {ACSAC '21}
}

@inproceedings{10.1145/3488560.3498509,
author = {Malhotra, Ganeshan and Waheed, Abdul and Srivastava, Aseem and Akhtar, Md Shad and Chakraborty, Tanmoy},
title = {Speaker and Time-Aware Joint Contextual Learning for Dialogue-Act Classification in Counselling Conversations},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498509},
doi = {10.1145/3488560.3498509},
abstract = {The onset of the COVID-19 pandemic has brought the mental health of people under risk. Social counselling has gained remarkable significance in this environment. Unlike general goal-oriented dialogues, a conversation between a patient and a therapist is considerably implicit, though the objective of the conversation is quite apparent. In such a case, understanding the intent of the patient is imperative in providing effective counselling in therapy sessions, and the same applies to a dialogue system as well. In this work, we take forward a small but an important step in the development of an automated dialogue system for mental-health counselling. We develop a novel dataset, named HOPE, to provide a platform for the dialogue-act classification in counselling conversations. We identify the requirement of such conversation and propose twelve domain-specific dialogue-act (DAC) labels. We collect ~ 12.9K utterances from publicly-available counselling session videos on YouTube, extract their transcripts, clean, and annotate them with DAC labels. Further, we propose SPARTA, a transformer-based architecture with a novel speaker- and time-aware contextual learning for the dialogue-act classification. Our evaluation shows convincing performance over several baselines, achieving state-of-the-art on HOPE. We also supplement our experiments with extensive empirical and qualitative analyses of SPARTA.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {735–745},
numpages = {11},
keywords = {mental-health counselling, dialogue-act classification},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3565011.3569059,
author = {Kottmann, Felix and Ma, Richard},
title = {A Limit-Order Bandwidth Market Design for Data Delivery},
year = {2022},
isbn = {9781450399234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565011.3569059},
doi = {10.1145/3565011.3569059},
abstract = {Data spaces are an emerging solution to address increasing challenges on privacy, data security, and data sovereignty. However, additional requirements to data delivery emerge, concerning path-awareness and Quality of Service. New internet architectures and their services enable path-aware routing and guaranteed bandwidth. Those features do not only enable the creation of new services but also enable the potential of trading resources such as bandwidth. We design a market mechanism that determines the bandwidth allocation and prices for buyers who express their service requirements in terms of limit orders. Our design builds a price-quantity allocation around a Fisher-market with Leontief utility functions, addressing directly the allocation of bandwidth along one route and allowing for partial filling of orders in the case of congestion using price-priority.},
booktitle = {Proceedings of the 1st International Workshop on Data Economy},
pages = {21–26},
numpages = {6},
location = {Rome, Italy},
series = {DE '22}
}

@inproceedings{10.1145/3493425.3502749,
author = {Kulkarni, Umakant and Sheoran, Amit and Fahmy, Sonia},
title = {The Cost of Stateless Network Functions in 5G},
year = {2022},
isbn = {9781450391689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493425.3502749},
doi = {10.1145/3493425.3502749},
abstract = {The adoption of a cloud-native architecture in 5G networks has facilitated rapid deployment and update of cellular services. An important part of this architecture is the implementation of 5G network functions statelessly. However, statelessness and its associated serialization and de-serialization of data and database interaction significantly increase latency. In this work, we take the first steps towards quantifying the cost of statelessness in a cloud-native 5G system. We compare the cost of different state management paradigms, and propose a number of optimizations to reduce this cost. Our preliminary results indicate that sharing user state among 5G functions reduces the overall cost by on an average of 10\% in experiments with 100 to 1000 simultaneous requests. Optimizations such as non-blocking calls and custom database APIs also reduce cost, albeit to a lower extent. We believe that the paradigms proposed in this paper can aid operators and software vendors as they design cloud-native 5G networks.},
booktitle = {Proceedings of the Symposium on Architectures for Networking and Communications Systems},
pages = {73–79},
numpages = {7},
keywords = {Stateless Network Functions, 5G, Cellular networks, Cloud-native architectures},
location = {Layfette, IN, USA},
series = {ANCS '21}
}

@article{10.1145/3505250,
author = {Vijaykumar, Nandita and Olgun, Ataberk and Kanellopoulos, Konstantinos and Bostanci, F. Nisa and Hassan, Hasan and Lotfi, Mehrshad and Gibbons, Phillip B. and Mutlu, Onur},
title = {MetaSys: A Practical Open-Source Metadata Management System to Implement and Evaluate Cross-Layer Optimizations},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3505250},
doi = {10.1145/3505250},
abstract = {This article introduces the first open-source FPGA-based infrastructure, MetaSys, with a prototype in a RISC-V system, to enable the rapid implementation and evaluation of a wide range of cross-layer techniques in real hardware. Hardware-software cooperative techniques are powerful approaches to improving the performance, quality of service, and security of general-purpose processors. They are, however, typically challenging to rapidly implement and evaluate in real hardware as they require full-stack changes to the hardware, system software, and instruction-set architecture (ISA).MetaSys implements a rich hardware-software interface and lightweight metadata support that can be used as a common basis to rapidly implement and evaluate new cross-layer techniques. We demonstrate MetaSys’s versatility and ease-of-use by implementing and evaluating three cross-layer techniques for: (i) prefetching in graph analytics; (ii) bounds checking in memory unsafe languages, and (iii) return address protection in stack frames; each technique requiring only ~100 lines of Chisel code over MetaSys.Using MetaSys, we perform the first detailed experimental study to quantify the performance overheads of using a single metadata management system to enable multiple cross-layer optimizations in CPUs. We identify the key sources of bottlenecks and system inefficiency of a general metadata management system. We design MetaSys to minimize these inefficiencies and provide increased versatility compared to previously proposed metadata systems. Using three use cases and a detailed characterization, we demonstrate that a common metadata management system can be used to efficiently support diverse cross-layer techniques in CPUs. MetaSys is completely and freely available at .},
journal = {ACM Trans. Archit. Code Optim.},
month = {mar},
articleno = {26},
numpages = {29},
keywords = {metadata, open-source, Hardware-software cooperation, RISC-V, memory}
}

@article{10.1145/3618323,
author = {Li, Chenghong and Jin, Leyang and Zheng, Yujian and Yu, Yizhou and Han, Xiaoguang},
title = {EMS: 3D Eyebrow Modeling from Single-View Images},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3618323},
doi = {10.1145/3618323},
abstract = {Eyebrows play a critical role in facial expression and appearance. Although the 3D digitization of faces is well explored, less attention has been drawn to 3D eyebrow modeling. In this work, we propose EMS, the first learning-based framework for single-view 3D eyebrow reconstruction. Following the methods of scalp hair reconstruction, we also represent the eyebrow as a set of fiber curves and convert the reconstruction to fibers growing problem. Three modules are then carefully designed: RootFinder firstly localizes the fiber root positions which indicate where to grow; OriPredictor predicts an orientation field in the 3D space to guide the growing of fibers; FiberEnder is designed to determine when to stop the growth of each fiber. Our OriPredictor directly borrows the method used in hair reconstruction. Considering the differences between hair and eyebrows, both RootFinder and FiberEnder are newly proposed. Specifically, to cope with the challenge that the root location is severely occluded, we formulate root localization as a density map estimation task. Given the predicted density map, a density-based clustering method is further used for finding the roots. For each fiber, the growth starts from the root point and moves step by step until the ending, where each step is defined as an oriented line segment with a constant length according to the predicted orientation field. To determine when to end, a pixel-aligned RNN architecture is designed to form a binary classifier, which outputs stop or not for each growing step. To support the training of all proposed networks, we build the first 3D synthetic eyebrow dataset that contains 400 high-quality eyebrow models manually created by artists. Extensive experiments have demonstrated the effectiveness of the proposed EMS pipeline on a variety of different eyebrow styles and lengths, ranging from short and sparse to long bushy eyebrows.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {269},
numpages = {19},
keywords = {deep neural networks, dataset, single-view modeling, eyebrow}
}

@inproceedings{10.1145/3477314.3507073,
author = {Ardimento, Pasquale and Aversano, Lerina and Bernardi, Mario Luca and Cimitile, Marta},
title = {Design Patterns Mining Using Neural Sub-Graph Matching},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507073},
doi = {10.1145/3477314.3507073},
abstract = {Design Patterns detection in Object-Oriented software systems is essential for effectively supporting program comprehension and re-engineering tasks. It helps to recover, from source code, the developers' design decisions and trade-offs that could be not up-to-date or even not documented. Several approaches to mine design patterns from source code have been defined in the last twelve years and they are all based on the analysis of object-oriented systems components, their relationships, and behaviors to identify the roles played in the patterns. Both static and dynamic approaches need to perform matching between data captured from the system with the design patterns specification that encodes the structure and the behavior of the micro-architectural solution. The matching process, in principle, can be formulated as a sub-graph matching problem that is NP-complete. This problem has been addressed in the literature using heuristics designed to produce good solutions in an acceptable time, but the task is still expensive with a significant trade-off between accuracy and performance. This work proposes the adoption of a neural-based approach that exploits graph neural networks to perform detection using a more efficient sub-graph matching step outperforming existing heuristics proposed for this task. The pattern detection approach has been assessed on several open-source systems widely used to perform design pattern detection obtaining very good results for both detection performances and efficiency.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1545–1553},
numpages = {9},
keywords = {design patterns, sub-graph matching, graph neural networks},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3459637.3481916,
author = {Derrow-Pinion, Austin and She, Jennifer and Wong, David and Lange, Oliver and Hester, Todd and Perez, Luis and Nunkesser, Marc and Lee, Seongjae and Guo, Xueying and Wiltshire, Brett and Battaglia, Peter W. and Gupta, Vishal and Li, Ang and Xu, Zhongwen and Sanchez-Gonzalez, Alvaro and Li, Yujia and Velickovic, Petar},
title = {ETA Prediction with Graph Neural Networks in Google Maps},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481916},
doi = {10.1145/3459637.3481916},
abstract = {Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events---such as rush hours---that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+\% in cities like Sydney).},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {3767–3776},
numpages = {10},
keywords = {graph neural networks, metagradients, google maps},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3487552.3487812,
author = {Zhang, Zesen and Marder, Alexander and Mok, Ricky and Huffaker, Bradley and Luckie, Matthew and Claffy, K C and Schulman, Aaron},
title = {Inferring Regional Access Network Topologies: Methods and Applications},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487812},
doi = {10.1145/3487552.3487812},
abstract = {Using a toolbox of Internet cartography methods, and new ways of applying them, we have undertaken a comprehensive active measurement-driven study of the topology of U.S. regional access ISPs. We used state-of-the-art approaches in various combinations to accommodate the geographic scope, scale, and architectural richness of U.S. regional access ISPs. In addition to vantage points from research platforms, we used public WiFi hotspots and public transit of mobile devices to acquire the visibility needed to thoroughly map access networks across regions. We observed many different approaches to aggregation and redundancy, across links, nodes, buildings, and at different levels of the hierarchy. One result is substantial disparity in latency from some Edge COs to their backbone COs, with implications for end users of cloud services. Our methods and results can inform future analysis of critical infrastructure, including resilience to disasters, persistence of the digital divide, and challenges for the future of 5G and edge computing.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {720–738},
numpages = {19},
keywords = {access networks, mobile networks, internet topology, traceroute},
location = {Virtual Event},
series = {IMC '21}
}

@inproceedings{10.1145/3479241.3486697,
author = {Gringoli, Francesco and Leith, Douglas J.},
title = {Modelling Downlink Aggregation in Paced WLANs},
year = {2021},
isbn = {9781450390798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479241.3486697},
doi = {10.1145/3479241.3486697},
abstract = {We derive an analytic model of packet aggregation on the the downlink of an 802.11 WLAN when packet arrivals are paced. The model is closed-form and so suitable for both analysis and design. We validate the model against both simulations and experimental measurements and we show its remarkable accuracy despite its simplicity. With proposed next generation architectures for over-the-top services it is straightforward to introduce packet pacing at the network edge and indeed it is this observation that motivates the current work. With this in mind the model developed here provides a new basis for the analysis and design of next generation edge networks.},
booktitle = {Proceedings of the 19th ACM International Symposium on Mobility Management and Wireless Access},
pages = {137–140},
numpages = {4},
keywords = {WLAN modelling, 802.11, aggregation},
location = {Alicante, Spain},
series = {MobiWac '21}
}

@inproceedings{10.1145/3604915.3608772,
author = {Tao, Xuewen and Ha, Mingming and Ma, Qiongxu and Cheng, Hongwei and Lin, Wenfang and Guo, Xiaobo and Cheng, Linxun and Han, Bing},
title = {Task Aware Feature Extraction Framework for Sequential Dependence Multi-Task Learning},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608772},
doi = {10.1145/3604915.3608772},
abstract = {In online recommendation, financial service, etc., the most common application of multi-task learning (MTL) is the multi-step conversion estimations. A core property of the multi-step conversion is the sequential dependence among tasks. However, most existing works focus far more on the specific post-view click-through rate (CTR) and post-click conversion rate (CVR) estimations, which neglect the generalization of sequential dependence multi-task learning (SDMTL). Additionally, the performance of the SDMTL framework is also deteriorated by the interference derived from implicitly conflict information passing between adjacent tasks. In this paper, a systematic learning paradigm of the SDMTL problem is established for the first time, which can transform the SDMTL problem into a general MTL problem with constraints and be applicable to more general multi-step conversion scenarios with stronger task dependence. Also, the distribution dependence relationship between adjacent task spaces is illustrated from a theoretical point of view. On the other hand, an SDMTL architecture, named Task Aware Feature Extraction (TAFE), is developed to enable dynamic task representation learning from a sample-wise view. TAFE selectively reconstructs the implicit shared information corresponding to each sample case and performs explicit task-specific extraction under dependence constraints. Extensive experiments on offline public and real-world industrial datasets, and online A/B implementations demonstrate the effectiveness and applicability of proposed theoretical and implementation frameworks.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {151–160},
numpages = {10},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3528535.3565240,
author = {Lebrun, Thomas and Boutet, Antoine and Aalmoes, Jan and Baud, Adrien},
title = {MixNN: Protection of Federated Learning against Inference Attacks by Mixing Neural Network Layers},
year = {2022},
isbn = {9781450393409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528535.3565240},
doi = {10.1145/3528535.3565240},
abstract = {Machine Learning (ML) has emerged as a core technology to provide learning models to perform complex tasks. Boosted by Machine Learning as a Service (MLaaS), the number of applications relying on ML capabilities is ever increasing. However, ML models are the source of different privacy violations through passive or active attacks from different entities. In this paper, we present MixNN a proxy-based privacy-preserving system for federated learning to protect the privacy of participants against a curious or malicious aggregation server trying to infer sensitive information (i.e., membership and attribute inferences). MixNN receives the model updates from participants and mixes layers between participants before sending the mixed updates to the aggregation server. This mixing strategy drastically reduces privacy leaks without any trade-off with utility. Indeed, mixing the updates of the model has no impact on the result of the aggregation of the updates computed by the server. We report on an extensive evaluation of MixNN using several datasets and neural networks architectures to quantify privacy leakage through membership and attribute inference attacks as well the robustness of the protection. We show that MixNN significantly limits both the membership and attribute inferences compared to a baseline using model compression and noisy gradient (well known to damage the utility) while keeping the same level of utility as classic federated learning.},
booktitle = {Proceedings of the 23rd ACM/IFIP International Middleware Conference},
pages = {135–147},
numpages = {13},
keywords = {machine learning, privacy, inference attacks, federated learning},
location = {Quebec, QC, Canada},
series = {Middleware '22}
}

@inproceedings{10.1145/3460120.3485383,
author = {Zheng, Baolin and Jiang, Peipei and Wang, Qian and Li, Qi and Shen, Chao and Wang, Cong and Ge, Yunjie and Teng, Qingyang and Zhang, Shenyi},
title = {Black-Box Adversarial Attacks on Commercial Speech Platforms with Minimal Information},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485383},
doi = {10.1145/3460120.3485383},
abstract = {Adversarial attacks against commercial black-box speech platforms, including cloud speech APIs and voice control devices, have received little attention until recent years. Constructing such attacks is difficult mainly due to the unique characteristics of time-domain speech signals and the much more complex architecture of acoustic systems. The current "black-box" attacks all heavily rely on the knowledge of prediction/confidence scores or other probability information to craft effective adversarial examples (AEs), which can be intuitively defended by service providers without returning these messages. In this paper, we take one more step forward and propose two novel adversarial attacks in more practical and rigorous scenarios. For commercial cloud speech APIs, we propose Occam, a decision-only black-box adversarial attack, where only final decisions are available to the adversary. In Occam, we formulate the decision-only AE generation as a discontinuous large-scale global optimization problem, and solve it by adaptively decomposing this complicated problem into a set of sub-problems and cooperatively optimizing each one. Our Occam is a one-size-fits-all approach, which achieves 100\% success rates of attacks (SRoA) with an average SNR of 14.23dB, on a wide range of popular speech and speaker recognition APIs, including Google, Alibaba, Microsoft, Tencent, iFlytek, and Jingdong, outperforming the state-of-the-art black-box attacks. For commercial voice control devices, we propose NI-Occam, the first non-interactive physical adversarial attack, where the adversary does not need to query the oracle and has no access to its internal information and training data. We, for the first time, combine adversarial attacks with model inversion attacks, and thus generate the physically-effective audio AEs with high transferability without any interaction with target devices. Our experimental results show that NI-Occam can successfully fool Apple Siri, Microsoft Cortana, Google Assistant, iFlytek and Amazon Echo with an average SRoA of 52\% and SNR of 9.65dB, shedding light on non-interactive physical attacks against voice control devices.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {86–107},
numpages = {22},
keywords = {speaker recognition, adversarial attacks, black-box attacks, speech recognition},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3479241.3486684,
author = {Leiter, \'{A}kos and Galambosi, N\'{a}ndor and Bokor, L\'{a}szl\'{o}},
title = {An Evolution of Proxy Mobile IPv6 to the Cloud},
year = {2021},
isbn = {9781450390798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479241.3486684},
doi = {10.1145/3479241.3486684},
abstract = {Network Function Virtualization (NFV) and Software Defined Networks (SDN) do not leave any legacy network services untouched. This work will present how Proxy Mobile IPv6 (PMIPv6) can evolve to the cloud. Our approach is to introduce this evolution within a step-by-step architecture guideline while keeping standards compatibility. We also show how PMIPv6 can fit into a central cloud - edge cloud environment on the top of Kubernetes and Openstack under a unified orchestration umbrella. The proper integration of PMIPv6 into this new environment does not just enforce acquiring new capabilities, e.g., scaling PMIPv6 elements; it also can ensure closed-loop orchestration where PMIPv6 can be controlled continuously by the actual network needs.},
booktitle = {Proceedings of the 19th ACM International Symposium on Mobility Management and Wireless Access},
pages = {107–115},
numpages = {9},
keywords = {kubernetes, cloud-native, CN-PMIPv6, openstack, ONAP, PMIPv6},
location = {Alicante, Spain},
series = {MobiWac '21}
}

@inproceedings{10.1145/3615366.3615419,
author = {Pontes, Davi and Silva, Fernando and Falc\~{a}o, Eduardo and Brito, Andrey},
title = {Attesting AMD SEV-SNP Virtual Machines with SPIRE},
year = {2023},
isbn = {9798400708442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615366.3615419},
doi = {10.1145/3615366.3615419},
abstract = {SPIRE is an open-source project that enables the provisioning of verifiable identities to software components based on an attestation of the software properties, avoiding the leakage risks of pre-provisioned secrets. This paper presents an implementation of a SPIRE plugin that enables the attestation of AMD SEV-SNP confidential virtual machines. Our approach leverages the pluggable architecture from SPIRE and depends only on minor changes to QEMU, changes taken from its open-source community, and that should soon be merged. As a result, application providers can now use SPIRE to restrict sensitive credentials to be available only to services in environments protected from malicious hosts and cloud operators using AMD SEV-SNP technology. Our experiments show that the steps needed to create and attest the confidential VM do not prohibitively increase boot times (from 10.8 to 20.9 seconds) and that confidential VMs with encrypted disks only slightly degrade the CPU and RAM performance (about ) of unmodified applications.},
booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing},
pages = {1–10},
numpages = {10},
keywords = {AMD SEV-SNP, SPIRE, confidential computing},
location = {<conf-loc>, <city>La Paz</city>, <country>Bolivia</country>, </conf-loc>},
series = {LADC '23}
}

@inproceedings{10.1145/3603269.3610864,
author = {Seehofer, Paul and Mahrt, Hendrik and Bless, Roland and Zitterbart, Martina},
title = {Demo: Enabling Autonomic Network Infrastructures with KIRA},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603269.3610864},
doi = {10.1145/3603269.3610864},
abstract = {Increasing dynamics and an ever growing number of devices make current and future mobile network infrastructures more and more complex. Managing such networks thus becomes progressively challenging. Introducing more autonomic behavior in network management becomes indispensable to not only handle the growing complexity, but also to make these infrastructures more resilient as they constitute a critical component of overall public infrastructures. Autonomic control planes provide a fundamental set of resilient, autonomic infrastructure services (e.g., connectivity) for higher-level autonomic behavior to build upon. In this demo we show how a first real-world implementation of the routing architecture KIRA provides zero-touch control plane connectivity to enable an autonomic 5G network infrastructure. The demonstrator allows an in-depth view of each step in the process of bootstrapping a 5G infrastructure as well as of KIRA's resilience when challenged by failures and dynamics. Based on this autonomic connectivity solution we present our vision of more dynamic and resilient autonomic control networks toward the future design of 6G core networks.},
booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
pages = {1165–1167},
numpages = {3},
keywords = {network management, autonomic networks, resilience},
location = {New York, NY, USA},
series = {ACM SIGCOMM '23}
}

@inproceedings{10.1145/3510450.3517280,
author = {Sodagar, Iraj and Giladi, Alex},
title = {Session-Based DASH Streaming: A New MPEG Standard for Customizing DASH Streaming per Session},
year = {2022},
isbn = {9781450392228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510450.3517280},
doi = {10.1145/3510450.3517280},
abstract = {The MPEG DASH standard is widely deployed in over-the-top streaming services. The standard defines two key components: a manifest format to describe the presentation and a set of segment formats to describe the media segments. While the DASH's manifest format, Media Presentation Document (MPD) provides a set of extensive tools to describe the presentation timeline, this document is usually created for a large set of DASH clients and therefore it can be cached in CDNs for a large population. If the MPD needs to be customized per client, the cache efficiency of storing a single MPD for all clients would be lost. Recently the MPEG Systems Working Group (ISO/IEC/SC29/WG3) developed a new standard that allows an MPD to be customized at each client using an external document and a set of processing rules. The first version of the Session-Based DASH standard (ISO/IEC 23009-8) was recently finalized and will be published in the upcoming months.ISO/IEC 23009-8 defines 3 components: 1) A Session-Based Document (SBD) which defines the MPD customization rules for a client for a given session, 2) A method of referencing the external Session-Based Document (SBD) in the DASH MPD, and finally, 3) a processing model for the client-side processing of SBDs. The SBD defines a post-processing procedure to customize each URL generated by the DASH client from an MPD. Before the DASH client requesting to download a resource using that URL, a process that is described in the SBD document is applied to the URL. The process can customize different parts of the URL, i.e. the host, path, port parts as well as its queries, using a template matching technique. The customization can be timed dependent, on the point in the media timeline that the URL corresponds to, or order dependent, i.e., on the location of the URL in the URL request orders. The result is a customized URL per client/session/URL that is produced from the given URL generated by the DASH client from MPD.The ISO/IEC 23009-8 standard defines an architecture for the session-based DASH streaming that has a few benefits: 1) From the content creation side, it separates the client-based and session-based customization from MPD and therefore maintains the MPD caching efficiency while allowing the customization. It also enables to produce customization after packaging of MPD which means that it can be added to the current workflows as a post-processing step. 2) From the client-side, it allows implementation of SBD client as a separate and independent process from the DASH client, and therefore it can be added to the current clients as a separate process. Furthermore, the SBD processing can occur on the device or a different network entity such as application servers. 3) From the content distribution side, the SBD creation or customization can occur at different nodes of the network, at the origin server, as distribution centers and CDNS, or even at the home network gateways. The standard also allows multiple SBDs to be applied to the URLs of an MPD, enabling the customization to be requested by one or multiple entities in the ingest or distribution chain.In this paper, we first describe the session-based DASH streaming architecture. Then the features of the SBD standard are outlined including the capabilities of customizing based on templates as well as the key-pair replacement and the possibilities of replacing various parts of a URL. Next, the SBD client processing model is described, and how the SBD client can be implemented on the device or as a network entity as a separate process. Finally, we demonstrate a forensic watermarking application using the SBD and demonstrate its capabilities and compare the efficiency of watermarking using the SBD standard vs the MPD customization per client/session.},
booktitle = {Proceedings of the 1st Mile-High Video Conference},
pages = {110},
numpages = {1},
keywords = {DASH, session-based, MPEG},
location = {Denver, Colorado},
series = {MHV '22}
}

@inproceedings{10.1145/3630047.3630192,
author = {Gr\"{o}lle, David and Schulz, Lars-Christian and Wehner, Robin and Hausheer, David},
title = {Poster: High-Speed Per-Packet Checksums on the Intel Tofino},
year = {2023},
isbn = {9798400704468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630047.3630192},
doi = {10.1145/3630047.3630192},
abstract = {Path-aware networking has introduced new possibilities to monitor and control network access and solved a multitude of modern-day Internet security issues. Being able to authorize usage of specific paths enables network operators to offer high-quality services to customers requiring highly reliable network access.Currently, securing a network path or an end host is only possible by using high-level solutions like VPNs. With EPIC-HP (Every Packet Is Checked - Hidden Path), it has been shown that it is possible to move this functionality down into the network itself. EPIC-HP extends the path-aware Internet architecture SCION by offering per-packet checksums, adding authentication to network traffic. This is used to combat DoS attacks on the network's end hosts and give high-priority access to specific end users. In this paper, we show that it is possible to implement the functionality of EPIC-HP along with SCION on the Intel Tofino 2 ASIC. EPIC-HP requires AES-based MAC verification with per-path keys in the data plane. By using the multi-pipeline structure of the Tofino, we implemented the required AES and AES-CMAC cryptography using three pipes of the switch's total four independent pipes.The throughput we achieve is an order of magnitude above the data rates previously achieved for EPIC-HP and is a significant step towards a more secure Internet.},
booktitle = {Proceedings of the 6th on European P4 Workshop},
pages = {49–52},
numpages = {4},
keywords = {aes, p4, tofino, future internet, scion},
location = {<conf-loc>, <city>Paris</city>, <country>France</country>, </conf-loc>},
series = {EuroP4 '23}
}

@article{10.1145/3569092,
author = {Azizifard, Narges and Gelauff, Lodewijk and Gransard-Desmond, Jean-Olivier and Redi, Miriam and Schifanella, Rossano},
title = {Wiki Loves Monuments: Crowdsourcing the Collective Image of the Worldwide Built Heritage},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3569092},
doi = {10.1145/3569092},
abstract = {The wide adoption of digital technologies in the cultural heritage sector has promoted the emergence of new, distributed ways of working, communicating, and investigating cultural products and services. In particular, collaborative online platforms and crowdsourcing mechanisms have been widely adopted in the effort to solicit input from the community and promote engagement. In this work, we provide an extensive analysis of the Wiki Loves Monuments initiative, an annual, international photography contest in which volunteers are invited to take pictures of the built cultural heritage and upload them to Wikimedia Commons. We explore the geographical, temporal, and topical dimensions across the 2010–2021 editions. We first adopt a set of CNN-based artificial systems that allow the learning of deep scene features for various scene recognition tasks, exploring cross-country (dis)similarities. To overcome the rigidity of the framework based on scene descriptors, we train a deep convolutional neural network model to label a photo with its country of origin. The resulting model captures the best representation of a heritage site uploaded in a country, and it allows the domain experts to explore the complexity of cross-national architectural styles. Finally, as a validation step, we explore the link between architectural heritage and intangible cultural values, operationalized using the framework developed within the World Value Survey research program. We observe that cross-country cultural similarities match to a fair extent the interrelations emerging in the architectural domain. We think this study contributes to highlighting the richness and the potential of the Wikimedia data and tools ecosystem to act as a scientific object for art historians, iconologists, and archaeologists.},
journal = {J. Comput. Cult. Herit.},
month = {dec},
articleno = {20},
numpages = {27},
keywords = {Wiki Loves Monuments, Cultural heritage, cross-cultural study}
}

@article{10.1145/3571072,
author = {Shen, Li-Hsiang and Feng, Kai-Ten and Hanzo, Lajos},
title = {Five Facets of 6G: Research Challenges and Opportunities},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571072},
doi = {10.1145/3571072},
abstract = {While the fifth-generation systems are being rolled out across the globe, researchers have turned their attention to the exploration of radical next-generation solutions. At this early evolutionary stage, we survey five main research facets of this field, namely Facet&nbsp;1: next-generation architectures, spectrum, and services; Facet&nbsp;2: next-generation networking; Facet&nbsp;3: Internet of Things; Facet&nbsp;4: wireless positioning and sensing; and Facet&nbsp;5: applications of deep learning in 6G networks. In this article, we provide a critical appraisal of the literature of promising techniques ranging from the associated architectures, networking, and applications, as well as designs. We portray a plethora of heterogeneous architectures relying on cooperative hybrid networks supported by diverse access and transmission mechanisms. The vulnerabilities of these techniques are also addressed and carefully considered for highlighting the most of promising future research directions. Additionally, we list a rich suite of learning-driven optimization techniques. We conclude by observing the evolutionary paradigm shift that has taken place from pure single-component bandwidth efficiency, power efficiency, or delay optimization toward multi-component designs, as exemplified by the twin-component ultra-reliable low-latency mode of the fifth-generation system. We advocate a further evolutionary step toward multi-component Pareto optimization, which requires the exploration of the entire Pareto front of all optimal solutions, where none of the components of the objective function may be improved without degrading at least one of the other components.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {235},
numpages = {39},
keywords = {deep learning, 5G, next-generation, 6G, IoT, communications and networking, positioning and sensing}
}

@inproceedings{10.1145/3267809.3275470,
author = {Nadgowda, Shripad and Isci, Canturk},
title = {Drishti: Disaggregated and Interoperable Security Analytics Framework for Cloud},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3275470},
doi = {10.1145/3267809.3275470},
abstract = {Application and platform security has always been critical for the success of any business. Traditionally, applications were deployed directly on physical servers. As a result, there are myriad traditional security solutions that were developed around this model to run as local agents on the systems they monitor and protect. These solutions are then refined and standardized with decades of experience. With the emergence of virtualization, cloud and particularly containerization, use of these solutions is becoming challenging with consolidation and scale. As we begin to deploy hundreds of cloud instances on a single node, traditional solutions, designed for local execution do not scale out. At the same time, the clean separation of a virtual machine (VM) or a container from the platform itself, and maturing introspection and inspection APIs provide a simple, practical way to decouple monitored from the monitors [3]. Furthermore, as the scope of cloud security expands from simple monitoring and auditing to more complex learning based analytics, analytics components are further offloaded to separate data services, where they can burn extensive cycles, and in some cases use specialized hardware for security analytics, out of the critical path of the monitored applications [5]. As a result, traditional agent-based tightly-coupled model is being replaced by a more dis-aggregated {system, observation, analytics, actions} architecture.To implement such dis-aggregated model in practice, first system state needs to be transferred from cloud platform to analytic platform. File system more generally is representative of the system state that persists features of interest for security analytics like processes, metrics, configurations, packages across various files. Remote replication or snapshotting [1] of whole file system is very in-efficient, since only small set of files are accessed during the analytics. As a result, a new family of cloud-native security solutions have recently emerged in the field that uses various specialized data collection techniques[2, 4]. These techniques perform out-of band introspection of systems to interpret and extract required system features from the file system to essentially serialize system state into data. This data is then transferred to an analytic platform for analysis. Unlike the traditional security solutions that work locally against the system's standard POSIXy file system interfaces, these emerging security analytics "work from data" on the analytic platform. However since the target system is now available as "data", existing agent-based security solutions become incompatible to work against the system. One mitigating solution is to rewrite all existing solutions, which requires huge amount of resources and effort.In Drishti, we address this challenge from a fundamentally different perspective. Instead of rewriting security solutions to work from data, we make the data work for traditional security applications. We achieve this by developing a pseudo-system interface over systems data collected from cloud instances. With this approach, existing solutions run unmodified, as "black box" software over this system interface, as if they were running on the actual cloud instance. Drishti framework is our realization of this approach. It is logically the inverse of the first step of cloud-native security analytics that convert system state into data. With Drishti we transform data back to system on the analytic platform by orchestrating two file system components. First, a standard native system interface is re-calibrated over the system data through our new FUSE file system, confuse or ClOud Native Filesystem in UserSpacE. Second, we mimic the "effect" of an agent installation via an overlay file system based on the the agent image. Within the Drishti framework the underlying data looks like a standard POSIX system to each on-boarded security solution. This allows us to run existing agent-based security solutions as is, but still decoupled from the actual system. Drishti also provides a standard and interoperable platform for designing new security analytic solutions.Overall, Drishti demonstrates a novel, pragmatic and highly-practical approach for bringing security analytics into the cloud. It enables us to leverage existing solutions built based on decades of experience by eliminating the need for reinventing the wheel for cloud.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {528},
numpages = {1},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@inproceedings{10.1145/2701126.2701226,
author = {Rizvi, Syed and Ryoo, Jungwoo and Kissell, John and Aiken, Bill},
title = {A Stakeholder-Oriented Assessment Index for Cloud Security Auditing},
year = {2015},
isbn = {9781450333771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701126.2701226},
doi = {10.1145/2701126.2701226},
abstract = {Cloud computing is an emerging computing model that provides numerous advantages to organizations (both service providers and customers) in terms of massive scalability, lower cost, and flexibility, to name a few. Despite these technical and economical advantages of cloud computing, many potential cloud consumers are still hesitant to adopt cloud computing due to security and privacy concerns. This paper describes some of the unique cloud computing security factors and subfactors that play a critical role in addressing cloud security and privacy concerns. To mitigate these concerns, we develop a security metric tool to provide information to cloud users about the security status of a given cloud vendor. The primary objective of the proposed metric is to produce a security index that describes the security level accomplished by an evaluated cloud computing vendor. The resultant security index will give confidence to different cloud stakeholders and is likely to help them in decision making, increase the predictability of the quality of service, and allow appropriate proactive planning if needed before migrating to the cloud. To show the practicality of the proposed metric, we provide two case studies based on the available security information about two well-known cloud service providers (CSP). The results of these case studies demonstrated the effectiveness of the security index in determining the overall security level of a CSP with respect to the security preferences of cloud users.},
booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication},
articleno = {55},
numpages = {7},
keywords = {cloud security, cloud auditing, security metrics, data privacy},
location = {Bali, Indonesia},
series = {IMCOM '15}
}

@inproceedings{10.1145/2668930.2688043,
author = {Becker, Matthias and Lehrig, Sebastian and Becker, Steffen},
title = {Systematically Deriving Quality Metrics for Cloud Computing Systems},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688043},
doi = {10.1145/2668930.2688043},
abstract = {In cloud computing, software architects develop systems for virtually unlimited resources that cloud providers account on a pay-per-use basis. Elasticity management systems provision these resources autonomously to deal with changing workload. Such changing workloads call for new objective metrics allowing architects to quantify quality properties like scalability, elasticity, and efficiency, e.g., for requirements/SLO engineering and software design analysis. In literature, initial metrics for these properties have been proposed. However, current metrics lack a systematic derivation and assume knowledge of implementation details like resource handling. Therefore, these metrics are inapplicable where such knowledge is unavailable.To cope with these lacks, this short paper derives metrics for scalability, elasticity, and efficiency properties of cloud computing systems using the goal question metric (GQM) method. Our derivation uses a running example that outlines characteristics of cloud computing systems. Eventually, this example allows us to set up a systematic GQM plan and to derive an initial set of six new metrics. We particularly show that our GQM plan allows to classify existing metrics.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {169–174},
numpages = {6},
keywords = {analysis, cloud computing, efficiency, slo, scalability, metric, elasticity, gqm},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/2980258.2982046,
author = {VasanthaAzhagu, A. Kannaki and Gnanasekar, J. M.},
title = {Cloud Computing Overview, Security Threats and Solutions-A Survey},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2982046},
doi = {10.1145/2980258.2982046},
abstract = {Cloud Computing aims to provide computing everywhere. It delivers computing resources on demand over internet in terms of anything anywhere anytime concept. It provides everything as a service to its users like infrastructure platform software hardware workplace data and security. Cloud computing has made revolutionary transformations in the government and business. Cloud Computing transforms the databases and application software to the huge data centers, where the management of the services and data may not be trustworthy. To verify the correctness, integrity, confidentially and availability of data in the cloud, in this paper, we focus on various cloud computing security threats and solution that have been used since security is an important measure for quality of service.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {109},
numpages = {6},
keywords = {Cloud Computing, Availability, Deployment Security threats, Integrity, Quality of Service (QoS)},
location = {Pondicherry, India},
series = {ICIA-16}
}

@article{10.1145/2557833.2557854,
author = {Yadav, Nikita and Khatri, Sujata and Singh, V. B.},
title = {Developing an Intelligent Cloud for Higher Education},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2557854},
doi = {10.1145/2557833.2557854},
abstract = {With rapid development in the IT world, technologies are becoming more dynamic and advanced. Today, technologies are changing with customer requirements. In the IT world, research is carried out to make technology better to meet the requirements that change with time. With the advancement in the IT world, online services have proliferated. Now a days, cloud computing is the hottest buzzword in the IT world. Cloud computing is not limited to the E-Governance and business worlds, but is also making a great impact in the education world. With growing demand for education, technologies and research, all universities and education institutions have their eyes on cloud computing. The main pillars of educational institutions are students, faculties, administrations and libraries. Faculty and students do research and need quality data while students of a particular field need a subject-oriented knowledge. Manually getting these kinds of data is time consuming as students depend on literature, books, different kind of software and hardware. With cloud computing in higher education, cost-effective measures can be taken to minimize the dependency on books, hardware and software. In this paper, we discuss how Artifical Intelligence based cloud computing in higher education will improve quality and ease the process of getting e-resources (software/hardware platform, storage etc.). This study will help in understanding effective cost-cutting measures. We also discuss how cloud computing in the library and administration will brighten the education prospects.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {1–5},
numpages = {5},
keywords = {cloud computing, E-learning, E-library, E-administration, higher education}
}

@inproceedings{10.1145/2896387.2896403,
author = {Al-Ghuwairi, Abdel-Rahman and Eid, Hazem and Aloran, Mohammad and Salah, Zaher and Baarah, Aladdin Hussein and Al-oqaily, Ahmad A.},
title = {A Mutation-Based Model to Rank Testing as a Service (TaaS) Providers in Cloud Computing},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2896403},
doi = {10.1145/2896387.2896403},
abstract = {With the increase of cloud computing service models, the need to measure and evaluate them are increased as well. In this paper, we proposed a novel measurement approach for the purpose of evaluating the quality of Testing as a Service (TaaS), which is considered as one of the most recent outstanding model within cloud computing environment. (TaaS) as outstanding model include the provision of multi-sub services, such as enabling cloud customer to verify his own code through the use of cloud provider resources. Its goes without questioning that testing over web environment requires high level of resources, time, and effort. Therefore, it should take high attention toward the quality of the used testing technique. Where, the quality of testing technique associated with set of attributes that has the ability to determine testing effectiveness. Thus, in this paper we propose a measurement approach to evaluate the effectiveness of TaaS, over cloud computing environment which relies on the use of mutation score. The main contribution of the proposed model represent in the use of mutation score to evaluate cloud providers ability to perform TaaS, and rank them according to the percentage of TaaS effectiveness.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {18},
numpages = {5},
keywords = {Effectiveness, Cloud computing, Testing as a services, Cloud services, Measurement, Mutation},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/3234781.3234787,
author = {Tse, Daniel and Yuen, Hok Hin and He, Qiran and Wang, Chaoya and Yu, Jiheng},
title = {The Security Vulnerabilities of On-Demand and Sharing Economy},
year = {2018},
isbn = {9781450364904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234781.3234787},
doi = {10.1145/3234781.3234787},
abstract = {The cloud computing has been widely in on-demand-and-sharing service economy and has become a hotspot in recent years especially in the IT industry which really lead to some changes in human's daily life. However, many users and researchers believed that the information security is the most significant challenge in cloud computing. Therefore, this paper aims to discover the threats and vulnerabilities of the cloud storage which is the most common application originating from the cloud computing. This research utilized a quantitative approach and all qualified respondents were asked to complete an online questionnaire. The result shows that (1) Data loss and leakage is the biggest threat in using cloud storage application (2) Abuse use of cloud computational resources is the most severe impact in cloud storage application (3) Respondents with different backgrounds have the different perspectives towards the cloud service (4) The countermeasures to minimize the security vulnerability are flexibility in choosing the protective measures, strengthen the infrastructure, improve the password authentication and strengthen the authorization.},
booktitle = {Proceedings of the 2nd International Conference on E-Commerce, E-Business and E-Government},
pages = {47–53},
numpages = {7},
keywords = {on-demand-and-sharing economy, vulnerabilities, cloud storage application, threats, cloud computing},
location = {Hong Kong, Hong Kong},
series = {ICEEG '18}
}

@inproceedings{10.1145/3453187.3453396,
author = {Yang, QiZhen and Xie, XiaoLan},
title = {Research on Cloud Computing Task Scheduling Based on Improved Evolutionary Algorithm},
year = {2021},
isbn = {9781450389099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453187.3453396},
doi = {10.1145/3453187.3453396},
abstract = {In the research of cloud computing, the advantages and disadvantages of cloud task scheduling algorithm will affect the operation efficiency and service quality of the whole cloud computing system. Evolutionary algorithm is the sum of a series of specific algorithms inspired by the phenomenon of biological evolution in nature. One of the common points of these algorithms is that individuals must be mutated according to certain rules in the running process, so as to avoid falling into local optimum. In order to improve the efficiency of cloud task scheduling in cloud computing, this paper proposes a new mutation strategy which changes the genetic algorithm in evolutionary algorithm. It uses cloudsim platform to simulate cloud task scheduling in cloud computing, and uses particle swarm optimization algorithm to optimize its parameters. The experimental results show that the proposed evolutionary algorithm with improved mutation strategy has the function of cloud task scheduling, and its performance is also improved after the parameters are optimized by particle swarm optimization algorithm. The proposed algorithm improves the mutation step and explores the essence of mutation in evolutionary algorithm, which provides a reference for other research.},
booktitle = {Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science},
pages = {566–572},
numpages = {7},
keywords = {Particle swarm optimization algorithm, Cloud computing, Task scheduling, Evolutionary algorithm, Cloudsim},
location = {Wuhan, China},
series = {EBIMCS '20}
}

@inproceedings{10.1145/3447654.3447669,
author = {WANG, XIANZHI and HUANG, PINGGUO and ISHIBASHI, YUTAKA and OKUDA, TAKASHI and WATANABE, HISTOSHI},
title = {Influence of Network Delay on QoS Control Using Neural Network in Remote Robot Systems with Force Feedback},
year = {2021},
isbn = {9781450388566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447654.3447669},
doi = {10.1145/3447654.3447669},
abstract = {In this paper, we focus on the application of big data, cloud computing, and AI (Artificial Intelligence) to QoS (Quality of Service) control to remote robot systems with force feedback. As the first step of our research, we investigate the influence of cloud delay on the remote robot systems while using big data, cloud computing, and AI technology by experiment. In the experiment, we deal with a task in which two robot arms of the two remote robot systems grasp an object and carry the object together. By using big data, cloud computing, and neural network, we predict the optimum value for the robot position control using force information, which we previously proposed as QoS control, in the system, and investigate the influence of cloud delay. Experimental results show that our method is effective, and the feedback force becomes larger as the delay increases.},
booktitle = {Proceedings of the 2020 9th International Conference on Networks, Communication and Computing},
pages = {104–111},
numpages = {8},
keywords = {QoS control, Experiment, Robot position control, Force feedback, Remote robot systems, Neural network},
location = {Tokyo, Japan},
series = {ICNCC '20}
}

@inproceedings{10.1145/3328020.3353936,
author = {Gao, Zhijun and Gao, Yuxin and Xu, Jingjing},
title = {Designing Metrics to Evaluate the Help Center of Baidu Cloud},
year = {2019},
isbn = {9781450367905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328020.3353936},
doi = {10.1145/3328020.3353936},
abstract = {Help centers are mainly designed to assist users with their product uses. The question as to how we measure the quality of a help center remains unanswered. As the first step of a joint research initiated by Peking University and Baidu Cloud that aims to develop a set of computable metrics to evaluate the quality of help centers, this experience report shares the results of data analysis on correlation between user behavioral data and technical documentation quality. The documents and data we use are a suite of cloud computing services provided by Baidu Cloud. The report begins with an introduction of the research goal; following reviews on the related work, it then lays out the design of the experiments with user data collected from Baidu Cloud. In our experiments, we categorize all documents into three groups and try to identify which metrics would affect documentation quality most. The result shows that the key index that contributes most to the model is PV/UV. At last, the report concludes with our current experimental efforts and future work in our plan.},
booktitle = {Proceedings of the 37th ACM International Conference on the Design of Communication},
articleno = {28},
numpages = {7},
keywords = {help center evaluation, technical information, quality evaluation, web metrics},
location = {Portland, Oregon},
series = {SIGDOC '19}
}

@inproceedings{10.5555/2872550.2872554,
author = {Yu, Ning and Gu, Feng and Guo, Xuan and He, Zaobo},
title = {A Fine-Grained Flow Control Model for Cloud-Assisted Data Broadcasting},
year = {2015},
isbn = {9781510801004},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Cloud-assisted data broadcasting is an emerging application where cloud computing assists data broadcasting to extend the capacity of system computing and improve the interactivity of the conventional media. However, with the increase in scale, it brings the difficulty on the complexity to provide the sufficient quality of service for diverse receivers. In order to obtain a fine-grained flow rate as well as the system stability, we propose a model based on parallel scheduling, fair queue and Proportional-Integral-Derivative (PID) controller to cope with these challenges. PID controller takes advantage of the feedback of the statistical output stream and automatically adjusts the transmission flow so that the system can achieve the fine-grained multiplexing performance. Meanwhile, we adopt a set of novel metrics to monitor and measure the quality of flow control in order to weaken the negative impact of coarse-grained flow to user-end devices to the minimum level. Extensive simulations and evaluations have illustrated the superiority of the proposed model in the performance and the quality of service in terms of proposed measurement metrics.},
booktitle = {Proceedings of the 18th Symposium on Communications \&amp; Networking},
pages = {24–31},
numpages = {8},
keywords = {impact energy, fine-grained flow control, time division multiplexing, proportional-integral-derivative (PID) controller, impact power, heterogeneous network, user-end devices, fair queue, quality of service, cloud-assisted data broadcasting, energy metric},
location = {Alexandria, Virginia},
series = {CNS '15}
}

@inproceedings{10.1145/2695664.2695921,
author = {Silva, Francisco Airton and Maciel, Paulo and Filho, Gileno and Matos, Rubens},
title = {A Scheduler for Mobile Cloud Based on Weighted Metrics and Dynamic Context Evaluation},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695921},
doi = {10.1145/2695664.2695921},
abstract = {Resource scarcity is a major obstacle for many mobile applications, since devices have limited energy power and processing potential. As an example, there are applications that seamlessly augment human cognition and typically require resources that far outstrip mobile hardware's capabilities, such as language translation, speech recognition, and face recognition. The use of cloud computing may tackle this problem. This study presents SmartRank, a scheduling framework to perform load partitioning and offloading for mobile applications using cloud computing to increase performance in terms of response time. We first explore a benchmarking of face recognition application using mobile cloud and confirms its suitability to be used as case study with SmartRank. We have applied the approach to a face recognition process based on two strategies: cloudlet federation and resource ranking through balanced metrics (level of CPU utilization and round-trip time). Second, using a full factorial experimental design we tuned the SmartRank with the most suitable partitioning decision calibrating scheduling parameters. Nevertheless, SmartRank uses an equation that is extensible to include new parameters and make it applicable to other scenarios.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {569–576},
numpages = {8},
keywords = {performance evaluation, mobile cloud computing, offloading, partitioning},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3267357.3267362,
author = {Arias-Cabarcos, Patricia and Almen\'{a}rez, Florina and D\'{\i}az-S\'{a}nchez, Daniel and Mar\'{\i}n, Andr\'{e}s},
title = {FRiCS: A Framework for Risk-Driven Cloud Selection},
year = {2018},
isbn = {9781450359887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267357.3267362},
doi = {10.1145/3267357.3267362},
abstract = {Our devices and interactions in a world where physical and digital realities are more and more blended, generate a continuum of multimedia data that needs to be stored, shared and processed to provide services that enrich our daily lives. Cloud computing plays a key role in these tasks, dissolving resource allocation and computational boundaries, but it also requires advanced security mechanisms to protect the data and provide privacy guarantees. Therefore, security assurance must be evaluated before offloading tasks to a cloud provider, a process which is currently manual, complex and inadequate for dynamic scenarios. However, though there are many tools for evaluating cloud providers according to quality of service criteria, automated categorization and selection based on risk metrics is still challenging. To address this gap, we present FRiCS, a Framework for Risk-driven Cloud Selection, which contributes with: 1) a set of cloud security metrics and risk-based weighting policies, 2) distributed components for metric extraction and aggregation, and 3) decision-making plugins for ranking and selection. We have implemented the whole system and conducted a case-study validation based on public cloud providers' security data, showing the benefits of the proposed approach.},
booktitle = {Proceedings of the 2nd International Workshop on Multimedia Privacy and Security},
pages = {18–26},
numpages = {9},
keywords = {security metrics, cloud computing, cloud-based multimedia systems, decision making, risk-driven security},
location = {Toronto, Canada},
series = {MPS '18}
}

@inproceedings{10.1145/2797143.2797145,
author = {Stephanakis, Ioannis M. and Chochliouros, Ioannis P. and Sfakianakis, Evangelos and Shirazi, Noorulhassan},
title = {Anomaly Detection In Secure Cloud Environments Using a Self-Organizing Feature Map (SOFM) Model For Clustering Sets of R-Ordered Vector-Structured Features},
year = {2015},
isbn = {9781450335805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797143.2797145},
doi = {10.1145/2797143.2797145},
abstract = {Cloud computing delivers services over virtualized networks to many end-users. Cloud services are characterized by such attributes as on-demand self-service, broad network access, resource pooling, rapid and elastic resource provisioning and metered services of various qualities. Cloud networks provide data as well as multimedia and video services. Cloud computing for critical structure IT is a relative new area of potential applications. Cloud networks are classified into private cloud networks, public cloud networks and hybrid cloud networks. Anomaly detection systems are defined as a branch of intrusion detection systems that deal with identifying anomalous events with respect to normal system behavior. A novel application of a Self-Organizing-Feature Map (SOFM) of reduced/aggregate sets of ordered vector structured features that are used for detecting anomalies in the context of secure cloud environments is herein proposed. Multivalue inputs consist of reduced/aggregate ordered sets of vector and binary features. The nodes of the SOFM - after training - are indicative of local distributions of feature measurements during normal cloud operation. Anomalies are detected as outliers of the trained SOFM. Each structured vector consists of binary as well as histogram data. The aggregated Canberra distance is used to order histogram data whereas the Jaccard distance is used for multivalue binary data. The so-called Cross-Order Distance Matrix is defined for both cases. The distance depends upon the selection of a similarity/distance measure and a method for operating upon the elements of the Cross-Order Distance Matrix. Several methods of estimating the distance between two ordered sets of features are investigated in the course of this paper.},
booktitle = {Proceedings of the 16th International Conference on Engineering Applications of Neural Networks (INNS)},
articleno = {27},
numpages = {9},
keywords = {Canberra distance, Jaccard distance, Reduced/aggregate-ordering, Secure cloud networks, Self-Organizing Feature Maps (SOFMs), clustering, intrusion detection},
location = {Rhodes, Island, Greece},
series = {EANN '15}
}

@inproceedings{10.1145/3090354.3090367,
author = {Aladwani, Tahani},
title = {Impact of Selecting Virtual Machine with Least Load on Tasks Scheduling Algorithms in Cloud Computing},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090367},
doi = {10.1145/3090354.3090367},
abstract = {Tasks scheduling algorithms consider the first and basic factors in controlling the cloud computing performance. In this paper, we attempt to improve scheduling algorithm's performance by a focus on the load balance factor due to its impact on distributing tasks across multiple virtual machine (VMs) to get best resources utilization, reducing waiting and execution time and enhancing cloud computing performance. This attempt to improve scheduling algorithm's performance by proposing a new strategy called selecting VM with least load (SVLL) can be applied in conjunction with any task scheduling algorithm to improve algorithms performance and increase its load balance. SVLL based on calculating the total load in each VM without taking in consideration number of tasks assigned to it. In order to measure the performance achieved by this method, it will be applied on a set of simple scheduling algorithms, such as First Come First Service (FCFS), Shortest Job First (SJF), and Max-Min scheduling algorithms.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {13},
numpages = {7},
keywords = {Cloud Computing, load balance, Task scheduling algorithms},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3447545.3451190,
author = {Henning, S\"{o}ren and Hasselbring, Wilhelm},
title = {How to Measure Scalability of Distributed Stream Processing Engines?},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451190},
doi = {10.1145/3447545.3451190},
abstract = {Scalability is promoted as a key quality feature of modern big data stream processing engines. However, even though research made huge efforts to provide precise definitions and corresponding metrics for the term scalability, experimental scalability evaluations or benchmarks of stream processing engines apply different and inconsistent metrics. With this paper, we aim to establish general metrics for scalability of stream processing engines. Derived from common definitions of scalability in cloud computing, we propose two metrics: a load capacity function and a resource demand function. Both metrics relate provisioned resources and load intensities, while requiring specific service level objectives to be fulfilled. We show how these metrics can be employed for scalability benchmarking and discuss their advantages in comparison to other metrics, used for stream processing engines and other software systems.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {85–88},
numpages = {4},
keywords = {cloud computing, stream processing, scalability, metrics},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3339186.3339207,
author = {Xia, Qiufen and Bai, Luyao and Liang, Weifa and Xu, Zichuan and Yao, Lin and Wang, Lei},
title = {QoS-Aware Proactive Data Replication for Big Data Analytics in Edge Clouds},
year = {2019},
isbn = {9781450371964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339186.3339207},
doi = {10.1145/3339186.3339207},
abstract = {We are in the era of big data and cloud computing, large quantity of computing resource is desperately needed to detect invaluable information hidden in the coarse big data through query evaluation. Users demand big data analytic services with various Quality of Service (QoS) requirements. However, cloud computing is facing new challenges in meeting stringent QoS requirements of users due to the remoteness from its users. Edge computing has emerged as a new paradigm to address such shortcomings by bringing cloud services to the edge of the operation network in proximity of users for performance improvement. To satisfy the QoS requirements of users for big data analytics in edge computing, the data replication and placement problem must be properly dealt with such that user requests can be efficiently and promptly responded. In this paper, we consider data replication and placement for big data analytic query evaluation. We first cast a novel proactive data replication and placement problem of big data analytics in a two-tier edge cloud environment, we then devise an approximation algorithm with an approximation ratio for it, we finally evaluate the proposed algorithm against existing benchmarks, using both simulation and experiment in a testbed based on real datasets, the evaluation results show that the proposed algorithm is promising.},
booktitle = {Workshop Proceedings of the 48th International Conference on Parallel Processing},
articleno = {26},
numpages = {10},
keywords = {Data replication and placement, query evaluation, big data analytics, edge clouds},
location = {Kyoto, Japan},
series = {ICPP Workshops '19}
}

@inproceedings{10.1145/3147213.3149214,
author = {Aske, Austin and Zhao, Xinghui},
title = {An Actor-Based Framework for Edge Computing},
year = {2017},
isbn = {9781450351492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147213.3149214},
doi = {10.1145/3147213.3149214},
abstract = {The Actor model provides inherent parallelism, along with other convenient features to build large-scale distributed systems. In this paper, we present ActorEdge, an Actor based distributed framework for edge computing. ActorEdge provides straitforward integration with existing technologies, while enabling application developers to dynamically utilize computational resources on the edge of the clouds. ActorEdge has proven to outperform cloud computing options by providing superior quality of service, measuring a 10x lower latency, 30\% less jitter, and greater bandwidth. Using this framework, programmers can easily develop and deploy their applications on a heterogeneous system, including cloud servers/data centers, edge servers, and mobile devices.},
booktitle = {Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {199–200},
numpages = {2},
keywords = {actors, edge computing, cloud computing, mobile clouds},
location = {Austin, Texas, USA},
series = {UCC '17}
}

@inproceedings{10.1145/3132847.3133045,
author = {Fang, Zhou and Yu, Tong and Mengshoel, Ole J. and Gupta, Rajesh K.},
title = {QoS-Aware Scheduling of Heterogeneous Servers for Inference in Deep Neural Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133045},
doi = {10.1145/3132847.3133045},
abstract = {Deep neural networks (DNNs) are popular in diverse fields such as computer vision and natural language processing. DNN inference tasks are emerging as a service provided by cloud computing environments. However, cloud-hosted DNN inference faces new challenges in workload scheduling for the best Quality of Service (QoS), due to dependence on batch size, model complexity and resource allocation. This paper represents the QoS metric as a utility function of response delay and inference accuracy. We first propose a simple and effective heuristic approach that keeps low response delay and satisfies the requirement on processing throughput. Then we describe an advanced deep reinforcement learning (RL) approach that learns to schedule from experience. The RL scheduler is trained to maximize QoS, using a set of system statuses as the input to the RL policy model. Our approach performs scheduling actions only when there are free GPUs, thus reduces scheduling overhead over common RL schedulers that run at every continuous time step. We evaluate the schedulers on a simulation platform and demonstrate the advantages of RL over heuristics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2067–2070},
numpages = {4},
keywords = {deep neural networks inference, reinforcement learning, deep reinforcement learning, web service, qos aware scheduling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/2668930.2688818,
author = {Lehrig, Sebastian and Becker, Steffen},
title = {The CloudScale Method for Software Scalability, Elasticity, and Efficiency Engineering: A Tutorial},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688818},
doi = {10.1145/2668930.2688818},
abstract = {In cloud computing, software engineers design systems for virtually unlimited resources that cloud providers account on a pay-per-use basis. Elasticity management systems provision these resource autonomously to deal with changing workloads. Such workloads call for new objective metrics allowing engineers to quantify quality properties like scalability, elasticity, and efficiency. However, software engineers currently lack engineering methods that aid them in engineering their software regarding such properties. Therefore, the CloudScale project developed tools for such engineering tasks. These tools cover reverse engineering of architectural models from source code, editors for manual design/adaption of such models, as well as tools for the analysis of modeled and operating software regarding scalability, elasticity, and efficiency. All tools are interconnected via ScaleDL, a common architectural language, and the CloudScale Method that leads through the engineering process. In this tutorial, we execute our method step-by-step such that every tool and ScaleDL are briefly introduced.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {329–331},
numpages = {3},
keywords = {elasticity, cloud computing, tutorial, metrics, cloudscale, scalability, engineering, efficiency, software analysis, method},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/3128128.3128161,
author = {Marwan, M. and Kartit, A. and Ouahmane, H.},
title = {Protecting Medical Data in Cloud Storage Using Fault-Tolerance Mechanism},
year = {2017},
isbn = {9781450352819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128128.3128161},
doi = {10.1145/3128128.3128161},
abstract = {Given the fact that cloud computing offers cost-efficient storage systems, medical organizations are more interested in using this alternative solution to safeguard their patients' data. Equally interestingly, users are charged based typically on the amount of occupied storage space. Basically, this concept is meant to cut costs and improve the quality of healthcare services. Consequently, implementing cloud storage would help clients to manage their data efficiently. Besides, it allows users to outsource the storage process by using virtual storage systems instead of local ones. Despite its significant impact in healthcare domain, adopting this paradigm to save medical data on remote servers poses serious challenges, especially security risks. Currently, various cryptographic techniques have been used to ensure data confidentiality and to avoid data disclosure. Globally, this model uses traditional cryptosystems such as AES, RSA to address security issues in cloud storage. As far as we know, there are only a few works in literature that deal with availability and data recovery in cloud computing. In general, the classical approach which is based on backup or replication is not suitable for cloud environment due to the highly dynamic nature of this model. The intent of this work is to enhance the reliability of cloud storage in order to meet security requirements. In this study, we propose a novel method based on Shamir's Secret Share Scheme and multi-cloud concept to avoid data loss and unauthorized access. More precisely, this technique seeks to divide consumers' data into several portions using Shamir's Secret Share to prevent privacy disclosure. Based on these considerations, we store these created portions in different nodes to minimize security risks, particularly internal attacks. To sum up, this method is designed to ensure fault-tolerance, which is the main subject of this study. In fact, we need just certain shares to reconstruct the secret data rather than using all parts. The experimental results are in accordance with the theoretical assumptions behind this model, and hence, confirm that the proposed framework provides necessary measures for preventing data loss in cloud storage.},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
pages = {214–219},
numpages = {6},
keywords = {fault tolerance, medical image, security, cloud computing},
location = {Rabat, Morocco},
series = {ICSDE '17}
}

@article{10.1145/3284553,
author = {Avgeris, Marios and Dechouniotis, Dimitrios and Athanasopoulos, Nikolaos and Papavassiliou, Symeon},
title = {Adaptive Resource Allocation for Computation Offloading: A Control-Theoretic Approach},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3284553},
doi = {10.1145/3284553},
abstract = {Although mobile devices today have powerful hardware and networking capabilities, they fall short when it comes to executing compute-intensive applications. Computation offloading (i.e., delegating resource-consuming tasks to servers located at the edge of the network) contributes toward moving to a mobile cloud computing paradigm. In this work, a two-level resource allocation and admission control mechanism for a cluster of edge servers offers an alternative choice to mobile users for executing their tasks. At the lower level, the behavior of edge servers is modeled by a set of linear systems, and linear controllers are designed to meet the system’s constraints and quality of service metrics, whereas at the upper level, an optimizer tackles the problems of load balancing and application placement toward the maximization of the number the offloaded requests. The evaluation illustrates the effectiveness of the proposed offloading mechanism regarding the performance indicators, such as application average response time, and the optimal utilization of the computational resources of edge servers.},
journal = {ACM Trans. Internet Technol.},
month = {apr},
articleno = {23},
numpages = {20},
keywords = {Edge computing, feedback control, linear modeling}
}

@inproceedings{10.1145/3425269.3425272,
author = {Silva, Jorge Luiz Machado da and de Fran\c{c}a, Breno B. Nicolau and Rubira, Cec\'{\i}lia Mary Fischer},
title = {Generating Trustworthiness Adaptation Plans Based on Quality Models for Cloud Platforms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425272},
doi = {10.1145/3425269.3425272},
abstract = {Cloud computing platforms can offer many benefits related to the provision of service processing and storage for hosting client applications. Trustworthiness can be defined as the trust of a customer in a cloud service and its provider; however, the assurance of this property is not trivial. First, trustworthiness in general is not composed by a single quality attribute, but by the combination of multiple attributes, such as data privacy, performance, reliability, etc. Second, during runtime clients can experience a change of the trustworthiness level required by their application due to the degradation of the cloud service. This article presents a solution that monitors during runtime the set of quality attributes of a specific application and generates adaptation plans in order to certify that an adequate resource amount be provided by the cloud in order to keep its trustworthiness level. Our solution is based on quality models to compute the metric associated to each non-functional requirement and their combination them into different types of trustworthiness levels. The main contribution of the solution is to provide an approach which deals with multiple requirements at the same time (or simultaneously) during runtime in order to adapt the cloud resources to keep the trustworthiness level required by the application. The solution was evaluated by an experiment considering a scenario where the application trustworthiness level was composed by three quality attributes: data privacy, performance and reliability. Initial results have shown that the approach is feasible in terms of the execution of the adaptation plans during runtime to certify the trustworthiness level required by the application.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {141–150},
numpages = {10},
keywords = {Cloud Computing, Self-adaptive Systems, Adaptation Planning, Trustworthiness},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1145/3331148,
author = {Magrofuoco, Nathan and Vanderdonckt, Jean},
title = {Gelicit: A Cloud Platform for Distributed Gesture Elicitation Studies},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {EICS},
url = {https://doi.org/10.1145/3331148},
doi = {10.1145/3331148},
abstract = {A gesture elicitation study, as originally defined, consists of gathering a sample of participants in a room, instructing them to produce gestures they would use for a particular set of tasks, materialized through a representation called referent, and asking them to fill in a series of tests, questionnaires, and feedback forms. Until now, this procedure is conducted manually in a single, physical, and synchronous setup. To relax the constraints imposed by this manual procedure and to support stakeholders in defining and conducting such studies in multiple contexts of use, this paper presents Gelicit, a cloud computing platform that supports gesture elicitation studies distributed in time and space structured into six stages: (1) define a study: a designer defines a set of tasks with their referents for eliciting gestures and specifies an experimental protocol by parameterizing its settings; (2) conduct a study: any participant receiving the invitation to join the study conducts the experiment anywhere, anytime, anyhow, by eliciting gestures and filling forms; (3) classify gestures: an experimenter classifies elicited gestures according to selected criteria and a vocabulary; (4) measure gestures: an experimenter computes gesture measures, like agreement, frequency, to understand their configuration; (5) discuss gestures: a designer discusses resulting gestures with the participants to reach a consensus; (6) export gestures: the consensus set of gestures resulting from the discussion is exported to be used with a gesture recognizer. The paper discusses Gelicit advantages and limitations with respect to three main contributions: as a conceptual model for gesture management, as a method for distributed gesture elicitation based on this model, and as a cloud computing platform supporting this distributed elicitation. We illustrate Gelicit through a study for eliciting 2D gestures executing Internet of Things tasks on a smartphone.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {6},
numpages = {41},
keywords = {elicitation technique, workflow analysis, gesture elicitation study, gesture interaction}
}

@inproceedings{10.1145/2857546.2857552,
author = {Anand, Priya and Ryoo, Jungwoo and Kim, Hyoungshick and Kim, Eunhyun},
title = {Threat Assessment in the Cloud Environment: A Quantitative Approach for Security Pattern Selection},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857552},
doi = {10.1145/2857546.2857552},
abstract = {Cloud computing has emerged as a fast-growing technology in the past few years. It provides a great flexibility for storing, sharing and delivering data over the Internet without investing on new technology or resources. In spite of the development and wide array of cloud usage, security perspective of cloud computing still remains its infancy. Security challenges faced by cloud environment becomes more complicated when we include various stakeholders' perspectives. In a cloud environment, security perspectives and requirements are usually designed by software engineers or security experts. Sometimes clients' requirements are either ignored or given a very high importance. In order to implement cloud security by providing equal importance to client organizations, software engineers and security experts, we propose a new methodology in this paper. We use Microsoft's STRIDE-DREAD model to assess threats existing in the cloud environment and also to measure its consequences. Our aim is to rank the threats based on the nature of its severity, and also giving a significant importance for clients' requirements on security perspective. Our methodology would act as a guiding tool for security experts and software engineers to proceed with securing process especially for a private or a hybrid cloud. Once threats are ranked, we provide a link to a well-known security pattern classification. Although we have some security pattern classification schemes in the literature, we need a methodology to select a particular category of patterns. In this paper, we provide a novel methodology to select a set of security patterns for securing a cloud software. This methodology could aid a security expert or a software professional to assess the current vulnerability condition and prioritize by also including client's security requirements in a cloud environment.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {5},
numpages = {8},
keywords = {Cloud Computing, Risk Analysis, STRIDE-DREAD Model, Security Patterns, Threat Assessment},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@article{10.1145/3236332,
author = {Herbst, Nikolas and Bauer, Andr\'{e} and Kounev, Samuel and Oikonomou, Giorgos and Eyk, Erwin Van and Kousiouris, George and Evangelinou, Athanasia and Krebs, Rouven and Brecht, Tim and Abad, Cristina L. and Iosup, Alexandru},
title = {Quantifying Cloud Performance and Dependability: Taxonomy, Metric Design, and Emerging Challenges},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3236332},
doi = {10.1145/3236332},
abstract = {In only a decade, cloud computing has emerged from a pursuit for a service-driven information and communication technology (ICT), becoming a significant fraction of the ICT market. Responding to the growth of the market, many alternative cloud services and their underlying systems are currently vying for the attention of cloud users and providers. To make informed choices between competing cloud service providers, permit the cost-benefit analysis of cloud-based systems, and enable system DevOps to evaluate and tune the performance of these complex ecosystems, appropriate performance metrics, benchmarks, tools, and methodologies are necessary. This requires re-examining old system properties and considering new system properties, possibly leading to the re-design of classic benchmarking metrics such as expressing performance as throughput and latency (response time). In this work, we address these requirements by focusing on four system properties: (i) elasticity of the cloud service, to accommodate large variations in the amount of service requested, (ii)&nbsp;performance isolation between the tenants of shared cloud systems and resulting performance variability, (iii)&nbsp;availability of cloud services and systems, and (iv) the operational risk of running a production system in a cloud environment. Focusing on key metrics for each of these properties, we review the state-of-the-art, then select or propose new metrics together with measurement approaches. We see the presented metrics as a foundation toward upcoming, future industry-standard cloud benchmarks.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {aug},
articleno = {19},
numpages = {36},
keywords = {Metrics, cloud, performance variability, performance isolation, elasticity, benchmarking, availability, operational risk}
}

@inproceedings{10.1145/2955193.2955208,
author = {Aman, Mortada A. and \c{C}etinkaya, Egemen K.},
title = {DSB-SEIS: A Deduplicating Secure Backup System with Encryption Intensity Selection},
year = {2016},
isbn = {9781450342209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2955193.2955208},
doi = {10.1145/2955193.2955208},
abstract = {Cloud computing is an emerging service that enables users to store and manage their data easily at a low cost. We propose a Deduplicating Secure Backup System with Encryption Intensity Selection (DSB-SEIS) that combines features to amend security and performance of cloud-based backup services. Our scheme introduces the concept of encryption intensity selection to cloud backup systems, which allows users to select the encryption intensity of their files. We also combine features such as deduplication, assured deletion, and multi-aspect awareness to further enhance our scheme. The DSB-SEIS performance is measured over an OpenStack cloud installed on CloudLab resources demonstrating that DSB-SEIS can improve the backup service.},
booktitle = {Proceedings of the 4th Workshop on Distributed Cloud Computing},
articleno = {11},
numpages = {1},
keywords = {security, cloud, deduplication, CloudLab, backup, integrity},
location = {Chicago, Illinois},
series = {DCC '16}
}

@inproceedings{10.1145/3030207.3030214,
author = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Papadopoulos, Alessandro V. and Ghit, Bogdan and Epema, Dick and Iosup, Alexandru},
title = {An Experimental Performance Evaluation of Autoscaling Policies for Complex Workflows},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030214},
doi = {10.1145/3030207.3030214},
abstract = {Simplifying the task of resource management and scheduling for customers, while still delivering complex Quality-of-Service (QoS), is key to cloud computing. Many autoscaling policies have been proposed in the past decade to decide on behalf of cloud customers when and how to provision resources to a cloud application utilizing cloud elasticity features. However, in prior work, when a new policy is proposed, it is seldom compared to the state-of-the-art, and is often compared only to static provisioning using a predefined QoS target. This reduces the ability of cloud customers and of cloud operators to choose and deploy an autoscaling policy. In our work, we conduct an experimental performance evaluation of autoscaling policies, using as application model workflows, a commonly used formalism for automating resource management for applications with well-defined yet complex structure. We present a detailed comparative study of general state-of-the-art autoscaling policies, along with two new workflow-specific policies. To understand the performance differences between the 7 policies, we conduct various forms of pairwise and group comparisons. We report both individual and aggregated metrics. Our results highlight the trade-offs between the suggested policies, and thus enable a better understanding of the current state-of-the-art.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {75–86},
numpages = {12},
keywords = {workflows, demand, directed acyclic graph, clouds, spec, dag, cloud computing, performance, auto-scaling, scheduling, metrics, workloads, autoscaling, opennebula, elasticity, supply, level of parallelism},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/3386367.3431670,
author = {Sacco, Alessio and Esposito, Flavio and Marchetto, Guido},
title = {A Distributed Reinforcement Learning Approach for Energy and Congestion-Aware Edge Networks},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431670},
doi = {10.1145/3386367.3431670},
abstract = {The abiding attempt of automation has also pervaded computer networks, with the ability to measure, analyze, and control themselves in an automated manner, by reacting to changes in the environment (e.g., demand) while exploiting existing flexibilities. When provided with these features, networks are often referred to as "self-driving". Network virtualization and machine learning are the drivers. In this regard, the provision and orchestration of physical or virtual resources are crucial for both Quality of Service guarantees and cost management in the edge/cloud computing ecosystem. Auto-scaling mechanisms are hence essential to effectively manage the lifecycle of network resources. In this poster, we propose Relevant, a distributed reinforcement learning approach to enable distributed automation for network orchestrators. Our solution aims at solving the congestion control problem within Software-Defined Network infrastructures, while being mindful of the energy consumption, helping resources to scale up and down as traffic demands fluctuate and energy optimization opportunities arise.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {546–547},
numpages = {2},
keywords = {reinforcement learning, self-driving networks, auto-scaling},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@article{10.1145/3460197,
author = {Nemati, Hani and Azhari, Seyed Vahid and Shakeri, Mahsa and Dagenais, Michel},
title = {Host-Based Virtual Machine Workload Characterization Using Hypervisor Trace Mining},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2376-3639},
url = {https://doi.org/10.1145/3460197},
doi = {10.1145/3460197},
abstract = {Cloud computing is a fast-growing technology that provides on-demand access to a pool of shared resources. This type of distributed and complex environment requires advanced resource management solutions that could model virtual machine (VM) behavior. Different workload measurements, such as CPU, memory, disk, and network usage, are usually derived from each VM to model resource utilization and group similar VMs. However, these course workload metrics require internal access to each VM with the available performance analysis toolkit, which is not feasible with many cloud environments privacy policies.In this article, we propose a non-intrusive host-based virtual machine workload characterization using hypervisor tracing. VM blockings duration, along with virtual interrupt injection rates, are derived as features to reveal multiple levels of resource intensiveness. In addition, the VM exit reason is considered, as well as the resource contention rate due to the host and other VMs. Moreover, the processes and threads preemption rates in each VM are extracted using the collected tracing logs. Our proposed approach further improves the selected features by exploiting a page ranking based algorithm to filter non-important processes running on each VM. Once the metric features are defined, a two-stage VM clustering technique is employed to perform both coarse- and fine-grain workload characterization. The inter-cluster and intra-cluster similarity metrics of the silhouette score is used to reveal distinct VM workload groups, as well as the ones with significant overlap. The proposed framework can provide a detailed vision of the underlying behavior of the running VMs. This can assist infrastructure administrators in efficient resource management, as well as root cause analysis.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {jun},
articleno = {4},
numpages = {25},
keywords = {machine learning, time series, workload characterization, PageRank, vCPU states, K-Means, performance analysis, VM clustering, tracing, virtual interrupts}
}

@inproceedings{10.1145/2996890.3007870,
author = {Uhlir, Vojtech and Tomanek, Ondrej and Kencl, Lukas},
title = {Latency-Based Benchmarking of Cloud Service Providers},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.3007870},
doi = {10.1145/2996890.3007870},
abstract = {With the ever-increasing trend of migration of applications to the Cloud environment, there is a growing need to thoroughly evaluate quality of the Cloud service itself, before deciding upon a hosting provider. Benchmarking the Cloud services is difficult though, due to the complex nature of the Cloud Computing setup and the diversity of locations, of applications and of their specific service requirements. However, such comparison may be crucial for decision making and for troubleshooting of services offered by the intermediate businesses - the so-called Cloud tenants. Existing cross-sectional studies and benchmarking methodologies provide only a shallow comparison of Cloud services, whereas state-of-the-art tooling for specific comparisons of application-performance parameters, such as for example latency, is insufficient. In this work, we propose a novel methodology for benchmarking of Cloud-service providers, which is based on latency measurements collected via active probing, and can be tailored to specific application needs. Furthermore, we demonstrate its applicability on a practical longitudinal study of real measurements of two major Cloud-service providers - Amazon and Microsoft.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {263–268},
numpages = {6},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1109/CCGRID.2017.39,
author = {Evangelidis, Alexandros and Parker, David and Bahsoon, Rami},
title = {Performance Modelling and Verification of Cloud-Based Auto-Scaling Policies},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.39},
doi = {10.1109/CCGRID.2017.39},
abstract = {Auto-scaling, a key property of cloud computing, allows application owners to acquire and release resources on demand. However, the shared environment, along with the exponentially large configuration space of available parameters, makes configuration of auto-scaling policies a challenging task. In particular, it is difficult to quantify, a priori, the impact of a policy on Quality of Service (QoS) provision. To address this problem, we propose a novel approach based on performance modelling and formal verification to produce performance guarantees on particular rule-based auto-scaling policies. We demonstrate the usefulness and efficiency of our model through a detailed validation process on the Amazon EC2 cloud, using two types of load patterns. Our experimental results show that it can be very effective in helping a cloud application owner configure an auto-scaling policy in order to minimise the QoS violations.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {355–364},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/2910017.2910602,
author = {Slivar, Ivan and Skorin-Kapov, Lea and Suznjevic, Mirko},
title = {Cloud Gaming QoE Models for Deriving Video Encoding Adaptation Strategies},
year = {2016},
isbn = {9781450342971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910017.2910602},
doi = {10.1145/2910017.2910602},
abstract = {Cloud gaming has been recognized as a promising shift in the online game industry, with the aim being to deliver high-quality graphics games to any type of end user device. The concepts of cloud computing are leveraged to render the game scene as a video stream which is then delivered to players in real-time. Given high bandwidth and strict latency requirements, a key challenge faced by cloud game providers lies in configuring the video encoding parameters so as to maximize player Quality of Experience (QoE) while meeting bandwidth availability constraints. In this paper we address this challenge by conducting a subjective laboratory study involving 52 players and two different games aimed at identifying QoE-driven video encoding adaptation strategies. Empirical results are used to derive analytical QoE estimation models as functions of bitrate and framerate, while also taking into account game type and player skill. Results have shown that under certain identified bandwidth conditions, reductions of framerate lead to QoE improvements due to improved graphics quality. Given that results indicate that different QoE-driven video adaptation policies should likely be applied for different types of games, we further report on objective video metrics that may be used to classify games for the purpose of choosing an appropriate and QoE-driven video codec configuration strategy.},
booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
articleno = {18},
numpages = {12},
keywords = {QoE modeling, cloud gaming QoE, QoE, cloud gaming},
location = {Klagenfurt, Austria},
series = {MMSys '16}
}

@inproceedings{10.1145/3005745.3005749,
author = {Mukerjee, Matthew K. and Bozkurt, Ilker Nadi and Maggs, Bruce and Seshan, Srinivasan and Zhang, Hui},
title = {The Impact of Brokers on the Future of Content Delivery},
year = {2016},
isbn = {9781450346610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3005745.3005749},
doi = {10.1145/3005745.3005749},
abstract = {Various trends are reshaping content delivery on the Internet: the explosive growth of traffic due to video, users' increasing expectations for higher quality of experience (QoE), and the proliferation of server capacity from a variety of sources (e.g., cloud computing, content provider-owned datacenters, and ISP-owned CDNs). In order to meet the scale and quality demands imposed by users, content providers have started to spread demand across a variety of CDNs using a broker. Brokers break many traditional CDN assumptions (e.g., unexpected traffic skew, significant variance in demand over short timescales, etc.). Through an analysis of data from a leading broker and a leading CDN, we show the potential challenges and opportunities that brokers impart on content delivery. We take the first steps towards improvement through a redesigned broker-CDN interface.},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
pages = {127–133},
numpages = {7},
location = {Atlanta, GA, USA},
series = {HotNets '16}
}

@inproceedings{10.1145/3463677.3463732,
author = {Ahn, Michael and Chu, Shengli},
title = {What Matters in Maintaining Effective Open Government Data Systems? The Role of Government Managerial Capacity, and Political and Legal Environment},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463732},
doi = {10.1145/3463677.3463732},
abstract = {This paper aims to identify key institutional factors that contribute to effective open data systems. Rapid advancement in new technologies such as machine learning, algorithms, IoT, and Cloud Computing has amplified the importance of national open data systems. The availability of relevant public data has become a crucial factor in creating sophisticated machine learning platforms or algorithms that will have a considerable impact on national competitiveness. Effective national open data strategies will matter in shaping an environment that will facilitate data production, dissemination, and utilization. Using multiple sources of data that measure the qualities of open data systems and various political, governmental, and legal attributes at the national level, we seek to identify key institutional factors that contribute to robust open data policies and outcomes. Our findings point to the importance of the existence of a national open data strategy and support (especially "open by default" strategy), pre-existing e-government capability, and countries operating under full democracy with its guarantees to civil liberties and political freedom. In addition, the nature of the open data matters as different managerial, political, and demographic conditions affected the quality of different open data systems. Policy implications of our findings are discussed.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {444–457},
numpages = {14},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@inproceedings{10.1145/2898445.2898446,
author = {Sun, Degang and Zhang, Jie and Fan, Wei and Wang, Tingting and Liu, Chao and Huang, Weiqing},
title = {SPLM: Security Protection of Live Virtual Machine Migration in Cloud Computing},
year = {2016},
isbn = {9781450342858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2898445.2898446},
doi = {10.1145/2898445.2898446},
abstract = {Virtual machine live migration technology, as an important support for cloud computing, has become a central issue in recent years. The virtual machines' runtime environment is migrated from the original physical server to another physical server, maintaining the virtual machines running at the same time. Therefore, it can make load balancing among servers and ensure the quality of service. However, virtual machine migration security issue cannot be ignored due to the immature development of it. This paper we analyze the security threats of the virtual machine migration, and compare the current proposed protection measures. While, these methods either rely on hardware, or lack adequate security and expansibility. In the end, we propose a security model of live virtual machine migration based on security policy transfer and encryption, named as SPLM (Security Protection of Live Migration) and analyze its security and reliability, which proves that SPLM is better than others. This paper can be useful for the researchers to work on this field. The security study of live virtual machine migration in this paper provides a certain reference for the research of virtualization security, and is of great significance.},
booktitle = {Proceedings of the 4th ACM International Workshop on Security in Cloud Computing},
pages = {2–9},
numpages = {8},
keywords = {virtualization, cloud computing, virtual machine, security, live migration},
location = {Xi'an, China},
series = {SCC '16}
}

@inproceedings{10.1109/CCGrid.2015.91,
author = {Naskos, Athanasios and Stachtiari, Emmanouela and Gounaris, Anastasios and Katsaros, Panagiotis and Tsoumakos, Dimitrios and Konstantinou, Ioannis and Sioutas, Spyros},
title = {Dependable Horizontal Scaling Based on Probabilistic Model Checking},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.91},
doi = {10.1109/CCGrid.2015.91},
abstract = {The focus of this work is the on-demand resource provisioning in cloud computing, which is commonly referred to as cloud elasticity. Although a lot of effort has been invested in developing systems and mechanisms that enable elasticity, the elasticity decision policies tend to be designed without quantifying or guaranteeing the quality of their operation. We present an approach towards the development of more formalized and dependable elasticity policies. We make two distinct contributions. First, we propose an extensible approach to enforcing elasticity through the dynamic instantiation and online quantitative verification of Markov Decision Processes (MDP) using probabilistic model checking. Second, various concrete elasticity models and elasticity policies are studied. We evaluate the decision policies using traces from a real NoSQL database cluster under constantly evolving external load. We reason about the behaviour of different modeling and elasticity policy options and we show that our proposal can improve upon the state-of-the-art in significantly decreasing under-provisioning while avoiding over-provisioning.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {31–40},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/2791405.2791446,
author = {Shrimali, Bela and Patel, Hiren},
title = {Performance Based Energy Efficient Techniques For VM Allocation In Cloud Environment},
year = {2015},
isbn = {9781450333610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791405.2791446},
doi = {10.1145/2791405.2791446},
abstract = {Cloud computing is emerging as a new paradigm for providing different services like platform, infrastructure and software as a large scale distributed computing applications via Internet. Computing resources are available in Cloud through virtualization. It divides a physical machine into many half or full isolated machines (known as Virtual Machines-VMs) using various allocation techniques. To identify a technique that can satisfy a quality of service in consideration of energy consumption in Cloud environment is one of the challenging issues for Virtual Machine allocation in Cloud as there are tradeoffs between energy consumption and performance. In the present research, we aim to survey various techniques that combine energy efficiency and performance. Hence, different real world virtual machine allocation policies are explored and the performance based energy efficient techniques for VM allocation are discussed. This survey may assist the researchers who wish to step in to the domain of performance based energy efficient VM techniques.},
booktitle = {Proceedings of the Third International Symposium on Women in Computing and Informatics},
pages = {477–486},
numpages = {10},
keywords = {Virtual machine, Cloud computing, Energy efficient, Performance},
location = {Kochi, India},
series = {WCI '15}
}

@inproceedings{10.1145/3064911.3069397,
author = {Fujimoto, Richard M. and Hunter, Michael and Biswas, Aradhya and Jackson, Mark and Neal, SaBra},
title = {Power Efficient Distributed Simulation},
year = {2017},
isbn = {9781450344890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064911.3069397},
doi = {10.1145/3064911.3069397},
abstract = {Energy and power consumption have become important concerns for many computing systems ranging from embedded and mobile systems operating on battery-powered devices to high performance and cloud computing applications running on supercomputers and in data centers. To date, only a limited amount of work has considered power consumption in parallel and distributed simulations. A variety of options to analyze and explore power consumption in distributed simulations are discussed. These options range from design decisions in developing the simulation model to selection of algorithms in distributed simulation middleware to exploitation of hardware techniques. Work to characterize the power and energy consumed by different elements of parallel and distributed simulation systems are discussed and empirical measurements presented to quantify energy and power use, suggestive of directions for future research in this area.},
booktitle = {Proceedings of the 2017 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {77–88},
numpages = {12},
keywords = {power aware computing, distributed simulation, parallel discrete event simulation},
location = {Singapore, Republic of Singapore},
series = {SIGSIM-PADS '17}
}

@inproceedings{10.1145/3030207.3044530,
author = {Michael, Nicolas and Ramannavar, Nitin and Shen, Yixiao and Patil, Sheetal and Sung, Jan-Lung},
title = {CloudPerf: A Performance Test Framework for Distributed and Dynamic Multi-Tenant Environments},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3044530},
doi = {10.1145/3030207.3044530},
abstract = {The evolution of cloud-computing imposes many challenges on performance testing and requires not only a different approach and methodology of performance evaluation and analysis, but also specialized tools and frameworks to support such work. In traditional performance testing, typically a single workload was run against a static test configuration. The main metrics derived from such experiments included throughput, response times, and system utilization at steady-state. While this may have been sufficient in the past, where in many cases a single application was run on dedicated hardware, this approach is no longer suitable for cloud-based deployments. Whether private or public cloud, such environments typically host a variety of applications on distributed shared hardware resources, simultaneously accessed by a large number of tenants running heterogeneous workloads. The number of tenants as well as their activity and resource needs dynamically change over time, and the cloud infrastructure reacts to this by reallocating existing or provisioning new resources. Besides metrics such as the number of tenants and overall resource utilization, performance testing in the cloud must be able to answer many more questions: How is the quality of service of a tenant impacted by the constantly changing activity of other tenants? How long does it take the cloud infrastructure to react to changes in demand, and what is the effect on tenants while it does so? How well are service level agreements met? What is the resource consumption of individual tenants? How can global performance metrics on application- and system-level in a distributed system be correlated to an individual tenant's perceived performance?In this paper we present CloudPerf, a performance test framework specifically designed for distributed and dynamic multi-tenant environments, capable of answering all of the above questions, and more. CloudPerf consists of a distributed harness, a protocol-independent load generator and workload modeling framework, an extensible statistics framework with live-monitoring and post-analysis tools, interfaces for cloud deployment operations, and a rich set of both low-level as well as high-level workloads from different domains.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {189–200},
numpages = {12},
keywords = {statistics collection, workload modeling, performance testing, cloud, multi-tenancy, load generation},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/3299819.3299836,
author = {Zheng, Wanwan and Jin, Mingzhe},
title = {Do We Need More Training Samples For Text Classification?},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299836},
doi = {10.1145/3299819.3299836},
abstract = {In recent years, with the rise of exceptional cloud computing technologies, machine learning approach in solving complex problems has been greatly accelerated. In the field of text classification, machine learning is a technology of providing computers the ability to learn and predict tasks without being explicitly labeled, and it is said that enough data are needed in order to let a machine to learn. However, more data tend to cause overfitting in machine learning algorithms, and there is no object criteria in deciding how many samples are required to achieve a desired level of performance. This article addresses this problem by using feature selection method. In our experiments, feature selection is proved to be able to decrease 66.67\% at the largest of the required size of training dataset. Meanwhile, the kappa coefficient as a performance measure of classifiers could increase 11 points at the maximum. Furthermore, feature selection as a technology to remove irrelevant features was found be able to prevent overfitting to a great extent.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {121–128},
numpages = {8},
keywords = {Size of Dataset, Text Classification, Feature Selection},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@article{10.1145/3132041,
author = {Slivar, Ivan and Suznjevic, Mirko and Skorin-Kapov, Lea},
title = {Game Categorization for Deriving QoE-Driven Video Encoding Configuration Strategies for Cloud Gaming},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3132041},
doi = {10.1145/3132041},
abstract = {Cloud gaming has been recognized as a promising shift in the online game industry, with the aim of implementing the “on demand” service concept that has achieved market success in other areas of digital entertainment such as movies and TV shows. The concepts of cloud computing are leveraged to render the game scene as a video stream that is then delivered to players in real-time. The main advantage of this approach is the capability of delivering high-quality graphics games to any type of end user device; however, at the cost of high bandwidth consumption and strict latency requirements. A key challenge faced by cloud game providers lies in configuring the video encoding parameters so as to maximize player Quality of Experience (QoE) while meeting bandwidth availability constraints. In this article, we tackle one aspect of this problem by addressing the following research question: Is it possible to improve service adaptation based on information about the characteristics of the game being streamed? To answer this question, two main challenges need to be addressed: the need for different QoE-driven video encoding (re-)configuration strategies for different categories of games, and how to determine a relevant game categorization to be used for assigning appropriate configuration strategies. We investigate these problems by conducting two subjective laboratory studies with a total of 80 players and three different games. Results indicate that different strategies should likely be applied for different types of games, and show that existing game classifications are not necessarily suitable for differentiating game types in this context. We thus further analyze objective video metrics of collected game play video traces as well as player actions per minute and use this as input data for clustering of games into two clusters. Subjective results verify that different video encoding configuration strategies may be applied to games belonging to different clusters.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jun},
articleno = {56},
numpages = {24},
keywords = {video codec configuration strategies, game categorization, Quality of Experience, Cloud gaming}
}

@article{10.1145/3164537,
author = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Bauer, Andr\'{e} and Papadopoulos, Alessandro V. and Epema, Dick and Iosup, Alexandru},
title = {An Experimental Performance Evaluation of Autoscalers for Complex Workflows},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3164537},
doi = {10.1145/3164537},
abstract = {Elasticity is one of the main features of cloud computing allowing customers to scale their resources based on the workload. Many autoscalers have been proposed in the past decade to decide on behalf of cloud customers when and how to provision resources to a cloud application based on the workload utilizing cloud elasticity features. However, in prior work, when a new policy is proposed, it is seldom compared to the state-of-the-art, and is often compared only to static provisioning using a predefined quality of service target. This reduces the ability of cloud customers and of cloud operators to choose and deploy an autoscaling policy, as there is seldom enough analysis on the performance of the autoscalers in different operating conditions and with different applications. In our work, we conduct an experimental performance evaluation of autoscaling policies, using as application model workflows, a popular formalism for automating resource management for applications with well-defined yet complex structures. We present a detailed comparative study of general state-of-the-art autoscaling policies, along with two new workflow-specific policies. To understand the performance differences between the seven policies, we conduct various experiments and compare their performance in both pairwise and group comparisons. We report both individual and aggregated metrics. As many workflows have deadline requirements on the tasks, we study the effect of autoscaling on workflow deadlines. Additionally, we look into the effect of autoscaling on the accounted and hourly based charged costs, and we evaluate performance variability caused by the autoscaler selection for each group of workflow sizes. Our results highlight the trade-offs between the suggested policies, how they can impact meeting the deadlines, and how they perform in different operating conditions, thus enabling a better understanding of the current state-of-the-art.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {apr},
articleno = {8},
numpages = {32},
keywords = {scientific workflows, Autoscaling, metrics, elasticity, benchmarking}
}

@inproceedings{10.1145/3123266.3123384,
author = {Zhu, Yifei and Liu, Jiangchuan and Wang, Zhi and Zhang, Cong},
title = {When Cloud Meets Uncertain Crowd: An Auction Approach for Crowdsourced Livecast Transcoding},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123384},
doi = {10.1145/3123266.3123384},
abstract = {In the emerging crowd sourced live cast services, numerous amateur broadcasters live stream their video contents to worldwide viewers and constantly interact with them through chat messages. Live video contents are transcoded into multiple quality versions to better service viewers with different network and device configurations. Cloud computing becomes a natural choice to handle these computational intensive tasks due to its elasticity and the "pay-as-you-go" billing model. However, given the significantly large number of concurrent channel numbers and the diverse viewer geo-distributions in this new crowd sourced live cast service, even the cloud becomes significantly expensive to cover the whole community and inadequate in fulfilling the latency requirement. In this paper, after observing the abundant computational resources residing in end viewers, we propose a Cloud-Crowd collaborative system, C2, which combines end viewers with cloud to perform video transcoding in a cost-efficient way. To quantify the heterogeneity and uncertainty of viewers and pass the asymmetric information barrier, we incorporate statistical descriptions into our bidding language and design truthful auctions to recruit stable viewers with appropriate incentives. We further tailor redundancy strategies for workloads with different Quality of Service requirements to improve the stability of our system. Desirable economic properties, like social efficiency, ex-post incentive compatibility, individual rationality, are proved to be guaranteed in our studied scenarios. Using traces captured from the popular Twitch platform, we show that C2 achieves up to 93\% more cost saving than a pure cloud-based solution, and significantly outperforms other baseline approaches in both social welfare and system stability.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1372–1380},
numpages = {9},
keywords = {cloud computing, auction mechanism, uncertainty, edge computing, crowdsourced livecast transcoding},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3069593.3069609,
author = {Khidzir, Nik Zulkarnaen and Ghani, Wan Safra Diyana Wan Abdul and Guan, Tan Tse},
title = {Cloud-Based Mobile-Retail Application for Textile Cyberpreneurs: Task-Technology Fit Perspective Analysis},
year = {2017},
isbn = {9781450348683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3069593.3069609},
doi = {10.1145/3069593.3069609},
abstract = {Cloud computing and mobile computing paradigms have enhanced the usage of current technology among people from various sectors to perform certain required tasks. The combination of these two paradigms have also created the existence of cloud-based mobile applications that are designed to be useful in business environments, such as to be used by textile cyberpreneurs for m-retail transaction. As cloud adoption was previously low among Malaysian entrepreneurs, the usage intention factors of related cloud technology and services should be determined for better clarification and future technology enhancements. Besides, studies on m-retail or m-shopping adoption in Malaysia were merely focused on customer's perspectives rather than retailer's perspectives. In measuring the suitability of Cloud-based M-Retail (CBMR) application with the task requirements of textile cyberpreneurs, Task-Technology Fit (TTF) model is used as the basis of this research. Results from a pilot study, through a survey of selected group of textile cyberpreneurs shows instrument's reliability and positive feedbacks to support the intention to use the technology. The future direction of the study in which to apply the instruments to a larger group of respondents is also discussed along with its potential contribution.},
booktitle = {Proceedings of the International Conference on High Performance Compilation, Computing and Communications},
pages = {65–70},
numpages = {6},
keywords = {mobile retail, cloud-based mobile application, cyberpreneurship, mobile shopping, behavioral intention, task-technology fit},
location = {Kuala Lumpur, Malaysia},
series = {HP3C-2017}
}

@inproceedings{10.1109/MICRO.2018.00056,
author = {Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
title = {CounterMiner: Mining Big Performance Data from Hardware Counters},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00056},
doi = {10.1109/MICRO.2018.00056},
abstract = {Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events1. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running ina"24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance.In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3\% to 7.7\% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {613–626},
numpages = {14},
keywords = {performance counters, computer architecture, big data, data mining},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1145/3199478.3199483,
author = {Castillo, Alexy Gene and Telan, Sherwin M. and Palaoag, Thelma},
title = {Cloud-Based Data Mining Framework: A Model to Improve Maternal Healthcare},
year = {2018},
isbn = {9781450363617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199478.3199483},
doi = {10.1145/3199478.3199483},
abstract = {The foundation of quality health care depends upon the presence of competent health personnel working in a situation where prescriptions and health supplies are accessible when required and in sufficient quantity and of guaranteed quality. This paper conduces to propound a decision support framework model for the Department of Health (DOH) which able to innovate the acquisition and allocation management of medicines and health supplies with the aim of improving the maternal healthcare in the Philippines.In-depth interviews were conducted to DOH officials and facility managers of Rural Health Units (RHU) and in the 3rd district of Albay, Bicol Philippines. Data triangulation and literature review are employed to design the framework. Finally to assess its applicability, a simulative-evaluation is conveyed.Respondents reported on the unreliability of obtaining healthcare supplies for RHU's, which results untimely and suboptimal rendering of healthcare services. Also, insufficient provision of medicines from the government and lack of accountability within the supply system due of inadequate and incoherent terminal reports were revealed to contribute to the current situation.To address the mentioned challenges, this study recommends the consideration of the proposed framework model employing cloud computing and data mining to remarkably improve the administration on the provision of medicines and health supplies, guaranteeing its auspicious accessibility for the benefit of Filipino pregnant women ensuring their health as carriers of the lives to be born as the future of the nation.},
booktitle = {Proceedings of the 2nd International Conference on Cryptography, Security and Privacy},
pages = {21–28},
numpages = {8},
keywords = {Maternal Health, Cloud Computing, Data Mining},
location = {Guiyang, China},
series = {ICCSP 2018}
}

@inproceedings{10.1145/2925426.2926276,
author = {Liu, Longjun and Sun, Hongbin and Li, Chao and Hu, Yang and Zheng, Nanning and Li, Tao},
title = {Towards an Adaptive Multi-Power-Source Datacenter},
year = {2016},
isbn = {9781450343619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2925426.2926276},
doi = {10.1145/2925426.2926276},
abstract = {Big data and cloud computing are accelerating the capacity growth of datacenters all over the world. Their energy costs and environmental issues have pushed datacenter operators to explore and integrate alternative energy sources, such as various renewable energy supplies and energy storage devices. Designing datacenters powered by multi-power supplies in the smart grid environment is becoming a promising trend in the next few decades. However, gracefully provisioning various power sources and efficiently manage them in datacenter is a significant challenge.In this paper, we explore an unconventional fine-grained power distribution architecture for multi-source powered datacenters. We thoroughly investigate how to deliver and manage multiple power sources from the power generation plant outside of the datacenter to datacenter inside. We then propose a novel Power Switch Network (PSN) for datacenters. PSN is a reconfigurable multi-power-source distribution architecture which enables datacenter to distribute various power sources with a fine-grained manner. Moreover, a tailored machine learning based power sources management framework is proposed for PSN to dynamically select different power sources and optimize user-demanded performance metrics. Compared with the conventional single-switch system, evaluation results show that PSN could improve solar energy utilization by 39.6\%, reduce utility power cost by 11.1\% and improve workload performance by 33.8\%, meanwhile enhancing battery lifetime by 9.3\%. We expect that our work could provide valuable guidelines for the emerging multi-power-source datacenter to improve their efficiency, sustainability and economy.},
booktitle = {Proceedings of the 2016 International Conference on Supercomputing},
articleno = {11},
numpages = {11},
keywords = {Power Distribution Architecture, Multi-Power-Source Management, Renewable Energy, Datacenters},
location = {Istanbul, Turkey},
series = {ICS '16}
}

@inproceedings{10.1145/3078505.3078530,
author = {Cohen, Maxime C. and Keller, Philipp and Mirrokni, Vahab and Zadimoghadddam, Morteza},
title = {Overcommitment in Cloud Services Bin Packing with Chance Constraints},
year = {2017},
isbn = {9781450350327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078505.3078530},
doi = {10.1145/3078505.3078530},
abstract = {This paper considers a traditional problem of resource allocation, scheduling jobs on machines. One such recent application is cloud computing, where jobs arrive in an online fashion with capacity requirements and need to be immediately scheduled on physical machines in data centers. It is often observed that the requested capacities are not fully utilized, hence offering an opportunity to employ an overcommitment policy, i.e., selling resources beyond capacity. Setting the right overcommitment level can induce a significant cost reduction for the cloud provider, while only inducing a very low risk of violating capacity constraints. We introduce and study a model that quantifies the value of overcommitment by modeling the problem as a bin packing with chance constraints. We then propose an alternative formulation that transforms each chance constraint into a submodular function. We show that our model captures the risk pooling effect and can guide scheduling and overcommitment decisions. We also develop a family of online algorithms that are intuitive, easy to implement and provide a constant factor guarantee from optimal. Finally, we calibrate our model using realistic workload data, and test our approach in a practical setting. Our analysis and experiments illustrate the benefit of overcommitment in cloud services, and suggest a cost reduction of 1.5\% to 17\% depending on the provider's risk tolerance.},
booktitle = {Proceedings of the 2017 ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems},
pages = {7},
numpages = {1},
keywords = {bin packing, job scheduling, chance constraints},
location = {Urbana-Champaign, Illinois, USA},
series = {SIGMETRICS '17 Abstracts}
}

@article{10.1145/3143314.3078530,
author = {Cohen, Maxime C. and Keller, Philipp and Mirrokni, Vahab and Zadimoghadddam, Morteza},
title = {Overcommitment in Cloud Services Bin Packing with Chance Constraints},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/3143314.3078530},
doi = {10.1145/3143314.3078530},
abstract = {This paper considers a traditional problem of resource allocation, scheduling jobs on machines. One such recent application is cloud computing, where jobs arrive in an online fashion with capacity requirements and need to be immediately scheduled on physical machines in data centers. It is often observed that the requested capacities are not fully utilized, hence offering an opportunity to employ an overcommitment policy, i.e., selling resources beyond capacity. Setting the right overcommitment level can induce a significant cost reduction for the cloud provider, while only inducing a very low risk of violating capacity constraints. We introduce and study a model that quantifies the value of overcommitment by modeling the problem as a bin packing with chance constraints. We then propose an alternative formulation that transforms each chance constraint into a submodular function. We show that our model captures the risk pooling effect and can guide scheduling and overcommitment decisions. We also develop a family of online algorithms that are intuitive, easy to implement and provide a constant factor guarantee from optimal. Finally, we calibrate our model using realistic workload data, and test our approach in a practical setting. Our analysis and experiments illustrate the benefit of overcommitment in cloud services, and suggest a cost reduction of 1.5\% to 17\% depending on the provider's risk tolerance.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {7},
numpages = {1},
keywords = {bin packing, job scheduling, chance constraints}
}

@inproceedings{10.1145/2731186.2731202,
author = {Wang, Hui and Isci, Canturk and Subramanian, Lavanya and Choi, Jongmoo and Qian, Depei and Mutlu, Onur},
title = {A-DRM: Architecture-Aware Distributed Resource Management of Virtualized Clusters},
year = {2015},
isbn = {9781450334501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2731186.2731202},
doi = {10.1145/2731186.2731202},
abstract = {Virtualization technologies has been widely adopted by large-scale cloud computing platforms. These virtualized systems employ distributed resource management (DRM) to achieve high resource utilization and energy savings by dynamically migrating and consolidating virtual machines. DRM schemes usually use operating-system-level metrics, such as CPU utilization, memory capacity demand and I/O utilization, to detect and balance resource contention. However, they are oblivious to microarchitecture-level resource interference (e.g., memory bandwidth contention between different VMs running on a host), which is currently not exposed to the operating system.We observe that the lack of visibility into microarchitecture-level resource interference significantly impacts the performance of virtualized systems. Motivated by this observation, we propose a novel architecture-aware DRM scheme (ADRM), that takes into account microarchitecture-level resource interference when making migration decisions in a virtualized cluster. ADRM makes use of three core techniques: 1) a profiler to monitor the microarchitecture-level resource usage behavior online for each physical host, 2) a memory bandwidth interference model to assess the interference degree among virtual machines on a host, and 3) a cost-benefit analysis to determine a candidate virtual machine and a host for migration.Real system experiments on thirty randomly selected combinations of applications from the CPU2006, PARSEC, STREAM, NAS Parallel Benchmark suites in a four-host virtualized cluster show that ADRM can improve performance by up to 26.55\%, with an average of 9.67\%, compared to traditional DRM schemes that lack visibility into microarchitecture-level resource utilization and contention.},
booktitle = {Proceedings of the 11th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {93–106},
numpages = {14},
keywords = {virtualization, microarchitecture, resource management, live migration, performance counters},
location = {Istanbul, Turkey},
series = {VEE '15}
}

@article{10.1145/2817817.2731202,
author = {Wang, Hui and Isci, Canturk and Subramanian, Lavanya and Choi, Jongmoo and Qian, Depei and Mutlu, Onur},
title = {A-DRM: Architecture-Aware Distributed Resource Management of Virtualized Clusters},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/2817817.2731202},
doi = {10.1145/2817817.2731202},
abstract = {Virtualization technologies has been widely adopted by large-scale cloud computing platforms. These virtualized systems employ distributed resource management (DRM) to achieve high resource utilization and energy savings by dynamically migrating and consolidating virtual machines. DRM schemes usually use operating-system-level metrics, such as CPU utilization, memory capacity demand and I/O utilization, to detect and balance resource contention. However, they are oblivious to microarchitecture-level resource interference (e.g., memory bandwidth contention between different VMs running on a host), which is currently not exposed to the operating system.We observe that the lack of visibility into microarchitecture-level resource interference significantly impacts the performance of virtualized systems. Motivated by this observation, we propose a novel architecture-aware DRM scheme (ADRM), that takes into account microarchitecture-level resource interference when making migration decisions in a virtualized cluster. ADRM makes use of three core techniques: 1) a profiler to monitor the microarchitecture-level resource usage behavior online for each physical host, 2) a memory bandwidth interference model to assess the interference degree among virtual machines on a host, and 3) a cost-benefit analysis to determine a candidate virtual machine and a host for migration.Real system experiments on thirty randomly selected combinations of applications from the CPU2006, PARSEC, STREAM, NAS Parallel Benchmark suites in a four-host virtualized cluster show that ADRM can improve performance by up to 26.55\%, with an average of 9.67\%, compared to traditional DRM schemes that lack visibility into microarchitecture-level resource utilization and contention.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {93–106},
numpages = {14},
keywords = {performance counters, microarchitecture, live migration, virtualization, resource management}
}

@inproceedings{10.1145/3285017.3285019,
author = {Meloni, P. and Loi, D. and Deriu, G. and Pimentel, A. D. and Sapra, D. and Moser, B. and Shepeleva, N. and Conti, F. and Benini, L. and Ripolles, O. and Solans, D. and Pintor, M. and Biggio, B. and Stefanov, T. and Minakova, S. and Fragoulis, N. and Theodorakopoulos, I. and Masin, M. and Palumbo, F.},
title = {ALOHA: An Architectural-Aware Framework for Deep Learning at the Edge},
year = {2018},
isbn = {9781450365987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3285017.3285019},
doi = {10.1145/3285017.3285019},
abstract = {Novel Deep Learning (DL) algorithms show ever-increasing accuracy and precision in multiple application domains. However, some steps further are needed towards the ubiquitous adoption of this kind of instrument. First, effort and skills required to develop new DL models, or to adapt existing ones to new use-cases, are hardly available for small- and medium-sized businesses. Second, DL inference must be brought at the edge, to overcome limitations posed by the classically-used cloud computing paradigm. This requires implementation on low-energy computing nodes, often heterogenous and parallel, that are usually more complex to program and to manage. This work describes the ALOHA framework, that proposes a solution to these issue by means of an integrated tool flow that automates most phases of the development process. The framework introduces architecture-awareness, considering the target inference platform very early, already during algorithm selection, and driving the optimal porting of the resulting embedded application. Moreover it considers security, power efficiency and adaptiveness as main objectives during the whole development process.},
booktitle = {Proceedings of the Workshop on INTelligent Embedded Systems Architectures and Applications},
pages = {19–26},
numpages = {8},
keywords = {convolutional neural networks, deep learning, computer aided design},
location = {Turin, Italy},
series = {INTESA '18}
}

@article{10.1145/2675353,
author = {Chen, Jinzhu and Tan, Rui and Wang, Yu and Xing, Guoliang and Wang, Xiaorui and Wang, Xiaodong and Punch, Bill and Colbry, Dirk},
title = {A Sensor System for High-Fidelity Temperature Distribution Forecasting in Data Centers},
year = {2014},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/2675353},
doi = {10.1145/2675353},
abstract = {Data centers have become a critical computing infrastructure in the era of cloud computing. Temperature monitoring and forecasting are essential for preventing server shutdowns because of overheating and improving a data center’s energy efficiency. This article presents a novel cyber-physical approach for temperature forecasting in data centers, one that integrates Computational Fluid Dynamics (CFD) modeling, in situ wireless sensing, and real-time data-driven prediction. To ensure forecasting fidelity, we leverage the realistic physical thermodynamic models of CFD to generate transient temperature distribution and calibrate it using sensor feedback. Both simulated temperature distribution and sensor measurements are then used to train a real-time prediction algorithm. As a result, our approach reduces not only the computational complexity of online temperature modeling and prediction, but also the number of deployed sensors, which enables a portable, noninvasive thermal monitoring solution that does not rely on the infrastructure of a monitored data center. We extensively evaluated the proposed system on a rack of 15 servers and a testbed of five racks and 229 servers in a small-scale production data center. Our results show that our system can predict the temperature evolution of servers with highly dynamic workloads at an average error of 0.52○C, within a duration up to 10 minutes. Moreover, our approach can reduce the required number of sensors by 67\% while maintaining desirable prediction fidelity.},
journal = {ACM Trans. Sen. Netw.},
month = {dec},
articleno = {30},
numpages = {25},
keywords = {computational fluid dynamics, cyber-physical system, temperature prediction, Data center, wireless sensor network}
}

@inproceedings{10.1145/3200947.3208069,
author = {Georgakopoulos, Spiros V. and Tasoulis, Sotiris K. and Vrahatis, Aristidis G. and Plagianakos, Vassilis P.},
title = {Convolutional Neural Networks for Toxic Comment Classification},
year = {2018},
isbn = {9781450364331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3200947.3208069},
doi = {10.1145/3200947.3208069},
abstract = {Flood of information is produced in a daily basis through the global internet usage arising from the online interactive communications among users. While this situation contributes significantly to the quality of human life, unfortunately it involves enormous dangers, since online texts with high toxicity can cause personal attacks, online harassment and bullying behaviors. This has triggered both industrial and research community in the last few years while there are several attempts to identify an efficient model for online toxic comment prediction. However, these steps are still in their infancy and new approaches and frameworks are required. On parallel, the data explosion that appears constantly, makes the construction of new machine learning computational tools for managing this information, an imperative need. Thankfully advances in hardware, cloud computing and big data management allow the development of Deep Learning approaches appearing very promising performance so far. For text classification in particular the use of Convolutional Neural Networks (CNN) have recently been proposed approaching text analytics in a modern manner emphasizing in the structure of words in a document. In this work, we employ this approach to discover toxic comments in a large pool of documents provided by a current Kaggle's competition regarding Wikipedia's talk page edits. To justify this decision we choose to compare CNNs against the traditional bag-of-words approach for text analysis combined with a selection of algorithms proven to be very effective in text classification. The reported results provide enough evidence that CNN enhance toxic comment classification reinforcing research interest towards this direction.},
booktitle = {Proceedings of the 10th Hellenic Conference on Artificial Intelligence},
articleno = {35},
numpages = {6},
keywords = {CNN for Text Mining, Word Embeddings, word2vec, Text Classification, Text mining, Toxic Text Classification, Convolutional Neural Networks},
location = {Patras, Greece},
series = {SETN '18}
}

@inproceedings{10.1145/3313150.3313228,
author = {Tange, Koen and De Donno, Michele and Fafoutis, Xenofon and Dragoni, Nicola},
title = {Towards a Systematic Survey of Industrial IoT Security Requirements: Research Method and Quantitative Analysis},
year = {2019},
isbn = {9781450366984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313150.3313228},
doi = {10.1145/3313150.3313228},
abstract = {Industry 4.0 and, in particular, Industrial Internet of Things (IIoT) represent two of the major automation and data exchange trends of the 21st century, driving a steady increase in the number of smart embedded devices used by industrial applications. However, IoT devices suffer from numerous security flaws, resulting in a number of large scale cyber-attacks. In this light, Fog computing, a relatively new paradigm born from the necessity of bridging the gap between Cloud computing and IoT, can be used as a security solution for the IIoT. To achieve this, the first step is to clearly identify the security requirements of the IIoT that can be subsequently used to design security solutions based on Fog computing. With this in mind, our paper represents a preliminary work towards a systematic literature review of IIoT security requirements. We focus on two key steps of the review: (1) the research method that will be used in the systematic work and (2) a quantitative analysis of the results produced by the study selection process. This lays the necessary foundations to enable the use of Fog computing as a security solution for the IIoT.},
booktitle = {Proceedings of the Workshop on Fog Computing and the IoT},
pages = {56–63},
numpages = {8},
keywords = {industry 4.0, fog computing, security, systematic literature review, industrial internet of things, IIoT},
location = {Montreal, Quebec, Canada},
series = {IoT-Fog '19}
}

@inproceedings{10.1145/2846012.2846052,
author = {Norta, Alex and Othman, Anis Ben and Taveter, Kuldar},
title = {Conflict-Resolution Lifecycles for Governed Decentralized Autonomous Organization Collaboration},
year = {2015},
isbn = {9781450340700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2846012.2846052},
doi = {10.1145/2846012.2846052},
abstract = {Recent blockchain-technology related innovations enable the governance of collaborating decentralized autonomous organizations (DAO) to engage in agile business-network collaborations that are based on the novel concept of smart contracting. DAOs utilize service-oriented cloud computing in a loosely coupled collaboration lifecycle with the main steps of setup, enactment, possible rollbacks and finally, an orderly termination. This lifecycle supports the selection of services provided and used by DAOs, smart contract negotiations, and behavior monitoring during enactment with the potential for breach management. Based on a sound understanding of the collaboration lifecycle in a Governance- as-a-Service (GaaS)-platform, a new type of conflict management must safeguard business-semantics induced consistency rules. This conflict management involves breach detection with recovery aspects. To fill the detected gap, we employ a formal design-notation that comprises the definition of structural and behavioral properties for exploring conflict-related exception- and compensation management during a decentralized collaboration. With the formal approach, we generate a highly dependable DAO-GaaS conflict model that does not collapse under left-behind clutter such as orphaned processes and exponentially growing database entries that require an unacceptable periodic GaaS reset.},
booktitle = {Proceedings of the 2015 2nd International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {244–257},
numpages = {14},
keywords = {smart contract, business process, conflict resolution, service orientation, e-governance, Industry 4.0, open cloud ecosystem, Decentralized autonomous organization},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '15}
}

@inproceedings{10.5555/2722129.2722142,
author = {Buchet, Micka\"{e}l and Chazal, Fr\'{e}d\'{e}ric and Oudot, Steve Y. and Sheehy, Donald R.},
title = {Efficient and Robust Persistent Homology for Measures},
year = {2015},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {A new paradigm for point cloud data analysis has emerged recently, where point clouds are no longer treated as mere compact sets but rather as empirical measures. A notion of distance to such measures has been defined and shown to be stable with respect to perturbations of the measure. This distance can easily be computed pointwise in the case of a point cloud, but its sublevel-sets, which carry the geometric information about the measure, remain hard to compute or approximate. This makes it challenging to adapt many powerful techniques based on the Euclidean distance to a point cloud to the more general setting of the distance to a measure on a metric space.We propose an efficient and reliable scheme to approximate the topological structure of the family of sublevel-sets of the distance to a measure. We obtain an algorithm for approximating the persistent homology of the distance to an empirical measure that works in arbitrary metric spaces. Precise quality and complexity guarantees are given with a discussion on the behavior of our approach in practice.},
booktitle = {Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {168–180},
numpages = {13},
location = {San Diego, California},
series = {SODA '15}
}

@inproceedings{10.1145/3017680.3017820,
author = {Siever, Bill and Rogers, Michael P.},
title = {An IoTa of IoT (Abstract Only)},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3017820},
doi = {10.1145/3017680.3017820},
abstract = {Internet of Things (IoT) devices -- networked microcontrollers with attached sensors and outputs (LEDs, actuators, etc.) -- are becoming ubiquitous in the home (e.g., smart light bulbs, security systems), on the road (e.g., smart parking meters, traffic control), in industry (e.g., equipment monitoring, asset tracking) and in healthcare (e.g., fitness monitors, drug monitors). Consequently, IoT provides an opportunity to demonstrate the pervasiveness and social relevance of computing. Moreover, today's hobbyist- oriented IoT platforms empower entry-level students to create meaningful, real-world IoT applications. This allows rich computer science topics, such as event driven programming, concurrency, networking, information representation, cloud computing, etc., to be introduced earlier in the curriculum. Most importantly, IoT examples provide a compelling context for students to hone their critical thinking skills while solving engaging, real-world problems. Faculty interested in including IoT topics face several challenges: selecting a suitable set of topics, identifying an appropriate pedagogical approach, and, perhaps most daunting, choosing a cost-effective platform that lends itself to classroom use. This workshop will introduce the basic terms and technologies in IoT, discuss issues that arise when including IoT topics in classes, compare and contrast the most popular platforms for IoT, and walk participants through several classroom-tested, hands-on examples using a classroom-friendly platform (Particle's Photon) where they create both Wi-Fi-based IoT devices and corresponding web apps. Participants will need a laptop (any OS) with Internet access.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {742},
numpages = {1},
keywords = {CS1, IoT, intro. C.S.},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1145/3318464.3383130,
author = {Khandelwal, Anurag and Kejariwal, Arun and Ramasamy, Karthikeyan},
title = {Le Taureau: Deconstructing the Serverless Landscape \&amp; A Look Forward},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383130},
doi = {10.1145/3318464.3383130},
abstract = {Akin to the natural evolution of programming in assembly language to high-level languages, serverless computing represents the next frontier in the evolution of cloud computing: bare metal -&gt; virtual machines -&gt; containers -&gt; serverless. The genesis of serverless computing can be traced back to the fundamental need of enabling a programmer to singularly focus on writing application code in a high-level language and isolating all facets of system management (for example, but not limited to, instance selection, scaling, deployment, logging, monitoring, fault tolerance and so on). This is particularly critical in light of today's, increasingly tightening, time-to-market constraints. Currently, serverless computing is supported by leading public cloud vendors, such as AWS Lambda, Google Cloud Functions, Azure Cloud Functions and others. While this is an important step in the right direction, there are many challenges going forward. For instance, but not limited to, how to enable support for dynamic optimization, how to extend support for stateful computation, how to efficiently bin-pack applications, how to support hardware heterogeneity (this will be key especially in light of the emergence of hardware accelerators for deep learning workloads). Inspired by Picasso's Le Taureau, in the tutorial proposed herein, we shall deconstruct evolution of serverless --- the overarching intent being to facilitate better understanding of the serverless landscape. This, we hope, would help push the innovation frontier on both fronts, the paradigm itself and the applications built atop of it.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2641–2650},
numpages = {10},
keywords = {ephemeral storage, distributed systems, serverless computing, real-time streaming, data analytics, cloud computing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1145/3019596,
author = {Kistowski, J\'{o}akim Von and Herbst, Nikolas and Kounev, Samuel and Groenda, Henning and Stier, Christian and Lehrig, Sebastian},
title = {Modeling and Extracting Load Intensity Profiles},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3019596},
doi = {10.1145/3019596},
abstract = {Today’s system developers and operators face the challenge of creating software systems that make efficient use of dynamically allocated resources under highly variable and dynamic load profiles, while at the same time delivering reliable performance. Autonomic controllers, for example, an advanced autoscaling mechanism in a cloud computing context, can benefit from an abstracted load model as knowledge to reconfigure on time and precisely. Existing workload characterization approaches have limited support to capture variations in the interarrival times of incoming work units over time (i.e., a variable load profile). For example, industrial and scientific benchmarks support constant or stepwise increasing load, or interarrival times defined by statistical distributions or recorded traces. These options show shortcomings either in representative character of load variation patterns or in abstraction and flexibility of their format.In this article, we present the Descartes Load Intensity Model (DLIM) approach addressing these issues. DLIM provides a modeling formalism for describing load intensity variations over time. A DLIM instance is a compact formal description of a load intensity trace. DLIM-based tools provide features for benchmarking, performance, and recorded load intensity trace analysis. As manually obtaining and maintaining DLIM instances becomes time consuming, we contribute three automated extraction methods and devised metrics for comparison and method selection. We discuss how these features are used to enhance system management approaches for adaptations during runtime, and how they are integrated into simulation contexts and enable benchmarking of elastic or adaptive behavior.We show that automatically extracted DLIM instances exhibit an average modeling error of 15.2\% over 10 different real-world traces that cover between 2 weeks and 7 months. These results underline DLIM model expressiveness. In terms of accuracy and processing speed, our proposed extraction methods for the descriptive models are comparable to existing time series decomposition methods. Additionally, we illustrate DLIM applicability by outlining approaches of workload modeling in systems engineering that employ or rely on our proposed load intensity modeling formalism.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {jan},
articleno = {23},
numpages = {28},
keywords = {load profile, model extraction, Load intensity variation, transformation, metamodeling, open workloads}
}

@article{10.1145/3264284,
author = {Squillante, Mark S.},
title = {Session Details: Special Issue on the Workshop on MAthematical Performance Modeling and Analysis (MAMA 2014)},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3264284},
doi = {10.1145/3264284},
abstract = {The complexity of computer systems, networks and applications, as well as the advancements in computer technology, continue to grow at a rapid pace. Mathematical analysis, modeling and optimization have been playing, and continue to play, an important role in research studies to investigate fundamental issues and trade-offs at the core of performance problems in the design and implementation of complex computer systems, networks and applications.On June 20, 2014, the 16th Workshop on MAthematical performance Modeling and Analysis (MAMA 2014) was held in Austin TX, USA, sponsored by ACM SIGMETRICS, and held in conjunction with SIGMETRICS 2014. This workshop seeks to bring together researchers working on the mathematical, methodological and theoretical aspects of performance analysis, modeling and optimization. It is intended to provide a forum at SIGMETRICS conferences for talks on early research in the more mathematical areas of computer performance analysis. These talks tend to be based on very recent research results (including work in progress) or on new research results that will be otherwise submitted only to a journal (or recently have been submitted to a journal). Thus, part of the goal is to complement and supplement the SIGMETRICS Conference program with such talks without removing any theoretical contributions from the main technical program. Furthermore, we continue to experience the desired result of having abstracts from previous MAMA workshops appear as full papers in the main program of subsequent SIGMETRICS and related conferences.All submissions were reviewed by at least 4 members of the program committee, from which a total of 13 were selected for presentation at the MAMA 2014 workshop. This special issue of Performance Evaluation Review includes extended abstracts relating to these presentations (arranged in the order of their presentation), which cover a wide range of topics in the area of mathematical performance analysis, modeling and optimization.The study of Gelenbe examines the backlog of energy and of data packets in a sensor node that harvests energy, computing the properties of energy and data backlogs and discussing system stability. Meyfroyt derives asymptotic results for the coverage ratio under a specific class of spatial stochastic models (Cooperative Sequential Adsorption) and investigates the scalability of the Trickle communication protocol algorithm. The study of Tune and Roughan applies the principle of maximum entropy to develop fast traffic matrix synthesis models, with the future goal of developing realistic spatio-temporal traffic matrices. Bradonji\'{c} et al. compare and contrast the capacity, congestion and reliability requirements for alternative connectivity models of large-scale data centers relative to fat trees. The study of Rochman et al. considers the problem of resource placement in network applications, based on a largescale service faced with regionally distributed demands for various resources in cloud computing. Xie and Lui investigate the design and analysis of a rating system and a mechanism to encourage users to participate in crowdsourcing and to incentivize workers to develop high-quality solutions. The study of Asadi et al. formulates a general problem for the joint per-user mode selection, connection activation and resource scheduling of connections using both LTE and WiFi resources within the context of device-todevice communications. Zheng and Tan consider a nonconvex joint rate and power control optimization to achieve egalitarian fairness (max-min weighted fairness) in wireless networks, exploiting the nonlinear Perron-Frobenius theory and nonnegative matrix theory. The study of Goldberg et al. derives an asymptotically optimal control policy for a stochastic capacity problem of dynamically matching supply resources and uncertain demand, based on connections with lost-sales inventory models. Ghaderi et al. investigate a dynamic stochastic bin packing problem, analyzing the fluid limits of the system under an asymptotic best-fit algorithm and showing it asymptotically minimizes the number of servers used in steady state. The study of Tizghadam and Leon-Garcia examines the impact of overlaying or removing a subgraph on the Moore-Penrose inverse of the Laplacian matrix of an existing network topology and proposes an iterative method to find key performance measures. Miyazawa considers a two-node generalized Jackson network in a phase-type setting as a special case of a Markov-modulated twodimensional reflecting random walk and analyzes the tail asymptotics for this reflecting process. The study of Squillante et al. investigates improvement in scalability of search in networks through the use of multiple random walks, deriving bounds on the hitting time to a set of nodes and on various performance metrics.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {sep},
numpages = {1}
}

@inproceedings{10.1145/3151848.3151850,
author = {Karadimce, Aleksandar and Davcev, Danco},
title = {Bayesian Network Model for Estimating User Satisfaction of Multimedia Cloud Services},
year = {2017},
isbn = {9781450353007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151848.3151850},
doi = {10.1145/3151848.3151850},
abstract = {The focus of this research is given to find a metric and determine the quality of the offered multimedia cloud services from an end users perception. The Quality of Experience (QoE) has been introduced to measure the quality features, which is used to determine the end-to-end user perceived quality of the used multimedia service. In this study, we have used students satisfaction survey, which provides direct subjective data on need, habits, and frequency of using different multimedia services. This data has been used for validation of the proposed Bayesian Network model for interactive estimation of the acceptability of multimedia cloud services based on the user preferences.},
booktitle = {Proceedings of the 15th International Conference on Advances in Mobile Computing \&amp; Multimedia},
pages = {3–12},
numpages = {10},
keywords = {survey evaluation, Bayesian Network, mobile cloud services, Quality of Experience},
location = {Salzburg, Austria},
series = {MoMM2017}
}

@inproceedings{10.1145/2713168.2723146,
author = {Pegus, Patrick and Cecchet, Emmanuel and Shenoy, Prashant},
title = {Video BenchLab Demo: An Open Platform for Video Realistic Streaming Benchmarking},
year = {2015},
isbn = {9781450333511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2713168.2723146},
doi = {10.1145/2713168.2723146},
abstract = {In this demonstration, we present an open, flexible and realistic benchmarking platform named Video BenchLab to measure the performance of streaming media workloads. While Video BenchLab can be used with any existing media server, we provide a set of tools for researchers to experiment with their own platform and protocols. The components include a MediaDrop video server, a suite of tools to bulk insert videos and generate streaming media workloads, a dataset of freely available video and a client runtime to replay videos in the native video players of real Web browsers such as Firefox, Chrome and Internet Explorer. Various metrics are collected to capture the quality of video playback and identify issues that can happen during video replay. Finally, we provide a Dashboard to manage experiments, collect results and perform analytics to compare performance between experiments.The demonstration showcases all the BenchLab video components including a MediaDrop server accessed by real web browsers running locally and in the cloud. We demo the whole experiment lifecycle from creation to deployment as well as result collection and analysis.},
booktitle = {Proceedings of the 6th ACM Multimedia Systems Conference},
pages = {101–104},
numpages = {4},
keywords = {benchmarking, video, web browsers, streaming},
location = {Portland, Oregon},
series = {MMSys '15}
}

@inproceedings{10.1145/3183767.3183776,
author = {Nobre, Ricardo and Reis, Lu\'{\i}s and Bispo, Jo\~{a}o and Carvalho, Tiago and Cardoso, Jo\~{a}o M.P. and Cherubin, Stefano and Agosta, Giovanni},
title = {Aspect-Driven Mixed-Precision Tuning Targeting GPUs},
year = {2018},
isbn = {9781450364447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183767.3183776},
doi = {10.1145/3183767.3183776},
abstract = {Writing mixed-precision kernels allows to achieve higher throughput together with outputs whose precision remain within given limits. The recent introduction of native half-precision arithmetic capabilities in several GPUs, such as NVIDIA P100 and AMD Vega 10, contributes to make precision-tuning even more relevant as of late. However, it is not trivial to manually find which variables are to be represented as half-precision instead of single- or double-precision. Although the use of half-precision arithmetic can speed up kernel execution considerably, it can also result in providing non-usable kernel outputs, whenever the wrong variables are declared using the half-precision data-type. In this paper we present an automatic approach for precision tuning. Given an OpenCL kernel with a set of inputs declared by a user (i.e., the person responsible for programming and/or tuning the kernel), our approach is capable of deriving the mixed-precision versions of the kernel that are better improve upon the original with respect to a given metric (e.g., time-to-solution, energy-to-solution). We allow the user to declare and/or select a metric to measure and to filter solutions based on the quality of the output. We implement a proof-of-concept of our approach using an aspect-oriented programming language called LARA. It is capable of generating mixed-precision kernels that result in considerably higher performance when compared with the original single-precision floating-point versions, while generating outputs that can be acceptable in some scenarios.},
booktitle = {Proceedings of the 9th Workshop and 7th Workshop on Parallel Programming and RunTime Management Techniques for Manycore Architectures and Design Tools and Architectures for Multicore Embedded Computing Platforms},
pages = {26–31},
numpages = {6},
keywords = {mixed-precision, aspect-driven, GPGPU},
location = {Manchester, United Kingdom},
series = {PARMA-DITAM '18}
}

@article{10.1145/3437881,
author = {Huang, Chun-ying and Cheng, Yun-chen and Huang, Guan-zhang and Fan, Ching-ling and Hsu, Cheng-hsin},
title = {On the Performance Comparisons of Native and Clientless Real-Time Screen-Sharing Technologies},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3437881},
doi = {10.1145/3437881},
abstract = {Real-time screen-sharing provides users with ubiquitous access to remote applications, such as computer games, movie players, and desktop applications (apps), anywhere and anytime. In this article, we study the performance of different screen-sharing technologies, which can be classified into native and clientless ones. The native ones dictate that users install special-purpose software, while the clientless ones directly run in web browsers. In particular, we conduct extensive experiments in three steps. First, we identify a suite of the most representative native and clientless screen-sharing technologies. Second, we propose a systematic measurement methodology for comparing screen-sharing technologies under diverse and dynamic network conditions using different performance metrics. Last, we conduct extensive experiments and perform in-depth analysis to quantify the performance gap between clientless and native screen-sharing technologies. We found that our WebRTC-based implementation achieves the best overall performance. More precisely, it consumes a maximum of 3 Mbps bandwidth while reaching a high decoding ratio and delivering good video quality. Moreover, it leads to a steadily high decoding ratio and video quality under dynamic network conditions. By presenting the very first rigorous comparisons of the native and clientless screen-sharing technologies, this article will stimulate more exciting studies on the emerging clientless screen-sharing technologies.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {may},
articleno = {54},
numpages = {26},
keywords = {performance optimization, performance evaluations, measurements, Live video streaming, real-time encoding}
}

@inproceedings{10.1109/CCGrid.2016.56,
author = {Ibrahim, Abdallah Ali Zainelabden A. and Kliazovich, Dzmitry and Bouvry, Pascal},
title = {Service Level Agreement Assurance between Cloud Services Providers and Cloud Customers},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.56},
doi = {10.1109/CCGrid.2016.56},
abstract = {Cloud services providers deliver cloud services to cloud customers on pay-per-use model while the quality of the provided services are defined using service level agreements also known as SLAs. Unfortunately, there is no standard mechanism which exists to verify and assure that delivered services satisfy the signed SLA agreement in an automatic way. There is no guarantee in terms of quality. Those applications have many performance metrics. In this doctoral thesis, we propose a framework for SLA assurance, which can be used by both cloud providers and cloud users. Inside the proposed framework, we will define the performance metrics for the different applications. We will assess the applications performance in different testing environment to assure good services quality as mentioned in SLA. The proposed framework will be evaluated through simulations and using testbed experiments. After testing the applications performance by measuring the performance metrics, we will review the time correlations between those metrics.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {588–591},
numpages = {4},
keywords = {applications, cloud computing, performance, data centers, quality of experience, metrics, quality of services, service level agreement, simulation},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@article{10.1109/TNET.2016.2614129,
author = {Zhao, Zhiwei and Dong, Wei and Bu, Jiajun and Gu, Tao and Min, Geyong},
title = {Accurate and Generic Sender Selection for Bulk Data Dissemination in Low-Power Wireless Networks},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2614129},
doi = {10.1109/TNET.2016.2614129},
abstract = {Data dissemination is a fundamental service offered by low-power wireless networks. Sender selection is the key to the dissemination performance and has been extensively studied. Sender impact metric plays a significant role in sender selection, since it determines which senders are selected for transmission. Recent studies have shown that spatial link diversity has a significant impact on the efficiency of broadcast. However, the existing metrics overlook such impact. Besides, they consider only gains but ignore the costs of sender candidates. As a result, existing works cannot achieve accurate estimation of the sender impact. Moreover, they cannot well support data dissemination with network coding, which is commonly used for lossy environments. In this paper, we first propose a novel sender impact metric, namely,  $gamma $ , which jointly exploits link quality and spatial link diversity to calculate the gain/cost ratio of the sender candidates. Then, we develop a generic sender selection scheme based on the  $gamma $  metric called  $gamma $ -component that can generally support both types of dissemination using native packets and network coding. Extensive evaluations are conducted through real testbed experiments and large-scale simulations. The performance results and analysis show that  $gamma $  achieves far more accurate impact estimation than the existing works. In addition, the dissemination protocols based on  $gamma $ -component outperform the existing protocols in terms of completion time and transmissions by 20.5\% and 23.1\%, respectively.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {948–959},
numpages = {12}
}

@article{10.1145/2998454,
author = {Li, Ning and Jiang, Hong and Feng, Dan and Shi, Zhan},
title = {Customizable SLO and Its Near-Precise Enforcement for Storage Bandwidth},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1553-3077},
url = {https://doi.org/10.1145/2998454},
doi = {10.1145/2998454},
abstract = {Cloud service is being adopted as a utility for large numbers of tenants by renting Virtual Machines (VMs). But for cloud storage, unpredictable IO characteristics make accurate Service-Level-Objective (SLO) enforcement challenging. As a result, it has been very difficult to support simple-to-use and technology-agnostic SLO specifying a particular value for a specific metric (e.g., storage bandwidth). This is because the quality of SLO enforcement depends on performance error and fluctuation that measure the precision of SLO enforcement. High precision of SLO enforcement is critical for user-oriented performance customization and user experiences. To address this challenge, this article presents V-Cup, a framework for VM-oriented customizable SLO and its near-precise enforcement. It consists of multiple auto-tuners, each of which exports an interface for a tenant to customize the desired storage bandwidth for a VM and enable the storage bandwidth of the VM to converge on the target value with a predictable precision. We design and implement V-Cup in the Xen hypervisor based on the fair sharing scheduler for VM-level resource management. Our V-Cup prototype evaluation shows that it achieves satisfying performance guarantees through near-precise SLO enforcement.},
journal = {ACM Trans. Storage},
month = {feb},
articleno = {6},
numpages = {25},
keywords = {end-to-end control, storage management, Cloud storage, service-level objective}
}

@inproceedings{10.1145/2713168.2723145,
author = {Pegus, Patrick and Cecchet, Emmanuel and Shenoy, Prashant},
title = {Video BenchLab: An Open Platform for Realistic Benchmarking of Streaming Media Workloads},
year = {2015},
isbn = {9781450333511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2713168.2723145},
doi = {10.1145/2713168.2723145},
abstract = {In this paper, we present an open, flexible and realistic benchmarking platform named Video BenchLab to measure the performance of streaming media workloads. While Video BenchLab can be used with any existing media server, we provide a set of tools for researchers to experiment with their own platform and protocols. The components include a MediaDrop video server, a suite of tools to bulk insert videos and generate streaming media workloads, a dataset of freely available video and a client runtime to replay videos in the native video players of real Web browsers such as Firefox, Chrome and Internet Explorer. We define simple metrics that are able to capture the quality of video playback and identify issues that can happen during video replay. Finally, we provide a Dashboard to manage experiments, collect results and perform analytics to compare performance between experiments.We present a series of experiments with Video BenchLab to illustrate how the video specific metrics can be used to measure the user perceived experience in real browsers when streaming videos. We also show Internet scale experiments by deploying clients in data centers distributed all over the globe. All the software, datasets, workloads and results used in this paper are made freely available on SourceForge for anyone to reuse and expand.},
booktitle = {Proceedings of the 6th ACM Multimedia Systems Conference},
pages = {165–176},
numpages = {12},
keywords = {benchmarking, web browsers, streaming, video},
location = {Portland, Oregon},
series = {MMSys '15}
}

@article{10.1145/3093893,
author = {Garc\'{\i}a-Dorado, Jos\'{e} Luis},
title = {Bandwidth Measurements within the Cloud: Characterizing Regular Behaviors and Correlating Downtimes},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3093893},
doi = {10.1145/3093893},
abstract = {The search for availability, reliability, and quality of service has led cloud infrastructure customers to disseminate their services, contents, and data over multiple cloud data centers, often involving several Cloud service providers (CSPs). The consequence of this is that a large amount of data must be transmitted across the public Cloud. However, little is known about the bandwidth dynamics involved. To address this, we have conducted a measurement campaign for bandwidth between 18 data centers of four major CSPs. This extensive campaign allowed us to characterize the resulting time series of bandwidth as the addition of a stationary component and some infrequent excursions (typically downtimes). While the former provides a description of the bandwidth users can expect in the Cloud, the latter is closely related to the robustness of the Cloud (i.e., the occurrence of downtimes is correlated). Both components have been studied further by applying factor analysis, specifically analysis of variance, as a mechanism to formally compare data centers’ behaviors and extract generalities. The results show that the stationary process is closely related to the data center locations and CSPs involved in transfers that, fortunately, make the Cloud more predictable and allow the set of reported measurements to be extrapolated. On the other hand, although correlation in the Cloud is low, that is, only 10\% of the measured pair of paths showed some correlation, we found evidence that such correlation depends on the particular relationships between pairs of data centers with little connection to more general factors. Positively, this implies that data centers either in the same area or within the same CSP do not show qualitatively more correlation than other data centers, which eases the deployment of robust infrastructures. On the downside, this metric is scarcely generalizable and, consequently, calls for exhaustive monitoring.},
journal = {ACM Trans. Internet Technol.},
month = {aug},
articleno = {39},
numpages = {25},
keywords = {inter-cloud, TCP bandwidth, Public cloud, traffic correlation, ANOVA}
}

@article{10.1109/TNET.2018.2851379,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet},
title = {Video Streaming in Distributed Erasure-Coded Storage Systems: Stall Duration Analysis},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2851379},
doi = {10.1109/TNET.2018.2851379},
abstract = {The demand for global video has been burgeoning across industries. With the expansion and improvement of video-streaming services, cloud-based video is evolving into a necessary feature of any successful business for reaching internal and external audiences. This paper considers video streaming over distributed systems where the video segments are encoded using an erasure code for better reliability, thus being the first work to our best knowledge that considers video streaming over erasure-coded distributed cloud systems. The download time of each coded chunk of each video segment is characterized, and the ordered statistics over the choice of the erasure-coded chunks is used to obtain the playback time of different video segments. Using the playback times, bounds on the moment generating function on the stall duration are used to bound the mean stall duration. Moment generating function-based bounds on the ordered statistics are also used to bound the stall duration tail probability, which determines the probability that the stall time is greater than a pre-defined number. These two metrics, mean stall duration and the stall duration tail probability, are important quality of experience QoE measures for the end users. Based on these metrics, we formulate an optimization problem to jointly minimize the convex combination of both the QoE metrics averaged over all requests over the placement and access of the video content. The non-convex problem is solved using an efficient iterative algorithm. Numerical results show a significant improvement in QoE metrics for cloud-based video compared to the considered baselines.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1921–1932},
numpages = {12}
}

@inproceedings{10.1145/2882903.2882943,
author = {Kalyvianaki, Evangelia and Fiscato, Marco and Salonidis, Theodoros and Pietzuch, Peter},
title = {THEMIS: Fairness in Federated Stream Processing under Overload},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882943},
doi = {10.1145/2882903.2882943},
abstract = {Federated stream processing systems, which utilise nodes from multiple independent domains, can be found increasingly in multi-provider cloud deployments, internet-of-things systems, collaborative sensing applications and large-scale grid systems. To pool resources from several sites and take advantage of local processing, submitted queries are split into query fragments, which are executed collaboratively by different sites. When supporting many concurrent users, however, queries may exhaust available processing resources, thus requiring constant load shedding. Given that individual sites have autonomy over how they allocate query fragments on their nodes, it is an open challenge how to ensure global fairness on processing quality experienced by queries in a federated scenario.We describe THEMIS, a federated stream processing system for resource-starved, multi-site deployments. It executes queries in a globally fair fashion and provides users with constant feedback on the experienced processing quality for their queries. THEMIS associates stream data with its source information content (SIC), a metric that quantifies the contribution of that data towards the query result, based on the amount of source data used to generate it. We provide the BALANCE-SIC distributed load shedding algorithm that balances the SIC values of result data. Our evaluation shows that the BALANCE-SIC algorithm yields balanced SIC values across queries, as measured by Jain's Fairness Index. Our approach also incurs a low execution time overhead.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {541–553},
numpages = {13},
keywords = {tuple shedding, approximate data processing, fairness, federated data stream processing},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3147234.3148103,
author = {Roloff, Eduardo and Diener, Matthias and Carre\~{n}o, Emmanuell D. and Moreira, Francis B. and Gaspary, Luciano P. and Navaux, Philippe O.A.},
title = {Exploiting Price and Performance Tradeoffs in Heterogeneous Clouds},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3148103},
doi = {10.1145/3147234.3148103},
abstract = {Parallel applications are composed of several tasks, which have different computational demands among them. Moreover, most cloud providers offer multiple instance configurations, with large variations of computational power and cost. A combination between the application requirements and the variety of instance types of the cloud could be explored to improve the cost efficiency of the application execution. In this paper, we introduce the cost-delay product as a metric to measure the cost efficiency of cloud systems. With this metric, cloud tenants can evaluate different tradeoffs between cost and performance for their application, depending on their preferences. We explore the use of multiple instance types to create heterogeneous cluster systems in the cloud. Our results show that heterogeneous clouds can have a better cost efficiency than homogeneous systems, reducing the price of execution while maintaining a similar application performance. Furthermore, by comparing the cost-delay product, the user can select an instance mix that is most suitable for his needs.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {71–76},
numpages = {6},
keywords = {cost efficiency, cloud computing, performance, heterogeneity},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@inproceedings{10.1145/2872427.2883053,
author = {Zhou, Ke and Redi, Miriam and Haines, Andrew and Lalmas, Mounia},
title = {Predicting Pre-Click Quality for Native Advertisements},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883053},
doi = {10.1145/2872427.2883053},
abstract = {Native advertising is a specific form of online advertising where ads replicate the look-and-feel of their serving platform. In such context, providing a good user experience with the served ads is crucial to ensure long-term user engagement. In this work, we explore the notion of ad quality, namely the effectiveness of advertising from a user experience perspective. We design a learning framework to predict the pre-click quality of native ads. More specifically, we look at detecting offensive native ads, showing that, to quantify ad quality, ad offensive user feedback rates are more reliable than the commonly used click-through rate metrics. We then conduct a crowd-sourcing study to identify which criteria drive user preferences in native advertising. We translate these criteria into a set of ad quality features that we extract from the ad text, image and advertiser, and then use them to train a model able to identify offensive ads. We show that our model is very effective in detecting offensive ads, and provide in-depth insights on how different features affect ad quality. Finally, we deploy a preliminary version of such model and show its effectiveness in the reduction of the offensive ad feedback rate.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {299–310},
numpages = {12},
keywords = {ad quality, ad feedback, image and text, pre-click experience, native advertising, features, offensive rate},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{10.1109/IPSN.2018.00030,
author = {Islam, Bashima and Islam, Md Tamzeed and Nirjon, Shahriar},
title = {A Motion-Triggered Stereo Camera for 3D Experience Capture: Demo Abstract},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00030},
doi = {10.1109/IPSN.2018.00030},
abstract = {This demo is an implementation of our motion-triggered camera system that captures, processes, stores, and transmits 3D visual information of a real-world environment using a low-cost camera-based sensor system that is constrained by its limited processing capability, storage, and battery life. This system can be used in applications such as capturing and sharing 3D content in the social media, training people in different professions, and post-facto analysis of an event. This system uses off-the-shelf hardware and standard computer vision algorithms. Its novelty lies in the ability to optimally control camera data acquisition and processing stages to guarantee the desired quality of captured information and battery life. The design of the controller is based on extensive measurements and modeling of the relationships between the linear and angular motion of a camera and the quality of generated 3D point clouds as well as the battery life of the system. To achieve this, we 1) devise a new metric to quantify the quality of generated 3D point clouds, 2) formulate an optimization problem to find an optimal trigger point for the camera system and prolongs its battery life while maximizing the quality of captured 3D environment, and 3) make the model adaptive so that the system evolves and its performance improves over time.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {134–135},
numpages = {2},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1109/IPSN.2018.00046,
author = {Islam, Bashima and Islam, Md Tamzeed and Nirjon, Shahriar},
title = {Glimpse.3D: A Motion-Triggered Stereo Body Camera for 3D Experience Capture and Preview},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00046},
doi = {10.1109/IPSN.2018.00046},
abstract = {The Glimpse.3D is a body-worn camera that captures, processes, stores, and transmits 3D visual information of a real-world environment using a low cost camera-based sensor system that is constrained by its limited processing capability, storage, and battery life. The 3D content is viewed on a mobile device such as a smartphone or a virtual reality headset. This system can be used in applications such as capturing and sharing 3D content in the social media, training people in different professions, and post-facto analysis of an event. Glimpse.3D uses off-the-shelf hardware and standard computer vision algorithms. Its novelty lies in the ability to optimally control camera data acquisition and processing stages to guarantee the desired quality of captured information and battery life. The design of the controller is based on extensive measurements and modeling of the relationships between the linear and angular motion of a body-worn camera and the quality of generated 3D point clouds as well as the battery life of the system. To achieve this, we 1) devise a new metric to quantify the quality of generated 3D point clouds, 2) formulate an optimization problem to find an optimal trigger point for the camera system that prolongs its battery life while maximizing the quality of captured 3D environment, and 3) make the model adaptive so that the system evolves and its performance improves over time.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {176–187},
numpages = {12},
keywords = {3D-reconstruction, body camera},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1145/3383812.3383838,
author = {Pacot, Mark Phil B. and Marcos, Nelson},
title = {Cloud Removal from Aerial Images Using Generative Adversarial Network with Simple Image Enhancement},
year = {2020},
isbn = {9781450377201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383812.3383838},
doi = {10.1145/3383812.3383838},
abstract = {The atmospheric condition of the presence of clouds is one of the biggest problems in most aerial imaging systems. It degrades the visual quality of images leading to the loss of information for ground scenes. Hence, an effective cloud removal algorithm is a significant factor for this kind of problem and other related applications. The proposed cloud removal technique using the generative adversarial network with simple image enhancement (SIE-GAN) is a useful tool in removing cloud formations, most notably in images acquired using Unmanned Aerial Vehicle System (UAVs). This technique showed flexibility in performing the given task with satisfactory results, which is a gauge based on No-Reference Image Quality Metric, specifically the Perception-based Image Quality Evaluator (PIQE). Also, the proposed algorithm outperformed some of existing cloud removal algorithms by producing a better quality output when tested on the too-cloudy satellite images. Overall, the authors introduced a new frontier in generating cloud-free aerial images and added a valuable contribution to the array of cloud removal algorithms.},
booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
pages = {77–81},
numpages = {5},
keywords = {generative adversarial network, cloud removal, simple image enhancement, no-reference image quality metric, unmanned aerial vehicle system},
location = {Singapore, Singapore},
series = {ICIGP '20}
}

@inproceedings{10.1109/AST.2019.000-2,
author = {Martinez-Ortiz, Andres-Leonardo and Lizcano, David and Ortega, Miguel},
title = {Software Metrics Artifacts Making Web Quality Measurable: AST 2019 Invited Paper},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2019.000-2},
doi = {10.1109/AST.2019.000-2},
abstract = {Mining open source repositories introduces an effective approach to put in practice empirical software engineering in a variety of technologies. Kernel development (Linux) first and then Internet (Chromium) and more recently cloud orchestration (Kubernetes) and machine learning (TensorFlow) are fundamental pieces not just for open source ecosystem but also for the industry leading software innovation. Empirical software engineering sustains a better understanding of these projects, reducing even more the barriers for adoption. In this work we focus on empirical quality assessment developing software metrics artifacts to make web components quality measurable. After reviewing the state of the art and main frameworks for software measurement, we will present our proposal for the empirical evaluation of quality metrics for web components, data collection, measurement and prediction, discussing main benefits and some drawback of the selected approach, which will be aimed at future works.},
booktitle = {Proceedings of the 14th International Workshop on Automation of Software Test},
pages = {1–6},
numpages = {6},
keywords = {software engineering, web technologies, open source, quality metrics},
location = {Montreal, Quebec, Canada},
series = {AST '19}
}

@inproceedings{10.1145/3319647.3325849,
author = {Nagin, Kenneth and Kassis, Andre and Lorenz, Dean and Barabash, Katherine and Raichstein, Eran},
title = {Estimating Client QoE from Measured Network QoS},
year = {2019},
isbn = {9781450367493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319647.3325849},
doi = {10.1145/3319647.3325849},
abstract = {This research is done in the context of the SliceNet project [4] that aims to extend 5G infrastructure with cognitive management of cross-domain, cross-layer network slices [1], with emphasis on Quality of Experience (QoE) for vertical industries. The provisioning of network slices with proper QoE guarantees is seen as one of the key enablers of future 5G-enabled networks. The challenge is to assess the QoE experienced by the vertical application and its users without requiring the applications or the users to measure and report QoE related metrics back to the provider. To address this challenge, we propose a method for deriving application-level QoE from network-level Quality of Service (QoS) measurements, easily accessible by the provider. In particular, we describe a PoC where QoE, perceived by application users, is estimated from low level network monitoring data, by applying cognitive methods. Our main goal is enabling the cloud provider to support the desired E2E QoE-based Service Level Agreements (SLAs), e.g. by monitoring QoS metrics within the provider's domain to optimize resource allocation through provider's actuators. Additional benefit can be achieved by applying the same technique to troubleshoot issues in the provider's infrastructure. In this work, we employed classical statistical methods to assess the relationship between the application-level QoE and the network-level QoS.},
booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage},
pages = {188},
numpages = {1},
location = {Haifa, Israel},
series = {SYSTOR '19}
}

@inproceedings{10.1145/2649563.2649571,
author = {Weber, Andreas and Herbst, Nikolas and Groenda, Henning and Kounev, Samuel},
title = {Towards a Resource Elasticity Benchmark for Cloud Environments},
year = {2014},
isbn = {9781450330596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2649563.2649571},
doi = {10.1145/2649563.2649571},
abstract = {Auto-scaling features offered by today's cloud infrastructures provide increased flexibility especially for customers that experience high variations in the load intensity over time. However, auto-scaling features introduce new system quality attributes when considering their accuracy, timing, and boundaries. Therefore, distinguishing between different offerings has become a complex task, as it is not yet supported by reliable metrics and measurement approaches. In this paper, we discuss shortcomings of existing approaches for measuring and evaluating elastic behavior and propose a novel benchmark methodology specifically designed for evaluating the elasticity aspects of modern cloud platforms. The benchmark is based on open workloads with realistic load variation profiles that are calibrated to induce identical resource demand variations independent of the underlying hardware performance. Furthermore, we propose new metrics that capture the accuracy of resource allocations and de-allocations, as well as the timing aspects of an auto-scaling mechanism explicitly.},
booktitle = {Proceedings of the 2nd International Workshop on Hot Topics in Cloud Service Scalability},
articleno = {5},
numpages = {8},
keywords = {Supply, Load Profile, Resource, Demand, Elasticity},
location = {Dublin, Ireland},
series = {HotTopiCS '14}
}

@inproceedings{10.1145/3466772.3467048,
author = {Kassir, Saadallah and de Veciana, Gustavo and Wang, Nannan and Wang, Xi and Palacharla, Paparao},
title = {Joint Update Rate Adaptation in Multiplayer Cloud-Edge Gaming Services: Spatial Geometry and Performance Tradeoffs},
year = {2021},
isbn = {9781450385589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466772.3467048},
doi = {10.1145/3466772.3467048},
abstract = {In this paper, we analyze the performance of Multiplayer Cloud Gaming (MCG) systems. To that end, we introduce a model and new MCG-Quality of Service (QoS) metric that captures the freshness of the players' updates and fairness in their gaming experience. We introduce an efficient measurement-based Joint Multiplayer Rate Adaptation (JMRA) algorithm that optimizes the MCG-QoS by overcoming large (possibly varying) network transport delays by increasing the associated players' update rates. The resulting MCG-QoS is shown to be Schur-concave in the network delays, leading to natural characterizations and performance comparisons associated with the players' spatial geometry and network congestion. In particular, joint rate adaptation enables service providers to combat variability in network delays and players' geographic spread to achieve high service coverage. This, in turn, allows us to explore the spatial density and capacity of compute resources that need to be provisioned. Finally, we leverage tools from majorization theory, to show how service placement decisions can be made to improve the robustness of the MCG-QoS to stochastic network delays.},
booktitle = {Proceedings of the Twenty-Second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {191–200},
numpages = {10},
keywords = {Multiplayer Cloud Gaming, Service Placement, Network Resource Provisioning, Rate Adaptation, Edge Computing},
location = {Shanghai, China},
series = {MobiHoc '21}
}

@inproceedings{10.5555/2755535.2755542,
author = {Huang, Chun-Ying and Chen, Po-Han and Huang, Yu-Ling and Chen, Kuan-Ta and Hsu, Cheng-Hsin},
title = {Measuring the Client Performance and Energy Consumption in Mobile Cloud Gaming},
year = {2014},
publisher = {IEEE Press},
abstract = {Mobile cloud gaming allows gamers to play games on resource-constrained mobile devices, and a measurement study to quality the client performance and energy consumption is crucial to attract and retain the gamers. In this paper, we adopt an open source cloud gaming platform to conduct extensive experiments on real mobile clients. Our experiment results show two major findings that are of interests to researchers, developers, and gamers. First, compared to mobile native games, mobile cloud games save energy by up to 30\%. Second, the frame rate, bit rate, and resolution all affect the decoders' resource consumption, while frame rate imposes the highest impact. These findings shed some light on the further enhancements of the emerging mobile cloud gaming platforms.},
booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
articleno = {5},
numpages = {3},
location = {Nagoya, Japan},
series = {NetGames '14}
}

@article{10.1145/2746232,
author = {Yoginath, Srikanth B. and Perumalla, Kalyan S.},
title = {Efficient Parallel Discrete Event Simulation on Cloud/Virtual Machine Platforms},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1049-3301},
url = {https://doi.org/10.1145/2746232},
doi = {10.1145/2746232},
abstract = {Cloud and Virtual Machine (VM) technologies present new challenges with respect to performance and monetary cost in executing parallel discrete event simulation (PDES) applications. Due to the introduction of overall cost as a metric, the traditional use of the highest-end computing configuration is no longer the most obvious choice. Moreover, the unique runtime dynamics and configuration choices of Cloud and VM platforms introduce new design considerations and runtime characteristics specific to PDES over Cloud/VMs. Here, an empirical study is presented to help understand the dynamics, trends, and trade-offs in executing PDES on Cloud/VM platforms. Performance and cost measures obtained from multiple PDES applications executed on the Amazon EC2 Cloud and on a high-end VM host machine reveal new, counterintuitive VM--PDES dynamics and guidelines. One of the critical aspects uncovered is the fundamental mismatch in hypervisor scheduler policies designed for general Cloud workloads versus the virtual time ordering needed for PDES workloads. This insight is supported by experimental data revealing the gross deterioration in PDES performance traceable to VM scheduling policy. To overcome this fundamental problem, the design and implementation of a new deadlock-free scheduler algorithm are presented, optimized specifically for PDES applications on VMs. The scalability of our scheduler has been tested in up to 128 VMs multiplexed on 32 cores, showing significant improvement in the runtime relative to the default Cloud/VM scheduler. The observations, algorithmic design, and results are timely for emerging Cloud/VM-based installations, highlighting the need for PDES-specific support in high-performance discrete event simulations on Cloud/VM platforms.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {jul},
articleno = {5},
numpages = {26},
keywords = {scheduler, virtual machines, Parallel discrete event simulation, time warp, global virtual time}
}

@inproceedings{10.1145/2649387.2660839,
author = {Lindsey, Aaron and Yeh, Hsin-Yi (Cindy) and Wu, Chih-Peng and Thomas, Shawna and Amato, Nancy M.},
title = {Improving Decoy Databases for Protein Folding Algorithms},
year = {2014},
isbn = {9781450328944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2649387.2660839},
doi = {10.1145/2649387.2660839},
abstract = {Predicting protein structures and simulating protein folding are two of the most important problems in computational biology today. Simulation methods rely on a scoring function to distinguish the native structure (the most energetically stable) from non-native structures. Decoy databases are collections of non-native structures used to test and verify these functions.We present a method to evaluate and improve the quality of decoy databases by adding novel structures and removing redundant structures. We test our approach on 17 different decoy databases of varying size and type and show significant improvement across a variety of metrics. We also test our improved databases on a popular modern scoring function and show that they contain a greater number of native-like structures than the original databases, thereby producing a more rigorous database for testing scoring functions.},
booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {717–724},
numpages = {8},
keywords = {decoy databases, sampling methods, protein folding},
location = {Newport Beach, California},
series = {BCB '14}
}

@article{10.1109/TNET.2019.2900434,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet and Ra, Moo-Ryong},
title = {Multi-Tier Caching Analysis in CDN-Based Over-the-Top Video Streaming Systems},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2900434},
doi = {10.1109/TNET.2019.2900434},
abstract = {Internet video traffic has been rapidly increasing and is further expected to increase with the emerging 5G applications, such as higher definition videos, the IoT, and augmented/virtual reality applications. As end users consume video in massive amounts and in an increasing number of ways, the content distribution network CDN should be efficiently managed to improve the system efficiency. The streaming service can include multiple caching tiers, at the distributed servers and the edge routers, and efficient content management at these locations affects the quality of experience QoE of the end users. In this paper, we propose a model for video streaming systems, typically composed of a centralized origin server, several CDN sites, and edge-caches located closer to the end user. We comprehensively consider different systems design factors, including the limited caching space at the CDN sites, allocation of CDN for a video request, choice of different ports or paths from the CDN and the central storage, bandwidth allocation, the edge-cache capacity, and the caching policy. We focus on minimizing a performance metric, stall duration tail probability SDTP, and present a novel and efficient algorithm accounting for the multiple design flexibilities. The theoretical bounds with respect to the SDTP metric are also analyzed and presented. The implementation of a virtualized cloud system managed by Openstack demonstrates that the proposed algorithms can significantly improve the SDTP metric compared with the baseline strategies.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {835–847},
numpages = {13}
}

@inproceedings{10.1145/2884781.2884814,
author = {Su, Guoxin and Rosenblum, David S. and Tamburrelli, Giordano},
title = {Reliability of Run-Time Quality-of-Service Evaluation Using Parametric Model Checking},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884814},
doi = {10.1145/2884781.2884814},
abstract = {Run-time Quality-of-Service (QoS) assurance is crucial for business-critical systems. Complex behavioral performance metrics (PMs) are useful but often difficult to monitor or measure. Probabilistic model checking, especially parametric model checking, can support the computation of aggregate functions for a broad range of those PMs. In practice, those PMs may be defined with parameters determined by run-time data. In this paper, we address the reliability of QoS evaluation using parametric model checking. Due to the imprecision with the instantiation of parameters, an evaluation outcome may mislead the judgment about requirement violations. Based on a general assumption of run-time data distribution, we present a novel framework that contains light-weight statistical inference methods to analyze the reliability of a parametric model checking output with respect to an intuitive criterion. We also present case studies in which we test the stability and accuracy of our inference methods and describe an application of our framework to a cloud server management problem.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {73–84},
numpages = {12},
keywords = {run-time evaluation, data distribution, reliability, Quality-of-Service, probabilistic model checking},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/2821357.2821366,
author = {Herbst, Nikolas Roman and Kounev, Samuel and Weber, Andreas and Groenda, Henning},
title = {BUNGEE: An Elasticity Benchmark for Self-Adaptive IaaS Cloud Environments},
year = {2015},
publisher = {IEEE Press},
abstract = {Today's infrastructure clouds provide resource elasticity (i.e. auto-scaling) mechanisms enabling self-adaptive resource provisioning to reflect variations in the load intensity over time. These mechanisms impact on the application performance, however, their effect in specific situations is hard to quantify and compare. To evaluate the quality of elasticity mechanisms provided by different platforms and configurations, respective metrics and benchmarks are required. Existing metrics for elasticity only consider the time required to provision and deprovision resources or the costs impact of adaptations. Existing benchmarks lack the capability to handle open workloads with realistic load intensity profiles and do not explicitly distinguish between the performance exhibited by the provisioned underlying resources, on the one hand, and the quality of the elasticity mechanisms themselves, on the other hand.In this paper, we propose reliable metrics for quantifying the timing aspects and accuracy of elasticity. Based on these metrics, we propose a novel approach for benchmarking the elasticity of Infrastructure-as-a-Service (IaaS) cloud platforms independent of the performance exhibited by the provisioned underlying resources. We show that the proposed metrics provide consistent ranking of elastic platforms on an ordinal scale. Finally, we present an extensive case study of real-world complexity demonstrating that the proposed approach is applicable in realistic scenarios and can cope with different levels of resource efficiency.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {46–56},
numpages = {11},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/3036290.3036321,
author = {L\'{o}pez, Cindy and Heinsen, Rene and Huh, Eui-Nam},
title = {Improving Availability Applying Intelligent Replication in Federated Cloud Storage Based on Log Analysis},
year = {2017},
isbn = {9781450348287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3036290.3036321},
doi = {10.1145/3036290.3036321},
abstract = {This study is focusing on improving the availability of federated storage services in order to provide better quality-of-service (QoS) to the customer with the minimum use of resources. One of the most efficient solutions to get the best experience in the cloud is to combine the services offered. In order for this to happen, there exist different approaches for selecting the best subset of services to reach the optimal performance. However, those works focus on one time selection processes, despite of customer's requirements are continuously changing and demanding adaptable storage service. In this research, I propose a method to improve storage availability through log sentiment analysis and intelligent replication. This methodology is based on the merging of two types of log analysis and the measurement of availability and performance metrics in order to select the best subset of services in cloud storage service federation.},
booktitle = {Proceedings of the 2017 International Conference on Machine Learning and Soft Computing},
pages = {148–153},
numpages = {6},
keywords = {cloud computing, replication, sentiment analysis, log analysis, Federated Cloud Storage, performance, availability, subset selection},
location = {Ho Chi Minh City, Vietnam},
series = {ICMLSC '17}
}

@inproceedings{10.1145/2792745.2792782,
author = {Soltani, Kiumars and Parameswaran, Aditya and Wang, Shaowen},
title = {GeoHashViz: Interactive Analytics for Mapping Spatiotemporal Diffusion of Twitter Hashtags},
year = {2015},
isbn = {9781450337205},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2792745.2792782},
doi = {10.1145/2792745.2792782},
abstract = {Since its birth in 2006, Twitter has evolved to a multi-purpose social media that attracts hundreds of millions of users to share their activities and ideas on a daily basis. The potential of capturing fine-grained activity log of users, combined with ever increasing geographical information derived from GPS-enabled devices, has made Twitter data a valuable source for spatiotemporal analysis of human activities. One of the early innovations of Twitter is the use of hashtag as a unique tagging mechanism to provide additional information about a user post. From its emergence in late 2007, hashtags have been used extensively to express ideas, group tweets and report events among Twitter users. The increasing popularity of hashtags, in addition to their simple and concise structure, has inspired multiple recent studies to propose hashtag as a medium to assess diffusion of ideas in a virtual world. Studying collective effort of users in making a hashtag go viral can shed light on the complex process of idea diffusion that involves psychological, sociological and geographical elements.Although most of the previous research on idea diffusion in virtual world purely focuses on the users social graph, recent studies have confirmed that the spatial relationship among users and regions also play a crucial role in its adoption patterns [1]. This comes back to First Law of Geography that was formulated by Waldo Tobler more than 40 years ago, as "everything is related to everything else, but near things are more related than distant things". However, previous work on designing an interactive visual analytical framework for hashtag diffusion (http://keyhole.co/, http://hashtracking.com/, https://tagboard.com/), lack in-depth spatial analysis capabilities, hence not well-suited to be used for studying diffusion patterns. This research aims to fill this gap by providing an interactive framework to offer visual analytics on geographical diffusion of hashtags over time. Our framework, called GeoHashViz, can provide both textual and visual analytics on the role of location in adoption of hashtags and offer insights on diffusion patterns among different hashtags. GeoHashViz processes large stream of incoming tweets using a Hadoop-based approach and calculates multiple measures that will be used to generate visual analytics for the user. Furthermore, it integrates online maps with a live animation tool to visualize both spatial and temporal diffusion of hashtags at the same time.Data Collection: we gather our data using the Twitter Streaming API (details in [3]).Since we are only interested in common hashtags, which have a certain level of popularity, we only keep the hashtags with more than 1000 appearances. Our unit of spatial resolution is set to cities in United States with a population larger than 60000 people that give us 645 unique locations. These locations will form our reference grid and every geographical point will be assigned to its nearest neighbor in the reference grid.Analytics: To formulate the problem of spatiotemporal analysis of hashtag diffusion, we recognized two main categories of hashtag-based and location-based analytics. In hashtag-based analytics we focus on specific hashtags and their associated diffusion patterns. On the other hand, location-based analytics study the similarity and closeness of locations in terms of their hashtag adoption. To evaluate the usability of the framework, we identify five core analytical features that cover wide ranges of research questions. However, our framework can be easily extended to include more analytical features. The five visual analytical capabilities are listed in Table 1. Spread and focus points (locations with highest occurrence of the hashtag [1]) provide users with a visual estimate of how the hashtag is diffused over time. However, we also provided four metrics that gives a user a more concrete sense of the diffusion patterns: a) Entropy: Measures the randomness of hashtag distribution [1] ;b) KL-divergence: Compare the geographical distribution of hashtag in consecutive time windows using KL-divergence method ;c) Spatial Dispersion: Measures how scattered is the hashtag from its geographical midpoint ;d) Count:. Plot the cumulative count of the hashtag over time.For location-based analytics we included two functions. Top-k hashtags calculate the most popular hashtags in a region and visualize that using a word cloud. However by simply looking at the counts, we may miss some locally significant due to their relative low count. To reduce the dominance of globally popular hashtags, we introduce another analytic that will visualize top-k locally significant hashtags. This analytic uses a Tf-idf like metric [5] to measure the local popularity of a hashtag in a specific region, hence assigning lower rank to the hashtags which are popular in other places as well. In addition, we provide two metrics for comparing two different regions in terms of hashtag adoption: a) Jaccard Similarity Compare the set of hashtag used in two different regions, with higher number assigned to more similar regions ;b) Adoption Lag This measure depicts how long it takes for a hashtag to travel between two region, by averaging the time difference between the first appearance of hashtags in two regions.Architecture: GeoHashViz framework follows a two-layer architecture: an offline-processing module and an interactive module. The offline-processing module, implemented entirely in Apache Hadoop and called periodically, processes the raw data and pre-computes measures related to spatiotemporal diffusion of hashtags. The interactive module on the other hand is called on demand and based on user requests. The two modules connect with each other through a distributed MongoDB database. The two-layer architecture enables a fast interactive final framework by reducing the data processing that interactive module is required to do.In the offline-processing module, significant hashtags are extracted and the points are laid on the geographical mesh that we defined above. Then two MapReduce jobs are executed: one for pre-computing measures related to hashtag-based analytics and one for location-based analytics. All the Hadoop experiments were conducted using XSEDE Gordon Hadoop cluster. The data-intensive nature of our problem, requiring aggregation of large number of tweets based on both hashtags and locations, make Hadoop an ideal choice for the offline-processing module. Using Hadoop, we distribute the tweets into multiple nodes, and then take advantage of MapReduce model to aggregate them based on their associated location on the mesh and their included hashtags. In the reduce step, having access to all the tweets for a certain location/hashtag, we can generate the analytics for different timestamps. In addition, since the nodes on Gordon Hadoop cluster have relatively high memory, we are able to store the geographical mesh in memory and quickly map the location of users to their closest point on the mesh (using kd-tree). The same technique is employed in the interactive module to find the set of mesh points which lies into the user-defined bounding box.The interactive module includes a web application and a Java Servlet. The web application is integrated into Cyber-GIS Gateway [2] to increase usability of the application and easier integration with other CyberGIS applications. Figure 1 shows a view of the application visualizing top 20 hashtags in the southern California region in September 2014.},
booktitle = {Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure},
articleno = {37},
numpages = {2},
keywords = {CyberGIS, interactive visualization, GeoHashViz, social media, Hadoop},
location = {St. Louis, Missouri},
series = {XSEDE '15}
}

@inproceedings{10.1145/2663165.2663333,
author = {Raghavan, Ajaykrishna and Chandra, Abhishek and Weissman, Jon B.},
title = {Tiera: Towards Flexible Multi-Tiered Cloud Storage Instances},
year = {2014},
isbn = {9781450327855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663165.2663333},
doi = {10.1145/2663165.2663333},
abstract = {Cloud providers offer an array of storage services that represent different points along the performance, cost, and durability spectrum. If an application desires the composite benefits of multiple storage tiers, then it must manage the complexity of different interfaces to these storage services and their diverse policies. We believe that it is possible to provide the benefits of customized tiered cloud storage to applications without compromising simplicity using a lightweight middleware. In this paper, we introduce Tiera, a middleware that enables the provision of multi-tiered cloud storage instances that are easy to specify, flexible, and enable a rich array of storage policies and desired metrics to be realized. Tiera's novelty lies in the first-class support for encapsulated tiered cloud storage, ease of programmability of data management policies, and support for runtime replacement and addition of policies and tiers. Tiera enables an application to realize a desired metric (e.g., low latency or low cost) by selecting different storage services that constitute a Tiera instance, and easily specifying a policy, using event and response pairs, to manage the life cycle of data stored in the instance. We illustrate the benefits of Tiera through a prototype implemented on the Amazon cloud. By deploying unmodified MySQL database engine and a TPC-W Web bookstore application on Tiera, we are able to improve their respective throughputs by 47\% -- 125\% and 46\% -- 69\%, over standard deployments. We further show the flexibility of Tiera in achieving different desired application metrics with minimal overhead.},
booktitle = {Proceedings of the 15th International Middleware Conference},
pages = {1–12},
numpages = {12},
location = {Bordeaux, France},
series = {Middleware '14}
}

@inproceedings{10.1109/UCC.2014.87,
author = {Ali-Eldin, Ahmed and Seleznjev, Oleg and Sj\"{o}stedt-de Luna, Sara and Tordsson, Johan and Elmroth, Erik},
title = {Measuring Cloud Workload Burstiness},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.87},
doi = {10.1109/UCC.2014.87},
abstract = {Workload burstiness and spikes are among the main reasons for service disruptions and decrease in the Quality-of-Service (QoS) of online services. They are hurdles that complicate autonomic resource management of data enters. In this paper, we review the state-of-the-art in online identification of workload spikes and quantifying burstiness. The applicability of some of the proposed techniques is examined for Cloud systems where various workloads are co-hosted on the same platform. We discuss Sample Entropy (Samp En), a measure used in biomedical signal analysis, as a potential measure for burstiness. A modification to the original measure is introduced to make it more suitable for Cloud workloads.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {566–572},
numpages = {7},
series = {UCC '14}
}

@article{10.1145/2853073.2853085,
author = {Baliyan, Niyati and Kumar, Sandeep},
title = {A Hierarchical Fuzzy System for Quality Assessment of Semantic Web Application as a Service},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853085},
doi = {10.1145/2853073.2853085},
abstract = {Semantic Web enabled applications are becoming popular due to the presence of their machine comprehensible description, which makes them easily sharable across machines. If such applications are deployed as services to the user through the Cloud, they can facilitate transparency and reusability. There exist no attributes, metrics, or models for monitoring the quality of such applications. In the current work, a hierarchical fuzzy system for quality assessment of Semantic Web based applications delivered as services on the Cloud, is proposed. The quality attributes proposed herein have been validated through the standard IEEE-1061 validation framework. Experimental results reveal that the proposed hierarchical fuzzy system handles the multiplicity of quality attributes, and can be used for the relative ranking of Semantic Web applications available as services},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {1–7},
numpages = {7},
keywords = {Cloud, Fuzzy Logic, Quality Metrics, Semantic Web}
}

@inproceedings{10.1145/3328886.3328892,
author = {Alcivar, Nayeth I. Solorzano and Gallego, Diego Carrera and Quijije, Lissenia Sornoza and Quelal, Marco Mendoza},
title = {Developing a Dashboard for Monitoring Usability of Educational Games Apps for Children},
year = {2019},
isbn = {9781450361682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328886.3328892},
doi = {10.1145/3328886.3328892},
abstract = {Nowadays digital game applications or interactive children's educational games implemented in mobile devices (to be identified as Apps), are beginning to be widely used to complement children's education, particularly during early childhood education. However, digital game Apps do not generate a timely collection of data that could be obtained, so that with a proper interpretation they can serve as a guide in making decisions about the content, types, and level of games that should be created as digital tools to support children's education. In this article, is indicated how through the development of a dashboard, linked to a database in the cloud, it is possible to obtain and present information that allows measuring the use and playability and usability factors for these types of Apps, in an orderly and precise manner. For the development of the dashboard and its link in real time with the Apps to monitor, JavaScript was used through the framework Sails.js and the database implemented in PostgreSQL. In parallel, for the data transmission tests, two mobile applications were implemented in Android, one programmed in Unity and the second using Adobe Animate. Both Apps were designed by recording internal data in JSON file format. To analyze and obtain results, we used PQM metrics 2014 (Playability Quality Model), and we applied an adapted theory which helps to facilitate the identification of factors affecting the use and adoption of information systems and technologies in Latin American local contexts. The Pilot tests were carried out with children from 4 to 8 years attending schools of marginal areas in the city of Guayaquil, Ecuador. These children with little knowledge of technology use, facilitate better evaluation of different scenarios to measure the behavioral use of the Apps and their contents without significant influence of previous knowledge about digital educational games. This article presents the first results of an extensive and longitudinal multidisciplinary research, relevant to organizations and people involved in early childhood education.},
booktitle = {Proceedings of the 2019 2nd International Conference on Computers in Management and Business},
pages = {70–75},
numpages = {6},
keywords = {Apps, Latin America, MIDI, Ecuador, Digital Games, Dashboard, Usability, Technology Adoption and Education, Children},
location = {Cambridge, United Kingdom},
series = {ICCMB '19}
}

@inproceedings{10.1145/2790060.2790071,
author = {Legrand, H\'{e}l\`{e}ne and Boubekeur, Tamy},
title = {Morton Integrals for High Speed Geometry Simplification},
year = {2015},
isbn = {9781450337076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790060.2790071},
doi = {10.1145/2790060.2790071},
abstract = {Real time geometry processing has progressively reached a performance level that makes a number of signal-inspired primitives practical for on-line applications scenarios. This often comes through the joint design of operators, data structure and even dedicated hardware. Among the major classes of geometric operators, filtering and super-sampling (via tessellation) have been successfully expressed under high-performance constraints. The subsampling operator i.e., adaptive simplification, remains however a challenging case for non-trivial input models. In this paper, we build a fast geometry simplification algorithm over a new concept: Morton Integrals. By summing up quadric error metric matrices along Morton-ordered surface samples, we can extract concurrently the nodes of an adaptive cut in the so-defined implicit hierarchy, and optimize all simplified vertices in parallel. This approach is inspired by integral images and exploits recent advances in high performance spatial hierarchy construction and traversal. As a result, our GPU implementation can downsample a mesh made of several millions of polygons at interactive rates, while providing better quality than uniform simplification and preserving important salient features. We present results for surface meshes, polygon soups and point clouds, and discuss variations of our approach to account for per-sample attributes and alternatives error metrics.},
booktitle = {Proceedings of the 7th Conference on High-Performance Graphics},
pages = {105–112},
numpages = {8},
keywords = {GPU algorithms, Morton code, mesh simplification, adaptive clustering},
location = {Los Angeles, California},
series = {HPG '15}
}

@inproceedings{10.1145/2739482.2764720,
author = {Oprescu, Ana-Maria and (Vintila) Filip, Alexandra and Kielmann, Thilo},
title = {Fast Pareto Front Approximation for Cloud Instance Pool Optimization},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764720},
doi = {10.1145/2739482.2764720},
abstract = {Computing the Pareto Set (PS) of optimal cloud schedules in terms of cost and makespan for a given application and set of cloud instance types is NP-complete. Moreover, cloud instances' volatility requires fast PS recomputations. While genetic algorithms (GA) are a promising approach, little knowledge of an approximated PS's quality leads to GAs running for overly many generations, contradicting the goal of quickly computing an approximate solution. We address this with MOO-GA, our GA enhanced with a domain-tailored termination criteria delivering fast, well-approximated Pareto sets. We compare to NSGAIII using PS convergence and diversity, and computational effort metrics. Results show MOO-GA consistently computing better quality Pareto sets within one second on average (df=98, p-value&lt;10-3).},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1443–1444},
numpages = {2},
keywords = {pareto frontier, infrastructure-as-a-service, genetic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3361525.3361543,
author = {Grohmann, Johannes and Nicholson, Patrick K. and Iglesias, Jesus Omana and Kounev, Samuel and Lugones, Diego},
title = {Monitorless: Predicting Performance Degradation in Cloud Applications with Machine Learning},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361543},
doi = {10.1145/3361525.3361543},
abstract = {Today, software operation engineers rely on application key performance indicators (KPIs) for sizing and orchestrating cloud resources dynamically. KPIs are monitored to assess the achievable performance and to configure various cloud-specific parameters such as flavors of instances and autoscaling rules, among others. Usually, keeping KPIs within acceptable levels requires application expertise which is expensive and can slow down the continuous delivery of software. Expertise is required because KPIs are normally based on application-specific quality-of-service metrics, like service response time and processing rate, instead of generic platform metrics, like those typical across various environments (e.g., CPU and memory utilization, I/O rate, etc.)In this paper, we investigate the feasibility of outsourcing the management of application performance from developers to cloud operators. In the same way that the serverless paradigm allows the execution environment to be fully managed by a third party, we discuss a monitorless model to streamline application deployment by delegating performance management. We show that training a machine learning model with platform-level data, collected from the execution of representative containerized services, allows inferring application KPI degradation. This is an opportunity to simplify operations as engineers can rely solely on platform metrics -- while still fulfilling application KPIs -- to configure portable and application agnostic rules and other cloud-specific parameters to automatically trigger actions such as autoscaling, instance migration, network slicing, etc.Results show that monitorless infers KPI degradation with an accuracy of 97\% and, notably, it performs similarly to typical autoscaling solutions, even when autoscaling rules are optimally tuned with knowledge of the expected workload.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {149–162},
numpages = {14},
keywords = {Machine learning, DevOps, Cloud computing, Monitoring},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/3194124.3194130,
author = {Shatnawi, Anas and Orr\`{u}, Matteo and Mobilio, Marco and Riganelli, Oliviero and Mariani, Leonardo},
title = {Cloudhealth: A Model-Driven Approach to Watch the Health of Cloud Services},
year = {2018},
isbn = {9781450357302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194124.3194130},
doi = {10.1145/3194124.3194130},
abstract = {Cloud systems are complex and large systems where services provided by different operators must coexist and eventually cooperate. In such a complex environment, controlling the health of both the whole environment and the individual services is extremely important to timely and effectively react to misbehaviours, unexpected events, and failures. Although there are solutions to monitor cloud systems at different granularity levels, how to relate the many KPIs that can be collected about the health of the system and how health information can be properly reported to operators are open questions.This paper reports the early results we achieved in the challenge of monitoring the health of cloud systems. In particular we present CloudHealth, a model-based health monitoring approach that can be used by operators to watch specific quality attributes. The Cloud-Health Monitoring Model describes how to operationalize high level monitoring goals by dividing them into subgoals, deriving metrics for the subgoals, and using probes to collect the metrics. We use the CloudHealth Monitoring Model to control the probes that must be deployed on the target system, the KPIs that are dynamically collected, and the visualization of the data in dashboards.},
booktitle = {Proceedings of the 1st International Workshop on Software Health},
pages = {40–47},
numpages = {8},
keywords = {monitoring model, cloud service, KPI, software health, monitoring, metrics, quality model},
location = {Gothenburg, Sweden},
series = {SoHeal '18}
}

@inproceedings{10.1145/2964284.2973806,
author = {Mekuria, Rufael and Cesar, Pablo},
title = {MP3DG-PCC, Open Source Software Framework for Implementation and Evaluation of Point Cloud Compression},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973806},
doi = {10.1145/2964284.2973806},
abstract = {We present MP3DG-PCC, an open source framework for design, implementation and evaluation of point cloud compression algorithms. The framework includes objective quality metrics, lossy and lossless anchor codecs, and a test bench for consistent comparative evaluation. The framework and proposed methodology is in use for the development of an international point cloud compression standard in MPEG. In addition, the library is integrated with the popular point cloud library, making a large number of point cloud processing available and aligning the work with the broader open source community.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1222–1226},
numpages = {5},
keywords = {evaluation, point cloud compression, 3d virtual reality, compression},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.1145/3284360,
author = {Dey, Tamal K. and Shi, Dayu and Wang, Yusu},
title = {SimBa: An Efficient Tool for Approximating Rips-Filtration Persistence via Simplicial Batch Collapse},
year = {2019},
issue_date = {2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
issn = {1084-6654},
url = {https://doi.org/10.1145/3284360},
doi = {10.1145/3284360},
abstract = {In topological data analysis, a point cloud data P extracted from a metric space is often analyzed by computing the persistence diagram or barcodes of a sequence of Rips complexes built on P indexed by a scale parameter. Unfortunately, even for input of moderate size, the size of the Rips complex may become prohibitively large as the scale parameter increases. Starting with the Sparse Rips filtration introduced by Sheehy, some existing methods aim to reduce the size of the complex to improve time efficiency as well. However, as we demonstrate, existing approaches still fall short of scaling well, especially for high-dimensional data. In this article, we investigate the advantages and limitations of existing approaches. Based on insights gained from the experiments, we propose an efficient new algorithm, called SimBa, for approximating the persistent homology of Rips filtrations with quality guarantees. Our new algorithm leverages a batch-collapse strategy as well as a new Sparse Rips-like filtration. We experiment on a variety of low- and high-dimensional datasets. We show that our strategy presents a significant size reduction and that our algorithm for approximating Rips filtration persistence is an order of magnitude faster than existing methods in practice.},
journal = {ACM J. Exp. Algorithmics},
month = {jan},
articleno = {1.5},
numpages = {16},
keywords = {simplicial maps, rips filtration, persistent homology, approximation, Topological data analysis}
}

@inproceedings{10.1145/2996890.2996906,
author = {Chhetri, Mohan Baruwal and Vo, Quoc Bao and Kowalczyk, Ryszard},
title = {CL-SLAM: Cross-Layer SLA Monitoring Framework for Cloud Service-Based Applications},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.2996906},
doi = {10.1145/2996890.2996906},
abstract = {Modern applications are increasingly being composed from multiple components that require and consume services at different layers of the cloud stack. The diverse, dynamic and unpredictable nature of both cloud services and application workloads makes quality-assured provision of such cloud service-based applications (CSBAs) a major challenge. While elasticity and autoscaling gives CSBA providers the ability to scale cloud resources on-demand, they require a comprehensive, system-wide view of the application performance in order to make timely, cost-effective and performance-efficient scaling decisions. In this paper, we propose, develop and validate CL-SLAM - a Cross-Layer SLA Monitoring Framework for CSBAs. Its main features include (a) realtime, fine-grained visibility into CSBA performance, (b) visual descriptive analytics to identify correlations and inter-dependencies between cross-layer performance metrics, (c) temporal profiling of CSBA performance, (d) proactive monitoring, detection and root-cause analysis of SLA violation, and (e) support for both reactive and proactive adaptation in support of quality-assured CSBA provision. We validate our approach through a prototype implementation.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {30–36},
numpages = {7},
keywords = {cross-layer SLA monitoring, cloud service-based application},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/3423328.3423497,
author = {Li, Yen-Chun and Hsu, Chia-Hsin and Lin, Yu-Chun and Hsu, Cheng-Hsin},
title = {Performance Measurements on a Cloud VR Gaming Platform},
year = {2020},
isbn = {9781450381581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423328.3423497},
doi = {10.1145/3423328.3423497},
abstract = {As cloud gaming and Virtual Reality (VR) games become popular in the game industry, game developers engage in these fields to boost their sales. Because cloud gaming possesses the merit of lifting computation loads from client devices to servers, it solves the high resource consumption issue of VR games on regular clients. However, it is important to know where is the bottleneck of the cloud VR gaming platform and how can it be improved in the future. In this paper, we conduct extensive experiments on the state-of-the-art cloud VR gaming platform--Air Light VR (ALVR). In particular, we analyze the performance of ALVR using both Quality-of-Service and Quality-of-Experience metrics. Our experiments reveal that latency (up to 90 ms RTT) has less influence on user experience compared to bandwidth limitation (as small as 35 Mbps) and packet loss rate (as high as 8\%) . Moreover, we find that VR gamers can hardly notice the difference between the gaming experience with different latency values (between 0 and 90 ms RTT). Such findings shed some lights on how to further improve the cloud VR gaming platform, e.g., a budget of up to 90 ms RTT may be used to absorb network dynamics when bandwidth is insufficient.},
booktitle = {Proceedings of the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications},
pages = {37–45},
numpages = {9},
keywords = {cloud computing, measurement, computer games, virtual reality, prototype},
location = {Seattle, WA, USA},
series = {QoEVMA'20}
}

@inproceedings{10.1145/2774993.2775063,
author = {Sun, Peng and Vanbever, Laurent and Rexford, Jennifer},
title = {Scalable Programmable Inbound Traffic Engineering},
year = {2015},
isbn = {9781450334518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2774993.2775063},
doi = {10.1145/2774993.2775063},
abstract = {With the rise of video streaming and cloud services, enterprise and access networks receive much more traffic than they send, and must rely on the Internet to offer good end-to-end performance. These edge networks often connect to multiple ISPs for better performance and reliability, but have only limited ways to influence which of their ISPs carries the traffic for each service. In this paper, we present Sprite, a software-defined solution for flexible inbound traffic engineering (TE). Sprite offers direct, fine-grained control over inbound traffic, by announcing different public IP prefixes to each ISP, and performing source network address translation (SNAT) on outbound request traffic. Our design achieves scalability in both the data plane (by performing SNAT on edge switches close to the clients) and the control plane (by having local agents install the SNAT rules). The controller translates high-level TE objectives, based on client and server names, as well as performance metrics, to a dynamic network policy based on real-time traffic and performance measurements. We evaluate Sprite with live data from "in the wild" experiments on an EC2-based testbed, and demonstrate how Sprite dynamically adapts the network policy to achieve high-level TE objectives, such as balancing YouTube traffic among ISPs to improve video quality.},
booktitle = {Proceedings of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research},
articleno = {12},
numpages = {7},
keywords = {software-defined networking, scalability, traffic engineering},
location = {Santa Clara, California},
series = {SOSR '15}
}

@inproceedings{10.1145/3308560.3317075,
author = {Debattista, Jeremy and Attard, Judie and Brennan, Rob and O'Sullivan, Declan},
title = {Is the LOD Cloud at Risk of Becoming a Museum for Datasets? Looking Ahead towards a Fully Collaborative and Sustainable LOD Cloud},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317075},
doi = {10.1145/3308560.3317075},
abstract = {The Linked Open Data (LOD) cloud has been around since 2007. Throughout the years, this prominent depiction served as the epitome for Linked Data and acted as a starting point for many. In this article we perform a number of experiments on the dataset metadata provided by the LOD cloud, in order to understand better whether the current visualised datasets are accessible and with an open license. Furthermore, we perform quality assessment of 17 metrics over accessible datasets that are part of the LOD cloud. These experiments were compared with previous experiments performed on older versions of the LOD cloud. The results showed that there was no improvement on previously identified problems. Based on our findings, we therefore propose a strategy and architecture for a potential collaborative and sustainable LOD cloud.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {850–858},
numpages = {9},
keywords = {Linked Data, LOD cloud, metadata quality, data quality, sustainable services},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3344341.3368796,
author = {Kuhlenkamp, J\"{o}rn and Werner, Sebastian and Borges, Maria C. and El Tal, Karim and Tai, Stefan},
title = {An Evaluation of FaaS Platforms as a Foundation for Serverless Big Data Processing},
year = {2019},
isbn = {9781450368940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344341.3368796},
doi = {10.1145/3344341.3368796},
abstract = {Function-as-a-Service (FaaS), offers a new alternative to operate cloud-based applications. FaaS platforms enable developers to define their application only through a set of service functions, relieving them of infrastructure management tasks, which are executed automatically by the platform. Since its introduction, FaaS has grown to support workloads beyond the lightweight use-cases it was originally intended for, and now serves as a viable paradigm for big data processing. However, several questions regarding FaaS platform quality are still unanswered. Specifically, the impact of automatic infrastructure management on serverless big data applications remains unexplored.In this paper, we propose a novel evaluation method (SIEM) to understand the impact of these tasks. For this purpose, we introduce new metrics to quantify quality in different big data application scenarios. We show an application of SIEM by evaluating the four major FaaS providers, and contribute results and new insights for FaaS-based big data processing.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
pages = {1–9},
numpages = {9},
keywords = {serverless, benchmarking, cloud computing, big data processing},
location = {Auckland, New Zealand},
series = {UCC'19}
}

@inproceedings{10.1145/3307630.3342385,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {HADAS: Analysing Quality Attributes of Software Configurations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342385},
doi = {10.1145/3307630.3342385},
abstract = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses of SPLs rely on solvers to navigate complex dependencies among features and find legal solutions. Variability analysis tools are complex due to the diversity of products and domain-specific knowledge. On that, while there are experimental studies that analyse quality attributes, the knowledge is not easily accessible for developers, and its appliance is not trivial. Aiming to allow the industry to quality-explore SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects quality attributes metrics in a cloud repository, and (2) reasons about it helping developers with quality attributes requirements.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {model, variability, attribute, software product line, NFQA, numerical},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3038912.3052560,
author = {Haq, Osama and Raja, Mamoon and Dogar, Fahad R.},
title = {Measuring and Improving the Reliability of Wide-Area Cloud Paths},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052560},
doi = {10.1145/3038912.3052560},
abstract = {Many popular cloud applications use inter-data center paths; yet, little is known about the characteristics of these ``cloud paths''. Over an eighteen month period, we measure the inter-continental cloud paths of three providers (Amazon, Google, and Microsoft) using client side (VM-to-VM) measurements. We find that cloud paths are more predictable compared to public Internet paths, with an order of magnitude lower loss rate and jitter at the tail (95th percentile and beyond) compared to public Internet paths. We also investigate the nature of packet losses on these paths (e.g., random vs. bursty) and potential reasons why these paths may be better in quality. Based on our insights, we consider how we can further improve the quality of these paths with the help of existing loss mitigation techniques. We demonstrate that using the cloud path in conjunction with a detour path can mask most of the cloud losses, resulting in up to five 9's of network availability for applications.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {253–262},
numpages = {10},
keywords = {latency, cloud paths reliability, bandwidth, detour routing, inter-data center networks, cloud availability, loss rate},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.1145/2816795.2818061,
author = {Liao, Jing and Finch, Mark and Hoppe, Hugues},
title = {Fast Computation of Seamless Video Loops},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2816795.2818061},
doi = {10.1145/2816795.2818061},
abstract = {Short looping videos concisely capture the dynamism of natural scenes. Creating seamless loops usually involves maximizing spatiotemporal consistency and applying Poisson blending. We take an end-to-end view of the problem and present new techniques that jointly improve loop quality while also significantly reducing processing time. A key idea is to relax the consistency constraints to anticipate the subsequent blending, thereby enabling looping of low-frequency content like moving clouds and changing illumination. We also analyze the input video to remove an undesired bias toward short loops. The quality gains are demonstrated visually and confirmed quantitatively using a new gradient-domain consistency metric. We improve system performance by classifying potentially loopable pixels, masking the 2D graph cut, pruning graph-cut labels based on dominant periods, and optimizing on a coarse grid while retaining finer detail. Together these techniques reduce computation times from tens of minutes to nearly real-time.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {197},
numpages = {10},
keywords = {blend-aware consistency, cinemagraphs, video textures}
}

@inproceedings{10.1145/2822332.2822339,
author = {Evans, Kieran and Jones, Andrew and Preece, Alun and Quevedo, Francisco and Rogers, David and Spasi\'{c}, Irena and Taylor, Ian and Stankovski, Vlado and Taherizadeh, Salman and Trnkoczy, Jernej and Suciu, George and Suciu, Victor and Martin, Paul and Wang, Junchao and Zhao, Zhiming},
title = {Dynamically Reconfigurable Workflows for Time-Critical Applications},
year = {2015},
isbn = {9781450339896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2822332.2822339},
doi = {10.1145/2822332.2822339},
abstract = {Cloud-based applications that depend on time-critical data processing or network throughput require the capability of reconfiguring their infrastructure on demand as and when conditions change. Although the ability to apply quality of service constraints on the current Cloud offering is limited, there are ongoing efforts to change this. One such effort is the European funded SWITCH project that aims to provide a programming model and toolkit to help programmers specify quality of service and quality of experience metrics of their distributed application and to provide the means to specify the reconfiguration actions which can be taken to maintain these requirements. In this paper, we present an approach to application reconfiguration by applying a workflow methodology to implement a prototype involving multiple reconfiguration scenarios of a distributed real-time social media analysis application, called Sentinel. We show that by using a lightweight RPC-based workflow approach, we can monitor a live application in real time and spawn dependency-based workflows to reconfigure the underlying Docker containers that implement the distributed components of the application. We propose to use this prototype as the basis for part of the SWITCH workbench, which will support more advanced programmable infrastructures.},
booktitle = {Proceedings of the 10th Workshop on Workflows in Support of Large-Scale Science},
articleno = {7},
numpages = {10},
keywords = {workflows, quality of service, time-critical applications, quality of experience, dynamic data driven systems},
location = {Austin, Texas},
series = {WORKS '15}
}

@inproceedings{10.1145/3173162.3173207,
author = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A. and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark},
title = {Vbench: Benchmarking Video Transcoding in the Cloud},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173207},
doi = {10.1145/3173162.3173207},
abstract = {This paper presents vbench, a publicly available benchmark for cloud video services. We are the first study, to the best of our knowledge, to characterize the emerging video-as-a-service workload. Unlike prior video processing benchmarks, vbench's videos are algorithmically selected to represent a large commercial corpus of millions of videos. Reflecting the complex infrastructure that processes and hosts these videos, vbench includes carefully constructed metrics and baselines. The combination of validated corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and compression. We demonstrate the importance of video selection with a microarchitectural study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial corpus that are not visible in other video corpuses. Our experiments with GPUs under vbench's scoring scenarios reveal that context is critical: GPUs are well suited for live-streaming, while for video-on-demand shift costs from compute to storage and network. Counterintuitively, they are not viable for popular videos, for which highly compressed, high quality copies are required. We instead find that popular videos are currently well-served by the current trajectory of software encoders.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {797–809},
numpages = {13},
keywords = {benchmark, accelerator, video transcoding},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}

@article{10.1145/3296957.3173207,
author = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A. and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark},
title = {Vbench: Benchmarking Video Transcoding in the Cloud},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3173207},
doi = {10.1145/3296957.3173207},
abstract = {This paper presents vbench, a publicly available benchmark for cloud video services. We are the first study, to the best of our knowledge, to characterize the emerging video-as-a-service workload. Unlike prior video processing benchmarks, vbench's videos are algorithmically selected to represent a large commercial corpus of millions of videos. Reflecting the complex infrastructure that processes and hosts these videos, vbench includes carefully constructed metrics and baselines. The combination of validated corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and compression. We demonstrate the importance of video selection with a microarchitectural study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial corpus that are not visible in other video corpuses. Our experiments with GPUs under vbench's scoring scenarios reveal that context is critical: GPUs are well suited for live-streaming, while for video-on-demand shift costs from compute to storage and network. Counterintuitively, they are not viable for popular videos, for which highly compressed, high quality copies are required. We instead find that popular videos are currently well-served by the current trajectory of software encoders.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {797–809},
numpages = {13},
keywords = {accelerator, benchmark, video transcoding}
}

@inproceedings{10.1145/2666310.2666376,
author = {Qamar, Ahmad M. and Afyouni, Imad and Rahman, Md. Abdur and Rehman, Faizan Ur and Hussain, Delwar and Basalamah, Saleh and Lbath, Ahmed},
title = {A GIS-Based Serious Game Interface for Therapy Monitoring},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666376},
doi = {10.1145/2666310.2666376},
abstract = {In this paper, we present a novel idea of a map-based therapy environment for people with Hemiplegia. The therapy environment is designed according to the suggestions of therapists, which consists of a spatial map browsing serious game augmented with our novel multi-sensory natural user interface (NUI). The NUI is based on 3D motion sensors that can recognize different hand and body gestures used for browsing a 3D or 2D map. The 3D motion sensors work in a non-invasive way; hence, they do not require any wearable body attachments and can be used at home without assistance from the therapists. The map-browsing environment provides an immersive experience to the disabled users, which helps in performing therapy in an interesting and entertaining manner. We have developed analytics for measuring certain quality of health improvement metrics from each type of spatial map browsing movements. The 3D motion sensors have been tested with Nokia, Google, ESRI, and a number of other maps that allow a subject to visualize and browse the 3D and 2D maps of the world. The map browsing session data shows the nature of big data; hence, the session data is stored in a cloud environment. Our developed serious game environment is web-based; thus anyone having the appropriate low cost sensor hardware can plug it in and start experiencing a natural way of hands free map browsing. We have deployed our framework in a hospital that treats Hemiplegic patients. Based on the feedback obtained, the developed platform shows a huge potential for use in hospitals that provide physiotherapy services as well as at patients' home as an assistive therapeutic service.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {589–592},
numpages = {4},
keywords = {kinect, serious games, GIS, e-health, therapy, leap},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/3380851.3416742,
author = {Berger, Arthur},
title = {Designing an Analytics Approach for Technical Content},
year = {2020},
isbn = {9781450375252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380851.3416742},
doi = {10.1145/3380851.3416742},
abstract = {Working on an enterprise cloud product, my documentation team rethought our approach to content analytics. Despite a variety of tools and awareness of industry best practices, my team felt stuck using analytics only in annual or on-demand reports to management, instead of to produce value for our end users. We employed Design Thinking practices to guide a multifaceted user research project that led to changes in the way that we created documentation and automated quality content checks. Key takeaways include to involve the technical documentation team in identifying not only what metrics to collect, but also how to collect, report, and use the metrics in order to increase buy-in and the likelihood that data analytics about content leads to meaningful change within the content itself.},
booktitle = {Proceedings of the 38th ACM International Conference on Design of Communication},
articleno = {7},
numpages = {5},
keywords = {content strategy, design thinking,, Data analytics},
location = {Denton, TX, USA},
series = {SIGDOC '20}
}

@inproceedings{10.1109/CCGRID.2018.00021,
author = {Imai, Shigeru and Patterson, Stacy and Varela, Carlos A.},
title = {Uncertainty-Aware Elastic Virtual Machine Scheduling for Stream Processing Systems},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00021},
doi = {10.1109/CCGRID.2018.00021},
abstract = {Stream processing systems deployed on the cloud need to be elastic to effectively accommodate workload variations over time. Performance models can predict maximum sustainable throughput (MST) as a function of the number of VMs allocated. We present a scheduling framework that incorporates three statistical techniques to improve Quality of Service (QoS) of cloud stream processing systems: (i) uncertainty quantification to consider variance in the MST model; (ii) online learning to update MST model as new performance metrics are gathered; and (iii) workload models to predict input data stream rates assuming regular patterns occur over time. Our framework can be parameterized by a QoS satisfaction target that statistically finds the best performance/cost tradeoff. Our results illustrate that each of the three techniques alone significantly improves QoS, from 52\% to 73-81\% QoS satisfaction rates on average for eight benchmark applications. Furthermore, applying all three techniques allows us to reach 98.62\% QoS satisfaction rate with a cost less than twice the cost of the optimal (in hindsight) VM allocations, and half of the cost of allocating VMs for the peak demand in the workload.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {62–71},
numpages = {10},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/3412841.3441886,
author = {Chikhaoui, Amina and Lemarchand, Laurent and Boukhalfa, Kamel and Boukhobza, Jalil},
title = {StorNIR, a Multi-Objective Replica Placement Strategy for Cloud Federations},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441886},
doi = {10.1145/3412841.3441886},
abstract = {Federation of clouds makes it possible to transparently extend the resources of Cloud Service Providers (CSPs). For storage services several metrics need to be considered to satisfy customers QoS, that is storage performance, network latency and data availability. Data replication is a key strategy to optimize such metrics. For a CSP, member of a Federation, an effective placement of customers data object replicas is crucial to satisfy QoS demands. In this paper, we modeled the replica placement problem as a multi-objective optimization problem (MOOP) taking into account the local storage classes, other federation CSPs (external) storage services, and customers requirements. To solve this problem, we propose StorNIR a cost-efficient data object Storing scheme based on NSGAII upgraded with Injection and Reparation operators. StorNIR is a matheuristic that consists in hybridizing an exact method with NSGAII meta-heuristic. A repair operator was designed to make the solutions feasible with regards to the system constraints (storage volume, IOPs, etc). StorNIR performed better than both NSGAII meta-heuristic and the exact method in terms of quality of solutions and scalability. The repair function improves the NSGAII meta-heuristic up to 7 times with 7.4\% more extra time execution. On average, StorNIR enhances by 17 times the quality of the initial solutions calculated by CPLEX in terms of Hypervolume. In addition, the designed matheuristic approach can be generalized to other meta-heuristics than NSGAII such as MOPSO meta-heuristic.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {50–59},
numpages = {10},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/3291291.3291304,
author = {Silva, Gabriel Costa and R\'{e}, Reginaldo and Silva, Marco Aur\'{e}lio Graciotto},
title = {Evaluating Efficiency, Effectiveness and Satisfaction of AWS and Azure from the Perspective of Cloud Beginners},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Quality has long been regarded as an important driver of cloud adoption. In particular, quality in use (QiU) of cloud platforms may drive cloud beginners to the cloud platform that offers the best cloud experience. Cloud beginners are critical to the cloud market because they currently represent nearly a third of cloud users. We carried out three experiments to measure the QiU (dependent variable) of public cloud platforms (independent variable) regarding efficiency, effectiveness and satisfaction. AWS EC2 and Azure Virtual Machines are the two cloud services used as representative proxies to evaluate cloud platforms (treatments). Eleven undergraduate students with limited cloud knowledge (participants) manually created 152 VMs (task) using the web interface of cloud platforms (instrument) following seven different configurations (trials) for each cloud platform. Whereas AWS performed significantly better than Azure for efficiency (p-value not exceeding 0.001, A-statistic = 0.68), we could not find a significant difference between platforms for effectiveness (p-value exceeding 0.05) - although the effect size was found relevant (odds ratio = 0.41). Regarding satisfaction, most of our participants perceived the AWS as (i) having the best GUI to benefiting user interaction, (ii) the easiest platform to use, and (iii) the preferred cloud platform for creating VMs. Once confirmed by independent replications, our results suggest that AWS outperforms Azure regarding QiU. Therefore, cloud beginners might have a better cloud experience starting off their cloud projects by using AWS rather than Azure. In addition, our results may help to explain the AWS's cloud leadership.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {114–125},
numpages = {12},
keywords = {quality in use, experimentation, cloud platforms},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3452940.3452990,
author = {Yu, Minqi and Xie, Linjin and Huang, Rui and He, Xing and Yang, Maotao and Yang, Libin},
title = {Impedance Measurement of 0.4kV Power Supply Line in the Station Area Based on the Smart Energy Meter},
year = {2021},
isbn = {9781450388665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452940.3452990},
doi = {10.1145/3452940.3452990},
abstract = {Develop a monitoring and management system based on a cloud platform, and finally implement related applications such as abnormal monitoring of the electric energy meter wiring process, wiring fault diagnosis, line aging assessment, and power outage warning analysis on the cloud platform, which can promptly warn the occurrence of power outages and comprehensively improve high-quality power supply services Level. Therefore, this paper proposes a method for phase line and neutral line impedance estimation of 0.4kV low voltage distribution network based on intelligent electricity meters. In this paper, the voltage of each sequence, current of each sequence, and information of complex power of each sequence of each node are extracted by intelligent electricity meter, and then the measurement of line impedance is completed piecewise. Finally, the impedance measurement model of the 0.4kV low-voltage distribution network was built. The simulation verification of the line impedance measurement was completed through the effective cooperation with the high-precision and high-synchronous sampling smart electricity meter supporting the impedance measurement and the acquisition terminal supporting the of the impedance measurement. The simulation results show that the line impedance measurement error is small, and the prediction of line loss and impedance trend can be completed effectively.},
booktitle = {Proceedings of the 3rd International Conference on Information Technologies and Electrical Engineering},
pages = {255–260},
numpages = {6},
keywords = {Intelligent electric energy meter, Edge calculation, Line impedance measurement, Intelligent diagnosis},
location = {Changde City, Hunan, China},
series = {ICITEE '20}
}

@inproceedings{10.1145/3290688.3290705,
author = {Tesfamicael, Aklilu Daniel and Liu, Vicky and Foo, Ernest and Caelli, Bill},
title = {QoE Estimation Model for a Secure Real-Time Voice Communication System in the Cloud},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290705},
doi = {10.1145/3290688.3290705},
abstract = {As moving towards cloud-based real-time services, we are witnessing the shift from a technology-driven services to service provisioning paradigms, that is, from Quality of Service (QoS) to Quality of Experience (QoE). User experience and satisfaction are placed at the epicenter of the system design. QoE is a measurement of user experience on the provided service by a system. Often QoE is measured by subjective mechanisms, such as user experience surveys and mean opinion scores (MOS) methods, which can be a costly and time-consuming process. Using an adequate QoE model to measure user experience of perceived quality is cost-effective, compared to using time-consuming subjective surveys. Applying an adequate QoE model to assess user experience is advantageous for cloud-based real-time services such as voice and video. This study uses a formula-based QoE estimation model to estimate and predict QoE prior to the deployment or during the planning stage of the system service. This study investigates a real-world scenario of a company that recently moved to its premises-based real-time trading communication system (TCS) to a public cloud. A simulation system using OPNET is also implemented to illustrate the usefulness of the model. Our result shows that the effect of delay on the users experience of the service provided by the cloud-based TCS is minimum comparing to packet loss rate (PLR) and Jitter. However, it has been observed that the overhead of the different security settings of the TCS system had no major negative impact to the user experience. The proposed model can be used as a QoE control mechanism and network optimization for cloud-based TCS services.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {10},
numpages = {9},
keywords = {VoIP, QoS, E-Model, QoE, TCS, Real-time},
location = {Sydney, NSW, Australia},
series = {ACSW '19}
}

@inproceedings{10.1145/3078633.3081037,
author = {Wade, April W. and Kulkarni, Prasad A. and Jantz, Michael R.},
title = {AOT vs. JIT: Impact of Profile Data on Code Quality},
year = {2017},
isbn = {9781450350303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078633.3081037},
doi = {10.1145/3078633.3081037},
abstract = {Just-in-time (JIT) compilation during program execution and ahead-of-time (AOT) compilation during software installation are alternate techniques used by managed language virtual machines (VM) to generate optimized native code while simultaneously achieving binary code portability and high execution performance. Profile data collected by JIT compilers at run-time can enable profile-guided optimizations (PGO) to customize the generated native code to different program inputs. AOT compilation removes the speed and energy overhead of online profile collection and dynamic compilation, but may not be able to achieve the quality and performance of customized native code. The goal of this work is to investigate and quantify the implications of the AOT compilation model on the quality of the generated native code for current VMs.  First, we quantify the quality of native code generated by the two compilation models for a state-of-the-art (HotSpot) Java VM. Second, we determine how the amount of profile data collected affects the quality of generated code. Third, we develop a mechanism to determine the accuracy or similarity for different profile data for a given program run, and investigate how the accuracy of profile data affects its ability to effectively guide PGOs. Finally, we categorize the profile data types in our VM and explore the contribution of each such category to performance.},
booktitle = {Proceedings of the 18th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {1–10},
numpages = {10},
keywords = {Profile-guided optimizations, Program profiling},
location = {Barcelona, Spain},
series = {LCTES 2017}
}

@article{10.1145/3140582.3081037,
author = {Wade, April W. and Kulkarni, Prasad A. and Jantz, Michael R.},
title = {AOT vs. JIT: Impact of Profile Data on Code Quality},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/3140582.3081037},
doi = {10.1145/3140582.3081037},
abstract = {Just-in-time (JIT) compilation during program execution and ahead-of-time (AOT) compilation during software installation are alternate techniques used by managed language virtual machines (VM) to generate optimized native code while simultaneously achieving binary code portability and high execution performance. Profile data collected by JIT compilers at run-time can enable profile-guided optimizations (PGO) to customize the generated native code to different program inputs. AOT compilation removes the speed and energy overhead of online profile collection and dynamic compilation, but may not be able to achieve the quality and performance of customized native code. The goal of this work is to investigate and quantify the implications of the AOT compilation model on the quality of the generated native code for current VMs.  First, we quantify the quality of native code generated by the two compilation models for a state-of-the-art (HotSpot) Java VM. Second, we determine how the amount of profile data collected affects the quality of generated code. Third, we develop a mechanism to determine the accuracy or similarity for different profile data for a given program run, and investigate how the accuracy of profile data affects its ability to effectively guide PGOs. Finally, we categorize the profile data types in our VM and explore the contribution of each such category to performance.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {1–10},
numpages = {10},
keywords = {Program profiling, Profile-guided optimizations}
}

@article{10.1145/3263878,
author = {Carlsson, Niklas and Liu, Zhenhua and Nguyen, Thu and Rosenberg, Catherine and Wierman, Adam},
title = {Session Details: Special Issue on the 2016 Greenmetrics Workshop},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3263878},
doi = {10.1145/3263878},
abstract = {The seventh annual GreenMetrics Workshop was held on June 14, 2016 in Antibes Juan-les-Pins, France, in conjunction with the ACM SIGMETRICS/IFIP Performance 2016 conference. For the past five years the workshop has been expanded from topics on the energy and ecological impact of Information and Communication Technology (ICT) systems, to include emerging work on the Smart Grid. Topics of interest fall broadly into three main areas: designing sustainable ICT, ICT for sustainability, and building a smarter, more sustainable electricity grid. The workshop brought together researchers from the traditional SIGMETRICS and Performance communities with researchers and practitioners in the three areas above, to exchange technical ideas and experiences on issues related to sustainability and ICT.The workshop program included three 45-min keynote talks, and nine 20-min presentations of technical papers. All papers are included in this special issue and we briefly summarize the keynote talks here.In the first keynote "The New Sharing Economy for the Grid2050", Kameshwar Poolla from UC Berkeley discussed three sharing economy opportunities in the electricity sector- sharing storage, sharing PV generation, and sharing recruited demand flexibility. He also discussed regulatory and technical challenges to these opportunities. In addition, he presented a micro-economic analysis of decisions by firms, and quantify the benefits of sharing to various participants. Xue (Steve) Liu from McGill University presented the second keynote talk, titled "When Bits Meet Joules: A View from Data Center Operations' Perspective". He used data centers as an example to illustrate the importance of the codesign of information technologies and new energy technologies. Specifically, he focused on how to design cost-saving power management strategies for Internet data center operations.Our third keynote talk was by Florian D\"{o}rfler from ETH Z\"{u}rich, titled "Virtual Inertia Emulation and Placement in Power Grids". He presented a comprehensive analysis to address the optimal inertia placement problem, in particular, by providing a set of closed-form global optimality results for particular problem instances as well as a computational approach resulting in locally optimal solutions. He illustrated the results with a three-region power grid case study. The best student paper award was given to "Opportunities for Price Manipulation by Aggregators in Electricity Markets" by Ruhi et al. The award was determined by a committee of the invited speakers, chaired by Catherine Rosenberg, after considering both the papers and the presentations of the candidates. The authors quantified the profit an aggregator can obtain through strategic curtailment of generation in an electricity market. Efficient algorithms were shown to exist when the topology of the network is radial (acyclic). Further, significant increases in profit can be obtained through strategic curtailment in practical settings.Demand response is discussed in the following two papers. In "Optimizing the Level of Commitment in Demand Response", Comden et al. proposed a generalized demand response framework called Flexible Commitment Demand Response (FCDR) to allow for explicit choices of the level of commitment. Numerical simulations were conducted to demonstrate that FCDR brings in significant (around 50\%) social cost reductions and benefits both the LSE and customers simultaneously. In "An Emergency Demand Response Mechanism for Cloud Computing", Zhou et al. proposed an online auction for dynamic cloud resource provisioning under the emergency demand response program, which runs in polynomial time, achieves truthfulness and close-to-optimal social welfare for the cloud ecosystem.Geographical load balancing was examined by Neglia et al. in "Geographical Load Balancing Across Green Datacenters: a Mean Field Analysis". They modeled via a Markov Chain the problem of scheduling jobs by prioritizing datacenters where renewable energy is currently available. Mean field techniques were employed to derive an asymptotic approximate model and to investigate relationships and tradeoffs among the various system parameters. In "Emergence of Shared Behaviour in Distributed Scheduling Systems for Domestic Appliances", Facchini et al. showed social interaction can increase the flexibility of users and lower the peak power, resulting in a more smooth usage of energy throughout the day. Rossi et al. examined public lighting in "AURORA: an Energy Efficient Public Lighting IoT System for Smart Cities" by proposing Aurora: a low-budget, easy-to-deploy IoT control system. Aurora was deployed in a mid-size Italian municipality and its performance over 4 months was evaluated to quantify both the power and the economic saving.Wireless and wired network power consumption was studied in the following three papers. In "Radio Resource Management for Improving Energy Self-Sufficiency of Green Mobile Networks", Dalmasso et al. designed Resource on Demand strategies to reduce the base station cluster energy consumption and to adapt it to energy availability. Fan etal. also examined base stations in "Boosting Service Availability for Base Stations of Cellular Networks by Event-Driven Battery Profiling" by conducting a systematical analysis on a real world dataset and proposing an event-driven battery profiling approach to precisely extract the features that cause the working condition degradation of the battery group. Last but not least, in "Toward Power-Efficient Backbone Routers", Lu et al. studied how InTerFaces can distribute traffic flows to the Processing Engines (PEs) so that the offered loads on all active PEs are near-perfectly balanced over time, and kept close to a target load, so that the number of active PEs can be minimized.The papers presented at the workshop reflected a current concern of energy consumption associated with proliferating data centers, and other fundamental issues in green computing. The workshop incited interesting discussions and exchange among participants from North America, Europe, and Asia.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {sep},
numpages = {1}
}

@inproceedings{10.1145/2740908.2742827,
author = {Assaf, Ahmad and Senart, Aline and Troncy, Rapha\"{e}l},
title = {Roomba: Automatic Validation, Correction and Generation of Dataset Metadata},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742827},
doi = {10.1145/2740908.2742827},
abstract = {Data is being published by both the public and private sectors and covers a diverse set of domains ranging from life sciences to media or government data. An example is the Linked Open Data (LOD) cloud which is potentially a gold mine for organizations and individuals who are trying to leverage external data sources in order to produce more informed business decisions. Considering the significant variation in size, the languages used and the freshness of the data, one realizes that spotting spam datasets or simply finding useful datasets without prior knowledge is increasingly complicated. In this paper, we propose Roomba, a scalable automatic approach for extracting, validating, correcting and generating descriptive linked dataset profiles. While Roomba is generic, we target CKAN-based data portals and we validate our approach against a set of open data portals including the Linked Open Data (LOD) cloud as viewed on the DataHub. The results demonstrate that the general state of various datasets and groups, including the LOD cloud group, needs more attention as most of the datasets suffer from bad quality metadata and lack some informative metrics that are required to facilitate dataset search.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {159–162},
numpages = {4},
keywords = {linked data, dataset profile, data quality, metadata},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3373724.3373726,
author = {Chen, Wenyu and Xiong, Wei and Cheng, Jierong and Li, Yusha},
title = {Automatic Dimensional Measurement Using Datums Generated from Point Clouds},
year = {2020},
isbn = {9781450372350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373724.3373726},
doi = {10.1145/3373724.3373726},
abstract = {Dimensional measurement is critical for quality control. Manual dimensional measurement using standard gauges can only be applied on a few datums. To measure a huge number of datums, a component needs to be scanned into a point cloud and measured digitally. For precision components, datum generation on the scanned point cloud is labor-intensive. Given a raw point cloud from scanner, this paper proposes an automatic dimensional measurement solution with an adaptive local registration algorithm and an adaptive datum generation algorithm. Using datums on the CAD model as reference, the adaptive local registration algorithm selects local regions on the scanned model to compensate the local deviation between the CAD model and the scanned model. After that, with outliers and noises in the raw data, the adaptive datum generation algorithm creates the correct datums on the scanned model adaptive to the actual geometry. Dimensional measurement based on the generated datums can be achieved automatically. Moreover, the solution does not require users to manually preprocess the point cloud, such as outlier and noise removal. As such, it improves the productivity in dimensional inspection.},
booktitle = {Proceedings of the 5th International Conference on Robotics and Artificial Intelligence},
pages = {59–63},
numpages = {5},
keywords = {Inspection, Dimensional measurement, Datum generation},
location = {Singapore, Singapore},
series = {ICRAI '19}
}

@inproceedings{10.1145/3196398.3196422,
author = {Widder, David Gray and Hilton, Michael and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {I'm Leaving You, Travis: A Continuous Integration Breakup Story},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196422},
doi = {10.1145/3196398.3196422},
abstract = {Continuous Integration (CI) services, which can automatically build, test, and deploy software projects, are an invaluable asset in distributed teams, increasing productivity and helping to maintain code quality. Prior work has shown that CI pipelines can be sophisticated, and choosing and configuring a CI system involves tradeoffs. As CI technology matures, new CI tool offerings arise to meet the distinct wants and needs of software teams, as they negotiate a path through these tradeoffs, depending on their context. In this paper, we begin to uncover these nuances, and tell the story of open-source projects falling out of love with Travis, the earliest and most popular cloud-based CI system. Using logistic regression, we quantify the effects that open-source community factors and project technical factors have on the rate of Travis abandonment. We find that increased build complexity reduces the chances of abandonment, that larger projects abandon at higher rates, and that a project's dominant language has significant but varying effects. Finally, we find the surprising result that metrics of configuration attempts and knowledge dispersion in the project do not affect the rate of abandonment.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {165–169},
numpages = {5},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3447526.3472057,
author = {Chaudhary, Akash and Belani, Manshul and Maheshwari, Naman and Parnami, Aman},
title = {Verbose : Designing a Context-Based Educational System for Improving Communicative Expressions},
year = {2021},
isbn = {9781450383288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447526.3472057},
doi = {10.1145/3447526.3472057},
abstract = {ESL (English as a second language) speakers tend to follow the tone structure of their first language, making their speech difficult to understand for native speakers, thereby limiting their opportunities for education and employment. To address this problem, we build an interactive smartphone-based educational mobile application using the user-centered design process. This application teaches English intonations based on globally consistent pitch patterns through conversations with a trained chat assistant, which inculcates expert linguists’ teaching principles. After co-designing the application’s parameters with primary stakeholders and expert visual designers, we assess its effectiveness by measuring the pre and post-performance of the users after the system usage, using various quantitative measures, like intonation scores, SEQ, and SUS. Feedback from users suggests that ESL speakers find significant improvement in the perception of their vocal expressions, thereby highlighting the necessity of such a system in improving the quality of conversations that people have in general.},
booktitle = {Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction},
articleno = {41},
numpages = {13},
keywords = {Intonations, Learning application, Stress-timed language, Context-based learning, Communicative expressions},
location = {Toulouse \&amp; Virtual, France},
series = {MobileHCI '21}
}

@inproceedings{10.1145/3098603.3098608,
author = {Tasiopoulos, Argyrios G. and Atarashi, Ray and Psaras, Ioannis and Pavlou, George},
title = {On the Bitrate Adaptation of Shared Media Experience Services},
year = {2017},
isbn = {9781450350563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098603.3098608},
doi = {10.1145/3098603.3098608},
abstract = {In Shared Media Experience Services (SMESs), a group of people is interested in streaming consumption in a synchronised way, like in the case of cloud gaming, live streaming, and interactive social applications. However, group synchronisation comes at the expense of other Quality of Experience (QoE) factors due to both the dynamic and diverse network conditions that each group member experiences. Someone might wonder if there is a way to keep a group synchronised while maintaining the highest possible QoE for each one of its members. In this work, at first we create a Quality Assessment Framework capable of evaluating different SMESs improvement approaches with respect to traditional metrics like media bitrate quality, playback disruption, and end user desynchronisation. Secondly, we focus on the bitrate adaptation for improving the QoE of SMESs, as an incrementally deployable end user triggered approach, and we formulate the problem in the context of Adaptive Real Time Dynamic Programming (ARTDP). Finally, we develop and apply a simple QoE aware bitrate adaptation mechanism that we compare against youtube live-streaming traces to find that it improves the youtube performance by more than 30\%.},
booktitle = {Proceedings of the Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {25–30},
numpages = {6},
keywords = {QoE Assessment Framework, Bitrate Adaptation, Shared Media Experience Services (SMESs)},
location = {Los Angeles, CA, USA},
series = {Internet QoE '17}
}

@inproceedings{10.5555/2755535.2755538,
author = {Claypool, Mark and Finkel, David},
title = {The Effects of Latency on Player Performance in Cloud-Based Games},
year = {2014},
publisher = {IEEE Press},
abstract = {Cloud-based games are an increasingly popular method to distribute and play computer games on the Internet. While there has been some work studying network aspects of cloud-based games and examining the effects of latency on traditional games, there has not been sufficient research on the impact of latency on cloud-based games nor a comparison of the impact of latency on cloud-based games versus traditional games. This paper presents the results of two user studies that measure the objective and subjective effects of latency on cloud-based games, one study using the commercial cloud game system OnLive and the other study using the academic cloud game system GamingAnywhere. Analysis of the results shows both quality of experience and user performance degrade linearly with an increase in latency. More significantly, latency affects cloud-based games in a manner most similar to that of traditional first-person avatar games, the most sensitive class of games, despite the fact that the cloud-based games may have a different user perspective. These results have implications for cloud-based game designers and cloud system developers.},
booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
articleno = {2},
numpages = {6},
location = {Nagoya, Japan},
series = {NetGames '14}
}

@inproceedings{10.1145/3097895.3097900,
author = {Zhang, Wenxiao and Han, Bo and Hui, Pan},
title = {On the Networking Challenges of Mobile Augmented Reality},
year = {2017},
isbn = {9781450350556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097895.3097900},
doi = {10.1145/3097895.3097900},
abstract = {In this paper, we conduct a reality check for Augmented Reality (AR) on mobile devices. We dissect and measure the cloud-offloading feature for computation-intensive visual tasks of two popular commercial AR systems. Our key finding is that their cloud-based recognition is still not mature and not optimized for latency, data usage and energy consumption. In order to identify the opportunities for further improving the Quality of Experience (QoE) for mobile AR, we break down the end-to-end latency of the pipeline for typical cloud-based mobile AR and pinpoint the dominating components in the critical path.},
booktitle = {Proceedings of the Workshop on Virtual Reality and Augmented Reality Network},
pages = {24–29},
numpages = {6},
keywords = {cloud offloading, networking challenges, reality check, Augmented reality, end-to-end latency},
location = {Los Angeles, CA, USA},
series = {VR/AR Network '17}
}

@article{10.1145/3337956,
author = {Moghaddam, Sara Kardani and Buyya, Rajkumar and Ramamohanarao, Kotagiri},
title = {Performance-Aware Management of Cloud Resources: A Taxonomy and Future Directions},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3337956},
doi = {10.1145/3337956},
abstract = {The dynamic nature of the cloud environment has made the distributed resource management process a challenge for cloud service providers. The importance of maintaining quality of service in accordance with customer expectations and the highly dynamic nature of cloud-hosted applications add new levels of complexity to the process. Advances in big-data learning approaches have shifted conventional static capacity planning solutions to complex performance-aware resource management methods. It is shown that the process of decision-making for resource adjustment is closely related to the behavior of the system, including the utilization of resources and application components. Therefore, a continuous monitoring of system attributes and performance metrics provides the raw data for the analysis of problems affecting the performance of the application. Data analytic methods, such as statistical and machine-learning approaches, offer the required concepts, models, and tools to dig into the data and find general rules, patterns, and characteristics that define the functionality of the system. Obtained knowledge from the data analysis process helps to determine the changes in the workloads, faulty components, or problems that can cause system performance to degrade. A timely reaction to performance degradation can avoid violations of service level agreements, including performing proper corrective actions such as auto-scaling or other resource adjustment solutions. In this article, we investigate the main requirements and limitations of cloud resource management, including a study of the approaches to workload and anomaly analysis in the context of performance management in the cloud. A taxonomy of the works on this problem is presented that identifies main approaches in existing research from the data analysis side to resource adjustment techniques. Finally, considering the observed gaps in the general direction of the reviewed works, a list of these gaps is proposed for future researchers to pursue.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {84},
numpages = {37},
keywords = {Anomaly detection, performance management, big-data analytics, resource management}
}

@article{10.1145/3383464,
author = {Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv},
title = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3383464},
doi = {10.1145/3383464},
abstract = {Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {46},
numpages = {40},
keywords = {big data analytics application, Big data, SLA, service layer, service level agreement, SLA metrics}
}

@inproceedings{10.1145/3141128.3141132,
author = {Sastri, Yedhu and Feldhoff, Kim and Starru\ss{}, J\"{o}rn and J\"{a}kel, Ren\'{e} and M\"{u}ller-Pfefferkorn, Ralph},
title = {A Workflow for the Integral Performance Analysis of Cloud Applications Using Monitoring and Tracing Techniques},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141132},
doi = {10.1145/3141128.3141132},
abstract = {Considering the cost effectiveness, elasticity, and flexibility of virtualized cloud environments, porting HPC applications to those environments and executing them within these settings is becoming more and more popular. For this purpose, traditional HPC applications have to be redesigned as cloud applications. An analysis of the performance of the redesigned applications within the cloud environment is in-dispensable, if the applications should be efficiently executed in the cloud environment.This paper proposes a workflow for the integral performance analysis of cloud applications within a cloud environment using monitoring and tracing techniques. For this, collectd acts as a lightweight monitoring daemon for recording performance data from outside of the applications, Score-P as a profiling and tracing tool for recording the performance data from the inside. Thus, this workflow will help in answering the question "How and why an application behaves like this within the cloud environment?".In order to show the usability of the proposed workflow, a parallel client server application was selected and adapted for the execution in a private OpenStack cloud. Performance measurements of the example running in the cloud environment could be successfully done according to the proposed workflow. In particular, performance metrics from both the outside and the inside of the application could be obtained to analyze and evaluate the performance of the application in detail.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {73–78},
numpages = {6},
keywords = {Docker, Score-P, Container, Performance analysis, Micro services, Workflow, Tracing, Cloud application, Monitoring, collectd},
location = {London, United Kingdom},
series = {ICCBDC '17}
}

@inproceedings{10.1145/3428502.3428618,
author = {Symeonidis, Panagiotis and Mitropoulos, Pantelis and Taskaris, Simeon and Vakkas, Theodoros and Adamopoulou, Eleni and Karakirios, Dimitrios and Salamalikis, Vasileios and Kosmopoulos, Georgios and Kazantzidis, Andreas},
title = {ThermiAir: An Innovative Air Quality Monitoring System for Airborne Particulate Matter in Thermi, Greece},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428618},
doi = {10.1145/3428502.3428618},
abstract = {This paper presents the development of an innovative air quality monitoring platform for the Municipality of Thermi in Thessaloniki. The monitoring network consists of 25 low cost but very accurate IoT sensors measuring the concentration of Particulate Matter (PM 10, PM 2.5, PM 1.0). Using these new generation of sensors, it is feasible to monitor air quality at city block level, revealing the spatial pattern of air pollution, and thus allowing local and regional agencies to design and apply the most suitable policies and measures to tackle the air pollution problem. The real time measurements are stored in the Cloud and are disseminated to the citizens and the local authorities' stakeholders through a web and a mobile app. The web application provides an air quality dashboard which presents the overall air quality in the Municipality. Both the Air Quality Index (AQI) and raw concentration data are used. Various types of presentations are available including maps and charts. The web application provides also a three-day air quality forecast using the Copernicus forecast data. The mobile app provides easy access to the real time data in a simple to understand way, suitable for the public users.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {775–778},
numpages = {4},
keywords = {geographic information systems, data analytics, Air pollution, IoT sensors, Air quality},
location = {Athens, Greece},
series = {ICEGOV '20}
}

@article{10.1145/3047646,
author = {Chen, Qi and Liu, Ye and Liu, Guangchi and Yang, Qing and Shi, Xianming and Gao, Hongwei and Su, Lu and Li, Quanlong},
title = {Harvest Energy from the Water: A Self-Sustained Wireless Water Quality Sensing System},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3047646},
doi = {10.1145/3047646},
abstract = {Water quality data is incredibly important and valuable, but its acquisition is not always trivial. A promising solution is to distribute a wireless sensor network in water to measure and collect the data; however, a drawback exists in that the batteries of the system must be replaced or recharged after being exhausted. To mitigate this issue, we designed a self-sustained water quality sensing system that is powered by renewable bioenergy generated from microbial fuel cells (MFCs). MFCs collect the energy released from native magnesium oxidizing microorganisms (MOMs) that are abundant in natural waters. The proposed energy-harvesting technology is environmentally friendly and can provide maintenance-free power to sensors for several years. Despite these benefits, an MFC can only provide microwatt-level power that is not sufficient to continuously power a sensor. To address this issue, we designed a power management module to accumulate energy when the input voltage is as low as 0.33V. We also proposed a radio-frequency (RF) activation technique to remotely activate sensors that otherwise are switched off in default. With this innovative technique, a sensor’s energy consumption in sleep mode can be completely avoided. Additionally, this design can enable on-demand data acquisitions from sensors. We implement the proposed system and evaluate its performance in a stream. In 3-month field experiments, we find the system is able to reliably collect water quality data and is robust to environment changes.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {3},
numpages = {24},
keywords = {radio-frequency (RF) activation, Energy harvesting, water quality monitoring, microbial fuel cell, power management}
}

@inproceedings{10.1145/3388440.3414205,
author = {Vijayan, Vipin and Gu, Shawn and Krebs, Eric T. and Meng, Lei and Milenkovi\'{c}, Tijana},
title = {Pairwise Versus Multiple Global Network Alignment},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3414205},
doi = {10.1145/3388440.3414205},
abstract = {This abstract is based on the following paper: Vijayan, Vipin, Shawn Gu, Eric T. Krebs, Lei Meng, and Tijana Milenkovi\'{c}. "Pairwise Versus Multiple Global Network Alignment." IEEE Access 8 (2020): 41961--41974.Proteins, the major macromolecules of life, interact with each other to carry out cellular functioning. Thus, analyses of protein-protein interaction (PPI) networks can yield important insights into biological function, disease, and evolution. While biotechnological advancements have made PPI network data available for many species, functions of many proteins in many of these species remain unknown. One way to uncover these functions is to transfer biological knowledge from a well-studied species to a poorly-studied one. Genomic sequence alignment, which has revolutionized our biomedical understanding, can be used for this purpose. However, sequence alignment has a major drawback: it does not consider interactions between proteins (which are ultimately what carry out function). So, biological network alignment (NA) can be used in a complementary fashion to predict protein functional knowledge that sequence alignment alone cannot predict. Specifically, NA compares PPI networks of different species to find regions of their similarity (or conservation), thus allowing for the transfer of functional knowledge across conserved network (rather than just sequence) regions.Like genomic sequence alignment, NA can be local or global. Just as the recent trend in the NA field, we also focus on global NA, which can be pairwise (PNA) and multiple (MNA). While PNA aligns two networks, MNA can align more than two networks at once. Since MNA can capture conserved network regions between more networks than PNA, it is hypothesized that MNA leads to deeper biological insights compared to PNA. However, due to different outputs of PNA and MNA, a PNA method is only compared to other PNA methods, and an MNA method is only compared to other MNA methods. Comparison of PNA against MNA must be done to evaluate whether MNA indeed yields more biologically meaningful alignments than PNA, as only this would justify MNA's higher computational complexity.We introduce a framework that allows for this. We evaluate eight prominent PNA and MNA methods, on synthetic and real-world biological networks, using topological and functional alignment quality measures. We compare PNA against MNA in both a pairwise (native to PNA) and multiple (native to MNA) manner. PNA is expected to lead to higher-quality alignments than MNA under the pairwise evaluation framework. Indeed, this is what we find. MNA is expected to lead to higher-quality alignments than PNA under the multiple evaluation framework. Shockingly, we find this not always to hold; PNA is often better than MNA in this framework, depending on the choice of evaluation test. Thus, we believe that any new MNA methods should be compared not just to existing MNA methods, but also to existing PNA methods using our evaluation framework, to properly judge the quality of alignments that they produce. Also, we confirm empirically that PNA is faster than MNA in both evaluation frameworks. These results indicate that currently, MNA offers little advantage over PNA; in order for MNA to gain an advantage, a drastic redesign of MNA's current algorithmic principles might be needed.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {4},
numpages = {1},
keywords = {protein function prediction, biological network alignment, multi-network comparison},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.5555/2755535.2755555,
author = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Xiao, Yu and Yl\"{a}-J\"{a}\"{a}ski, Antti},
title = {Towards Pervasive and Mobile Gaming with Distributed Cloud Infrastructure},
year = {2014},
publisher = {IEEE Press},
abstract = {Cloud gaming, where the game is rendered in the cloud and is streamed to an end-user device through a thin client, is rapidly gaining ground. Latency is still a key challenge to cloud gaming: highly interactive games can become unplayable even with response delays below 100 ms. To overcome this issue, we propose to deploy gaming services on a more distributed cloud infrastructure, and to instantiate gaming servers in close proximity of the user when necessary in order to shorten the response delay. Our prototype distributed cloud gaming platform also allows flexible configuration of gaming controls and video streams, enabling the use of public displays in mobile cloud gaming. We test our prototype with two games in different deployment scenarios, and measure the response delay and power consumption of the mobile devices. Our experiment results confirm that it is feasible to improve the quality of gaming experience through the deployment strategies provided by the proposed system.},
booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
articleno = {16},
numpages = {6},
location = {Nagoya, Japan},
series = {NetGames '14}
}

@inproceedings{10.1145/3349611.3355543,
author = {Loh, Frank and Vomhoff, Viktoria and Wamser, Florian and Metzger, Florian and Ho\ss{}feld, Tobias},
title = {Traffic Measurement Study on Video Streaming with the Amazon Echo Show},
year = {2019},
isbn = {9781450369275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349611.3355543},
doi = {10.1145/3349611.3355543},
abstract = {The Amazon Echo Show is one of the most widely used smart speakers with the ability to stream video. Due to its popularity, the traffic profiles of such devices are of interest to network operators and providers. This work presents a measurement study of the Amazon Echo Show in terms of network traffic and streaming behavior. More than 470,hours of streaming data are collected and analyzed at network layer. Based on this, streaming quality is derived at application layer. The study quantifies the traffic and shows that streaming with the Amazon Echo Show is comparable to streaming with a native web browser, but in a more conservative way.},
booktitle = {Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {31–36},
numpages = {6},
keywords = {traffic analysis, qoe, amazon echo, alexa, streaming},
location = {Los Cabos, Mexico},
series = {Internet-QoE'19}
}

@article{10.1145/3391894,
author = {Wade, April W. and Kulkarni, Prasad A. and Jantz, Michael R.},
title = {Exploring Impact of Profile Data on Code Quality in the HotSpot JVM},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3391894},
doi = {10.1145/3391894},
abstract = {Managed language virtual machines (VM) rely on dynamic or just-in-time (JIT) compilation to generate optimized native code at run-time to deliver high execution performance. Many VMs and JIT compilers collect profile data at run-time to enable profile-guided optimizations (PGO) that customize the generated native code to different program inputs. PGOs are generally considered integral for VMs to produce high-quality and performant native code.In this work, we study and quantify the performance benefits of PGOs, understand the importance of profiling data quantity and quality/accuracy to effectively guide PGOs, and assess the impact of individual PGOs on VM performance. The insights obtained from this work can be used to understand the current state of PGOs, develop strategies to more efficiently balance the cost and exploit the potential of PGOs, and explore the implications of and challenges for the alternative ahead-of-time (AOT) compilation model used by VMs.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {48},
numpages = {26},
keywords = {Program profiling, profile-guided optimizations}
}

@inproceedings{10.1145/3001913.3006645,
author = {Gibson, Marsalis T. and Rosa, Javier and Brewer, Eric A.},
title = {MDB: A Metadata Tracking Microcontroller Micro-Database},
year = {2016},
isbn = {9781450346498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001913.3006645},
doi = {10.1145/3001913.3006645},
abstract = {This work in progress explores a database designed to enable data sharing on custom hardware data collection devices and prototypes. Projects and systems are frequently based on the Arduino framework, examples include ODK's FoneAstra [3], the Open Energy Monitor [7], and the Grove system of sensors [5]. The Arduino platform is targeted because of its ease of use, community support, and low cost as a data collecting device compared to other off-the-shelf sensors. However, there is a need for a framework suitable for microcontrollers that enable ease of integration into other data collection systems. This includes the ability to synchronize data with collection and aggregation devices designed to work offline as well as the ability to track sensors and describe data sources for other machines and users. To address the issue, we propose a solution based on an existing small database usable on the Arduino platform that would integrate into the Mezuri [6] data collection system. The database is designed to fit within the running memory constraints on a microcontroller to store sensor data with relatively few fields per reading on flash media. This framework, with explicit support for metadata, enables users in emerging regions to directly measure physical quantities as well as indirectly measure human behavior in future development projects involving direct sensing. The database can be used by a non-expert. In particular, we investigate the qualities that a technically inclined social scientist would look for when storing such data on microcontrollers. To enable Mezuri integration we will support metadata as a first class object accessible with additional utility functions and native synchronization support.},
booktitle = {Proceedings of the 7th Annual Symposium on Computing for Development},
articleno = {36},
numpages = {4},
keywords = {Emerging Regions, Arduino, Metadata, Data Collection, Microcontroller, Embedded Databases, Sensors},
location = {Nairobi, Kenya},
series = {ACM DEV '16}
}

@inproceedings{10.1145/3018896.3025135,
author = {Saha, Debanshee and Shinde, Manasi and Thadeshwar, Shail},
title = {IoT Based Air Quality Monitoring System Using Wireless Sensors Deployed in Public Bus Services},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3025135},
doi = {10.1145/3018896.3025135},
abstract = {The ambient air quality monitoring network involves the measurement of a number of air pollutants at various locations in the city so as to maintain a sustainable air quality. It is the need of hour to monitor air quality in order to reduce air pollution. Exposure to air pollution can lead to respiratory and cardiovascular diseases, which is estimated to be the cause for 620,000 early deaths in 2010, and the impact on health due to air pollution in India has been calculated at 3 percent of its GDP. In recent years, air pollution has acquired critical dimensions and the air quality in most cities that monitor outdoor air pollution fail to meet WHO guidelines for safe levels. Air pollution is a major environmental change that causes many hazardous effects on human beings which need to be controlled. With the advancements in technology, several innovations have been made in the field of communications that are transitioning to the Internet of Things (IoT). In this domain, Wireless Sensor Networks (WSN) are one of those independent sensing devices to monitor physical and environmental conditions along with thousands of applications in other fields. In this paper, we are proposing the deployment of WSN sensor nodes in public transport buses for the constant monitoring of air pollution. The data regarding the air pollution particles such as emissions, smoke, and other pollutants will be collected via sensors on the public transport bus and the data will be aggregated and transmitted to the nearest sink node. Using the concept of the Internet of Things (IoT) the collected data will be uploaded on the cloud server also called as the IoT cloud where a large amount of the data is stored. This data can then be accessed at any point to analyze and accurate measures can be taken to map the air pollution.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {87},
numpages = {6},
keywords = {wireless sensor networks, internet of things (IoT), smart city, air pollution},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3472163.3472173,
author = {Zouari, Firas and Kabachi, Nadia and Boukadi, Khouloud and Ghedira Guegan, Chirine},
title = {Data Management in the Data Lake: A Systematic Mapping},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472173},
doi = {10.1145/3472163.3472173},
abstract = {The computer science community is paying more and more attention to data due to its crucial role in performing analysis and prediction. Researchers have proposed many data containers such as files, databases, data warehouses, cloud systems, and recently data lakes in the last decade. The latter enables holding data in its native format, making it suitable for performing massive data prediction, particularly for real-time application development. Although data lake is well adopted in the computer science industry, its acceptance by the research community is still in its infancy stage. This paper sheds light on existing works for performing analysis and predictions on data placed in data lakes. Our study reveals the necessary data management steps, which need to be followed in a decision process, and the requirements to be respected, namely curation, quality evaluation, privacy-preservation, and prediction. This study aims to categorize and analyze proposals related to each step mentioned above.},
booktitle = {Proceedings of the 25th International Database Engineering \&amp; Applications Symposium},
pages = {280–284},
numpages = {5},
keywords = {Data management, Data lake, Systematic mapping},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1109/CCGrid.2014.32,
author = {Balaji, Mahesh and Rao, G Subrahmanya Vrk and Kumar, Ch. Aswani},
title = {A Comparitive Study of Predictive Models for Cloud Infrastructure Management},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.32},
doi = {10.1109/CCGrid.2014.32},
abstract = {Cloud service providers, monitor average resource (for e.g. CPU) consumption and based on predefined limits (for e.g. CPU-Idle-time &gt; 500 milliseconds), provision or de-provision resources. Traditionally this is a reactive approach and doesn't fully address the wide range of enterprise use cases. Implementation of predictive approach to resource management has been rarely reported even though they could perform potentially better than their counterpart. Identification of a suitable model for predicting the performance of the system under a load is an ideal precursor in managing resources on a cloud environment. The current study compares the performance of two such predictive models namely Holt-Winter and ARIMA using a public web server data set Request rate was used as the metric to monitor resource consumption. The experiment results show that Holt-Winter model performs better than a few selected ARIMA models, which could be subsequently used for managing resources on cloud if the data request rates follow a similar pattern},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {923–926},
numpages = {4},
keywords = {holt-winter, resource management, cloud computing, ARIMA, predictive modeling},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/2578260.2578273,
author = {Wang, Cong and Zink, Michael},
title = {On the Feasibility of DASH Streaming in the Cloud},
year = {2018},
isbn = {9781450327060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578260.2578273},
doi = {10.1145/2578260.2578273},
abstract = {As shown in recent studies, video streaming is by far the biggest category of backbone Internet traffic in the US. As a measure to reduce the cost of highly over-provisioned physical infrastructures while remaining the quality of video services, many streaming service providers started to use cloud services where physical resources can be dynamically allocated based on current demand. This paper characterizes the performance of Dynamic Adaptive Streaming over HTTP (DASH), a new MPEG standard on adaptive streaming, in the cloud. We seek to answer the following questions that are critical to content providers that are hosting video in clouds: Which data center is the best to host videos? Does geographical distance matter? What type of instance is best suitable depending on different needs? How to efficiently solve the trade-off between performance and cost? The measurement methods and results presented in this paper can be easily expanded into other VoD services, and they allow us to i) characterize DASH behavior when streaming from the cloud; ii) identify the key factors that influence the DASH performance; and iii) suggest improvements for related services.},
booktitle = {Proceedings of Network and Operating System Support on Digital Audio and Video Workshop},
pages = {49–54},
numpages = {6},
keywords = {HTTP adaptive streaming, Cloud computing, video-on-demand, quality of experience},
location = {Singapore, Singapore},
series = {NOSSDAV '14}
}

@inproceedings{10.1145/2597176.2578273,
author = {Wang, Cong and Zink, Michael},
title = {On the Feasibility of DASH Streaming in the Cloud},
year = {2014},
isbn = {9781450327060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597176.2578273},
doi = {10.1145/2597176.2578273},
abstract = {As shown in recent studies, video streaming is by far the biggest category of backbone Internet traffic in the US. As a measure to reduce the cost of highly over-provisioned physical infrastructures while remaining the quality of video services, many streaming service providers started to use cloud services where physical resources can be dynamically allocated based on current demand. This paper characterizes the performance of Dynamic Adaptive Streaming over HTTP (DASH), a new MPEG standard on adaptive streaming, in the cloud. We seek to answer the following questions that are critical to content providers that are hosting video in clouds: Which data center is the best to host videos? Does geographical distance matter? What type of instance is best suitable depending on different needs? How to efficiently solve the trade-off between performance and cost? The measurement methods and results presented in this paper can be easily expanded into other VoD services, and they allow us to i) characterize DASH behavior when streaming from the cloud; ii) identify the key factors that influence the DASH performance; and iii) suggest improvements for related services.},
booktitle = {Proceedings of Network and Operating System Support on Digital Audio and Video Workshop},
pages = {49–54},
numpages = {6},
keywords = {HTTP adaptive streaming, quality of experience, video-on-demand, Cloud computing},
location = {Singapore, Singapore},
series = {NOSSDAV '14}
}

@inproceedings{10.1145/2950290.2994157,
author = {Rossi, Chuck and Shibley, Elisa and Su, Shi and Beck, Kent and Savor, Tony and Stumm, Michael},
title = {Continuous Deployment of Mobile Software at Facebook (Showcase)},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2994157},
doi = {10.1145/2950290.2994157},
abstract = {Continuous deployment is the practice of releasing software updates to production as soon as it is ready, which is receiving increased adoption in industry. The frequency of updates of mobile software has traditionally lagged the state of practice for cloud-based services for a number of reasons. Mobile versions can only be released periodically. Users can choose when and if to upgrade, which means that several different releases coexist in production. There are hundreds of Android hardware variants, which increases the risk of having errors in the software being deployed.  Facebook has made significant progress in increasing the frequency of its mobile deployments. Over a period of 4 years, the Android release has gone from a deployment every 8 weeks to a deployment every week. In this paper, we describe in detail the mobile deployment process at FB. We present our findings from an extensive analysis of software engineering metrics based on data collected over a period of 7 years. A key finding is that the frequency of deployment does not directly affect developer productivity or software quality. We argue that this finding is due to the fact that increasing the frequency of continuous deployment forces improved release and deployment automation, which in turn reduces developer workload. Additionally, the data we present shows that dog-fooding and obtaining feedback from alpha and beta customers is critical to maintaining release quality.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {12–23},
numpages = {12},
keywords = {Agile development, Mobile code testing, Continuous delivery, Software release, Continuous deployment},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3341105.3373915,
author = {Santos, Guilherme and Paulino, Herv\'{e} and Vardasca, Tom\'{e}},
title = {QoE-Aware Auto-Scaling of Heterogeneous Containerized Services (and Its Application to Health Services)},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373915},
doi = {10.1145/3341105.3373915},
abstract = {Containerized service is currently a widely adopted solution to deploy services in the cloud. However, many companies offer a very diverse set of Web accessible services that are subjected to very distinctive workloads. Consequently, to correctly provision the right amount of resources for each of these services is a challenge. In this paper we propose the Autonomic ConTainerized Service Scaler (ACTS), an autonomic system able to horizontally and vertically scale a set of heterogeneous containerized services subjected to different workloads. The adaptation decisions depended on a set of high-level Quality of Experience (QoE) metrics centered on the services' end-user. We have applied ACTS to some of the digital services of the Shared Services of the Ministry of Health (SPMS) public company. The experimental results show that our solution is able to adequately adapt the configuration of each service, as a direct response to alterations on its workload.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {242–249},
numpages = {8},
keywords = {auto-scaling, health care, containers, quality of experience},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/2940136.2940143,
author = {Wamser, Florian and Seufert, Michael and H\"{o}fner, Steffen and Tran-Gia, Phuoc},
title = {Concept for Client-Initiated Selection of Cloud Instances for Improving QoE of Distributed Cloud Services},
year = {2016},
isbn = {9781450344258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940136.2940143},
doi = {10.1145/2940136.2940143},
abstract = {We introduce a concept for client-initiated selection of service location and service quality for improving the Quality of Experience (QoE) of general cloud services. It is loosely based on the HTTP adaptive streaming approach (e.g., MPEG DASH). A manifest file compiled by the cloud service provider specifies the available service locations and qualities, from which the user selects the optimal service instance based on contextual information obtained from client measurements and user preferences. The proposed concept is defined and is implemented in two client-based decision algorithms for improving the QoE of a simple picture gallery cloud service. These decision algorithms are evaluated and their impact on the service delivery is discussed. The evaluation shows that it is possible to improve the service location and quality selection by light-weight client-based algorithms.},
booktitle = {Proceedings of the 2016 Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {49–54},
numpages = {6},
keywords = {Cloud Services, Quality of Experience, Client-based Access for Cloud Services},
location = {Florianopolis, Brazil},
series = {Internet-QoE '16}
}

@inproceedings{10.1145/3127479.3131614,
author = {Yadwadkar, Neeraja J. and Hariharan, Bharath and Gonzalez, Joseph E. and Smith, Burton and Katz, Randy H.},
title = {Selecting the Best VM across Multiple Public Clouds: A Data-Driven Performance Modeling Approach},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3131614},
doi = {10.1145/3127479.3131614},
abstract = {Users of cloud services are presented with a bewildering choice of VM types and the choice of VM can have significant implications on performance and cost. In this paper we address the fundamental problem of accurately and economically choosing the best VM for a given workload and user goals. To address the problem of optimal VM selection, we present PARIS, a data-driven system that uses a novel hybrid offline and online data collection and modeling framework to provide accurate performance estimates with minimal data collection. PARIS is able to predict workload performance for different user-specified metrics, and resulting costs for a wide range of VM types and workloads across multiple cloud providers. When compared to sophisticated baselines, including collaborative filtering and a linear interpolation model using measured workload performance on two VM types, PARIS produces significantly better estimates of performance. For instance, it reduces runtime prediction error by a factor of 4 for some workloads on both AWS and Azure. The increased accuracy translates into a 45\% reduction in user cost while maintaining performance.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {452–465},
numpages = {14},
keywords = {cloud computing, data-driven modeling, resource allocation, performance prediction},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/3458305.3463384,
author = {Sethuraman, Manasvini and Sarma, Anirudh and Dhekne, Ashutosh and Ramachandran, Umakishore},
title = {Foresight: Planning for Spatial and Temporal Variations in Bandwidth for Streaming Services on Mobile Devices},
year = {2021},
isbn = {9781450384346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458305.3463384},
doi = {10.1145/3458305.3463384},
abstract = {Spatiotemporal variation in cellular bandwidth availability is well-known and could affect a mobile user's quality of experience (QoE), especially while using bandwidth intensive streaming applications such as movies, podcasts, and music videos during commute. If such variations are made available to a streaming service in advance it could perhaps plan better to avoid sub-optimal performance while the user travels through regions of low bandwidth availability. The intuition is that such future knowledge could be used to buffer additional content in regions of higher bandwidth availability to tide over the deficits in regions of low bandwidth availability. Foresight is a service designed to provide this future knowledge for client apps running on a mobile device. It comprises three components: (a) a crowd-sourced bandwidth estimate reporting facility, (b) an on-cloud bandwidth service that records the spatiotemporal variations in bandwidth and serves queries for bandwidth availability from mobile users, and (c) an on-device bandwidth manager that caters to the bandwidth requirements from client apps by providing them with bandwidth allocation schedules. Foresight is implemented in the Android framework. As a proof of concept for using this service, we have modified an open-source video player---Exoplayer---to use the results of Foresight in its video buffer management. Our performance evaluation shows Foresight's scalability. We also showcase the opportunity that Foresight offers to ExoPlayer to enhance video quality of experience (QoE) despite spatiotemporal bandwidth variations for metrics such as overall higher bitrate of playback, reduction in number of bitrate switches, and reduction in the number of stalls during video playback.},
booktitle = {Proceedings of the 12th ACM Multimedia Systems Conference},
pages = {227–240},
numpages = {14},
keywords = {spatiotemporal bandwidth information, bandwidth management},
location = {Istanbul, Turkey},
series = {MMSys '21}
}

@inproceedings{10.1145/2835075.2835078,
author = {Adegboyega, Abiola},
title = {An Adaptive Resource Provisioning Scheme for Effective QoS Maintenance in the IaaS Cloud},
year = {2015},
isbn = {9781450337328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835075.2835078},
doi = {10.1145/2835075.2835078},
abstract = {Effective bandwidth provisioning is of vital importance in the virtualized cloud where tenants with unique SLAs share a finite network. Different tenants collocated on the same physical server deployed with increasing VM density necessitates Quality of Service (QoS) provisioning beginning at the hypervisor. Recent efforts at provisioning the cloud network through various reservation methodologies have achieved some measure of success. However most of them do not account for the entire path over which application components communicate and cannot provide the necessary Service Level Agreement (SLA). Cloud applications components often communicate across multiple network devices aggregated into layers connected over finite bandwidth links that affect application response. Furthermore, traffic to and from tenant applications display volatility. In view of this, we design a virtual network reservation framework that is mindful of application performance across multiple network devices \&amp; traffic volatility. Our network reservation framework is based on a forecasting engine motivated by the volatility existent in traffic to and from virtualized cloud environments. This forecasting engine is able to maintain SLAs by employing dynamic time-series models to develop novel bandwidth provisioning thresholds that adapt to the time-variation in tenant workloads. We test the effectiveness of our methods in the OpenStack cloud environment focusing on traffic directionality in the datacenter network, VM density and QoS across multiple flows competing for finite bandwidth. Our forecasting method offers a 25\% improvement in prediction accuracy over existing methods while the reservation framework maintains SLAs at 95\%.},
booktitle = {Proceedings of the International Workshop on Virtualization Technologies},
articleno = {2},
numpages = {6},
keywords = {Volatility, SDN, Forecasting, Virtualization, QoS},
location = {Vancouver, BC, Canada},
series = {VT15}
}

@inproceedings{10.1145/3185768.3186297,
author = {Versluis, Laurens and van Eyk, Erwin and Iosup, Alexandru},
title = {An Analysis of Workflow Formalisms for Workflows with Complex Non-Functional Requirements},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186297},
doi = {10.1145/3185768.3186297},
abstract = {Cloud and datacenter operators offer progressively more sophisticated service level agreements to customers. The Quality-of-Service guarantees by these operators have started to entail non-functional requirements customers have regarding their applications. At the same time, expressing applications as workflows in datacenters is increasingly more common. Currently, non-functional requirements (NFRs) can only be defined on entire workflows and cannot be changed at runtime, possibly wasting valuable resources. To move towards modifiable NFRs at the task level, there is a need for a formalism capable of expressing this. Existing formalisms do not support this level of granularity or are restricted to a subset of NFRs. In this work, we investigate the current support for NFRs in existing formalisms. Using a library containing workflows with and without NFRs, we inspect the capability of existing formalisms to express these requirements. Additionally, we create and evaluate five metrics to qualitatively and quantitatively compare each formalism. Our main findings are that although current formalisms do not support arbitrary NFRs per-task, the Directed Acyclic Graphs (DAGs) formalism is the most suitable to extend.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {107–112},
numpages = {6},
keywords = {formalism, datacenter, non-functional requirement, workflow, cloud},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1109/SEAMS.2017.2,
author = {Moreno, Gabriel A. and Papadopoulos, Alessandro V. and Angelopoulos, Konstantinos and C\'{a}mara, Javier and Schmerl, Bradley},
title = {Comparing Model-Based Predictive Approaches to Self-Adaptation: CobRA and PLA},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.2},
doi = {10.1109/SEAMS.2017.2},
abstract = {Modern software-intensive systems must often guarantee certain quality requirements under changing run-time conditions and high levels of uncertainty. Self-adaptation has proven to be an effective way to engineer systems that can address such challenges, but many of these approaches are purely reactive and adapt only after a failure has taken place. To overcome some of the limitations of reactive approaches (e.g., lagging behind environment changes and favoring short-term improvements), recent proactive self-adaptation mechanisms apply ideas from control theory, such as model predictive control (MPC), to improve adaptation. When selecting which MPC approach to apply, the improvement that can be obtained with each approach is scenario-dependent, and so guidance is needed to better understand how to choose an approach for a given situation. In this paper, we compare CobRA and PLA, two approaches that are inspired by MPC. CobRA is a requirements-based approach that applies control theory, whereas PLA is architecture-based and applies stochastic analysis. We compare the two approaches applied to RUBiS, a benchmark system for web and cloud application performance, discussing the required expertise needed to use both approaches and comparing their run-time performance with respect to different metrics.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {42–53},
numpages = {12},
keywords = {CobRA, latency, self-adaptation, adaptive system, model predictive control, PLA},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@inproceedings{10.1145/2930238.2930249,
author = {Musto, Cataldo and Lops, Pasquale and Basile, Pierpaolo and de Gemmis, Marco and Semeraro, Giovanni},
title = {Semantics-Aware Graph-Based Recommender Systems Exploiting Linked Open Data},
year = {2016},
isbn = {9781450343688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2930238.2930249},
doi = {10.1145/2930238.2930249},
abstract = {The ever increasing interest in semantic technologies and the availability of several open knowledge sources have fueled recent progress in the field of recommender systems. In this paper we feed recommender systems with features coming from the Linked Open Data (LOD) cloud - a huge amount of machine-readable knowledge encoded as RDF statements - with the aim of improving recommender systems effectiveness. In order to exploit the natural graph-based structure of RDF data, we study the impact of the knowledge coming from the LOD cloud on the overall performance of a graph-based recommendation algorithm. In more detail, we investigate whether the integration of LOD-based features improves the effectiveness of the algorithm and to what extent the choice of different feature selection techniques influences its performance in terms of accuracy and diversity. The experimental evaluation on two state of the art datasets shows a clear correlation between the feature selection technique and the ability of the algorithm to maximize a specific evaluation metric. Moreover, the graph-based algorithm leveraging LOD-based features is able to overcome several state of the art baselines, such as collaborative filtering and matrix factorization, thus confirming the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization},
pages = {229–237},
numpages = {9},
keywords = {graphs, graph-based recommender systems, feature selection, diversity, linked open data, pagerank},
location = {Halifax, Nova Scotia, Canada},
series = {UMAP '16}
}

@inproceedings{10.1145/3070607.3070608,
author = {Hutchison, Dylan and Howe, Bill and Suciu, Dan},
title = {LaraDB: A Minimalist Kernel for Linear and Relational Algebra Computation},
year = {2017},
isbn = {9781450350198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3070607.3070608},
doi = {10.1145/3070607.3070608},
abstract = {Analytics tasks manipulate structured data with variants of relational algebra (RA) and quantitative data with variants of linear algebra (LA). The two computational models have overlapping expressiveness, motivating a common programming model that affords unified reasoning and algorithm design. At the logical level we propose LARA, a lean algebra of three operators, that expresses RA and LA as well as relevant optimization rules. We show a series of proofs that position LARA at just the right level of expressiveness for a middleware algebra: more explicit than MapReduce but more general than RA or LA. At the physical level we find that the LARA operators afford efficient implementations using a single primitive that is available in a variety of backend engines: range scans over partitioned sorted maps.To evaluate these ideas, we implemented the LARA operators as range iterators in Apache Accumulo, a popular implementation of Google's BigTable. First we show how LARA expresses a sensor quality control task, and we measure the performance impact of optimizations LARA admits on this task. Second we show that the LARADB implementation outperforms Accumulo's native MapReduce integration on a core task involving join and aggregation in the form of matrix multiply, especially at smaller scales that are typically a poor fit for scale-out approaches. We find that LARADB offers a conceptually lean framework for optimizing mixed-abstraction analytics tasks, without giving up fast record-level updates and scans.},
booktitle = {Proceedings of the 4th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond},
articleno = {2},
numpages = {10},
location = {Chicago, IL, USA},
series = {BeyondMR'17}
}

@inproceedings{10.1145/3417113.3422184,
author = {Malavolta, Ivano and Grua, Eoin Martino and Lam, Cheng-Yu and de Vries, Randy and Tan, Franky and Zielinski, Eric and Peters, Michael and Kaandorp, Luuk},
title = {A Framework for the Automatic Execution of Measurement-Based Experiments on Android Devices},
year = {2021},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3422184},
doi = {10.1145/3417113.3422184},
abstract = {Conducting measurement-based experiments is fundamental for assessing the quality of Android apps in terms of, e.g., energy consumption, CPU, and memory usage. However, orchestrating such experiments is not trivial as it requires large boilerplate code, careful setup of measurement tools, and the adoption of various empirical best practices scattered across the literature. All together, those factors are slowing down the scientific advancement and harming experiments' replicability in the mobile software engineering area.In this paper we present Android Runner (AR), a framework for automatically executing measurement-based experiments on native and web apps running on Android devices. In AR, an experiment is defined once in a descriptive fashion, and then its execution is fully automatic, customizable, and replicable. AR is implemented in Python and it can be extended with third-party profilers.AR has been used in more than 25 scientific studies primarily targeting performance and energy efficiency.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {61–66},
numpages = {6},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3408293,
author = {He, Xin and Liu, Qiong and Yang, You},
title = {Make Full Use of Priors: Cross-View Optimized Filter for Multi-View Depth Enhancement},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3408293},
doi = {10.1145/3408293},
abstract = {Multi-view video plus depth (MVD) is the promising and widely adopted data representation for future 3D visual applications and interactive media. However, compression distortions on depth videos impede the development of such applications, and filters are crucially needed for the quality enhancement at the terminal side. Cross-view priors can intuitively be involved in filter design, but these priors are also distorted in compression and thus the contribution of them can hardly be considered in previous research. In this article, we propose a cross-view optimized filter for depth map quality enhancement by making full use of inner- and cross-view priors. We dedicate to evaluate the contributions of distorted cross-view priors in filtering the current view of depth, and then both inner- and cross-view priors can be involved in the filter design. Thus, distortions of cross-view priors are not barriers again as before. For the purpose of that, mutual information guided cross-view consistency is designed to evaluate the contributions of cross-view priors from compression distortions of MVD. After that, under the framework of global optimization, both inner- and cross-view priors are modeled and taken to minimize the designed energy function where both data accuracy and spatial smoothness are modeled. The experimental results show that the proposed model outperforms state-of-the-art methods, where 3.289 dB and 0.0407 average gains on peak signal-to-noise ratio and structural similarity metrics can be obtained, respectively. For the subjective evaluations, object details and structure information are recovered in the compressed depth video. We also verify our method via several practical applications, including virtual view synthesis for smooth interaction and point cloud for 3D modeling for accuracy evaluation. In these verifications, the ringing and malposition artifacts on object contours are properly handled for interactive video, and discontinuous object surfaces are restored for 3D modeling. All of these results suggest that compression distortions in MVD can be properly filtered by the proposed model, which provides a promising solution for future bandwidth constrained 3D and interactive visual applications.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {dec},
articleno = {127},
numpages = {19},
keywords = {global optimization, view consistency, Multi-view video plus depth}
}

@inproceedings{10.1145/3458306.3458873,
author = {Huang, Tianchi and Zhang, Rui-Xiao and Sun, Lifeng},
title = {Deep Reinforced Bitrate Ladders for Adaptive Video Streaming},
year = {2021},
isbn = {9781450384353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458306.3458873},
doi = {10.1145/3458306.3458873},
abstract = {In the typical transcoding pipeline for adaptive video streaming, raw videos are pre-chunked and pre-encoded according to a set of resolution-bitrate or resolution-quality pairs on the server-side, where the pair is often named as bitrate ladder. Different from existing heuristics, we argue that a good bitrate ladder should be optimized by considering video content features, network capacity, and storage costs on the cloud. We propose DeepLadder, a per-chunk optimization scheme which adopts state-of-the-art deep reinforcement learning (DRL) method to optimize the bitrate ladder w.r.t the above concerns. Technically, DeepLadder selects the proper setting for each video resolution autoregressively. We use over 8,000 video chunks, measure over 1,000,000 perceptual video qualities, collect real-world network traces for more than 50 hours, and invent faithful virtual environments to help train DeepLadder efficiently. Across a series of comprehensive experiments on both Constant Bitrate (CBR) and Variable Bitrate (VBR)-encoded videos, we demonstrate significant improvements in average video quality bandwidth utilization, and storage overhead in comparison to prior work as well as the ability to be deployed in the real-world transcoding framework.},
booktitle = {Proceedings of the 31st ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {66–73},
numpages = {8},
keywords = {bitrate ladder, adaptive video streaming},
location = {Istanbul, Turkey},
series = {NOSSDAV '21}
}

@inproceedings{10.1145/3277453.3277484,
author = {Shanthasheela, A. and Shanmugavadivu, P.},
title = {An Exploratory Analysis of Speckle Noise Removal Methods for Satellite Images},
year = {2018},
isbn = {9781450365413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277453.3277484},
doi = {10.1145/3277453.3277484},
abstract = {Satellite images captured in a variety of modalities serve as the primary source for many applications. Satellite image processing extracts the image /spectral information represented in the form of pixels, classifies those pixels based on the similarity measures and further analyzes the inherent data, as per the requirements. The foremost objective of satellite processing is to automatically categorize the pixels in an image into the respective land cover class labels or themes. These pixels are classified by its spectral information and it is determined by the relative reflectance in various bands of wavelength. The accuracy and outcomes of any satellite image processing procedure, irrespective of the application domain, directly depends on its quality. Satellite images are invariably degraded by speckle noise. Hence, preprocessing the images for speckle noise suppression and/or cloud removal is deemed an inevitable component in satellite image processing. Researchers have proposed a spectrum of methods for speckle noise/cloud removal. A detailed review on the significant research publications on speckle noise removal are summarized in this article. The consolidation of methodology merits and demerits of the select research articles are presented in this paper. This review article on speckle noise removal is designed as a ready-reference for those researchers working in satellite image processing.},
booktitle = {Proceedings of the 2018 International Conference on Electronics and Electrical Engineering Technology},
pages = {217–222},
numpages = {6},
keywords = {Literature Survey, Noise filters, RADAR, Speckle Noise, Review, SAR, Satellite images},
location = {Tianjin, China},
series = {EEET '18}
}

@inproceedings{10.1145/3349611.3355546,
author = {Schwind, Anika and Haberzettl, Lorenz and Wamser, Florian and Ho\ss{}feld, Tobias},
title = {QoE Analysis of Spotify Audio Streaming and App Browsing},
year = {2019},
isbn = {9781450369275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349611.3355546},
doi = {10.1145/3349611.3355546},
abstract = {Spotify is the most-listened audio streaming provider in 2019 with 217 million active users per month. Providers are therefore interested in the quality and functionality of Spotify in order to provide their users with the best possible streaming quality. While video streaming services such as Netflix and their streaming approach have been extensively explored in previous research, audio streaming services like Spotify and their corresponding behavior at certain network conditions have not been considered in detail yet. In this paper, we perform a QoE analysis under various network conditions and examine the app browsing performance of the audio streaming platform Spotify using its native Android mobile application. We have developed a measurement tool that emulates a user listening to audio through Spotify. While streaming, application and network layer parameters are captured that have a high correlation to the user's QoE. The paper shows a baseline scenario including the streaming of a single song as well as playlist streaming behavior. Next, the effect of interruptions on the streaming behavior is evaluated and finally, the influence of network impairments on QoE key performance indicators such as initial delay is shown.},
booktitle = {Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {25–30},
numpages = {6},
keywords = {qoe, spotify, audio streaming, mobile application, browsing},
location = {Los Cabos, Mexico},
series = {Internet-QoE'19}
}

@article{10.1145/3106158,
author = {Willnecker, Felix and Krcmar, Helmut},
title = {Multi-Objective Optimization of Deployment Topologies for Distributed Applications},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3106158},
doi = {10.1145/3106158},
abstract = {Modern applications are typically implemented as distributed systems comprising several components. Deciding where to deploy which component is a difficult task that today is usually assisted by logical topology recommendations. Choosing inefficient topologies allocates the wrong amount of resources, leads to unnecessary operation costs, or results in poor performance. Testing different topologies to find good solutions takes a lot of time and might delay productive operations. Therefore, this work introduces a software-based deployment topology optimization approach for distributed applications. We use an enhanced performance model generator that extracts models from operational monitoring data of running applications. The extracted model is used to simulate performance metrics (e.g., resource utilization, response times, throughput) and runtime costs of distributed applications. Subsequently, we introduce a deployment topology optimizer, which selects an optimized topology for a specified workload and considers on-premise, cloud, and hybrid topologies. The following three optimization goals are presented in this work: (i) minimum response time for an optimized user experience, (ii) approximate resource utilization around certain peaks, and (iii) minimum cost for running the application. To evaluate the approach, we use the SPECjEnterpriseNEXT industry benchmark as distributed application in an on-premise and in a cloud/on-premise hybrid environment. The evaluation demonstrates the accuracy of the simulation compared to the actual deployment by deploying an optimized topology and comparing measurements with simulation results.},
journal = {ACM Trans. Internet Technol.},
month = {jan},
articleno = {21},
numpages = {21},
keywords = {performance model, performance model generation, Deployment topology optimzation, distributed enterprise applications, memory simulation}
}

@inproceedings{10.1145/3318396.3318427,
author = {Ho, P. C. W. and Fok, W. W. T. and Chan, C. K. K. and Yeung, H. H. Au and Ng, H. W. and Wong, S. L. and Ngai, S. Y. and Kwok, P. H. and Ho, Y. S. and Chan, K. H.},
title = {Flipping the Learning and Teaching of Reading Strategies and Comprehension through a Cloud-Based Interactive Big Data Reading Platform},
year = {2019},
isbn = {9781450362672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318396.3318427},
doi = {10.1145/3318396.3318427},
abstract = {This study investigates the learning approach of the designed Flipped Reading Platform (FRP) and its effects on primary school students' general Chinese reading and comprehension capabilities. This study was undertaken as part of the Quality Education Fund project in Hong Kong, titled "Flipped Reading: Enhancing the Learning and Teaching of Reading Strategies and Comprehension in Chinese via an Interactive Cloud Platform."This paper presents the design of the Interactive Cloud Platform FRP, which incorporates elements of both reading strategies and learning activities, and investigates the changes in students' reading performance, applied strategies, and active learning level with the application of FRP. The results show the experimental students using the FRP in the pilot scheme generally gained more in three stages of reading comprehension, and that low-achieving students learned reading strategies better. Analysis of FRP log activities shows students' active engagement in reading and perceived competence. Different learning outcomes were also found within the experimental group, categorized by BYOD and non-BYOD classes. Implications of the study show the effectiveness of FRP, and the design demonstrates how the reading measures integrated the assessment indicators of both international and local standards in the domain of Chinese Language reading. Further research can be developed to examine individual online reading performance and learning behaviour on FRP.},
booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
pages = {185–191},
numpages = {7},
keywords = {reading strategy, Cloud Platform, e-Learning, Chinese Language, Flipped reading, Big data},
location = {Cambridge, United Kingdom},
series = {ICEIT 2019}
}

@inproceedings{10.1145/3229591.3229592,
author = {R\"{u}th, Jan and Glebke, Ren\'{e} and Wehrle, Klaus and Causevic, Vedad and Hirche, Sandra},
title = {Towards In-Network Industrial Feedback Control},
year = {2018},
isbn = {9781450359085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229591.3229592},
doi = {10.1145/3229591.3229592},
abstract = {Controlling physical machinery and processes is at the core of production automation. However, challenged by inflexibility, automation and control is evaluating to outsource this control to resourceful cloud environments. While this enables to derive better control through a plethora of measurements, it challenges the control quality through delay introduced through networks.In this paper, we show how to unify control and communication by offloading delay sensitive control tasks from the cloud to local network elements --- a previously unexplored area for in-network processing --- enabling both, ultra-high quality-of-control and scalable orchestration through cloud environments. Our implementation demonstrates how we combine state of the art control with communication. We achieve this by expressing the control and the datapath in P4 which we synthesize to BPF programs that we execute in XDP environments on Netronome SmartNICs. Further, we highlight the demands of control towards communication to build more involved and complex in-network controllers.},
booktitle = {Proceedings of the 2018 Morning Workshop on In-Network Computing},
pages = {14–19},
numpages = {6},
location = {Budapest, Hungary},
series = {NetCompute '18}
}

@inproceedings{10.1145/3233547.3233666,
author = {Kotlar, Alex V. and Wingo, Thomas S.},
title = {Tutorial: Rapidly Identifying Disease-Associated Rare Variants Using Annotation and Machine Learning at Whole-Genome Scale Online},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233666},
doi = {10.1145/3233547.3233666},
abstract = {Accurately identifying disease-associated alleles from large sequencing experiments remains challenging. During this tutorial, participants will learn how to use a new variant annotation and filtering web app called Bystro (https://bystro.io/) to analyze sequencing experiments. Bystro is the first online, cloud-based application that makes variant annotation and filtering accessible to all researchers for even the largest, terabyte-sized whole-genome experiments containing thousands of samples. Using its general-purpose, natural-language filtering engine, attendees will be shown how to perform quality control measures and identify alleles of interest. They will then be guided in exporting those variants, and using them in both a regression context by performing rare-variant association tests in R, as well as classification context by training new machine learning models in Python's scikit-learn library.},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {558},
numpages = {1},
keywords = {variant classification, bioinformatics, machine learning, rare-variant association tests},
location = {Washington, DC, USA},
series = {BCB '18}
}

@inproceedings{10.1145/3410992.3410996,
author = {Noura, Mahda and Heil, Sebastian and Gaedke, Martin},
title = {Natural Language Goal Understanding for Smart Home Environments},
year = {2020},
isbn = {9781450387583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410992.3410996},
doi = {10.1145/3410992.3410996},
abstract = {One of the main challenges of the Internet of Things (IoT) is to enable end-users without technical experience to use, control or monitor smart devices. However, enabling end-users to interact with these smart devices in an intuitive and natural way becomes increasingly important as they become more pervasive in our homes, workplaces and public environments. Voice-based interfaces are the emerging trend to provide a more natural human-device interaction in smart environments. Such interfaces require Natural Language Understanding (NLU) approaches to identify the meaning of end-users' voice inputs. Designing voice interfaces that are not limited to a small, fixed set of pre-defined commands is far from trivial. Existing voice-based solutions in the smart home domain either restrict the end-users to follow a strict language pattern, do not support indirect goals, require a large training dataset, or need a voice assistant located in the cloud. In this paper, we propose an approach for understanding end-users goals from voice inputs in smart homes. Our approach alleviates the need for end-users to learn or remember concrete operations of the devices and specific words/pattern structures rather it enables them to control their smart homes based on the desired goals (effects). We evaluate the approach through application to a collection of 253 goals from real end-users and report on quality metrics. The results demonstrate that our solution provides a good accuracy, high precision and acceptable recall for understanding end-users goals in the smart home domain.},
booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
articleno = {1},
numpages = {8},
keywords = {natural language understanding, internet of things, smart home, goal recognition, voice interface},
location = {Malm\"{o}, Sweden},
series = {IoT '20}
}

@article{10.1145/2930659,
author = {Papadopoulos, Alessandro Vittorio and Ali-Eldin, Ahmed and \r{A}rz\'{e}n, Karl-Erik and Tordsson, Johan and Elmroth, Erik},
title = {PEAS: A Performance Evaluation Framework for Auto-Scaling Strategies in Cloud Applications},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/2930659},
doi = {10.1145/2930659},
abstract = {Numerous auto-scaling strategies have been proposed in the past few years for improving various Quality of Service (QoS) indicators of cloud applications, for example, response time and throughput, by adapting the amount of resources assigned to the application to meet the workload demand. However, the evaluation of a proposed auto-scaler is usually achieved through experiments under specific conditions and seldom includes extensive testing to account for uncertainties in the workloads and unexpected behaviors of the system. These tests by no means can provide guarantees about the behavior of the system in general conditions. In this article, we present a Performance Evaluation framework for Auto-Scaling (PEAS) strategies in the presence of uncertainties. The evaluation is formulated as a chance constrained optimization problem, which is solved using scenario theory. The adoption of such a technique allows one to give probabilistic guarantees of the obtainable performance. Six different auto-scaling strategies have been selected from the literature for extensive test evaluation and compared using the proposed framework. We build a discrete event simulator and parameterize it based on real experiments. Using the simulator, each auto-scaler’s performance is evaluated using 796 distinct real workload traces from projects hosted on the Wikimedia foundations’ servers, and their performance is compared using PEAS. The evaluation is carried out using different performance metrics, highlighting the flexibility of the framework, while providing probabilistic bounds on the evaluation and the performance of the algorithms. Our results highlight the problem of generalizing the conclusions of the original published studies and show that based on the evaluation criteria, a controller can be shown to be better than other controllers.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {aug},
articleno = {15},
numpages = {31},
keywords = {Performance evaluation, auto-scaling, elasticity, cloud computing, randomized optimization}
}

@inproceedings{10.1145/3090354.3090366,
author = {Zertal, Soumia and Batouche, Mohamed Chawki},
title = {A Hybrid Approach for Optimized Composition of Cloud Services},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090366},
doi = {10.1145/3090354.3090366},
abstract = {The increasing use of Cloud services as well as the increasing demands of complex cloud services creates the need for a dynamic and adaptive composition of services, in a decentralized and large scale environment, where the quality of services may increase or decrease. Early attempts for dynamic composition of services have been proposed. But they are limited by their ability to adapt when deploying in highly dynamic and open environments. For better performance measurements, we use, in this paper, the Particle Swarm Optimization (PSO) algorithm to find and provide the services that meets the user's query. To assess the utility of each service, we take into consideration its values of service quality provided in the past. The latter is represented by the mechanism of stigmergy which uses the pheromone as a means of communication between services.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {12},
numpages = {7},
keywords = {Particle Swarm Optimization, Service Composition, Optimization, Stigmegy, Cloud Computing},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@article{10.1145/3038919,
author = {Olson, Judith S. and Wang, Dakuo and Olson, Gary M. and Zhang, Jingwen},
title = {How People Write Together Now: Beginning the Investigation with Advanced Undergraduates in a Project Course},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3038919},
doi = {10.1145/3038919},
abstract = {Today's commercially available word processors allow people to write collaboratively in the cloud, both in the familiar asynchronous mode and now in synchronous mode as well. This opens up new ways of working together. We examined the data traces of collaborative writing behavior in student teams’ use of Google Docs to discover how they are writing together now. We found that student teams write both synchronously and asynchronously, take fluid roles in the writing and editing of the documents, and show a variety of styles of collaborative writing, including writing from scratch, beginning with an outline, pasting in a related example as a template to organize their own writing, and three more. We also found that the document serves as a place where they share a number of things not included in the final document, including links or references to related materials, the assignment requirements from the instructor, and informal discussions to coordinate the collaboration or to structure the document. We computed a number of measures to depict a group's collaboration behavior and asked external graders to score these documents for quality. We found that the documents that included balanced participation and/or exhibited leadership were judged higher in quality, as were those that were longer. We then suggested system design implications and behavioral guidelines to support people writing together better, and concluded the paper with future research directions.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {mar},
articleno = {4},
numpages = {40},
keywords = {writing style, collaboration, co-authoring, Google docs}
}

@inproceedings{10.1109/CCGRID.2017.120,
author = {Shekhar, Shashank and Gokhale, Aniruddha},
title = {Dynamic Resource Management Across Cloud-Edge Resources for Performance-Sensitive Applications},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.120},
doi = {10.1109/CCGRID.2017.120},
abstract = {A large number of modern applications and systems are cloud-hosted, however, limitations in performance assurances from the cloud, and the longer and often unpredictable end-to-end network latencies between the end user and the cloud can be detrimental to the response time requirements of the applications, specifically those that have stringent Quality of Service (QoS) requirements. Although edge resources, such as cloudlets, may alleviate some of the latency concerns, there is a general lack of mechanisms that can dynamically manage resources across the cloud-edge spectrum. To address these gaps, this research proposes Dynamic Data Driven Cloud and Edge Systems (D3CES). It uses measurement data collected from adaptively instrumenting the cloud and edge resources to learn and enhance models of the distributed resource pool. In turn, the framework uses the learned models in a feedback loop to make effective resource management decisions to host applications and deliver their QoS properties. D3CES is being evaluated in the context of a variety of cyber physical systems, such as smart city, online games, and augmented reality applications.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {707–710},
numpages = {4},
keywords = {Resource Management, IoT, Fog Computing, DDDAS, CPS, Edge Computing, Cloud Computing},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3417113.3423001,
author = {Pathania, Priyavanshi and Mithani, Rajan Dilavar},
title = {Sustainability in Migrating Workloads to Public Clouds},
year = {2021},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3423001},
doi = {10.1145/3417113.3423001},
abstract = {In recent times, there has been a considerable increase in Cloud-Based applications and infrastructure. This has led to quicker innovations, agile businesses, availability of new services over the internet, improved collaboration, and better security. With the growth of new technologies like blockchain, quantum computing, mobility-focused applications, and edge computing, there has been an increased interest in adopting cloud services. In this paper, we highlight the different sustainability metrics and benefits while migrating workloads from the on-prem data center to the public clouds. Also, the clouds are elastic, scalable, cost-efficient, robust, and overall a better alternative to host the client applications and services. We present how the major Cloud Service Providers (CSPs) are continuously working on improving their infrastructure for a more energy efficient cloud. But with so many factors like the cost of cloud services, the location of the data center to name a few, it becomes quite a tedious task for the clients to select a cloud service provider when moving from their on-premise data center(s). Hence, we also briefly propose our solution that we are currently working on. The final goal is to have a cross-platform advisory that based on a wide-range of client-based inputs and a rich repository of current energy efficient clouds and their sustainability metrics, aims to provide them a detailed recommendation about their preferred cloud service provider. In case the client does not provide any such preference, the advisory should also recommend an ideal cloud service provider for their particular workload. This suggested action will be able to fulfill the client's constraints as well as provide them an energy efficient cloud along with a sustainability score. This score is indicative of how much improvement in the energy consumed and carbon footprint can be achieved through this migration to the suggested cloud.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {166–169},
numpages = {4},
keywords = {pre-migration cloud sustainability, carbon footprint, cloud sustainability, cloud computing, energy efficient cloud},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3030207.3053676,
author = {Chow, Kingsum and Zhu, Wanyi},
title = {Software Performance Analytics in the Cloud},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3053676},
doi = {10.1145/3030207.3053676},
abstract = {The emergence of large-scale software deployments in the cloud has led to several challenges: (1) measuring software performance in the data center, and (2) optimizing software for resource management. This tutorial addresses the two challenges by bringing the knowledge of software performance monitoring in the data center to the world of applying performance analytics. It introduces data transformations for software performance metrics. The transformations enable effective applications of analytics. This tutorial starts with software performance in the small and ends with applying analytics to software performance in the large. In software performance in the small, it summarizes performance tools, data collection and manual analysis. Then it describes monitoring tools that are helpful in performance analysis in the large. The tutorial will guide the audience in applying analytics to performance data obtained by common tools. This tutorial describes how to select analytical methods and what precautions should be taken to get effective results.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {419–421},
numpages = {3},
keywords = {analytics, datacenter efficiency, software performance, capacity planning},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/3339825.3393581,
author = {Taraghi, Babak and Zabrovskiy, Anatoliy and Timmerer, Christian and Hellwagner, Hermann},
title = {CAdViSE: Cloud-Based Adaptive Video Streaming Evaluation Framework for the Automated Testing of Media Players},
year = {2020},
isbn = {9781450368452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339825.3393581},
doi = {10.1145/3339825.3393581},
abstract = {Attempting to cope with fluctuations of network conditions in terms of available bandwidth, latency and packet loss, and to deliver the highest quality of video (and audio) content to users, research on adaptive video streaming has attracted intense efforts from the research community and huge investments from technology giants. How successful these efforts and investments are, is a question that needs precise measurements of the results of those technological advancements. HTTP-based Adaptive Streaming (HAS) algorithms, which seek to improve video streaming over the Internet, introduce video bitrate adaptivity in a way that is scalable and efficient. However, how each HAS implementation takes into account the wide spectrum of variables and configuration options, brings a high complexity to the task of measuring the results and visualizing the statistics of the performance and quality of experience. In this paper, we introduce CAdViSE, our Cloud-based Adaptive Video Streaming Evaluation framework for the automated testing of adaptive media players. The paper aims to demonstrate a test environment which can be instantiated in a cloud infrastructure, examines multiple media players with different network attributes at defined points of the experiment time, and finally concludes the evaluation with visualized statistics and insights into the results.},
booktitle = {Proceedings of the 11th ACM Multimedia Systems Conference},
pages = {349–352},
numpages = {4},
keywords = {quality of experience, media players, automated testing, network emulation, MPEG-DASH, HTTP adaptive streaming},
location = {Istanbul, Turkey},
series = {MMSys '20}
}

@inproceedings{10.1145/3428502.3428511,
author = {Branco, Te\'{o}filo T. and Kawashita, Ilka M. and de S\'{a}-Soares, Filipe and Monteiro, Cl\'{a}udio N.},
title = {An IoT Application Case Study to Optimize Electricity Consumption in the Government Sector},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428511},
doi = {10.1145/3428502.3428511},
abstract = {This paper presents a case study where sensor modules supported by Internet of Things (IoT) technology were used to monitor and control electricity consumption of air conditioning units in an innovation center of a public government institution. This study evaluates alternatives to improve the management of electricity consumption in Salvador City Hall's facilities. To contribute to the economy and sustainability of the Administration, we aim to increase the efficiency of the processes currently adopted. Our focus is on minimizing electricity waste and reducing costs. Installed sensor modules measure electricity consumption and control the operation of air conditioning equipment, allowing the administrator to manage the operation of these devices. The installation of smart sensor modules connected to an IoT platform allows energy consumption data to be sent to a computing Cloud and to be monitored remotely through dashboards generated by specialized software. A quantitative analysis was conducted to measure the efficiency of the air conditioning control system and identify opportunities for applying the IoT solution to control natural resources in the public sector. The monitoring of these signals subsidized the analyzes required for informed decision making of interventions to improve the system's stability and promote the reduction of consumption. Also, the system has demonstrated its ability to protect air conditioners, monitor the quality of the power supplied, proactively control consumption, and establish appropriate user behaviors for reducing consumption. Results demonstrated the feasibility of implementing automated systems to improve the consumption of natural resources in the public sector. We also identified some managerial behaviors required to enable this type of technological solution.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {70–81},
numpages = {12},
keywords = {Innovation, Smart Technologies, Sustainability, E-government, Internet of Thinks (IoT)},
location = {Athens, Greece},
series = {ICEGOV '20}
}

@inproceedings{10.1145/3458305.3463380,
author = {Ramos-Chavez, Roberto and Mekuria, Rufael and Karagkioules, Theo and Griffioen, Dirk and Wagenaar, Arjen and Ogle, Mark},
title = {MPEG NBMP Testbed for Evaluation of Real-Time Distributed Media Processing Workflows at Scale},
year = {2021},
isbn = {9781450384346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458305.3463380},
doi = {10.1145/3458305.3463380},
abstract = {Real-time Distributed Media Processing Workflows (DMPW) are popular for online media delivery. Combining distributed media sources and processing can reduce storage costs and increase flexibility. However, high request rates may result in unacceptable latency or even failures in incorrect configurations. Thus, testing DMPW deployments at scale is key, particularly for real-time cases. We propose the new MPEG Network Based Media Processing (NBMP) standard for this and present a testbed implementation that includes all the reference components. In addition, the testbed includes a set of configurable functions for load generation, monitoring, data-collection and visualization. The testbed is used to test Dynamic Adaptive HTTP streaming functions under different workloads in a standardized and reproducible manner. A total of 327 tests with different loads and Real-Time DMPW configurations were completed. The results provide insights in the performance, reliability and time-consistency of each configuration. Based on these tests, we selected the preferred cloud instance type, considering hypervisor options and different function implementation configurations. Further, we analyzed different processing tasks and options for distributed deployments on edge and centralized clouds. Last, a classifier was developed to detect if failures happen under a certain workload. Results also show that, normalized inter-experiment standard deviation of the metric means can be an indicator for unstable or incorrect configurations.},
booktitle = {Proceedings of the 12th ACM Multimedia Systems Conference},
pages = {173–185},
numpages = {13},
keywords = {standards, experimentation},
location = {Istanbul, Turkey},
series = {MMSys '21}
}

@article{10.1109/TNET.2020.2971587,
author = {Cheng, Yingying and Jia, Xiaohua},
title = {NAMP: Network-Aware Multipathing in Software-Defined Data Center Networks},
year = {2020},
issue_date = {April 2020},
publisher = {IEEE Press},
volume = {28},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2971587},
doi = {10.1109/TNET.2020.2971587},
abstract = {Data center networks employ parallel paths to perform load balancing. Existing traffic splitting schemes propose weighted traffic distribution across multiple paths via a centralized view. An SDN controller computes the traffic splitting ratio of a flow group among all the paths, and implements the ratio by creating multiple rules in the flow table of OpenFlow switches. However, since the number of rules in TCAM-based flow table is limited, it is not scalable to implement the ideal splitting ratio for every flow group. Existing solutions, WCMP and Niagara, aim at reducing the maximum oversubscription of all egress ports and reducing traffic imbalance, respectively. However, the transmission time of flow groups, which measures the quality of cloud services, is sub-optimal in existing solutions that ignore heterogeneous network bandwidth. We propose and implement NAMP, a multipathing scheme considering the network heterogeneity, to efficiently optimize the transmission time of flow groups. Experimental results show that NAMP reduces the transmission time by up to 45.4% than Niagara, up to 50% than WCMP, and up to 60% than ECMP.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {846–859},
numpages = {14}
}

@inproceedings{10.1109/CCGrid.2014.22,
author = {Byholm, Benjamin and Porres, Iv\'{a}n},
title = {Cost-Efficient, Reliable, Utility-Based Session Management in the Cloud},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.22},
doi = {10.1109/CCGrid.2014.22},
abstract = {We present a model and system for cost-efficient and reliable management of sessions in a Cloud, based on the von Neumann-Morgenstern utility theorem. Our model enables a web application provider to maximize profit while maintaining a desired quality of service. The objective is to determine whether, when, where, and how long to store a session, given multiple storage options with various properties, e.g. cost, capacity, and reliability. Reliability is affected by three factors: how often session state is stored, how many stores are used, and how reliable those stores are. To account for these factors, we use a Markovian reliability model and treat the valid storage options for each session as a von Neumann-Morgenstern lottery. We proceed by representing the resulting problem as a knapsack problem, which can be heuristically solved for a good compromise between efficiency and effectiveness. We analyze the results from a discrete-event simulation involving multiple session management policies, including two utility-based policies: a greedy heuristic policy intended to give real-time performance and a reference policy based on solving the linear programming relaxation of the knapsack problem, giving a theoretical upper bound on achievable utility. As the focus of this work is exploratory, rather than performance-based, we do not directly measure the time required for solving the model. Instead, we give the computational complexity of the algorithms. Our results indicate that otherwise unprofitable services become profitable through utility-based session management in a cloud setting. However, if the costs are much lower than the expected revenues, all policies manage to turn a profit. Different policies performed the best under different circumstances.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {102–111},
numpages = {10},
keywords = {analytical models, utility theory, simulation, reliability, availability, web-based services, and serviceability, markov processes, distributed systems},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/2801694.2801710,
author = {Ganesan, Deepak},
title = {Towards Ultra-Low Power Wearable Health Sensing with Sparse Sampling and Asymmetric Communication},
year = {2015},
isbn = {9781450337014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801694.2801710},
doi = {10.1145/2801694.2801710},
abstract = {Wearable sensors offer tremendous opportunities for accelerating biomedical discovery, and improving population-scale health and wellness. There is a growing appetite for health analytics -- we are no longer content with wearables that count steps and calories, we want to measure physiology, behavior, activities, cognition, affect, and other parameters with the expectation that such data will lead to deep insights that can improve quality of life.But a chasm separates expectations and reality. How do we extract such insights from sensor platforms with tiny energy budgets? How do we communicate high-rate sensor data to the cloud for enabling deep analytics while operating within these energy budgets? How do we deal with noise, confounders, and artifacts that make insights hard to extract from signals collected in real-world settings?In this talk, I will discuss a few strategies to tackle these problems. I will discuss how we can design an low-power computational eyeglass that continually tracks eye and visual context by leveraging sparsity, how we can transfer data at Megabits/second from wearables while operating at tens of micro-watts of power, and how we can leverage these techniques in the context of mobile health.},
booktitle = {Proceedings of the 2015 Workshop on Wireless of the Students, by the Students, \&amp; for the Students},
pages = {34},
numpages = {1},
keywords = {backscatter communication, mobile health, eye tracking},
location = {Paris, France},
series = {S3 '15}
}

@inproceedings{10.1145/2996890.3007864,
author = {Muhammad-Bello, Bilkisu Larai and Aritsugi, Masayoshi},
title = {TCloud: A Transparent Framework for Public Cloud Service Comparison},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.3007864},
doi = {10.1145/2996890.3007864},
abstract = {Whilst there are many attributes that need to be considered for cloud service selection, performance remains one of the most crucial aspects. Thus, we argue for a transparent cloud provider comparison framework in this study. We initiate the development of TCloud: a transparent framework for public cloud service comparison. Our framework helps prospective cloud users to decipher public cloud benchmarking data and appraise the performance of public cloud services relative to their performance goals. We carried out experiments on the real public cloud environment to implement our framework and demonstrated how prospective cloud users can use the TCloud framework in understanding how well virtualized public cloud resources meet their application requirements. Unlike previous studies, the TCloud framework presents a more realistic method of appraising the performance of virtualized resources in the public cloud. TCloud is unique in the sense that it collates public cloud benchmarking data and correlates the observed performance metrics to prospective cloud users' actual application workload requirements.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {228–233},
numpages = {6},
keywords = {cloud service selection, workload-based performance analysis, cloud performance benchmarking},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/3213344.3213354,
author = {Khan, Jamal Ahmad and Shahzad, Muhammad and Butt, Ali R.},
title = {Sizing Buffers of IoT Edge Routers},
year = {2018},
isbn = {9781450358378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213344.3213354},
doi = {10.1145/3213344.3213354},
abstract = {In typical IoT systems, sensors and actuators are connected to small embedded computers, called IoT devices, and the IoT devices are connected to one or more appropriate cloud services over the internet through an edge access router. A very important design aspect of an IoT edge router is the size of the output packet buffer of its interface that connects to the access link. Selecting an appropriate size for this buffer is crucial because it directly impacts two key performance metrics: 1) access link utilization and 2) latency. In this paper, we calculate the size of the output buffer that ensures that the access link stays highly utilized and at the same time, significantly lowers the average latency experienced by the packets. To calculate this buffer size, we theoretically model the average TCP congestion window size of all IoT devices while eliminating three key assumptions of prior art that do not hold true for IoT TCP traffic, as we will demonstrate through a measurement study. We show that for IoT traffic, buffer size calculated by our method results in 50\% lower queuing delay compared to the state of the art schemes while achieving similar access link utilization and loss-rate.},
booktitle = {Proceedings of the 1st International Workshop on Edge Systems, Analytics and Networking},
pages = {55–60},
numpages = {6},
keywords = {IoT, Buffers, Edge Routers},
location = {Munich, Germany},
series = {EdgeSys'18}
}

@inproceedings{10.1145/3412382.3458276,
author = {Klugman, Noah and Adkins, Joshua and Paszkiewicz, Emily and Hickman, Molly G. and Podolsky, Matthew and Taneja, Jay and Dutta, Prabal},
title = {Watching the Grid: Utility-Independent Measurements of Electricity Reliability in Accra, Ghana},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3458276},
doi = {10.1145/3412382.3458276},
abstract = {In much of the world, electricity grids are not instrumented at the customer level, limiting insights into the power quality experienced by utility customers. Moreover, to understand grid performance, regulators and investors must depend on utilities to self-report reliability data. To address these challenges, we introduce PowerWatch, an agile methodology to directly measure customer experience and aggregated grid performance without relying on the utility for deployment or management. PowerWatch employs a system of distributed sensors coupled with cloud-based analytics. We evaluate the PowerWatch methodology by deploying 462 sensors in homes and businesses in Accra, Ghana for over a year, yielding the largest open-source data set on electricity reliability at the customer-level in the region. We describe the architecture, design, and performance of PowerWatch, as well as the data that are collected, explaining how we determine the accuracy and coverage of our methodology without ground truth. Finally, we report on grid performance issues, finding nearly twice as many outages as the utility observed, suggesting a need for better grid performance monitoring.},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {341–356},
numpages = {16},
keywords = {Sensor deployments, Grid reliability metering},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@inproceedings{10.1145/3395027.3419595,
author = {Ughetta, William and Kernighan, Brian W.},
title = {The Old Bailey and OCR: Benchmarking AWS, Azure, and GCP with 180,000 Page Images},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419595},
doi = {10.1145/3395027.3419595},
abstract = {The Proceedings of the Old Bailey is a corpus of over 180,000 page images of court records printed from April 1674 to April 1913 and presents a comprehensive challenge for Optical Character Recognition (OCR) services. The Old Bailey is an ideal benchmark for historical document OCR, representing more than two centuries of variations in documents, including spellings, formats, and printing and preservation qualities. In addition to its historical and sociological significance, the Old Bailey is filled with imperfections that reflect the reality of coping with large-scale historical data. Most importantly, the Old Bailey contains human transcriptions for each page, which can be used to help measure OCR accuracy. Since humans do make mistakes in transcriptions, the relative performance of OCR services will be more informative than their absolute performance. This paper compares three leading commercial OCR cloud services: Amazon Web Services's Textract (AWS); Microsoft Azure's Cognitive Services (Azure); and Google Cloud Platform's Vision (GCP). Benchmarking involved downloading over 180,000 images, executing the OCR, and measuring the error rate of the OCR text against the human transcriptions. Our results found that AWS had the lowest median error rate, Azure had the lowest median round trip time, and GCP had the best combination of a low error rate and a low duration.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {19},
numpages = {4},
keywords = {Google Cloud Platform, Optical Character Recognition, Old Bailey, Amazon Web Services, Microsoft Azure, Historical Documents},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@inproceedings{10.1145/2676662.2676677,
author = {Patiniotakis, Ioannis and Verginadis, Yiannis and Mentzas, Gregoris},
title = {Preference-Based Cloud Service Recommendation as a Brokerage Service},
year = {2014},
isbn = {9781450332330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676662.2676677},
doi = {10.1145/2676662.2676677},
abstract = {As the multitude and complexity of cloud services increases, the role of cloud brokers in the cloud service ecosystems becomes increasingly important. In particular, the lack of standard mechanisms that allow for the comparison of cloud service specifications against user requirements taking into account the implicit uncertainty and vagueness is a major hindrance during the cloud service evaluation and selection. In this paper, we discuss the Preference-based cLoud Service Recommender (PuLSaR) that uses a holistic multi-criteria decision making (MCDM) approach for offering optimisation as brokerage service. The specification and implementation details of this dedicated software are thoroughly discussed while the background method used is summarised. Both method and brokerage service allow for the multi-objective assessment of cloud services in a unified way, taking into account precise and imprecise metrics and dealing with their fuzziness.},
booktitle = {Proceedings of the 2nd International Workshop on CrossCloud Systems},
articleno = {5},
numpages = {6},
keywords = {cloud service broker, service ranking, MCDM, optimisation},
location = {Bordeaux, France},
series = {CCB '14}
}

@inproceedings{10.1145/3386293.3397116,
author = {Sabet, Saeed Shafiee and Schmidt, Steven and Zadtootaghaj, Saman and Griwodz, Carsten and M\"{o}ller, Sebastian},
title = {Delay Sensitivity Classification of Cloud Gaming Content},
year = {2020},
isbn = {9781450379472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386293.3397116},
doi = {10.1145/3386293.3397116},
abstract = {Cloud Gaming is an emerging service that catches growing interest in the research community as well as industry. Cloud Gaming require a highly reliable and low latency network to achieve a satisfying Quality of Experience (QoE) for its users. Using a cloud gaming service with high latency would harm the interaction of the user with the game, leading to a decrease in playing performance and, thus players frustrations. However, the negative effect of delay on gaming QoE depends strongly on the game content. At a certain level of delay, a slow-paced card game is typically not as delay sensitive as a shooting game. For optimal resource allocation and quality estimation, it is highly important for cloud providers, game developers, and network planners to consider the impact of the game content. This paper contributes to a better understanding of the delay impact on QoE for cloud gaming applications by identifying game characteristics influencing the delay perception of the users. In addition, an expert evaluation methodology to quantify these characteristics as well as a delay sensitivity classification based on a decision tree are presented. The results indicated an excellent level of agreement, which demonstrates the reliability of the proposed method. Additionally, the decision tree reached an accuracy of 90\% on determining the delay sensitivity classes which were derived from a large dataset of subjective input quality ratings during a series of experiments.},
booktitle = {Proceedings of the 12th ACM International Workshop on Immersive Mixed and Virtual Environment Systems},
pages = {25–30},
numpages = {6},
keywords = {cloud gaming, delay, QoE, content classification},
location = {Istanbul, Turkey},
series = {MMVE '20}
}

@inproceedings{10.1145/2987550.2987584,
author = {Cano, Ignacio and Aiyar, Srinivas and Krishnamurthy, Arvind},
title = {Characterizing Private Clouds: A Large-Scale Empirical Analysis of Enterprise Clusters},
year = {2016},
isbn = {9781450345255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987550.2987584},
doi = {10.1145/2987550.2987584},
abstract = {There is an increasing trend in the use of on-premise clusters within companies. Security, regulatory constraints, and enhanced service quality push organizations to work in these so called private cloud environments. On the other hand, the deployment of private enterprise clusters requires careful consideration of what will be necessary or may happen in the future, both in terms of compute demands and failures, as they lack the public cloud's flexibility to immediately provision new nodes in case of demand spikes or node failures.In order to better understand the challenges and tradeoffs of operating in private settings, we perform, to the best of our knowledge, the first extensive characterization of on-premise clusters. Specifically, we analyze data ranging from hardware failures to typical compute/storage requirements and workload profiles, from a large number of Nutanix clusters deployed at various companies.We show that private cloud hardware failure rates are lower, and that load/demand needs are more predictable than in other settings. Finally, we demonstrate the value of the measurements by using them to provide an analytical model for computing durability in private clouds, as well as a machine learning-driven approach for characterizing private clouds' growth.},
booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
pages = {29–41},
numpages = {13},
keywords = {Reliability, Performance, Private clouds, Measurements},
location = {Santa Clara, CA, USA},
series = {SoCC '16}
}

@inproceedings{10.1145/3293320.3293326,
author = {Kaliszan, Damian and F\"{u}rst, Steffen and Gienger, Michael and Gogolenko, Sergiy and Meyer, Norbert and Petruczynik, Sebastian},
title = {Comparative Benchmarking of HPC Systems for GSS Applications: GSS Applications in the HPC Ecosystem},
year = {2019},
isbn = {9781450366328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293320.3293326},
doi = {10.1145/3293320.3293326},
abstract = {The work undertaken in this paper was done in the Centre of Excellence for Global Systems Science (CoeGSS), an interdisciplinary project, funded by the European Commission.The project provides decision-support in the face of global challenges. It brings together HPC and global systems science. This paper presents a proposition of GSS benchmark with the aim to find the most suitable HPC architecture and the best HPC system which allows to run GSS applications effectively. The GSS provides evidence about global systems challenges, e.g. the network structure of the world economy, energy, water and food supply systems, the global financial system or the global city system, and the scientific community.The outcome of the analysis is defining a benchmark which represents the GSS environment in the best way. Three exemplary challenges were defined as pilot applications: Health Habits, Green Growth and Global Urbanisation extended with additional applications from GSS ecosystem: Iterative proportional fitting (IPF), Data rastering - a preprocessing process converting all vectorial representations of georeferenced data into raster files to be later used as simulation input, Weather Research and Forecasting (WRF) model, CMAQ/CCTM (Community Air Multiscale Quality Modelling System/The CMAQ Chemistry-Transport Mode), CM1 (Cloud Modelling), ABMS (Agent-based Modelling and Simulation), OpenSWPC (An Open-source Seismic Wave Propagation Code). The above list seems to be quite rich and reflects the real GSS world as much as possible, having in mind, for example the real-world applications availability.Additionally, the authors tested new HPC platforms based on Intel® Xeon® Gold 6140, AMD EpycTM, ARM Hi1616 and IBM Power8+. Due to the hardware availability, the testbed consisted of a limited number of nodes. This restricted the ability to provide full tests of scalability for given applications. However, this small number of available computational units (cores) can provide valuable outcome including architecture comparison for different applications based on execution times, TDPs1 and TCO2. These are the basic metrics used for providing a ranking of HPC architectures. Finally, this document is thought to be valuable information for the GSS community for future purposes and analysis to determine their specific demands as well as - in general - to help develop a mature final benchmark set reflecting the GSS environment requirements and specialty. As none of the existing benchmarks is dedicated to the GSS community, the authors decided to create one by calling it a GSS benchmark to serve and help GSS users in their future work.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {43–52},
numpages = {10},
keywords = {e-Infrastructure evaluation, Global Systems Science, HPC benchmarks, parallel applications},
location = {<conf-loc>, <city>Guangzhou</city>, <country>China</country>, </conf-loc>},
series = {HPCAsia '19}
}

@inproceedings{10.1109/UCC.2014.168,
author = {Almanea, Mohammed Ibrahim M.},
title = {Cloud Advisor - A Framework towards Assessing the Trustworthiness and Transparency of Cloud Providers},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.168},
doi = {10.1109/UCC.2014.168},
abstract = {We propose a Cloud Advisor framework that couples two salient features: trustworthiness and transparency measurement. It provides a mechanism to measure trustworthiness based on the history of the cloud provider taking into account evidence support and to measure transparency based on the Cloud Controls Matrix (CCM) framework. The selection process is based on a set of assurance requirements that if are met by the cloud provider or if it has been considered in a tool it could bring assurance and confidence to cloud customers.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {1018–1019},
numpages = {2},
keywords = {framework, cloud computing, trustworthiness, measurement, cloud providers, transparency, assurance requirements},
series = {UCC '14}
}

@inproceedings{10.1145/2594449.2579468,
author = {Huang, Chun-Ying and Hsu, Cheng-Hsin and Chen, De-Yu and Chen, Kuan-Ta},
title = {Quantifying User Satisfaction in Mobile Cloud Games},
year = {2014},
isbn = {9781450327077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594449.2579468},
doi = {10.1145/2594449.2579468},
abstract = {We conduct real experiments to quantify user satisfaction in mobile cloud games using a real cloud gaming system built on the open-sourced GamingAnywhere. We share our experiences in porting GamingAnywhere client to Android OS and perform extensive experiments on both the mobile and desktop clients. The experiment results reveal several new insights: (1) gamers are more satisfied with the graphics quality on mobile devices, while they are more satisfied with the control quality on desktops, (2) the bitrate, frame rate, and network delay significantly affect the graphics and smoothness quality, and (3) the control quality only depends on the client type (mobile versus desktop). To the best of our knowledge, such user studies have never been done in the literature.},
booktitle = {Proceedings of Workshop on Mobile Video Delivery},
articleno = {4},
numpages = {6},
keywords = {Cloud games, user studies, performance evaluation, mobile games},
location = {Singapore, Singapore},
series = {MoViD'14}
}

@inproceedings{10.1145/2579465.2579468,
author = {Huang, Chun-Ying and Hsu, Cheng-Hsin and Chen, De-Yu and Chen, Kuan-Ta},
title = {Quantifying User Satisfaction in Mobile Cloud Games},
year = {2018},
isbn = {9781450327077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2579465.2579468},
doi = {10.1145/2579465.2579468},
abstract = {We conduct real experiments to quantify user satisfaction in mobile cloud games using a real cloud gaming system built on the open-sourced GamingAnywhere. We share our experiences in porting GamingAnywhere client to Android OS and perform extensive experiments on both the mobile and desktop clients. The experiment results reveal several new insights: (1) gamers are more satisfied with the graphics quality on mobile devices, while they are more satisfied with the control quality on desktops, (2) the bitrate, frame rate, and network delay significantly affect the graphics and smoothness quality, and (3) the control quality only depends on the client type (mobile versus desktop). To the best of our knowledge, such user studies have never been done in the literature.},
booktitle = {Proceedings of Workshop on Mobile Video Delivery},
pages = {1–6},
numpages = {6},
keywords = {mobile games, performance evaluation, user studies, Cloud games},
location = {Singapore, Singapore},
series = {MoViD'14}
}

@inproceedings{10.1145/3462203.3475873,
author = {Agossou, B. Emmanuel and Toshiro, Takahara},
title = {IoT \&amp; AI Based System for Fish Farming: Case Study of Benin},
year = {2021},
isbn = {9781450384780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462203.3475873},
doi = {10.1145/3462203.3475873},
abstract = {Agriculture including aquaculture has been changing through multiple technological transformations in recent years. The Internet of Things (IoT) and Artificial Intelligence (AI) are providing remarkable technological innovations on fish farming. In this research, we present an automated IoT and AI-based system to improve fish farming. The proposed system uses multiple sensors to measure in real-time water quality chemical parameters such as: temperature, pH, turbidity, electrical conductivity, total dissolved solids, etc., from the fish pond and send them on a cloud database to allow fish farmers to access them in realtime with their devices (mobile phone, PC, tablets). The system contains three web applications which fish farmers can use. The first web application enables farmers with realtime visualizations of sensors data, issues alerts and remote pumps controls. Fish farmers can use the second web application for fish disease detection and to receive suggestions for diseases' care. This would help to classify two fish diseases which are: Epizootic Ulcerative Syndrome(EUS), and Ichthyophthirus(Ich). The third web application is a digital community platform for knowledge sharing, capacity building, market opportunities and collaboration among fish farmers. Our system can help reduce human efforts, reinforce capacity building, increase fish production and market opportunities for fish farmers.},
booktitle = {Proceedings of the Conference on Information Technology for Social Good},
pages = {259–264},
numpages = {6},
keywords = {eFish Farm, Convolutional Neural Network, IoT, Smart Fish Farming, Arduino, ESP32, MQTT, AI},
location = {Roma, Italy},
series = {GoodIT '21}
}

@article{10.1109/TCBB.2016.2566617,
author = {Zhang, Gui-Jun and Zhou, Xiao-Gen and Yu, Xu-Feng and Hao, Xiao-Hu and Yu, Li},
title = {Enhancing Protein Conformational Space Sampling Using Distance Profile-Guided Differential Evolution},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2566617},
doi = {10.1109/TCBB.2016.2566617},
abstract = {De novo protein structure prediction aims to search for low-energy conformations as it follows the thermodynamics hypothesis that places native conformations at the global minimum of the protein energy surface. However, the native conformation is not necessarily located in the lowest-energy regions owing to the inaccuracies of the energy model. This study presents a differential evolution algorithm using distance profile-based selection strategy to sample conformations with reasonable structure effectively. In the proposed algorithm, besides energy, the residue-residue distance is considered another measure of the conformation. The average distance errors of decoys between the distance of each residue pair and the corresponding distance in the distance profiles are first calculated when the trial conformation yields a larger energy value than that of the target. Then, the distance acceptance probability of the trial conformation is designed based on distance profiles if the trial conformation obtains a lower average distance error compared with that of the target conformation. The trial conformation is accepted to the next generation in accordance with its distance acceptance probability. By using the dual constraints of energy and distance in guiding sampling, the algorithm can sample conformations with lower energies and more reasonable structures. Experimental results of 28 benchmark proteins show that the proposed algorithm can effectively predict near-native protein structures.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {nov},
pages = {1288–1301},
numpages = {14}
}

@article{10.14778/2994509.2994527,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {On Measuring the Lattice of Commonalities among Several Linked Datasets},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994527},
doi = {10.14778/2994509.2994527},
abstract = {A big number of datasets has been published according to the principles of Linked Data and this number keeps increasing. Although the ultimate objective is linking and integration, it is not currently evident how connected the current LOD cloud is. Measurements (and indexes) that involve more than two datasets are not available although they are important: (a) for obtaining complete information about one particular URI (or set of URIs) with provenance (b) for aiding dataset discovery and selection, (c) for assessing the connectivity between any set of datasets for quality checking and for monitoring their evolution over time, (d) for constructing visualizations that provide more informative overviews. Since it would be prohibitively expensive to perform all these measurements in a na\"{\i}ve way, in this paper we introduce indexes (and their construction algorithms) that can speedup such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and finally (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. We discuss the speedup obtained by the introduced indexes and algorithms through comparative results and finally we report measurements about connectivity of the LOD cloud that have never been carried out so far.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1101–1112},
numpages = {12}
}

@inproceedings{10.1145/2980258.2980451,
author = {Bhattacharya, Adrija and Choudhury, Sankhayan},
title = {An Efficient Service Selection Approach through a Goodness Measure of the Participating QoS},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2980451},
doi = {10.1145/2980258.2980451},
abstract = {The service repository in cloud consists of atomic services those need to be composed as per the requirement of consumers. In general, various providers offer different atomic services with same functionalities. These are called similar services and the service selection is the process to choose the best one among them based on the associated Quality of Services. Thus a service selection problem for satisfying the requirement of a consumer with given constraints is conceptualized as a multi-objective optimization problem. Sometime it involves the objectives that have conflict among them and as a result the complexity of the problem increases. In such cases users are requested to provide the feedback on the required QoS and accordingly the solution is offered. This demands sufficient domain knowledge from a user that may not be feasible in real cases. As a result the offered solution may deviate from the intended one. In this work we have proposed a method to calculate an overall measure of a service considering all QoS. It converts the multi-objective problem to single objective. This reduces the exponential complexity of NP-Hard problem into a problem solvable in polynomial time. The proposed Service Selection algorithm does not require any feedback from the users. The algorithm is capable to offer a moderate solution to users considering all requested QoS. The experiment shows that almost in every case the proposed algorithm is able to deliver a solution satisfying all QoS as referred by a user.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {94},
numpages = {6},
keywords = {Service Selection, Goodness, QoS},
location = {Pondicherry, India},
series = {ICIA-16}
}

@inproceedings{10.1145/2683405.2683409,
author = {Nguyen, Hoang Minh and W\"{u}nsche, Burkhard and Delmas, Patrice and Lutteroth, Christof},
title = {Identifying Low Confidence Mesh Regions: Uncertainty Measures and Segmentation},
year = {2014},
isbn = {9781450331845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2683405.2683409},
doi = {10.1145/2683405.2683409},
abstract = {3D digital models have become an important part of diverse applications ranging from computer games, virtual reality, architectural design to visual impact studies. One common method to create 3D models is to create a point cloud using laser scanners, structured lighting sensors, or image-based modelling techniques, and then construct a 3D mesh, and texture-map it using photographs of the observed scene. Attributed to the inherent properties of general 3D scenes such as occluded or inaccessible parts, reflective surfaces, lighting conditions or poor-quality inputs, 3D models produced by these approaches often exhibit unsatisfactory and erroneous mesh regions. In many cases, it is desirable to identify and extract such regions so that they can be constructed or corrected through other means. While much effort has been invested into the problem of 3D reconstructions, the task of evaluating existing models and preparing them for subsequent enhancement processes has been largely neglected. In this paper, we present a novel method for automatically detecting and segmenting mesh regions with low confidence in their correctness. The confidence estimation is achieved by exploiting and integrating various uncertainty measures such as geometric distances, normal variations and texture discrepancies. Low-confidence mesh regions are isolated and removed in such a way that the extracted region's boundary is as simple as possible in order to facilitate subsequent automatic or manual improvement of these regions. Segmentation is achieved by minimising an energy function that takes the genus and boundary length and smoothness of the extracted regions into account.},
booktitle = {Proceedings of the 29th International Conference on Image and Vision Computing New Zealand},
pages = {48–53},
numpages = {6},
keywords = {Uncertainty Measure, 3D Reconstruction, Mesh Classification},
location = {Hamilton, New Zealand},
series = {IVCNZ '14}
}

@article{10.1145/3165713,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3165713},
doi = {10.1145/3165713},
abstract = {Although the ultimate objective of Linked Data is linking and integration, it is not currently evident how connected the current Linked Open Data (LOD) cloud is. In this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference, i.e., for obtaining complete information about a set of entities, including provenance information; (c) Data Quality Assessment and Improvement, i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d) Dataset Visualizations; and various other tasks. Since it would be prohibitively expensive to perform all these measurements in a na\"{\i}ve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. For enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. Finally, we report measurements about the connectivity of the (billion triples-sized) LOD cloud that have never been carried out so far.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {15},
numpages = {49},
keywords = {connectivity, dataset discovery, Data quality, spark, big data, linked data, dataset selection, mapreduce, lattice of measurements}
}

@article{10.1145/3089262.3089268,
author = {Hossfeld, Tobias},
title = {2016 International Teletraffic Congress (ITC 28) Report},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/3089262.3089268},
doi = {10.1145/3089262.3089268},
abstract = {The 28th International Teletraffic Congress (ITC 28) was held on 12--16 September 2016 at the University of W"urzburg, Germany. The conference was technically cosponsored by the IEEE Communications Society and the Information Technology Society within VDE, and in cooperation with ACM SIGCOMM. ITC 28 provided a forum for leading researchers from academia and industry to present and discuss the latest advances and developments in design, modelling, measurement, and performance evaluation of communication systems, networks, and services. The main theme of ITC 28, emph{Digital Connected World}, reflects the evolution of communications and networking, which is continually changing the world we are living in. The technical program was composed of 37 contributed full papers, 6 short demo papers and three keynote addresses. Three workshops dedicated to timely topics were sponsored: Programmability for Cloud Networks and Applications, Quality of Experience Centric Management, Quality Engineering for a Reliable Internet of Services.See ITC 28 Homepage: url{https://itc28.org/}},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {30–35},
numpages = {6},
keywords = {Clouds and Data Center, Information Centric Networks, Video Streaming, Softwarization, Virtualization, Traffic and Network Management, Wireless and Cellular, Caching, Measurements, Performance Analysis and Modeling}
}

@inproceedings{10.1145/2695664.2695814,
author = {Kunde, Shruti and Mukherjee, Tridib},
title = {Workload Characterization Model for Optimal Resource Allocation in Cloud Middleware},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695814},
doi = {10.1145/2695664.2695814},
abstract = {With increasing focus on inter-operability across cloud offerings to leverage their disparate capabilities, it has become more and more important to enable a flexible framework for sharing of heterogeneous resources in the cloud infrastructure. At the same time, it is imperative to be aware of the performance implications of hosting application workloads on different resources in order to guarantee Service Level Agreements (SLAs) to the applications. This paper focusses on experimental characterization of performance implications of different heterogeneous resources in hosting big-data analytics application workloads (one of the most critical applications in modern times). To create the knowledge, based on which the recommendations are provided, we benchmark the performance of big-data analytics applications, using a Hadoop cluster setup. Specifically, we study parameters of interest such as turnaround time and throughput, which are most likely to influence our choice of infrastructure for a particular application. Our experiments are conducted on varied platforms, both internal to Xerox and external cloud providers. We present a model based on our experiments, that facilitates the characterization of hetergeneous applications, thus enabling the cloud middleware to select an appropriate infrastructure and metrics in order to attain the desired SLA.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {442–447},
numpages = {6},
keywords = {resource sharing, cloud middleware},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3383812.3383823,
author = {Vani, K. Suvarna and M., Arul Raj and M., Padmaja and Kumar, K. Praveen and A., Jitendra and A., Ravi Raja},
title = {Detection and Extraction of Roads Using Cartosat-2 High Resolution Satellite Imagery},
year = {2020},
isbn = {9781450377201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383812.3383823},
doi = {10.1145/3383812.3383823},
abstract = {The extraction of roads from panchromatic images has many insightful applications in the fields of urban planning, setting up of transportation, disaster management and cartography in geographical information systems (GIS). The process of extracting roads from high-resolution images is composite, due to the presence of different noises (i.e., buildings, shadows, clouds etc.). Various image processing techniques and various quality measures are applied on the high-resolution remote sensing satellite images to improve the quality of the image and interactively extract the information of roads. Cartosat-2 images available in Bhuvan website of ISRO are taken for testing the validity of proposed method. The proposed method enhances the images using Contrast Limited Adaptive Histogram Equalization (CLAHE) and Line Detector for detecting road segments. Connected component analysis (CCA) is performed on segmented image for connection of disconnected objects in the segmented image. Morphological operations fill the holes caused by the presence of shadows, buildings and trees on the road surface.},
booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
pages = {7–11},
numpages = {5},
keywords = {morphological operations, line segment detector, cartosat-2 dataset, image processing, GIS},
location = {Singapore, Singapore},
series = {ICIGP '20}
}

@inproceedings{10.1109/CCGRID.2017.12,
author = {Davatz, Christian and Inzinger, Christian and Scheuner, Joel and Leitner, Philipp},
title = {An Approach and Case Study of Cloud Instance Type Selection for Multi-Tier Web Applications},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.12},
doi = {10.1109/CCGRID.2017.12},
abstract = {A challenging problem for users of Infrastructure-as-a-Service (IaaS) clouds is selecting cloud providers, regions, and instance types cost-optimally for a given desired service level. Issues such as hardware heterogeneity, contention, and virtual machine (VM) placement can result in considerably differing performance across supposedly equivalent cloud resources. Existing research on cloud benchmarking helps, but often the focus is on providing low-level microbenchmarks (e.g., CPU or network speed), which are hard to map to concrete business metrics of enterprise cloud applications, such as request throughput of a multi-tier Web application. In this paper, we propose Okta, a general approach for fairly and comprehensively benchmarking the performance and cost of a multi-tier Web application hosted in an IaaS cloud. We exemplify our approach for a case study based on the two-tier AcmeAir application, which we evaluate for 11 real-life deployment configurations on Amazon EC2 and Google Compute Engine. Our results show that for this application, choosing compute-optimized instance types in the Web layer and small bursting instances for the database tier leads to the overall most cost-effective deployments. This result held true for both cloud providers. The least cost-effective configuration in our study provides only about 67\% of throughput per US dollar spent. Our case study can serve as a blueprint for future industrial or academic application benchmarking projects.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {534–543},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/2642687.2642704,
author = {Palomares, Daniel and Migault, Daniel and Hendrik, Hendrik and Laurent, Maryline and Pujolle, Guy},
title = {Elastic Virtual Private Cloud},
year = {2014},
isbn = {9781450330275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642687.2642704},
doi = {10.1145/2642687.2642704},
abstract = {Several Virtual Private Networks are based on IPsec. However, IPsec has not been designed with elasticity in mind, which makes clusters of IPsec security gateways hard to manage for providing high Service Level Agreement (SLA). Thus, these SG clusters need management techniques to maintain their Quality of Service. For example, ISPs use VPNs to secure millions of communications when offloading End-Users from Radio Access Networks towards alternative access networks such as WLANs. Additionally, Virtual Private Cloud (VPC) providers also handle thousands of VPN connections when remote EUs access private clouds services. This paper describes how to provide Traffic Management (TM) and High Availability (HA) for VPN infrastructures by sharing or transferring an IPsec session. TM and HA have been implemented and evaluated over a 2-nodes cluster. We measured their impact on a real time audio streaming simulating a phone conversation. We found out that over a 2 minutes conversation, the impact on QoS measured with POLQA while applying TM or HA, is less than 3\%.},
booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {127–131},
numpages = {5},
keywords = {IKEV2, context transfer, high availability, QoS, virtual private cloud, VPN management, IPSEC, POLQA},
location = {Montreal, QC, Canada},
series = {Q2SWinet '14}
}

@inproceedings{10.1145/3448891.3448918,
author = {Du, Yifan and Sailhan, Fran\c{c}oise and Issarny, Val\'{e}rie},
title = {IAM&nbsp;– Interpolation and Aggregation on the Move: Collaborative Crowdsensing for Spatio-Temporal Phenomena},
year = {2021},
isbn = {9781450388405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448891.3448918},
doi = {10.1145/3448891.3448918},
abstract = {Crowdsensing allows citizens to contribute to the monitoring of their living environment using the sensors embedded in their mobile devices, e.g., smartphones. However, crowdsensing at scale involves significant communication, computation, and financial costs due to the dependence on cloud infrastructures for the analysis (e.g., interpolation and aggregation) of spatio-temporal data. This limits the adoption of crowdsensing by activists although sorely needed to inform our knowledge of the environment. As an alternative to the centralized analysis of crowdsensed observations, this paper introduces a fully distributed interpolation-mediated aggregation approach running on smartphones. To achieve so efficiently, we model the interpolation as a distributed tensor completion problem, and we introduce a lightweight aggregation strategy that anticipates the likelihood of future encounters according to the quality of the interpolation. Our approach thus shifts the centralized post-processing of crowdsensed data to distributed pre-processing on the move, based on opportunistic encounters of crowdsensors through state-of-the-art D2D networking. The evaluation using a dataset of quantitative environmental measurements collected from 550 crowdsensors over 1 year shows that our solution significantly reduces –and may even eliminate– the dependence on the cloud infrastructure, while it incurs a limited resource cost on end devices. Meanwhile, the overall data accuracy remains comparable to that of the centralized approach.},
booktitle = {MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {337–346},
numpages = {10},
keywords = {Opportunistic Relay, Aggregation, Ubiquitous Sensing, Interpolation, Pervasive Computing, Crowdsensing},
location = {Darmstadt, Germany},
series = {MobiQuitous '20}
}

@inproceedings{10.1145/2729094.2742618,
author = {Pal, Yogendra and Iyer, Sridhar},
title = {Classroom Versus Screencast for Native Language Learners: Effect of Medium of Instruction on Knowledge of Programming},
year = {2015},
isbn = {9781450334402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729094.2742618},
doi = {10.1145/2729094.2742618},
abstract = {Students, who study in their native language in K-12 and go on to do their undergraduate education in English, have difficulty in acquiring programming knowledge. Solutions targeted towards improving their English proficiency take time, while those that continue with native language in the classroom limit the students' ability to compete in a global market. Another solution could be the use of video-based instructional material to empower a student for self-paced learning. In this paper, we present a comparative study of classroom instruction versus self-paced screencasts for native language learners' acquisition of programming concepts. We conducted four introductory programming workshops, each of six days duration. Two workshops were classroom based, one having Hindi (native language) as the medium of instruction and other in English. Two other workshops were screencast based, again one in Hindi and one in English. We measured differences between the groups using a post-test, across different content types such as fact, concepts and process. We found that when medium of instruction is different from language of K-12 instruction, there is an adverse impact on learning. However, when self-paced screencast is used instead of classroom environment, there is a statistically significant improvement in performance. Our work informs the choice of MoI and choice of environment for native language learners.},
booktitle = {Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {290–295},
numpages = {6},
keywords = {computer programming education, native language instruction, screencast},
location = {Vilnius, Lithuania},
series = {ITiCSE '15}
}

@inproceedings{10.1145/2851553.2851554,
author = {Matsuki, Tatsuma and Matsuoka, Naoki},
title = {A Resource Contention Analysis Framework for Diagnosis of Application Performance Anomalies in Consolidated Cloud Environments},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2851554},
doi = {10.1145/2851553.2851554},
abstract = {Cloud services have made large contributions to the agile developments and rapid revisions of various applications. However, the performance of these applications is still one of the largest concerns for developers. Although it has created many performance analysis frameworks, most of them have not been efficient for the rapid application revisions because they have required performance models, which may have had to be remodeled whenever application revisions occurred. We propose an analysis framework for diagnosis of application performance anomalies. We designed our framework so that it did not require any performance models to be efficient in rapid application revisions. That investigates the Pearson correlation and association rules between system metrics and application performance. The association rules are widely used in data-mining areas to find relations between variables in databases. We demonstrated through an experiment and testing on a real data set that our framework could select causal metrics even when the metrics were temporally correlated, which reduced the false negatives obtained from cause diagnosis. We evaluated our framework from the perspective of the expected remaining diagnostic costs of framework users. The results indicated that it was expected to reduce the diagnostic costs by 84.8\% at most, compared with a method that only used the Pearson correlation.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {173–184},
numpages = {12},
keywords = {performance diagnosis, association rule, cloud computing, correlation analysis},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1145/2737095.2742919,
author = {Nasser, Soliman and Barry, Andew and Doniec, Marek and Peled, Guy and Rosman, Guy and Rus, Daniela and Volkov, Mikhail and Feldman, Dan},
title = {Fleye on the Car: Big Data Meets the Internet of Things},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2742919},
doi = {10.1145/2737095.2742919},
abstract = {Vehicle-based vision algorithms, such as the collision alert systems [4], are able to interpret a scene in real-time and provide drivers with immediate feedback. However, such technologies are based on cameras on the car, limited to the vicinity of the car, severely limiting their potential. They cannot find empty parking slots, bypass traffic jams, or warn about dangers outside the car's immediate surrounding. An intelligent driving system augmented with additional sensors and network inputs may significantly reduce the number of accidents, improve traffic congestion, and care for the safety and quality of people's lives.We propose an open-code system, called Fleye, that consists of an autonomous drone (nano quadrotor) that carries a radio camera and flies few meters in front and above the car. The streaming video is transmitted in real time from the quadcopter to Amazon's EC2 cloud together with information about the driver, the drone, and the car's state. The output is then transmitted to the "smart glasses" of the driver. The control of the drone, as well as the sensor data collection from the driver, is done by low cost (&lt;30$) minicomputer. Most computation is done in the cloud, allowing straightforward integration of multiple vehicle behaviour and additional sensors, as well as greater computational capability.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {382–383},
numpages = {2},
keywords = {video streaming, quadrotors, internet of things, collision alert system},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/3357223.3365441,
author = {Liu, Yang and Xu, Huanle and Lau, Wing Cheong},
title = {Accordia: Adaptive Cloud Configuration Optimization for Recurring Data-Intensive Applications},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3365441},
doi = {10.1145/3357223.3365441},
abstract = {Recognizing the diversity of big data analytic jobs, cloud providers offer a wide range of virtual machine (VM) instances for different use cases. The choice of cloud instance configurations can have significant impact on the response time and running cost of data-intensive, recurring jobs for production. A poor choice of cloud instance-type/configuration can substantially degrade the response time by 5x, or increase the cost by 10x. Identifying the best cloud configuration under low search budget is a challenging problem due to i) the large and high-dimensional configuration-parameters space, ii) the dynamically varying price of some instance types, iii) job response time variation even given the same configuration, and iv) gradual drifts/ unexpected changes of the characteristics of the recurring jobs. To tackle this problem, we have designed and implemented Accordia, a system which enables Adaptive Cloud Configuration Optimization for Recurring Data-Intensive Applications.Accordia extends the Gaussian-Process Upper Confidence Bound (GP-UCB) approach in [3] to search for and track the potentially dynamic optimal cloud configuration within a high-dimensional para-meter-space. Unlike other state-of-the-art schemes, such as CherryPick[1] and Arrow[2], Accordia can handle time-varying instance pricing while providing a performance guarantee of sub-linear regret when comparing with the static, offline optimial solution.Figure 1 depicts the system architecture of our implementation of Accordia for Apache Spark running over Kubernetes. When a job is submitted, a Spark driver and multiple Spark executors are deployed as containers, each within its own Kubernetes pod. Accordia then dynamically adjusts the resource types/ allocation for the containers within their respective pods to minimize the job completion cost using the GP-UCB online-learning approach.To evaluate the performance of Accordia, we have run different mixes of recurring Spark jobs over the Google public cloud. In our experiments, Accordia dynamically learns the best cloud configuration from over 7000 candidate choices within a 5-dimensional parameter space, covering the number of executors, as well as the number of CPU cores and memory (RAM) allocation for the driver and the executor pods. Empirical measurements show that Accordia can find a near-cost-optimal configuration for a recurring job (i.e. within 10\% of the optimal cost) with fewer than 20 runs, which translates to a 2X-speedup and a 20.9\% cost-savings, when comparing to CherryPick. To highlight Accordia's capability to handle abrupt/unexpected changes of the characteristics of a recurring job, we even dynamically switch the type of a recurring job (without notifying Accordia) over exponentially-distributed time-intervals. Under such cases, Accordia can still achieve on average a cost-savings of 18.4\% over CherryPick. The full technical report is available at http://mobitec.ie.cuhk.edu.hk/cloudComputing/Accordia.pdf.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {479},
numpages = {1},
keywords = {Cloud configuration, Big data analytics, Kubernetes, Gaussian-Process UCB},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.5555/3233397.3233412,
author = {Perez-Palacin, Diego and Mirandola, Raffaela and Monterisi, Federico and Montoli, Andrea},
title = {QoS-Driven Probabilistic Runtime Evaluations of Virtual Machine Placement on Hosts},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {We tackle the cloud providers challenge of virtual machine placement when the client experienced Quality of Service (QoS) is of paramount importance and resource demand of virtual machines varies over time. To this end, this work investigates approaches that leverage measured dynamic data for placement decisions. Relying on dynamic data to guide decisions has, on the one hand, the potential to optimize hardware utilization, while, on the other hand, increases the risk on the provided QoS. In this context, we present three probabilistic methods for evaluation of host suitability to allocate new virtual machines. We also present experiments results that illustrate the differences in the outcomes of presented approaches.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {90–94},
numpages = {5},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1145/2668975.2669018,
author = {Ganihar, Syed Altaf and Joshi, Shreyas and Setty, Shankar and Mudenagudi, Uma},
title = {3D Object Decomposition and Super Resolution},
year = {2014},
isbn = {9781450327923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668975.2669018},
doi = {10.1145/2668975.2669018},
abstract = {In this paper we propose to address the problem of 3D object decomposition and super resolution. We model the 3D object as a set of Riemannian manifolds and propose metric tensor and Christoffel symbols as a novel set of features for 3D object decomposition using polynomial kernel SVM classifier. The super resolution of the 3D point clouds is carried out using the decomposed object by using selective interpolation techniques. The effectiveness of the proposed framework is demonstrated on 3D objects obtained from different datasets and achieve comparable results.},
booktitle = {SIGGRAPH Asia 2014 Posters},
articleno = {5},
numpages = {1},
location = {Shenzhen, China},
series = {SA '14}
}

@inproceedings{10.1145/3400286.3418281,
author = {Lee, Yena and An, Jae-Hoon and Kim, Younghwan},
title = {Scheduler for Distributed and Collaborative Container Clusters Based on Multi-Resource Metric},
year = {2020},
isbn = {9781450380256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400286.3418281},
doi = {10.1145/3400286.3418281},
abstract = {With the development of cloud technology, distributed and collaborative container platform technology has emerged to overcome the limitations of the existing stand-alone container platform, which has limitations in the mobility and resource scalability of cloud services. Distributed and collaborative container platform technology enables flexible expansion of resources and maximization of service mobility between container platforms distributed locally.In this paper, we propose a two-stage scheduler based on multi-resource metrics. The proposed scheduler determines the proper federated cluster where the request deployment can be deployed in a distributed and collaborative cluster environment. In order to select an proper federated cluster, filtering to select candidate clusters to which the scheduling request deployment can be deployed and scoring to evaluate the preference of each filtered cluster are performed.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {279–281},
numpages = {3},
keywords = {Distributed and Collaborative, Scheduling, Cloud Computing},
location = {Gwangju, Republic of Korea},
series = {RACS '20}
}

@inproceedings{10.1145/2976749.2978316,
author = {Shen, Yilin and Jin, Hongxia},
title = {EpicRec: Towards Practical Differentially Private Framework for Personalized Recommendation},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978316},
doi = {10.1145/2976749.2978316},
abstract = {Recommender systems typically require users' history data to provide a list of recommendations and such recommendations usually reside on the cloud/server. However, the release of such private data to the cloud has been shown to put users at risk. It is highly desirable to provide users high-quality personalized services while respecting their privacy. In this paper, we develop the first Enhanced Privacy-built-In Client for Personalized Recommendation (EpicRec) system that performs the data perturbation on the client side to protect users' privacy. Our system needs no assumption of trusted server and no change on the recommendation algorithms on the server side; and needs minimum user interaction in their preferred manner, which makes our solution fit very well into real world practical use.The design of EpicRec system incorporates three main modules: (1) usable privacy control interface that enables two user preferred privacy controls, overall and category-based controls, in the way they understand; (2) user privacy level quantification that automatically quantifies user privacy concern level from these user understandable inputs; (3) lightweight data perturbation algorithm that perturbs user private data with provable guarantees on both differential privacy and data utility.Using large-scale real world datasets, we show that, for both overall and category-based privacy controls, EpicRec performs best with respect to both perturbation quality and personalized recommendation, with negligible computational overhead. Therefore, EpicRec enables two contradictory goals, privacy preservation and recommendation accuracy. We also implement a proof-of-concept EpicRec system to demonstrate a privacy-preserving personal computer for movie recommendation with web-based privacy controls. We believe EpicRec is an important step towards designing a practical system that enables companies to monetize on user data using high quality personalized services with strong provable privacy protection to gain user acceptance and adoption of their services.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {180–191},
numpages = {12},
keywords = {privacy paradox, privacy-preserving recommendation, differential privacy},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/3277593.3277619,
author = {Belkaroui, Rami and Bertaux, Aur\'{e}lie and Labbani, Ouassila and Hugol-Gential, Cl\'{e}mentine and Nicolle, Christophe},
title = {Towards Events Ontology Based on Data Sensors Network for Viticulture Domain},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277619},
doi = {10.1145/3277593.3277619},
abstract = {Wine Cloud project is the first "Big Data" platform on the french viticulture value chain. The aim of this platform is to provide a complete traceability of the life cycle of the wine, from the wine-grower to the consumer. In particular, Wine Cloud may qualify as an agricultural decision platform that will be used for vine life cycle management in order to predict the occurrence of major risks (vine diseases, grape vine pests, physiological risks, fermentation stoppage, oxidation of vine, etc...). Also to make wine production more rational by offering winegrower a set of recommendation regarding their strategy's of production development.The proposed platform "Wine Cloud" is based on heterogeneous sensors network (agricultural machines, plant sensors and measuring stations) deployed throughout a vineyard. These sensors allow for capturing data from the agricultural process and remote monitoring vineyards in the Internet of Things (IoT) era. However, the sensors data from different source is hard to work together for lack of semantic. Therefore, the task of coherently combining heterogeneous sensors data becomes very challenging. The integration of heterogeneous data from sensors can be achieved by data mining algorithms able to build correlations. Nevertheless, the meaning and the value of these correlations is difficult to perceive without highlighting the meaning of the data and the semantic description of the measured environment.In order to bridge this gap and build causality relationships form heterogeneous sensor data, we propose an ontology-based approach, that consists in exploring heterogeneous sensor data (light, temperature, atmospheric pressure, etc) in terms of ontologies enriched with semantic meta-data describing the life cycle of the monitored environment.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {44},
numpages = {7},
keywords = {semantic sensor data, big data, smart viticulture, ontologies, event ontology, IoT},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/2896377.2901452,
author = {Zheng, Liang and Joe-Wong, Carlee and Brinton, Christopher G. and Tan, Chee Wei and Ha, Sangtae and Chiang, Mung},
title = {On the Viability of a Cloud Virtual Service Provider},
year = {2016},
isbn = {9781450342667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896377.2901452},
doi = {10.1145/2896377.2901452},
abstract = {Cloud service providers (CSPs) often face highly dynamic user demands for their resources, which can make it difficult for them to maintain consistent quality-of-service. Some CSPs try to stabilize user demands by offering sustained-use discounts to jobs that consume more instance-hours per month. These discounts present an opportunity for users to pool their usage together into a single ``job.'' In this paper, we examine the viability of a middleman, the cloud virtual service provider (CVSP), that rents cloud resources from a CSP and then resells them to users. We show that the CVSP's business model is only viable if the average job runtimes and thresholds for sustained-use discounts are sufficiently small; otherwise, the CVSP cannot simultaneously maintain low job waiting times while qualifying for a sustained-use discount. We quantify these viability conditions by modeling the CVSP's job scheduling and then use this model to derive users' utility-maximizing demands and the CVSP's profit-maximizing price, as well as the optimal number of instances that the CVSP should rent from the CSP. We verify our results on a one-month trace from Google's production compute cluster, through which we first validate our assumptions on the job arrival and runtime distributions, and then show that the CVSP is viable under these workload traces. Indeed, the CVSP can earn a positive profit without significantly impacting the CSP's revenue, indicating that the CSP and CVSP can coexist in the cloud market.},
booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
pages = {235–248},
numpages = {14},
keywords = {virtual service provider, cloud pricing, economic viability},
location = {Antibes Juan-les-Pins, France},
series = {SIGMETRICS '16}
}

@article{10.1145/2964791.2901452,
author = {Zheng, Liang and Joe-Wong, Carlee and Brinton, Christopher G. and Tan, Chee Wei and Ha, Sangtae and Chiang, Mung},
title = {On the Viability of a Cloud Virtual Service Provider},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2964791.2901452},
doi = {10.1145/2964791.2901452},
abstract = {Cloud service providers (CSPs) often face highly dynamic user demands for their resources, which can make it difficult for them to maintain consistent quality-of-service. Some CSPs try to stabilize user demands by offering sustained-use discounts to jobs that consume more instance-hours per month. These discounts present an opportunity for users to pool their usage together into a single ``job.'' In this paper, we examine the viability of a middleman, the cloud virtual service provider (CVSP), that rents cloud resources from a CSP and then resells them to users. We show that the CVSP's business model is only viable if the average job runtimes and thresholds for sustained-use discounts are sufficiently small; otherwise, the CVSP cannot simultaneously maintain low job waiting times while qualifying for a sustained-use discount. We quantify these viability conditions by modeling the CVSP's job scheduling and then use this model to derive users' utility-maximizing demands and the CVSP's profit-maximizing price, as well as the optimal number of instances that the CVSP should rent from the CSP. We verify our results on a one-month trace from Google's production compute cluster, through which we first validate our assumptions on the job arrival and runtime distributions, and then show that the CVSP is viable under these workload traces. Indeed, the CVSP can earn a positive profit without significantly impacting the CSP's revenue, indicating that the CSP and CVSP can coexist in the cloud market.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {235–248},
numpages = {14},
keywords = {cloud pricing, economic viability, virtual service provider}
}

@inproceedings{10.1145/3365245.3365247,
author = {Liu, Xiaofeng and Zou, Hui and Niu, Wanyu and Song, Yuqing and He, Wenzhang},
title = {An Approach of Traffic Accident Scene Reconstruction Using Unmanned Aerial Vehicle Photogrammetry},
year = {2019},
isbn = {9781450372435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365245.3365247},
doi = {10.1145/3365245.3365247},
abstract = {Accurate and detailed information of traffic accident scene is important for accident investigation, and the current investigation methods (tape measuring and total station survey) always need highway closure, and their working process is time-consuming. From different angles or at different altitudes, unmanned aerial vehicle (UAV) can monitor accident site without interrupting the traffic flow, therefore, UAV is introduced for accident scene reconstruction. Firstly, the method framework of accident scene reconstruction was proposed, in which UAV was used to take pictures of accident site, and imaging system was adopted to reconstruct the 2D and 3D accident scene. Then, 3D reconstruction, point cloud generation, and model optimization were presented. Next, a UAV flight experiment was conducted for traffic accident scene reconstruction, and two evaluation indexes, signal-to-noise ratio and structural similarity, were introduced to assess the image quality of accident scene reconstruction. The case study demonstrates that compared with current methods, the proposed method is efficient; moreover, the effect of accident scene reconstruction is satisfactory.},
booktitle = {Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing},
pages = {31–34},
numpages = {4},
keywords = {traffic investigation, Scene reconstruction, photogrammetry, unmanned aerial vehicle},
location = {Prague, Czech Republic},
series = {SSIP '19}
}

@inproceedings{10.1145/3316782.3322746,
author = {Haslwanter, Jean D. Hallewell and Heiml, Michael and Wolfartsberger, Josef},
title = {Lost in Translation: Machine Translation and Text-to-Speech in Industry 4.0},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3322746},
doi = {10.1145/3316782.3322746},
abstract = {Small lot sizes are becoming more common in modern manufacturing. Rather than automate every possible product variant, companies may rely on manual assembly to be more flexible. However, it can be difficult for people to remember the steps for every possible product variant. Assistive systems providing instructions can support workers. In this paper, we present a study investigating whether existing machine translation and text-to-speech engines provide sufficient quality to enable on-the-fly translations to provide assistance to workers in their native languages. The results of our tests indicate that machine translation is not yet sufficient for this application.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {333–342},
numpages = {10},
keywords = {TTS, assistive systems, MT, text to speech, manual assembly, machine translation, computer-assisted instruction},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/3307334.3326087,
author = {Shi, Shu and Gupta, Varun and Jana, Rittwik},
title = {Freedom: Fast Recovery Enhanced VR Delivery Over Mobile Networks},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326087},
doi = {10.1145/3307334.3326087},
abstract = {In this paper we design and implement Freedom, a mobile VR system that deliver high quality VR content on today's mobile devices using 4G/LTE cellular networks. Compared to existing state-of-the-art, Freedom does not rely on any video frame pre- rendering or viewpoint prediction. We send a latency-adaptive VAM frame that contains pixels around the FoV. This allows the clients to render locally at a high refresh rate of 60 Hz to accommodate and compensate for the user's head movements before the next server update arrives. We demonstrate that Freedom is the first system in the world that can support dynamic and live 8K resolution VR content, while adapting to the real-world latency variations experienced in cellular networks. Compared to streaming the whole 360° panoramic VR content, we show that Freedom achieves up to 80\% bandwidth savings. Finally, we provide detailed end to end latency measurements of actual VR systems by running extensive experiments in a private LTE testbed using a Mobile Edge Cloud (MEC).},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {130–141},
numpages = {12},
keywords = {remote rendering, mobile edge cloud, 360 video, mobile vr, motion-to-update latency},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@inproceedings{10.1145/2964284.2964327,
author = {Wu, Chao and Jia, Jia and Zhu, Wenwu and Chen, Xu and Yang, Bowen and Zhang, Yaoxue},
title = {Affective Contextual Mobile Recommender System},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2964327},
doi = {10.1145/2964284.2964327},
abstract = {Exponential growth of media consumption in online social networks demands effective recommendation to improve the quality of experience especially for on-the-go mobile users. By means of large-scale trace-driven measurements over mobile Twitter traces from users, we reveal the significance of affective features in shaping users' social media behaviors. Existing recommender systems however, rarely support this psychological effect in real-life. To capture this effect, in this paper we propose Kaleido, a real mobile system to achieve an affect-aware learning-based social media recommendation.Specifically, we design a machine learning mechanism to infer the affective feature within media contents. Furthermore, a cluster-based latent bias model is provided for jointly training the affect, behavior and social contexts. Our comprehensive experiments on Android prototype expose a superior prediction accuracy of 82\%, with more than 20\% accuracy improvement over existing mobile recommender systems. Moreover, by enabling users to offload their machine learning procedures to the deployed edge-cloud testbed, our system achieves speed-up of a factor of 1,000 against the local data training execution on smartphones.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1375–1384},
numpages = {10},
keywords = {mobile application, social networks, recommender system, affective computing},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.5555/2602339.2602357,
author = {Misra, Prasant Kumar and Hu, Wen and Jin, Yuzhe and Liu, Jie and Souza de Paula, Amanda and Wirstrom, Niklas and Voigt, Thiemo},
title = {Energy Efficient GPS Acquisition with Sparse-Gps},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {Following rising demands in positioning with GPS, low-cost receivers are becoming widely available; but their energy demands are still too high. For energy efficient GPS sensing in delay-tolerant applications, the possibility of offloading a few milliseconds of raw signal samples and leveraging the greater processing power of the cloud for obtaining a position fix is being actively investigated. In an attempt to reduce the energy cost of this data offloading operation, we propose Sparse-GPS1: a new computing framework for GPS acquisition via sparse approximation. Within the framework, GPS signals can be efficiently compressed by random ensembles. The sparse acquisition information, pertaining to the visible satellites that are embedded within these limited measurements, can subsequently be recovered by our proposed representation dictionary. By extensive empirical evaluations, we demonstrate the acquisition quality and energy gains of Sparse-GPS. We show that it is twice as energy efficient than offloading uncompressed data, and has 5-10 times lower energy costs than standalone GPS; with a median positioning accuracy of 40 m.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {155–166},
numpages = {12},
keywords = {gps, sparse approximation, synchronization, compressed sensing, location sensing, energy efficiency},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3330430.3333644,
author = {Bruechner, Dominik and Renz, Jan and Klingbeil, Mandy},
title = {Creating a Framework for User-Centered Development and Improvement of Digital Education},
year = {2019},
isbn = {9781450368049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330430.3333644},
doi = {10.1145/3330430.3333644},
abstract = {We investigate how the technology acceptance and learning experience of the digital education platform HPI Schul-Cloud (HPI School Cloud) for German secondary school teachers can be improved by proposing a user-centered research and development framework. We highlight the importance of developing digital learning technologies in a user-centered way to take differences in the requirements of educators and students into account. We suggest applying qualitative and quantitative methods to build a solid understanding of a learning platform's users, their needs, requirements, and their context of use. After concept development and idea generation of features and areas of opportunity based on the user research, we emphasize on the application of a multi-attribute utility analysis decision-making framework to prioritize ideas rationally, taking results of user research into account. Afterward, we recommend applying the principle build-learn-iterate to build prototypes in different resolutions while learning from user tests and improving the selected opportunities. Last but not least, we propose an approach for continuous short- and long-term user experience controlling and monitoring, extending existing web- and learning analytics metrics.},
booktitle = {Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale},
articleno = {31},
numpages = {4},
keywords = {HPI Schul-Cloud, user-centered design, learning platform, user research framework, evaluation, user experience},
location = {Chicago, IL, USA},
series = {L@S '19}
}

@inproceedings{10.1145/3240508.3240620,
author = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Eerik\"{a}inen, Jukka and Yl\"{a}-J\"{a}\"{a}ski, Antti},
title = {CloudVR: Cloud Accelerated Interactive Mobile Virtual Reality},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240620},
doi = {10.1145/3240508.3240620},
abstract = {High quality immersive Virtual Reality experience currently requires a PC setup with cable connected head mounted display, which is expensive and restricts user mobility. This paper presents CloudVR which is a system for cloud accelerated interactive mobile VR. It is designed to provide short rotation and interaction latencies through panoramic rendering and dynamic object placement. CloudVR also includes rendering optimizations to reduce server-side computational load and bandwidth requirements between the server and client. Performance measurements with a CloudVR prototype suggest that the optimizations make it possible to double the server's framerate and halve the amount of bandwidth required and that small objects can be quickly moved at run time to client device for rendering to provide shorter interaction latency. A small-scale user study indicates that CloudVR users do not notice small network latencies (20ms) and even much longer ones (100-200ms) become non-trivial to detect when they do not affect the interaction with objects. Finally, we present a design of CloudVR extension to multi-user scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1181–1189},
numpages = {9},
keywords = {optimization, rendering, unity, virtual reality, cloud, edge computing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/2639108.2639118,
author = {Li, Liqun and Shen, Guobin and Zhao, Chunshui and Moscibroda, Thomas and Lin, Jyh-Han and Zhao, Feng},
title = {Experiencing and Handling the Diversity in Data Density and Environmental Locality in an Indoor Positioning Service},
year = {2014},
isbn = {9781450327831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639108.2639118},
doi = {10.1145/2639108.2639118},
abstract = {Diversity in training data density and environment locality is intrinsic in the real-world deployment of indoor localization systems and has a major impact on the performance of existing localization approaches. In this paper, through micro-benchmarks, we find that fingerprint-based approaches are preferable in scenarios where a dense database is available; while model-based approaches are the method of choice in the case of sparse data. It should be noted, however, that practical situations are complex. A single deployment often features both sparse and dense sampled areas. Furthermore, the internal layout affects the propagation of radio signals and exhibits environmental impacts. A certain number of measurement samples may be sufficient for one part of the building, but entirely insufficient for another. Thus, finding the right indoor localization algorithm for a given large-scale deployment is challenging, if not impossible; there is no one-size-fits-all indoor localization approach.Realizing the fundamental fact that the quality of the location database capturing the actual radio map dictates localization accuracy, in this paper, we propose Modellet, an algorithmic approach that optimally approximates the actual radio map by unifying model-based and fingerprint-based approaches. Modellet represents the radio map using a fingerprint-cloud that incorporates both measured real fingerprints and virtual fingerprints, which are computed from models with a local support, based on the key concept of the supporting set. We evaluate Modellet with data collected from an office building as well as 13 large-scale deployment venues (shopping malls and airports), located across China, U.S., and Germany. Comparing Modellet with two representative baseline approaches, RADAR and EZPerfect, demonstrates that Modellet effectively adapts to different data densities and environmental conditions, substantially outperforming existing approaches.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Computing and Networking},
pages = {459–470},
numpages = {12},
keywords = {model, fingerprint, indoor localization},
location = {Maui, Hawaii, USA},
series = {MobiCom '14}
}

@inproceedings{10.1145/3036290.3036312,
author = {Jaiswal, Akshay and Mishra, R. B.},
title = {Cloud Service Selection Using TOPSIS and Fuzzy TOPSIS with AHP and ANP},
year = {2017},
isbn = {9781450348287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3036290.3036312},
doi = {10.1145/3036290.3036312},
abstract = {The growing demand and availability of cloud services have triggered the need for comparison of their features available to customers at different prices and performance. It is necessary to be said that relevant and fair comparison is still challenging due to diverse deployment options and unique features of different services.The aim of this paper is to rank cloud services based on quantified QoS (Quality of Service) attributes using Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) and fuzzy TOPSIS, and comparing them to find out which method suits more in different scenarios.A comparative study of Analytic Hierarchy Process (AHP) and Analytic Network process (ANP) is also done while extracting the weights of criteria for TOPSIS and fuzzy TOPSIS.},
booktitle = {Proceedings of the 2017 International Conference on Machine Learning and Soft Computing},
pages = {136–142},
numpages = {7},
keywords = {AHP, TOPSIS, Cloud Computing, Multi Attribute Decision Making, ANP, Fuzzy set theory},
location = {Ho Chi Minh City, Vietnam},
series = {ICMLSC '17}
}

@article{10.1145/3131778,
author = {Setty, Shankar and Mudenagudi, Uma},
title = {Region of Interest-Based 3D Inpainting of Cultural Heritage Artifacts},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3131778},
doi = {10.1145/3131778},
abstract = {In this article, we address the problem of 3D inpainting using an exemplar-based method for point clouds. 3D inpainting is a process of filling holes or missing regions in the reconstructed 3D models. Typically, inpainting methods addressed in the literature fill missing regions due to occlusions or inaccurate scanning of 3D models. However, we focus on scenarios involving naturally existing damaged models, which are partly broken or incomplete in the artifacts at cultural heritage sites. We propose an exemplar-based inpainting technique using the region of interest (ROI)-based method to inpaint the missing regions of the damaged model. The ROI of a 3D model is represented as a set of Riemannian manifolds, and metric tensor and Christoffel symbols are used as geometric features to capture the inherent geometry. We then decompose the ROI into basic shape regions, namely, spherical, conical, and cylindrical components, and identify the best-fit match for inpainting. Instead of using a single similar exemplar for inpainting, we select the most relevant best-fit region to fill the missing region from the basic shape regions library obtained from n similar exemplars. We demonstrate the performance of the proposed inpainting method on artifacts at UNESCO World Heritage site Hampi temples, India with varying complexities and sizes for both synthetically generated holes and real missing regions in 3D objects.},
journal = {J. Comput. Cult. Herit.},
month = {may},
articleno = {9},
numpages = {21},
keywords = {point cloud data, region of interest, cultural heritage artifacts, geometric features, 3D inpainting, Riemannian manifolds}
}

@inproceedings{10.1145/3204949.3208126,
author = {Taheri, Sajjad and Vedienbaum, Alexander and Nicolau, Alexandru and Hu, Ningxin and Haghighat, Mohammad R.},
title = {OpenCV.Js: Computer Vision Processing for the Open Web Platform},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3208126},
doi = {10.1145/3204949.3208126},
abstract = {The Web is the world's most ubiquitous compute platform and the foundation of digital economy. Ever since its birth in early 1990's, web capabilities have been increasing in both quantity and quality. However, in spite of all such progress, computer vision is not mainstream on the web yet. The reasons are historical and include lack of sufficient performance of JavaScript, lack of camera support in the standard web APIs, and lack of comprehensive computer-vision libraries. These problems are about to get solved, resulting in the potential of an immersive and perceptual web with transformational effects including in online shopping, education, and entertainment among others. This work aims to enable web with computer vision by bringing hundreds of OpenCV functions to the open web platform. OpenCV is the most popular computer-vision library with a comprehensive set of vision functions and a large developer community. OpenCV is implemented in C++ and up until now, it was not available in the web browsers without the help of unpopular native plugins. This work leverage OpenCV efficiency, completeness, API maturity, and its communitys collective knowledge. It is provided in a format that is easy for JavaScript engines to highly optimize and has an API that is easy for the web programmers to adopt and develop applications. In addition, OpenCV parallel implementations that target SIMD units and multiprocessors can be ported to equivalent web primitives, providing better performance for real-time and interactive use cases.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {478–483},
numpages = {6},
keywords = {multimedia, web, javascript, performance, computer vision, parallel processing},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1109/IPSN.2018.00025,
author = {Adkins, Joshua and Ghena, Branden and Jackson, Neal and Pannuto, Pat and Rohrer, Samuel and Campbell, Bradford and Dutta, Prabal},
title = {Applications on the Signpost Platform for City-Scale Sensing: Demo Abstract},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00025},
doi = {10.1109/IPSN.2018.00025},
abstract = {City-scale sensing holds the promise of enabling deeper insight into how our urban environments function. Applications such as observing air quality and measuring traffic flows can have powerful impacts, allowing city planners and citizen scientists alike to understand and improve their world. However, the path from conceiving applications to implementing them is fraught with difficulty. A successful city-scale deployment requires physical installation, power management, and communications---all challenging tasks standing between a good idea and a realized one.The Signpost platform, presented at IPSN 2018, has been created to address these challenges. Signpost enables easy deployment by relying on harvested, solar energy and wireless networking rather than their wired counterparts. To further lower the bar to deploying applications, the platform provides the key resources necessary to support its pluggable sensor modules in their distributed sensing tasks. In this demo, we present the Signpost hardware and several applications running on a deployment of Signposts on UC Berkeley's campus, including distributed, energy-adaptive traffic monitoring and fine grained weather reporting. Additionally we show the cloud infrastructure supporting the Signpost deployment, specifically the ability to push new applications and parameters down to existing sensors, with the goal of demonstrating that the existing deployment can serve as a future testbed.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {124–125},
numpages = {2},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1145/3172944.3172975,
author = {Vanderdonckt, Jean and Bouzit, Sara and Calvary, Ga\"{e}lle and Ch\^{e}ne, Denis},
title = {Cloud Menus: A Circular Adaptive Menu for Small Screens},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172975},
doi = {10.1145/3172944.3172975},
abstract = {This paper presents Cloud Menus, a split adaptive menu for small screens where the predicted menu items are arranged in a circular tag cloud with a location consistent with their corresponding position in the static menu and a font size depending on their prediction level. This layout results from a 3-step design process: (i) defining an initial design space on Bertin's 8 visual variables and 4 quality properties, (ii) identifying the most preferred layout based on agreement rate, and (iii) implementing it into Cloud Menus, a new widget for Android with circular layout. An empirical study suggests that cloud menus reduce item selection time and error rate when prediction is correct without penalizing it when prediction is incorrect, compared to two baselines: a non-adaptive static menu and an adaptive linear menu. From this study, design guidelines for cloud menus are elaborated.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {317–328},
numpages = {12},
keywords = {tag cloud, prediction window, split menu, adaptive menu},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/2742854.2742858,
author = {Han, Rui and Wang, Junwei and Ge, Fengming and Vazquez-Poletti, Jose Luis and Zhan, Jianfeng},
title = {SARP: Producing Approximate Results with Small Correctness Losses for Cloud Interactive Services},
year = {2015},
isbn = {9781450333580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742854.2742858},
doi = {10.1145/2742854.2742858},
abstract = {Despite the importance of providing fluid responsiveness to user requests for interactive services, such request processing is very resource expensive when dealing with large-scale input data. These often exceed the application owners' budget when services are deployed on a cloud, in which resources are charged in monetary terms. Providing approximate processing results is a feasible solution for such problem that trades off request correctness (quantified by output quality) for response time reduction. However, existing techniques in this area either use partial input data or skip expensive computations to produce approximate results, thus resulting in large losses in output quality on a tight resource budget. In this paper, we propose SARP, a Synopsis-based Approximate Request Processing framework to produce approximate results with small correctness losses even using small amount of resources. To achieve this, SARP conducts full computations over the statistical aggregation of the entire input data using two key ideas: (1) offline synopsis management that generates and maintains a set of synopses that represent the statistical aggregation of original input data at different approximation levels. (2) Online synopsis selection that considers both the current resource allocation and the workload status so as to select the synopsis with the maximal length that can be processed within the required response time. We demonstrate the effectiveness of our approach by testing the recommendation services in E-commerce sites using a large, real-world dataset. Using prediction accuracy as the output quality, the results demonstrate: (i) SARP achieves significant response time reduction with very small quality losses compared to the exact processing results.(ii) Using the same processing time, SARP demonstrates a considerable reduction in quality loss compared to existing approximation techniques.},
booktitle = {Proceedings of the 12th ACM International Conference on Computing Frontiers},
articleno = {22},
numpages = {8},
keywords = {synopsis, interactive service, output quality, approximate results, result correctness, cloud},
location = {Ischia, Italy},
series = {CF '15}
}

@inproceedings{10.1145/3323503.3349545,
author = {de Amorim, Irandir O. and de Melo, Jose F. V. and Balieiro, Andson M. and Santos, Bruno B. dos},
title = {An Evolutionary Approach for Video Application Energy Consumption Estimation in Mobile Devices},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3349545},
doi = {10.1145/3323503.3349545},
abstract = {In the last years, the multimedia traffic has increased significantly and the mobile devices (e.g. smart phones and tablets) have been widely used to consume this content type. Video applications demand high energy consumption of the device because they perform complex operations and deal with a large data amount. Although hardware improvements in the mobile devices have been achieved, the advances in battery technology have not kept the same pace. In this respect, the combination of video applications with the limited battery capacity of the mobile devices has challenged the academia and industry in the development of techniques for energy management and provision of quality of experience (QoE) to the user. Energy consumption estimation models may assist these techniques, as well as, the decision made process when the computational offloading from the mobile device to the cloud is considered. This paper presents an evolutionary approach based on Genetic Algorithms (GAs) and Swarm Particle Optimization (PSO) for energy consumption estimation in mobile devices running video applications. The proposal is directly applicable to different model types (linear and non-linear ones), without the linearization cost, and it is evaluated in terms of mean squared error (MSE), using energy consumption measurement data of videos with different configurations. Results show the superiority of our proposal in comparison to the literature that adopts the Ordinary Least Squares method.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {169–175},
numpages = {7},
keywords = {energy consumption model for mobile devices, video application, genetic algorithms, particle swarm optimization},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/2565585.2565607,
author = {Klugman, Noah and Rosa, Javier and Pannuto, Pat and Podolsky, Matthew and Huang, William and Dutta, Prabal},
title = {Grid Watch: Mapping Blackouts with Smart Phones},
year = {2014},
isbn = {9781450327428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2565585.2565607},
doi = {10.1145/2565585.2565607},
abstract = {The power grid is one of humanity's most significant engineering undertakings and it is essential in developed and developing nations alike. Currently, transparency into the power grid relies on utility companies and more fine-grained insight is provided by costly smart meter deployments. We claim that greater visibility into power grid conditions can be provided in an inexpensive and crowd-sourced manner independent of utility companies by leveraging existing smartphones. Our key insight is that an unmodified smartphone can detect power outages by monitoring changes to its own power state, locally verifying these outages using a variety of sensors that reduce the likelihood of false power outage reports, and corroborating actual reports with other phones through data aggregation in the cloud. The proposed approach enables a decentralized system that can scale, potentially providing researchers and concerned citizens with a powerful new tool to analyze the power grid and hold utility companies accountable for poor power quality. This paper demonstrates the viability of the basic idea, identifies a number of challenges that are specific to this application as well as ones that are common to many crowd-sourced applications, and highlights some improvements to smartphone operating systems that could better support such applications in the future.},
booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
articleno = {1},
numpages = {6},
keywords = {crowdsourcing, smartphone applications, smart grid, side channel information, power monitoring},
location = {Santa Barbara, California},
series = {HotMobile '14}
}

@inproceedings{10.1109/CCGRID.2018.00029,
author = {Chatterjee, Subarna and Morin, Christine},
title = {Experimental Study on the Performance and Resource Utilization of Data Streaming Frameworks},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00029},
doi = {10.1109/CCGRID.2018.00029},
abstract = {With the advent of the Internet of Things (IoT), data stream processing have gained increased attention due to the ever-increasing need to process heterogeneous and voluminous data streams. This work addresses the problem of selecting a correct stream processing framework for a given application to be executed within a specific physical infrastructure. For this purpose, we focus on a thorough comparative analysis of three data stream processing platforms - Apache Flink, Apache Storm, and Twitter Heron (the enhanced version of Apache Storm), that are chosen based on their potential to process both streams and batches in real-time. The goal of the work is to enlighten the cloud-clients and the cloud-providers with the knowledge of the choice of the resource-efficient and requirement-adaptive streaming platform for a given application so that they can plan during allocation or assignment of Virtual Machines for application execution. For the comparative performance analysis of the chosen platforms, we have experimented using 8-node clusters on Grid5000 experimentation testbed and have selected a wide variety of applications ranging from a conventional benchmark to sensor-based IoT application and statistical batch processing application. In addition to the various performance metrics related to the elasticity and resource usage of the platforms, this work presents a comparative study of the "green-ness" of the streaming platforms by analyzing their power consumption - one of the first attempts of its kind. The obtained results are thoroughly analyzed to illustrate the functional behavior of these platforms under different computing scenarios.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {143–152},
numpages = {10},
keywords = {Apache spark, stream processing, internet of things, Apache flink, Twitter heron},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/2716281.2836120,
author = {Kateja, Rajat and Baranasuriya, Nimantha and Navda, Vishnu and Padmanabhan, Venkata N.},
title = {DiversiFi: Robust Multi-Link Interactive Streaming},
year = {2015},
isbn = {9781450334129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2716281.2836120},
doi = {10.1145/2716281.2836120},
abstract = {Real-time, interactive streaming for applications such as audio-video conferencing (e.g., Skype) and cloud-based gaming depends critically on the network providing low latency, jitter, and packet loss, much more so than on-demand streaming (e.g., YouTube) does. However, WiFi networks pose a challenge; our analysis of data from a large VoIP provider and from our own measurements shows that the WiFi access link is a significant cause of poor streaming experience.To improve streaming quality over WiFi, we present DiversiFi, which takes advantage of the diversity of WiFi links available in the vicinity, even when the individual links are poor. Leveraging such cross-link spatial and channel diversity outperforms both traditional link selection and the temporal diversity arising from retransmissions on the same link. It also provides significant gains over and above the PHY-layer spatial diversity provided by MIMO. Our experimental evaluation shows that, for a client with two NICs, enabling replication across two WiFi links helps cut down the poor call rate (PCR) for VoIP by 2.24x.Finally, we present the design and implementation of DiversiFi, which enables it to operate with single-NIC clients, and with either minimally modified APs or unmodified APs augmented with a middlebox. Over 61 runs, where the baseline average PCR is 4.9\%, DiversiFi running with a single NIC, switching between two links, helps cut the PCR down to 0\%, while duplicating wastefully only 0.62\% of the packets and impacting competing TCP throughput by only 2.5\%. Thus, DiversiFi provides the benefit of multi-link diversity for real-time interactive streaming in a manner that is deployable and imposes little overhead, thereby ensuring coexistence with other applications.},
booktitle = {Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies},
articleno = {35},
numpages = {13},
keywords = {wi-fi, multi-path, VoIP, real-time streaming},
location = {Heidelberg, Germany},
series = {CoNEXT '15}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Software Product Line, Web System, Energy Aware, Machine Learning},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/3307681.3325409,
author = {Li, Yusen and Shan, Chuxu and Chen, Ruobing and Tang, Xueyan and Cai, Wentong and Tang, Shanjiang and Liu, Xiaoguang and Wang, Gang and Gong, Xiaoli and Zhang, Ying},
title = {GAugur: Quantifying Performance Interference of Colocated Games for Improving Resource Utilization in Cloud Gaming},
year = {2019},
isbn = {9781450366700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307681.3325409},
doi = {10.1145/3307681.3325409},
abstract = {Cloud gaming has been very popular recently, but providing satisfactory gaming experiences to players at a modest cost is still challenging. Colocating several games onto one server could improve server utilization. To enable efficient colocations while providing Quality of Service (QoS) guarantees, a precise quantification of performance interference among colocated games is required. However, achieving such precise interference prediction is very challenging for games due to the complexity introduced by the contention on many shared resources across CPU and GPU. Moreover, the distinctive properties of cloud gaming require that the prediction model should be constructed beforehand and the prediction should be made instantaneously at request arrivals, which further increases the difficulty. The existing solutions are either not applicable or not effective due to many limitations. In this paper, we present GAugur, a novel methodology that enables highly accurate prediction of the performance interference among games arbitrarily colocated. By leveraging machine learning technologies, GAugur is able to capture the complex relationship between the interference and the contention features of colocated games. We evaluate GAugur through extensive experiments using a large number of real popular games. The results show that GAugur is able to identify whether a colocated game satisfies QoS requirement within an average error of 5\%, and is able to quantify the performance degradation of a colocated game within an average error of 7.9\%, which significantly outperforms the alternatives. Moreover, GAugur incurs an offline profiling cost linear to the number of games, and negligible overhead for online prediction. We apply GAugur to guiding efficient game colocations for cloud gaming. Experimental results show that GAugur is able to increase the resource utilization by 20\% to 60\%, and improve the overall performance by up to 15\%, compared to the state-of-the-art solutions.},
booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {231–242},
numpages = {12},
keywords = {machine learning, performance prediction, performance interference, game co-location, cloud gaming},
location = {Phoenix, AZ, USA},
series = {HPDC '19}
}

@inproceedings{10.1145/3094405.3094406,
author = {Zhao, Yang and Xia, Nai and Tian, Chen and Li, Bo and Tang, Yizhou and Wang, Yi and Zhang, Gong and Li, Rui and Liu, Alex X.},
title = {Performance of Container Networking Technologies},
year = {2017},
isbn = {9781450350587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094405.3094406},
doi = {10.1145/3094405.3094406},
abstract = {Container networking is now an important part of cloud virtualization architectures. It provides network access for containers by connecting both virtual and physical network interfaces. The performance of container networking has multiple dependencies, and each factor may significantly affect the performance. In this paper, we perform systematic experiments to study the performance of container networking technologies. For every measurement result, we try our best to qualify influencing factors.},
booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
pages = {1–6},
numpages = {6},
keywords = {Container, Measurement, Networking},
location = {Los Angeles, CA, USA},
series = {HotConNet '17}
}

@inproceedings{10.1145/3446132.3446412,
author = {Maskat, Ruhaila and Faizzuddin Zainal, Muhammad and Ismail, Nurrissammimayantie and Ardi, Norizah and Ahmad, Amirah and Daud, Noriza},
title = {Automatic Labelling of Malay Cyberbullying Twitter Corpus Using Combinations of Sentiment, Emotion and Toxicity Polarities},
year = {2021},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446412},
doi = {10.1145/3446132.3446412},
abstract = {Automatic labelling is essential in large corpuses. Engaging in human experts to label can be challenging. Semantic understanding can differ from one labeler to another based on individual's language ability. Platforms such as AmazonTurk are not able to ensure the quality of annotations in every domain. Extensive steps such as qualification and counter checking of labels may be implemented which will increase the cost of data annotation. Thus, the higher quality of labelled data expected, the greater the cost that needs to be expended. This scenario is made worse when the language is of low resource where in this work is the Malay language. Malay is a language used mostly in Malaysia, Indonesia, Singapore and Brunei. Unlike English which has large resources to tap into the semantics of sentences, making automatic labelling faster to mature, resources in Malay language are still limited. Further compounded is the use of social media data where the text is short, unnormalized and the inherent presence of code switching. The availability of qualified native Malay labelers is also scarce. To overcome this, we devised a method to automatically label a total of 219,444 Malay tweets by using a combination of sentiment, emotion and toxicity polarities. We extend the work from Arslan et al. who proposed the use of sentiment and emotion to identify cyberbullying text. Our work added toxicity polarity in the context of automatic labelling of cyberbully tweets in Malay. We were able to employ 5 experts with formal degrees in Malay language to label our training set. We applied this method to Malay cyberbullying corpus to determine “bully” and “not bully” labels. We have tested our method on 54,867 manually labelled data and achieved high accuracy.},
booktitle = {Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {85},
numpages = {6},
keywords = {Twitter, Cyberbullying, Malay language, Automatic labelling},
location = {Sanya, China},
series = {ACAI '20}
}

@inproceedings{10.1145/3307339.3343464,
author = {Humphrey, Marty and Lin, Vincent and Notani, Shweta and Mattos, Jose},
title = {Leveraging the Cloud for Intelligent Clinical Data Registries},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3343464},
doi = {10.1145/3307339.3343464},
abstract = {Public cloud platforms provide an amazing set of capabilities, but it can be an overwhelming challenge to create a design, implementation, and deployment that properly leverages today's existing public cloud capabilities while not precluding the use of near-future new services and infrastructure. We tackle this challenge in the context of clinical data registries, and create Cloud-based Patient Outcomes Platform (CPOP), our scalable public cloud application for clinical patient data. Doctors are able to visualize collected medical data in different chart formats and patients are able to check their data and submit medical survey forms. The specific domain of interest in this paper is Chronic Rhinosinusitis (CRS), a largely under-recognized chronic disease in our society. The primary barrier to quality improvement in CRS is the difficulty in collecting data from patients, tracking appropriate follow-up time intervals, and analyzing outcomes results in a prospective and ongoing fashion. We describe key aspects and design experiences of CPOP-CRS in Amazon Web Services.We also provide quantitative evaluation of a key feature of CPOP-CRS, which is the ability of CRS doctors to upload an audio clip of a doctor-patient interaction, and have the cloud render a text-based representation, and show a word error rate of 15.6\%. We outline next steps in the development of the CPOP/CPOP-CRS, and provide guidance for other users considering the public cloud for their next parallel and cloud-based Bioinformatics and Biomedicine project.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {675–682},
numpages = {8},
keywords = {cloud computing, amazon web services, clinical data registries},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/3152881.3152887,
author = {Rahman, Mahmudur and Hong, Hua-Jun and Rahman, Amatur and Tsai, Pei-Hsuan and Afrin, Afia and Uddin, Md Yusuf Sarwar and Venkatasubramanian, Nalini and Hsu, Cheng-Hsin},
title = {Adaptive Sensing Using Internet-of-Things with Constrained Communications},
year = {2017},
isbn = {9781450351683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152881.3152887},
doi = {10.1145/3152881.3152887},
abstract = {In this paper, we design and implement an Internet-of-Things (IoT) based platform for developing cities using environmental sensing as driving application with a set of air quality sensors that periodically upload sensor data to the cloud. Ubiquitous and free WiFi access is unavailable in most developing cities; IoT deployments must leverage 3G cellular connections that are expensive and metered. In order to best utilize the limited 3G data plan, we envision two adaptation strategies to drive sensing and sensemaking. The first technique is an infrastructure-level adaptation approach where we adjust sensing intervals of periodic sensors so that the data volume remains bounded within the plan. The second approach is at the information-level where application-specific analytics are deployed on board devices (or the edge) through container technologies (Docker and Kubernetes); the use case focuses on multimedia sensors that process captured raw information to lower volume semantic data that is communicated. This approach is implemented through the EnviroSCALE (Environmental Sensing and Community Alert Network) platform, an inexpensive Raspberry Pi based environmental sensing system that periodically publishes sensor data over a 3G connection with a limited data plan. We outline our deployment experience of EnviroSCALE in Dhaka city, the capital of Bangladesh. For information-level adaptation, we enhanced EnviroSCALE with Docker containers with rich media analytics, along Kubernetes for provisioning IoT devices and deploying the Docker images. To limit data communication overhead, the Docker images are preloaded in the board but a small footprint of analytic code is transferred whenever required. Our experiment results demonstrate the practicality of adaptive sensing and triggering rich sensing analytics via user-specified criteria, even over constrained data connections.},
booktitle = {Proceedings of the 16th Workshop on Adaptive and Reflective Middleware},
articleno = {6},
numpages = {6},
location = {Las Vegas, Nevada},
series = {ARM '17}
}

@inproceedings{10.1145/2593793.2593798,
author = {Bersani, Marcello M. and Bianculli, Domenico and Dustdar, Schahram and Gambi, Alessio and Ghezzi, Carlo and Krsti\'{c}, Sr\textcrd{}an},
title = {Towards the Formalization of Properties of Cloud-Based Elastic Systems},
year = {2014},
isbn = {9781450328418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593793.2593798},
doi = {10.1145/2593793.2593798},
abstract = {Cloud-based elastic systems run on a cloud infrastructure and have the capability of dynamically adjusting the allocation of their resources in response to changes in the workload, in a way that balances the trade-off between the desired quality-of-service and the operational costs. The actual elastic behavior of these systems is determined by a combination of factors, including the input workload, the logic of the elastic controller determining the type of resource adjustment, and the underlying technological platform implementing the cloud infrastructure. All these factors have to be taken into account to express the desired elastic behavior of a system, as well as to verify whether the system manifests or not such a behavior.  In this paper, we take a first step into these directions, by proposing a formalization, based on the CLTL^t(D) temporal logic, of several concepts and properties related to the behavior of cloud-based elastic systems. We also report on our preliminary evaluation of the feasibility to check the (formalized) properties on execution traces using an automated verification tool.},
booktitle = {Proceedings of the 6th International Workshop on Principles of Engineering Service-Oriented and Cloud Systems},
pages = {38–47},
numpages = {10},
keywords = {elastic systems, temporal logic, Cloud computing},
location = {Hyderabad, India},
series = {PESOS 2014}
}

@inproceedings{10.5555/2755753.2757193,
author = {Liu, Zhiqing and Liu, Chuangwen and Young, Evangeline F. Y.},
title = {An Effective Triple Patterning Aware Grid-Based Detailed Routing Approach},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Triple patterning lithography (TPL) is attracting more and more attention due to further scaling of the critical feature size. How fully the benefits of TPL can be utilized depends very much on both the decomposition and layout steps. However, it is non-trivial to perform detailed routing and layout decomposition simultaneously on a large-scale complicated circuit to achieve decomposability on one hand, and short wirelength, small number of stitches and small number of vias on the other hand. In our approach, routing and coloring are done iteratively but integrated closely to reduce the problem complexity. The routing step is able to detect and avoid native conflicts as much as possible. If any conflicts occur in the coloring step, the router will rip-up and re-route to get rid of them. This technique proves to be effective and efficient in improving the quality of the coloring assignment. Compared with previous works [1] on TPL using simultaneous routing and coloring, the number of stitches and the number of vias are reduced by 76.8\% and 2.1\% respectively while our running time is 36.6\% less and the wirelength is very comparable.},
booktitle = {Proceedings of the 2015 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition},
pages = {1641–1646},
numpages = {6},
location = {Grenoble, France},
series = {DATE '15}
}

@article{10.1145/3369875,
author = {Haller, Armin and Fern\'{a}ndez, Javier D. and Kamdar, Maulik R. and Polleres, Axel},
title = {What Are Links in Linked Open Data? A Characterization and Evaluation of Links between Knowledge Graphs on the Web},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3369875},
doi = {10.1145/3369875},
abstract = {Linked Open Data promises to provide guiding principles to publish interlinked knowledge graphs on the Web in the form of findable, accessible, interoperable, and reusable datasets. We argue that while as such, Linked Data may be viewed as a basis for instantiating the FAIR principles, there are still a number of open issues that cause significant data quality issues even when knowledge graphs are published as Linked Data. First, to define boundaries of single coherent knowledge graphs within Linked Data, a principled notion of what a dataset is, or, respectively, what links within and between datasets are, has been missing. Second, we argue that to enable FAIR knowledge graphs, Linked Data misses standardised findability and accessability mechanism via a single entry link. To address the first issue, we (i) propose a rigorous definition of a naming authority for a Linked Data dataset, (ii) define different link types for data in Linked datasets, (iii) provide an empirical analysis of linkage among the datasets of the Linked Open Data cloud, and (iv) analyse the dereferenceability of those links. We base our analyses and link computations on a scalable mechanism implemented on top of the HDT format, which allows us to analyse quantity and quality of different link types at scale.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {9},
numpages = {34},
keywords = {RDF, Linked Data}
}

@inproceedings{10.1145/3423336.3429345,
author = {Oehmcke, Stefan and Chen, Tzu-Hsin Karen and Prishchepov, Alexander V. and Gieseke, Fabian},
title = {Creating Cloud-Free Satellite Imagery from Image Time Series with Deep Learning},
year = {2020},
isbn = {9781450381628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423336.3429345},
doi = {10.1145/3423336.3429345},
abstract = {Optical satellite images are important for environmental monitoring. Unfortunately, such images are often affected by distortions, such as clouds, shadows, or missing data. This work proposes a deep learning approach for cleaning and imputing satellite images, which could serve as a reliable preprocessing step for spatial and spatio-temporal analyzes. More specifically, a coherent and cloud-free image for a specific target date and region is created based on a sequence of images of that region obtained at previous dates. Our model first extracts information from the previous time steps via a special gating function and then resorts to a modified version of the well-known U-Net architecture to obtain the desired output image. The model uses supplementary data, namely the approximate cloud coverage of input images, the temporal distance to the target time, and a missing data mask for each input time step. During the training phase we condition our model with the targets cloud coverage and missing values (disabled in production), which allows us to use data afflicted by distortion during training and thus does not require pre-selection of distortion-free data. Our experimental evaluation, conducted on data of the Landsat missions, shows that our approach outperforms the commonly utilized approach that resorts to taking the median of cloud-free pixels for a given position. This is especially the case when the quality of the data for the considered period is poor (e.g., lack of cloud free-images during the winter/fall periods). Our deep learning approach allows to improve the utility of the entire Landsat archive, the only existing global medium-resolution free-access satellite archive dating back to the 1970s. It therefore holds scientific and societal potential for future analyses conducted on data from this and other satellite imagery repositories.},
booktitle = {Proceedings of the 9th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
articleno = {3},
numpages = {10},
keywords = {satellite imagery, image reconstruction, remote sensing},
location = {Seattle, Washington},
series = {BigSpatial '20}
}

@inproceedings{10.1145/3208806.3208813,
author = {Klomann, Marcel and Englert, Michael and Weber, Kai and Grimm, Paul and Jung, Yvonne},
title = {Improving Mobile MR Applications Using a Cloud-Based Image Segmentation Approach with Synthetic Training Data},
year = {2018},
isbn = {9781450358002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208806.3208813},
doi = {10.1145/3208806.3208813},
abstract = {In this paper, we show how the quality of augmentation in mobile Mixed Reality applications can be improved using a cloud-based image segmentation approach with synthetic training data. Many modern Augmented Reality frameworks are based on visual inertial odometry on mobile devices and therefore have limited access to tracking hardware (e.g., depth sensor). Consequently, tracking still suffers from drift that makes it difficult to utilize in use cases that require a higher precision. To improve tracking quality, we propose a cloud tracking approach that uses machine learning based image segmentation to recognize known objects in a real scene, which allows us to estimate a precise camera pose. Augmented Reality applications that utilize our web service can use the resulting camera pose to correct drift from time to time, while still using local tracking between key frames. Moreover, the device's position in the real world, when starting the application, is usually used as reference coordinate system. Therefore, we simplify the authoring of MR applications significantly due to a well-defined coordinate system, which is context-based and not dependend on the starting position of a user. We present all steps from web-based initialization over the generation of synthetic training data up to usage in production. In addition, we describe the underlying algorithms in detail. Finally, we show a mobile Mixed Reality application, which is based on this novel approach and discuss its advantages.},
booktitle = {Proceedings of the 23rd International ACM Conference on 3D Web Technology},
articleno = {4},
numpages = {7},
keywords = {mobile mixed reality, AR authoring, computer vision, tracking, training data generation, image segmentation, machine learning},
location = {Pozna\'{n}, Poland},
series = {Web3D '18}
}

@inproceedings{10.1145/3447555.3465326,
author = {Hanafy, Walid A. and Molom-Ochir, Tergel and Shenoy, Rohan},
title = {Design Considerations for Energy-Efficient Inference on Edge Devices},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3465326},
doi = {10.1145/3447555.3465326},
abstract = {The emergence of low-power accelerators has enabled deep learning models to be executed on mobile or embedded edge devices without relying on cloud resources. The energy-constrained nature of these devices requires a judicious choice of a deep learning model and system configuration parameter to meet application needs while optimizing energy used during deep learning inference.In this paper, we carry out an experimental evaluation of more than 40 popular pretrained deep learning models to characterize trends in their accuracy, latency, and energy when running on edge accelerators. Our results show that as models have grown in size, the marginal increase in their accuracy has come at a much higher energy cost. Consequently, simply choosing the most accurate model for an application task comes at a higher energy cost; the application designer needs to consider the tradeoff between latency, accuracy, and energy use to make an appropriate choice. Since the relation between these metrics is non-linear, we present a recommendation algorithm to enable application designers to choose the best deep learning model for an application that meets energy budget constraints. Our results show that our technique can provide recommendations that are within 3 to 7\% of the specified budget while maximizing accuracy and minimizing energy.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {302–308},
numpages = {7},
keywords = {Deep learning Inference, Edge Computing, Energy-efficient Deep Learning},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@inproceedings{10.1145/3469263.3469858,
author = {Baldoni, Gabriele and Loudet, Julien and Cominardi, Luca and Corsaro, Angelo and He, Yong},
title = {Facilitating Distributed Data-Flow Programming with Eclipse Zenoh: The ERDOS Case},
year = {2021},
isbn = {9781450386036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469263.3469858},
doi = {10.1145/3469263.3469858},
abstract = {Data-flow programming is the computational model of choice for a large class of application domains, such as, real-time data processing, robotics platforms, and big-data analytics. Traditionally, dataflows are deployed and executed within well-defined system boundaries, such as robots, radars, or data-centers. These boundaries however are expected to blur with the advent of Edge Computing, which provides a multi-tier infrastructure spanning from the cloud to the things and enables for the distribution of applications across this continuum. In this paper we make a step towards the design of an Edge-native data-flow by mixing technologies coming from both worlds: ERDOS, a novel data-flow framework, and Eclipse Zenoh, a Named-Data-Networking built for the Edge Computing. More specifically, we (i) investigate how ERDOS can be expanded to cover Edge deployments by leveraging Zenoh, (ii) analyze the advantages provided by this integration, and (iii) evaluate the performance of a Zenoh-powered ERDOS. Our results show that ERDOS experiences a higher throughput and bounded latency when operating over Zenoh. Moreover, Zenoh enhances ERDOS with full location transparency, allowing developers and system designers to focus on the logic of their application as opposed to the topology deployment. Finally, our integration of Zenoh and ERDOS is available as open source at https://github.com/atolab/erdos-on-zenoh.},
booktitle = {Proceedings of the 1st Workshop on Serverless Mobile Networking for 6G Communications},
pages = {13–18},
numpages = {6},
keywords = {serverless, zenoh, ERDOS, data-flow, FaaS, NDN},
location = {Virtual, WI, USA},
series = {MobileServerless'21}
}

@article{10.1109/TNET.2021.3106937,
author = {Zhang, Sheng and Wang, Can and Jin, Yibo and Wu, Jie and Qian, Zhuzhong and Xiao, Mingjun and Lu, Sanglu},
title = {Adaptive Configuration Selection and Bandwidth Allocation for Edge-Based Video Analytics},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3106937},
doi = {10.1109/TNET.2021.3106937},
abstract = {Major cities worldwide have millions of cameras deployed for surveillance, business intelligence, traffic control, crime prevention, etc. Real-time analytics on video data demands intensive computation resources and high energy consumption. Traditional cloud-based video analytics relies on large centralized clusters to ingest video streams. With edge computing, we can offload compute-intensive analysis tasks to nearby servers, thus mitigating long latency incurred by data transmission via wide area networks. When offloading video frames from the front-end device to an edge server, the application configuration (i.e., frame sampling rate and frame resolution) will impact several metrics, such as energy consumption, analytics accuracy and user-perceived latency. In this paper, we study the configuration selection and bandwidth allocation for multiple video streams, which are connected to the same edge node sharing an upload link. We propose an efficient online algorithm, called JCAB, which jointly optimizes configuration adaption and bandwidth allocation to address a number of key challenges in edge-based video analytics systems, including edge capacity limitation, unknown network variation, intrusive dynamics of video contents. Our algorithm is developed based on Lyapunov optimization and Markov approximation, works online without requiring future information, and achieves a provable performance bound. We also extend the proposed algorithms to the multi-edge scenario in which each user or video stream has an additional choice about which edge server to connect. Extensive evaluation results show that the proposed solutions can effectively balance the analytics accuracy and energy consumption while keeping low system latency in a variety of settings.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {285–298},
numpages = {14}
}

@inproceedings{10.1145/3365609.3365865,
author = {Arnold, Todd and Calder, Matt and Cunha, Italo and Gupta, Arpit and Madhyastha, Harsha V. and Schapira, Michael and Katz-Bassett, Ethan},
title = {Beating BGP is Harder than We Thought},
year = {2019},
isbn = {9781450370202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365609.3365865},
doi = {10.1145/3365609.3365865},
abstract = {Online services all seek to provide their customers with the best Quality of Experience (QoE) possible. Milliseconds of delay can cause users to abandon a cat video or move onto a different shopping site, which translates into lost revenue. Thus, minimizing latency between users and content is crucial. To reduce latency, content and cloud providers have built massive, global networks. However, their networks must interact with customer ISPs via BGP, which has no concept of performance.The shortcomings of BGP are many and well documented, but in this paper we ask the community to take a step back and rethink what we know about BGP. We examine three separate studies of performance using large content and cloud provider networks and find that performance-aware routing schemes rarely achieve lower latency than BGP. We lay out a map for research to further study the idea that beating BGP may be more difficult than previously thought.},
booktitle = {Proceedings of the 18th ACM Workshop on Hot Topics in Networks},
pages = {9–16},
numpages = {8},
keywords = {BGP, performance, content delivery, traffic engineering},
location = {Princeton, NJ, USA},
series = {HotNets '19}
}

@article{10.1145/3450626.3459679,
author = {Ma, Xiaohe and Kang, Kaizhang and Zhu, Ruisheng and Wu, Hongzhi and Zhou, Kun},
title = {Free-Form Scanning of Non-Planar Appearance with Neural Trace Photography},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459679},
doi = {10.1145/3450626.3459679},
abstract = {We propose neural trace photography, a novel framework to automatically learn high-quality scanning of non-planar, complex anisotropic appearance. Our key insight is that free-form appearance scanning can be cast as a geometry learning problem on unstructured point clouds, each of which represents an image measurement and the corresponding acquisition condition. Based on this connection, we carefully design a neural network, to jointly optimize the lighting conditions to be used in acquisition, as well as the spatially independent reconstruction of reflectance from corresponding measurements. Our framework is not tied to a specific setup, and can adapt to various factors in a data-driven manner. We demonstrate the effectiveness of our framework on a number of physical objects with a wide variation in appearance. The objects are captured with a light-weight mobile device, consisting of a single camera and an RGB LED array. We also generalize the framework to other common types of light sources, including a point, a linear and an area light.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {124},
numpages = {13},
keywords = {illumination multiplexing, optimal lighting pattern, SVBRDF}
}

@inproceedings{10.1145/3105762.3105772,
author = {Mazumdar, Amrita and Alaghi, Armin and Barron, Jonathan T. and Gallup, David and Ceze, Luis and Oskin, Mark and Seitz, Steven M.},
title = {A Hardware-Friendly Bilateral Solver for Real-Time Virtual Reality Video},
year = {2017},
isbn = {9781450351010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105762.3105772},
doi = {10.1145/3105762.3105772},
abstract = {Rendering 3D-360° VR video from a camera rig is computation-intensive and typically performed offline. In this paper, we target the most time-consuming step of the VR video creation process, high-quality flow estimation with the bilateral solver. We propose a new algorithm, the hardware-friendly bilateral solver, that enables faster runtimes than existing algorithms of similar quality. Our algorithm is easily parallelized, achieving a 4\texttimes{} speedup on CPU and 32\texttimes{} speedup on GPU over a baseline CPU implementation. We also design an FPGA-based hardware accelerator that utilizes reduced-precision computation and the parallelism inherent in our algorithm to achieve further speedups over our CPU and GPU implementations while consuming an order of magnitude less power. The FPGA design's power efficiency enables practical real-time VR video processing at the camera rig or in the cloud.},
booktitle = {Proceedings of High Performance Graphics},
articleno = {13},
numpages = {10},
keywords = {hardware accelerators, parallelism, FPGA design, GPU algorithm, virtual reality, real-time image processing},
location = {Los Angeles, California},
series = {HPG '17}
}

@inproceedings{10.1145/3132211.3134452,
author = {Grassi, Giulio and Jamieson, Kyle and Bahl, Paramvir and Pau, Giovanni},
title = {Parkmaster: An in-Vehicle, Edge-Based Video Analytics Service for Detecting Open Parking Spaces in Urban Environments},
year = {2017},
isbn = {9781450350877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132211.3134452},
doi = {10.1145/3132211.3134452},
abstract = {We present the design and implementation of ParkMaster, a system that leverages the ubiquitous smartphone to help drivers find parking spaces in the urban environment. ParkMaster estimates parking space availability using video gleaned from drivers' dash-mounted smartphones on the network's edge, uploading analytics about the street to the cloud in real time as participants drive. Novel lightweight parked-car localization algorithms enable the system to estimate each parked car's approximate location by fusing information from phone's camera, GPS, and inertial sensors, tracking and counting parked cars as they move through the driving car's camera frame of view. To visually calibrate the system, ParkMaster relies only on the size of well-known objects in the urban environment for on-the-go calibration. We implement and deploy ParkMaster on Android smartphones, uploading parking analytics to the Azure cloud. On-the-road experiments in three different environments comprising Los Angeles, Paris and an Italian village measure the end-to-end accuracy of the system's parking estimates (close to 90\%) as well as the amount of cellular data usage the system requires (less than one mega-byte per hour). Drill-down microbenchmarks then analyze the factors contributing to this end-to-end performance, as video resolution, vision algorithm parameters, and CPU resources.},
booktitle = {Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
articleno = {16},
numpages = {14},
keywords = {fog computing, mobile systems, edge computing, visual analytics},
location = {San Jose, California},
series = {SEC '17}
}

@inproceedings{10.1145/3460231.3474615,
author = {Zhang, Yuxi and Xie, Kexin},
title = {Boosting Local Recommendations With Partially Trained Global Model},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474615},
doi = {10.1145/3460231.3474615},
abstract = {Building recommendation systems for enterprise software has many unique challenges that are different from consumer-facing systems. When applied to different organizations, the data used to power those recommendation systems vary substantially in both quality and quantity due to differences in their operational practices, marketing strategies, and targeted audiences. At Salesforce, as a cloud provider of such a system with data across many different organizations, naturally, it makes sense to pool data from different organizations to build a model that combines all values from different brands. However, multiple issues like how do we make sure a model trained with pooled data can still capture customer specific characteristics, how do we design the system to handle those data responsibly and ethically, i.e., respecting contractual agreements with our clients, legal and compliance requirements, and the privacy of all the consumers. In this proposal, We present a framework that not only utilizes enriched user-level data across organizations, but also boosts business-specific characteristics in generating personal recommendations. We will also walk through key privacy considerations when designing such a system.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {533–535},
numpages = {3},
keywords = {global model, recommendation system, privacy},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/2933349.2933356,
author = {Kyzirakos, Kostis and Alvanaki, Foteini and Kersten, Martin},
title = {In Memory Processing of Massive Point Clouds for Multi-Core Systems},
year = {2016},
isbn = {9781450343190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933349.2933356},
doi = {10.1145/2933349.2933356},
abstract = {LIDAR is a popular remote sensing method used to examine the surface of the Earth. LIDAR instruments use light in the form of a pulsed laser to measure ranges (variable distances) and generate vast amounts of precise three dimensional point data describing the shape of the Earth. Processing large collections of point cloud data and combining them with auxiliary GIS data remain an open research problem.Past research in the area of geographic information systems focused on handling large collections of complex geometric objects stored on disk and most algorithms have been designed and studied in a single-thread setting even though multi-core systems are well established. In this paper, we describe parallel alternatives of known algorithms for evaluating spatial selections over point clouds and spatial joins between point clouds and rectangle collections.},
booktitle = {Proceedings of the 12th International Workshop on Data Management on New Hardware},
articleno = {7},
numpages = {10},
location = {San Francisco, California},
series = {DaMoN '16}
}

@inproceedings{10.1145/2851613.2852009,
author = {Ullah, Amjad},
title = {Towards Workload-Aware Fine-Grained Control over Cloud Resources: Student Research Abstract},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2852009},
doi = {10.1145/2851613.2852009},
abstract = {The systems deployed over cloud are subject to unpredictable workload conditions that vary from time to time, e.g. an ecommerce website may face higher workloads than normal during festivals or promotional schemes. In order to maintain the performance of such systems, an efficient elastic resource provisioning strategy is required. However, providing such a strategy that determines the right amount of cloud resources that fulfills the Quality of Service (QoS) demand is a challenging task. Over the period, many proposals have been introduced using techniques like threshold based rules, reinforcement learning and control theory, etc. The existing proposals, however, suffer from issues like lack of expertise to appropriately set the quantitative specification of thresholds, online training time overhead of the algorithm, too specific to work well in particular situation like when there is sudden burst in workload or work well in nominal conditions for stable workload, etc. Moreover, the existing approaches do not address uncertainty. Our proposed framework is a step forward to address the mentioned issues for systems that hold time varying workload conditions.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {488–489},
numpages = {2},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3469096.3469864,
author = {Frieder, Ophir},
title = {Searching Harsh Documents},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469864},
doi = {10.1145/3469096.3469864},
abstract = {Conventional, textual document search is arguably well understood. Traditional and modern (neural) algorithms are available; benchmark collections and evaluation metrics are prevalent. However, not all documents are conventional or purely textual. We explore what is takes to search "harsh" document collections. Such collections comprise potentially of documents that are natively non-digital, are multilingual, include components that are not strictly textual, are corrupted, or are a combination thereof. We address machine readability and its implication on search. We overview component segmentation and integration as a search process. We describe the processing of search queries that are informationally deficient or corrupt. We then comment on the evaluation of the selected efforts presented and highlight their history from concept to practice. We conclude with a brief commentary on ongoing efforts.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {4},
numpages = {1},
keywords = {enhancement, non-digital text processing, benchmarks, corrupted text},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@inproceedings{10.5555/3172795.3172826,
author = {Pravato, Laura and Doyle, Thomas E.},
title = {IoT for Remote Wireless Electrophysiological Monitoring: Proof of Concept},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {The Internet of Things (IoT) offers integrated sensing of all aspects of daily life. The field of healthcare offers the greatest potential for IoT to benefit society, but also presents significant challenges. A key component of IoT is the development of intelligent ubiquitous sensing. Achieving this requires circuits and systems that require low power and efficient computation.As a proof of concept, we present a prototype design of a continuous wireless electrocardiogram (ECG) monitoring device that uses a small, low-cost IoT wi-fi module to upload real-time data to the cloud. Two IoT cloud services were evaluated to record and plot real-time ECG data: IBM Bluemix and ThingSpeak. Preliminary data quality was analyzed using kurtosis and spectral distribution ratio. Future development is necessary to improve battery power and to implement real-time data analysis.Remote medical and health monitoring is an important step in supporting personalized predictive analytics, smart homes, and chronic illness management. The presented device has the potential to provide health professionals with real-time ECG data allowing for diagnosis of cardiac pathologies, monitoring of patients suffering from heart disease and/or patients recovering from cardiac conditions.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {254–258},
numpages = {5},
keywords = {electrocardiogram, wearables, ESP32, IBM watson, IBM Bluemix, ECG, ThingSpeak, ESP8266, IoT, internet of things},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3243046.3243049,
author = {Caballero-Gil, Pino and Caballero-Gil, C\'{a}ndido and Molina-Gil, Jezabel},
title = {Ubiquitous System to Monitor Transport and Logistics},
year = {2018},
isbn = {9781450359610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243046.3243049},
doi = {10.1145/3243046.3243049},
abstract = {In the management of transport and logistics, which includes the delivery, movement and collection of goods through roads, ports and airports, participate, in general, many different actors. The most critical aspects of supply chain systems include time, space and interdependencies. Besides, there are several security challenges that can be caused both by unintentional and intentional errors. With all this in mind, this work proposes the combination of technologies such as RFID, GPS, WiFi Direct and LTE/3G to automate product authentication and merchandise tracking, reducing the negative effects caused either by mismanagement or attacks against the process of the supply chain. In this way, this work proposes a ubiquitous management scheme for the monitoring through the cloud of freight and logistics systems, including demand management, customization and automatic replenishment of out-of-stock goods. The proposal implies an improvement in the efficiency of the systems, which can be quantified in a reduction of time and cost in the inventory and distribution processes, and in a greater facility for the detection of counterfeit versions of branded articles. In addition, it can be used to create safer and more efficient schemes that help companies and organizations to improve the quality of the service and the traceability of the transported goods.},
booktitle = {Proceedings of the 15th ACM International Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, \&amp; Ubiquitous Networks},
pages = {71–75},
numpages = {5},
keywords = {ubiquitous system, security, logistics, wireless technologies},
location = {Montreal, QC, Canada},
series = {PE-WASUN'18}
}

@article{10.1145/3409772,
author = {Kaur, Kuljeet and Garg, Sahil and Kaddoum, Georges and Kumar, Neeraj},
title = {Energy and SLA-Driven MapReduce Job Scheduling Framework for Cloud-Based Cyber-Physical Systems},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3409772},
doi = {10.1145/3409772},
abstract = {Energy consumption minimization of cloud data centers (DCs) has attracted much attention from the research community in the recent years; particularly due to the increasing dependence of emerging Cyber-Physical Systems on them. An effective way to improve the energy efficiency of DCs is by using efficient job scheduling strategies. However, the most challenging issue in selection of efficient job scheduling strategy is to ensure service-level agreement (SLA) bindings of the scheduled tasks. Hence, an energy-aware and SLA-driven job scheduling framework based on MapReduce is presented in this article. The primary aim of the proposed framework is to explore task-to-slot/container mapping problem as a special case of energy-aware scheduling in deadline-constrained scenario. Thus, this problem can be viewed as a complex multi-objective problem comprised of different constraints. To address this problem efficiently, it is segregated into three major subproblems (SPs), namely, deadline segregation, map and reduce phase energy-aware scheduling. These SPs are individually formulated using Integer Linear Programming. To solve these SPs effectively, heuristics based on Greedy strategy along with classical Hungarian algorithm for serial and serial-parallel systems are used. Moreover, the proposed scheme also explores the potential of splitting Map/Reduce phase(s) into multiple stages to achieve higher energy reductions. This is achieved by leveraging the concepts of classical Greedy approach and priority queues. The proposed scheme has been validated using real-time data traces acquired from OpenCloud. Moreover, the performance of the proposed scheme is compared with the existing schemes using different evaluation metrics, namely, number of stages, total energy consumption, total makespan, and SLA violated. The results obtained prove the efficacy of the proposed scheme in comparison to the other schemes under different workload scenarios.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {31},
numpages = {24},
keywords = {energy optimization, and MapReduce, Hungarian algorithm, Cyber-physical systems, greedy approach, job scheduling}
}

@inproceedings{10.1109/CCGrid.2014.42,
author = {Glatard, Tristan and Rousseau, Marc-Etienne and Rioux, Pierre and Adalat, Reza and Evans, Alan C.},
title = {Controlling the Deployment of Virtual Machines on Clusters and Clouds for Scientific Computing in CBRAIN},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.42},
doi = {10.1109/CCGrid.2014.42},
abstract = {The emergence of hardware virtualization, notably exploited by cloud infrastructures, led to a paradigm shift in distributed computing by enabling complete software customization and elastic scaling of resources. However, new software architectures and deployment algorithms are still required to fully exploit virtualization in web platforms used for scientific computing, commonly called science gateways. We propose a software architecture and an algorithm to enable and optimize the deployment of virtual machines on clusters and clouds in science gateways. Our architecture is based on 3 design principles: (i) separation between resource provisioning and task scheduling (ii) encapsulation of VMs in regular computing tasks (iii) association of a virtual computing site to each disk image. Our algorithm submits and removes VMs on clusters and clouds based on the current system workload, the number of available job slots in active VMs, the cost and current performance of clouds clusters, and a parameter quantifying the performance-cost trade-off. To cope with variable queuing and booting times, it replicates VMs on independent computing sites selected from a minimization of a makespan-cost linear combination in the Pareto set of non-dominated solutions. Makespan and cost are estimated from the last measured queuing, booting, and task execution times, using an exponential model of the gain yielded by VM replication. We implement this algorithm in CBRAIN, a science gateway widely used for neuroimaging, and we evaluate it on an infrastructure of 2 clusters and 1 cloud. Results show that it is able to reach some points of the performance-cost trade-off associated to VM deployment.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {384–393},
numpages = {10},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3375555.3384939,
author = {Bondi, Andr\'{e} B.},
title = {WOSP-C 2020: Workshop on Challenges and Opportunities in Large-Scale Performance: Welcoming Remarks},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3384939},
doi = {10.1145/3375555.3384939},
abstract = {It is my great pleasure to welcome you to WOSP-C 2020, the Workshop on Challenges and Opportunities in Large Scale Performance. Our theme this year relates to the use of analytics to interpret system performance and resource usage measurements that can now be gathered rapidly on a large scale. Our four invited speakers hail from industry. All three presentations in the first session and the last presentation in the second session deal with modeling and measurement to automate the making of decisions about system configuration or the recognition of anomalies, especially for cloud-based systems. The other two papers in the second session address measurement and modeling issues at a granular level. These topics are highly relevant to the issues systems architects and other stakeholders face when deploying systems in the cloud, because doing so need not guarantee good performance. The recent emergence of the ability to gather vast numbers of performance and resource usage measurements facilitates the informed choice of target cloud platforms and their configurations. The presentations in this workshop deal with various aspects of how this can be achieved.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {5–6},
numpages = {2},
keywords = {cloud measurement, monitoring and tuning, software performance engineering, automated system performance modeling},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3341302.3342073,
author = {Jin, Yuchen and Renganathan, Sundararajan and Ananthanarayanan, Ganesh and Jiang, Junchen and Padmanabhan, Venkata N. and Schroder, Manuel and Calder, Matt and Krishnamurthy, Arvind},
title = {Zooming in on Wide-Area Latencies to a Global Cloud Provider},
year = {2019},
isbn = {9781450359566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341302.3342073},
doi = {10.1145/3341302.3342073},
abstract = {The network communications between the cloud and the client have become the weak link for global cloud services that aim to provide low latency services to their clients. In this paper, we first characterize WAN latency from the viewpoint of a large cloud provider Azure, whose network edges serve hundreds of billions of TCP connections a day across hundreds of locations worldwide. In particular, we focus on instances of latency degradation and design a tool, BlameIt, that enables cloud operators to localize the cause (i.e., faulty AS) of such degradation. BlameIt uses passive diagnosis, using measurements of existing connections between clients and the cloud locations, to localize the cause to one of cloud, middle, or client segments. Then it invokes selective active probing (within a probing budget) to localize the cause more precisely. We validate BlameIt by comparing its automatic fault localization results with that arrived at by network engineers manually, and observe that BlameIt correctly localized the problem in all the 88 incidents. Further, BlameIt issues 72X fewer active probes than a solution relying on active probing alone, and is deployed in production at Azure.},
booktitle = {Proceedings of the ACM Special Interest Group on Data Communication},
pages = {104–116},
numpages = {13},
keywords = {networkfault localization, network diagnosis, tomography, active network probes, internet latency measurement, wide-area network},
location = {Beijing, China},
series = {SIGCOMM '19}
}

@inproceedings{10.1145/2945292.2945311,
author = {Devaux, Alexandre and Br\'{e}dif, Mathieu},
title = {Realtime Projective Multi-Texturing of Pointclouds and Meshes for a Realistic Street-View Web Navigation},
year = {2016},
isbn = {9781450344289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945292.2945311},
doi = {10.1145/2945292.2945311},
abstract = {Street-view web applications have now gained widespread popularity. Targeting the general public, they offer ease of use, but while they allow efficient navigation from a pedestrian level, the immersive quality of such renderings is still low. The user is usually stuck at specific positions and transitions bring out artefacts, in particular parallax and aliasing. We propose a method to enhance the realism of street view navigation systems using a hybrid rendering based on realtime projective texturing on meshes and pointclouds with occlusion handling, requiring extremely minimized pre-processing steps allowing fast data update, progressive streaming (mesh-based approximation, with point cloud details) and unaltered raw data precise visualization.},
booktitle = {Proceedings of the 21st International Conference on Web3D Technology},
pages = {105–108},
numpages = {4},
keywords = {image based rendering, GIS, projective texturing, WebGL, street-view, point based rendering},
location = {Anaheim, California},
series = {Web3D '16}
}

@inproceedings{10.1145/3434581.3434632,
author = {Mao, Deng and Jie, Liu},
title = {Streak Image Compression},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434632},
doi = {10.1145/3434581.3434632},
abstract = {This Streak image contains a lot of noise signals and it is not easy to accurately detect the target signal position. This article gives a method of determining target signal region and achieving streak image compression, which uses multi-resolution wavelet denoising and detecting target signal considering its overall continuous distribution as a rule and using the stepped-like piecewise function of range as the detection object. In target signal region, convolution filtering is used to detect the peaks, and then the three-dimensional image is calculated. Experiments show that the method can reduce the scale of data for computing, improve the quality of generating point cloud data and reconstruct 3D images more accurately.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {450–454},
numpages = {5},
keywords = {stepped-like piecewise function, overall continuous distribution, Streak image compression, target signal},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1145/3437914.3437980,
author = {M. Pittman, Jason},
title = {DRAT - A Dynamic Resource Allocation Tool for Estimating Compute Power in a Cybersecurity Engineering Learning Facility},
year = {2021},
isbn = {9781450389594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437914.3437980},
doi = {10.1145/3437914.3437980},
abstract = {Cybersecurity laboratory infrastructure has direct impact on the quality of student learning experiences. Because of this, the computing education field has developed a variety of approaches to designing and implementing these learning facilities. Yet, little work has gone into how to properly size cybersecurity laboratory infrastructure relative to student population and curricular compute power demands. The result has been laboratory infrastructures that do not scale with degree programs. Consequently, laboratories are either underpowered, thus limiting learning experiences, or overpowered which wastes financial resources. Accordingly, this work presents DRAT, an open-source software tool, for estimating necessary compute power in a cybersecurity engineering learning facility. More specifically, DRAT is designed to estimate the required discrete compute power on a per exercise basis in a cybersecurity engineering learning facility operating in a private cloud model. Such discrete estimations are intended to communicate physical host hardware requirements such as physical CPU core count, virtual RAM, and total Hard Disk space. The first step in designing DRAT was to forge a model estimator function. Then, we identified a series of scalar abstractions representing learning facility hardware infrastructure and behaving as conversion factors between the model function and output. Because the goal of this work was to provide estimates for cloud compute power requirements, DRAT outputs the number of physical cores, total RAM, total Disk, and total (virtual or physical) Network interfaces required to run the indicated scenario. The implication is that such estimates can inform purchasing and configuration decisions which directly impact student learning outcomes.},
booktitle = {Proceedings of the 5th Conference on Computing Education Practice},
pages = {39},
numpages = {1},
location = {<conf-loc>, <city>Durham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CEP '21}
}

@inproceedings{10.1109/CCGRID.2017.118,
author = {Xhagjika, Vamis and Escoda, \`{O}scar Divorra and Navarro, Leandro and Vlassov, Vladimir},
title = {Load and Video Performance Patterns of a Cloud Based WebRTC Architecture},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.118},
doi = {10.1109/CCGRID.2017.118},
abstract = {Web Real-Time Communication or Realtime communication in the Web (WebRTC/RTCWeb) is a prolific new standard and technology stack, providing full audio/video agnostic communications for the Web. Service providers implementing such technology deal with various levels of complexity ranging anywhere from: high service distribution, multiclient integration, P2P and Cloud assisted communication backends, content delivery, real-time constraints and across clouds resource allocation. This work presents a study of the joint factors including multi-cloud distribution, network performance, media parameters and back-end resource loads, in Cloud based Media Selective Forwarding Units for WebRTC infrastructures. The monitored workload is sampled from a large population of real users of our testing infrastructure, additionally the performance data is sampled both by passive user measurements as well as server side measurements. Patterns correlating such factors enable designing adaptive resource allocation algorithms and defining media Service Level Objectives (SLO) spanning over multiple data-centers or servers. Based on our analysis, we discover strong periodical load patterns even though the nature of user interaction with the system is mostly not predetermined with variable user churn.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {739–744},
numpages = {6},
keywords = {media, webrtc, stream allocation, load measurements, bitrate, rtp/rtcp},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3366030.3366041,
author = {da Silva, Gabriela Oliveira Mota and Dur\~{a}o, Frederico Ara\'{u}jo and Capretz, Miriam},
title = {PLDSD: Personalized Linked Data Semantic Distance for LOD-Based Recommender Systems},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366041},
doi = {10.1145/3366030.3366041},
abstract = {A vast amount of data that can be easily read by machines have been published in freely accessible and interconnected datasets, creating the so-called Linked Open Data cloud. This phenomenon has opened opportunities for the development of semantic applications, including recommender systems. In this paper, we propose Personalized Linked Data Semantic Distance (PLDSD), a novel similarity measure for linked data that personalizes the RDF graph by adding weights to the edges, based on previous user's choices. Thus, our approach has the purpose of minimizing the sparsity problem by ranking the best features for a particular user, and also, of solving the item cold-start problem, since the feature ranking task is based on features shared between old items and the new item. We evaluate PLDSD in the context of a LOD-based Recommender System using mixed data from DBpedia and MovieLens, and the experimental results indicate better accuracy of recommendations compared to a non-personalized baseline similarity method.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {294–303},
numpages = {10},
keywords = {Semantic Similarity, Linked Open Data, Feature Selection, Recommender Systems, Graph Personalization},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/2631775.2631824,
author = {Kim, Suin and Weber, Ingmar and Wei, Li and Oh, Alice},
title = {Sociolinguistic Analysis of Twitter in Multilingual Societies},
year = {2014},
isbn = {9781450329545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631775.2631824},
doi = {10.1145/2631775.2631824},
abstract = {In a multilingual society, language not only reflects culture and heritage, but also has implications for social status and the degree of integration in society. Different languages can be a barrier between monolingual communities, and the dynamics of language choice could explain the prosperity or demise of local languages in an international setting. We study this interplay of language and network structure in diverse, multi-lingual societies, using Twitter. In our analysis, we are particularly interested in the role of bilinguals. Concretely, we attempt to quantify the degree to which users are the "bridge-builders" between monolingual language groups, while monolingual users cluster together. Also, with the revalidation of English as a lingua franca on Twitter, we reveal users of the native non-English language have higher influence than English users, and the language convergence pattern is consistent across the regions. Furthermore, we explore for which topics these users prefer their native language rather than English. To the best of our knowledge, this is the largest sociolinguistic study in a network setting.},
booktitle = {Proceedings of the 25th ACM Conference on Hypertext and Social Media},
pages = {243–248},
numpages = {6},
keywords = {multilingualism, social media, topic modeling, sociolinguistics},
location = {Santiago, Chile},
series = {HT '14}
}

@inproceedings{10.1145/2541940.2541946,
author = {Dall, Christoffer and Nieh, Jason},
title = {KVM/ARM: The Design and Implementation of the Linux ARM Hypervisor},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541946},
doi = {10.1145/2541940.2541946},
abstract = {As ARM CPUs become increasingly common in mobile devices and servers, there is a growing demand for providing the benefits of virtualization for ARM-based devices. We present our experiences building the Linux ARM hypervisor, KVM/ARM, the first full system ARM virtualization solution that can run unmodified guest operating systems on ARM multicore hardware. KVM/ARM introduces split-mode virtualization, allowing a hypervisor to split its execution across CPU modes and be integrated into the Linux kernel. This allows KVM/ARM to leverage existing Linux hardware support and functionality to simplify hypervisor development and maintainability while utilizing recent ARM hardware virtualization extensions to run virtual machines with comparable performance to native execution. KVM/ARM has been successfully merged into the mainline Linux kernel, ensuring that it will gain wide adoption as the virtualization platform of choice for ARM. We provide the first measurements on real hardware of a complete hypervisor using ARM hardware virtualization support. Our results demonstrate that KVM/ARM has modest virtualization performance and power costs, and can achieve lower performance and power costs compared to x86-based Linux virtualization on multicore hardware.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {333–348},
numpages = {16},
keywords = {multicore, virtualization, arm, operating systems, hypervisors, linux},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@article{10.1145/2644865.2541946,
author = {Dall, Christoffer and Nieh, Jason},
title = {KVM/ARM: The Design and Implementation of the Linux ARM Hypervisor},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2644865.2541946},
doi = {10.1145/2644865.2541946},
abstract = {As ARM CPUs become increasingly common in mobile devices and servers, there is a growing demand for providing the benefits of virtualization for ARM-based devices. We present our experiences building the Linux ARM hypervisor, KVM/ARM, the first full system ARM virtualization solution that can run unmodified guest operating systems on ARM multicore hardware. KVM/ARM introduces split-mode virtualization, allowing a hypervisor to split its execution across CPU modes and be integrated into the Linux kernel. This allows KVM/ARM to leverage existing Linux hardware support and functionality to simplify hypervisor development and maintainability while utilizing recent ARM hardware virtualization extensions to run virtual machines with comparable performance to native execution. KVM/ARM has been successfully merged into the mainline Linux kernel, ensuring that it will gain wide adoption as the virtualization platform of choice for ARM. We provide the first measurements on real hardware of a complete hypervisor using ARM hardware virtualization support. Our results demonstrate that KVM/ARM has modest virtualization performance and power costs, and can achieve lower performance and power costs compared to x86-based Linux virtualization on multicore hardware.},
journal = {SIGPLAN Not.},
month = {feb},
pages = {333–348},
numpages = {16},
keywords = {arm, multicore, hypervisors, linux, operating systems, virtualization}
}

@article{10.1145/2654822.2541946,
author = {Dall, Christoffer and Nieh, Jason},
title = {KVM/ARM: The Design and Implementation of the Linux ARM Hypervisor},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2654822.2541946},
doi = {10.1145/2654822.2541946},
abstract = {As ARM CPUs become increasingly common in mobile devices and servers, there is a growing demand for providing the benefits of virtualization for ARM-based devices. We present our experiences building the Linux ARM hypervisor, KVM/ARM, the first full system ARM virtualization solution that can run unmodified guest operating systems on ARM multicore hardware. KVM/ARM introduces split-mode virtualization, allowing a hypervisor to split its execution across CPU modes and be integrated into the Linux kernel. This allows KVM/ARM to leverage existing Linux hardware support and functionality to simplify hypervisor development and maintainability while utilizing recent ARM hardware virtualization extensions to run virtual machines with comparable performance to native execution. KVM/ARM has been successfully merged into the mainline Linux kernel, ensuring that it will gain wide adoption as the virtualization platform of choice for ARM. We provide the first measurements on real hardware of a complete hypervisor using ARM hardware virtualization support. Our results demonstrate that KVM/ARM has modest virtualization performance and power costs, and can achieve lower performance and power costs compared to x86-based Linux virtualization on multicore hardware.},
journal = {SIGARCH Comput. Archit. News},
month = {feb},
pages = {333–348},
numpages = {16},
keywords = {virtualization, linux, multicore, arm, operating systems, hypervisors}
}

@inproceedings{10.1145/3266157.3266208,
author = {Kempfle, Jochen and Van Laerhoven, Kristof},
title = {Respiration Rate Estimation with Depth Cameras: An Evaluation of Parameters},
year = {2018},
isbn = {9781450364874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266157.3266208},
doi = {10.1145/3266157.3266208},
abstract = {Depth cameras have been known to be capable of picking up the small changes in distance from users' torsos, to estimate respiration rate. Several studies have shown that under certain conditions, the respiration rate from a non-mobile user facing the camera can be accurately estimated from parts of the depth data. It is however to date not clear, what factors might hinder the application of this technology in any setting, what areas of the torso need to be observed, and how readings are affected for persons at larger distances from the RGB-D camera. In this paper, we present a benchmark dataset that consists of the point cloud data from a depth camera, which monitors 7 volunteers at variable distances, for variable methods to pin-point the person's torso, and at variable breathing rates. Our findings show that the respiration signal's signal-to-noise ratio becomes debilitating as the distance to the person approaches 4 metres, and that bigger windows over the person's chest work particularly well. The sampling rate of the depth camera was also found to impact the signal's quality significantly.},
booktitle = {Proceedings of the 5th International Workshop on Sensor-Based Activity Recognition and Interaction},
articleno = {4},
numpages = {10},
keywords = {Kinect v2, non-contact measurement, respiration measurement, respiratory rate, ToF sensing},
location = {Berlin, Germany},
series = {iWOAR '18}
}

@inproceedings{10.1145/3445814.3446699,
author = {Zha, Yue and Li, Jing},
title = {When Application-Specific ISA Meets FPGAs: A Multi-Layer Virtualization Framework for Heterogeneous Cloud FPGAs},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446699},
doi = {10.1145/3445814.3446699},
abstract = {While field-programmable gate arrays (FPGAs) have been widely deployed into cloud platforms, the high programming complexity and the inability to manage FPGA resources in an elastic/scalable manner largely limits the adoption of FPGA acceleration. Existing FPGA virtualization mechanisms partially address these limitations. Application-specific (AS) ISA provides a nice abstraction to enable a simple software programming flow that makes FPGA acceleration accessible by the mainstream software application developers. Nevertheless, existing AS ISA-based approaches can only manage FPGA resources at a per-device granularity, leading to a low resource utilization. Alternatively, hardware-specific (HS) abstraction improves the resource utilization by spatially sharing one FPGA among multiple applications. But it cannot reduce the programming complexity due to the lack of a high-level programming model. In this paper, we propose a virtualization mechanism for heterogeneous cloud FPGAs that combines AS ISA and HS abstraction to fully address aforementioned limitations. To efficiently combine these two abstractions, we provide a multi-layer virtualization framework with a new system abstraction as an indirection layer between them. This indirection layer hides the FPGA-specific resource constraints and leverages parallel pattern to effectively reduce the mapping complexity. It simplifies the mapping process into two steps, where the first step decomposes an AS ISA-based accelerator under no resource constraint to extract all fine-grained parallel patterns, and the second step leverages the extracted parallel patterns to simplify the process of mapping the decomposed accelerators onto the underlying HS abstraction. While system designers might be able to manually perform these steps for small accelerator designs, we develop a set of custom tools to automate this process and achieve a high mapping quality. By hiding FPGA-specific resource constraints, the proposed system abstraction provides a homogeneous view for the heterogeneous cloud FPGAs to simplify the runtime resource management. The extracted parallel patterns could also be leveraged by the runtime system to improve the performance of scale-out acceleration by maximally hiding the inter-FPGA communication latency. We use an AS ISA similar to the one proposed in BrainWave project and a recently proposed HS abstraction as a case study to demonstrate the effectiveness of the proposed virtualization framework. The performance is evaluated on a custom-built FPGA cluster with heterogeneous FPGA resources. Compared with the baseline system that only uses AS ISA, the proposed framework effectively combines these two abstractions and improves the aggregated system throughput by 2.54\texttimes{} with a marginal virtualization overhead.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {123–134},
numpages = {12},
keywords = {Virtualization, Parallel patterns, Application-specific ISA, Heterogeneous cloud FPGAs},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{10.1145/3269206.3271692,
author = {Wu, Xuan and Zhao, Lingxiao and Akoglu, Leman},
title = {A Quest for Structure: Jointly Learning the Graph Structure and Semi-Supervised Classification},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271692},
doi = {10.1145/3269206.3271692},
abstract = {Semi-supervised learning (SSL) is effectively used for numerous classification problems, thanks to its ability to make use of abundant unlabeled data. The main assumption of various SSL algorithms is that the nearby points on the data manifold are likely to share a label. Graph-based SSL constructs a graph from point-cloud data as an approximation to the underlying manifold, followed by label inference. It is no surprise that the quality of the constructed graph in capturing the essential structure of the data is critical to the accuracy of the subsequent inference step [6].How should one construct a graph from the input point-cloud data for graph-based SSL? In this work we introduce a new, parallel graph learning framework (called PG-learn) for the graph construction step of SSL. Our solution has two main ingredients: (1) a gradient-based optimization of the edge weights (more specifically, different kernel bandwidths in each dimension) based on a validation loss function, and (2) a parallel hyperparameter search algorithm with an adaptive resource allocation scheme. In essence, (1) allows us to search around a (random) initial hyperparameter configuration for a better one with lower validation loss. Since the search space of hyperparameters is huge for high-dimensional problems, (2) empowers our gradient-based search to go through as many different initial configurations as possible, where runs for relatively unpromising starting configurations are terminated early to allocate the time for others. As such, PG-learn is a carefully-designed hybrid of random and adaptive search. Through experiments on multi-class classification problems, we show that PG-learn significantly outperforms a variety of existing graph construction schemes in accuracy (per fixed time budget for hyperparameter tuning), and scales more effectively to high dimensional problems.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {87–96},
numpages = {10},
keywords = {hyperparamer optimization, hyperparamer inference, semi-supervised learning, graph learning, graph construction, parallel graph learning},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3306306.3328001,
author = {Blanchard, Sam and Huang, Jia-Bin and Williams, Christopher B. and Meenakshisundaram, Viswanath and Kubalak, Joseph and Lokegaonkar, Sanket},
title = {Source Form an Automated Crowdsourced Object Generator},
year = {2019},
isbn = {9781450363167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306306.3328001},
doi = {10.1145/3306306.3328001},
abstract = {Source Form is a stand-alone device capable of collecting crowdsourced images of a user-defined object, stitching together available visual data (e.g., photos tagged with search term) through photogrammetry, creating watertight models from the resulting point cloud and 3D printing a physical form. This device works completely independent of subjective user input resulting in two possible outcomes:1. Produce iterative versions of a specific object (e.g., the Statue of Liberty) increasing in detail and accuracy over time as the collective dataset (e.g., uploaded images of the statue) grows.2. Produce democratized versions of common objects (e.g., an apple) by aggregating a spectrum of tagged image results.This project demonstrates that an increase in readily available image data closes the gap between physical and digital perceptions of form through time. For example, when Source Form is asked to print the Statue of Liberty today and then print again 6 months from now, the later result will be more accurate and detailed than the previous version. As people continue to take pictures of the monument and upload them to social media, blogs and photo sharing sites, the database of images grows in quantity and quality. Because Source Form gathers a new dataset with each print, the resulting forms will always be evolving. The collection of prints the machine produces over time are cataloged and displayed in linear groupings, providing viewers an opportunity to see growth and change in physical space.In addition to rendering change over time, a snapshot of a more common object's web perception could be created. For example, when an image search for "apple" is performed, the results are a spectrum of condition and species from rotting crab apples to gleaming Granny-Smith's. Source Form aggregates all of these images into one model and outputs the collective web presence of an "apple". Characteristics of the model are guided by the frequency and order in response to the image web search. The resulting democratized forms are emblematic of the web's collective and popular perceptions.},
booktitle = {ACM SIGGRAPH 2019 Studio},
articleno = {13},
numpages = {2},
keywords = {3D printing, photogrammetry, hardware, sculpture, fine art, crowdsourcing, automation},
location = {Los Angeles, California},
series = {SIGGRAPH '19}
}

@article{10.1145/3072959.3073635,
author = {Schertler, Nico and Tarini, Marco and Jakob, Wenzel and Kazhdan, Misha and Gumhold, Stefan and Panozzo, Daniele},
title = {Field-Aligned Online Surface Reconstruction},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073635},
doi = {10.1145/3072959.3073635},
abstract = {Today's 3D scanning pipelines can be classified into two overarching categories: offline, high accuracy methods that rely on global optimization to reconstruct complex scenes with hundreds of millions of samples, and online methods that produce real-time but low-quality output, usually from structure-from-motion or depth sensors. The method proposed in this paper is the first to combine the benefits of both approaches, supporting online reconstruction of scenes with hundreds of millions of samples from high-resolution sensing modalities such as structured light or laser scanners. The key property of our algorithm is that it sidesteps the signed-distance computation of classical reconstruction techniques in favor of direct filtering, parametrization, and mesh and texture extraction. All of these steps can be realized using only weak notions of spatial neighborhoods, which allows for an implementation that scales approximately linearly with the size of each dataset that is integrated into a partial reconstruction. Combined, these algorithmic differences enable a drastically more efficient output-driven interactive scanning and reconstruction workflow, where the user is able to see the final quality field-aligned textured mesh during the entirety of the scanning procedure. Holes or parts with registration problems are displayed in real-time to the user and can be easily resolved by adding further localized scans, or by adjusting the input point cloud using our interactive editing tools with immediate visual feedback on the output mesh. We demonstrate the effectiveness of our algorithm in conjunction with a state-of-the-art structured light scanner and optical tracking system and test it on a large variety of challenging models.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {77},
numpages = {13},
keywords = {surface reconstruction, parameterization}
}

@inproceedings{10.1109/UCC.2014.157,
author = {Wehrle, Dennis and Liebetraut, Thomas and Valizada, Isgandar and Rechert, Klaus},
title = {Emulation-as-a-Service - Workflows and Infrastructure to Support Recomputable Science},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.157},
doi = {10.1109/UCC.2014.157},
abstract = {The computational age and its fast technological progress boosted research output of almost all disciplines. However, these early benefits come along with a burden: while the exchange of research data and ideas is easier and more effective than ever, assuring both short- and long-term access to fundamental scientific methods is more difficult than anticipated. In particular, functional access to data processing methods, tool-chains and scientific workflows is indispensable in order to verify and replicate research findings. This article is proposing emulation as a technique for generalization of data processing environments, serving as first step towards long-term accessibility of research data and associated methods. We present a Cloud-based emulation-as-a-service framework for publication and citation of scientific workflows and research data, since having re-usable processing environments, emulation can be used for technical verification of research data, i.e. Ensure minimal quality assurance like completeness and an explicit list of potential external dependencies, fundamental for future risk-assessment.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {962–967},
numpages = {6},
keywords = {long-term preservation, citing research data, workflows, re-use, verification, research data, replicability},
series = {UCC '14}
}

@inproceedings{10.1145/3338498.3358650,
author = {Matyunin, Nikolay and Wang, Yujue and Arul, Tolga and Kullmann, Kristian and Szefer, Jakub and Katzenbeisser, Stefan},
title = {MagneticSpy: Exploiting Magnetometer in Mobile Devices for Website and Application Fingerprinting},
year = {2019},
isbn = {9781450368308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338498.3358650},
doi = {10.1145/3338498.3358650},
abstract = {Recent studies have shown that aggregate CPU usage and power consumption traces on smartphones can leak information about applications running on the system or websites visited. In response, access to such data has been blocked for mobile applications starting from Android 8. In this work, we explore a new source of side-channel leakage for this class of attacks. Our method is based on the fact that electromagnetic activity caused by mobile processors leads to noticeable disturbances in magnetic sensor measurements on mobile devices, with the amplitude being proportional to the CPU workload. Therefore, recorded sensor data can be analyzed to reveal information about ongoing activities. The attack works on a number of devices: we evaluated 80 models of modern smartphones and tablets and observed the reaction of the magnetometer to the CPU activity on 56 of them. On selected devices we were able to successfully identify which application has been opened (with up to 90\% accuracy) or which web page has been loaded (up to 91\% accuracy). The presented side channel poses a significant risk to end users' privacy, as the sensor data can be recorded from native apps or even from web pages without user permissions. Finally, we discuss possible countermeasures to prevent the presented information leakage.},
booktitle = {Proceedings of the 18th ACM Workshop on Privacy in the Electronic Society},
pages = {135–149},
numpages = {15},
keywords = {magnetometer, hardware side channels, mobile security, smartphone sensors, information leakage, website fingerprinting, application fingerprinting},
location = {London, United Kingdom},
series = {WPES'19}
}

@inproceedings{10.1145/3297067.3297079,
author = {Al-Omair, Osamah M. and Huang, Shihong},
title = {A Comparative Study on Detection Accuracy of Cloud-Based Emotion Recognition Services},
year = {2018},
isbn = {9781450366052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297067.3297079},
doi = {10.1145/3297067.3297079},
abstract = {The ability of software systems adapting to human's input is a key element in the symbiosis of human-system co-adaptation, where human and software-based systems work together in a close partnership to achieve synergetic goals. This seamless integration will eliminate the barriers between human and machine. A critical requirement for co-adaptive systems is software system's ability to recognize human emotion, in which the system can detect and interpret users' emotions and adapt accordingly. There are numerous solutions that provide the technologies for emotion recognition. However, selecting an appropriate solution for a given task within a specific application domain can be challenging. The vast variation between these solutions makes the selecting task even more difficult. This paper compares cloud-based emotion recognition services offered by Amazon, Google, and Microsoft. These services detect human emotion through facial expression recognition with the utilization of computer vision. The focus of this paper is to measure the detection accuracy of these services. Accuracy is calculated based on the highest confidence rating returned by each service. All three services have been tested with the same dataset. This paper concludes with findings and recommendations based on our comparative analysis among these services.},
booktitle = {Proceedings of the 2018 International Conference on Signal Processing and Machine Learning},
pages = {142–148},
numpages = {7},
keywords = {Co-adaptive systems, Affective computing, Machine emotional intelligence, Machine learning, Facial expression recognition, Human-computer interaction, Emotion recognition},
location = {Shanghai, China},
series = {SPML '18}
}

@inproceedings{10.1145/3236024.3236043,
author = {Basios, Michail and Li, Lingbo and Wu, Fan and Kanthan, Leslie and Barr, Earl T.},
title = {Darwinian Data Structure Selection},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236043},
doi = {10.1145/3236024.3236043},
abstract = {Data structure selection and tuning is laborious but can vastly improve an application’s performance and memory footprint. Some data structures share a common interface and enjoy multiple implementations. We call them Darwinian Data Structures (DDS), since we can subject their implementations to survival of the fittest. We introduce ARTEMIS a multi-objective, cloud-based search-based optimisation framework that automatically finds optimal, tuned DDS modulo a test suite, then changes an application to use that DDS. ARTEMIS achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects and 30 uniformly sampled projects from GitHub. For execution time, CPU usage, and memory consumption, ARTEMIS finds at least one solution that improves all measures for 86\% (37/43) of the projects. The median improvement across the best solutions is 4.8\%, 10.1\%, 5.1\% for runtime, memory and CPU usage. These aggregate results understate ARTEMIS’s potential impact. Some of the benchmarks it improves are libraries or utility functions. Two examples are gson, a ubiquitous Java serialization framework, and xalan, Apache’s XML transformation tool. ARTEMIS improves gson by 16.5\%, 1\% and 2.2\% for memory, runtime, and CPU; ARTEMIS improves xalan’s memory consumption by 23.5\%. Every client of these projects will benefit from these performance improvements.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {118–128},
numpages = {11},
keywords = {Search-based Software Engineering, Software Analysis and Optimisation, Data Structure Optimisation, Genetic Improvement},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3328778.3372567,
author = {Bachrach, Mayra and Morreale, Patricia and Verdi, Gail},
title = {Improving the Outcomes of Hispanics in AP Computer Science},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3372567},
doi = {10.1145/3328778.3372567},
abstract = {This lightning talk describes a proof of concept research project funded by a Google CS Education (CS-ER) Research grant. The project focuses on pedagogical interventions aimed at improving the outcomes of English Language Learners (ELLs) in Advanced Placement Computer Science. The research underway examines the use of Sheltered Instruction (SI), a model from English as a Second Language (ESL) and bilingual education, used in mainstream classrooms across other content areas, in the AP CSA and AP CSP classroom. Strategies and pedagogy from the Sheltered Instruction model are being infused into AP Computer Science curriculum and used in classrooms in participating districts. The districts have been selected to include a range of ELLs and native English speakers. The impact of this approach will be measured by comparing the AP CS exam scores of students in the participating districts with the national and state AP CS exam scores. This lightning talk will focus on the pedagogy development which has taken place and preliminary findings from two cohorts of AP CS and AP CSA teachers, in particular the impact and changes to the teacher's development of CS education lessons and in-class lesson delivery. The project is an interdisciplinary collaboration between faculty from the School of Computer Science and the School of Curriculum and Teaching.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1411},
numpages = {1},
keywords = {academic-discourse, scaffolding, academic-language, equity, hispanics, pedagogy},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/3323503.3349543,
author = {Costa, Arthur F. da and D'Addio, Rafael M. and Fressato, Eduardo P. and Manzato, Marcelo G.},
title = {A Personalized Clustering-Based Approach Using Open Linked Data for Search Space Reduction in Recommender Systems},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3349543},
doi = {10.1145/3323503.3349543},
abstract = {Recommender systems use information about the users' preferences to define relatedness scores towards items. Regardless of the method, a noticeable problem is that the system is required to compute scores for a large amount of unknown items in the database, even though these items may not be related to a determined user. In this manuscript, we propose a technique called search space reduction for recommender systems (SSR4Rec) that reduces the number of unknown pairs the recommender must process. As a pre-processing step, we cluster related items and assign only the closest group to each user, producing a reduced set of unknown pairs. The distance between items, and between clusters and users, is computed by comparing item representations and user profiles built based on attributes extracted from the Linked Open Data cloud. We assess the quality of SSR4Rec by applying it into two well-known RS and comparing the results against the same recommenders without our pre-processing step, as well as against other related baselines. Results show a significant improvement in both ranking accuracy and computational time.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {409–416},
numpages = {8},
keywords = {recommender systems, search space reduction, clustering, linked open data},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/2835022.2835024,
author = {Zang, Andi and Chen, Xin and Trajcevski, Goce},
title = {Digital Terrain Model Generation Using LiDAR Ground Points},
year = {2015},
isbn = {9781450339735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835022.2835024},
doi = {10.1145/2835022.2835024},
abstract = {As the trend of autonomous self-driving cars is becoming more of a reality, High-quality navigation methods and tools become a paramount. This, in turn, is crucially dependent on High-definition maps, for which one of the enabling tools is high resolution Digital Terrain Model (DTM) -- the role and values of which have already been demonstrated even in the settings of manned cars. Traditional DTM generation methods have insurmountable barriers in creating centimeter-level resolution. In this paper, we propose a novel method for fully-automated, high precision DTM generation using the database generated and maintained in our existed dataset, and with no additional overheads in terms of extract labor and equipment cost. The input data is a point cloud captured by the vehicle-mount LiDAR devices which, naturally, has extremely large volume. We show how with Ground Points Processing and DTM Generation steps, we can generate a centimeter-resolution DTM and, as our experiments demonstrate, when compared to DTM form U.S. Geological Survey (USGS) and altitude data from a third party surveying dataset, our proposed DTM indeed provides a higher precision.},
booktitle = {Proceedings of the 1st International ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
pages = {9–15},
numpages = {7},
keywords = {LiDAR, GIS, Digital Terrain Model, Point cloud processing},
location = {Bellevue, WA, USA},
series = {UrbanGIS'15}
}

@inproceedings{10.17210/hcik.2016.01.298,
author = {Kim, Jungwoo and Kim, Hyesook and Choi, Jaeboong},
title = {Development of Smart Product, DUET Using SQFD and Storytelling},
year = {2016},
isbn = {9788968487910},
publisher = {Hanbit Media, Inc.},
address = {Seoul, KOR},
url = {https://doi.org/10.17210/hcik.2016.01.298},
doi = {10.17210/hcik.2016.01.298},
abstract = {This paper presents a smart product design process for a wearable device to provide empathy and fun to users. As the first step, keywords were extracted using open-coding methods from text WebData of online sites for wearable devices, Smardi, Sblog, and Wsite. The Smart Quality Function Deployment (SQFD) was then applied to prioritize the keywords and corresponding user requirements. The key user requirements such as 'separable band from core module' and 'function for media control' were then materialized into a wearable band, DUET, using rapid prototyping, and refined through three stages of user evaluation. DUET connectable to iOS and Android smartphones was introduced by a storytelling transmedia videoclip by experts with a theme of empathy and fun. It was also advertised on a cloud funding site, Indiegogo, and through a PPL in S entertainment program, and received positive responses. Further detailed analysis of user responses was performed for 72 days through the operation of facebook-DUET site and for 10 days through Google keyword marketing which derived various levels of user activities.},
booktitle = {Proceedings of HCI Korea},
pages = {298–306},
numpages = {9},
keywords = {RP, SQFD},
location = {Jeongseon, Republic of Korea},
series = {HCIK '16}
}

@inproceedings{10.1145/3183713.3190656,
author = {Jindal, Alekh and Qiao, Shi and Patel, Hiren and Yin, Zhicheng and Di, Jieming and Bag, Malay and Friedman, Marc and Lin, Yifung and Karanasos, Konstantinos and Rao, Sriram},
title = {Computation Reuse in Analytics Job Service at Microsoft},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3190656},
doi = {10.1145/3183713.3190656},
abstract = {Analytics-as-a-service, or analytics job service, is emerging as a new paradigm for data analytics, be it in a cloud environment or within enterprises. In this setting, users are not required to manage or tune their hardware and software infrastructure, and they pay only for the processing resources consumed per job. However, the shared nature of these job services across several users and teams leads to significant overlaps in partial computations, i.e., parts of the processing are duplicated across multiple jobs, thus generating redundant costs. In this paper, we describe a computation reuse framework, coined CLOUDVIEWS, which we built to address the computation overlap problem in Microsoft's SCOPE job service. We present a detailed analysis from our production workloads to motivate the computation overlap problem and the possible gains from computation reuse. The key aspects of our system are the following: (i) we reuse computations by creating materialized views over recurring workloads, i.e., periodically executing jobs that have the same script templates but process new data each time, (ii) we select the views to materialize using a feedback loop that reconciles the compile-time and run-time statistics and gathers precise measures of the utility and cost of each overlapping computation, and (iii) we create materialized views in an online fashion, without requiring an offline phase to materialize the overlapping computations.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {191–203},
numpages = {13},
keywords = {shared clouds, computation reuse, materialized views},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3394810.3394818,
author = {Ibarz, Jean and Lauer, Micha\"{e}l and Roy, Matthieu and Fabre, Jean-Charles and Fl\'{e}bus, Olivier},
title = {Optimizing Vehicle-to-Cloud Data Transfers Using Soft Real-Time Scheduling Concepts},
year = {2020},
isbn = {9781450375931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394810.3394818},
doi = {10.1145/3394810.3394818},
abstract = {The main promise of intelligent transportation systems (ITS) is that leveraging the information sensed by millions of vehicles will increase the quality of the user's experience. However, the unpredictable nature of road events, combined with a projected network overload, calls for a careful optimization of the vehicles' data transfers, taking into account spatio-temporal, safety and value constraints. In this article, we provide a methodical solution to optimize vehicle-to-cloud (V2C) data transfers, based on a series of steps. First, we show that this optimization problem can be modeled as a soft real-time scheduling problem. Second, we provide an extension of a classical algorithm for the generation of workloads, by increasing its coverage with regards to our use-case representation. Third, we estimate the bounds of an optimal clairvoyant algorithm in order to have a baseline for a fair comparison of existing scheduling algorithms. The results show that, within all these algorithms, one clearly outperforms the others regardless of the load rate. Interestingly, its performance gain increases when overload grows, and it can be implemented very efficiently, which makes it highly suitable for embedded systems.},
booktitle = {Proceedings of the 28th International Conference on Real-Time Networks and Systems},
pages = {161–171},
numpages = {11},
keywords = {data collection, event-based, reliability, embedded system, V2C, real-time, distributed sensing, IoT, mobile system},
location = {Paris, France},
series = {RTNS '20}
}

@inproceedings{10.1145/3178461.3178464,
author = {Tahat, Ashraf and Aburub, Ruba and Al-Zyoude, Aseel and Talhi, Chamseddine},
title = {A Smart City Environmental Monitoring Network and Analysis Relying on Big Data Techniques},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178464},
doi = {10.1145/3178461.3178464},
abstract = {A new integrated environmental monitoring system to carry-out real-time measurements on board a moving vehicle is presented. It is composed of an arbitrary number of Electronic Measurements Units (EMU), a smart phone application to relay collected data, and a cloud Central Processing Platform (CPP) to perform analysis utilizing big data techniques and algorithms. Each EMU consists of an electric circuit that incorporates an ultra violet (UV) sensor, an air particles concentration sensor, a temperature sensor and a humidity sensor that all interface to a microcontroller. Bluetooth is employed for communication between the EMU and the smart phone application, while a 3G/4G cellular communications network furnishes the wireless connectivity to the remote CPP. When the collected data reaches the designated cloud server (CPP), it is immediately stored for subsequent analysis. Finally, big data statistical analysis (clustering and classification), mapping and plotting are performed to deduce correlations and to facilitate inferencing. Moreover, the scalability and low-cost of selected components of this realistic system makes it very feasible for large scale deployments in the context of smart cities initiatives, ad-hoc designs, or educational projects.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {82–86},
numpages = {5},
keywords = {environment, air particles, temperature sensor, smart phone, telemetry, Big data, UV index},
location = {Casablanca, Morocco},
series = {ICSIM '18}
}

@inproceedings{10.1145/3447548.3470819,
author = {Barajas, Joel and Bhamidipati, Narayan and Shanahan, James G.},
title = {Online Advertising Incrementality Testing And Experimentation: Industry Practical Lessons},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470819},
doi = {10.1145/3447548.3470819},
abstract = {Online advertising has historically been approached as user targeting and ad-to-user matching problems within sophisticated optimization algorithms. As the research area and ad tech industry have progressed over the last couple of decades, advertisers have increasingly emphasized the causal effect estimation of their ads (aka incrementality) using controlled experiments (or A/B testing). Even though observational approaches have been derived in marketing science since the 80s including media mix models, the availability of online advertising personalization has enabled the deployment of more rigorous randomized controlled experiments with millions of individuals. These evolutions in marketing science, online advertising, and the ad tech industry have posed incredible challenges for engineers, data scientists, and marketers alike. With low effect percentage differences (or lift) and often sparse conversion rates, the development of incrementality testing platforms at scale suggests tremendous engineering challenges in the measurement precision and detailed implementation. Similarly, the correct interpretation of results addressing a business goal within the marketing science domain requires significant data science and experimentation research expertise. All these challenges on the ongoing evolution of the online advertising industry and the heterogeneity of its sources (social, paid search, native, programmatic, etc). In the current tutorial, we propose a practical, grounded view in the incrementality testing landscape, including: The business need Solutions in the literature Design and choices in the development of incrementality testing platform The testing cycle, case studies, and recommendations to effective results delivery Incrementality testing evolution in the industry We will provide first-hand lessons on developing and operationalizing such a platform in a major combined DSP and ad network; these are based on running tens of experiments for up to two months each over the last couple of years.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {4027–4028},
numpages = {2},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3427771.3427852,
author = {Ahmed, Shamim and Bons, Marc},
title = {Edge Computed NILM: A Phone-Based Implementation Using MobileNet Compressed by Tensorflow Lite},
year = {2020},
isbn = {9781450381918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427771.3427852},
doi = {10.1145/3427771.3427852},
abstract = {In the context of residential Non-intrusive load monitoring (NILM), the usual service deployment process consists of collecting data from a metering device to the cloud, run algorithms on the cloud and then display results in a Web front or in an App. This approach comes with two major problems: on the one hand, important resources are allocated to the cloud process (including maintenance) while selling the solution on a substantial subscription basis is still a challenge. On the other hand, end-users are more and more reluctant to see their personal data being uploaded. In order to propose an alternative, this research has focused on edge computed NILM, namely the possibility to run NILM algorithms on existing devices on the end-user side, such as a smart phone. A two-stage model development has been carried out to obtain good disaggregation accuracy with lower model size. In the first stage, an efficient deep learning algorithm (MobileNet) has been adopted to obtain an accurate and light weight model. In the second stage, TensorFlow Lite has been used to compress further, in order to reduce edge device memory usage and computing time. To deal with real-life diversity, we have built large and diverse training and testing sets based on a combination of HES, UKDALE and REFIT datasets. Disaggregation performance has been assessed for both models: before and after TensorFlow Lite compression. Comparative analysis has been performed to facilitate implementation choices.},
booktitle = {Proceedings of the 5th International Workshop on Non-Intrusive Load Monitoring},
pages = {44–48},
numpages = {5},
keywords = {TensorFlow Lite, Energy Disaggregation, Deep Learning, MobileNet, NILM, Edge Computing},
location = {Virtual Event, Japan},
series = {NILM'20}
}

@inproceedings{10.1145/3307650.3322273,
author = {Murali, Prakash and Linke, Norbert Matthias and Martonosi, Margaret and Abhari, Ali Javadi and Nguyen, Nhung Hong and Alderete, Cinthia Huerta},
title = {Full-Stack, Real-System Quantum Computer Studies: Architectural Comparisons and Design Insights},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322273},
doi = {10.1145/3307650.3322273},
abstract = {In recent years, Quantum Computing (QC) has progressed to the point where small working prototypes are available for use. Termed Noisy Intermediate-Scale Quantum (NISQ) computers, these prototypes are too small for large benchmarks or even for Quantum Error Correction (QEC), but they do have sufficient resources to run small benchmarks, particularly if compiled with optimizations to make use of scarce qubits and limited operation counts and coherence times. QC has not yet, however, settled on a particular preferred device implementation technology, and indeed different NISQ prototypes implement qubits with very different physical approaches and therefore widely-varying device and machine characteristics.Our work performs a full-stack, benchmark-driven hardware-software analysis of QC systems. We evaluate QC architectural possibilities, software-visible gates, and software optimizations to tackle fundamental design questions about gate set choices, communication topology, the factors affecting benchmark performance and compiler optimizations. In order to answer key cross-technology and cross-platform design questions, our work has built the first top-to-bottom toolflow to target different qubit device technologies, including superconducting and trapped ion qubits which are the current QC front-runners. We use our toolflow, TriQ, to conduct real-system measurements on seven running QC prototypes from three different groups, IBM, Rigetti, and University of Maryland. Overall, we demonstrate that leveraging microarchitecture details in the compiler improves program success rate up to 28x on IBM (geomean 3x), 2.3x on Rigetti (geomean 1.45x), and 1.47x on UMDTI (geomean 1.17x), compared to vendor toolflows. In addition, from these real-system experiences at QC's hardware-software interface, we make observations and recommendations about native and software-visible gates for different QC technologies, as well as communication topologies, and the value of noise-aware compilation even on lower-noise platforms. This is the largest cross-platform real-system QC study performed thus far; its results have the potential to inform both QC device and compiler design going forward.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {527–540},
numpages = {14},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3365265.3365270,
author = {Ellwein, Carsten and Schmidt, Alexander and Lechler, Armin and Riedel, Oliver},
title = {Distributed Manufacturing: A Vision about Shareconomy in the Manufacturing Industry},
year = {2019},
isbn = {9781450372886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365265.3365270},
doi = {10.1145/3365265.3365270},
abstract = {Four major trends in recent manufacturing technology have been identified and are introduced. Those trends are mass customization, shareconomy, digitalization and cloud manufacturing. The impact of those trends on manufacturing paradigms has been evaluated and three possible paradigms have been identified. Those manufacturing paradigms are separation of design and manufacturing, collaboration across company borders and on-site production. The separation of design and manufacturing does empower customers with regard to the product and does allow true mass customization where customers are included in the product description process. The collaboration across company borders does empower customers in regard of the process and lets them choose their contractual partner for every production step. The following on-site-production focuses on the throughput and thus on the delivery time by re-location of the production into the end-customers' daily field of action. Each paradigm is explained, the vision of possible future implementations is drawn and the possible benefits are outlined. However, the technical realization of those paradigms is yet not fully feasible due to still unsolved problems and challenges. Therefore, a research agenda has been composed to list and address those deficits. The main deficits that have been identified are the lack of standardized data models, an integrated and automated toolchain, the protection of intellectual property and the compliance with quality demands.},
booktitle = {Proceedings of the 2019 3rd International Conference on Automation, Control and Robots},
pages = {90–95},
numpages = {6},
keywords = {Distributed Manufacturing, Prosumption, Shareconomy, Manufacturing Access Point, Cloud Manufacturing, Mass Customization},
location = {Prague, Czech Republic},
series = {ICACR 2019}
}

@article{10.1145/3386361,
author = {Seeger, Jan and Br\"{o}ring, Arne and Carle, Georg},
title = {Optimally Self-Healing IoT Choreographies},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3386361},
doi = {10.1145/3386361},
abstract = {In the industrial Internet of Things domain, applications are moving from the Cloud into the Edge, closer to the devices producing and consuming data. This means that applications move from the scalable and homogeneous Cloud environment into a potentially constrained heterogeneous Edge network. Making Edge applications reliable enough to fulfill Industry 4.0 use cases remains an open research challenge. Maintaining operation of an Edge system requires advanced management techniques to mitigate the failure of devices. This article tackles this challenge with a twofold approach: (1) a policy-enabled failure detector that enables adaptable failure detection and (2) an allocation component for the efficient selection of failure mitigation actions. The parameters and performance of the failure detection approach are evaluated, and the performance of an energy-efficient allocation technique is measured. Finally, a vision for a complete system and an example use case are presented.},
journal = {ACM Trans. Internet Technol.},
month = {jul},
articleno = {27},
numpages = {20},
keywords = {optimization, IOT, failure detection}
}

@inproceedings{10.1145/3208806.3208816,
author = {Discher, S\"{o}ren and Richter, Rico and D\"{o}llner, J\"{u}rgen},
title = {A Scalable WebGL-Based Approach for Visualizing Massive 3D Point Clouds Using Semantics-Dependent Rendering Techniques},
year = {2018},
isbn = {9781450358002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208806.3208816},
doi = {10.1145/3208806.3208816},
abstract = {3D point cloud technology facilitates the automated and highly detailed digital acquisition of real-world environments such as assets, sites, cities, and countries; the acquired 3D point clouds represent an essential category of geodata used in a variety of geoinformation applications and systems. In this paper, we present a web-based system for the interactive and collaborative exploration and inspection of arbitrary large 3D point clouds. Our approach is based on standard WebGL on the client side and is able to render 3D point clouds with billions of points. It uses spatial data structures and level-of-detail representations to manage the 3D point cloud data and to deploy out-of-core and web-based rendering concepts. By providing functionality for both, thin-client and thick-client applications, the system scales for client devices that are vastly different in computing capabilities. Different 3D point-based rendering techniques and post-processing effects are provided to enable task-specific and data-specific filtering and highlighting, e.g., based on per-point surface categories or temporal information. A set of interaction techniques allows users to collaboratively work with the data, e.g., by measuring distances and areas, by annotating, or by selecting and extracting data subsets. Additional value is provided by the system's ability to display additional, context-providing geodata alongside 3D point clouds and to integrate task-specific processing and analysis operations. We have evaluated the presented techniques and the prototype system with different data sets from aerial, mobile, and terrestrial acquisition campaigns with up to 120 billion points to show their practicality and feasibility.},
booktitle = {Proceedings of the 23rd International ACM Conference on 3D Web Technology},
articleno = {19},
numpages = {9},
keywords = {point-based rendering, 3D point clouds, web-based rendering},
location = {Pozna\'{n}, Poland},
series = {Web3D '18}
}

@inproceedings{10.1145/3132847.3132931,
author = {Rong, Yu and Cheng, Hong},
title = {Minimizing Dependence between Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132931},
doi = {10.1145/3132847.3132931},
abstract = {In recent years, modeling the relation between two graphs has received unprecedented attention from researchers due to its wide applications in many areas, such as social analysis and bioinformatics. The nature of relations between two graphs can be divided into two categories: the vertex relation and the link relation. Many studies focus on modeling the vertex relation between graphs and try to find the vertex correspondence between two graphs. However, the link relation between graphs has not been fully studied. Specifically, we model the cross-graph link relation as cross-graph dependence, which reflects the dependence of a vertex in one graph on a vertex in the other graph. A generic problem, called Graph Dependence Minimization (GDM), is defined as: given two graphs with cross-graph dependence, how to select a subset of vertexes from one graph and copy them to the other, so as to minimize the cross-graph dependence. Many real applications can benefit from the solution to GDM. Examples include reducing the cross-language links in online encyclopedias, optimizing the cross-platform communication cost between different cloud services, and so on. This problem is trivial if we can select as many vertexes as we want to copy. But what if we can only choose a limited number of vertexes to copy so as to make the two graphs as independent as possible? We formulate GDM with a budget constraint into a combinatorial optimization problem, which is proven to be NP-hard. We propose two algorithms to solve GDM. Firstly, we prove the submodularity of the objective function of GDM and adopt the size-constrained submodular minimization (SSM) algorithm to solve it. Since the SSM-based algorithm cannot scale to large graphs, we design a heuristic algorithm with a provable approximation guarantee. We prove that the error achieved by the heuristic algorithm is bounded by an additive factor which is proportional to the square of the given budget. Extensive experiments on both synthetic and real-world graphs show that the proposed algorithms consistently outperform the well-studied graph centrality measure based solutions. Furthermore, we conduct a case study on the Wikipedia graphs with millions of vertexes and links to demonstrate the potential of GDM to solve real-world problems.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1827–1836},
numpages = {10},
keywords = {submodular minimization, graph analytics, graph dependence minimization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3361821.3361825,
author = {Podhoranyi, Michal and Vojacek, Lukas},
title = {Social Media Data Processing Infrastructure by Using Apache Spark Big Data Platform: Twitter Data Analysis},
year = {2019},
isbn = {9781450372411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361821.3361825},
doi = {10.1145/3361821.3361825},
abstract = {Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.},
booktitle = {Proceedings of the 2019 4th International Conference on Cloud Computing and Internet of Things},
pages = {1–6},
numpages = {6},
keywords = {Apache Spark, social network data, data processing architecture, Twitter},
location = {Tokyo, Japan},
series = {CCIOT '19}
}

@inproceedings{10.1145/3117811.3131250,
author = {Hogan, Mary and Esposito, Flavio},
title = {Poster: A Portfolio Theory Approach to Edge Traffic Engineering via Bayesian Networks},
year = {2017},
isbn = {9781450349161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3117811.3131250},
doi = {10.1145/3117811.3131250},
abstract = {One of the main goals of mobile edge computing is to support new generation latency-sensitive networked applications. To manage such demanding applications, a fine-grained control of end-to-end paths is imperative. End-to-end delay estimation and forecast techniques were essential traffic engineering tools even before the mobile edge computing paradigm pushed the cloud closer to the end user. In this paper, we model the path selection problem for edge traffic engineering using a risk minimization technique inspired by portfolio theory in economics, and we use machine learning to estimate the risk of a path. In particular, using real latency time series measurements, collected with and without the GENI testbed, we compare four short-horizon latency estimation techniques, commonly used by the finance community to estimate prices of volatile financial instruments. Our initial results suggest that a Bayesian Network approach may lead to good latency estimation performance and open a few research questions that we are currently exploring.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking},
pages = {555–557},
numpages = {3},
keywords = {Bayesian network, latency prediction, portfolio theory, machine learning, edge computing},
location = {Snowbird, Utah, USA},
series = {MobiCom '17}
}

@article{10.1145/3319618,
author = {Belson, Bruce and Holdsworth, Jason and Xiang, Wei and Philippa, Bronson},
title = {A Survey of Asynchronous Programming Using Coroutines in the Internet of Things and Embedded Systems},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3319618},
doi = {10.1145/3319618},
abstract = {Many Internet of Things and embedded projects are event driven, and therefore require asynchronous and concurrent programming. Current proposals for C++20 suggest that coroutines will have native language support. It is timely to survey the current use of coroutines in embedded systems development. This article investigates existing research which uses or describes coroutines on resource-constrained platforms. The existing research is analysed with regard to: software platform, hardware platform, and capacity; use cases and intended benefits; and the application programming interface design used for coroutines. A systematic mapping study was performed, to select studies published between 2007 and 2018 which contained original research into the application of coroutines on resource-constrained platforms. An initial set of 566 candidate papers, collated from on-line databases, were reduced to only 35 after filters were applied, revealing the following taxonomy. The C 8 C++ programming languages were used by 22 studies out of 35. As regards hardware, 16 studies used 8- or 16-bit processors while 13 used 32-bit processors. The four most common use cases were concurrency (17 papers), network communication (15), sensor readings (9), and data flow (7). The leading intended benefits were code style and simplicity (12 papers), scheduling (9), and efficiency (8). A wide variety of techniques have been used to implement coroutines, including native macros, additional tool chain steps, new language features, and non-portable assembly language. We conclude that there is widespread demand for coroutines on resource-constrained devices. Our findings suggest that there is significant demand for a formalised, stable, well-supported implementation of coroutines in C++, designed with consideration of the special needs of resource-constrained devices, and further that such an implementation would bring benefits specific to such devices.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jun},
articleno = {21},
numpages = {21},
keywords = {asynchronous, scheduling, Embedded, direct style, resource-constrained}
}

@inproceedings{10.5555/3374138.3374140,
author = {Weyl, Julius and Lenfers, Ulfia A. and Clemen, Thomas and Glake, Daniel and Panse, Fabian and Ritter, Norbert},
title = {Large-Scale Traffic Simulation for Smart City Planning with Mars},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Understanding individual mobility in larger cities is an important success factor for future smart cities. Related simulation scenarios incorporate enormous numbers of agents, with the disadvantage of long run times. In order to provide large-scale and multimodal traffic simulations, we developed MARS V3. Adapting the Modeling and Simulation as a Service (MSaaS) paradigm, a seamless workflow can be provided to the modeling community. An integrated domain-specific language allows model descriptions without a technical overhead. For this study, selected parts of an individual-based traffic model of the City of Hamburg, Germany, were taken as an example. The entire workflow from model development, open data integration, simulation, and result analysis will be described and evaluated. Performance was measured for local and cloud-based simulation execution for up to one million agents. First results show that this concept can be utilized for building decision support systems for smart cities in the near future.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {2},
numpages = {12},
keywords = {domain-specific-language, MSaaS, individual mobility, agent-based, large-scale traffic scenario},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@article{10.1145/3403954,
author = {Sharma, Pratima and Jindal, Rajni and Borah, Malaya Dutta},
title = {Blockchain Technology for Cloud Storage: A Systematic Literature Review},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3403954},
doi = {10.1145/3403954},
abstract = {The demand for Blockchain innovation and the significance of its application has inspired ever-progressing exploration in various scientific and practical areas. Even though it is still in the initial testing stage, the blockchain is being viewed as a progressive solution to address present-day technology concerns, such as decentralization, identity, trust, character, ownership of data, and information-driven choices. Simultaneously, the world is facing an increase in the diversity and quantity of digital information produced by machines and users. While effectively looking for the ideal approach to storing and processing cloud data, the blockchain innovation provides significant inputs. This article reviews the application of blockchain technology for securing cloud storage.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {89},
numpages = {32},
keywords = {decentralization, cloud storage, Blockchain technology, cloud security, cloud computing}
}

@inproceedings{10.1145/2749246.2749252,
author = {Cheng, Yue and Iqbal, M. Safdar and Gupta, Aayush and Butt, Ali R.},
title = {CAST: Tiering Storage for Data Analytics in the Cloud},
year = {2015},
isbn = {9781450335508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749246.2749252},
doi = {10.1145/2749246.2749252},
abstract = {Enterprises are increasingly moving their big data analytics to the cloud with the goal of reducing costs without sacrificing application performance. Cloud service providers offer their tenants a myriad of storage options, which while flexible, makes the choice of storage deployment non trivial. Crafting deployment scenarios to leverage these choices in a cost-effective manner - under the unique pricing models and multi-tenancy dynamics of the cloud environment - presents unique challenges in designing cloud-based data analytics frameworks.In this paper, we propose CAST, a Cloud Analytics Storage Tiering solution that cloud tenants can use to reduce monetary cost and improve performance of analytics workloads. The approach takes the first step towards providing storage tiering support for data analytics in the cloud. CAST performs offline workload profiling to construct job performance prediction models on different cloud storage services, and combines these models with workload specifications and high-level tenant goals to generate a cost-effective data placement and storage provisioning plan. Furthermore, we build CAST++ to enhance CAST's optimization model by incorporating data reuse patterns and across-jobs interdependencies common in realistic analytics workloads. Tests with production workload traces from Facebook and a 400-core Google Cloud based Hadoop cluster demonstrate that CAST++ achieves 1.21X performance and reduces deployment costs by 51.4\% compared to local storage configuration.},
booktitle = {Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {45–56},
numpages = {12},
keywords = {big data analytics, cloud computing, mapreduce, storage tiering},
location = {Portland, Oregon, USA},
series = {HPDC '15}
}

@inproceedings{10.1145/3318216.3363300,
author = {Chen, Qi and Ma, Xu and Tang, Sihai and Guo, Jingda and Yang, Qing and Fu, Song},
title = {F-Cooper: Feature Based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363300},
doi = {10.1145/3318216.3363300},
abstract = {Autonomous vehicles are heavily reliant upon their sensors to perfect the perception of surrounding environments, however, with the current state of technology, the data which a vehicle uses is confined to that from its own sensors. Data sharing between vehicles and/or edge servers is limited by the available network bandwidth and the stringent real-time constraints of autonomous driving applications. To address these issues, we propose a point cloud feature based cooperative perception framework (F-Cooper) for connected autonomous vehicles to achieve a better object detection precision. Not only will feature based data be sufficient for the training process, we also use the features' intrinsically small size to achieve real-time edge computing, without running the risk of congesting the network. Our experiment results show that by fusing features, we are able to achieve a better object detection result, around 10\% improvement for detection within 20 meters and 30\% for further distances, as well as achieve faster edge computing with a low communication delay, requiring 71 milliseconds in certain feature selections. To the best of our knowledge, we are the first to introduce feature-level data fusion to connected autonomous vehicles for the purpose of enhancing object detection and making real-time edge computing on inter-vehicle data feasible for autonomous vehicles.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {88–100},
numpages = {13},
keywords = {feature fusion, edge computing, connected autonomous vehicle},
location = {Arlington, Virginia},
series = {SEC '19}
}

@inproceedings{10.1145/3339825.3391862,
author = {Hansen, Henry Haugsten and Muchallil, Sayed and Griwodz, Carsten and Sillerud, Vetle and Johanssen, Fredrik},
title = {Dense LIDAR Point Clouds from Room-Scale Scans},
year = {2020},
isbn = {9781450368452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339825.3391862},
doi = {10.1145/3339825.3391862},
abstract = {LiDARs can capture distances with high accuracy and should be very useful to create point clouds that provide highly detailed representations of an environment. If these reconstructions are meant as baseline or ground truth for other algorithms, they must have a high density and accuracy.Currently available LiDARs do still face some limitations. Either they have a limited range, or they have a rather limited resolution in one or more dimensions. As a consequence, all of them have to undergo motion to capture a larger environment. While some systems follow extremely well-predictable motion paths such as satellite trajectories or robotic arms, others require more spontaneous and flexible motion. These systems use either visual simultaneous localization and mapping (vSLAM), GPS or IMU to achieve this, but they are generally designed in such a way that human intervention is required during the creation of high-quality point clouds.In this paper, we make use of a rotating LiDAR with an attached IMU to create dense point clouds of room-scale environments with the base accuracy of the LiDAR by compensating for the various inaccuracies that are introduced by the LiDAR's motion. The resulting dense scans are suitable as ground truths for other techniques because we retain the error distribution of the LiDAR itself through the densification.In contrast to other works, we do not aim at a visually pleasing or easily meshable result and we can therefore avoid potentially inaccurate assumptions about the flatness of surfaces. We take a two-step approach. First, we densify from a stationary position changing only the LiDAR's pitch. Second, we add free motion to expose obstructed views. We show that motion paths determined by repeated Iterative Closest Point (ICP) as well as image matching on height maps can be used to create feasible priors for densification using ICP.},
booktitle = {Proceedings of the 11th ACM Multimedia Systems Conference},
pages = {88–98},
numpages = {11},
keywords = {machine learning, point clouds, densification, height maps},
location = {Istanbul, Turkey},
series = {MMSys '20}
}

@inproceedings{10.1145/2726935.2726938,
author = {Rewari, Gaurav and Kapoor, Rahul},
title = {Analytics Applications on the Cloud: Business Potential, Solution Requirements, and Research Opportunities (Invited Talk)},
year = {2015},
isbn = {9781450334051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2726935.2726938},
doi = {10.1145/2726935.2726938},
abstract = {The rapid adoption of cloud applications like SalesForce, ServiceNow, NetSuite, Marketo etc. has opened up an interesting opportunity for vendors of analytical applications and BI middleware as these modern cloud data sources are not well served by existing BI Tools and applications. For customers, part or all of their data moving to the cloud also means any existing on-premise warehouses and analytical applications are partially or fully defunct, and with the increased acceptance of moving application level functionality to the cloud there is little interest in upgrading the defunct on-premise offerings. A new class of software vendors, including Numerify, step into this gap by providing a cloud based, end-to-end solution for data extraction, transformation and warehouse based analytical applications, directly to the business user in select domains (e.g. IT Service Management, Human Resources and Financials). This talk summarizes the overall business potential for analytics on the cloud, expands on “analytical applications” with some examples, discusses engineering challenges in implementing cloud based analytics solutions highlighting some advances in ETL techniques, and points to potential research opportunities in this space.},
booktitle = {Proceedings of the 2nd Workshop on Parallel Programming for Analytics Applications},
pages = {3},
numpages = {1},
keywords = {MDM, Warehousing, Data Integration, Analytical Applications, Data Quality, Business Intelligence, ETL, Cloud Software},
location = {San Francisco, CA, USA},
series = {PPAA 2015}
}

@article{10.1145/2903146,
author = {Mencagli, Gabriele},
title = {A Game-Theoretic Approach for Elastic Distributed Data Stream Processing},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/2903146},
doi = {10.1145/2903146},
abstract = {Distributed data stream processing applications are structured as graphs of interconnected modules able to ingest high-speed data and to transform them in order to generate results of interest. Elasticity is one of the most appealing features of stream processing applications. It makes it possible to scale up/down the allocated computing resources on demand in response to fluctuations of the workload. On clouds, this represents a necessary feature to keep the operating cost at affordable levels while accommodating user-defined QoS requirements. In this article, we study this problem from a game-theoretic perspective. The control logic driving elasticity is distributed among local control agents capable of choosing the right amount of resources to use by each module. In a first step, we model the problem as a noncooperative game in which agents pursue their self-interest. We identify the Nash equilibria and we design a distributed procedure to reach the best equilibrium in the Pareto sense. As a second step, we extend the noncooperative formulation with a decentralized incentive-based mechanism in order to promote cooperation by moving the agreement point closer to the system optimum. Simulations confirm the results of our theoretical analysis and the quality of our strategies.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {jun},
articleno = {13},
numpages = {34},
keywords = {game theory, data stream processing, elasticity, Autonomic computing}
}

@article{10.1145/3414685.3417812,
author = {Jones, R. Kenny and Barton, Theresa and Xu, Xianghao and Wang, Kai and Jiang, Ellen and Guerrero, Paul and Mitra, Niloy J. and Ritchie, Daniel},
title = {ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417812},
doi = {10.1145/3414685.3417812},
abstract = {Manually authoring 3D shapes is difficult and time consuming; generative models of 3D shapes offer compelling alternatives. Procedural representations are one such possibility: they offer high-quality and editable results but are difficult to author and often produce outputs with limited diversity. On the other extreme are deep generative models: given enough data, they can learn to generate any class of shape but their outputs have artifacts and the representation is not editable.In this paper, we take a step towards achieving the best of both worlds for novel 3D shape synthesis. First, we propose ShapeAssembly, a domain-specific "assembly-language" for 3D shape structures. ShapeAssembly programs construct shape structures by declaring cuboid part proxies and attaching them to one another, in a hierarchical and symmetrical fashion. ShapeAssembly functions are parameterized with continuous free variables, so that one program structure is able to capture a family of related shapes.We show how to extract ShapeAssembly programs from existing shape structures in the PartNet dataset. Then, we train a deep generative model, a hierarchical sequence VAE, that learns to write novel ShapeAssembly programs. Our approach leverages the strengths of each representation: the program captures the subset of shape variability that is interpretable and editable, and the deep generative model captures variability and correlations across shape collections that is hard to express procedurally.We evaluate our approach by comparing the shapes output by our generated programs to those from other recent shape structure synthesis models. We find that our generated shapes are more plausible and physically-valid than those of other methods. Additionally, we assess the latent spaces of these models, and find that ours is better structured and produces smoother interpolations. As an application, we use our generative model and differentiable program interpreter to infer and fit shape programs to unstructured geometry, such as point clouds.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {234},
numpages = {20},
keywords = {neurosymbolic models, procedural modeling, deep learning, shape synthesis, shape analysis, generative models}
}

@article{10.1145/3369818,
author = {Qin, Xin and Chen, Yiqiang and Wang, Jindong and Yu, Chaohui},
title = {Cross-Dataset Activity Recognition via Adaptive Spatial-Temporal Transfer Learning},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3369818},
doi = {10.1145/3369818},
abstract = {Human activity recognition (HAR) aims at recognizing activities by training models on the large quantity of sensor data. Since it is time-consuming and expensive to acquire abundant labeled data, transfer learning becomes necessary for HAR by transferring knowledge from existing domains. However, there are two challenges existing in cross-dataset activity recognition. The first challenge is source domain selection. Given a target task and several available source domains, it is difficult to determine how to select the most similar source domain to the target domain such that negative transfer can be avoided. The second one is accurately activity transfer. After source domain selection, how to achieve accurate knowledge transfer between the selected source and the target domain remains another challenge. In this paper, we propose an Adaptive Spatial-Temporal Transfer Learning (ASTTL) approach to tackle both of the above two challenges in cross-dataset HAR. ASTTL learns the spatial features in transfer learning by adaptively evaluating the relative importance between the marginal and conditional probability distributions. Besides, it captures the temporal features via incremental manifold learning. Therefore, ASTTL can learn the adaptive spatial-temporal features for cross-dataset HAR and can be used for both source domain selection and accurate activity transfer. We evaluate the performance of ASTTL through extensive experiments on 4 public HAR datasets, which demonstrates its effectiveness. Furthermore, based on ASTTL, we design and implement an adaptive cross-dataset HAR system called Client-Cloud Collaborative Adaptive Activity Recognition System (3C2ARS) to perform HAR in the real environment. By collecting activities in the smartphone and transferring knowledge in the cloud server, ASTTL can significantly improve the performance of source domain selection and accurate activity transfer.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {148},
numpages = {25},
keywords = {Domain Adaptation, Transfer Learning, Human Activity Recognition, Cross-Dataset Recognition}
}

@inproceedings{10.1145/2665970.2665977,
author = {Jung, Seunghwan and Lee, Sejoon and Kim, Sangwoo and Nam, Hojung},
title = {Identification of Genomic Features in the Classification of Loss- and Gain-of-Function Mutation: [Extended Abstract]},
year = {2014},
isbn = {9781450312752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2665970.2665977},
doi = {10.1145/2665970.2665977},
abstract = {In this work, we propose a comprehensive analysis of the genomic features of the human in mutations to classify loss-of-function (LoF) and gain-of-function (GoF) mutations. Through these genetic mutations, a protein can lose its native function, or it can confer a new function. However, when a mutation occurs, it is difficult to determine whether it will result in a LoF or a GoF. Therefore, we propose a study that analyzes the human genomic features of LoF and GoF instances to find features that can be used to classify LoF and GoF mutations. In order to collect experimentally verified LoF and GoF mutational information, we obtained 816 LoF mutations and 474 GoF mutations from a literature text-mining process. Next, with data-preprocessing steps, 258 LoF and 129 GoF mutations remained for a further analysis. We analyzed the properties of these LoF and GoF mutations. Among the properties, we selected features which show different tendencies between the two groups. Finally, we implemented classifications using support vector machine, random forest, and logistic regression methods to confirm whether or not these features can identify LoF and GoF mutations. By implementing classifications with the selected features, it is demonstrated that the selected features have good discriminative power.},
booktitle = {Proceedings of the ACM 8th International Workshop on Data and Text Mining in Bioinformatics},
pages = {23},
numpages = {1},
keywords = {gain-of-function, loss-of-function},
location = {Shanghai, China},
series = {DTMBIO '14}
}

@inproceedings{10.1145/3427921.3450256,
author = {Ogden, Samuel S. and Kong, Xiangnan and Guo, Tian},
title = {PieSlicer: Dynamically Improving Response Time for Cloud-Based CNN Inference},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450256},
doi = {10.1145/3427921.3450256},
abstract = {Executing deep-learning inference on cloud servers enables the usage of high complexity models for mobile devices with limited resources. However, pre-execution time-the time it takes to prepare and transfer data to the cloud-is variable and can take orders of magnitude longer to complete than inference execution itself. This pre-execution time can be reduced by dynamically deciding the order of two essential steps, preprocessing and data transfer, to better take advantage of on-device resources and network conditions. In this work, we present PieSlicer, a system for making dynamic preprocessing decisions to improve cloud inference performance using linear regression models. PieSlicer then leverages these models to select the appropriate preprocessing location. We show that for image classification applications PieSlicer reduces median and 99th percentile pre-execution time by up to 50.2ms and 217.2ms respectively when compared to static preprocessing methods.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {249–256},
numpages = {8},
keywords = {cloud inference, performance modeling, mobile deep learning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.14778/3484224.3484227,
author = {Mailis, Theofilos and Kotidis, Yannis and Christoforidis, Stamatis and Kharlamov, Evgeny and Ioannidis, Yannis},
title = {View Selection over Knowledge Graphs in Triple Stores},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484227},
doi = {10.14778/3484224.3484227},
abstract = {Knowledge Graphs (KGs) are collections of interconnected and annotated entities that have become powerful assets for data integration, search enhancement, and other industrial applications. Knowledge Graphs such as DBPEDIA may contain billion of triple relations and are intensively queried with millions of queries per day. A prominent approach to enhance query answering on Knowledge Graph databases is View Materialization, ie., the materialization of an appropriate set of computations that will improve query performance.We study the problem of view materialization and propose a view selection methodology for processing query workloads with more than a million queries. Our approach heavily relies on subgraph pattern mining techniques that allow to create efficient summarizations of massive query workloads while also identifying the candidate views for materialization. In the core of our work is the correspondence between the view selection problem to that of Maximizing a Nondecreasing Submodular Set Function Subject to a Knapsack Constraint. The latter leads to a tractable view-selection process for native triple stores that allows a (1 - e---1)-approximation of the optimal selection of views. Our experimental evaluation shows that all the steps of the view-selection process are completed in a few minutes, while the corresponding rewritings accelerate 67.68\% of the queries in the DBPEDIA query workload. Those queries are executed in 2.19\% of their initial time on average.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3281–3294},
numpages = {14}
}

@article{10.1162/EVCO_a_00176,
author = {Garza-Fabre, Mario and Kandathil, Shaun M. and Handl, Julia and Knowles, Joshua and Lovell, Simon C.},
title = {Generating, Maintaining, and Exploiting Diversity in a Memetic Algorithm for Protein Structure Prediction},
year = {2016},
issue_date = {Winter 2016},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {24},
number = {4},
issn = {1063-6560},
url = {https://doi.org/10.1162/EVCO_a_00176},
doi = {10.1162/EVCO_a_00176},
abstract = {Computational approaches to de novo protein tertiary structure prediction, including those based on the preeminent "fragment-assembly" technique, have failed to scale up fully to larger proteins on the order of 100 residues and above. A number of limiting factors are thought to contribute to the scaling problem over and above the simple combinatorial explosion, but the key ones relate to the lack of exploration of properly diverse protein folds, and to an acute form of "deception" in the energy function, whereby low-energy conformations do not reliably equate with native structures. In this article, solutions to both of these problems are investigated through a multistage memetic algorithm incorporating the successful Rosetta method as a local search routine. We found that specialised genetic operators significantly add to structural diversity and that this translates well to reaching low energies. The use of a generalised stochastic ranking procedure for selection enables the memetic algorithm to handle and traverse deep energy wells that can be considered deceptive, which further adds to the ability of the algorithm to obtain a much-improved diversity of folds. The results should translate to a tangible improvement in the performance of protein structure prediction algorithms in blind experiments such as CASP, and potentially to a further step towards the more challenging problem of predicting the three-dimensional shape of large proteins.},
journal = {Evol. Comput.},
month = {dec},
pages = {577–607},
numpages = {31},
keywords = {Memetic algorithms., Protein structure prediction, Fragment assembly}
}

@inproceedings{10.1145/3311790.3400853,
author = {Choi, In Kwon and Abeysinghe, Eroma and Coulter, Eric and Marru, Suresh and Pierce, Marlon and Liu, Xiaowen},
title = {TopPIC Gateway: A Web Gateway for Top-Down Mass Spectrometry Data Interpretation},
year = {2020},
isbn = {9781450366892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311790.3400853},
doi = {10.1145/3311790.3400853},
abstract = {Top-down mass spectrometry-based proteomics has become the method of choice for identifying and quantifying intact proteoforms in biological samples. We present a web-based gateway for TopPIC suite, a widely used software suite consisting of four software tools for top-down mass spectrometry data interpretation: TopFD, TopPIC, TopMG, and TopDiff. The gateway enables the community to use heterogeneous collection of computing resources that includes high performance computing clusters at Indiana University and virtual clusters on XSEDE’s Jetstream Cloud resource for top-down mass spectral data analysis using TopPIC suite. The gateway will be a useful resource for proteomics researchers and students who have limited access to high-performance computing resources or who are not familiar with interacting with server-side supercomputers.},
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {461–464},
numpages = {4},
keywords = {Proteomics, Apache Airavata, SciGaP, XSEDE, Top-down mass spectrometry, Science Gateways},
location = {Portland, OR, USA},
series = {PEARC '20}
}

@inproceedings{10.1145/2950290.2983930,
author = {Gulzar, Muhammad Ali and Interlandi, Matteo and Condie, Tyson and Kim, Miryung},
title = {BigDebug: Interactive Debugger for Big Data Analytics in Apache Spark},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983930},
doi = {10.1145/2950290.2983930},
abstract = {To process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. In terms of debugging, DISC systems support post-mortem log analysis but do not provide interactive debugging features in realtime. This tool demonstration paper showcases a set of concrete usecases on how BigDebug can help debug Big Data Applications by providing interactive, realtime debug primitives. To emulate interactive step-wise debugging without reducing throughput, BigDebug provides simulated breakpoints to enable a user to inspect a program without actually pausing the entire computation. To minimize unnecessary communication and data transfer, BigDebug provides on-demand watchpoints that enable a user to retrieve intermediate data using a guard and transfer the selected data on demand. To support systematic and efficient trial-and-error debugging, BigDebug also enables users to change program logic in response to an error at runtime and replay the execution from that step. BigDebug is available for download at http://web.cs.ucla.edu/~miryung/software.html},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {1033–1037},
numpages = {5},
keywords = {interactive tools, fault localization and recovery, big data analytics, data-intensive scalable computing (DISC), Debugging},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3378679.3394534,
author = {Cooke, Ryan A. and Fahmy, Suhaib A.},
title = {Quantifying the Latency Benefits of Near-Edge and in-Network FPGA Acceleration},
year = {2020},
isbn = {9781450371322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378679.3394534},
doi = {10.1145/3378679.3394534},
abstract = {Transmitting data to cloud datacenters in distributed IoT applications introduces significant communication latency, but is often the only feasible solution when source nodes are computationally limited. To address latency concerns, Cloudlets, in-network computing, and more capable edge nodes are all being explored as a way of moving processing capability towards the edge of the network. Hardware acceleration using Field programmable gate arrays (FPGAs) is also seeing increased interest due to reduced computation time and improved efficiency. This paper evaluates the the implications of these offloading approaches using a case study neural network based image classification application, quantifying both the computation and communication latency resulting from different platform choices. We demonstrate that emerging in-network accelerator approaches offer much improved and predictable performance as well as better scaling to support multiple data sources.},
booktitle = {Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking},
pages = {7–12},
numpages = {6},
keywords = {edge computing, hardware acceleration},
location = {Heraklion, Greece},
series = {EdgeSys '20}
}

@article{10.14778/3476249.3476284,
author = {Li, Side and Kumar, Arun},
title = {Towards an Optimized GROUP by Abstraction for Large-Scale Machine Learning},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476284},
doi = {10.14778/3476249.3476284},
abstract = {Many applications that use large-scale machine learning (ML) increasingly prefer different models for subgroups (e.g., countries) to improve accuracy, fairness, or other desiderata. We call this emerging popular practice learning over groups, analogizing to GROUP BY in SQL, albeit for ML training instead of SQL aggregates. From the systems standpoint, this practice compounds the already data-intensive workload of ML model selection (e.g., hyperparameter tuning). Often, thousands of models may need to be trained, necessitating high-throughput parallel execution. Alas, most ML systems today focus on training one model at a time or at best, parallelizing hyperparameter tuning. This status quo leads to resource wastage, low throughput, and high runtimes. In this work, we take the first step towards enabling and optimizing learning over groups from the data systems standpoint for three popular classes of ML: linear models, neural networks, and gradient-boosted decision trees. Analytically and empirically, we compare standard approaches to execute this workload today: task-parallelism and data-parallelism. We find neither is universally dominant. We put forth a novel hybrid approach we call grouped learning that avoids redundancy in communications and I/O using a novel form of parallel gradient descent we call Gradient Accumulation Parallelism (GAP). We prototype our ideas into a system we call Kingpin built on top of existing ML tools and the flexible massively-parallel runtime Ray. An extensive empirical evaluation on large ML benchmark datasets shows that Kingpin matches or is 4x to 14x faster than state-of-the-art ML systems, including Ray's native execution and PyTorch DDP.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2327–2340},
numpages = {14}
}

@inproceedings{10.1145/2568088.2576761,
author = {Guo, Yong and Varbanescu, Ana Lucia and Iosup, Alexandru and Martella, Claudio and Willke, Theodore L.},
title = {Benchmarking Graph-Processing Platforms: A Vision},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2576761},
doi = {10.1145/2568088.2576761},
abstract = {Processing graphs, especially at large scale, is an increasingly useful activity in a variety of business, engineering, and scientific domains. Already, there are tens of graph-processing platforms, such as Hadoop, Giraph, GraphLab, etc., each with a different design and functionality. For graph-processing to continue to evolve, users have to find it easy to select a graph-processing platform, and developers and system integrators have to find it easy to quantify the performance and other non-functional aspects of interest. However, the state of performance analysis of graph-processing platforms is still immature: there are few studies and, for the few that exist, there are few similarities, and relatively little understanding of the impact of dataset and algorithm diversity on performance. Our vision is to develop, with the help of the performance-savvy community, a comprehensive benchmarking suite for graph-processing platforms. In this work, we take a step in this direction, by proposing a set of seven challenges, summarizing our previous work on performance evaluation of distributed graph-processing platforms, and introducing our on-going work within the SPEC Research Group's Cloud Working Group.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {289–292},
numpages = {4},
keywords = {benchmarking, experimentation, performance, graph processing},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1145/3341105.3373948,
author = {Kuhlenkamp, J\"{o}rn and Werner, Sebastian and Borges, Maria C. and Ernst, Dominik and Wenzel, Daniel},
title = {Benchmarking Elasticity of FaaS Platforms as a Foundation for Objective-Driven Design of Serverless Applications},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373948},
doi = {10.1145/3341105.3373948},
abstract = {Application providers have to solve the trade-off between performance and deployment costs by selecting the "right" amount of provisioned computing resources for their application. The high value of changing this trade-off decision at runtime fueled a decade of combined efforts by industry and research to develop elastic applications. Despite these efforts, the development of elastic applications still demands significant time and expertise from application providers.To address this demand, FaaS platforms shift responsibilities associated with elasticity from the application developer to the cloud provider. While this shift is highly promising, FaaS platforms do not quantify elasticity; thus, application developers are unaware of how elastic FaaS platforms are. This lack of knowledge significantly impairs effective objective-driven design of serverless applications.In this paper, we present an experiment design and corresponding toolkit for quantifying elasticity and its associated trade-offs with latency, reliability, and execution costs. We present results for the evaluation of four popular FaaS platforms by AWS, Google, IBM, Microsoft, and show significant differences between the service offers. Based on our results, we assess the applicability of the individual FaaS platforms in three scenarios under different objectives: web serving, online data analysis, and offline batch processing.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1576–1585},
numpages = {10},
keywords = {serverless, experimentation, elasticity, benchmarking},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/2683483.2683544,
author = {Karthik, M. Siva and Mittal, Sudhanshu and Krishna, K. Madhava},
title = {Guess from Far, Recognize When Near: Searching the Floor for Small Objects},
year = {2014},
isbn = {9781450330619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2683483.2683544},
doi = {10.1145/2683483.2683544},
abstract = {In indoor environments, there would be several small objects lying around on the floor. In this work, we develop an efficient strategy to search for a set of queried objects amongst a large number of small objects lying around. Small objects of the order of 1cm – 5cm, appear very small, making it difficult for the present algorithms to recognize them from far away. A human like strategy in such cases is to infer each object's similarity to the queried objects, from far away. Subsequently, the objects of interest are approached and analyzed from a closer proximity through an optimal plan. We develop an optimal plan for the robot, to strategically visit a selected few among all the objects. From far away, we assign Existential Probabilities to the objects, indicating their similarity to queried objects. A Bayes' Net is constructed over the probabilities, to overlay and orient a Viewpoint Object Potential(VOP) map over potential search objects. VOP quantifies the probability of accurately recognizing an object through its RGB-D Point Cloud at various viewpoints. The belief from the Bayes' Net and the discriminative viewpoints from the VOP are utilized to formulate a Decision Tree which helps in building an optimal control plan. Hence, the robot reaches strategic viewpoints around potential objects, to recognize them through their RGB-D point clouds. The framework is experimentally evaluated using Kinect mounted on a Turtlebot using ROS platform.},
booktitle = {Proceedings of the 2014 Indian Conference on Computer Vision Graphics and Image Processing},
articleno = {61},
numpages = {8},
keywords = {Mobile Robotics, Visual Object Search},
location = {Bangalore, India},
series = {ICVGIP '14}
}

@inproceedings{10.1145/2910017.2910633,
author = {Bondarenko, Olga and De Schepper, Koen and Tsang, Ing-Jyh and Briscoe, Bob and Petlund, Andreas and Griwodz, Carsten},
title = {Ultra-Low Delay for All: Live Experience, Live Analysis},
year = {2016},
isbn = {9781450342971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910017.2910633},
doi = {10.1145/2910017.2910633},
abstract = {This demo dramatically illustrates how replacing 'Classic' TCP congestion control (Reno, Cubic, etc.) with a 'Scalable' alternative like Data Centre TCP (DCTCP) keeps queuing delay ultra-low; not just for a select few light applications like voice or gaming, but even when a variety of interactive applications all heavily load the same (emulated) Internet access. DCTCP has so far been confined to data centres because it is too aggressive---it starves Classic TCP flows. To allow DCTCP to be exploited on the public Internet, we developed DualQ Coupled Active Queue Management (AQM), which allows the two TCP types to safely co-exist. Visitors can test all these claims. As well as running Web-based apps, they can pan and zoom a panoramic video of a football stadium on a touch-screen, and experience how their personalized HD scene seems to stick to their finger, even though it is encoded on the fly on servers accessed via an emulated delay, representing 'the cloud'. A pair of VR goggles can be used at the same time, making a similar point. The demo provides a dashboard so that visitors can not only experience the interactivity of each application live, but they can also quantify it via a wide range of performance stats, updated live. It also includes controls so visitors can configure different TCP variants, AQMs, network parameters and background loads and immediately test the effect.},
booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
articleno = {33},
numpages = {4},
location = {Klagenfurt, Austria},
series = {MMSys '16}
}

@inproceedings{10.1145/3433210.3453095,
author = {Boutet, Antoine and Frindel, Carole and Gambs, S\'{e}bastien and Jourdan, Th\'{e}o and Ngueveu, Rosin Claude},
title = {DySan: Dynamically Sanitizing Motion Sensor Data Against Sensitive Inferences through Adversarial Networks},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3453095},
doi = {10.1145/3433210.3453095},
abstract = {With the widespread development of the quantified-self movement, an increasing number of users rely on mobile applications to monitor their physical activity through their smartphones. However, granting applications a direct access to sensor data exposes users to privacy risks. In particular, motion sensor data are usually transmitted to analytics applications hosted in the cloud, which leverages on machine learning models to provide feedback on their activity status to users. In this setting, nothing prevents the service provider to infer private and sensitive information about a user such as health or demographic attributes. To address this issue, we propose DySan, a privacy-preserving framework to sanitize motion sensor data against unwanted sensitive inferences (i.e., improving privacy) while limiting the loss of accuracy on the physical activity monitoring (i.e., maintaining data utility). Our approach is inspired from the framework of Generative Adversarial Networks to sanitize the sensor data for the purpose of ensuring a good trade-off between utility and privacy. More precisely, by learning in a competitive manner several networks, DySan is able to build models that sanitize motion data against inferences on a specified sensitive attribute (e.g., gender) while maintaining an accurate activity recognition. DySan builds various sanitizing models, characterized by different sets of hyperparameters in the global loss function, to propose a transfer learning scheme over time by dynamically selecting the model which provides the best utility and privacy trade-off according to the incoming data. Experiments conducted on real datasets demonstrate that DySan can drastically limit the gender inference up to 41\% (from 98\% with raw data to 57\% with sanitized data) while only reducing the accuracy of activity recognition by 3\% (from 95\% with raw data to 92\% with sanitized data).},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {672–686},
numpages = {15},
keywords = {gan, privacy, utility-privacy trade-off, activity detection},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@inproceedings{10.1145/3141128.3141146,
author = {Sianipar, Johannes and Willems, Christian and Meinel, Christoph},
title = {Team Placement in Crowd-Resourcing Virtual Laboratory for IT Security e-Learning},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141146},
doi = {10.1145/3141128.3141146},
abstract = {A crowd-resourcing virtual laboratory is a virtual laboratory in which some of the resources are obtained from the crowd. The virtual laboratory is for IT Security e-Learning, where a trainee needs an isolated laboratory environment to do the practical exercises. The isolated laboratory environment, which is called as a Team, consists of virtual machines (VMs) or containers and virtual network devices. The crowd contributes their resources such as virtual machines or physical machines, to the virtual laboratory. The virtual laboratory automatically occupies the contributed resources and uses them to create a Team. The team that consists of containers, will be run in a VM. Since there could be a lot of VMs available, the system needs to select the best VM to run a Team. We present CTPlace, an approach for Team Placement in crowd-resourcing virtual laboratory.CTPlace groups the VMs into tree hierarchical clusters based on the Geo-location of the VMs. CTPlace has two steps in the Team placement. First, it selects a nearest cluster to the trainee location to get the highest throughput. Second, it selects a VM inside the selected cluster. To select a VM inside a public cloud cluster, it uses Most-Full-First algorithm to reduce service cost by reducing the number of running VMs. To select a VM inside a private cloud or within contributed resources, it uses Least-Full-First and Tag-Pack to balance the load and try to place the same type of Teams on the same VM. We compare the CTPlace with three other placement algorithms in a simulated environment, to evaluate the performance of the CTPlace.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {60–66},
numpages = {7},
keywords = {Virtual Laboratory, Cloud Computing, Placement Algorithm, Crowd-resourcing},
location = {London, United Kingdom},
series = {ICCBDC '17}
}

@inproceedings{10.1145/3306131.3317023,
author = {Steinlechner, Harald and Rainer, Bernhard and Schw\"{a}rzler, Michael and Haaser, Georg and Szabo, Attila and Maierhofer, Stefan and Wimmer, Michael},
title = {Adaptive Pointcloud Segmentation for Assisted Interactions},
year = {2019},
isbn = {9781450363105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306131.3317023},
doi = {10.1145/3306131.3317023},
abstract = {In this work, we propose an interaction-driven approach streamlined to support and improve a wide range of real-time 2D interaction metaphors for arbitrarily large pointclouds based on detected primitive shapes. Rather than performing shape detection as a costly pre-processing step on the entire point cloud at once, a user-controlled interaction determines the region that is to be segmented next. By keeping the size of the region and the number of points small, the algorithm produces meaningful results and therefore feedback on the local geometry within a fraction of a second. We can apply these finding for improved picking and selection metaphors in large point clouds, and propose further novel shape-assisted interactions that utilize this local semantic information to improve the user's workflow.},
booktitle = {Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games},
articleno = {14},
numpages = {9},
keywords = {interactive editing, shape detection, pointcloud segmentation},
location = {Montreal, Quebec, Canada},
series = {I3D '19}
}

@article{10.1145/3451964.3451979,
author = {Butnaru, Andrei-M\u{a}d\u{a}lin},
title = {Machine Learning Applied in Natural Language Processing},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3451964.3451979},
doi = {10.1145/3451964.3451979},
abstract = {Machine Learning is present in our lives now more than ever. One of the most researched areas in machine learning is focused on creating systems that are able to understand natural language. Natural language processing is a broad domain, having a vast number of applications with a significant impact in society. In our current era, we rely on tools that can ease our lives. We can search through thousands of documents to find something that we need, but this can take a lot of time. Having a system that can understand a simple query and return only relevant documents is more efficient. Although current approaches are well capable of understanding natural language, there is still space for improvement.This thesis studies multiple natural language processing tasks, presenting approaches on applications such as information retrieval, polarity detection, dialect identification [Butnaru and Ionescu, 2018], automatic essay scoring [Cozma et al., 2018], and methods that can help other systems to understand documents better. Part of the described approaches from this thesis are employing kernel methods, especially string kernels. A method based on string kernels that can determine in what dialect a document is written is presented in this thesis. The approach is treating texts at the character level, extracting features in the form of p-grams of characters, and combining several kernels, including presence bits kernel and intersection kernel. Kernel methods are also presented as a solution for defining the complexity of a specific word. By combining multiple low-level features and high-level semantic features, the approach can find if a non-native speaker of a language can see a word as complicated or not. With one focus on string kernels, this thesis proposes two transductive methods that can improve the results obtained by employing string kernels. One approach suggests using the pairwise string kernel similarities between samples from the training and test sets as features. The other method defines a simple self-training algorithm composed of two iterations. As usual, a classifier is trained over the training data, then is it used to predict the labels of the test samples. In the second iteration, the algorithm adds a predefined number of test samples to the training set for another round of training. These two transductive methods work by adapting the learning method to the test set.A novel cross-dialectal corpus is shown in this thesis. The Moldavian versus Romanian Corpus (MOROCO) [Butnaru and Ionescu, 2019a] contains over 30.000 samples collected from the news domain, split across six categories. Several studies can be employed over this corpus such as binary classification between Romanian and Moldavian samples, intra-dialect multi-class categorization by topic, and cross-dialect multi-class classification by topic. Two baseline approaches are presented for this collection of texts. One method is based on a simple string kernel model. The second approach consists of a character-level deep neural network, which includes several Squeeze-and-Excitation Blocks (SE-blocks). As known at this moment, this is the first time when a SE-block is employed in a natural language processing context. This thesis also presents a method for German Dialect Identification composed on a voting scheme that combines a Character-level Convolutional Neural Network, a Long Short-Term Memory Network, and a model based on String Kernels.Word sense disambiguation is still one of the challenges of the NLP domain. In this context, this thesis tackles this challenge and presents a novel disambiguation algorithm, known as ShowtgunWSD [Butnaru and Ionescu, 2019b]. By treating the global disambiguation problem as multiple local disambiguation problems, ShotgunWSD is capable of determining the sense of the words in an unsupervised and deterministic way, using WordNet as a resource. For this method to work, three functions that can compute the similarity between two words senses are defined. The disambiguation algorithm works as follows. The document is split into multiple windows of words of a specific size for each window. After that, a brute-force algorithm that computes every combination of senses for each word within that window is employed. For every window combination, a score is calculated using one of the three similarity functions. The last step merges the windows using a prefix and suffix matching to form more significant and relevant windows. In the end, the formed windows are ranked by the length and score, and the top ones, based on a voting scheme, will determine the sense for each word.Documents can contain a variable number of words, therefore employing them in machine learning may be hard at times. This thesis presents two novel approaches [Ionescu and Butnaru, 2019] that can represent documents using a finite number of features. Both methods are inspired by computer vision, and they work by first transforming the words within documents to a word representation, such as word2vec. Having words represented in this way, a k-means clustering algorithm can be applied over the words. The centroids of the formed clusters are gathered into a vocabulary. Each word from a document is then represented by the closest centroid from the previously formed vocabulary. To this point, both methods share the same steps. One approach is designed to compute the final representation of a document by calculating the frequency of each centroid found inside it. This method is named Bag of Super Word Embeddings (BOSWE) because each centroid can be viewed as a super word. The second approach presented in this thesis, known as Vector of Locally-Aggregated Word Embeddings (VLAWE), computes the document representation by accumulating the differences between each centroid and each word vector associated with the respective centroid. This thesis also describes a new way to score essays automatically by combining a low-level string kernel model with a high-level semantic feature representation, namely the BOSWE representation.The methods described in this thesis exhibit state-of-the-art performance levels over multiple tasks. One fact to support this claim is that the string kernel method employed for Arabic Dialect Identification obtained the first place, two years in a row at the Fourth and Fifth Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial). The same string kernel model obtained the fifth place at the German Dialect Identification Closed Shared Task at VarDial Workshop of EACL 2017. Second of all, the Complex Word Identification model scored a third-place at the CWI Shared Task of the BEA-13 of NAACL 2018. Third of all, it is worth to mention that the ShotgunWSD algorithm surpassed the MCS baseline on several datasets. Lastly, the model that combines string kernel and bag of super word embeddings obtained state-of-the-art performance over the Automated Student Assessment Prize dataset.},
journal = {SIGIR Forum},
month = {feb},
articleno = {15},
numpages = {3}
}

@inproceedings{10.1145/3443467.3443846,
author = {Wang, Shaomin},
title = {Sentiment Analysis of the Song "Mojito"},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443846},
doi = {10.1145/3443467.3443846},
abstract = {With the development of the Internet, people share views and opinions on things anytime and anywhere. While receiving information, people also produce various information. Based on the evaluation of Jay Chou's new song mojito by different users on Douban, this paper uses Python's JSON tool to calculate the positive and negative probability value of each comment by setting the probability value of positive tendency greater than 0.5 as positive evaluation, otherwise as negative. In order to understand the reasons for user ratings directly, a word cloud map is drawn based on comment data.On the basis of determining the positive and negative emotional tags, the first step is data processing, such as data cleaning, Chinese word segmentation, removing stop words, text vectorization, etc. Then, three different models of naive Bayes, logistic regression and support vector machine are established for comparison. Finally, naive Bayes model is selected for prediction based on cross validation score. Through confusion matrix evaluation, it is found that the model is more accurate for negative evaluation classification results, but not accurate enough for positive evaluation prediction. This may be related to the expressions of irony and double negation in text reviews.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {739–744},
numpages = {6},
keywords = {support vector machine, Sentiment analysis, logistic regression, naive Bayes, sentiment tendency probability},
location = {Xiamen, China},
series = {EITCE '20}
}

@inproceedings{10.1145/3448016.3457274,
author = {Shah, Vraj and Lacanlale, Jonathan and Kumar, Premanand and Yang, Kevin and Kumar, Arun},
title = {Towards Benchmarking Feature Type Inference for AutoML Platforms},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457274},
doi = {10.1145/3448016.3457274},
abstract = {The paradigm of AutoML has created an opportunity to enable ML for the masses. Emerging industrial-scale cloud AutoML platforms aim to automate the end-to-end ML workflow. While many works have looked into automated feature engineering, model selection, or hyper-parameter search in AutoML, little work has studied a crucial step that serves as an entry point to this workflow: ML feature type inference. The semantic gap between attribute types (e.g., strings, numbers) in databases/files and ML feature types (e.g., Numeric, Categorical) necessitates type inference. In this work, we formalize and standardize this task by creating the first ever benchmark labeled dataset, which we use to objectively evaluate existing AutoML tools. Our dataset has 9921 examples and a 9-class label vocabulary. Our labeled data also offers an alternative approach to automate this task than existing rule-based or syntax-based approaches: use ML itself to predict feature types. We collate a benchmark suite of 30 classification and regression tasks to assess the importance of type inference for downstream models. Empirical comparison on our labeled data shows that an ML-based approach delivers a lift of an average 14\% and up to 38\% in accuracy for identifying feature types compared to prominent industrial tools. Our downstream benchmark suite reveals that the ML-based approach outperforms existing industrial-strength tools for 47 out of 60 downstream models. We release our labeled dataset, models, and downstream benchmarks in a public repository with a leaderboard.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1584–1596},
numpages = {13},
keywords = {benchmark data, data preparation, ML feature type inference, labeled data, autoML},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3239576.3239607,
author = {Du, Yuntao and Zhang, Lu and Shi, Jiahao and Tang, Jingjuan and Yin, Ying},
title = {Feature-Grouping-Based Two Steps Feature Selection Algorithm in Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239607},
doi = {10.1145/3239576.3239607},
abstract = {In order to improve the effect of software defect prediction, many algorithms including feature selection, have been proposed. Based on Wrapper and Filter hybrid framework, a feature-grouping-based feature selection algorithm is proposed in this paper. The algorithm is composed of two steps. In the first step, in order to remove the redundant features, we group the features according to the redundancy between the features. The symmetry uncertainty is used as the constant indicator of the correlation and the FCBF-based grouping algorithm is used to group the features. In the second step, a subset of the features are selected from each group to form the final subset of features. Many classical methods select the representative feature from each group. We consider that when the number of intra-group features is large, the representative features are not enough to reflect the information in this group. Therefore, we require that at least one feature be selected within each group, in this step, the PSO algorithm is used for Searching Randomly from each group. We tested on the open source NASA and PROMISE data sets. Using three kinds of classifier. Compared to the other methods tested in this article, our method resulted in 90\% improvement in the predictive performance of 30 sets of results on 10 data sets. Compared with the algorithms without feature selection, the AUC values of this method in the Logistic regression, Naive Bayesian, and K-neighbor classifiers are improved by 5.94\% and 4.69\% And 8.05\%. The FCBF algorithm can also be regarded as a kind of first performing feature grouping. Compared with the FCBF algorithm, the AUC values of this method are improved by 4.78\%, 6.41\% and 4.4\% on the basis of Logistic regression, Naive Bayes and K-neighbor. We can also see that for the FCBF-based grouping algorithm, it could be better to choose a characteristic cloud from each group than to choose a representative one.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {173–178},
numpages = {6},
keywords = {FCBF-based grouping algorithm, PSO, Feature grouping, Software defect prediction, Intra-group feature selection},
location = {Chengdu, China},
series = {ICAIP '18}
}

@inproceedings{10.1145/3093338.3104153,
author = {Chourasia, A. and Nadeau, D. and Norman, M.},
title = {SeedMe: Data Sharing Building Blocks},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3104153},
doi = {10.1145/3093338.3104153},
abstract = {Data sharing is essential and pervasive in scientific research. The requirements for data sharing vary as research projects mature and iterate through early designs and prototypes with a small number of collaborators, and develop into publishable results and larger collaborator teams. Along the way, preliminary and transient results often need to be shared, discussed, and visualized with a quick turn-around time in order to guide the next steps of the project. Data sharing throughout this process requires that the data itself be shared, along with essential context, such as descriptions, provenance, scripts, visualizations, and threaded discussions. However, current consumer-oriented data sharing solutions mainly rely on local or cloud file systems or web-based drop boxes. These mechanisms are rather basic and are largely focused on data storage for individual use, rather than data collaboration. Using them for scientific data sharing is cumbersome.SeedMe is a platform that enables easy sharing of transient and preliminary data for a broad research computing community by offering cyberinfrastructure as a service and a modular software stack that could be customized. SeedMe is based on Drupal content management system as a set of building blocks with additional PHP modules and web services clients.In this poster we present our progress on implementing a web based modular data sharing platform that collocates shared data, along with the data's context, including descriptions, discussion, light-weight visualizations, and support files. This project is an evolution of the earlier SeedMe[1, 2] project, which created prototype data sharing tools and garnered user feedback from realworld use. The new SeedMe platform is developing modular components for data sharing, light-weight visualization, collaboration, DOI registration, video encoding and playback, REST APIs, command-line data import/export tools, and more. These modules may be added to any web site based upon the widely-used open-source Drupal content management system.The new SeedMe modules allow extensive customization enabling the sites to select and enhance functionality to provide features specific to a research community or a project. The SeedMe modules are widely applicable to a broad research community. They will be released as a suite of open source extensible building blocks. With this poster we showcase current progress along with an interactive demonstration of the project and engage with the HPC community to get feedback.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {69},
numpages = {1},
keywords = {Data sharing, Cloud, CMS, Visualization, HPC, Collaboration},
location = {New Orleans, LA, USA},
series = {PEARC '17}
}

@inproceedings{10.1145/3427921.3450250,
author = {Carnevali, Laura and Reali, Riccardo and Vicario, Enrico},
title = {Compositional Evaluation of Stochastic Workflows for Response Time Analysis of Composite Web Services},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450250},
doi = {10.1145/3427921.3450250},
abstract = {Workflows are patterns of orchestrated activities designed to deliver some specific output, with application in various relevant contexts including software services, business processes, supply chain management. In most of these scenarios, durational properties of individual activities can be identified from logged data and cast in stochastic models, enabling quantitative evaluation of time behavior for diagnostic and predictive analytics. However, effective fitting of observed durations commonly requires that distributions break the limits of memoryless behavior and unbounded support of Exponential distributions, casting the problem in the class of non-Markovian models. This results in a major hurdle for numerical solution, largely exacerbated by the concurrency structure of workflows, which natively subtend concurrent activities with overlapping execution intervals and a limited number of regeneration points, i.e., time points at which the Markov property is satisfied and analysis can be decomposed according to a renewal argument. We propose a compositional method for quantitative evaluation of end-to-end response time of complex workflows. The workflow is modeled through Stochastic Time Petri Nets (STPNs), associating activity durations with Exponential distributions truncated over bilateral firmly bounded supports that fit mean and coefficient of variation of real logged histograms. Based on the model structure, the workflow is decomposed into a hierarchy of subworkflows, each amenable to efficient numerical solution through Markov regenerative transient analysis. In this step, the grain of decomposition is driven by non-deterministic analysis of the space of feasible behaviors in the underlying Time Petri Net (TPN) model, which permits efficient characterization of the factors that affect behavior complexity between regeneration points. Duration distributions of the subworkflows obtained through separate analyses are then repeatedly recomposed in numerical form to compute the response time distribution of the overall workflow.Applicability is demonstrated on a case from the literature of composite web services, here extended in complexity to demonstrate scalability of the approach towards finer grain composition schemes, and associated with a variety of durations randomly selected from a data set in the literature of service oriented computing, so as to assess variability of accuracy and complexity of the overall approach with respect to specific timings.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {177–188},
numpages = {12},
keywords = {Markov regenerative processes, regenerative transient analysis, composite web services, performance evaluation, stochastic workflows},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3319619.3321891,
author = {Coetzee, Leon and Nitschke, Geoff},
title = {Evolving Optimal Sun-Shading Building Fa\c{c}ades},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3321891},
doi = {10.1145/3319619.3321891},
abstract = {Evolutionary algorithms have been applied to numerous architectural design applications in what is popularly known as evolutionary design [3], [4], [6]. Such applications include architectural support [7] and structural design for buildings [5] and floor-plan layout design [8]. However, evolutionary design of optimally shaped building fa\c{c}ades is less explored in evolutionary architectural design applications [6], [12], [13].This research investigates the evolutionary design of building fa\c{c}ades, optimally shaped for a given climate. This study applies evolutionary methods to optimally design sun-shades (covering windows on building fa\c{c}ades). Ideally, sun-shades will maximally block direct sunlight but minimize window coverage, thus allowing unobstructed views out of the window and maximizing ambient natural lighting inside. Also, sun-shades help to passively control building climate and determine occupant comfort. Optimal sun-shade designs allow direct sunlight (solar penetration) to enter interior spaces in winter months, heating the building, and minimize solar penetration in summer months, cooling the building [11].This study applies an Evolutionary Strategy (ES) [1] to automate sun-shade design such that solar penetration is minimized for both east and west facing windows, given summer solstice daylight hours in various geographic locations. An ES was selected given the demonstrated effectiveness of such evolutionary optimization on a range of engineering design problems with various constraints [9]. We focus on sun-shade design for rectangular shaped windows (vertical Y axis is 1.5 times the length of the horizontal X axis), where we anticipate sun-shade design will be replicated for many identical windows comprising a building's fa\c{c}ade, as is the case for many modern tall buildings [14].The ES was initialized with 20000 uniform random [1] points in a continuous three-dimensional (1.0 x 1.0 x 1.0) space adjacent to the window (figure 1). These points were possible mesh vertices for sun-shade design and thus the design solution space. The fitness function computed sun-shade effectiveness via calculating how many sun-rays were blocked assuming an increasing or decreasing sun height above the horizontal plane (angle V in figure 1). Thus, we tested the portion of sun-rays blocked by an evolving sun-shade (mesh formed by 20000 vertices) over half of daylight hours (separate sun-shades were evolved for east and west facing fa\c{c}ades). In successive generations, sun-shade mesh vertices blocking sun-rays (at varying degrees of inclination and declination) aimed at the window were selected for as vertices in evolving designs.Evolving sun-shade effectiveness was computed as the intersection of sun-rays at 15 second intervals during simulated half-days. For east facing fa\c{c}ades, from the point where sun is on the horizontal plane (Y axis in figure 1) and incrementally increases until it is directly above the vertical axis of the building fa\c{c}ade (Y-Z plane in figure 1), and for west facing fa\c{c}ades where the sun starts at this midday point and incrementally declines. Sun-shades were evolved for east and west facing fa\c{c}ades given half of summer solstice daylight hours1 (for east versus west fa\c{c}ades) indicative of Cape Town, South Africa, and Amsterdam, the Netherlands (~ 14 hours, 25 minutes and 16 hours, 48 minutes, respectively).At these two geographic locations, 15 second intervals indicated incremental sun movements during day-light hours. For Cape Town, this was approximated as 0.052° increases and decreases and for Amsterdam, 0.045° increases and decreases (for east and west facing fa\c{c}ades, respectively). Half-day simulations thus tested, every 15 seconds, sun-ray intersection (vector: Xp, Yp, Zp at angle V from the horizontal or vertical plane) with any point in the sun-shade. This was a point-cloud in generation 1 and mesh-points in subsequent generations (figure 1). Points intersecting the sun-ray were given maximum (normalized) fitness of 1.0, and points within a given ray distance were assigned a logarithmically decreasing fitness that equalled 0.0 at the maximum ray distance. To account for random variation and diffusion of sun-ray light, each 15 seconds, a random angle (in the range: [-0.01°, +0.01°]) was added to the sun-ray's vector value V.Evolutionary design used a µ+λ ES [1], where (λ = 20000) off-spring were created per generation. This combined population was ranked by fitness and the least ft λ genotypes discarded. Each genotype encoded an (x, y, z) point in an N point-mesh (evolving sun-shade design), and corresponding σ mutation step-size for each coordinate. For simplicity, the X, Y, Z dimensions of the 3D solution space for evolving sun-shades (adjacent to the window) was normalized the range [0.0, 1.0] and the window dimensions normalized to the range [0.0, 1.5] for the X, Y window axes, respectively. Thus, sun-shades only evolved to cover the top two-thirds of a window, ensuring that sufficient ambient light still entered the building and that occupants have a view out of the window.One generation was the evaluation of all 20000 genotypes (in sun-ray simulations), where the fittest 10\% were selected, mutation operators: σxNx(0,1), σyNy(0, 1), σzNz(0,1) applied to permutate each genotype's coordinate and step-size values (p=1.0 and p=0.05, respectively), such that (λ=20000) offspring genotypes were created. All µ+λ genotypes were then evaluated and the fittest 20000 selected as survivors [1]. Sun-shade evolution for Cape Town and Amsterdam constituted experiment set 1 and 2, respectively. Each experiment set was 10 ES runs, for east and west facing fa\c{c}ades, and each run was 100 generations (ES run stopping condition).Sun-shade fitness was the portion of points (constituting a sunshade design) that blocked or partially blocked sun-rays during each half-day simulation. Points that intersected a sun-ray were assigned a maximum fitness of 1.0, and points close to a sun-ray (&lt; ray distance) were assigned a partial fitness in the range: (0.0, 1.0). In generation 1, all 20,000 possible points were considered for sun-shade design. In subsequent generations only points given a fitness value were considered part of the evolving sun-shade (point-mesh) design. For simplicity, sun-shade fitness was normalized to the range: [0.0, 1.0], where 0.0 indicated no sun-rays blocked and 1 indicated all sun-rays blocked (over all day-light hours tested).As a benchmark comparison for evolved sun-shade effectiveness, the fittest sun-shades evolved for east and west facing fa\c{c}ades (at both locations) were selected from each run and compared to ten heuristic design sun-shades (figure 1). The effectiveness of these sun-shades was similarly computed using sun-ray simulations of 15 second intervals during half-day periods for east and west facing fa\c{c}ades and a given number of day-light hours at both locations.Thus for each heuristic design sun-shade a fitness value was similarly calculated, normalized to the range: [0.0, 1.0], where 0 indicated no sun-rays were blocked and 1.0 indicated that all sun-rays were blocked during a sun-ray simulation.Results indicated that, on average, evolved sun-shades, for both shorter and longer day lengths and east versus west facing fa\c{c}ades, were significantly more effective (with statistical significance, two-tailed t-test, p &lt; 0.05, [2]) compared to the ten tested heuristic designed sun-shades. Results also indicated that evolutionary design is suitable for automating optimal sun-shade (and potentially building fa\c{c}ade) design and support current hypotheses on the efficacy of evolutionary design for improving current architectural designs and automating efficient and effective industrial design production [3], [4], [12]. Ongoing work is evaluating sun-shade evolution in comparison to other heuristic designs in various geographic locations, as well as evolving sun-shades that dynamically adapt their form to suit varying daylight lengths and sun intensity.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {393–394},
numpages = {2},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.5555/3615924.3615929,
author = {Joydeep, Mukherjee and Sumona, Mukhopadhyay and Marin, Litoiu},
title = {Detecting Software Anomalies Using Spectrograms and Convolutional Neural Network},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Microservice applications are increasingly embracing cloud platforms to run their services. These applications can often be impacted by anomalies. Detecting anomalies at runtime is vital to ensure that cloud-native applications meet specified requirements and ensure good Quality-of-Service. However, this is challenging to do since application owners do not always have access to the underlying cloud infrastructure and hence can not use host level metrics and hardware counters as done in the past. One potential way to address this challenge is to use a machine learning based anomaly detection approach which uses metrics that can be easily collected from applications running on public cloud platforms. In this paper, we develop a classifier using deep learning for pattern recognition of anomalies for detecting runtime software anomalies in cloud-native applications. We use textured images known as spectrograms that are obtained from time series measurement readily available from cloud-native applications. These spectrogram images are used to train a Convolutional Neural Network (CNN) classifier for anomaly detection. We evaluate our approach on two real-world datasets that capture known software anomalies in public cloud platforms. Results show that the proposed spectrogram based CNN classifier yields detection accuracy of 99\% for both datasets at runtime and outperforms a competing non-image based classifier.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {44–53},
numpages = {10},
keywords = {Cloud Computing, Deep Learning, Software Anomalies},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3543507.3583460,
author = {Feng, Binbin and Ding, Zhijun},
title = {GROUP: An End-to-End Multi-Step-Ahead Workload Prediction Approach Focusing on Workload Group Behavior},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583460},
doi = {10.1145/3543507.3583460},
abstract = {Accurately forecasting workloads can enable web service providers to achieve proactive runtime management for applications and ensure service quality and cost efficiency. For cloud-native applications, multiple containers collaborate to handle user requests, making each container’s workload changes influenced by workload group behavior. However, existing approaches mainly analyze the individual changes of each container and do not explicitly model the workload group evolution of containers, resulting in sub-optimal results. Therefore, we propose a workload prediction method, GROUP, which implements the shifts of workload prediction focus from individual to group, workload group behavior representation from data similarity to data correlation, and workload group behavior evolution from implicit modeling to explicit modeling. First, we model the workload group behavior and its evolution from multiple perspectives. Second, we propose a container correlation calculation algorithm that considers static and dynamic container information to represent the workload group behavior. Third, we propose an end-to-end multi-step-ahead prediction method that explicitly portrays the complex relationship between the evolution of workload group behavior and the workload changes of each container. Lastly, enough experiments on public datasets show the advantages of GROUP, which provides an effective solution to achieve workload prediction for cloud-native applications.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3098–3108},
numpages = {11},
keywords = {deep learning., workload prediction, Cloud Computing},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3578245.3584728,
author = {Klinaku, Floriment and Speth, Sandro and Zilch, Markus and Becker, Steffen},
title = {Hitchhiker's Guide for Explainability in Autoscaling},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3584728},
doi = {10.1145/3578245.3584728},
abstract = {Cloud-native applications force increasingly powerful and complex autoscalers to guarantee the applications' quality of service. For software engineers with operational tasks understanding the autoscalers' behavior and applying appropriate reconfigurations is challenging due to their internal mechanisms, inherent distribution, and decentralized decision-making. Hence, engineers seek appropriate explanations. However, engineers' expectations on feedback and explanations of autoscalers are unclear. In this paper, through a workshop with a representative sample of engineers responsible for operating an autoscaler, we elicit requirements for explainability in autoscaling. Based on the requirements, we propose an evaluation scheme for evaluating explainability as a non-functional property of the autoscaling process and guide software engineers in choosing the best-fitting autoscaler for their scenario. The evaluation scheme is based on a Goal Question Metric approach and contains three goals, nine questions to assess explainability, and metrics to answer these questions. The evaluation scheme should help engineers choose a suitable and explainable autoscaler or guide them in building their own.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {277–282},
numpages = {6},
keywords = {elasticity, explainability, requirements, cloud, evaluation},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3491204.3527467,
author = {Kousiouris, George and Giannakos, Chris and Tserpes, Konstantinos and Stamati, Teta},
title = {Measuring Baseline Overheads in Different Orchestration Mechanisms for Large FaaS Workflows},
year = {2022},
isbn = {9781450391597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491204.3527467},
doi = {10.1145/3491204.3527467},
abstract = {Serverless environments have attracted significant attention in recent years as a result of their agility in execution as well as inherent scaling capabilities as a cloud-native execution model. While extensive analysis has been performed in various critical performance aspects of these environments, such as cold start times, the aspect of workflow orchestration delays has been neglected. Given that this paradigm has become more mature in recent years and application complexity has started to rise from a few functions to more complex application structures, the issue of delays in orchestrating these functions may become severe. In this work, one of the main open source FaaS platforms, Openwhisk, is utilized in order to measure and investigate its orchestration delays for the main sequence operator of the platform. These are compared to delays included in orchestration of functions through two alternative means, including the execution of orchestrator logic functions in supporting runtimes based on Node-RED. The delays inserted by each different orchestration mode are measured and modeled, while boundary points of selection between each mode are presented, based on the number and expected delay of the functions that constitute the workflow. It is indicative that in certain cases, the orchestration overheads might range from 0.29\% to 235\% compared to the beneficial computational time needed for the workflow functions. The results can extend simulation and estimation mechanisms with information on the orchestration overheads.},
booktitle = {Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
pages = {61–68},
numpages = {8},
keywords = {performance, serverless, orchestration, faas, overhead, openwhisk},
location = {Bejing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3491204.3527490,
author = {Tuli, Shreshth and Casale, Giuliano},
title = {Optimizing the Performance of Fog Computing Environments Using AI and Co-Simulation},
year = {2022},
isbn = {9781450391597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491204.3527490},
doi = {10.1145/3491204.3527490},
abstract = {This tutorial presents a performance engineering approach for optimizing the Quality of Service (QoS) of Edge/Fog/Cloud Computing environments using AI and Coupled-Simulation being developed as part of the Co-Simulation based Container Orchestration (COSCO) framework. It introduces fundamental AI and co-simulation concepts, their importance in QoS optimization and performance engineering challenges in the context of Fog computing. It also discusses how AI models, specifically, deep neural networks (DNNs), can be used in tandem with simulated estimates to take optimal resource management decisions. Additionally, we discuss a few use cases of training DNNs as surrogates to estimate key QoS metrics and utilize such models to build policies for dynamic scheduling in a distributed fog environment. The tutorial demonstrates these concepts using the COSCO framework. Metric monitoring and simulation primitives in COSCO demonstrates the efficacy of an AI and simulation based scheduler on a fog/cloud platform. Finally, we provide AI baselines for resource management problems that arise in the area of fog management.},
booktitle = {Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
pages = {25–28},
numpages = {4},
keywords = {co-simulation., performance engineering, fog computing, artificial intelligence},
location = {Bejing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3493700.3493731,
author = {Lodha, Ishaan and Kolur, Lakshana and Krishnan, Keerthan and Dheenadayalan, Kumar and Sitaram, Dinkar and Nandi, Siddhartha},
title = {Cost-Optimized Video Transfer Using Real-Time Super Resolution Convolutional Neural Networks},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493731},
doi = {10.1145/3493700.3493731},
abstract = {The explosion of video generation and consumption, coupled with an inadequate rise in network bandwidth has led to network delays and decreased Quality of Experience, limiting the opportunities to tap into the full potential of video data. These deficiencies in network resources with a shift to cloud computing models have resulted in the need to revisit the overall mechanism for video transfer and storage of videos between edge devices and the cloud. We propose a novel multi-scale real-time super-resolution convolutional neural network to achieve the composite task of optimizing the entire cost of video transfer with minimal loss of quality that can be used for any application involving the transfer of video data. To achieve this, we develop a cost-optimized video transfer system that optimizes the metrics of video transfer to give the best quality video output, given the user budget. The model makes use of Convolution blocks for extracting features and output creation with multiple sub-pixel convolutions in a novel structure. For upscaling to full High Definition video at 30 fps, the model successfully retained the frame rate while the system achieved savings in transfer time and bandwidth usage. This model has been trained on surveillance videos (VIRAT dataset), but consistent results were obtained during testing even on feature films and sports videos which demonstrates its content invariance. The evaluation of our approach averaged over 376 videos, yielded meager quality losses of 8\%, measured by a novel non-referential quality metric, also proposed in this paper. Additionally, average network bandwidth savings of 80\% and average video transfer time reduction of 52\% were achieved.},
booktitle = {Proceedings of the 5th Joint International Conference on Data Science \&amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {213–221},
numpages = {9},
keywords = {super resolution, cost optimization, CNN, video transfer, deep neural networks, GAN},
location = {<conf-loc>, <city>Bangalore</city>, <country>India</country>, </conf-loc>},
series = {CODS-COMAD '22}
}

@inproceedings{10.1145/3543712.3543715,
author = {Petrov, Valerii and Gennadinik, Anna and Avksentieva, Elena},
title = {Metrics for Machine Learning Evaluation Methods in Cloud Monitoring Systems},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543715},
doi = {10.1145/3543712.3543715},
abstract = {During the machine learning pipeline development, engineers need to validate the efficiency of the machine learning methods in order to assess the quality of the made forecast.Due to the wide deployment and implementation of the machine learning models and methods across monitoring systems, the actual scientific problem is the assessment of these methods in the monitoring systems. This research has concluded that the current standard metrics are not sufficient to get the accurate assessment for the used machine learning methods.This research has provided the new complex rating for anomaly detection regarding the use-cases of cloud monitoring systems. The main difference from the standard metrics is that the new approach includes better integration to the business processes, demanding resources, and a critical glance to the false-positive alerts. The new approach might be used in the model assessment in monitoring systems with the similar requirements:Cost-effective use of computing resourcesLow amount of false-positivesFast detection of anomaliesFurthermore, the current research proposes new methods of computation capacity planning for different anomaly detection methods. These methods are not even limited to anomaly detection and could be used as a basis for developing capacity planning for other machine learning techniques and approaches.· Applied computing∼Operations research∼Forecasting · Computer systems organization∼Architectures∼Distributed architectures ∼Cloud computing∼Forecasting · Computing methodologies∼Machine learning},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {168–175},
numpages = {8},
keywords = {capacity planning, machine learning metrics, quality assessment, monitoring},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@article{10.1145/3512893,
author = {Ming, Zhao and Li, Xiuhua and Sun, Chuan and Fan, Qilin and Wang, Xiaofei and Leung, Victor C. M.},
title = {Sleeping Cell Detection for Resiliency Enhancements in 5G/B5G Mobile Edge-Cloud Computing Networks},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3512893},
doi = {10.1145/3512893},
abstract = {The rapid increase of data traffic has brought great challenges to the maintenance and optimization of 5G and beyond, and some smart critical infrastructures, e.g., small base stations (SBSs) in cellular cells, are facing serious security and failure threats, causing resiliency degradation concerns. Among special smart critical infrastructure failures, the sleeping cell failure is hard to address since no alarm is generally triggered. Sleeping cells can remain undetected for a long time and can severely affect the quality of service/quality of experience to users. To enhance the resiliency of the SBSs in sleeping cells, we design a mobile edge-cloud computing system and propose a semi-supervised learning-based framework to dynamically detect the sleeping cells. Particularly, we consider two indicators, recovery proportion and recovery speed, to measure the resiliency of the SBSs. Moreover, in the proposed scheme, experts’ optimization experience and each period’s detection results can be utilized to iteratively improve the performance. Then we adopt a dataset from real-world networks for performance evaluation. Trace-driven evaluation results demonstrate that the proposed scheme outperforms existing sleeping cell detection schemes, and can also reduce the communication and runtime costs and enhance the resiliency of the SBSs.},
journal = {ACM Trans. Sen. Netw.},
month = {apr},
articleno = {42},
numpages = {30},
keywords = {resiliency enhancement, semi-supervised learning, sleeping cell detection, Smart critical infrastructures, 5G/B5G, mobile edge-cloud computing}
}

@inproceedings{10.1145/3629479.3629502,
author = {Andrade, \'{A}lan J\'{u}nior da Cruz and Veloso, Ednilson and Santos, Gleison},
title = {What We Know About Software Dependability in DevOps - A Tertiary Study},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629479.3629502},
doi = {10.1145/3629479.3629502},
abstract = {Background: DevOps is viewed as an alternative approach to achieving high-quality software products. Dependability is recognized as a crucial aspect of software product quality. Existing literature highlights the lack of established standards, models, or methods for evaluating product quality within the DevOps paradigm. This emphasizes the need for further research to investigate the impact of DevOps on software quality attributes, particularly in relation to dependability.Objective: Our objective is to evaluate the scope of research on dependability in DevOps and identify what is known about this context by relating DevOps practices with dependability attributes. Method: We conducted a tertiary study to enhance the understanding of dependability in the context of DevOps. Results: We found 13 secondary studies that address dependability in DevOps. Within these studies, we identified 16 DevOps practices that have an impact on dependability and 12 attributes that are affected by DevOps practices. Additionally, we identified 6 measures related to dependability in the context of DevOps. Among the DevOps practices, the most commonly reported ones that impact dependability are Automation Practices, including deployment, testing, and infrastructure automation, as well as Cloud Computing Implementation. Conclusions: The results show that DevOps practices contribute to improve software dependability, mainly due to the impacts of these practices on dependability attributes. However, even though the literature reports some measures related to dependability, there is still a gap in understanding how organizations can assess dependability in DevOps.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
pages = {178–187},
numpages = {10},
keywords = {Dependability, Software Product Quality, DevOps},
location = {<conf-loc>, <city>Bras\'{\i}lia</city>, <country>Brazil</country>, </conf-loc>},
series = {SBQS '23}
}

@inproceedings{10.1145/3578244.3583728,
author = {Volpert, Simon and Erb, Benjamin and Eisenhart, Georg and Seybold, Daniel and Wesner, Stefan and Domaschka, J\"{o}rg},
title = {A Methodology and Framework to Determine the Isolation Capabilities of Virtualisation Technologies},
year = {2023},
isbn = {9798400700682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578244.3583728},
doi = {10.1145/3578244.3583728},
abstract = {The capability to isolate system resources is an essential characteristic of virtualisation technologies and is therefore important for research and industry alike. It allows the co-location of experiments and workloads, the partitioning of system resources and enables multi-tenant business models such as cloud computing. Poor isolation among tenants bears the risk of noisy-neighbour and contention effects which negatively impacts all of those use-cases. These effects describe the negative impact of one tenant onto another by utilising shared resources. Both industry and research provide many different concepts and technologies to realise isolation. Yet, the isolation capabilities of all these different approaches are not well understood; nor is there an established way to measure the quality of their isolation capabilities. Such an understanding, however, is of uttermost importance in practice to elaborately decide on a suited implementation. Hence, in this work, we present a novel methodology to measure the isolation capabilities of virtualisation technologies for system resources, that fulfils all requirements to benchmarking including reliability. It relies on an immutable approach, based on Experiment-as-Code. The complete process holistically includes everything from bare metal resource provisioning to the actual experiment enactment.The results determined by this methodology help in the decision for a virtualisation technology regarding its capability to isolate given resources. Such results are presented here as a closing example in order to validate the proposed methodology.},
booktitle = {Proceedings of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {149–160},
numpages = {12},
keywords = {virtualisation, framework, benchmarking, isolation},
location = {Coimbra, Portugal},
series = {ICPE '23}
}

@inproceedings{10.1145/3590837.3590922,
author = {M, Saravanan and S, Shiva Prasad},
title = {A Blockchain-Based, Distributed, Self Hosted And End To End Encrypted Cloud Storage System},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590922},
doi = {10.1145/3590837.3590922},
abstract = {Cloud computing is fast taking over due to its convenience and greater safety. You may access your files anytime you need them by using the cloud. Having a large following makes things simpler for consumers. Computing makes phase, programming, and system structure all possible. With the aid of these components, increased cloud-based information and profits might be obtained. It is great to plan and arrange work based on data. The most difficult issue for people with a background in logical thinking is planning work procedures to achieve customer service goals while keeping expenses in control. Although leveraging cloud storage to reduce expenses has been suggested, doing so can be difficult. Although leveraging cloud storage to reduce expenses has been suggested, doing so can be difficult. To handle cloud resources efficiently, a modular infrastructure is needed. Utilizing standards and calculations, parallel resource and service management is maximized in the cloud. Using different work flows to structure work processes is the most entertaining activity in cloud computing. Timing and price have an influence on service quality (tasks). Workflow-based relocation of virtual machines is more effective. NP-hard algorithms for subset and choice issues. Making a choice allows the server to save time and money. PSO and GWO interactions that are advantageous. In this undertaking, both time and money are considerations. The new approach should be used. The study affects the validity of process parameters. intuition with a convex shape. utilizing the PEFT technique. GWO analyses the time and money spent on cloud processes. It is suggested that VMs be optimized as hybrid, both locally and globally. heuristic algorithm based on PEFT. Optimization reduces the likelihood of making a mistake right away. The Grey Wolf Optimization and Floral pollination algorithm outperforms genetic and flower pollination techniques. Biomimicry is compared with swarm intelligence. For our analysis, we use LIGO, GENOME, CYBER SHAKE, and SIPHT. The difficulty and quantity of the jobs have an impact on workflow. A bio-inspired GA, GWO, and FPA are used in the optimization process. In an experimental arrangement, time and cost analysis for two to twenty VMs and workflows may be done. Compared to FPA with PEFT, GWO requires less time and money. In hybrid optimization, GWO and FPA are combined. In this project, efficiency and speed are highly valued. GWO optimizes VM globally, whereas FPA concentrates on local enhancements. FPA GA uses the collective wisdom of the group to identify correlations. As labor prices grow, more virtual machines (VMs) are employed for processing and tasks. Wait times drop and costs rise. Local and global optimization have an impact on virtual machine (VM) and compute time. ACO and PSO are used to accomplish local and global optimization, however employing them requires more time.},
booktitle = {Proceedings of the 4th International Conference on Information Management \&amp; Machine Intelligence},
articleno = {85},
numpages = {7},
keywords = {Cryptography, Cloud storage, Peer to Peer Network, Blockchain, IPFS},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@article{10.14778/3611540.3611566,
author = {Pan, Zhicheng and Wang, Yihang and Zhang, Yingying and Yang, Sean Bin and Cheng, Yunyao and Chen, Peng and Guo, Chenjuan and Wen, Qingsong and Tian, Xiduo and Dou, Yunliang and Zhou, Zhiqiang and Yang, Chengcheng and Zhou, Aoying and Yang, Bin},
title = {MagicScaler: Uncertainty-Aware, Predictive Autoscaling},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611566},
doi = {10.14778/3611540.3611566},
abstract = {Predictive autoscaling is a key enabler for optimizing cloud resource allocation in Alibaba Cloud's computing platforms, which dynamically adjust the Elastic Compute Service (ECS) instances based on predicted user demands to ensure Quality of Service (QoS). However, user demands in the cloud are often highly complex, with high uncertainty and scale-sensitive temporal dependencies, thus posing great challenges for accurate prediction of future demands. These in turn make autoscaling challenging---autoscaling needs to properly account for demand uncertainty while maintaining a reasonable trade-off between two contradictory factors, i.e., low instance running costs vs. low QoS violation risks.To address the above challenges, we propose a novel predictive autoscaling framework MagicScaler, consisting of a Multi-scale attentive Gaussian process based predictor and an uncertainty-aware scaler. First, the predictor carefully bridges the best of two successful prediction methodologies---multi-scale attention mechanisms, which are good at capturing complex, multi-scale features, and stochastic process regression, which can quantify prediction uncertainty, thus achieving accurate demand prediction with quantified uncertainty. Second, the scaler takes the quantified future demand uncertainty into a judiciously designed loss function with stochastic constraints, enabling flexible trade-off between running costs and QoS violation risks. Extensive experiments on three clusters of Alibaba Cloud in different Chinese cities demonstrate the effectiveness and efficiency of MagicScaler, which outperforms other commonly adopted scalers, thus justifying our design choices.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3808–3821},
numpages = {14}
}

@inproceedings{10.1145/3510513.3510534,
author = {Zhao, Yuhan and Chong, Zheng and Han, Xueying and Du, Zongpeng and Yu, Ke and Huang, Xiaohong},
title = {Simulation Study of Routing Mechanism in the Computing-Aware Network},
year = {2022},
isbn = {9781450385848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510513.3510534},
doi = {10.1145/3510513.3510534},
abstract = {Traditional routing algorithms select the path based on the cost, hop, or other network-related metrics. However, with cloud computing and edge computing development, it is essential to develop a computing-aware routing mechanism to distribute the task to the computing nodes with better capabilities. In order to achieve the combined scheduling of computing resources and network resources, Computing-aware Network (CAN) is proposed. The solution aims to comprehensively consider the computing resources and network resources existing when routing and forwarding user requests, which is essential to improve the current situation of uneven utilization of edge computing resources. As many theories and structures are proposed, the requirements for experimental verification of CAN are gradually increasing. However, there is still a lack of experimental simulation prototypes available for study. In this work, motivated by the gap, we propose a simulation prototype based on the idea of the CAN, where routing nodes can incorporate both the computing status and network status into routing and forward packets to an edge server according to the computing demand service type. We verified the effectiveness of the CAN prototype from simulation experiments on a cross-domain topology. From the results, we can conclude that the utilization rate of computing resources has been effectively improved compared to not considering computing status.},
booktitle = {Proceedings of the 2021 10th International Conference on Networks, Communication and Computing},
pages = {126–134},
numpages = {9},
keywords = {computing resources, utilization rate, Computing-aware Network, simulation prototype},
location = {Beijing, China},
series = {ICNCC '21}
}

@inproceedings{10.1145/3487552.3487854,
author = {Dang, The Khang and Mohan, Nitinder and Corneo, Lorenzo and Zavodovski, Aleksandr and Ott, J\"{o}rg and Kangasharju, Jussi},
title = {Cloudy with a Chance of Short RTTs: Analyzing Cloud Connectivity in the Internet},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487854},
doi = {10.1145/3487552.3487854},
abstract = {Cloud computing has seen continuous growth over the last decade. The recent rise in popularity of next-generation applications brings forth the question: "Can current cloud infrastructure support the low latency requirements of such apps?" Specifically, the interplay of wireless last-mile and investments of cloud operators in setting up direct peering agreements with ISPs globally to current cloud reachability and latency has remained largely unexplored.This paper investigates the state of end-user to cloud connectivity over wireless media through extensive measurements over six months. We leverage 115,000 wireless probes on the Speed-checker platform and 195 cloud regions from 9 well-established cloud providers. We evaluate the suitability of current cloud infrastructure to meet the needs of emerging applications and highlight various hindering pressure points. We also compare our results to a previous study over RIPE Atlas. Our key findings are: (i) the most impact on latency comes from the geographical distance to the datacenter; (ii) the choice of a measurement platform can significantly influence the results; (iii) wireless last-mile access contributes significantly to the overall latency, almost surpassing the impact of the geographical distance in many cases. We also observe that cloud providers with their own private network backbone and direct peering agreements with serving ISPs offer noticeable improvements in latency, especially in its consistency over longer distances.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {62–79},
numpages = {18},
keywords = {cloud connectivity, edge computing, peering, last-mile latency},
location = {Virtual Event},
series = {IMC '21}
}

@article{10.1145/3554735,
author = {Huang, Lihua and Zheng, Peng},
title = {Human-Computer Collaborative Visual Design Creation Assisted by Artificial Intelligence},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3554735},
doi = {10.1145/3554735},
abstract = {With the support and promotion of big data and cloud computing, AI has penetrated into every field of people's lives more and more deeply, with its characteristics of sustainable work, extremely fast computing speed, and a certain intelligence. This is an effective way to solve the general lack of demand and productivity of visual design, and relieve the pressure off designers to deal with relatively low-quality and high-demand designs. Therefore, the combination of design and artificial intelligence technology is a necessity. Research on the application of artificial intelligence technology for visual design is also in full swing at home and abroad However, at present, teams at home and abroad are in the exploratory stage. This paper considers whether it is possible to build an intelligent visual design and creation system by using artificial intelligence technology to help graphic communication designers achieve high-quality, high-efficiency, and high-quantity design output. Additionally, this paperexplores how to combine artificial intelligence technology with designers' design workflow so as to form a complementary human-computer cooperation mode. We will explore how to integrate AI technology with designers' design workflow and then create a human-machine collaboration model with complementary advantages to achieve the high quality, high efficiency, and high quantity of design output required by the intelligent visual design creation system being built. Finally, a basic framework of a generative smart human-computer collaborative visual design creation system based on a subset of neural network expert systems in multiple domains and an aggregate of different modules supported by the system is formed, and the working principle and usage process of the system are further elaborated with the example of packaging design.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
articleno = {221},
numpages = {21},
keywords = {cooperation mode, creation system, visual design creation, collaboration model, Artificial intelligence, packaging design, human-computer collaboration}
}

@inproceedings{10.1145/3590837.3590950,
author = {V, Vanjipriya and Annamalai, Suresh},
title = {Machine Learning Technique for Energy, Performance and Cost-Effective Resource Management in Multi-Access Edge Computing},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590950},
doi = {10.1145/3590837.3590950},
abstract = {Modern cloud interconnects on efficient resource allocation and provisioning to reduce their energy footprint. Resource management is complicated by factors such as data centre energy utilisation, virtual machine migration, operating expense, and overhead. Researchers have been using virtualized technologies and methods as optimal-Multi-objective particle swarm optimization, Dynamic Power Saving Resource Allocation (DPRA), Least Squares Regression, etc. to improve the management of their study. Accurately allocating resources to cloud users to meet their requests and offer QoS is a difficult task because of the preceding steps. Allocating cloud infrastructure's resources in the most efficient way possible benefits both users and service providers. The difficulties of resource management are tackled in this study by employing novel approaches, heuristics, authentication, and virtualization. In order to distribute workloads over several physical nodes, cloud computing relies on dynamic scheduling with load balancing. Using the help of host load prediction and a Markov chain model with Particle Swarm Optimization (PSO), VM resources are dynamically allocated to appropriate input requests. High quality of service (QoS) for cloud applications is achieved by SLA-based resource optimization with deadline, cost, storage, and bandwidth targets. Compliance with Service Level Agreements (SLAs), efficient use of resources, and low energy consumption are all achieved using a prioritisation technique based on SLAs. Scheduled users can receive resources in a predetermined order thanks to queuing. We developed the M/M/c/K queuing paradigm for numerous users per server to lessen the burden on data centres. Hardware resource models, such as CPU, I/O, and memory use, reveal VM resource allocation. Information gathering enhances resource utilisation and reduces energy consumption.},
booktitle = {Proceedings of the 4th International Conference on Information Management \&amp; Machine Intelligence},
articleno = {113},
numpages = {7},
keywords = {Load balancing, Service Level Agreements, Quality of Service, Virtual machine, Dynamic Power Saving Resource Allocation},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@inproceedings{10.1145/3491418.3530295,
author = {Rochlin, Nick and Gardner, Jeff and Kinney, Elizabeth},
title = {Evaluating Research Computing Training and Support as Part of a Broader Digital Research Infrastructure Needs Assessment},
year = {2022},
isbn = {9781450391610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491418.3530295},
doi = {10.1145/3491418.3530295},
abstract = {Digital Research Infrastructure (DRI) refers to the suite of tools and services that enables the collection, processing, dissemination, and disposition of research data. This includes strategies for planning, organizing, storing, sharing, computing, and ultimately archiving or destroying one's research data.&nbsp; These services must be supported by highly qualified personnel with the appropriate expertise.&nbsp; From May 17 - June 12, 2021, the University of British Columbia (UBC) Advanced Research Computing (UBC ARC) and the UBC Library from both Vancouver and Okanagan Campuses launched the DRI Needs Assessment Survey to investigate UBC researchers’ needs in 25 distinct DRI tools and services.&nbsp; The survey received a total of 241 responses, and following the survey, three focus groups were conducted with survey respondents to gain additional insights.This paper outlines the DRI Needs Assessment Survey and its findings, focusing on those directly related to UBC ARC services and training in high-performance computing (HPC) and cloud computing (“Cloud”), and discusses next steps for implementing a more collaborative, comprehensive research computing training and support model. Key findings suggest that while advanced research computing infrastructure is a key pillar of DRI, researchers utilizing UBC ARC also rely on a number of other DRI tools and services to conduct their research.&nbsp; These services are widely scattered across various departments and groups within and outside the institution and are oftentimes not well communicated, impacting researchers’ ability to find them.&nbsp; Current research training and support has been found to be inadequate, and there are duplicated service efforts occurring in silos, resulting in an inefficient service model and wasted funds.},
booktitle = {Practice and Experience in Advanced Research Computing},
articleno = {28},
numpages = {8},
keywords = {advanced research computing, user community, training, hpc, education, collaboration, cloud, digital research infrastructure},
location = {Boston, MA, USA},
series = {PEARC '22}
}

@article{10.1145/3550274,
author = {Liu, Yipeng and Yang, Qi and Xu, Yiling and Yang, Le},
title = {Point Cloud Quality Assessment: Dataset Construction and Learning-Based No-Reference Metric},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3550274},
doi = {10.1145/3550274},
abstract = {Full-reference (FR) point cloud quality assessment (PCQA) has achieved impressive progress in recent years. However, in many cases, obtaining the reference point clouds is difficult, so no-reference (NR) metrics have become a research hotspot. Few researches about NR-PCQA are carried out due to the lack of a large-scale PCQA dataset. In this article, we first build a large-scale PCQA dataset named LS-PCQA, which includes 104 reference point clouds and more than 22,000 distorted samples. In the dataset, each reference point cloud is augmented with 31 types of impairments (e.g., Gaussian noise, contrast distortion, local missing, and compression loss) at 7 distortion levels. Besides, each distorted point cloud is assigned with a pseudo-quality score as its substitute of Mean Opinion Score. Inspired by the hierarchical perception system and considering the intrinsic attributes of point clouds, we propose a NR metric ResSCNN based on sparse convolutional neural network (CNN) to accurately estimate the subjective quality of point clouds. We conduct several experiments to evaluate the performance of the proposed NR metric. The results demonstrate that ResSCNN exhibits the state-of-the-art performance among all the existing NR-PCQA metrics and even outperforms some FR metrics. The dataset presented in this work will be made publicly accessible at . The source code for the proposed ResSCNN can be found at .},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {80},
numpages = {26},
keywords = {sparse convolution, large-scale dataset, point cloud, Blind quality assessment, learning-based metric}
}

@inproceedings{10.1145/3592834.3592877,
author = {Lee, Kuan-Yu and Fang, Jia-Wei and Sun, Yuan-Chun and Hsu, Cheng-Hsin},
title = {Modeling Gamer Quality-of-Experience Using a Real Cloud VR Gaming Testbed},
year = {2023},
isbn = {9798400701894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592834.3592877},
doi = {10.1145/3592834.3592877},
abstract = {Cloud Virtual Reality (VR) gaming offloads computationally-intensive VR games to resourceful data centers. Ensuring good Quality of Experience (QoE) in cloud VR gaming, however, is inherently challenging as VR gamers demand high visual quality, short response time, and low cybersickness. In this paper, we investigate the QoE of cloud VR gaming in multiple steps. First, we build a cloud VR gaming testbed, which allows us to measure various Quality of Service (QoS) metrics. Second, we carry out a user study to understand the effects of diverse factors, including encoding settings, network conditions, and game genres on gamer QoE, quantified by Mean Opinion Score (MOS). Using our user study results, we construct QoE models for cloud VR gaming, which to the best of our knowledge, has not been done in the literature. Last, we apply our QoE models to develop a bitrate allocation algorithm for multiple cloud VR gamers to achieve better overall QoE compared to the bandwidth-fair bitrate allocation.},
booktitle = {Proceedings of the 15th International Workshop on Immersive Mixed and Virtual Environment Systems},
pages = {12–17},
numpages = {6},
keywords = {QoE modeling, bitrate allocation, VR gaming, cloud gaming},
location = {Vancouver, BC, Canada},
series = {MMVE '23}
}

@inproceedings{10.1145/3469877.3490578,
author = {Zhang, Ke-xin and Jiang, Gang-yi and Yu, Mei},
title = {FQM-GC: Full-Reference Quality Metric for Colored Point Cloud Based on Graph Signal Features and Color Features},
year = {2022},
isbn = {9781450386074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469877.3490578},
doi = {10.1145/3469877.3490578},
abstract = {Colored Point Cloud (CPC) is often distorted in the processes of its acquisition, processing, and compression, so reliable quality assessment metrics are required to estimate the perception of distortion of CPC. We propose a Full-reference Quality Metric for colored point cloud based on Graph signal features and Color features (FQM-GC). For geometric distortion, the normal and coordinate information of the sub-clouds divided via geometric segmentation is used to construct their underlying graphs, then, the geometric structure features are extracted. For color distortion, the corresponding color statistical features are extracted from regions divided with color attribution. Meanwhile, the color features of different regions are weighted to simulate the visual masking effect. Finally, all the extracted features are formed into a feature vector to estimate the quality of CPCs. Experimental results on three databases (CPCD2.0, IRPC and SJTU-PCQA) show that the proposed metric FQM-GC is more consistent with human visual perception.},
booktitle = {Proceedings of the 3rd ACM International Conference on Multimedia in Asia},
articleno = {48},
numpages = {5},
keywords = {graph signal processing, visual quality assessment, Colored point cloud, point cloud segmentation},
location = {<conf-loc>, <city>Gold Coast</city>, <country>Australia</country>, </conf-loc>},
series = {MMAsia '21}
}

@inproceedings{10.1145/3552482.3556552,
author = {Freitas, Pedro G. and Lucafo, Giovani D. and Gon\c{c}alves, Mateus and Homonnai, Johann and Diniz, Rafael and Farias, Myl\`{e}ne C.Q.},
title = {Comparative Evaluation of Temporal Pooling Methods for No-Reference Quality Assessment of Dynamic Point Clouds},
year = {2022},
isbn = {9781450395007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552482.3556552},
doi = {10.1145/3552482.3556552},
abstract = {Point Cloud Quality Assessment (PCQA) has become an important task in immersive multimedia since it is fundamental for improving computer graphics applications and ensuring the best Quality of Experience(QoE) for the end user. In recent years, the field of PCQA has made exemplary progress, with state-of-the-art methods achieving better predictive performance at lower computational complexity. However, most of this progress was made using Full Reference (FR) metrics. Since, in many cases, the reference point cloud is not available, the design of No-Reference (NR) methods has become increasingly important. In this paper, we investigate the suitability of geometric-aware texture descriptors to blindly assess the quality of colored Dynamic Point Cloud (DPC). The proposed metric first uses a descriptor to extract features of the assessed Point Cloud (PC) frames. Then, the descriptor statistics are used to extract quality-aware features. Finally, a machine learning algorithm is employed to regress the quality-aware features into visual quality scores, and these scores are aggregated using a temporal pooling function. Then we study the effects of different temporal pooling strategies on the performance of DPC quality assessment methods. Our experimental tests were carried out using the latest publicly available database and demonstrated the efficiency of the evaluated temporal pooling models. This work aims to provide a direction on how to apply a temporal pooling function to combine per-frame quality predictions generated with descriptor based PC quality assessment methods to estimate the quality of dynamic PCs. An implementation of the metric described in this paper can be found in https://gitlab.com/gpds-unb/no-referencedpcqa-temporal-pooling.},
booktitle = {Proceedings of the 1st Workshop on Photorealistic Image and Environment Synthesis for Multimedia Experiments},
pages = {35–41},
numpages = {7},
keywords = {quality assessment, quality metric, qoe methods, point cloud},
location = {Lisboa, Portugal},
series = {PIES-ME '22}
}

@inproceedings{10.1145/3552469.3555710,
author = {Tliba, Marouane and Chetouani, Aladine and Valenzise, Giuseppe and Dufaux, Frederic},
title = {Point Cloud Quality Assessment Using Cross-Correlation of Deep Features},
year = {2022},
isbn = {9781450394994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552469.3555710},
doi = {10.1145/3552469.3555710},
abstract = {3D point clouds have emerged as a preferred format for recent immersive communication systems, due to the six degrees of freedom they offer. The huge data size of point clouds, which consists of both geometry and color information, has motivated the development of efficient compression schemes recently. To support the optimization of these algorithms, adequate and efficient perceptual quality metrics are needed. In this paper we propose a novel end-to-end deep full-reference framework for 3D point cloud quality assessment, considering both the geometry and color information. We use two identical neural networks, based on a residual permutation-invariant architecture, for extracting local features from a sparse set of patches extracted from the point cloud. Afterwards, we measure the cross-correlation between the embedding of pristine and distorted point clouds to quantify the global shift in the features due to visual distortion. The proposed scheme achieves comparable results to state-of-the-art metrics even when a small number of centroids are used, reducing the computational complexity.},
booktitle = {Proceedings of the 2nd Workshop on Quality of Experience in Visual Multimedia Applications},
pages = {63–68},
numpages = {6},
keywords = {cross correlation, deep learning, point cloud, quality assessment},
location = {Lisboa, Portugal},
series = {QoEVMA '22}
}

@inproceedings{10.1145/3569052.3578907,
author = {Hogan, Taylor},
title = {Goal Driven PCB Synthesis Using Machine Learning and CloudScale Compute},
year = {2023},
isbn = {9781450399784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569052.3578907},
doi = {10.1145/3569052.3578907},
abstract = {X AI is a cloud-based system that leverages machine learning, and search to place and route printed circuit boards using physics-based analysis and high-level design. We propose a feedback-based Monte Carlo Tree Search (MCTS) algorithm to explore the space of possible designs. A metric, or metrics, is given to evaluate the quality of designs as MCTS learns about possible solutions. A policy and value network are trained during exploration to learn to accurately weight quality actions and identify useful design states. This is performed as a feedback loop in conjunction with other feedforward tools for placement and routing.},
booktitle = {Proceedings of the 2023 International Symposium on Physical Design},
pages = {80},
numpages = {1},
keywords = {monte carlo tree search, si/pi driven synthesis, reinforcement learning, machine learning, pcb design},
location = {Virtual Event, USA},
series = {ISPD '23}
}

@inproceedings{10.1145/3581783.3613847,
author = {Zhang, Junteng and Chen, Tong and Ding, Dandan and Ma, Zhan},
title = {YOGA: Yet Another Geometry-Based Point Cloud Compressor},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613847},
doi = {10.1145/3581783.3613847},
abstract = {A learning-based YOGA (Yet Another Geometry-based Point Cloud Compressor) is proposed. It is flexible, allowing for the separable lossy compression of geometry and color attributes, and variable-rate coding using a single neural model; it is high-efficiency, significantly outperforming the latest G-PCC standard quantitatively and qualitatively, e.g., 25\% BD-BR gains using PCQM (Point Cloud Quality Metric) as the distortion assessment, and it is lightweight, e.g., similar runtime as the G-PCC codec, owing to the use of sparse convolution and parallel entropy coding. To this end, YOGA adopts a unified end-to-end learning-based backbone for separate geometry and attribute compression. The backbone uses a two-layer structure, where the downscaled thumbnail point cloud is encoded using G-PCC at the base layer, and upon G-PCC compressed priors, multiscale sparse convolutions are stacked at the enhancement layer to effectively characterize spatial correlations to compactly represent the full-resolution sample. In addition, YOGA integrates the adaptive quantization and entropy model group to enable variable-rate control, as well as adaptive filters for better quality restoration.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9070–9081},
numpages = {12},
keywords = {point cloud compression, layered coding, attribute, geometry},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{10.1145/3507623.3507628,
author = {Bhambani, Krisha and Takalikar, Mukta},
title = {DeCloud GAN: An Advanced Generative Adversarial Network for Removing Cloud Cover in Optical Remote Sensing Imagery},
year = {2022},
isbn = {9781450385930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507623.3507628},
doi = {10.1145/3507623.3507628},
abstract = {Optical Remote Sensing imagery has several applications in monitoring the states of natural and man-made features around the globe. However, due to clouds and other climatic conditions, information extracted from the imagery retrieved is very limited. Deep learning has often been used in several image processing and remote sensing tasks. In this work, we propose the usage of generative adversarial networks to remove clouds and other climatic interference from high-resolution remote sensing imagery. We have trained and tested upon the Remote sensing Image Cloud rEmoving dataset (RICE). The novel network(DeCloud GAN) we propose, makes use of residual UNets and pixel shuffle layers in the generator, which yield high quality cloudless satellite images. We have tested 4 methods for comparison, and have found that DeCloudGAN achieves the best performance on two main metrics, peak signal to noise ratio (PSNR) and structural similarity index (SSIM), to measure similarity in visual perception of the produced and target images.},
booktitle = {Proceedings of the 2021 4th International Conference on Computational Intelligence and Intelligent Systems},
pages = {25–30},
numpages = {6},
keywords = {Remote Sensing, Generative Adversarial Networks, Image generation, Deep Learning, Optical Imagery},
location = {Tokyo, Japan},
series = {CIIS '21}
}

@inproceedings{10.1145/3615452.3617942,
author = {Yin, Haofei and Xiao, Mengbai and Yu, Dongxiao},
title = {Preserving High Quality in A Learning-Based Compression Model for Point Cloud Videos},
year = {2023},
isbn = {9798400703393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615452.3617942},
doi = {10.1145/3615452.3617942},
abstract = {High-resolution point cloud videos combined with 3D scenes can create creative viewing modes. However, their enormous data volume demands effective compression techniques. In this work, we propose a deep learning-based model for compressing point cloud videos. Compared to D-PCC, the state-of-the-art model designed for preserving high quality after decompression, the reconstruction result of our method achieves up to a 29.73\% improvement in the density metric and a noticeable improvement in human visual perception. We also discuss extending our model to compress color information and effectively remove inter-frame redundancy.},
booktitle = {Proceedings of the 1st ACM Workshop on Mobile Immersive Computing, Networking, and Systems},
pages = {215–221},
numpages = {7},
location = {Madrid, Spain},
series = {ImmerCom '23}
}

@inproceedings{10.1145/3479986.3480000,
author = {Couto, Luis and Teixeira Lopes, Carla},
title = {Equal Opportunities in the Access to Quality Online Health Information? A Multi-Lingual Study on Wikipedia},
year = {2021},
isbn = {9781450385008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479986.3480000},
doi = {10.1145/3479986.3480000},
abstract = {Wikipedia is a free, multilingual, and collaborative online encyclopedia. Nowadays, it is one of the largest sources of online knowledge, often appearing at the top of the results of the major search engines, being one of the most sought-after resources by the public searching for health information. The collaborative nature of Wikipedia raises security concerns since this information is used for decision-making, especially in the health area. Despite being available in hundreds of idioms, there are asymmetries between idioms, namely regarding their quality. In this work, we compare the quality of health information on Wikipedia between idioms with 100 million native speakers or more, and also in Greek, Italian, Korean, Turkish, Persian, Catalan and Hebrew, for historical tradition. Quality metrics are applied to health and medical articles in English, maintained by WikiProject Medicine, and their versions in the above idioms. With this, we contribute to a clarification of the role of Wikipedia in the access to health information. We demonstrate differences in both the quantity and quality of information available between idioms. English is the idiom with the highest quality in general. Urdu, Greek, Indonesian, and Hindi achieved lower values of quality.},
booktitle = {Proceedings of the 17th International Symposium on Open Collaboration},
articleno = {13},
numpages = {13},
keywords = {Information Quality, Health information, Wikipedia, Multilingual information access},
location = {Online, Spain},
series = {OpenSym '21}
}

@inproceedings{10.1145/3552469.3555713,
author = {Bourbia, Salima and Karine, Ayoub and Chetouani, Aladine and El Hassouni, Mohammed and Jridi, Maher},
title = {No-Reference Point Clouds Quality Assessment Using Transformer and Visual Saliency},
year = {2022},
isbn = {9781450394994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552469.3555713},
doi = {10.1145/3552469.3555713},
abstract = {Quality estimation of 3D objects/scenes represented by cloud point is a crucial and challenging task in computer vision. In real-world applications, reference data is not always available, which motivates the development of new point cloud quality assessment (PCQA) metrics that do not require the original 3D point cloud (3DPC). This family of methods is called no-reference or blind PCQA. In this context, we propose a deep-learning-based approach that benefits from the advantage of the self-attention mechanism in transformers to accurately predict the perceptual quality score for each degraded 3DPC. Additionally, we introduce the use of saliency maps to reflect the human visual system behavior that is attracted to some specific regions compared to others during the evaluation. To this end, we first render 2D projections (i.e. views) of a 3DPC from different viewpoints. Then, we weight the obtained projected images with their corresponding saliency maps. After that, we discard the majority of the background information by extracting sub-salient images. The latter is introduced as a sequential input of the vision transformer in order to extract the global contextual information and to predict the quality scores of the sub-images. Finally, we average the scores of all the salient sub-images to obtain the perceptual 3DPC quality score. We evaluate the performance of our model on the ICIP2020 and SJTU point cloud quality assessment benchmarks. Experimental results show that our model achieves promising performance compared to the state-of-the-art point cloud quality assessment metrics.},
booktitle = {Proceedings of the 2nd Workshop on Quality of Experience in Visual Multimedia Applications},
pages = {57–62},
numpages = {6},
keywords = {attention, visual saliency, 3d point clouds, objective quality assessment, transformer},
location = {Lisboa, Portugal},
series = {QoEVMA '22}
}

@inproceedings{10.1145/3552457.3555730,
author = {Prazeres, Joao and Rodrigues, Rafael and Pereira, Manuela and Pinheiro, Antonio M.G.},
title = {Quality Evaluation of Machine Learning-Based Point Cloud Coding Solutions},
year = {2022},
isbn = {9781450394918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552457.3555730},
doi = {10.1145/3552457.3555730},
abstract = {In this paper, a quality evaluation of three point cloud coding solutions based on machine learning technology is presented, notably, ADLPCC, PCC_GEO_CNN, and PCGC, as well as LUT_SR, which uses multi-resolution Look-Up Tables. Moreover, the MPEG G-PCC was used as an anchor. A set of six point clouds, representing both landscapes and objects were coded using the five encoders at different bit rates, and a subjective test, where the distorted and reference point clouds were rotated in a video sequence side by side, is carried out to assess their performance. Furthermore, the performance of point cloud objective quality metrics that usually provide a good representation of the coded content is analyzed against the subjective evaluation results. The obtained results suggest that some of these metrics fail to provide a good representation of the perceived quality, and thus are not suitable to evaluate some distortions created by machine learning-based solutions. A comparison between the analyzed metrics and the type of represented scene or codec is also presented.},
booktitle = {Proceedings of the 1st International Workshop on Advances in Point Cloud Compression, Processing and Analysis},
pages = {57–65},
numpages = {9},
keywords = {machine learning, point clouds, quality evaluation, coding},
location = {Lisboa, Portugal},
series = {APCCPA '22}
}

@inproceedings{10.1145/3503161.3548374,
author = {Jamshidi Avanaki, Nasim and Schmidt, Steven and Michael, Thilo and Zadtootaghaj, Saman and M\"{o}ller, Sebastian},
title = {Deep-BVQM: A Deep-Learning Bitstream-Based Video Quality Model},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548374},
doi = {10.1145/3503161.3548374},
abstract = {With the rapid increase of video streaming content, high-quality video quality metrics, mainly signal-based video quality metrics, are emerging, notably VMAF, SSIMPLUS, and AVQM. Besides signal-based video quality metrics, within the standardization body, ITU-T Study Group 12, two well-known bitstream-based video quality metrics are developed named P.1203 and P.1204.3. Due to the low complexity and low level of access to the bitstream data, these models gained attention from network providers and service providers. In this paper, we proposed a new bitstream-based model named Deep-BVQM, which outperforms the standard models on the tested datasets. While the model comes with slightly higher computational complexity, it offers a frame-level quality prediction which is essential diagnostic information for some video streaming services such as cloud gaming. Deep-BVQM is developed in two layers; first, the frame quality was predicted using a lightweight CNN model. Next, the latent features of the CNN were used to train an LSTM network to predict the video quality in a short-term duration.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {915–923},
numpages = {9},
keywords = {bitstream-based quality model, quality of experience, video quality},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3489517.3530496,
author = {Yayla, Mikail and Chen, Jian-Jia},
title = {Memory-Efficient Training of Binarized Neural Networks on the Edge},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530496},
doi = {10.1145/3489517.3530496},
abstract = {A visionary computing paradigm is to train resource efficient neural networks on the edge using dedicated low-power accelerators instead of cloud infrastructures, eliminating communication overheads and privacy concerns. One promising resource-efficient approach for inference is binarized neural networks (BNNs), which binarize parameters and activations. However, training BNNs remains resource demanding. State-of-the-art BNN training methods, such as the binary optimizer (Bop), require to store and update a large number of momentum values in the floating point (FP) format.In this work, we focus on memory-efficient FP encodings for the momentum values in Bop. To achieve this, we first investigate the impact of arbitrary FP encodings. When the FP format is not properly chosen, we prove that the updates of the momentum values can be lost and the quality of training is therefore dropped. With the insights, we formulate a metric to determine the number of unchanged momentum values in a training iteration due to the FP encoding. Based on the metric, we develop an algorithm to find FP encodings that are more memory-efficient than the standard FP encodings. In our experiments, the memory usage in BNN training is decreased by factors 2.47x, 2.43x, 2.04x, depending on the BNN model, with minimal accuracy cost (smaller than 1\%) compared to using 32-bit FP encoding.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {661–666},
numpages = {6},
location = {San Francisco, California},
series = {DAC '22}
}

@inproceedings{10.1145/3617233.3617247,
author = {Bourbia, Salima and Karine, Ayoub and Chetouani, Aladine and El Hassouni, Mohammed and Jridi, Maher},
title = {Multi-Stream Point-Based Model for Blind Geometric Point Cloud Quality Assessment},
year = {2023},
isbn = {9798400709128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617233.3617247},
doi = {10.1145/3617233.3617247},
abstract = {The evaluation of 3D point cloud quality is a critical component in the development of immersive multimedia systems for real-world applications. While perceptual quality evaluation technics for 2D images and videos have reached high performances, developing robust and efficient blind metrics for point cloud quality assessment is still challenging. In this paper, we propose a no-reference point cloud quality assessment method that evaluates the quality of degraded 3D objects using an end-to-end point-based multi-stream model. To capture the geometric degradation of the point cloud, we incorporate normals, curvatures and geometric coordinates. Then, we divide the distorted object into sub-objects, which are fed to a multi-stream network to extract significant features of the geometric degradation. Afterward, these features are used to predict the quality of each sub-object, and the perceptual quality score of the point cloud is obtained by averaging the quality scores of all sub-objects. Experimental results demonstrate that the proposed model achieves promising performance compared to state-of-the- art full and reduced methods.},
booktitle = {Proceedings of the 20th International Conference on Content-Based Multimedia Indexing},
pages = {224–228},
numpages = {5},
keywords = {Quality assessment, 3D point cloud, Multi-stream, Deep learning., Point-based model},
location = {<conf-loc>, <city>Orleans</city>, <country>France</country>, </conf-loc>},
series = {CBMI '23}
}

@inproceedings{10.1145/3611643.3616316,
author = {Agarwal, Shubham and Chakraborty, Sarthak and Garg, Shaddy and Bisht, Sumit and Jain, Chahat and Gonuguntla, Ashritha and Saini, Shiv},
title = {Outage-Watch: Early Prediction of Outages Using Extreme Event Regularizer},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616316},
doi = {10.1145/3611643.3616316},
abstract = {Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is predicted if the probability of any one of the QoS metrics crossing threshold changes significantly. Our evaluation on a real-world SaaS company dataset shows that Outage-Watch significantly outperforms traditional methods with an average AUC of 0.98. Additionally, Outage-Watch detects all the outages exhibiting a change in service metrics and reduces the Mean Time To Detection (MTTD) of outages by up to 88\% when deployed in an enterprise cloud-service system, demonstrating efficacy of our proposed method.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {682–694},
numpages = {13},
keywords = {Distribution Learning, Mixture Density Network, Outage Forecasting, System reliability and monitoring},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ISCA52012.2021.00044,
author = {Zha, Yue and Li, Jing},
title = {Hetero-ViTAL: A Virtualization Stack for Heterogeneous FPGA Clusters},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00044},
doi = {10.1109/ISCA52012.2021.00044},
abstract = {With field-programmable gate arrays (FPGAs) being widely deployed into data centers, an efficient virtualization support is required to fully unleash the potential of cloud FPGAs. Nevertheless, existing FPGA virtualization solutions only support a homogeneous FPGA cluster comprising identical FPGA devices. Representative work such as ViTAL provides sufficient system support for scale-out acceleration and improves the overall resource utilization through a fine-grained spatial sharing. While these existing solutions (including ViTAL) can efficiently virtualize a homogeneous cluster, it is hard to extend them to virtualizing a heterogeneous cluster which comprises multiple types of FPGAs. We expect the future cloud FPGAs are likely to be more heterogeneous due to hardware rolling upgrade.In this paper, we rethink FPGA virtualization from ground up and propose HETERO-VITAL to virtualize heterogeneous FPGA clusters. We identify the conflicting requirements of runtime management and offline compilation when designing the abstraction for a heterogeneous cluster, which is also the fundamental reason why the single-level abstraction as proposed in ViTAL (and other prior works) cannot be trivially extended to the heterogeneous case. To decouple these conflicting requirements, we provide a two-level system abstraction in HETERO-VITAL. Specifically, the high-level abstraction is FPGA-agnostic and provides a simple and homogeneous view of the FPGA resources to simplify the runtime management. On the contrary, the low-level abstraction is FPGA-specific and exposes sufficient spatial resource constraints to the compilation framework to ensure the mapping quality. Rather than simply adding a layer on top of the single-level abstraction as proposed in ViTAL and other prior work, we judiciously determine how much hardware details should be exposed at each level to balance the management complexity, mapping quality and compilation cost. We then develop a compilation framework to map applications onto this two-level abstraction with several optimization techniques to further improve the mapping quality. We also provide a runtime management policy to alleviate the fragmentation issue, which becomes more severe in a heterogeneous cluster due to the distinct resource capacities of diverse FPGAs.We evaluate HETERO-VITAL on a custom-built FPGA cluster and demonstrate its effectiveness using machine learning and image processing applications. Results show that HETERO-VITAL reduces the average response time (a critical metric for QoS) by 79.2\% for a heterogeneous cluster compared to the non-virtualized baseline. When virtualizing a homogeneous cluster, HETERO-VITAL also reduces the average response time by 42.0\% compared with ViTAL due to a better system design.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {470–483},
numpages = {14},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1145/3474085.3475294,
author = {Zhang, Yujie and Yang, Qi and Xu, Yiling},
title = {MS-GraphSIM: Inferring Point Cloud Quality via Multiscale Graph Similarity},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475294},
doi = {10.1145/3474085.3475294},
abstract = {To address the point cloud quality assessment (PCQA) problem, GraphSIM was proposed via jointly considering geometrical and color features, which shows compelling performance in multiple distortion detection. However, GraphSIM does not take into account the mutiscale characteristics of human perception. In this paper, we propose a multiscale PCQA model, called Multiscale Graph Similarity (MS-GraphSIM), that can better predict human subjective perception. First, exploring the multiscale processing method used in image processing, we introduce a multiscale representation of point clouds based on graph signal processing. Second, we extend GraphSIM into multiscale version based on the proposed multiscale representation. Specifically, MS-GraphSIM constructs a multiscale representation for each local patch extracted from the reference point cloud or the distorted point cloud, and then fuses GraphSIM at different scales to obtain an overall quality score. Experiment results demonstrate that the proposed MS-GraphSIM outperforms the state-of-the-art PCQA metrics over two fairly large and independent databases. Ablation studies further prove the proposed MS-GraphSIM is robust to different model hyperparameter settings. The code is available at https://github.com/zyj1318053/MS_GraphSIM.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1230–1238},
numpages = {9},
keywords = {graph signal processing, multiscale representation, quality assessment, point cloud},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1145/3595244.3595257,
author = {Prakash, R Sri and Karamchandani, Nikhil and Moharir, Sharayu},
title = {On the Regret of Online Edge Service Hosting},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/3595244.3595257},
doi = {10.1145/3595244.3595257},
abstract = {We consider the problem of service hosting where a service provider can dynamically rent edge resources via short term contracts to ensure better quality of service to its customers. The total cost incurred by the system is modeled as a combination of the rent cost, the service cost incurred due to latency in serving customers, and the fetch cost incurred as a result of the bandwidth used to fetch the code/databases of the service from the cloud servers to host the service at the edge. In this paper, we compare multiple hosting policies with regret as a metric, defined as the difference in the cost incurred by the policy and the optimal policy over some time horizon T. In particular we consider the Retro Renting (RR) and Follow The Perturbed Leader (FTPL) policies proposed in the literature and provide performance guarantees on the regret of these policies. We show that under i.i.d Bernoulli arrivals, RR policy has linear regret while FTPL policy has constant regret. Next, we propose a variant of FTPL, namely Wait then FTPL (W-FTPL), which also has constant regret while demonstrating much better dependence on the fetch cost. We also show that under adversarial arrivals, RR policy has linear regret while both FTPL and W-FTPL have regret O(pT) which is order-optimal.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {35–37},
numpages = {3}
}

@inproceedings{10.1145/3578244.3583721,
author = {Straesser, Martin and Eismann, Simon and von Kistowski, J\'{o}akim and Bauer, Andr\'{e} and Kounev, Samuel},
title = {Autoscaler Evaluation and Configuration: A Practitioner's Guideline},
year = {2023},
isbn = {9798400700682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578244.3583721},
doi = {10.1145/3578244.3583721},
abstract = {Autoscalers are indispensable parts of modern cloud deployments and determine the service quality and cost of a cloud application in dynamic workloads. The configuration of an autoscaler strongly influences its performance and is also one of the biggest challenges and showstoppers for the practical applicability of many research autoscalers. Many proposed cloud experiment methodologies can only be partially applied in practice, and many autoscaling papers use custom evaluation methods and metrics. This paper presents a practical guideline for obtaining meaningful and interpretable results on autoscaler performance with reasonable overhead. We provide step-by-step instructions for defining realistic usage behaviors and traffic patterns. We divide the analysis of autoscaler performance into a qualitative antipattern-based analysis and a quantitative analysis. To demonstrate the applicability of our guideline, we conduct several experiments with a microservice of our industry partner in a realistic test environment.},
booktitle = {Proceedings of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {31–41},
numpages = {11},
keywords = {antipatterns, evaluation, methodology, autoscaling, guideline},
location = {Coimbra, Portugal},
series = {ICPE '23}
}

@article{10.1145/3510415,
author = {Zhong, Zhiheng and Xu, Minxian and Rodriguez, Maria Alejandra and Xu, Chengzhong and Buyya, Rajkumar},
title = {Machine Learning-Based Orchestration of Containers: A Taxonomy and Future Directions},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3510415},
doi = {10.1145/3510415},
abstract = {Containerization is a lightweight application virtualization technology, providing high environmental consistency, operating system distribution portability, and resource isolation. Existing mainstream cloud service providers have prevalently adopted container technologies in their distributed system infrastructures for automated application management. To handle the automation of deployment, maintenance, autoscaling, and networking of containerized applications, container orchestration is proposed as an essential research problem. However, the highly dynamic and diverse feature of cloud workloads and environments considerably raises the complexity of orchestration mechanisms. Machine learning algorithms are accordingly employed by container orchestration systems for behavior modeling and prediction of multi-dimensional performance metrics. Such insights could further improve the quality of resource provisioning decisions in response to the changing workloads under complex environments. In this article, we present a comprehensive literature review of existing machine learning-based container orchestration approaches. Detailed taxonomies are proposed to classify the current researches by their common features. Moreover, the evolution of machine learning-based container orchestration technologies from the year 2016 to 2021 has been designed based on objectives and metrics. A comparative analysis of the reviewed techniques is conducted according to the proposed taxonomies, with emphasis on their key characteristics. Finally, various open research challenges and potential future directions are highlighted.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {217},
numpages = {35},
keywords = {machine learning, systematic review, Container orchestration, cloud computing, resource provisioning}
}

@inproceedings{10.1145/3526059.3533617,
author = {Makris, Antonios and Psomakelis, Evangelos and Theodoropoulos, Theodoros and Tserpes, Konstantinos},
title = {Towards a Distributed Storage Framework for Edge Computing Infrastructures},
year = {2022},
isbn = {9781450393102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526059.3533617},
doi = {10.1145/3526059.3533617},
abstract = {Due to the continuous development of Internet of Things (IoT), the volume of the data these devices generate are expected to grow dramatically in the future. As a result, managing and processing such massive data amounts at the edge becomes a vital issue. Edge computing moves data and computation closer to the client enabling latency- and bandwidth-sensitive applications, that would not be feasible using cloud and remote processing alone. Nevertheless, implementing an efficient edge-enabled storage system is challenging due to the distributed and heterogeneous nature of the edge and its limited resource capabilities. To this end, we propose a lightweight hybrid distributed edge/cloud storage framework which aims to improve the Quality of Experience (QoE) of the end-users by migrating data close to them, thus reducing data transfers delays and network utilization. The proposed edge storage component (ESC) exploits the Dynamic Lifecycle Framework, in order to enable transparent and automated access for containerized applications to remote workloads. The effectiveness of the ESC is evaluated by employing a number of resource utilization and Quality of Service (QoS) metrics.},
booktitle = {Proceedings of the 2nd Workshop on Flexible Resource and Application Management on the Edge},
pages = {9–14},
numpages = {6},
keywords = {edge computing, edge storage, cloud computing, kubernetes, internet of things, container-based, minio, virtualization},
location = {Minneapolis, MN, USA},
series = {FRAME '22}
}

@inproceedings{10.1145/3487552.3487862,
author = {Mok, Ricky K. P. and Zou, Hongyu and Yang, Rui and Koch, Tom and Katz-Bassett, Ethan and Claffy, K C},
title = {Measuring the Network Performance of Google Cloud Platform},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487862},
doi = {10.1145/3487552.3487862},
abstract = {Public cloud platforms are vital in supporting online applications for remote learning and telecommuting during the COVID-19 pandemic. The network performance between cloud regions and access networks directly impacts application performance and users' quality of experience (QoE). However, the location and network connectivity of vantage points often limits the visibility of edge-based measurement platforms (e.g., RIPE Atlas).We designed and implemented the CLoud-based Applications Speed Platform (CLASP) to measure performance to various networks from virtual machines in cloud regions with speed test servers that have been widely deployed on the Internet. In our five-month longitudinal measurements in Google Cloud Platform (GCP), we found that 30-70\% of ISPs we measured showed severe throughput degradation from the peak throughput of the day.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {54–61},
numpages = {8},
keywords = {cloud networking, network throughput, speed test},
location = {Virtual Event},
series = {IMC '21}
}

@inproceedings{10.1145/3581783.3611998,
author = {Xie, Wuyuan and Wang, Kaimin and Ju, Yakun and Wang, Miaohui},
title = {PmBQA: Projection-Based Blind Point Cloud Quality Assessment via Multimodal Learning},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611998},
doi = {10.1145/3581783.3611998},
abstract = {With the increasing communication and storage of point cloud data, there is an urgent need for an effective objective method to measure the quality before and after processing. To address this difficulty, we propose a projection-based blind quality indicator via multimodal learning for point cloud data, which can perceive both geometric distortion and texture distortion by using four homogeneous modalities (i.e., texture, normal, depth and roughness). To fully exploit the multimodal information, we further develop a deformable convolutionbased alignment module and a graph-based feature fusion module, and investigate a graph node attention-based evaluation method to forecast the quality score. Extensive experimental results on three benchmark databases show that our method achieves more accurate evaluation performance in comparison with 12 competitive methods.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {3250–3258},
numpages = {9},
keywords = {quality assessment, no reference, point cloud, multimodal information},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{10.1145/3630590.3630602,
author = {Mukhia, Raunak and Sarambage Jayarathna, Kalana Gayashan and Lertsinsrubtavee, Adisorn},
title = {Performance Evaluation of LoRaWAN Forest Fire Monitoring Network in the Wild},
year = {2023},
isbn = {9798400709395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630590.3630602},
doi = {10.1145/3630590.3630602},
abstract = {To leverage the potential of LoRaWAN, we have successfully developed a real-world field wireless sensor network dedicated to monitoring forest fires and air quality. This network operates across remote forested regions and semi-urban areas. The development effort encompasses the complete LoRaWAN network stack, including a tailored circuit board designed for LoRa communication and multi-sensor nodes, the establishment of network infrastructure, and a cloud-based data collection platform that strictly adheres to the LoRaWAN standard. In this context, we have introduced a retransmission mechanism in the LoRaWAN application layer for sensor data completeness, along with lorawanatd which operates the LoRaWAN hardware. This extension serves to enhance communication robustness between end nodes and the network cloud, ensuring a seamless and reliable data transmission. Our initiative also involved a comprehensive series of experiments, conducted using sensor nodes situated in proximity to forest fire-prone areas. These experiments were conducted to delve into optimal configurations and constraints related to radio wave propagation. Key performance metrics guiding these investigations include the Received Signal Strength Indicator (RSSI), Signal to Noise Ratio (SNR), Packet Delivery Ratio (PDR), and Data Completeness Ratio (DCR).},
booktitle = {Proceedings of the 18th Asian Internet Engineering Conference},
pages = {96–104},
numpages = {9},
keywords = {LoRaWAN, Low Power Wide Area Networks, Forest Fire, Wireless Sensor Network, Internet of Things},
location = {<conf-loc>, <city>Hanoi</city>, <country>Vietnam</country>, </conf-loc>},
series = {AINTEC '23}
}

@article{10.1145/3470972,
author = {Dias, Alexandre H. T. and Correia, Luiz. H. A. and Malheiros, Neumar},
title = {A Systematic Literature Review on Virtual Machine Consolidation},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3470972},
doi = {10.1145/3470972},
abstract = {Virtual machine consolidation has been a widely explored topic in recent years due to Cloud Data Centers’ effect on global energy consumption. Thus, academia and companies made efforts to achieve green computing, reducing energy consumption to minimize environmental impact. By consolidating Virtual Machines into a fewer number of Physical Machines, resource provisioning mechanisms can shutdown idle Physical Machines to reduce energy consumption and improve resource utilization. However, there is a tradeoff between reducing energy consumption while assuring the Quality of Service established on the Service Level Agreement. This work introduces a Systematic Literature Review of one year of advances in virtual machine consolidation. It provides a discussion on methods used in each step of the virtual machine consolidation, a classification of papers according to their contribution, and a quantitative and qualitative analysis of datasets, scenarios, and metrics.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {176},
numpages = {38},
keywords = {systematic literature review, virtual machines consolidation, Cloud computing, green computing}
}

@inproceedings{10.1145/3615453.3616511,
author = {Kak, A. and Thieu, H. -T. and Pham, V. -Q. and Sheshadri, R. K. and Choi, N. and Guan, Y. and Yin, M. and Han, T.},
title = {AweRAN: Making a Case for Application-Aware Radio Access Network Slicing},
year = {2023},
isbn = {9798400703409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615453.3616511},
doi = {10.1145/3615453.3616511},
abstract = {As communications service providers ponder ways to cater to the diverse traffic requirements of mobile applications that range from the classic telephony to modern augmented reality (AR)-related use cases, the traditional quality of service (QoS)-based radio resource management (RRM) techniques for RAN slicing that are agnostic to the intrinsic workings of applications can result in a poor quality of experience (QoE) for the end-user. We argue that in addition to QoS, RAN slicing strategies should also consider QoE for efficient resource utilization. However, without comprehensively understanding the interplay between QoS, QoE and how various RRM techniques can potentially influence them, it is impossible to incorporate QoE-driven feedback for resource allocation. Consequently, in this work, we conduct a first-of-its-kind in-depth experimental campaign on an O-RAN compliant 5G cellular testbed to evaluate the performance of the QoE metrics of three varied applications---video-enabled voice calling, cloud gaming, and AR---under various RAN slice configurations. We discuss the key findings of this elaborate study, and motivate the need for a QoE-aware RRM framework for RAN slicing.},
booktitle = {Proceedings of the 17th ACM Workshop on Wireless Network Testbeds, Experimental Evaluation \&amp; Characterization},
pages = {41–48},
numpages = {8},
keywords = {Open RAN (O-RAN), Mobile Networks, Wireless Networks},
location = {Madrid, Spain},
series = {WiNTECH '23}
}

@article{10.1145/3529162,
author = {Duboc, Leticia and Bahsoon, Rami and Alrebeish, Faisal and Mera-G\'{o}mez, Carlos and Nallur, Vivek and Kazman, Rick and Bianco, Philip and Babar, Ali and Buyya, Rajkumar},
title = {Systematic Scalability Modeling of QoS-Aware Dynamic Service Composition},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3–4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3529162},
doi = {10.1145/3529162},
abstract = {In Dynamic Service Composition (DSC), an application can be dynamically composed using web services to achieve its functional and Quality of Services (QoS) goals. DSC is a relatively mature area of research that crosscuts autonomous and services computing. Complex autonomous and self-adaptive computing paradigms (e.g., multi-tenant cloud services, mobile/smart services, services discovery and composition in intelligent environments such as smart cities) have been leveraging DSC to dynamically and adaptively maintain the desired QoS, cost and to stabilize long-lived software systems. While DSC is fundamentally known to be an NP-hard problem, systematic attempts to analyze its scalability have been limited, if not absent, though such analysis is of a paramount importance for their effective, efficient, and stable operations.This article reports on a new application of goal-modeling, providing a systematic technique that can support DSC designers and architects in identifying DSC-relevant characteristics and metrics that can potentially affect the scalability goals of a system. The article then applies the technique to two different approaches for QoS-aware dynamic services composition, where the article describes two detailed exemplars that exemplify its application. The exemplars hope to provide researchers and practitioners with guidance and transferable knowledge in situations where the scalability analysis may not be straightforward. The contributions provide architects and designers for QoS-aware dynamic service composition with the fundamentals for assessing the scalability of their own solutions, along with goal models and a list of application domain characteristics and metrics that might be relevant to other solutions. Our experience has shown that the technique was able to identify in both exemplars application domain characteristics and metrics that had been overlooked in previous scalability analyses of these DSC, some of which indeed limited their scalability. It has also shown that the experiences and knowledge can be transferable: The first exemplar was used as an example to inform and ease the work of applying the technique in the second one, reducing the time to create the model, even for a non-expert.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {nov},
articleno = {10},
numpages = {39},
keywords = {Scalability modelling, autonomous and adaptive systems, dynamic service composition}
}

@inproceedings{10.1145/3468737.3494094,
author = {Wallis, Kevin and Reich, Christoph and Varghese, Blesson and Schindelhauer, Christian},
title = {QUDOS: Quorum-Based Cloud-Edge Distributed DNNs for Security Enhanced Industry 4.0},
year = {2021},
isbn = {9781450385640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468737.3494094},
doi = {10.1145/3468737.3494094},
abstract = {Distributed machine learning algorithms that employ Deep Neural Networks (DNNs) are widely used in Industry 4.0 applications, such as smart manufacturing. The layers of a DNN can be mapped onto different nodes located in the cloud, edge and shop floor for preserving privacy. The quality of the data that is fed into and processed through the DNN is of utmost importance for critical tasks, such as inspection and quality control. Distributed Data Validation Networks (DDVNs) are used to validate the quality of the data. However, they are prone to single points of failure when an attack occurs. This paper proposes QUDOS, an approach that enhances the security of a distributed DNN that is supported by DDVNs using quorums. The proposed approach allows individual nodes that are corrupted due to an attack to be detected or excluded when the DNN produces an output. Metrics such as corruption factor and success probability of an attack are considered for evaluating the security aspects of DNNs. A simulation study demonstrates that if the number of corrupted nodes is less than a given threshold for decision-making in a quorum, the QUDOS approach always prevents attacks. Furthermore, the study shows that increasing the size of the quorum has a better impact on security than increasing the number of layers. One merit of QUDOS is that it enhances the security of DNNs without requiring any modifications to the algorithm and can therefore be applied to other classes of problems.},
booktitle = {Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing},
articleno = {5},
numpages = {10},
keywords = {industry 4.0, cloud-edge computing, distributed DNN, distributed data validation network, edge security},
location = {Leicester, United Kingdom},
series = {UCC '21}
}

@inproceedings{10.1145/3517745.3561464,
author = {Xu, Xiaokun and Claypool, Mark},
title = {Measurement of Cloud-Based Game Streaming System Response to Competing TCP Cubic or TCP BBR Flows},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561464},
doi = {10.1145/3517745.3561464},
abstract = {Cloud-based game streaming is emerging as a convenient way to play games when clients have a good network connection. However, high-quality game streams need high bitrates and low latencies, a challenge when competing for network capacity with other flows. While some network aspects of cloud-based game streaming have been studied, missing are comparative performance and congestion responses to competing TCP flows. This paper presents results from experiments that measure how three popular commercial cloud-based game streaming systems - Google Stadia, NVidia GeForce Now, and Amazon Luna - respond and then recover to TCP Cubic and TCP BBR flows on a congested network link. Analysis of bitrates, loss rates and round-trip times show the three systems have markedly different responses to the arrival and departure of competing network traffic.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {305–316},
numpages = {12},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3484274.3484307,
author = {Fang, Yuan and Fan, Lei},
title = {Comparisons of Eight Simplification Methods for Data Reduction of Terrain Point Cloud},
year = {2021},
isbn = {9781450390477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3484274.3484307},
doi = {10.1145/3484274.3484307},
abstract = {In recent years, the applications of 3D point cloud data representing terrain surfaces have been growing rapidly. Such data typically have a very fine spatial resolution, which can lead to computational and visualisation issues. To overcome these issues, it is a common practice to reduce the density of point cloud data during initial data processing. As such, various simplification methods had been developed and used in practice. The choice of those methods is crucial to preserve features and shapes of the terrain in the simplified point cloud data. Previous studies on this matter were focused mainly on the methods commonly used in geosciences, but did not consider those in computer graphics. In this study, a total of eight simplification methods that are used widely in both geosciences and computer graphics were compared and analyzed using four sets of terrain surface point cloud data. In addition, unlike previous studies where a global RMSE (root mean squared error) was used as the metric for comparing different methods, the standard deviation of local RMSEs (root mean squared errors) was also calculated in this study to check the uniformity of local RMSEs over the whole terrain areas considered. The results show that the adaptive sampling method yielded thinned point cloud data of higher overall accuracy and more consistent local RMSEs than those obtained using the other methods considered.},
booktitle = {Proceedings of the 4th International Conference on Control and Computer Vision},
pages = {135–141},
numpages = {7},
keywords = {data density, computer graphics, terrain, simplification, sampling, point clouds},
location = {Macau, China},
series = {ICCCV '21}
}

@article{10.1145/3508030,
author = {Bhuyan, Sandeepa and Zhao, Shulin and Ying, Ziyu and Kandemir, Mahmut T. and Das, Chita R.},
title = {End-to-End Characterization of Game Streaming Applications on Mobile Platforms},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3508030},
doi = {10.1145/3508030},
abstract = {With the advent of 5G, supporting high-quality game streaming applications on edge devices has become a reality. This is evidenced by a recent surge in cloud gaming applications on mobile devices. In contrast to video streaming applications, interactive games require much more compute power for supporting improved rendering (such as 4K streaming) with the stipulated frames-per second (FPS) constraints. This in turn consumes more battery power in a power-constrained mobile device. Thus, the state-of-the-art gaming applications suffer from lower video quality (QoS) and/or energy efficiency. While there has been a plethora of recent works on optimizing game streaming applications, to our knowledge, there is no study that systematically investigates the &lt;QoS, Energy&gt; design pairs on the end-to-end game streaming pipeline across the cloud, network, and edge devices to understand the individual contributions of the different stages of the pipeline for improving the overall QoS and energy efficiency. In this context, this paper presents a comprehensive performance and power analysis of the entire game streaming pipeline consisting of the server/cloud side, network, and edge. Through extensive measurements with a high-end workstation mimicking the cloud end, an open-source platform (Moonlight-GameStreaming) emulating the edge device/mobile platform, and two network settings (WiFi and 5G) we conduct a detailed measurement-based study with seven representative games with different characteristics. We characterize the performance in terms of frame latency, QoS, bitrate, and energy consumption for different stages of the gaming pipeline. Our study shows that the rendering stage and the encoding stage at the cloud end are the bottlenecks to support 4K streaming. While 5G is certainly more suitable for supporting enhanced video quality with 4K streaming, it is more expensive in terms of power consumption compared to WiFi. Further, fluctuations in 5G network quality can lead to huge frame drops thus affecting QoS, which needs to be addressed by a coordinated design between the edge device and the server. Finally, the network interface and the decoder units in a mobile platform need more energy-efficient design to support high quality games at a lower cost. These observations should help in designing more cost-effective future cloud gaming platforms.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {feb},
articleno = {10},
numpages = {25},
keywords = {energy efficiency, cloud gaming, smartphones, 5g, performance}
}

@inproceedings{10.1145/3580305.3599878,
author = {Xi, Yunjia and Liu, Weiwen and Wang, Yang and Tang, Ruiming and Zhang, Weinan and Zhu, Yue and Zhang, Rui and Yu, Yong},
title = {On-Device Integrated Re-Ranking with Heterogeneous Behavior Modeling},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599878},
doi = {10.1145/3580305.3599878},
abstract = {As an emerging field driven by industrial applications, integrated re-ranking combines lists from upstream sources into a single list, and presents it to the user. The quality of integrated re-ranking is especially sensitive to real-time user behaviors and preferences. However, existing methods are all built on the cloud-to-edge framework, where mixed lists are generated by the cloud model and then sent to the devices. Despite its effectiveness, such a framework fails to capture users' real-time preferences due to the network bandwidth and latency. Hence, we propose to place the integrated re-ranking model on devices, allowing for the full exploitation of real-time behaviors. To achieve this, we need to address two key issues: first, how to extract users' preferences for different sources from heterogeneous and imbalanced user behaviors; second, how to explore the correlation between the extracted personalized preferences and the candidate items. In this work, we present the first on-Device Integrated Re-ranking framework, DIR, to avoid delays in processing real-time user behaviors. DIR includes a multi-sequence behavior modeling module to extract the user's source-level preferences, and a preference-adaptive re-ranking module to incorporate personalized source-level preferences into the re-ranking of candidate items. Besides, we design exposure loss and utility loss to jointly optimize exposure fairness and overall utility. Extensive experiments on three datasets show that DIR significantly outperforms the state-of-the-art baselines in utility-based and fairness-based metrics.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5225–5236},
numpages = {12},
keywords = {integrated re-ranking, edge computing, recommender system},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@inproceedings{10.1145/3564858.3564860,
author = {Yang, Liu and Wang, Jian and Zhao, Zebin},
title = {Quality Evaluation of Government Epidemic Data Openness Based on Cloud Model and PSR Theory},
year = {2022},
isbn = {9781450396721},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564858.3564860},
doi = {10.1145/3564858.3564860},
abstract = {Government data opening has become one of the key measures of government emergency management in the big data era. Investigating deeply the quality of government data opening under public health emergencies can help grasp the current situation and provide experience for future work. This paper evaluates the 31 regional health commissions’ epidemic data openness using a framework based on PSR theory in China, and the cloud model was used to evaluate the quality of government data opening at the national and regional levels. The comprehensive evaluation level of government data openness under the epidemic situation in China was ordinary level and the level of data openness varies greatly among regions. Data state is the main factor restricting the level of data openness.},
booktitle = {Proceedings of the 5th International Conference on Information Management and Management Science},
pages = {7–14},
numpages = {8},
keywords = {public health emergencies, cloud model, open government data, PSR theory},
location = {Chengdu, China},
series = {IMMS '22}
}

@inproceedings{10.1145/3579990.3580010,
author = {Li, Bolun and Su, Pengfei and Chabbi, Milind and Jiao, Shuyin and Liu, Xu},
title = {DJXPerf: Identifying Memory Inefficiencies via Object-Centric Profiling for Java},
year = {2023},
isbn = {9798400701016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579990.3580010},
doi = {10.1145/3579990.3580010},
abstract = {Java is the “go-to” programming language choice for developing scalable enterprise cloud applications. In such systems, even a few percent CPU time savings can offer a significant competitive advantage and cost savings. Although performance tools abound for Java, those that focus on the data locality in the memory hierarchy are rare.  

In this paper, we first categorize data locality issues in Java programs. We then present DJXPerf, a lightweight, object-centric memory profiler for Java, which associates memory-hierarchy performance metrics (e.g., cache/TLB misses) with Java objects. DJXPerf uses statistical sampling of hardware performance monitoring counters to attribute metrics to not only source code locations but also Java objects. DJXPerf presents Java object allocation contexts combined with their usage contexts and presents them ordered by the poor locality behaviors. DJXPerf’s performance measurement, object attribution, and presentation techniques guide optimizing object allocation, layout, and access patterns. DJXPerf incurs only ~8.5\% runtime overhead and ∼6\% memory overhead on average, requiring no modifications to hardware, OS, Java virtual machine, or application source code, which makes it attractive to use in production. Guided by DJXPerf, we study and optimize a number of Java and Scala programs, including well-known benchmarks and real-world applications, and demonstrate significant speedups.},
booktitle = {Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {81–94},
numpages = {14},
keywords = {performance optimization, Java, profiling, PMU},
location = {Montr\'{e}al, QC, Canada},
series = {CGO 2023}
}

@inproceedings{10.1145/3489048.3522650,
author = {Bhuyan, Sandeepa and Zhao, Shulin and Ying, Ziyu and Kandemir, Mahmut T. and Das, Chita R.},
title = {End-to-End Characterization of Game Streaming Applications on Mobile Platforms},
year = {2022},
isbn = {9781450391412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489048.3522650},
doi = {10.1145/3489048.3522650},
abstract = {With the advent of 5G, hosting high-quality game streaming applications on mobile devices has become a reality. To our knowledge, no prior study systematically investigates the &lt; QoS, Energy &gt; tuple on the end-to-end game streaming pipeline across the cloud, network, and edge devices to understand the individual contributions of the different pipeline stages. In this paper, we present a comprehensive performance and power analysis of the entire game streaming pipeline through extensive measurements with a high-end workstation mimicking the cloud end, an open-source platform (Moonlight-GameStreaming) emulating the edge device/mobile platform, and two network settings (WiFi and 5G). Our study shows that the rendering stage and the encoding stage at the cloud end are the bottlenecks for 4K streaming. While 5G is certainly more suitable for supporting enhanced video quality with 4K streaming, it is more expensive in terms of power consumption compared to WiFi. Further, the network interface and the decoder units in mobile devices need more energy-efficient design to support high quality games at a lower cost. These observations should help in designing more cost-effective future cloud gaming platforms.},
booktitle = {Abstract Proceedings of the 2022 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {11–12},
numpages = {2},
keywords = {performance, energy efficiency, smartphones, 5g, cloud gaming},
location = {Mumbai, India},
series = {SIGMETRICS/PERFORMANCE '22}
}

@article{10.1145/3547353.3522650,
author = {Bhuyan, Sandeepa and Zhao, Shulin and Ying, Ziyu and Kandemir, Mahmut T. and Das, Chita R.},
title = {End-to-End Characterization of Game Streaming Applications on Mobile Platforms},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/3547353.3522650},
doi = {10.1145/3547353.3522650},
abstract = {With the advent of 5G, hosting high-quality game streaming applications on mobile devices has become a reality. To our knowledge, no prior study systematically investigates the &lt; QoS, Energy &gt; tuple on the end-to-end game streaming pipeline across the cloud, network, and edge devices to understand the individual contributions of the different pipeline stages. In this paper, we present a comprehensive performance and power analysis of the entire game streaming pipeline through extensive measurements with a high-end workstation mimicking the cloud end, an open-source platform (Moonlight-GameStreaming) emulating the edge device/mobile platform, and two network settings (WiFi and 5G). Our study shows that the rendering stage and the encoding stage at the cloud end are the bottlenecks for 4K streaming. While 5G is certainly more suitable for supporting enhanced video quality with 4K streaming, it is more expensive in terms of power consumption compared to WiFi. Further, the network interface and the decoder units in mobile devices need more energy-efficient design to support high quality games at a lower cost. These observations should help in designing more cost-effective future cloud gaming platforms.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jul},
pages = {11–12},
numpages = {2},
keywords = {cloud gaming, energy efficiency, 5g, performance, smartphones}
}

@article{10.1145/3470970,
author = {Min, Xiongkuo and Gu, Ke and Zhai, Guangtao and Yang, Xiaokang and Zhang, Wenjun and Le Callet, Patrick and Chen, Chang Wen},
title = {Screen Content Quality Assessment: Overview, Benchmark, and Beyond},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3470970},
doi = {10.1145/3470970},
abstract = {Screen content, which is often computer-generated, has many characteristics distinctly different from conventional camera-captured natural scene content. Such characteristic differences impose major challenges to the corresponding content quality assessment, which plays a critical role to ensure and improve the final user-perceived quality of experience (QoE) in various screen content communication and networking systems. Quality assessment of such screen content has attracted much attention recently, primarily because the screen content grows explosively due to the prevalence of cloud and remote computing applications in recent years, and due to the fact that conventional quality assessment methods can not handle such content effectively. As the most technology-oriented part of QoE modeling, image/video content/media quality assessment has drawn wide attention from researchers, and a large amount of work has been carried out to tackle the problem of screen content quality assessment. This article is intended to provide a systematic and timely review on this emerging research field, including (1) background of natural scene vs. screen content quality assessment; (2) characteristics of natural scene vs. screen content; (3) overview of screen content quality assessment methodologies and measures; (4) relevant benchmarks and comprehensive evaluation of the state-of-the-art; (5) discussions on generalizations from screen content quality assessment to QoE assessment, and other techniques beyond QoE assessment; and (6) unresolved challenges and promising future research directions. Throughout this article, we focus on the differences and similarities between screen content and conventional natural scene content. We expect that this review article shall provide readers with an overview of the background, history, recent progress, and future of the emerging screen content quality assessment research.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {187},
numpages = {36},
keywords = {natural scene, quality of experience, Screen content, quality assessment}
}

@inproceedings{10.1145/3617023.3617068,
author = {Cesar, Pablo},
title = {Towards Volumetric Video Conferencing},
year = {2023},
isbn = {9798400709081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617023.3617068},
doi = {10.1145/3617023.3617068},
abstract = {With Social Extended Reality (XR) emerging as a new medium, where users can remotely experience immersive content with others, the vision of a true feeling of ‘being there together’ has become a realistic goal. This keynote will provide an overview about the challenges to achieve such a goal, based on results from practical case studies like the TRANSMIXR and MediaScape XR projects. We will discuss about different technologies, like point clouds, that can be used as the format for representing highly-realistic digital humans, and about metrics and protocols for quantifying the quality of experience. The final intention of the talk is to shed some light on social XR, as a new group of virtual reality experiences based on social photorealistic immersive content. We will discuss about the challenges regarding production and user-centric processes, and discover the new opportunities open by this new medium},
booktitle = {Proceedings of the 29th Brazilian Symposium on Multimedia and the Web},
pages = {5},
numpages = {1},
keywords = {social extended reality, quality of experience, Volumetric video},
location = {<conf-loc>, <city>Ribeir\~{a}o Preto</city>, <country>Brazil</country>, </conf-loc>},
series = {WebMedia '23}
}

@inproceedings{10.1145/3485447.3512276,
author = {Alhilal, Ahmad and Braud, Tristan and Han, Bo and Hui, Pan},
title = {Nebula: Reliable Low-Latency Video Transmission for Mobile Cloud Gaming},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512276},
doi = {10.1145/3485447.3512276},
abstract = {Mobile cloud gaming enables high-end games on constrained devices by streaming the game content from powerful servers through mobile networks. Mobile networks suffer from highly variable bandwidth, latency, and losses that affect the gaming experience. This paper introduces , an end-to-end cloud gaming framework to minimize the impact of network conditions on the user experience. relies on an end-to-end distortion model adapting the video source rate and the amount of frame-level redundancy based on the measured network conditions. As a result, it minimizes the motion-to-photon (MTP) latency while protecting the frames from losses. We fully implement and evaluate its performance against the state-of-the-art techniques and latest research in real-time mobile cloud gaming transmission on a physical testbed over emulated and real wireless networks. consistently balances MTP latency (&lt;140&nbsp;ms) and visual quality (&gt;31dB) even in highly variable environments. A user experiment confirms that maximizes the user experience with high perceived video quality, playability, and low user load.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3407–3417},
numpages = {11},
keywords = {Adaptive Rate., Forward Error Correction, Mobile Cloud Gaming},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3578495.3578498,
author = {Wien, Mathias},
title = {MPEG Visual Quality Assessment Advisory Group: Overview and Perspectives},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
url = {https://doi.org/10.1145/3578495.3578498},
doi = {10.1145/3578495.3578498},
abstract = {The perceived visual quality is of utmost importance in the context of visual media compression, such as 2D, 3D, immersive video, and point clouds. The trade-off between compression efficiency and computational/implementation complexity has a crucial impact on the success of a compression scheme. This specifically holds for the development of visual media compression standards which typically aims at maximum compression efficiency using state-of-the-art coding technology. In MPEG, the subjective and objective assessment of visual quality has always been an integral part of the standards development process. Due to the significant effort of formal subjective evaluations, the standardization process typically relies on such formal tests in the starting phase and for verification while in the development phase objective metrics are used. In the new MPEG structure, established in 2020, a dedicated advisory group has been installed for the purpose of providing, maintaining, and developing visual quality assessment methods suitable for use in the standardization process.This column lays out the scope and tasks of this advisory group and reports on its first achievements and developments. After a brief overview of the organizational structure, current projects are presented, and initial results are presented.},
journal = {SIGMultimedia Rec.},
month = {dec},
articleno = {3},
numpages = {1}
}

@inproceedings{10.1145/3524273.3532898,
author = {Slivar, Ivan and Bacic, Kresimir and Orsolic, Irena and Skorin-Kapov, Lea and Suznjevic, Mirko},
title = {CGD: A Cloud Gaming Dataset with Gameplay Video and Network Recordings},
year = {2022},
isbn = {9781450392839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524273.3532898},
doi = {10.1145/3524273.3532898},
abstract = {With advances in network capabilities, the gaming industry is increasingly turning towards offering "gaming on demand" solutions, with cloud gaming services such as Sony PlayStation Now, Google Stadia, and NVIDIA GeForce NOW expanding their market offerings. Similar to adaptive video streaming services, cloud gaming services typically adapt the quality of game streams (e.g., bitrate, resolution, frame rate) in accordance with current network conditions. To select the most appropriate video encoding parameters given certain conditions, it is important to understand their impact on Quality of Experience (QoE). On the other hand, network operators are interested in understanding the relationships between parameters measurable in the network and cloud gaming QoE, to be able to invoke QoE-aware network management mechanisms. To encourage developments in these areas, comprehensive datasets are crucial, including both network and application layer data. This paper presents CGD, a dataset consisting of 600 game streaming sessions corresponding to 10 games of different genres being played and streamed using the following encoding parameters: bitrate (5, 10, 20 Mbps), resolution (720p, 1080p), and frame rate (30, 60 fps). For every combination repeated five times for each game, the dataset includes: 1) gameplay video recordings, 2) network traffic traces, 3) user input logs (mouse and keyboard), and 4) streaming performance logs.},
booktitle = {Proceedings of the 13th ACM Multimedia Systems Conference},
pages = {272–278},
numpages = {7},
keywords = {network traffic, dataset, cloud gaming, video metrics, gameplay, user input, raw video},
location = {Athlone, Ireland},
series = {MMSys '22}
}

@inproceedings{10.1145/3582935.3583055,
author = {Wu, Guofang and Dong, Guoliang and Xu, Shuquan},
title = {Application of Personnel Safety Management System in Network Security Guarantee},
year = {2023},
isbn = {9781450396806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582935.3583055},
doi = {10.1145/3582935.3583055},
abstract = {The rapid development of cloud protection technology provides high-quality security protection barriers for the security of websites deployed on the Internet from a technical level. After the website administrators patch the relevant vulnerabilities in time, the occurrence of network security accidents can be greatly reduced at the technical level. However, cyber attackers take advantage of the negligence of the website's personnel management to launch attacks on the website, resulting in frequent network security incidents such as account theft, web page tampering, website downtime, and core data theft. As a major part of network security management, the formulation and implementation of personnel management systems are crucial to the security of websites. This paper summarizes the experience of network security management and security over the years, puts forward the requirements and methods of personnel management in network security security, and strengthens the management requirements of human factors in the network security system. These measures have played an effective role in network security assurance.},
booktitle = {Proceedings of the 5th International Conference on Information Technologies and Electrical Engineering},
pages = {714–718},
numpages = {5},
keywords = {Network security, Permissions, Cloud computer room, Security awareness, Personnel management, Security administrator},
location = {Changsha, China},
series = {ICITEE '22}
}

@inproceedings{10.1145/3581783.3613827,
author = {Zhang, Junzhe and Chen, Tong and Ding, Dandan and Ma, Zhan},
title = {G-PCC++: Enhanced Geometry-Based Point Cloud Compression},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613827},
doi = {10.1145/3581783.3613827},
abstract = {MPEG Geometry-based Point Cloud Compression (G-PCC) standard is developed for lossy encoding of point clouds to enable immersive services over the Internet. However, lossy G-PCC introduces superimposed distortions from both geometry and attribute information, seriously deteriorating the Quality of Experience (QoE). This paper thus proposes the Enhanced G-PCC (GPCC++), to effectively address the compression distortion and restore the quality. G-PCC++ separates the enhancement into two stages: it first enhances the geometry and then maps the decoded attribute to the enhanced geometry for refinement. As for geometry restoration, a k Nearest Neighbors (kNN)-based Linear Interpolation is first used to generate a denser geometry representation, on top of which GeoNet further generates sufficient candidates to restore geometry through probability-sorted selection. For attribute enhancement, a kNN-based Gaussian Distance Weighted Mapping is devised to re-colorize all points in enhanced geometry tensor, which are then refined by AttNet for the final reconstruction. G-PCC++ is the first solution addressing the geometry and attribute artifacts together. Extensive experiments on several public datasets demonstrate the superiority of G-PCC++, e.g., on the solid point cloud dataset 8iVFB, G-PCC++ outperforms G-PCC by 88.24\% (80.54\%) BD-BR in D1 (D2) measurement of geometry and by 14.64\% (13.09\%) BD-BR in Y (YUV) attribute. Moreover, when considering both geometry and attribute, G-PCC++ also largely surpasses G-PCC by 25.58\% BD-BR using PCQM assessment.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {1352–1363},
numpages = {12},
keywords = {point cloud compression, quality enhancement, compression artifact},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@article{10.1145/3512937,
author = {Zhang, Yongle and Asamoah Owusu, Dennis and Carpuat, Marine and Gao, Ge},
title = {Facilitating Global Team Meetings Between Language-Based Subgroups: When and How Can Machine Translation Help?},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512937},
doi = {10.1145/3512937},
abstract = {Global teams frequently consist of language-based subgroups who put together complementary information to achieve common goals. Previous research outlines a two-step work communication flow in these teams. There are team meetings using a required common language (i.e., English); in preparation for those meetings, people have subgroup conversations in their native languages. Work communication at team meetings is often less effective than in subgroup conversations. In the current study, we investigate the idea of leveraging machine translation (MT) to facilitate global team meetings. We hypothesize that exchanging subgroup conversation logs before a team meeting offers contextual information that benefits teamwork at the meeting. MT can translate these logs, which enables comprehension at a low cost. To test our hypothesis, we conducted a between-subjects experiment where twenty quartets of participants performed a personnel selection task. Each quartet included two English native speakers (NS) and two non-native speakers (NNS) whose native language was Mandarin. All participants began the task with subgroup conversations in their native languages, then proceeded to team meetings in English. We manipulated the exchange of subgroup conversation logs prior to team meetings: with MT-mediated exchanges versus without. Analysis of participants' subjective experience, task performance, and depth of discussions as reflected through their conversational moves jointly indicates that team meeting quality improved when there were MT-mediated exchanges of subgroup conversation logs as opposed to no exchanges. We conclude with reflections on when and how MT could be applied to enhance global teamwork across a language barrier.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {90},
numpages = {26},
keywords = {global teams, shared context, language choice, machine translation}
}

@inproceedings{10.1145/3603269.3604826,
author = {Hamadanian, Pouya and Gallatin, Doug and Alizadeh, Mohammad and Chintalapudi, Krishna},
title = {Ekho: Synchronizing Cloud Gaming Media across Multiple Endpoints},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603269.3604826},
doi = {10.1145/3603269.3604826},
abstract = {Online cloud gaming platforms stream game media to multiple end-points (e.g., a television display and a controller-connected headset) via possibly different networks with considerably different latencies. This leads to the media being played out of sync with one another, and severely degrades user experience. Typical approaches that rely on network and software timing measurements fail to reach synchronization goals. In this work, we propose Ekho, a robust and efficient end-to-end approach for synchronizing streams transmitted to two devices. Ekho adds faint, human-inaudible pseudo-noise (PN) markers to the game audio, and listens for these markers in the chat audio captured by the player's microphone to measure inter-stream delay (ISD). The game server then compensates for the ISD to synchronize the streams. We evaluate Ekho in depth, with a corpus of audio samples from popular online games, and demonstrate that it calculates ISD with sub-millisecond accuracy, has low computational overhead, and is resilient to background chatter, compression and microphone quality. In end-to-end tests over WiFi and cellular links with frequent packet loss and playback disruption, Ekho maintains human-imperceptible ISD (&lt; 10 ms) 86.8\% of the time. Without Ekho, the ISD exceeds 50 ms at all times.},
booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
pages = {533–549},
numpages = {17},
keywords = {cloud gaming, inter-device synchronization, ITU-T P.808, media synchronization},
location = {New York, NY, USA},
series = {ACM SIGCOMM '23}
}

@inproceedings{10.1145/3503161.3547807,
author = {Zhang, Rui-Xiao and Yang, Changpeng and Wang, Xiaochan and Huang, Tianchi and Wu, Chenglei and Liu, Jiangchuan and Sun, Lifeng},
title = {AggCast: Practical Cost-Effective Scheduling for Large-Scale Cloud-Edge Crowdsourced Live Streaming},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547807},
doi = {10.1145/3503161.3547807},
abstract = {Conventional wisdom claims that in order to improve viewer engagement, the cloud-edge providers should serve the viewers with the nearest edge nodes, however, we show that doing this for crowdsourced live streaming (CLS) services can introduce significant costs inefficiency. We observe that the massive number of channels has greatly burdened the operating expenditure of the cloud-edge providers, and most importantly, unbalanced viewer distribution makes the edge nodes suffer significant costs inefficiency. To tackle the above concerns, we propose AggCast, a novel CLS scheduling framework to optimize the edge node utilization for the cloud-edge provider. The core idea of AggCast is to aggregate some viewers who are initially scattered on different regions, and assign them to fewer pre-selected nodes, thereby reducing bandwidth costs. In particular, by leveraging the insights obtained from our large-scale measurement, AggCast can not only ensure quality of experience (QoS), but also satisfy the systematic requirements of CLS services. AggCast has been A/B tested and fully deployed in a top cloud-edge provider in China for over eight months. The online and trace-driven experiments show that, compared to the common practice, AggCast can save over 15\% back-to-source (BTS) bandwidth costs while having no negative impacts on QoS.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3026–3034},
numpages = {9},
keywords = {content delivery, cloud edge computing, live streaming},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3474717.3483959,
author = {Bhore, Sujoy and Ganian, Robert and Li, Guangping and N\"{o}llenburg, Martin and Wulms, Jules},
title = {Worbel: Aggregating Point Labels into Word Clouds},
year = {2021},
isbn = {9781450386647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474717.3483959},
doi = {10.1145/3474717.3483959},
abstract = {Point feature labeling is a classical problem in cartography and GIS that has been extensively studied for geospatial point data. At the same time, word clouds are a popular visualization tool to show the most important words in text data which has also been extended to visualize geospatial data (Buchin et al. PacificVis 2016).In this paper, we study a hybrid visualization, which combines aspects of word clouds and point labeling. In the considered setting, the input data consists of a set of points grouped into categories and our aim is to place multiple disjoint and axis-aligned rectangles, each representing a category, such that they cover points of (mostly) the same category under some natural quality constraints.In our visualization, we then place category names inside the computed rectangles to produce a labeling of the covered points which summarizes the predominant categories globally (in a word-cloud-like fashion) while locally avoiding excessive misrepresentation of points (i.e., retaining the precision of point labeling).We show that computing a minimum set of such rectangles is NP-hard. Hence, we turn our attention to developing heuristics and exact SAT models to compute our visualizations. We evaluate our algorithms quantitatively, measuring running time and quality of the produced solutions, on several artificial and real-world data sets. Our experiments show that the heuristics produce solutions of comparable quality to the SAT models while running much faster.},
booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
pages = {256–267},
numpages = {12},
keywords = {labeling, categorical point data, word clouds},
location = {Beijing, China},
series = {SIGSPATIAL '21}
}

@article{10.1145/3603376,
author = {Bhore, Sujoy and Ganian, Robert and Li, Guangping and N\"{o}llenburg, Martin and Wulms, Jules},
title = {Worbel: Aggregating Point Labels into Word Clouds},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2374-0353},
url = {https://doi.org/10.1145/3603376},
doi = {10.1145/3603376},
abstract = {Point feature labeling is a classical problem in cartography and GIS that has been extensively studied for geospatial point data. At the same time, word clouds are a popular visualization tool to show the most important words in text data which has also been extended to visualize geospatial data (Buchin et&nbsp;al. PacificVis 2016). In this article, we study a hybrid visualization, which combines aspects of word clouds and point labeling. In the considered setting, the input data consist of a set of points grouped into categories and our aim is to place multiple disjoint and axis-aligned rectangles, each representing a category, such that they cover points of (mostly) the same category under some natural quality constraints. In our visualization, we then place category names inside the computed rectangles to produce a labeling of the covered points which summarizes the predominant categories globally (in a word-cloud-like fashion) while locally avoiding excessive misrepresentation of points (i.e., retaining the precision of point labeling). We show that computing a minimum set of such rectangles is NP-hard. Hence, we turn our attention to developing a heuristic with (optional) exact components using SAT models to compute our visualizations. We evaluate our algorithms quantitatively, measuring running time and quality of the produced solutions, on several synthetic and real-world data sets. Our experiments show that the fully heuristic approach produces solutions of comparable quality to heuristics combined with exact SAT models, while running much faster.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = {aug},
articleno = {19},
numpages = {32},
keywords = {word clouds, categorical point data, Labeling}
}

@inproceedings{10.1145/3546000.3546005,
author = {Lou, Ren and Zhang, Jiacheng and Zhang, Lei and Hong, Qiang and Zhou, Yueqi and Li, Xinghua},
title = {Research on Highway CPS-T of Wide Area Communication and Data Cloud},
year = {2022},
isbn = {9781450396295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546000.3546005},
doi = {10.1145/3546000.3546005},
abstract = {Cyber physical transportation system (CPS-T) is a traffic perception, control and service system based on algorithm, model, data and computing power. With the help of radar point cloud, video image, GNSS, sensor and other types of monitoring equipment, the highway data can be collected with high quality and transmitted with high reliability under the condition of relatively complete communication conditions. This paper studies the technical application of CPS-T for highways. Through the deployment of communication technology and data cloud platform, it can intelligently perceive and analyze dynamic and static operation data, accurately identify or predict key ramps, bottleneck sections and mainstream traffic channels, and dynamically implement active control strategies such as ramp control, shoulder control, lane control and rate adjustment, so as to realize the advance guidance and control of highway traffic flow. The relevant research has been measured in the highway sections of Shanghai and Zhejiang Province, and the research results have strong engineering reference value.},
booktitle = {Proceedings of the 6th International Conference on High Performance Compilation, Computing and Communications},
pages = {32–37},
numpages = {6},
location = {Jilin, China},
series = {HP3C '22}
}

@article{10.1145/3630614.3630616,
author = {Tannu, Swamit and Nair, Prashant J.},
title = {The Dirty Secret of SSDs: Embodied Carbon},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3630614.3630616},
doi = {10.1145/3630614.3630616},
abstract = {Scalable Solid-State Drives (SSDs) have ushered in a transformative era in data storage and accessibility, spanning both data centers and portable devices. However, the strides made in scaling this technology can bear significant environmental consequences. On a global scale, a notable portion of semiconductor manufacturing relies on electricity derived from coal and natural gas sources. A striking example of this is the manufacturing process for a single Gigabyte of Flash memory, which emits approximately 0.16 Kg of CO2 - a considerable fraction of the total carbon emissions attributed to the system. Remarkably, the manufacturing of storage devices alone contributed to an estimated 20 million metric tonnes of CO2 emissions in the year 2021.In light of these environmental concerns, this paper delves into an analysis of the sustainability trade-offs inherent in Solid-State Drives (SSDs) when compared to traditional Hard Disk Drives (HDDs). Moreover, this study proposes methodologies to gauge the embodied carbon costs associated with storage systems effectively. The research encompasses four key strategies to enhance the sustainability of storage systems.Firstly, the paper offers insightful guidance for selecting the most suitable storage medium, be it SSDs or HDDs, considering the broader ecological impact. Secondly, the paper advocates for implementing techniques that extend the lifespan of SSDs, thereby mitigating premature replacements and their attendant environmental toll. Thirdly, the paper emphasizes the need for efficient recycling and reuse of high-density multi-level cell-based SSDs, underscoring the significance of minimizing electronic waste.Lastly, for handheld devices, the paper underscores the potential of harnessing the elasticity offered by cloud storage solutions as a means to curtail the ecological repercussions of localized data storage. In summation, this study critically addresses the embodied carbon issues associated with SSDs, comparing them with HDDs, and proposes a comprehensive framework of strategies to enhance the sustainability of storage systems.},
journal = {SIGENERGY Energy Inform. Rev.},
month = {oct},
pages = {4–9},
numpages = {6},
keywords = {sustainability, solid state drives, embodied carbon, hard disk drive}
}

@inproceedings{10.1145/3604930.3605721,
author = {Arora, Rohan and Devi, Umamaheswari and Eilam, Tamar and Goyal, Aanchal and Narayanaswami, Chandra and Parida, Pritish},
title = {Towards Carbon Footprint Management in Hybrid Multicloud},
year = {2023},
isbn = {9798400702426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604930.3605721},
doi = {10.1145/3604930.3605721},
abstract = {Enterprises today aspire to optimize the operating costs and carbon footprint (CFP) of their IT operations jointly without compromising their business imperatives. This has given rise to a hybrid approach in which enterprises retain the dynamic choice to leverage private data centers and one or more public clouds in conjunction. While cloud service providers (CSPs) have long provided APIs for estimating, reconciling, and optimizing operating costs, they have only recently started exposing APIs related to CFP.Indeed, this is a step in the right direction. Nevertheless, our analyses of these APIs reveals many gaps that need to be addressed to facilitate sizing and placement decisions that can factor in carbon. First, there is a lack of standardized, transparent methodology for CFP quantification across different CSPs. Second, the coarse granularity of the CFP data provided today can help with post-facto reporting but is not suitable for proactive fine-grained optimization. Last, enterprises themselves are unable to independently compute the current CFP or estimate potential CFP savings since CSPs do not share the required power usage data.To address these gaps, enterprises have started developing their own carbon assessment methodologies and tools to estimate the CFP of workloads running on public clouds using the available user-facing APIs. These systems hold the promise for an independent and unbiased evaluation and estimation of relative savings between different deployment options by cloud users. We describe and analyze the details of CSP-native carbon-reporting tools and their quantification methodology, and the "outside-of-the-cloud" estimation approaches. Finally, we present opportunities for future research in the direction of trustworthy, fine-grained, public cloud workload CFP estimation, which is a prerequisite for meaningful realization of carbon optimization.},
booktitle = {Proceedings of the 2nd Workshop on Sustainable Computer Systems},
articleno = {9},
numpages = {7},
keywords = {GHG accounting, data centers, carbon-aware optimization, sustainable computing, GHG emissions, cloud, carbon footprint},
location = {Boston, MA, USA},
series = {HotCarbon '23}
}

@inproceedings{10.1145/3510858.3511355,
author = {Zhang, Tianze},
title = {An Optimal Grasping Point Identification Method Based on Deep Learning and Point Cloud Processing},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511355},
doi = {10.1145/3510858.3511355},
abstract = {With the development of the field of robotics and the increasingly convenient acquisition of point cloud data, point cloud is widely used in robots to complete many tasks more accurately because of its rich three-dimensional information. In this paper, we propose a method based on PointNet++ combining deep learning and point cloud processing to extract the grasping points of the objects. This method selects YCB dataset. The grasping point pairs in each perspective are sampled by point cloud feature point detection and scored by Force Closure together with Shape of the grasp polygon metrics. The input of PointNet++ is the single-perspective field point cloud after the searching algorithm based on grasping point pairs and the final output classification results are divided into two classes. Experimental results conducted based on Kinect2 and UR5 mechanical arm show that our method can achieve 85\% accuracy in the two-class simulation experiment and 80.32\% average success rate in the real grasping tasks, whose robustness is also verified in this paper.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {667–671},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00026,
author = {Yan, Xiaohan and Hsieh, Ken and Liyanage, Yasitha and Ma, Minghua and Chintalapati, Murali and Lin, Qingwei and Dang, Yingnong and Zhang, Dongmei},
title = {Aegis: Attribution of Control Plane Change Impact across Layers and Components for Cloud Systems},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00026},
doi = {10.1109/ICSE-SEIP58684.2023.00026},
abstract = {Modern cloud control plane infrastructure like Microsoft Azure has evolved into a complex one to serve customer needs for diverse types of services and adequate cloud-based resources. On such interconnected system, implementing changes at one component can have an impact on other components, even across different hierarchical computing layers. As a result of the complexity and interconnected nature of the cloud-based services, it poses a challenge to correctly attribute service quality degradation to a control plane change, to infer causality between the two and to mitigate any negative impact. In this paper, we present Aegis, an end-to-end analytical service for attributing control plane change impact across computing layers and service components in large-scale real-world cloud systems. Aegis processes and correlates service health signals and control plane changes across components to construct the most probable causal relationship. Aegis at its core leverages a domain knowledge-driven correlation algorithm to attribute platform signals to changes, and a counterfactual projection model to quantify control plane change impact to customers. Aegis can mitigate the impact of bad changes by alerting service team and recommending pausing the bad ones. Since Aegis' inception in Azure Control Plane 12 months ago, it has caught several bad changes across service components and layers, and promptly paused them to guard the quality of service. Aegis achieves precision and recall around 80\% on real-world control plane deployments.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {222–233},
numpages = {12},
keywords = {regression detection, counterfactual analysis, impact assessment, safe deployment, cloud computing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3543873.3584626,
author = {Zhao, Xinping and Zhang, Ying and Xiao, Qiang and Ren, Yuming and Yang, Yingchun},
title = {Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3584626},
doi = {10.1145/3543873.3584626},
abstract = {We study a particular matching task we call Music Cold-Start Matching. In short, given a cold-start song request, we expect to retrieve songs with similar audiences and then fastly push the cold-start song to the audiences of the retrieved songs to warm up it. However, there are hardly any studies done on this task. Therefore, in this paper, we will formalize the problem of Music Cold-Start Matching detailedly and give a scheme. During the offline training, we attempt to learn high-quality song representations based on song content features. But, we find supervision signals typically follow power-law distribution causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization. During the online serving, to locate the target audiences more accurately, we propose Clustering-based Audience Targeting (CAT) that clusters audience representations to acquire a few cluster centroids and then locate the target audiences by measuring the relevance between the audience representations and the cluster centroids. Extensive experiments on the offline dataset and online system demonstrate the effectiveness and efficiency of our method. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {351–355},
numpages = {5},
keywords = {Bootstrapping Contrastive Learning, Clustering-based Audience Targeting, Music Cold-Start Matching},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3589806.3600032,
author = {Rosendo, Daniel and Keahey, Kate and Costan, Alexandru and Simonin, Matthieu and Valduriez, Patrick and Antoniu, Gabriel},
title = {KheOps: Cost-Effective Repeatability, Reproducibility, and Replicability of Edge-to-Cloud Experiments},
year = {2023},
isbn = {9798400701764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589806.3600032},
doi = {10.1145/3589806.3600032},
abstract = {Distributed infrastructures for computation and analytics are now evolving towards an interconnected ecosystem allowing complex scientific workflows to be executed across hybrid systems spanning from IoT Edge devices to Clouds, and sometimes to supercomputers (the Computing Continuum). Understanding the performance trade-offs of large-scale workflows deployed on such complex Edge-to-Cloud Continuum is challenging. To achieve this, one needs to systematically perform experiments, to enable their reproducibility and allow other researchers to replicate the study and the obtained conclusions on different infrastructures. This breaks down to the tedious process of reconciling the numerous experimental requirements and constraints with low-level infrastructure design choices. To address the limitations of the main state-of-the-art approaches for distributed, collaborative experimentation, such as Google Colab, Kaggle, and Code Ocean, we propose KheOps, a collaborative environment specifically designed to enable cost-effective reproducibility and replicability of Edge-to-Cloud experiments. KheOps is composed of three core elements: (1) an experiment repository; (2) a notebook environment; and (3) a multi-platform experiment methodology. We illustrate KheOps with a real-life Edge-to-Cloud application. The evaluations explore the point of view of the authors of an experiment described in an article (who aim to make their experiments reproducible) and the perspective of their readers (who aim to replicate the experiment). The results show how KheOps helps authors to systematically perform repeatable and reproducible experiments on the Grid5000 + FIT IoT LAB testbeds. Furthermore, KheOps helps readers to cost-effectively replicate authors experiments in different infrastructures such as Chameleon Cloud + CHI@Edge testbeds, and obtain the same conclusions with high accuracies (&gt; 88\% for all performance metrics).},
booktitle = {Proceedings of the 2023 ACM Conference on Reproducibility and Replicability},
pages = {62–73},
numpages = {12},
keywords = {Computing Continuum, Reproducibility, Repeatability, Edge Computing, Cloud Computing, Workflows, Replicability},
location = {Santa Cruz, CA, USA},
series = {ACM REP '23}
}

@inproceedings{10.1145/3487552.3487847,
author = {Chang, Hyunseok and Varvello, Matteo and Hao, Fang and Mukherjee, Sarit},
title = {Can You See Me Now? A Measurement Study of Zoom, Webex, and Meet},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487847},
doi = {10.1145/3487552.3487847},
abstract = {Since the outbreak of the COVID-19 pandemic, videoconferencing has become the default mode of communication in our daily lives at homes, workplaces and schools, and it is likely to remain an important part of our lives in the post-pandemic world. Despite its significance, there has not been any systematic study characterizing the user-perceived performance of existing videoconferencing systems other than anecdotal reports. In this paper, we present a detailed measurement study that compares three major videoconferencing systems: Zoom, Webex and Google Meet. Our study is based on 48 hours' worth of more than 700 videoconferencing sessions, which were created with a mix of emulated videoconferencing clients deployed in the cloud, as well as real mobile devices running from a residential network. We find that the existing videoconferencing systems vary in terms of geographic scope, which in turns determines streaming lag experienced by users. We also observe that streaming rate can change under different conditions (e.g., number of users in a session, mobile device status, etc), which affects user-perceived streaming quality. Beyond these findings, our measurement methodology can enable reproducible benchmark analysis for any types of comparative or longitudinal study on available videoconferencing systems.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {216–228},
numpages = {13},
location = {Virtual Event},
series = {IMC '21}
}

@inproceedings{10.1145/3581784.3607075,
author = {Zhang, Wang and Shi, Zhan and Liao, Ziyi and Li, Yiling and Du, Yu and Wu, Yutong and Wang, Fang and Feng, Dan},
title = {Graph3PO: A Temporal Graph Data Processing Method for Latency QoS Guarantee in Object Cloud Storage System},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3607075},
doi = {10.1145/3581784.3607075},
abstract = {Object cloud storage systems are deployed with diverse applications that have varying latency service level objectives (SLOs), posting challenges for supporting quality of service with limited storage resources. Existing methods provide prediction-based recommendations for dispatching requests from applications to storage devices, but the prediction accuracy can be affected by complex system topology. To address this issue, Graph3PO is designed to combine storage device queue information with system topological information for forming a temporal graph, which can accurately predict device queue states. Additionally, Graph3PO contains the urgency degree model and cost model for measuring SLO violation risks and penalties of scheduling requests on storage device queues. When the urgency degree of a request exceeds a threshold, Graph3PO determines whether to schedule it in the queue or initiate a hedge request to another storage device. Experimental results show that Graph3PO outperforms its competitors, with SLO violation rates 2.8 to 201.1 times lower.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {23},
numpages = {16},
keywords = {latency QoS guarantee, object cloud storage system, temporal graph},
location = {<conf-loc>, <city>Denver</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {SC '23}
}

@article{10.1145/3520132,
author = {Herzog, Benedict and Reif, Stefan and Hemp, Judith and H\"{o}nig, Timo and Schr\"{o}der-Preikschat, Wolfgang},
title = {Resource-Demand Estimation for Edge Tensor Processing Units},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3520132},
doi = {10.1145/3520132},
abstract = {Machine learning has shown tremendous success in a large variety of applications. The evolution of machine-learning applications from cloud-based systems to mobile and embedded devices has shifted the focus from only quality-related aspects towards the resource demand of machine learning. For embedded systems, dedicated accelerator hardware promises the energy-efficient execution of neural network inferences. Their precise resource demand in terms of execution time and power demand, however, is undocumented. Developers, therefore, face the challenge to fine-tune their neural networks such that their resource demand matches the available budgets. This article presents Precious, a comprehensive approach to estimate the resource demand of an embedded neural network accelerator. We generate randomised neural networks, analyse them statically, execute them on an embedded accelerator while measuring their actual power draw and execution time, and train estimators that map the statically analysed neural network properties to the measured resource demand. In addition, this article provides an in-depth analysis of the neural networks’ resource demands and the responsible network properties. We demonstrate that the estimation error of Precious can be below 1.5\% for both power draw and execution time. Furthermore, we discuss what estimator accuracy is practically achievable and how much effort is required to achieve sufficient accuracy.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {58},
numpages = {24},
keywords = {resource awareness, Neural network accelerator}
}

@article{10.1109/TNET.2022.3171467,
author = {Chang, Hyunseok and Varvello, Matteo and Hao, Fang and Mukherjee, Sarit},
title = {A Tale of Three Videoconferencing Applications: Zoom, Webex, and Meet},
year = {2022},
issue_date = {Oct. 2022},
publisher = {IEEE Press},
volume = {30},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3171467},
doi = {10.1109/TNET.2022.3171467},
abstract = {Since the outbreak of the COVID-19 pandemic, videoconferencing has become the default mode of communication in our daily lives at homes, workplaces and schools, and it is likely to remain an important part of our lives in the post-pandemic world. Despite its significance, there has not been any systematic study characterizing the user-perceived performance of existing videoconferencing systems other than anecdotal reports. In this paper, we present a detailed measurement study that compares three major videoconferencing systems: Zoom, Webex and Google Meet. Our study is based on 62 hours’ worth of more than 1.1K videoconferencing sessions, which were created with a mix of emulated videoconferencing clients deployed in the cloud, as well as real mobile devices running from a residential network over two separate periods with nine months apart. We find that the existing videoconferencing systems vary in terms of geographic scope and resource provisioning strategies, which in turns determine streaming lag experienced by users. We also observe that streaming rate can change under different conditions (e.g., available bandwidth, number of users in a session, mobile device status), which affects user-perceived streaming quality. Beyond these findings, our measurement methodology enables reproducible benchmark analysis for any types of comparative or longitudinal study on available videoconferencing systems.},
journal = {IEEE/ACM Trans. Netw.},
month = {may},
pages = {2343–2358},
numpages = {16}
}

@article{10.1145/3629138,
author = {Martin, Noah and Dogar, Fahad},
title = {Divided at the Edge - Measuring Performance and the Digital Divide of Cloud Edge Data Centers},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CoNEXT3},
url = {https://doi.org/10.1145/3629138},
doi = {10.1145/3629138},
abstract = {Cloud providers are highly incentivized to reduce latency. One way they do this is by locating data centers as close to users as possible. These “cloud edge” data centers are placed in metropolitan areas and enable edge computing for residents of these cities. Therefore, which cities are selected to host edge data centers determines who has the fastest access to applications requiring edge compute — creating a digital divide between those closest and furthest from the edge. In this study we measure latency to the current and predicted cloud edge of three major cloud providers around the world. Our measurements use the RIPE Atlas platform targeting cloud regions, AWS Local Zones, and network optimization services that minimize the path to the cloud edge. An analysis of the digital divide shows rising inequality as the relative difference between users closest and farthest from cloud compute increases. We also find this inequality unfairly affects lower income census tracts in the US. This result is extended globally using remotely sensed night time lights as a proxy for wealth. Finally, we demonstrate that low earth orbit satellite internet can help to close this digital divide and provide more fair access to the cloud edge.},
journal = {Proc. ACM Netw.},
month = {nov},
articleno = {16},
numpages = {23},
keywords = {networks, edge, measurement, digital divide, datacenter}
}

@article{10.1145/3528412,
author = {Maalek, Reza and Maalek, Shahrokh},
title = {Automatic Recognition and Digital Documentation of Cultural Heritage Hemispherical Domes Using Images},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3528412},
doi = {10.1145/3528412},
abstract = {Recent advancements in optical metrology have enabled continuous documentation of dense 3-dimensional (3D) point clouds of construction projects, including cultural heritage preservation projects. These point clouds must then be further processed to generate semantic digital models, which is integral to the lifecycle management of heritage sites. For large-scale and continuous digital documentation, processing of dense 3D point clouds is computationally cumbersome, and consequentially requires additional hardware for data management and analysis, increasing the time, cost, and complexity of the project. Fast and reliable solutions for generating the geometric digital models is, hence, eminently desirable. This article presents an original approach to generate reliable semantic digital models of heritage hemispherical domes using only two images. New closed formulations were derived to establish the relationships between a sphere and its projected ellipse onto an image. These formulations were then utilised to create new methods for: (i) selecting the best pair of images from an image network; (ii) detecting ellipses corresponding to projection of spheres in images; (iii) matching of the detected ellipses between images; and (iv) generating the sphere's geometric digital models. The effectiveness of the proposed method was evaluated under both laboratory and real-world datasets. Laboratory experiments revealed that the proposed process using the best pair of images provided results as accurate as that achieved using eight randomly selected images, while improving computation time by a factor of 50. The results of the two real-world datasets showed that the digital model of a hemispherical dome was generated with 6.2 mm accuracy, while improving the total computation time of current best practice by a factor of 7. Real-world experimentation also showed that the proposed method can provide metric-scale definition for photogrammetric point clouds with 3 mm accuracy using spherical targets. The results suggest that the proposed method was successful in automatically generating fast and accurate geometric digital models of hemispherical domes.},
journal = {J. Comput. Cult. Herit.},
month = {dec},
articleno = {6},
numpages = {21},
keywords = {spherical targets, metric scale definition, hemispherical domes, Sphere detection, sphere projection in images, digital documentation of spheres}
}

@inproceedings{10.1145/3628797.3628985,
author = {Pham, Stefan and Midoglu, Cise and Seeliger, Robert and Arbanowski, Stefan and Steglich, Stephan},
title = {A Novel Approach to Streaming QoE Score Calculation by Integrating Error Impacts},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628985},
doi = {10.1145/3628797.3628985},
abstract = {Video streaming services have become prominent in the last decade. As any other cloud service, these services are error-prone, and the errors during startup and/or playback affect the viewing experience of end-users. Hence, the calculation of Quality-of-Experience (QoE) scores should also account for error impacts. In this paper, we introduce a player-based error classification scheme, which classifies errors based on origin and severity. We use this scheme to quantify the quality degradation due to errors, and propose to improve the QoE score by integrating these quality factors. We instrument the open-source media players dash.js and Exoplayer in our proposed system which follows the guidelines of various multimedia streaming standards. We define several scenarios focusing on different QoE influencing factors, and assess our proposed model’s performance. Comparisons with various state-of-the-art QoE models show that our model captures the effect on user experience better in scenarios induced with player-related errors.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {415–421},
numpages = {7},
keywords = {streaming analytics, CMCD, video players, error classification, QoE, SAND},
location = {<conf-loc>, <city>Ho Chi Minh</city>, <country>Vietnam</country>, </conf-loc>},
series = {SOICT '23}
}

@inproceedings{10.1145/3613424.3614307,
author = {Zhu, Ligeng and Hu, Lanxiang and Lin, Ji and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
title = {PockEngine: Sparse and Efficient Fine-Tuning in a Pocket},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614307},
doi = {10.1145/3613424.3614307},
abstract = {On-device learning and efficient fine-tuning enable continuous and privacy-preserving customization (e.g., locally fine-tuning large language models on personalized data). However, existing training frameworks are designed for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and lack the optimizations for learning on the edge, which faces challenges of resource limitations and edge hardware diversity. We introduce PockEngine: a tiny, sparse and efficient engine to enable fine-tuning on various edge devices. PockEngine supports sparse backpropagation: it prunes the backward graph and sparsely updates the model with measured memory saving and latency reduction while maintaining the model quality. Secondly, PockEngine is compilation first: the entire training graph (including forward, backward and optimization steps) is derived at compile-time, which reduces the runtime overhead and brings opportunities for graph transformations. PockEngine also integrates a rich set of training graph optimizations, thus can further accelerate the training cost, including operator reordering and backend switching. PockEngine supports diverse applications, frontends and hardware backends: it flexibly compiles and tunes models defined in PyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. We evaluated PockEngine on both vision models and large language models. PockEngine achieves up to 15 \texttimes{} speedup over off-the-shelf TensorFlow (Raspberry Pi), 5.6 \texttimes{} memory saving back-propagation (Jetson AGX Orin). Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orin at 550 tokens/s, 7.9 \texttimes{} faster than the PyTorch.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1381–1394},
numpages = {14},
keywords = {neural network, on-device training, sparse update, efficient finetuning},
location = {<conf-loc>, <city>Toronto</city>, <state>ON</state>, <country>Canada</country>, </conf-loc>},
series = {MICRO '23}
}

@article{10.1145/3569471,
author = {Sangar, Yaman and Biradavolu, Yoganand and Pederson, Kai and Ranganathan, Vaishnavi and Krishnaswamy, Bhuvana},
title = {PACT: Scalable, Long-Range Communication for Monitoring and Tracking Systems Using Battery-Less Tags},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3569471},
doi = {10.1145/3569471},
abstract = {The food and drug industry is facing the need to monitor the quality and safety of their products. This has made them turn to low-cost solutions that can enable smart sensing and tracking without adding much overhead. One such popular low-power solution is backscatter-based sensing and communication system. While it offers the promise of battery-less tags, it does so at the cost of a reduced communication range. In this work, we propose PACT - a scalable communication system that leverages the knowledge asymmetry in the network to improve the communication range of the tags. Borrowing from the backscatter principles, we design custom PACT Tags that are battery-less but use an active radio to extend the communication range beyond standard passive tags. They operate using the energy harvested from the PACT Source. A wide-band Reader is used to receive multiple Tag responses concurrently and upload them to a cloud server, enabling real-time monitoring and tracking at a longer range. We identify and address the challenges in the practical design of battery-less PACT Tags using an active radio and prototype them using off-the-shelf components. We show experimentally that our Tag consumes only 23μJ energy, which is harvested from an excitation Source that is up to 24 meters away from the Tag. We show that in outdoor deployments, the responses from an estimated 520 Tags can be received by a Reader concurrently while being 400 meters away from the Tags.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jan},
articleno = {180},
numpages = {27},
keywords = {passive tag, RF harvesting, backscatter, battery-less}
}

@inproceedings{10.1145/3482632.3487548,
author = {Zhang, Xiaoxiao},
title = {Design and Implementation of College Physical Education Intelligent Management System Based on Big Data Cloud Platform},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487548},
doi = {10.1145/3482632.3487548},
abstract = {For the development of information education in Colleges and universities, the Ministry of education has proposed to adopt the teaching method of big data and cloud platform to improve the teaching level, promote the continuous renewal and development of the traditional education model and integrate the advantages and advantages of online teaching. Take the integrated "cloud platform education mode combining online and offline" as the current normal teaching mode. Based on modern high-tech microelectronics technology, intelligent IC card technology, database technology and network technology, this paper develops an intelligent management system of PE. Through practice, PE has been transformed from traditional decentralized management of teachers to systematic management of sports departments, and from simple qualitative management of objectives or processes to comprehensive management, thus avoiding the phenomenon of inconsistent scale and content of teacher management, getting rid of the defects of insufficient quantity and excessive quality, and strengthening the management level and improving the management efficiency in essence.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2958–2963},
numpages = {6},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3558819.3558836,
author = {Zhang, Yaqiong and Zeng, Wenming and Li, Guanghui and Wen, Yixiao and Yu, Manjiang and Lu, Zhen and Ruan, Hongli and Li, Yuling},
title = {Design and Development of Precise Mango Irrigation Decision-Making System Based on Lora},
year = {2022},
isbn = {9781450397414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558819.3558836},
doi = {10.1145/3558819.3558836},
abstract = {Aiming at the waste of irrigation water resources caused by inaccurate irrigation in mango orchard, a mango accurate irrigation decision-making system based on Lora is designed and developed to improve the yield and quality of mango. The system forms a wireless transmission and remote control network by wireless soil moisture sensor, Lora data transmission terminal, PLC control center, plc4G gateway, etc. Soil moisture data is wirelessly transmitted to the cloud platform in real time, and then is transmitted to the precision irrigation decision-making system through OPC for analysis, processing and storage. The intelligent remote control of the start-stop operation of valves in the irrigation area is realized through the PLC control center. The experiment on system timeliness and accuracy is carried out in mango orchard. According to experimental results, the absolute error between the temperature and humidity data collected by soil moisture sensor and the standard value was small; the measured temperature and humidity data was more accurate, and the accuracy of the data collected by soil moisture sensor was higher; the irrigation decision-making system started quickly as a whole; the instruction delay was small, and the data reporting was relatively rapid; the system had good overall timeliness and accuracy. It could realize the precise irrigation of mango and achieve the purpose of water saving.},
booktitle = {Proceedings of the 7th International Conference on Cyber Security and Information Engineering},
pages = {95–102},
numpages = {8},
keywords = {Lora, Precision irrigation, Cloud platform, Mango, Decision-making system},
location = {<conf-loc>, <city>Brisbane</city>, <state>QLD</state>, <country>Australia</country>, </conf-loc>},
series = {ICCSIE '22}
}

@article{10.1145/3533768,
author = {Wang, Pengfei and Wang, Zixiong and Xin, Shiqing and Gao, Xifeng and Wang, Wenping and Tu, Changhe},
title = {Restricted Delaunay Triangulation for Explicit Surface Reconstruction},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3533768},
doi = {10.1145/3533768},
abstract = {The task of explicit surface reconstruction is to generate a surface mesh by interpolating a given point cloud. Explicit surface reconstruction is necessary when the point cloud is required to appear exactly on the surface. However, for a non-perfect input, such as lack of normals, low density, irregular distribution, thin and tiny parts, and high genus, a robust explicit reconstruction method that can generate a high-quality manifold triangulation is missing.We propose a robust explicit surface reconstruction method that starts from an initial simple surface mesh, alternately performs a Filmsticking step and a Sculpting step of the initial mesh, and converges when the surface mesh interpolates all input points (except outliers) and remains stable. The Filmsticking is to minimize the geometric distance between the surface mesh and the point cloud through iteratively performing a restricted Voronoi diagram technique on the surface mesh, whereas the Sculpting is to bootstrap the Filmsticking iteration from local minima by applying appropriate geometric and topological changes of the surface mesh.Our algorithm is fully automatic and produces high-quality surface meshes for non-perfect inputs that are typically considered to be challenging for prior state of the art. We conducted extensive experiments on simulated scans and real scans to validate the effectiveness of our approach.},
journal = {ACM Trans. Graph.},
month = {oct},
articleno = {180},
numpages = {20},
keywords = {watertight manifold, Surface reconstruction, winding number, restricted Voronoi diagram}
}

@inproceedings{10.1145/3573942.3574093,
author = {Wang, Ziwei and Sun, Wei and Tian, Linyang},
title = {3D Point Cloud Denoising Based on Hybrid Attention Mechanism and Score Matching},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3574093},
doi = {10.1145/3573942.3574093},
abstract = {Due to the limitations of the acquisition equipment, sensors, and the illumination or reflection characteristics of the ground, the acquired point clouds will inevitably be noisy. Noise degrades the quality of point clouds and hinders the subsequent point cloud processing tasks, so the denoising technique becomes a crucial step in point cloud processing. This paper proposes a point cloud denoising algorithm based on a hybrid attention mechanism, which takes into account the complexity of the internal features of point clouds and the randomness of point cloud transformations. Generates channel and spatial attention by parallel maximum pooling and average pooling of point cloud data, trains adaptive attention weights using a multilayer perceptron with shared weights, and serially fuses them, multiplies them with the input features to obtain more robust point cloud features, and connect to the score estimation module using the residuals. By studying and analyzing the mechanism proposed in this paper, it is experimentally demonstrated that the performance of the proposed model under various noise models is vastly improved over the baseline network and outperforms the advanced denoising methods without significantly increasing the network operation cost.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {767–772},
numpages = {6},
keywords = {Point cloud, Hybrid attention module, Denoising, Filtering},
location = {Xiamen, China},
series = {AIPR '22}
}

@article{10.1145/3522742,
author = {Ulvi, Ali},
title = {Using UAV Photogrammetric Technique for Monitoring, Change Detection, and Analysis of Archeological Excavation Sites},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3522742},
doi = {10.1145/3522742},
abstract = {The shrinkage of the sensors installed in unmanned aerial vehicles and the increase in data quality have provided great advantages to UAV users, especially in analysis and interpretation works. Archaeologists, in particular, can take full advantage of new opportunities to research and identify objects and artifacts, using remote sensing methods, by studying the past at excavation sites using modern technologies such as UAVs. These methods enable researchers to discover objects on the ground with the help of sensors. This study includes the UAV monitoring, documentation, and analyses of the excavation works that took place in 2014 (phase 1), 2017 (phase 2- phase 3), and 2020 (phase 4) at the Ancient Theatre of Uzuncabur\c{c} built in the Roman Empire. For this purpose, photos were taken with the UAV for each phase, and measurements were made from the excavation site's points with precision gauges (total-station and GNSS). 3D point cloud, orthophoto map, Digital Elevation Model (DEM) map, and 3D models of each phase were produced with the taken pictures. Since UAV photogrammetry was used in this study, each excavation phase was recorded precisely. This, unlike classical documentation techniques, enabled the deformations in the excavation areas to be revealed. The 3D position accuracy calculated for the control point (ChP) used in the four excavation phases ranges from 5.8 mm to 33.5 mm. The most important feature of this study is the sensitive examination of the changes in the excavation area for many years with the UAV photogrammetry technique. At the end of the study, the excavation phases and the determination of the deformation points in the excavation area were recorded digitally.},
journal = {J. Comput. Cult. Herit.},
month = {sep},
articleno = {58},
numpages = {19},
keywords = {monitoring, excavation analyze, archeological excavation, UAV photogrammetry, Word}
}

@inproceedings{10.1145/3501409.3501571,
author = {Ning, YeYan and Wang, ChunLei and Zhang, ZhenYu and Li, YunJi},
title = {Precise Point Cloud Segmentation Method Based on Distance Judgment Function},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501571},
doi = {10.1145/3501409.3501571},
abstract = {Effective segmentation of point cloud data is an important step in point cloud processing, and it is also a popular research direction in 3D point cloud processing. Traditional region growing algorithms are simple and easy to implement, and are widely used in 3D point cloud segmentation. However, the disorder and complexity of point cloud data and the uncertainty of initial seed node selection lead to over-segmentation and under-segmentation. This paper proposes a region growing algorithm based on distance judgment function calculation. First, we use the octree method to establish the topological relationship of the point cloud data, and construct local k neighborhoods, and eliminate outliers based on its density information; second, we perform k neighborhood search on the data points to obtain the covariance matrix of the neighborhood points, and use principal component analysis to calculate the eigenvalues and eigenvectors of the matrix; we use the minimum spanning tree method to compare the vector dot product, adjust the direction of the normal vector, and ensure the global consistency of the point cloud data; through the average curvature and Gaussian curvature Combined with calculation, the minimum curvature point is selected as the initial seed node, which improves the stability of seed node selection and avoids repeated segmentation; introduces a distance judgment function to judge the attributes of the seed point, calculates the normal distance from the selected seed point to its tangent plane, and passes the distance Threshold divides the point cloud data into flat points and sharp points to improve the efficiency of point cloud adjustment; filter the neighboring points according to the angle between the normal of the seed point and the normal of the neighboring point; finally set the curvature threshold reasonably and determine Guidelines for regional growth. According to the experimental results of segmentation, the region growing algorithm based on the distance judgment function improves the accuracy and stability of part segmentation, and improves the quality of segmentation.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {895–900},
numpages = {6},
keywords = {Distance judgment function, Region growth, Principal component analysis, Three-dimensional point cloud},
location = {Xiamen, China},
series = {EITCE '21}
}

@article{10.1145/3488586,
author = {Wu, Chao and Horiuchi, Shingo and Murase, Kenji and Kikushima, Hiroaki and Tayama, Kenichi},
title = {An Intent-Driven DaaS Management Framework to Enhance User Quality of Experience},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3488586},
doi = {10.1145/3488586},
abstract = {Desktop as a Service (DaaS) has become widely used by enterprises. In 2020, the use of DaaS increased dramatically due to the demand to work remotely from home during the COVID-19 pandemic. The DaaS market is expected to continue growing rapidly [1]. The quality of experience (QoE) of a DaaS service has been one of the main factors to enhance DaaS user satisfaction. To ensure user QoE, the amount of cloud computation resources for a DaaS service must be appropriately designed. We propose an Intent-driven DaaS Management (IDM) framework to autonomously determine the cloud-resource-amount configurations for a given DaaS QoE requirement. IDM enables autonomous resource design by abstracting the knowledge about the dependency between DaaS workload, resource configuration, and performance from previous DaaS performance log data. To ensure the IDM framework's applicability to actual DaaS services, we analyzed five main challenges in applying the IDM framework to actual DaaS services: identifying the resource-design objective, quantifying DaaS QoE, addressing low log data availability, designing performance-inference models, and addressing low resource variations in the log data. We addressed these challenges through detailed designing of IDM modules. The effectiveness of the IDM framework was assessed from the aspects of DaaS performance-inference precision, DaaS resource design, and time and human-resource cost reduction.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {98},
numpages = {25},
keywords = {DaaS, intent-driven management, cloud resource design}
}

@inproceedings{10.1145/3526114.3558724,
author = {Feng, K. J. Kevin and Gao, Alice and Karras, Johanna Suvi},
title = {Towards Semantically Aware Word Cloud Shape Generation},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558724},
doi = {10.1145/3526114.3558724},
abstract = {Word clouds are a data visualization technique that showcases a subset of words from a body of text in a cluster form, where a word’s font size encodes some measure of its relative importance—typically frequency—in the text. This technique is primarily used to help viewers glean the most pertinent information from long text documents and to compare and contrast different pieces of text. Despite their popularity, previous research has shown that word cloud designs are often not optimally suited for analytical tasks such as summarization or topic understanding. We propose a solution for generating more effective visualization technique that shapes the word cloud to reflect the key topic(s) of the text. Our method automates the processes of manual image selection and masking required from current word cloud tools to generate shaped word clouds, better allowing for quick summarization. We showcase two approaches using classical and state-of-the-art methods. Upon successfully generating semantically shaped word clouds using both methods, we performed preliminary evaluations with 5 participants. We found that although most participants preferred shaped word clouds over regular ones, the shape can be distracting and detrimental to information extraction if it is not directly relevant to the text or contains graphical imperfections. Our work has implications on future semantically-aware word cloud generation tools as well as efforts to balance visual appeal of word clouds with their effectiveness in textual comprehension.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {35},
numpages = {5},
keywords = {Text visualization, word clouds, multimodal computer vision},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3482632.3482646,
author = {Cheng, Weiku},
title = {Research on the Value Core and Practice Path Selection of Curriculum Ideology Based on Education Cloud Platform},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482646},
doi = {10.1145/3482632.3482646},
abstract = {The progress of science and technology and the development of computer technology have led us into the information age, and information education has become an inevitable trend in the development of Chinese education. "Curriculum ideology" points to a new concept of ideology, which is an inevitable choice for higher education to realize the whole process and all-round education. The reform of higher vocational curriculum ideology is based on moral education, with the goal of realizing students' all-round growth, emphasizing the organic combination of moral education and intellectual education in the process of education, which is an important measure to solve the "isolated island" dilemma of higher vocational curriculum ideology, build a "great ideology" collaborative education pattern, and train socialist successors who are responsible for national rejuvenation. The emergence of educational cloud platform provides a direction for the reform of the teaching mode of "ideology", and the integration of educational resources is realized by using educational cloud platform to carry out online teaching of "ideology". This paper analyzes the core essence and internal mechanism of "Curriculum ideology" based on educational cloud platform, and puts forward the practical path of synergy effect of curriculum ideology.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {65–69},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3588444.3590997,
author = {Reznik, Yuriy and Barman, Nabajeet and Wagstrom, Patrick},
title = {Improving the Performance of Web-Streaming by Super-Resolution Upscaling},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588444.3590997},
doi = {10.1145/3588444.3590997},
abstract = {In recent years, we have seen significant progress in advanced image and video upscaling techniques, sometimes called super-resolution, or AI-based upscaling. Such algorithms are now broadly available in the forms of software SDKs, as well as functions natively supported by modern graphics cards. However, to take advantage of such technologies in video streaming applications, one needs to (a) add support for super-resolution upscaling in the video rendering chain, (b) develop means for quantifying the effects of using different upscaling techniques on perceived quality, and c) modify streaming clients to use such more advanced scaling techniques in a way that leads to improvements in quality, efficiency, or both.In this paper, we discuss several techniques addressing these challenges. We first present an overview of super-resolution technology. We review available SDKs and libraries for adding super-resolution functionality in streaming players. We next propose a parametric quality model suitable for modeling the effects of different upscaling techniques. We validate it by using an existing widely used dataset with subjective scores. And finally, we present an improved adaptation logic for streaming clients, allowing them to save bandwidth while maintaining quality at the level achievable by standard scaling techniques. Our experiments show that this logic can reduce streaming bitrates by up to 38.9\%.},
booktitle = {Proceedings of the 2nd Mile-High Video Conference},
pages = {8–13},
numpages = {6},
keywords = {quality enhancement, super-resolution, video streaming, machine learning, deep learning, upsampling, adaptive streaming},
location = {Denver, CO, USA},
series = {MHV '23}
}

@inproceedings{10.1145/3578356.3592588,
author = {Christofidi, Georgia and Papaioannou, Konstantinos and Doudali, Thaleia Dimitra},
title = {Toward Pattern-Based Model Selection for Cloud Resource Forecasting},
year = {2023},
isbn = {9798400700842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578356.3592588},
doi = {10.1145/3578356.3592588},
abstract = {Cloud resource management solutions, such as autoscaling and overcommitment policies, often leverage robust prediction models to forecast future resource utilization at the task-, job- and machine-level. Such solutions maintain a collection of different models and at decision time select to use the model that provides the best performance, typically minimizing a cost function. In this paper, we explore a more generalizable model selection approach, based on the patterns of resource usage that are common across the tasks of a job. To learn such patterns, we train a collection of Long Short Term Memory (LSTM) neural networks, at the granularity of a job. During inference, we select which model to use to predict the resource usage of a given task via distance-based time series comparisons. Our experimentation with various time series data representations and similarity metrics reveals cases where even sophisticated approaches, such as dynamic time warping, lead to suboptimal model selection and as a result significantly lower prediction accuracy. Our analysis establishes the importance and impact of pattern-based model selection, and discusses relevant challenges, opportunities and future directions based on our findings.},
booktitle = {Proceedings of the 3rd Workshop on Machine Learning and Systems},
pages = {115–122},
numpages = {8},
keywords = {deep neural network, cloud resource forecasting, long short term memory, cloud computing, timeseries comparison, pattern matching, machine learning},
location = {Rome, Italy},
series = {EuroMLSys '23}
}

@article{10.1145/3577949.3577967,
author = {Loureiro, J. and Cec\'{\i}lio, J.},
title = {Deep Learning for Reliable Communication Optimization on Autonomous Vehicles},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1094-3641},
url = {https://doi.org/10.1145/3577949.3577967},
doi = {10.1145/3577949.3577967},
abstract = {Recent breakthroughs in the autonomous vehicle industry have brought this technology closer to consumers. However, the cost of self-driving solutions still constitutes an entry barrier to many potential users due to its reliance on powerful onboard computers. As an alternative, autonomous driving algorithm processing may be offloaded to remote machines, which requires a reliable connection to the cloud servers. However, despite significant 5G coverage in many countries, mobile network reliability and latency are still inadequate for this purpose. This work explores deep learning concepts to forecast signal quality as a vehicle moves, predicting when periods of degraded network quality will occur. We develop a Long Short-Term Memory (LSTM)-based neural network, trained on multivariate time series containing historical data on several mobile network parameters, and evaluate the results of multi-step Reference Signal Received Power (RSRP) prediction. Results show that our model achieves a rapidly increasing Root-Mean-Square Error (RMSE), reaching over 8 dBm after 25-time steps. This error does not allow for the accurate prediction of future signal quality.},
journal = {Ada Lett.},
month = {dec},
pages = {90–94},
numpages = {5},
keywords = {deep learning, signal quality, autonomous vehicle, forecasting}
}

@inproceedings{10.1145/3511808.3557124,
author = {Pasupuleti, Krishna Kantikiran and Das, Dinesh and Valluri, Satyanarayana R and Zait, Mohamed},
title = {Observability of SQL Hints in Oracle},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557124},
doi = {10.1145/3511808.3557124},
abstract = {Observability is a critical requirement of increasingly complex and cloud-first data management systems. In most commercial databases, this relies on telemetry like logs, traces, and metrics, which helps to identify, mitigate, and resolve issues expeditiously. SQL monitoring tools, for example, can show how a query is performing. One area that has received comparatively less attention is the observability of the query optimizer whose inner workings are often shrouded in mystery. Optimizer traces can illuminate the plan selection process for a query, but they are comprehensible only to human experts and are not easily machine-parsable to remediate sub-optimal plans. Hints are directives that guide the optimizer toward specific directions. While hints can be used manually, they are often used by automatic SQL plan management tools that can quickly identify and resolve regressions by selecting alternate plans. It is important to know when input hints are inapplicable so that the tools can try other strategies. For example, a manual hint may have syntax errors, or an index in an automatic hint may have been accidentally dropped. In this paper, we describe the design and implementation of Oracle's hint observability framework which provides a comprehensive usage report of all hints, manual or otherwise, used to compile a query. The report, which is available directly in the execution plan in a human-understandable and machine-readable format, can be used to automate any necessary corrective actions. This feature is available in Oracle Autonomous Database 19c.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3441–3450},
numpages = {10},
keywords = {SQL plan management, SQL hints, autonomous database administration, observability, query optimization},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3550454.3555443,
author = {Xu, Rui and Wang, Zixiong and Dou, Zhiyang and Zong, Chen and Xin, Shiqing and Jiang, Mingyan and Ju, Tao and Tu, Changhe},
title = {RFEPS: Reconstructing Feature-Line Equipped Polygonal Surface},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555443},
doi = {10.1145/3550454.3555443},
abstract = {Feature lines are important geometric cues in characterizing the structure of a CAD model. Despite great progress in both explicit reconstruction and implicit reconstruction, it remains a challenging task to reconstruct a polygonal surface equipped with feature lines, especially when the input point cloud is noisy and lacks faithful normal vectors. In this paper, we develop a multistage algorithm, named RFEPS, to address this challenge. The key steps include (1) denoising the point cloud based on the assumption of local planarity, (2) identifying the feature-line zone by optimization of discrete optimal transport, (3) augmenting the point set so that sufficiently many additional points are generated on potential geometry edges, and (4) generating a polygonal surface that interpolates the augmented point set based on restricted power diagram. We demonstrate through extensive experiments that RFEPS, benefiting from the edge-point augmentation and the feature preserving explicit reconstruction, outperforms state of the art methods in terms of the reconstruction quality, especially in terms of the ability to reconstruct missing feature lines.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {228},
numpages = {15},
keywords = {feature line, point cloud, restricted power diagram, computer-aided design, surface reconstruction}
}

@inproceedings{10.1145/3581783.3612257,
author = {Tang, Sheng-Ming and Sun, Yuan-Chun and Hsu, Cheng-Hsin},
title = {A Blind Streaming System for Multi-Client Online 6-DoF View Touring},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612257},
doi = {10.1145/3581783.3612257},
abstract = {Online 6-DoF view touring has become increasingly popular due to hardware advances and the recent pandemic. One way for content creators to support many 6-DoF clients is by transmitting 3D content to them, which leads to content leakage. Another way for content creators is to render and stream novel views for 6-DoF clients, which incurs staggering computational and networking workloads. In this paper, we develop a blind streaming system that leverages cloud service providers between content creators and 6-DoF clients. Our system has two core design objectives: (i) to generate high-quality novel views for 6-DoF clients without retrieving 3D content from content creators, (ii) to support many 6-DoF clients without overloading the content creators. We achieve these two goals in the following steps. First, we design a source view request/response interface between cloud service providers and content creators for efficient communications. Second, we design novel view optimization algorithms for cloud service providers to intelligently select the minimal set of source views while considering the workload of content creators. Third, we employ scalable client side view synthesis for 6-DoF clients with heterogeneous device capabilities and personalized 6-DoF client poses and preferences. Our evaluation results demonstrate the merits of our solution, compared to the state-of-the-arts, our system: (i) improves synthesized novel views by 2.27 dB in PSNR and 12 in VMAF on average and (ii) reduces the bandwidth consumption by 94\% on average. In fact, our solution approaches the performance of an unrealistic optimal solution with unlimited source views, achieving performance gaps as small as 0.75 dB in PSNR and 3.8 in VMAF.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9124–9133},
numpages = {10},
keywords = {system design, discrete optimization, content privacy, computer graphics, view synthesis},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{10.1145/3510450.3517285,
author = {Hadar, Ravid and Schapira, Michael},
title = {Network Congestion Control and Its Impact on Video Streaming QoE},
year = {2022},
isbn = {9781450392228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510450.3517285},
doi = {10.1145/3510450.3517285},
abstract = {Congestion control plays a crucial role in Internet-based content delivery. Congestion control brings order to the Internet's crowded traffic system by sharing the scarce network bandwidth between competing services and users. Congestion control algorithms continuously modulate the rate at which data packets are injected into the network by traffic sources in response to network conditions.Congestion control immensely impacts quality of experience (QoE) for services like video streaming, video conferencing, and cloud gaming; sending packets too slowly prevents supporting high video quality (HD/UHD); sending too fast can overwhelm the network, resulting in data being lost or delayed, leading to phenomena such as video rebuffering.While congestion control has been a key focus for both academic and industrial research for decades, the exact correlation between the performance of the congestion control algorithms employed by video servers and the QoE experienced by video clients remains poorly understood. We will report on our experimental results along these lines.We evaluated and contrasted three dominant congestion control schemes: TCP Cubic [3], which is the default for many operating systems, and two recently proposed congestion control schemes, namely, Google's Bottleneck-Bandwidth-and-RTT (BBR) [1] protocol, and Performance-oriented Congestion Control (PCC) [2].Our experimental setup consisted of a video cache that sends http-based video traffic across an emulated network environment towards a video client. We took into consideration both MPEG-DASH and HLS-based video streaming and both wired and wireless networks. We ran multiple experiments for varying network conditions (e.g., the available bandwidth, non-congestion-related packet loss, network latency, and depth of in-network buffers, etc.).By monitoring the behavior of the congestion controller and examining the QoE data from the video player (e.g., video start-time, average bitrate, rebuffering ratio, etc.), we have been able to draw meaningful conclusions. Specifically, our results shed light on the features of network-level performance that most impact user-perceived QoE, quantify the benefits for performance of employing modern congestion control protocols, and provide insights into the interplay between congestion control, the network environment, and the video player.Below is a diagram describing the experiment setup:},
booktitle = {Proceedings of the 1st Mile-High Video Conference},
pages = {111},
numpages = {1},
keywords = {quality of experience, online learning, congestion control, transport protocols, QoE, video streaming},
location = {Denver, Colorado},
series = {MHV '22}
}

@inproceedings{10.1145/3487553.3524628,
author = {Gregoriadis, Marcel and Muth, Robert and Florian, Martin},
title = {Analysis of Arbitrary Content on Blockchain-Based Systems Using BigQuery},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524628},
doi = {10.1145/3487553.3524628},
abstract = {Blockchain-based systems have gained immense popularity as enablers of independent asset transfers and smart contract functionality. They have also, since as early as the first Bitcoin blocks, been used for storing arbitrary contents such as texts and images. On-chain data storage functionality is useful for a variety of legitimate use cases. It does, however, also pose a systematic risk. If abused, for example by posting illegal contents on a public blockchain, data storage functionality can lead to legal consequences for operators and users that need to store and distribute the blockchain, thereby threatening the operational availability of entire blockchain ecosystems. In this paper, we develop and apply a cloud-based approach for quickly discovering and classifying content on public blockchains. Our method can be adapted to different blockchain systems and offers insights into content-related usage patterns and potential cases of abuse. We apply our method on the two most prominent public blockchain systems—Bitcoin and Ethereum—and discuss our results. To the best of our knowledge, the presented study is the first to systematically analyze non-financial content stored on the Ethereum blockchain and the first to present a side-by-side comparison between different blockchains in terms of the quality and quantity of stored data.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {478–487},
numpages = {10},
keywords = {Cryptocurrency, BigQuery, Blockchain, Ethereum, Bitcoin},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3605573.3605643,
author = {Zhang, Songli and Zheng, Zhenzhe and Wu, Fan and Li, Bingshuai and Shao, Yunfeng and Chen, Guihai},
title = {Learning From Your Neighbours: Mobility-Driven Device-Edge-Cloud Federated Learning},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605573.3605643},
doi = {10.1145/3605573.3605643},
abstract = {Federated learning (FL) in large-scale wireless networks is implemented in a hierarchical way by introducing edge servers as relays between the cloud server and devices, where devices are dispersed within multiple clusters coordinated by edges. However, the devices are usually mobile users with unpredictable mobile trajectories, whose effects on the model training process are still less studied. In this work, we propose a new MobIlity-Driven feDerated LEarning framework, namely MIDDLE in wireless networks, which can relieve unbalanced and biased model updates by leveraging the new model aggregation opportunities on mobile devices due to their mobility across edges. Specifically, mobile devices can have different models while traversing across edges, and adequately aggregate these models on the device. By theoretical analysis, we can show that this on-device model aggregation can reduce the bias of model updating on edges and cloud, and then accelerate the convergence of model training in FL. Then, we define a model similarity utility to measure the difference in gradient updates among various models, which guides the adaptive on-device model aggregation and in-edge device selection to facilitate the comprehensive information sharing between edges. Extensive experiment results validate that MIDDLE can achieve 1.51 \texttimes{} −6.85 \texttimes{} speedup on the model training, compared with the state-of-the-art model training approaches in hierarchical FL.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing},
pages = {462–471},
numpages = {10},
keywords = {Device Mobility., Device-Edge-Cloud Cooperation, Federated Learning},
location = {<conf-loc>, <city>Salt Lake City</city>, <state>UT</state>, <country>USA</country>, </conf-loc>},
series = {ICPP '23}
}

@article{10.1145/3550454.3555497,
author = {Kopanas, Georgios and Leimk\"{u}hler, Thomas and Rainer, Gilles and Jambon, Cl\'{e}ment and Drettakis, George},
title = {Neural Point Catacaustics for Novel-View Synthesis of Reflections},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555497},
doi = {10.1145/3550454.3555497},
abstract = {View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {201},
numpages = {15},
keywords = {neural rendering, catacaustics, point-based rendering, differentiable rasterization, reflections}
}

@article{10.1109/TNET.2021.3103796,
author = {Ma, Richard T. B.},
title = {Internet Transport Economics: Model and Analysis},
year = {2021},
issue_date = {Dec. 2021},
publisher = {IEEE Press},
volume = {29},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3103796},
doi = {10.1109/TNET.2021.3103796},
abstract = {With the rise of video streaming and cloud services, the Internet has evolved into a content-centric service platform. Due to the best-effort service model of the Internet, the quality of service (QoS) of Internet services however cannot be guaranteed. Furthermore, characterizing QoS is challenging since it depends on the autonomous business decisions such as capacity planning, routing strategies and peering agreements of network providers. To quantify the QoS for Internet-based services, we regard the Internet infrastructure as a transport system for data packets and study the Internet ecosystem and the economics of transport services collectively provided by the autonomous network providers. In contrast to the traditional transport economics that studies the movement of people and goods over space and time, our focus in the &lt;italic&gt;Internet transport economics&lt;/italic&gt; is the movement of streams of data packets that create information services. In particular, we model the supply of network capacities and demands of throughput driven by network protocols and establish a macroscopic network equilibrium under which both the end-to-end delays and drop rates of Internet routes can be derived. We show that this equilibrium solution always exists and its uniqueness can be guaranteed under various realistic scenarios. We analyze the impacts of user demands and resource capacities on the network equilibrium and provide implications of Netflix-Comcast type of peering on the QoS of users. We demonstrate that our framework can be used as a building block to understand the routing strategies under a Wardrop equilibrium and to enable further studies such as Internet peering and in-network caching.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {2843–2854},
numpages = {12}
}

@inproceedings{10.1145/3542929.3563502,
author = {Wang, Haodong and Du, Kuntai and Jiang, Junchen},
title = {Minimizing Packet Retransmission for Real-Time Video Analytics},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3542929.3563502},
doi = {10.1145/3542929.3563502},
abstract = {In smart-city and video analytics (VA) applications, high-quality data streams (video frames) must be accurately analyzed with a low delay. Since maintaining high accuracy requires compute-intensive deep neural nets (DNNs), these applications often stream massive video data to remote, more powerful cloud servers, giving rise to a strong need for low streaming delay between video sensors and cloud servers while still delivering enough data for accurate DNN inference. In response, many recent efforts have proposed distributed VA systems that aggressively compress/prune video frames deemed less important to DNN inference, with the underlying assumptions being that (1) without increasing available bandwidth, reducing delays means sending fewer bits, and (2) the most important frames can be precisely determined before streaming. This short paper challenges both views. First, in high-bandwidth networks, the delay of real-time videos is primarily bounded by packet losses and delay jitters, so reducing bitrate is not always as effective as reducing packet retransmissions. Second, for many DNNs, the impact of missing a video frame depends not only on itself but also on which other frames have been received or lost. We argue that some changes must be made in the transport layer, to determine whether to resend a packet based on the packet's impact on DNN's inference dependent on which packets have been received. While much research is needed toward an optimal design of DNN-driven transport layer, we believe that we have taken the first step in reducing streaming delay while maintaining a high inference accuracy.},
booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
pages = {340–347},
numpages = {8},
keywords = {systems for machine learning, action recognition, transport layer protocol, video analytics},
location = {San Francisco, California},
series = {SoCC '22}
}

@inproceedings{10.1145/3512388.3512401,
author = {Guo, Shaogang and Xu, Yunfei and Li, Wang},
title = {Spatial Non-Cooperative Target Point Cloud Reconstruction},
year = {2022},
isbn = {9781450395465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512388.3512401},
doi = {10.1145/3512388.3512401},
abstract = {Due to the inherent defects of laser sensors, the original point cloud of non-cooperate targets are usually irregularly distributed, which brings great challenges to high-quality 3D surface reconstruction of non-cooperate targets. In this paper, we leverage a method based local hierarchical clustering (LHC) to improve the consistency of point distribution. Specifically, our method includes two main steps. The first one is the adaptive octree-based 3D spatial decomposition and the second one is hierarchical clustering. The main purpose of the former one is to reduce the complexity of the algorithm, and the later aims to convert the non-uniform point set to uniform one. We carried out experiments on three non-cooperative target models. The results of visualization and quantitative calculation verify the effectiveness of our method.},
booktitle = {Proceedings of the 2022 5th International Conference on Image and Graphics Processing},
pages = {84–88},
numpages = {5},
keywords = {Point cloud, Non-cooperative target, 3D reconstruction},
location = {Beijing, China},
series = {ICIGP '22}
}

@article{10.1145/3517805,
author = {Yinying, Cai and Li, Juan and Wang, Bo},
title = {Data Mining Techniques and Machine Learning Algorithms in the Multimedia System to Enhance Engineering Education},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3517805},
doi = {10.1145/3517805},
abstract = {In the current digital era, engineering education worldwide faces a massive challenge in education and career development. By authorizing educators and administrators to migrate to the actions, cloud services technology has transformed into the educational environment. A Multimedia assisted smart learning system (MSLS) has been suggested in this paper where universities/colleges will advocate future development and begin skill-set enhancement courses by e-learning. To classify their employment prospects at the early stage of graduation, this proposed system measures learners' academic/skill data. Machine learning and Data mining are advanced research fields whose accelerated advancement is attributable to developments in data processing research, database industry growth, and business requirements for methods capable of extracting useful information from massive data stores. In addition, for skill set evaluation, a practical algorithm is suggested to find different groups of students that lack the appropriate skill set. The anticipated student groups can be provided with opportunities by e-learning to enhance their required skill set. The findings suggest that more critical choices can boost employment prospects and overall educational development by implementing the new engineering education system.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {112},
numpages = {21},
keywords = {multimedia system, Machine learning, data mining, engineering education}
}

@inproceedings{10.1145/3517206.3526269,
author = {B\"{a}urle, Simon and Mohan, Nitinder},
title = {ComB: A Flexible, Application-Oriented Benchmark for Edge Computing},
year = {2022},
isbn = {9781450392532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517206.3526269},
doi = {10.1145/3517206.3526269},
abstract = {Edge computing is an attractive platform where applications, previously hosted in the cloud, shift parts of their workload on resources closer to the users. The field is still in its nascent stages with significant ongoing innovation in small form-factor hardware designed to operate at the edge. However, the increased hardware heterogeneity at the edge makes it difficult for application developers to determine if their workloads will operate as desired. Simultaneously, edge providers have to make expensive deployment choices for the "correct" hardware that will remain suitable for the near future. We present ComB, an application-oriented benchmarking suite for edge that assists early adopters in evaluating the suitability of an edge deployment. ComB is flexible, extensible, and incorporates a microservice-based video analytics pipeline as default workload to measure underlying hardware's compute and networking capabilities accurately. Our evaluation on a heterogeneous testbed shows that ComB enables both providers and developers to understand better the runtime capabilities of different hardware configurations for supporting operations of applications designed for the edge.},
booktitle = {Proceedings of the 5th International Workshop on Edge Systems, Analytics and Networking},
pages = {19–24},
numpages = {6},
keywords = {edge computing, benchmarking, next-generation applications},
location = {Rennes, France},
series = {EdgeSys '22}
}

@inproceedings{10.1145/3538969.3539012,
author = {Ardagna, Claudio A. and Bena, Nicola and de Pozuelo, Ramon Mart\'{\i}n},
title = {Bridging the Gap Between Certification and Software Development},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3539012},
doi = {10.1145/3538969.3539012},
abstract = {While certification is widely recognized as a means to increase system trustworthiness and reduce uncertainty in decision making, it faces severe challenges preventing a wider adoption thereof. Certification is not adequately planned and integrated within the development process, leading to suboptimal scenarios where certification introduces the need to further modify the developed system with high costs. We propose a methodology that bridges the gap between software development and certification processes. Our methodology automatically produces the certification requirements driving all steps of the development process, and maximizes the strength of certificates while taking costs under control. We formalize the above problem as a multi-objective mathematical program and solve it through a genetic algorithm. The proposed approach is tested in a real-world, cloud-based financial scenario at CaixaBank and its performance and quality is evaluated in a simulated scenario.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {19},
numpages = {10},
keywords = {Software Development, Security, Certification},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.1145/3604930.3605711,
author = {Maji, Diptyaroop and Pfaff, Ben and P R, Vipin and Sreenivasan, Rajagopal and Firoiu, Victor and Iyer, Sreeram and Josephson, Colleen and Pan, Zhelong and Sitaraman, Ramesh K},
title = {Bringing Carbon Awareness to Multi-Cloud Application Delivery},
year = {2023},
isbn = {9798400702426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604930.3605711},
doi = {10.1145/3604930.3605711},
abstract = {Data centers consume roughly 1--2\% of the world's electricity, with the majority of it attributed to compute, making the computing industry a substantial source of greenhouse gas emissions. Resources in data centers typically focus on providing high performance and availability, but the question of sustainability in managing these distributed resources often goes unnoticed over these other metrics. This problem will only exacerbate as the data center computing demand continues to increase.In this paper, we address the sustainability aspect of load balancing in VMware's Avi Global Server Load Balancer (GSLB). Our GSLB deployment spans data centers across geographies and clouds and relies on geographical proximity to shift client application requests to the closest data center. In this work, we enhance the GSLB service to additionally consider the real-time carbon intensity at each data center as a factor in making a load-balancing choice. Our carbon-aware prototype shows an average of 21\% and a maximum of 51\% reduction in carbon emissions while operating with an acceptable latency.},
booktitle = {Proceedings of the 2nd Workshop on Sustainable Computer Systems},
articleno = {6},
numpages = {6},
keywords = {marginal carbon intensity, data center computing, spatial load balancing, stateless workloads},
location = {Boston, MA, USA},
series = {HotCarbon '23}
}

@inproceedings{10.1145/3540250.3558958,
author = {Shetty, Manish and Bansal, Chetan and Upadhyayula, Sai Pramod and Radhakrishna, Arjun and Gupta, Anurag},
title = {AutoTSG: Learning and Synthesis for Incident Troubleshooting},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558958},
doi = {10.1145/3540250.3558958},
abstract = {Incident management is a key aspect of operating large-scale cloud services. To aid with faster and efficient resolution of incidents, engineering teams document frequent troubleshooting steps in the form of Troubleshooting Guides (TSGs), to be used by on-call engineers (OCEs). However, TSGs are siloed, unstructured, and often incomplete, requiring developers to manually understand and execute necessary steps. This results in a plethora of issues such as on-call fatigue, reduced productivity, and human errors. In this work, we conduct a large-scale empirical study of over 4K+ TSGs mapped to incidents and find that TSGs are widely used and help significantly reduce mitigation efforts. We then analyze feedback on TSGs provided by 400+ OCEs and propose a taxonomy of issues that highlights significant gaps in TSG quality. To alleviate these gaps, we investigate the automation of TSGs and propose AutoTSG -- a novel framework for automation of TSGs to executable workflows by combining machine learning and program synthesis. Our evaluation of AutoTSG on 50 TSGs shows the effectiveness in both identifying TSG statements (accuracy 0.89) and parsing them for execution (precision 0.94 and recall 0.91). Lastly, we survey ten Microsoft engineers and show the importance of TSG automation and the usefulness of AutoTSG.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1477–1488},
numpages = {12},
keywords = {Meta Learning, Incident Management, Troubleshooting, Program Synthesis, Cloud Reliability},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3524273.3535781,
author = {O'Sullivan, Samantha and Murray, Niall and Rodrigues, Thiago Braga},
title = {A Telehealth and Sensor-Based System for User-Centered Physical Therapy in Parkinson's Disease: Research Proposal},
year = {2022},
isbn = {9781450392839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524273.3535781},
doi = {10.1145/3524273.3535781},
abstract = {This paper contains the research proposal of Samantha O'Sullivan that was presented at the MMSys 2022 doctoral symposium. The use of wearable sensors for the understanding and quantification of movement within research communities working on Parkinson's Disease (PD) has increased significantly in recent years with a motivation to objectively diagnose, assess and then understand the progression of the disease. Most studies taking this approach for PD have stated that there is a need for a long-term solution, due to varying symptoms at different stages of the disease. COVID-19 has brought further limitations in the delivery of clinical care, reducing time with therapists and doctors whilst increasing the preference for at-home care. The necessity for a system for patients with PD is extremely significant. There is no clinically available long-term assessment for tremors, which is an issue highlighted in the literature. By using wireless sensors to track tremor severity continuously, and telehealth to create communication between patient and clinician, this proposed system will allow for better targeted therapy, accurate statistics, and constant accessible data. In this context, this work will design, build, and evaluate a novel system that would allow for constant monitoring of a patient with tremors. By using wireless sensors and telehealth, it will provide more detailed data that may enable directed and informed physical therapy. It will also improve communication creating a data flow constantly between clinician and patient to improve person-centered feedback, and aid towards the diagnosis and assessment of disease progression. The incorporation of a mobile/cloud-based application to assist this is due to the current heightened preference for home-based healthcare, long-term evaluation of tremors and personalized physical therapy. The primary focus of the PhD will be on capturing tremor activity and progression through a telehealth-based system. This proposed system will obtain real-time readings of tremors using wireless sensors and an application that will communicate consistently with healthcare professionals. The aim will be to provide better home-based care, person-centered physical therapy and improve quality of life.},
booktitle = {Proceedings of the 13th ACM Multimedia Systems Conference},
pages = {383–387},
numpages = {5},
keywords = {sensors, rehabilitation, quantitative motor assessment, telehealth, Parkinson's disease},
location = {Athlone, Ireland},
series = {MMSys '22}
}

@inproceedings{10.1145/3587828.3587866,
author = {Bimenyimana, Emmanuel and Nsengiyumva, Philibert and Ngoga, Said Rutabayiro},
title = {IoT Monitoring and Control System of Distribution Transformers in Rwanda},
year = {2023},
isbn = {9781450398589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587828.3587866},
doi = {10.1145/3587828.3587866},
abstract = {In developing countries, many customers do not get good quality of electricity power supply due to frequent and prolonged power fluctuations/cuts. Distribution transformer&nbsp;(DT) is a&nbsp;service transformer&nbsp;that provides the final&nbsp;voltage&nbsp;transformation in the&nbsp;electric power distribution&nbsp;system, stepping down the voltage to the level used by the customers. Monitoring and control of DT (as a crucial and expensive asset in the network) is a key enabler of power stability to consumers. A power utility is said to be a business oriented with good image representation if it delivers a reliable and affordable electricity. Modern technologies such as the Internet of Things (IoT) offer a wide range of applications in the energy sector to smoothly monitor, control and optimize processes. Currently, many energy companies in developing countries are not yet implementing the remote system to control and monitor the secondary side of DTs and timely get the notifications of fluctuations/abnormalities occurred on those DTs. That is why it is still challenging and time consuming to intervene urgently and do the necessary actions to prevent severe and prolonged power cuts/fluctuations and safeguard the damage of DTs themselves with customer's appliances connected on those DTs. The secondary side of DT is the one connected directly and supply power to the customers. For this reason, we developed an affordable IoT system that automatically detects the abnormalities/fluctuations of three core technical parameters of DT (which are voltage, current and temperature) using current sensors, voltage sensors, and temperature sensor with ATmega 328P Microcontroller to collect and process data from sensors connected to DT system. Once one or all of those technical parameters become abnormal, the system cut off automatically the secondary side of DT in 2 seconds to isolate and protect the customers’ load with safeguarding DT itself using a power relay. At the same time, GSM/GPRS module uploads the sensed abnormal data to the cloud storage, displays them on web-based application for visualization, and sends the corresponding short message service (sms) to notify the issue to the authorized person in 5 seconds for speeding up the interventions and power restoration. In case there is a movement related to the vandalism in the compound of DT, a PIR sensor detects the human motion then a camera takes the related picture and send it to the utility with the corresponding sms. A buzzer generates an audio signaling to warn the culprit/criminal until he left the site. If there is no abnormality detected, the system keeps sensing without sending the data to the cloud. We can open and close remotely the secondary side of DT and buzzer. This system is powered using a rechargeable battery.},
booktitle = {Proceedings of the 2023 12th International Conference on Software and Computer Applications},
pages = {253–259},
numpages = {7},
keywords = {voltage, Internet of Things (IoT), current, temperature, distribution transformer},
location = {<conf-loc>, <city>Kuantan</city>, <country>Malaysia</country>, </conf-loc>},
series = {ICSCA '23}
}

@inproceedings{10.1145/3592813.3592898,
author = {Silva J\'{u}nior, Paulo Freitas and Fran\c{c}a, Tiago Cruz and Sampaio, Jonice Oliveira},
title = {CARAMEL: Ecosystem for Big Social Data},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592813.3592898},
doi = {10.1145/3592813.3592898},
abstract = {Context: A large volume of data produced in social media is analyzed through different perspectives. Much effort goes into retrieving and processing the data, maintaining the necessary infrastructure, and building and sharing the foundation between actors with different roles. These challenges are observed in data ecosystems. Problem: The central systems to support data analysis from social networks have some restrictions (data collection, sharing, reuse, etc.). Data collection and analysis require technical skills that some users need and do not have, impacting the quality of inferences, accounting, and conclusions. Solution: We propose an architecture for “Big Social Data” ecosystems considering the collaborative construction of data extraction and sharing mechanisms. IS Theory: This proposal is related to “knowledge-based theory,” as much knowledge can be inferred from social data. It also supports the Externalization and Combination steps of the Organizational knowledge creation model. Method: We observe aspects related to data analysis, considering the reuse of the mechanisms created and the sharing of bases that can run and be stored in a distributed way to meet even instantaneous analysis. Results: The architecture was implemented to work in a distributed way, contains a collector and a filter and allows data sharing. A data collection test was conducted during the 2022 presidential elections in Brazil. Contributions: The main contribution is the architecture of a Big Social Data Ecosystem, focused on the evolution of social data analysis that also observes the interoperability between distributed solutions. The technological contributions are an instance of this architecture for the cloud, social media data collectors, and datasets of the 2022 election in Brazil.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Information Systems},
pages = {136–142},
numpages = {7},
keywords = {Social networks., Social data analysis, Microservices, Data ecosystem, Cloud computing},
location = {<conf-loc>, <city>Macei\'{o}</city>, <country>Brazil</country>, </conf-loc>},
series = {SBSI '23}
}

@inproceedings{10.1145/3583133.3595841,
author = {Syu, Yang and Fanjiang, Yong-Yi},
title = {Multi-Step-Ahead Web Service QoS Time Series Forecasting: A Multi-Predictor-Based Genetic Programming Approach},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3595841},
doi = {10.1145/3583133.3595841},
abstract = {Previously, in a GECCO 2022 Hot-off-the-Press paper [1], we presented a comprehensive survey of the modeling and prediction of Web service (WS) quality of service (QoS) time series [2]. Based on the exhaustive investigation in [2], this research subject has already been deeply and widely studied for over a decade; for the one-step-ahead version of this problem, which can be considered its most primitive problem form, overall, our proposed and developed genetic programming (GP)-based solution outperforms competitors in terms of both modeling and forecasting accuracy, according to our ongoing study, which has been reported in [3] [4] [5]. Nevertheless, as argued in [6], for the long-term use and rental of cloud-based WSs, multi-step-ahead QoS time series prediction of these services is needed. Thus, the authors employed and revised the two most widely used single-predictor-based time series methods, namely, autoregressive integrated moving average (ARIMA) models and exponential smoothing (ES), to address this latest version of the problem.For this multi-step-ahead variant of the problem, in Y. Y. Fanjiang, Y. Syu and W. L. Huang, "Time Series QoS Forecasting for Web Services Using Multi-Predictor-based Genetic Programming", IEEE Transactions on Services Computing (TSC), Vol. 15, P.P. 1423--1435, 2022, we devise and employ a multipredictor-based approach to genetic programming. First, due to its superiority in our past work for the basic (i.e., one-step-ahead) version of the problem, we investigate the performance of GP on this newly emerged (multi-step-ahead) form of the problem. Second, instead of using a single model for predictions regarding multiple future time points, which is the method commonly adopted in prior research [2], we evolve and apply a dedicated GP-generated predictor for each targeted future time point and its projection. Furthermore, two different strategies for the consumed predictor inputs are tested to determine their differences and influence on accuracy so that a better strategy can be empirically determined. In addition, we propose in the reported paper [7] two disparate techniques to further enhance the resulting performance of our multi-predictor-based GP method.As in our previous GECCO Hot-off-the-Press paper [1], this abstract paper presents to the GECCO community a verified application of GP on a more difficult and challenging type of WS QoS time series forecasting. Our purpose is to enable the GECCO community to use this application of GP, to try to improve GP to obtain more accurate and better results, and to investigate other potential evolutionary paradigms and techniques for this issue.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {43–44},
numpages = {2},
keywords = {genetic programming, time series forecasting, service-oriented software engineering, machine learning, web services},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3566099.3569006,
author = {Lin, Hai and Chen, Xianfu},
title = {Transformer-Driven Multi-Agent Deep Reinforcement Learning Based Point Cloud Video Transmissions},
year = {2022},
isbn = {9781450397841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3566099.3569006},
doi = {10.1145/3566099.3569006},
abstract = {The point cloud videos, a medium for representing natural content in AR/VR with point clouds, have attracted a wide range of attention for its characteristics and have the potential to be the next generation of video technology. Given the high data volume, the point cloud video raises the challenge of intelligent transmission and resource scheduling in multi-user scenarios under time-varying system conditions. In this paper, we propose a multi-agent deep reinforcement learning (DRL) approach to optimize the expected long-term multi-user QoE and adopt a Field of View (FoV) prediction model with Transformer for high-accuracy FoV prediction. Over the time horizon, the proposed approach learns to select the tiles of the corresponding video in accordance with a proposed well-defined QoE model capable of quantifying users' satisfaction for transmissions in an iterative way. Under various settings, extensive numerical experiments based on real throughput data traces and different computation capabilities data demonstrate that the proposed approach is effective for long-term multi-agent point cloud video transmissions.},
booktitle = {Proceedings of the 1st Workshop on Digital Twin \&amp; Edge AI for Industrial IoT},
pages = {25–30},
numpages = {6},
keywords = {quality of experience, point cloud video, deep reinforcement learning, transformer},
location = {Sydney, NSW, Australia},
series = {AIIOT '22}
}

@article{10.1145/3478513.3480486,
author = {Liu, Yanchao and Guo, Jianwei and Benes, Bedrich and Deussen, Oliver and Zhang, Xiaopeng and Huang, Hui},
title = {TreePartNet: Neural Decomposition of Point Clouds for 3D Tree Reconstruction},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3478513.3480486},
doi = {10.1145/3478513.3480486},
abstract = {We present TreePartNet, a neural network aimed at reconstructing tree geometry from point clouds obtained by scanning real trees. Our key idea is to learn a natural neural decomposition exploiting the assumption that a tree comprises locally cylindrical shapes. In particular, reconstruction is a two-step process. First, two networks are used to detect priors from the point clouds. One detects semantic branching points, and the other network is trained to learn a cylindrical representation of the branches. In the second step, we apply a neural merging module to reduce the cylindrical representation to a final set of generalized cylinders combined by branches. We demonstrate results of reconstructing realistic tree geometry for a variety of input models and with varying input point quality, e.g., noise, outliers, and incompleteness. We evaluate our approach extensively by using data from both synthetic and real trees and comparing it with alternative methods.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {232},
numpages = {16},
keywords = {optimization, deep learning, procedural generation, geometric modeling, procedural modeling, 3D reconstruction}
}

@inproceedings{10.1145/3603165.3607394,
author = {Zhou, Ruiting and Yu, Jieling},
title = {A Reinforcement Learning Approach for Minimizing Job Completion Time in Clustered Federated Learning},
year = {2023},
isbn = {9798400702334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603165.3607394},
doi = {10.1145/3603165.3607394},
abstract = {Federated Learning (FL) enables potentially a large number of clients to collaboratively train a global model with the coordination of a central cloud server without exposing client raw data. However, the FL model convergence performance, often measured by the job completion time, is hindered by two critical factors: non independent and identically distributed (non-IID) data across clients and the straggler effect. In this work, we propose a clustered FL framework, MCFL, to minimize the job completion time by mitigating the influence of non-IID data and the straggler effect while guaranteeing the FL model convergence performance. MCFL builds upon a two-stage operation: i) a clustering algorithm constructs clusters, each containing clients with similar computing and communications capabilities to combat the straggler effect within a cluster; ii) a deep reinforcement learning (DRL) algorithm based on soft actor-critic with discrete actions intelligently selects a subset of clients from each cluster to mitigate the impact of non-IID data, and derives the number of intra-cluster aggregation iterations for each cluster to reduce the straggler effect among clusters. Extensive testbed experiments are conducted under various configurations to verify the efficacy of MCFL. The results show that MCFL can reduce the job completion time by up to compared with three state-of-the-art FL frameworks.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2023},
pages = {55–56},
numpages = {2},
location = {Wuhan, China},
series = {ACM TURC '23}
}

@article{10.1145/3470007,
author = {Bhimani, Janki and Yang, Zhengyu and Yang, Jingpei and Maruf, Adnan and Mi, Ningfang and Pandurangan, Rajinikanth and Choi, Changho and Balakrishnan, Vijay},
title = {Automatic Stream Identification to Improve Flash Endurance in Data Centers},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3470007},
doi = {10.1145/3470007},
abstract = {The demand for high performance I/O in Storage-as-a-Service (SaaS) is increasing day by day. To address this demand, NAND Flash-based Solid-state Drives (SSDs) are commonly used in data centers as cache- or top-tiers in the storage rack ascribe to their superior performance compared to traditional hard disk drives (HDDs). Meanwhile, with the capital expenditure of SSDs declining and the storage capacity of SSDs increasing, all-flash data centers are evolving to serve cloud services better than SSD-HDD hybrid data centers. During this transition, the biggest challenge is how to reduce the Write Amplification Factor (WAF) as well as to improve the endurance of SSD since this device has a limited program/erase cycles. A specified case is that storing data with different lifetimes (i.e., I/O streams with similar temporal fetching patterns such as reaccess frequency) in one single SSD can cause high WAF, reduce the endurance, and downgrade the performance of SSDs. Motivated by this, multi-stream SSDs have been developed to enable data with a different lifetime to be stored in different SSD regions. The logic behind this is to reduce the internal movement of data—when garbage collection is triggered, there are high chances of having data blocks with either all the pages being invalid or valid. However, the limitation of this technology is that the system needs to manually assign the same streamID to data with a similar lifetime. Unfortunately, when data arrives, it is not known how important this data is and how long this data will stay unmodified. Moreover, according to our observation, with different definitions of a lifetime (i.e., different calculation formulas based on selected features previously exhibited by data, such as sequentiality, and frequency), streamID identification may have varying impacts on the final WAF of multi-stream SSDs. Thus, in this article, we first develop a portable and adaptable framework to study the impacts of different workload features and their combinations on write amplification. We then propose a feature-based stream identification approach, which automatically co-relates the measurable workload attributes (such as I/O size, I/O rate, and so on.) with high-level workload features (such as frequency, sequentiality, and so on.) and determines a right combination of workload features for assigning streamIDs. Finally, we develop an adaptable stream assignment technique to assign streamID for changing workloads dynamically. Our evaluation results show that our automation approach of stream detection and separation can effectively reduce the WAF by using appropriate features for stream assignment with minimal implementation overhead.},
journal = {ACM Trans. Storage},
month = {apr},
articleno = {17},
numpages = {29},
keywords = {coherency, write amplification factor, NAND flash endurance, I/O workload characterization, multi-streaming, Solid state drives, I/O stream detection}
}

@inproceedings{10.1145/3576842.3582376,
author = {Nouma, Saif E. and Yavuz, Attila A.},
title = {Practical Cryptographic Forensic Tools for Lightweight Internet of Things and Cold Storage Systems},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3582376},
doi = {10.1145/3576842.3582376},
abstract = {Internet of Things (IoT) and Storage-as-a-Service (STaaS) continuum permit cost-effective maintenance of security-sensitive information collected by IoT devices over cloud systems. It is necessary to guarantee the security of sensitive data in IoT-STaaS applications. Especially, log entries trace critical events in computer systems and play a vital role in the trustworthiness of IoT-STaaS. An ideal log protection tool must be scalable and lightweight for vast quantities of resource-limited IoT devices while permitting efficient and public verification at STaaS. However, the existing cryptographic logging schemes either incur significant computation/signature overhead to the logger or extreme storage and verification costs to the cloud. There is a critical need for a cryptographic forensic log tool that respects the efficiency requirements of the IoT-STaaS continuum. In this paper, we created novel digital signatures for logs called Optimal Signatures for secure Logging (), which are the first (to the best of our knowledge) to offer both small-constant signature and public key sizes with near-optimal signing and batch verification via various granularities. We introduce new design features such as one-time randomness management, flexible aggregation along with various optimizations to attain these seemingly conflicting properties simultaneously. Our experiments show that &nbsp;offers 50 \texttimes{} faster verification (for 235 entries) than the most compact alternative with equal signature sizes, while also being several magnitudes of more compact than its most logger efficient counterparts. These properties make &nbsp;an ideal choice for the IoT-STaaS, wherein lightweight logging and efficient batch verification of massive-size logs are vital for the IoT edge and cold storage servers, respectively.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {340–353},
numpages = {14},
keywords = {secure logs, cold storage, digital signatures, Authentication},
location = {<conf-loc>, <city>San Antonio</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {IoTDI '23}
}

@inproceedings{10.1145/3546000.3546015,
author = {Qlu, Ye},
title = {Secure Mechanism of Intelligent Urban Railway Cloud Platform Based on Zero-Trust Security Architecture},
year = {2022},
isbn = {9781450396295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546000.3546015},
doi = {10.1145/3546000.3546015},
abstract = {Aiming to strengthen the stability of operation and maintenance of the urban rail transit network cloud platform at this stage, it is emerging to solve the security mechanism of the intelligent urban railway cloud platform. In this paper, we proposed a zero-trust network security solution for the rail transit system network construction. First, we built a zero-trust network construction for smart city rail transit at the architecture level, it can break the phenomenon of information security silo of rail transit line platform and minimize the system security risk based on a zero-trust network. Next, we focus on building a cloud security brain for urban rail transit networks and proposed the self-learning trust algorithm for a zero-trust network. Specifically, we illustrated the modified network model and constructed a dynamic updating user trust profile as the trustworthy access list. The parameters of the self-learning trust algorithm consist of the state, available chain road bandwidth, waiting for queue state of network traffic, linkage actions, and so on. We adopted a dynamic self-learning strategy for adjusting mitigation policy, the learning step predicted the state of the predetermined congestion and selected the rich links for execution. Finally, experiments show the efficiency of our secure mechanism of railway cloud platform based on zero-trust security architecture.},
booktitle = {Proceedings of the 6th International Conference on High Performance Compilation, Computing and Communications},
pages = {99–105},
numpages = {7},
keywords = {cloud platform, Zero-trust security mechanism, simulation analysis},
location = {Jilin, China},
series = {HP3C '22}
}

@inproceedings{10.1145/3607828.3617794,
author = {Abdur Rahman, Lubnaa and Papathanail, Ioannis and Brigato, Lorenzo and Mougiakakou, Stavroula},
title = {A Comparative Analysis of Sensor-, Geometry-, and Neural-Based Methods for Food Volume Estimation},
year = {2023},
isbn = {9798400702846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607828.3617794},
doi = {10.1145/3607828.3617794},
abstract = {With the rapid advancements in artificial intelligence and computer vision within health and nutrition fields, image-based automatic dietary assessment is gaining popularity. This automation involves food segmentation, recognition, volume estimation, and estimation of nutritional content. While considerable progress has been made in food segmentation and recognition, accurate volume estimation remains challenging. Measuring food volume is crucial in many fields, even thought this is difficult to automate precisely. This is hampering progress, and is leading to continued reliance on time-consuming traditional methods, such as manual computation of food volume through water displacement. The manuscript presents a comparative analysis of sensor-, geometry-, and neural-based methods for computing food volume. We have performed multiple experiments using 20 meal images captured under different settings, with reliable measurements of ground-truth volume obtained by capturing 360-degree views of the food items and computing their volumes in a 3D space. An extensive analysis of our results then serves to identify the strengths and limitations of each approach, and offers valuable insights for selecting the most suitable method in specific settings. Moreover, we have made the collected data (including RGB images, ground-truth point clouds, volumes, etc.) open-source. We intend this as a contribution to the research community and to address the scarcity of food datasets with depth-related information.},
booktitle = {Proceedings of the 8th International Workshop on Multimedia Assisted Dietary Management},
pages = {21–29},
numpages = {9},
keywords = {image-based food volume estimation, multimedia systems, computer vision, sensor-based volume estimation, stereo vision, depth prediction, artificial intelligence},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MADiMa '23}
}

@inproceedings{10.1145/3508398.3511503,
author = {Asvadishirehjini, Aref and Kantarcioglu, Murat and Malin, Bradley},
title = {GINN: Fast GPU-TEE Based Integrity for Neural Network Training},
year = {2022},
isbn = {9781450392204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508398.3511503},
doi = {10.1145/3508398.3511503},
abstract = {Machine learning models based on Deep Neural Networks (DNNs) are increasingly deployed in a wide variety of applications, ranging from self-driving cars to COVID-19 diagnosis. To support the computational power necessary to train a DNN, cloud environments with dedicated Graphical Processing Unit (GPU) hardware support have emerged as critical infrastructure. However, there are many integrity challenges associated with outsourcing the computation to use GPU power, due to its inherent lack of safeguards to ensure computational integrity. Various approaches have been developed to address these challenges, building on trusted execution environments (TEE). Yet, no existing approach scales up to support realistic integrity-preserving DNN model training for heavy workloads (e.g., deep architectures and millions of training examples) without sustaining a significant performance hit. To mitigate the running time difference between pure TEE (i.e., full integrity) and pure GPU (i.e., no integrity) , we combine random verification of selected computation steps with systematic adjustments of DNN hyperparameters (e.g., a narrow gradient clipping range), which limits the attacker's ability to shift the model parameters arbitrarily. Experimental analysis shows that the new approach can achieve a 2X to 20X performance improvement over a pure TEE-based solution while guaranteeing an extremely high probability of integrity (e.g., 0.999) with respect to state-of-the-art DNN backdoor attacks.},
booktitle = {Proceedings of the Twelfth ACM Conference on Data and Application Security and Privacy},
pages = {4–15},
numpages = {12},
keywords = {intel sgx, trusted exexution environments, deep learning, integrity preserving deep learning training},
location = {Baltimore, MD, USA},
series = {CODASPY '22}
}

@ARTICLE{10251860,
  author={Al-Debagy, O. and Martinek, P.},
  journal={Journal of Web Engineering}, 
  title={A Metrics Framework for Evaluating Microservices Architecture Designs}, 
  year={2020},
  volume={19},
  number={3–4},
  pages={341-370},
  abstract={Microservices are becoming a more popular software architecture among companies and developers. Therefore, there is a need to develop methods for quantifying the process of measuring the quality of microservices design. This paper has created a novel set of metrics for microservices architecture applications. The proposed metrics are the Service Granularity Metric “SGM”, the Lack of Cohesion Metric “LCOM”, and the Number of Operations “NOO”. The proposed metrics measure the granularity, cohesion, and complexity of individual microservices through analyzing the application programming interface “API”. Using these metrics, it is possible to evaluate the overall quality of the design of microservices applications. The proposed metrics were measured on 5 applications with different sizes and business cases. This research found that the value for the SGM metric needs to be between 0.2 and 0.6. Besides, the value of LCOM metric for a microservice needs to be between 0 and 0.8 with less than ten operations per microservice. These findings can be applied in the decomposition process of monolithic applications as well.},
  keywords={},
  doi={10.13052/jwe1540-9589.19341},
  ISSN={1544-5976},
  month={June},}

@INPROCEEDINGS{9101318,
  author={Santos, Nuno and Rito Silva, António},
  booktitle={2020 IEEE International Conference on Software Architecture (ICSA)}, 
  title={A Complexity Metric for Microservices Architecture Migration}, 
  year={2020},
  volume={},
  number={},
  pages={169-178},
  abstract={Monolith applications tend to be difficult to deploy, upgrade, maintain, and understand. Microservices, on the other hand, have the advantages of being independently developed, tested, deployed, scaled and, more importantly, easier to change and maintain. This paper addresses the problem of migrating a monolith to a microservices architecture. Therefore, we address two research questions: (1) Can we define the cost of decomposition in terms of the effort to redesign a functionality, which is implemented in the monolith as an ACID transaction, into several distributed transactions? (2) Considering several similarity measures between domain entities, which provide a better decomposition when they are compared using the proposed complexity metric? To answer the first research question, we propose a complexity metric, for each functionality of the monolith application, that measures the impact of relaxing the functionality consistency on the architecture redesign and implementation. Regarding the second research question, we experiment with four similarity measures, each based on a different type of information collected from monolith functionality implementation. We evaluated our approach with three monolith systems and compared our complexity metric against industry metrics of cohesion and coupling. We also evaluated the different similarity measures in terms of the complexity of the decomposition they produce. We were able to correctly correlate the complexity metric with other metrics of cohesion and coupling defined in other research and we conclude that no single combination of similarity measures outperforms the other, which is confirmed by the existing research. Additionally, we conclude that the approach can help on an incremental migration to microservices, which, actually, is the strategy proposed by the industry experts.},
  keywords={},
  doi={10.1109/ICSA47634.2020.00024},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9012140,
  author={Raj, Vinay and Ravichandra, S.},
  booktitle={2018 3rd IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)}, 
  title={Microservices: A perfect SOA based solution for Enterprise Applications compared to Web Services}, 
  year={2018},
  volume={},
  number={},
  pages={1531-1536},
  abstract={The Software Engineering community has defined different types of architectures to build applications. One among them is Service Oriented Architecture(SOA) which has created significant impact the way software applications are built. There are many implementations of SOA like Web Services, REST services etc. But Web Services and REST services do not fully follow all the principles of SOA. Microservices as an architectural style recently emerged from SOA by which we can develop business requirements with loosely coupled, self deploying and scalable services. Microservices have gained more popularity in application development as they are easy to understand, scale and deploy. In this paper we discuss principles of SOA, major drawbacks of web services and benefits of Microservices over SOA based web services. We have highlighted the importance of Microservices in software development. This paper gives information for architects as to why choose Microservices architecture over web services. We have also discussed metrics used for calculating Coupling between services and we evaluated by considering a smart payment application for ecommerce which is built using both the styles. We observed that Microservices architectural style has less coupling between services compared to Web Service style based on the metric values of the application.},
  keywords={},
  doi={10.1109/RTEICT42901.2018.9012140},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9095617,
  author={Rosa, Thatiane de Oliveira and Goldman, Alfredo and Guerra, Eduardo Martins},
  booktitle={2020 IEEE International Conference on Software Architecture Companion (ICSA-C)}, 
  title={How ‘micro’ are your services?}, 
  year={2020},
  volume={},
  number={},
  pages={75-78},
  abstract={Microservice is an architectural style that proposes that a complex system should be developed from small and independent services that work together. There is not a welldefined boundary about when a software architecture can be considered based on microservices or not. Because of that, defining microservices context and infrastructure is challenging, especially to characterize aspects related to microservice size, data consistency, and microservices coupling. Thus, it is crucial to understand the microservices-based software characteristics, to comprehend the impact of some evolutions on architecture, and evaluate how much a particular architecture fits the microservices architectural style. Therefore, based on bibliographic research and case studies conducted in academical and industrial environments, we aim to propose a model to characterize the architecture structure based on the main guidelines of the microservice architectural style. This model introduces dimensions that measure characteristics based on modules size, coupling to data sources, and service collaboration. This study should facilitate the mapping, measurement, and monitoring of different impacts generated in the software architecture from increments and refactoring performed. This work is on the initial development stage and as a result, we expected that the model supports architectural decisions that consider different quality attributes to achieve the right balance between service independence and collaboration for a given system.},
  keywords={},
  doi={10.1109/ICSA-C50368.2020.00023},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9672417,
  author={Yilmaz, Rahime and Buzluca, Feza},
  booktitle={2021 2nd International Informatics and Software Engineering Conference (IISEC)}, 
  title={A Fuzzy Quality Model to Measure the Maintainability of Microservice Architectures}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Microservice architecture (MSA) is a type of software and systems architecture that is based on the modularization principle. It proposes designing systems employing small-scaled, loosely coupled, and independently deployable microservices. There are several benefits of microservices architecture in terms of maintainability, scalability, and productivity which have led to rise in its popularity. Even though there are several studies about development in MSA, the studies on the quality of the microservice-based systems are limited. In this study, we propose a quality model based on fuzzy logic to measure and assess quality attributes of systems in MSA that can be used by software architects, developers, and project managers. We focus on maintainability of microservices because it is one of the most important quality attributes of software systems. We identified sub-characteristics and properties of microservices that affect maintainability, and constructed a hierarchical quality model based on ISO/IEC 250xy standard SQuaRE (System and Software Quality Requirements and Evaluation). Our fuzzy model measures maintainability of microservices in three levels, i.e., low, medium, and high. We provided a basis for the development and application of quality models in industrial practice as well as a basis for further extension. To demonstrate and evaluate our methodology, we used open-source applications designed in MSA. The results show that our method can assess maintainability of microservices realistically.},
  keywords={},
  doi={10.1109/IISEC54230.2021.9672417},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9095641,
  author={Avritzer, Alberto},
  booktitle={2020 IEEE International Conference on Software Architecture Companion (ICSA-C)}, 
  title={Challenges and Approaches for the Assessment of Micro-Service Architecture Deployment Alternatives in DevOps : A tutorial presented at ICSA 2020}, 
  year={2020},
  volume={},
  number={},
  pages={1-2},
  abstract={The goal of this tutorial is to provide an overview of challenges and approaches for architecture/dependability assessment in the context of DevOps and microservices. Specifically, we present approaches that employ operational data obtained from production-level application performance management (APM) tools, giving access to operational workload profiles, architectural information, failure models, and security intrusions. We use this data to automatically create and conFigure architecture assessments based on models, load tests, and resilience benchmarks. The focus of this tutorial is on approaches that employ production usage, because these approaches provide more accurate recommendations for microservice architecture dependability assessment than approaches that do not consider production usage.We present an overview of (1) the state-of-the-art approaches for obtaining operational data from production systems using APM tools, (2) the challenges of dependability for DevOps and microservices, (3) selected approaches based on operational data to assess dependability. The architecture assessment focus of this tutorial is on scalability, resilience, survivability, and security. Particularly, we present a demo of the automated approach for the evaluation of a domain-based scalability and security metric assessment that is based on the microservice architecture ability to satisfy the performance requirement under load and/or intrusions. We illustrate the approach by presenting experimental results using a benchmark microservice architecture.},
  keywords={},
  doi={10.1109/ICSA-C50368.2020.00007},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9335808,
  author={Levin, Joshua and Benson, Theophilus A.},
  booktitle={2020 IEEE 9th International Conference on Cloud Networking (CloudNet)}, 
  title={ViperProbe: Rethinking Microservice Observability with eBPF}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Recent shifts to microservice-based architectures and the supporting servicemesh radically disrupt the landscape of performance-oriented management tasks. While the adoption of frameworks like Istio and Kubernetes ease the management and organization of such systems, they do not themselves provide strong observability. Microservice observability requires diverse, highly specialized, and often adaptive, metrics and algorithms to monitor both the health of individual services and the larger application. However, modern metrics collection frameworks are relatively static and rigid. We introduce ViperProbe, an eBPF-based microservices collection framework that provides (1) dynamic sampling and (2) collection of deep, diverse, and precise system metrics. Viper-Probe builds on the observation that the adoption of a common set of design patterns, e.g., servicemesh, enables offline analysis. By examining the performance profile of these patterns before deploying on production, ViperProbe can effectively reduce the set of collected metrics, thereby improving the efficiency and effectiveness of those metrics. To the best of our knowledge, ViperProbe is the first scalable eBPF-based dynamic and adaptive microservices metrics collection framework. Our results show ViperProbe has limited overhead, while significantly more effective for traditional management tasks, e.g., horizontal autoscaling.},
  keywords={},
  doi={10.1109/CloudNet51028.2020.9335808},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8466390,
  author={Perera, K. J. P. G. and Perera, I.},
  booktitle={2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)}, 
  title={TheArchitect: A Serverless-Microservices Based High-level Architecture Generation Tool}, 
  year={2018},
  volume={},
  number={},
  pages={204-210},
  abstract={Software is ubiquitous in today's systems and business operations. Most importantly the architecture of a software system determines its quality and longevity, because the development work related to the software system will be carried out to be in line with its architecture design. Hence, it's highly important to structure the high-level software architecture accordingly to deliver the expected customer requirements while accounting for quality measures such as scalability, high availability and high performance. We propose TheArchitect, a serverless-microservices based high-level architecture generation tool, which will auto generate serverless-microservices based high-level architecture for a given business application, preserving the highlighted quality measures providing a tool based support for the software architect with respect to designing the high-level architecture. TheArchitect will provide any software developer to generate a proper architecture minimizing the involvement of an experienced software architect. Furthermore, the positives that microservices and serverless technologies has brought to the world of software engineering has made the software engineering community shift from the era of building large monolith applications containing overly complex designs, to microservices and serverless based technologies. Hence TheArchitect focuses on generating best fitted microservices and serverless based high-level architecture for a given application.},
  keywords={},
  doi={10.1109/ICIS.2018.8466390},
  ISSN={},
  month={June},}

@INPROCEEDINGS{8544423,
  author={Perera, K. J. P. G. and Perera, I.},
  booktitle={2018 IEEE International Systems Engineering Symposium (ISSE)}, 
  title={A Rule-based System for Automated Generation of Serverless-Microservices Architecture}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Software being ubiquitous in today's systems and business operations, it's highly important to structure the high-level architecture of a software application accordingly to deliver the expected customer requirements while accounting for quality measures such as scalability, high availability and high performance. We propose The Architect, a rule-based system for serverless-microservices based high-level architecture generation. In the process of auto generating serverless-microservices high-level architecture, TheArchitect will preserve the highlighted quality measures. It will also provide a tool based support for the high-level architecture designing process of the software architect. Any software developer will be able to use TheArchitect to generate a proper architecture minimizing the involvement of a software architect. Furthermore, the positives of microservices and serverless technologies have made a significant impact on the software engineering community in terms of shifting from the era of building large monolith applications containing overly complex designs, to microservices and serverless based technologies. Hence The Architect focuses on generating best fitted microservices and serverless based high-level architecture for a given application.},
  keywords={},
  doi={10.1109/SysEng.2018.8544423},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{7965739,
  author={Asik, Tugrul and Selcuk, Yunus Emre},
  booktitle={2017 IEEE 15th International Conference on Software Engineering Research, Management and Applications (SERA)}, 
  title={Policy enforcement upon software based on microservice architecture}, 
  year={2017},
  volume={},
  number={},
  pages={283-287},
  abstract={Microservice is an architectural style that has recently started gaining popularity to become a new architectural phenomenon. Microservice architecture provides new opportunities to deploy scalable, language free and dynamically adjustable applications. This type of applications consist of hundreds or more of service instances. So that, management, monitoring, refactoring and testing of applications are more complex than monolithic applications. Therefore, some metrics and policies for measuring the quality of an application which is based on microservice architecture is needed. Moreover, automated tools are needed to carry out those tasks and enforce those policies. This work represents such metrics and policies. Additionally, an automated tool is implemented for automatic analysis of those metrics and policies upon software.},
  keywords={},
  doi={10.1109/SERA.2017.7965739},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9527003,
  author={Weng, Tianjun and Yang, Wanqi and Yu, Guangba and Chen, Pengfei and Cui, Jieqi and Zhang, Chuanfu},
  booktitle={2021 IEEE/ACM International Workshop on Cloud Intelligence (CloudIntelligence)}, 
  title={Kmon: An In-kernel Transparent Monitoring System for Microservice Systems with eBPF}, 
  year={2021},
  volume={},
  number={},
  pages={25-30},
  abstract={Currently, the architecture of software systems is shifting from “monolith” to “microservice” which is an important enabling technology of cloud native systems. Since the advantages of microservice in agility, efficiency, and scaling, it has become the most popular architecture in the industry. However, as the increase of microservice complexity and scale, it becomes challenging to monitor such a large number of microservices. Traditional monitoring techniques such as end-to-end tracing cannot well fit microservice environment, because they need code instrumentation with great effort. Moreover, they cannot explore the fine-grained internal states of microservice instances. To tackle this problem, we propose Kmon, which is an In-kernel transparent monitoring system for microservice systems with extended Berkeley Packet Filter (eBPF). Kmon can provide multiple kinds of run-time information of micrservices such as latency, topology, performance metrics with a low overhead.},
  keywords={},
  doi={10.1109/CloudIntelligence52565.2021.00014},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9359175,
  author={Cui, Jieqi and Chen, Pengfei and Yu, Guangba},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={A Learning-based Dynamic Load Balancing Approach for Microservice Systems in Multi-cloud Environment}, 
  year={2020},
  volume={},
  number={},
  pages={334-341},
  abstract={Multi-cloud environment has become common since companies manage to prevent cloud vendor lock-in for security and cost concerns. Meanwhile, the microservice architecture is often considered for its flexibility. Combining multi-cloud with microservice, the problem of routing requests among all possible microservice instances in multi-cloud environment arises. This paper presents a learning-based approach to route requests in order to balance the load. In our approach, the performance of microservice is modeled explicitly through machine learning models. The model can derive the response time from request volume, route decision, and other cloud metrics. Then the balanced route decision is obtained from optimizing the model with Bayesian Optimization. With this approach, the request route decision can adjust to dynamic runtime metrics instead of remaining static for all different circumstances. Explicit performance modeling avoids searching on an actual microservice system which is time-consuming. Experiments show that our approach reduces average response time by 10% at least.},
  keywords={},
  doi={10.1109/ICPADS51040.2020.00052},
  ISSN={2690-5965},
  month={Dec},}

@INPROCEEDINGS{9101217,
  author={Zhang, Yukun and Liu, Bo and Dai, Liyun and Chen, Kang and Cao, Xuelian},
  booktitle={2020 IEEE International Conference on Software Architecture (ICSA)}, 
  title={Automated Microservice Identification in Legacy Systems with Functional and Non-Functional Metrics}, 
  year={2020},
  volume={},
  number={},
  pages={135-145},
  abstract={Since microservice has merged as a promising architectural style with advantages in maintainability, scalability, evolvability, etc., increasing companies choose to restructure their legacy monolithic software systems as the microservice architecture. However, it is quite a challenge to properly partitioning the systems into suitable parts as microservices. Most approaches perform microservices identification from a function-splitting perspective and with sufficient legacy software artifacts. That may be not realistic in industrial practices and possibly results in generating unexpected microservices. To address this, we proposed an automated microservice identification (AMI) approach that extracts microservices from the execution and performance logs without providing documentation, models or source codes, while taking both functional and non-functional metrics into considerations. Our work firstly collects logs from the executable legacy system. Then, controller objects (COs) are identified as the key objects to converge strongly related subordinate objects (SOs). Subsequently, the relation between each pair of CO and SO is evaluated by a relation matrix from both the functional and non-functional perspective. We ultimately cluster classes(objects) into the microservices by optimizing the multi-objective of high-cohesion-low-coupling and load balance. The usefulness of the proposed approach is illustrated by applying to a case study.},
  keywords={},
  doi={10.1109/ICSA47634.2020.00021},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9110450,
  author={Lin, Thomas and Leon-Garcia, Alberto},
  booktitle={NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium}, 
  title={Towards a Client-Centric QoS Auto-Scaling System}, 
  year={2020},
  volume={},
  number={},
  pages={1-9},
  abstract={Many modern day cloud services are composites of multiple smaller services working correctly together. This design has become increasingly prevalent due to the rise of the microservices application architecture, as well as service chaining in Network Function Virtualization (NFV). Future composite applications and services will be deployed on multi-tier clouds where their constituent microservices may be geographically spread over different regions. To optimize the delivery of such composites, the constituent microservices must be placed in locations where their clients, which may be other microservices, are able to meet certain QoS constraints. We propose an architecture and present a prototype system for incorporating network metrics into the auto-scaling and scheduling decisions of cloud management systems. Given a service with QoS constraints, our system monitors the network metrics (e.g. latency and bandwidth) of their clients. If a particular client is unable to receive the required latency or bandwidth of the service, our system auto-scales the service and strategically places the new instance(s) in a location capable of meeting the service quality, and re-directs traffic to the new instance.},
  keywords={},
  doi={10.1109/NOMS47738.2020.9110450},
  ISSN={2374-9709},
  month={April},}

@INPROCEEDINGS{9101266,
  author={Selmadji, Anfel and Seriai, Abdelhak-Djamel and Bouziane, Hinde Lilia and Oumarou Mahamane, Rahina and Zaragoza, Pascal and Dony, Christophe},
  booktitle={2020 IEEE International Conference on Software Architecture (ICSA)}, 
  title={From Monolithic Architecture Style to Microservice one Based on a Semi-Automatic Approach}, 
  year={2020},
  volume={},
  number={},
  pages={157-168},
  abstract={Due to its tremendous advantages, microservice architectural style has become an essential element for the development of applications deployed on the cloud and for those adopting the DevOps practices. Nevertheless, while microservices can be used to develop new applications, there are monolithic ones, that are not well adapted neither to the cloud nor to DevOps. Migrating these applications towards microservices appears as a solution to adapt them to both. In this context, we propose an approach aiming to achieve this objective by focusing on the step of microservices identification. The proposed identification, in this paper, is based on an analysis of the relationships between source code elements, their relationships with the persistent data manipulated in this code and finally the knowledge, often partial, of the architect concerning the system to migrate. A function that measures the quality of a microservice based on its ability to provide consistent service and its interdependence with others microservice in the resulting architecture was defined. Moreover, the architect recommendations are used, when available, to guide the identification process. The conducted experiment shows the relevance of the obtained microservices by our approach.},
  keywords={},
  doi={10.1109/ICSA47634.2020.00023},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9237055,
  author={Shiraishi, Takashi and Noro, Masaaki and Kondo, Reiko and Takano, Yosuke and Oguchi, Naoki},
  booktitle={2020 21st Asia-Pacific Network Operations and Management Symposium (APNOMS)}, 
  title={Real-time Monitoring System for Container Networks in the Era of Microservices}, 
  year={2020},
  volume={},
  number={},
  pages={161-166},
  abstract={Large-scale web services are increasingly adopting the microservice architecture that mainly utilizes container technologies. Microservices are operated on complex configured infrastructures, such as containers, virtual machines, and physical machines. To ensure service quality of microservices, it is important to monitor not only the quality of services but also the quality of the infrastructures utilized by the services. Therefore, the metrics of the infrastructure related with the services should be traced. An extended Berkeley Packet Filter (eBPF) is a relatively new Linux's function, which is effectively used as a sensor of container-network metrics. There are two key challenges in realizing the service-linked monitoring system. One challenge is making the full-stack topology between microservices, containers, and machines visible to set the sensor related with the services. Another challenge is dynamic sensor management that can relocate the sensor quickly after the topology's change. In this paper, we propose a real-time monitoring system that creates a full-stack topology and relocates the sensor in conjunction with events from a container orchestrator. The system enables a dynamic deployment of the sensors related with the monitored services.},
  keywords={},
  doi={10.23919/APNOMS50412.2020.9237055},
  ISSN={2576-8565},
  month={Sep.},}

@INPROCEEDINGS{9285951,
  author={Rossi, Fabiana and Cardellini, Valeria and Presti, Francesco Lo},
  booktitle={2020 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)}, 
  title={Self-adaptive Threshold-based Policy for Microservices Elasticity}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={The microservice architecture structures an application as a collection of loosely coupled and distributed services. Since application workloads usually change over time, the number of replicas per microservice should be accordingly scaled at run-time. The most widely adopted scaling policy relies on statically defined thresholds, expressed in terms of system-oriented metrics. This policy might not be well-suited to scale multi-component and latency-sensitive applications, which express requirements in terms of response time. In this paper, we present a two-layered hierarchical solution for controlling the elasticity of microservice-based applications. The higher-level controller estimates the microservice contribution to the application performance, and informs the lower-level components. The latter accordingly scale the single microservices using a dynamic threshold-based policy. So, we propose MB Threshold and QL Threshold, two policies that employ respectively model-based and model-free reinforcement learning approaches to learn threshold update strategies. These policies can compute different thresholds for the different application components, according to the desired deployment objectives. A wide set of simulation results shows the benefits and flexibility of the proposed solution, emphasizing the advantages of using dynamic thresholds over the most adopted policy that uses static thresholds.},
  keywords={},
  doi={10.1109/MASCOTS50786.2020.9285951},
  ISSN={2375-0227},
  month={Nov},}

@INPROCEEDINGS{9590257,
  author={Agarwal, Shivali and Sinha, Raunak and Sridhara, Giriprasad and Das, Pratap and Desai, Utkarsh and Tamilselvam, Srikanth and Singhee, Amith and Nakamuro, Hiroaki},
  booktitle={2021 IEEE International Conference on Web Services (ICWS)}, 
  title={Monolith to Microservice Candidates using Business Functionality Inference}, 
  year={2021},
  volume={},
  number={},
  pages={758-763},
  abstract={In this paper, we propose a novel approach for monolith decomposition, that maps the implementation structure of a monolith application to a functional structure that in turn can be mapped to business functionality. First, we infer the classes in the monolith application that are distinctively representative of the business functionality in the application domain. This is done using formal concept analysis on statically determined code flow structures in a completely automated manner. Then, we apply a clustering technique, guided by the inferred representatives, on the classes belonging to the monolith to group them into different types of partitions, mainly: 1) functional groups representing microservice candidates, 2) a utility class group, and 3) a group of classes that require significant refactoring to enable a clean microservice architecture. This results in microservice candidates that are naturally aligned with the different business functions exposed by the application. A detailed evaluation on four publicly available applications show that our approach is able to determine better quality microservice candidates when compared to other existing state of the art techniques. We also conclusively show that clustering quality metrics like modularity are not reliable indicators of microservice candidate goodness.},
  keywords={},
  doi={10.1109/ICWS53863.2021.00104},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9460467,
  author={Ramesh, Srinivasan and Malony, Allen D. and Carns, Philip and Ross, Robert B. and Dorier, Matthieu and Soumagne, Jerome and Snyder, Shane},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={SYMBIOSYS: A Methodology for Performance Analysis of Composable HPC Data Services}, 
  year={2021},
  volume={},
  number={},
  pages={35-45},
  abstract={Microservices are a powerful new way of building, customizing, and deploying distributed services owing to their flexibility and maintainability. Several large-scale distributed platforms have emerged to serve the growing needs of data-centric workloads and services in commercial computing. Concurrently, high-performance computing (HPC) systems and software are rapidly evolving to meet the demands of diversified applications and heterogeneity. The interplay of hardware factors, software configuration parameters, and the flexibility offered with a microservice architecture makes it nontrivial to estimate the optimal service instantiation for a given application workload. Further, this problem is exacerbated when considering that these services operate in a dynamic and heterogeneous HPC environment. An optimally integrated service can be vastly more performant than a haphazardly integrated one. Existing performance tools for HPC either fail to understand the request-response model of communication inherent to microservices or they operate within a narrow scope, limiting the insight that can be gleaned from employing them in isolation.We propose a methodology for integrated performance analysis of HPC microservices frameworks and applications called SYMBIOSYS. We describe its design and implementation within the context of the Mochi framework. This integration is achieved by combining distributed callpath profiling and tracing with a performance data exchange strategy that collects fine-grained, low-level metrics from the RPC communication library and network layers. The result is a portable, low-overhead performance analysis setup that provides a holistic profile of the dependencies among microservices and how they interact with the Mochi RPC software stack. Using HEPnOS, a production-quality Mochi data service, we demonstrate the low-overhead operation of SYMBIOSYS at scale and use it to identify the root causes of poorly performing service configurations.},
  keywords={},
  doi={10.1109/IPDPS49936.2021.00013},
  ISSN={1530-2075},
  month={May},}

@INPROCEEDINGS{9482273,
  author={Tang, Ming and Xia, Fei and Zou, Haodong and Hu, Youjun and Liu, Jun and Liu, Sai},
  booktitle={2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)}, 
  title={Cloud platform load balancing mechanism for microservice architecture}, 
  year={2021},
  volume={4},
  number={},
  pages={435-439},
  abstract={In response to the increase in request response latency under the microservice architecture, from the perspective of cloud platform load balancing, the average request latency and host load on the microservice chain are used as metrics to formalize the latency and problem environment. A request load balancing algorithm perceived by the microservice chain is proposed as the load balancing strategy of the load balancer. Simulation experiments prove that the algorithm in this paper can effectively reduce request latency in a complex microservice chain environment, and it can also maintain relatively good performance in an environment where instances are unevenly distributed, and for workloads between hosts.},
  keywords={},
  doi={10.1109/IMCEC51613.2021.9482273},
  ISSN={2693-2776},
  month={June},}

@INPROCEEDINGS{8258201,
  author={Alipour, Hanieh and Liu, Yan},
  booktitle={2017 IEEE International Conference on Big Data (Big Data)}, 
  title={Online machine learning for cloud resource provisioning of microservice backend systems}, 
  year={2017},
  volume={},
  number={},
  pages={2433-2441},
  abstract={Microservices are bundled and generating traffic on the backend systems that need to scale on demand. When microservices generate variant and unexpected, the challenge is to classify the workload on the backend systems and adjust the scaling policy to reflect the resource demand timely and accurately. In this paper, we propose a microservice architecture that encapsulates functions of monitoring metrics and learning workload pattern. Then this service architecture is used to predict the future workload for decision making on resource provisioning. We deploy two machine learning algorithms and predict the resource demand of the backend systems of microservices emulated by a Netflix workload benchmark application. This service architecture presents an integrated solution of implementing self-managing cloud data services under variant workload.},
  keywords={},
  doi={10.1109/BigData.2017.8258201},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9492620,
  author={Behrad, Shanay and Espes, David and Bertin, Philippe and Phan, Cao-Thanh},
  booktitle={2021 IEEE 7th International Conference on Network Softwarization (NetSoft)}, 
  title={Impacts of Service Decomposition Models on Security Attributes: A Case Study with 5G Network Repository Function}, 
  year={2021},
  volume={},
  number={},
  pages={470-476},
  abstract={Microservices-based architectures gain more and more attention in industry and academia due to their tremendous advantages such as providing resiliency, scalability, composability, etc. To benefit from these advantages, a proper architectural design is very important. The decomposition model of services into microservices and the granularity of these microservices affect the different aspects of the system such as flexibility, maintainability, performance, and security. An inappropriate service decomposition into microservices (improper granularity) may increase the attack surface of the system and lower its security level. In this paper, first, we study the probability of compromising services before and after decomposition. Then we formulate the impacts of possible service decomposition models on confidentiality, integrity, and availability attributes of the system. To do so, we provide equations for measuring confidentiality, integrity, and availability risks of the decomposed services in the system. It is also shown that the number of entry points to the decomposed services and the size of the microservices affect the security attributes of the system. As a use case, we propose three different service decomposition models for the 5G NRF (Network Repository Function) and calculate the impacts of these decomposition models on the confidentiality, integrity, and availability of the system using the provided equations.},
  keywords={},
  doi={10.1109/NetSoft51509.2021.9492620},
  ISSN={2693-9789},
  month={June},}

@INPROCEEDINGS{8940402,
  author={Fernandes Mioto de Oliveira dos Santos, Eduardo and Lima Werner, Claudia Maria},
  booktitle={2019 International Conference on Information Systems and Software Technologies (ICI2ST)}, 
  title={A Survey on Microservices Criticality Attributes on Established Architectures}, 
  year={2019},
  volume={},
  number={},
  pages={149-155},
  abstract={The microservice oriented software architecture considers the delegation of responsibilities by separate components, thus creating a set of interconnected but independent services. Information about the most critical microservices is relevant to software architects and other decision-makers, thus guiding the maintenance and evolution of architecture in a more assertive and guided way. This paper aims to observe the need for a method to measure criticality in a microservice oriented architecture, motivated by this purpose, during August 2019, a survey with twenty experienced participants from the industry and academia was conducted, where the lack of a grounded method to measure the criticality on established architectures was observed.},
  keywords={},
  doi={10.1109/ICI2ST.2019.00028},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9582573,
  author={de Toledo, Saulo S. and Martini, Antonio and Sjøberg, Dag I.K. and Przybyszewska, Agata and Frandsen, Johannes Skov},
  booktitle={2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={Reducing Incidents in Microservices by Repaying Architectural Technical Debt}, 
  year={2021},
  volume={},
  number={},
  pages={196-205},
  abstract={Architectural technical debt (ATD) may create a substantial extra effort in software development, which is called interest. There is little evidence about whether repaying ATD in microservices reduces such interest. Objectives: We wanted to conduct a first study on investigating the effect of removing ATD on the occurrence of incidents in a microservices architecture. Method: We conducted a quantitative and qualitative case study of a project with approximately 1000 microservices in a large, international financing services company. We measured and compared the number of software incidents of different categories before and after repaying ATD. Results: The total number of incidents was reduced by 84%, and the numbers of critical- and high-priority incidents were both reduced by approximately 90% after the architectural refactoring. The number of incidents in the architecture with the ATD was mainly constant over time, but we observed a slight increase of low priority incidents related to inaccessibility and the environment in the architecture without the ATD. Conclusion: This study shows evidence that refactoring ATDs, such as lack of communication standards, poor management of dead-letter queues, and the use of inadequate technologies in microservices, reduces the number of critical- and high-priority incidents and, thus, part of its interest, although some low priority incidents may increase.},
  keywords={},
  doi={10.1109/SEAA53835.2021.00033},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8845242,
  author={Tseng, Yuchia and Imadali, Sofiane and Houatra, Drissa and Aravinthan, Gopalasingham and Thomas, Laurent},
  booktitle={IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={Demo Abstract: Monitoring Virtualized Telco Services for Multisided Platforms with SQL-like Query}, 
  year={2019},
  volume={},
  number={},
  pages={949-950},
  abstract={The Telco ecosystem transformation towards cloud-native network services enables constructing an integrative platform business model in the form of a Multi-Sided Platform (MSP) leveraging microservice-based Virtualized Network Function architecture. In particular, MSP based architectures enable a multi-organizational ecosystem with increased automation possibilities for carrier-grade services creation and operations. We present a microservice-based monitoring system for virtualized Telco services based on OpenAirInterface (OAI) with an SQL-like query manager for metrics. We demonstrate two monitoring scenarios: (1) Average receiving (rx) PDU in bytes at MAC layer from the targeted user equipment (UE). (2) Finding the UE who consumes the most Physical Resource Blocks (PRB) within a specific time interval for the uplink and downlink transmission.},
  keywords={},
  doi={10.1109/INFCOMW.2019.8845242},
  ISSN={},
  month={April},}

@INPROCEEDINGS{9644910,
  author={Hou, Chuanjia and Jia, Tong and Wu, Yifan and Li, Ying and Han, Jing},
  booktitle={2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)}, 
  title={Diagnosing Performance Issues in Microservices with Heterogeneous Data Source}, 
  year={2021},
  volume={},
  number={},
  pages={493-500},
  abstract={Microservices architecture is vulnerable to performance issues due to its highly fine-grained decomposition of an application. To diagnose performance issues in microservices, existing works utilize system metrics as the specific indicator and do a lot of heavy computation such as building service dependency graphs during the diagnosing process.To improve the effectiveness and efficiency of issue diagnosing, we propose PDiagnose, a practical approach using multiple data sources including metrics, logs and traces jointly to diagnose performance issues in microservices systems. Through combining lightweight unsupervised anomaly detection algorithms and vote-based issue localization strategy, PDiagnose is application-agnostic and can localize root cause indicators accurately. Our evaluation on two public-available datasets shows that PDiagnose can achieve an overall recall of 84.8%, outperforming the best baseline approach. Meanwhile, the diagnosis duration of PDiagnose is also promising.},
  keywords={},
  doi={10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00074},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9556733,
  author={Song, Da and Yuan, Long and Zhao, Weixing and Yu, Quanxi and Du, Jie and Pan, Kaiyan},
  booktitle={2021 China International Conference on Electricity Distribution (CICED)}, 
  title={Cloud-Edge Computing Resource Collaborative Optimization Method for Power Distribution Fault Analysis Service}, 
  year={2021},
  volume={},
  number={},
  pages={627-632},
  abstract={The cloud-edge computing architecture of distribution network can meet the computing and communication requirements of most novel and traditional distribution services. However, the demand for computing resources of fault service is often greater than the resource capacity of edge computing terminal. Therefore, based on the cloud-edge collaborative architecture, this paper proposes a collaborative optimization method of cloud and edge computing resources for fault service in distribution network. Firstly, this paper elaborates the characteristics of fault service in distribution network, and describes the possibility of cloud-edge collaborative information interaction and microservice offloading based on container technology. Then, a cloud-edge collaborative service computing model and the microservice model of fault service are established. According to the offloading mechanism, a microservice offloading decision optimization model is established, which take the system operation cost and calculation delay as the comprehensive measuring index. Finally, the method proposed in this paper is simulated by MATLAB, and the simulation results show that this method can effectively reduce the microservice response time of distribution network and meet the computing resource requirements of fault service.},
  keywords={},
  doi={10.1109/CICED50259.2021.9556733},
  ISSN={2161-749X},
  month={April},}

@INPROCEEDINGS{9196461,
  author={Rossi, Fabiana and Cardellini, Valeria and Presti, Francesco Lo},
  booktitle={2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)}, 
  title={Hierarchical Scaling of Microservices in Kubernetes}, 
  year={2020},
  volume={},
  number={},
  pages={28-37},
  abstract={In the last years, we have seen the increasing adoption of the microservice architectural style where applications satisfy user requests by invoking a set of independently deployable services. Software containers and orchestration tools, such as Kubernetes, have simplified the development and management of microservices. To manage containers' horizontal elasticity, Kubernetes uses a decentralized threshold-based policy that requires to set thresholds on system-oriented metrics (i.e., CPU utilization). This might not be well-suited to scale latency-sensitive applications, which need to express requirements in terms of response time. Moreover, being a fully decentralized solution, it may lead to frequent and uncoordinated application reconfigurations. In this paper, we present me-kube (Multi-level Elastic Kubernetes), a Kubernetes extension that introduces a hierarchical architecture for controlling the elasticity of microservice-based applications. At higher level, a centralized per-application component coordinates the run-time adaptation of subordinated distributed components, which, in turn, locally control the adaptation of each microservice. Then, we propose novel proactive and reactive hierarchical control policies, based on queuing theory. To show that me-kube provides general mechanisms, we also integrate reinforcement learning-based scaling policies. Using me-kube, we perform a large set of experiments, aimed to show the advantages of a hierarchical control over the default Kubernetes autoscaler.},
  keywords={},
  doi={10.1109/ACSOS49614.2020.00023},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9355698,
  author={Ray, Kaustabha and Banerjee, Ansuman and Narendra, Nanjangud C.},
  booktitle={2020 IEEE/ACM Symposium on Edge Computing (SEC)}, 
  title={Proactive Microservice Placement and Migration for Mobile Edge Computing}, 
  year={2020},
  volume={},
  number={},
  pages={28-41},
  abstract={In recent times, Mobile Edge Computing (MEC) has emerged as a new paradigm allowing low-latency access to services deployed on edge nodes offering computation, storage and communication facilities. Vendors deploy their services on MEC servers to improve performance and mitigate network latencies often encountered in accessing cloud services. A service placement policy determines which services are deployed on which MEC servers. A number of mechanisms exist in literature to determine the optimal placement of services considering different performance metrics. However, for applications designed as microservice workflow architectures, service placement schemes need to be re-examined through a different lens owing to the inherent interdependencies which exist between microservices. Indeed, the dynamic environment, with stochastic user movement and service invocations, along with a large placement configuration space makes microservice placement in MEC a challenging task. Additionally, owing to user mobility, a placement scheme may need to be recalibrated, triggering service migrations to maintain the advantages offered by MEC. Existing microservice placement and migration schemes consider on-demand strategies. In this work, we take a different route and propose a Reinforcement Learning based proactive mechanism for microservice placement and migration. We use the San Francisco Taxi dataset to validate our approach. Experimental results show the effectiveness of our approach in comparison to other state-of-the-art methods.},
  keywords={},
  doi={10.1109/SEC50012.2020.00010},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9105640,
  author={Valdivia, José A. and Limón, Xavier and Cortes-Verdin, Karen},
  booktitle={2019 7th International Conference in Software Engineering Research and Innovation (CONISOFT)}, 
  title={Quality attributes in patterns related to microservice architecture: a Systematic Literature Review}, 
  year={2019},
  volume={},
  number={},
  pages={181-190},
  abstract={Microservices is an interesting option for those who want to migrate their systems to improve performance, maintainability, scalability, and interoperability. Microservice architecture is a collection of self-sufficient services working together to provide functionalities. Nowadays, there are many options to build microservices, some of them are lead by patterns. However, the mapping between quality attributes and patterns is not clear yet. This systematic literature review presents a microservice pattern collection, it describes their benefits and the association between patterns and quality attributes. Finally, some metrics of quality attributes are identified.},
  keywords={},
  doi={10.1109/CONISOFT.2019.00034},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9130466,
  author={Al-Debagy, Omar and Martinek, Péter},
  booktitle={2020 IEEE 15th International Conference of System of Systems Engineering (SoSE)}, 
  title={Extracting Microservices’ Candidates from Monolithic Applications: Interface Analysis and Evaluation Metrics Approach}, 
  year={2020},
  volume={},
  number={},
  pages={289-294},
  abstract={There is a migration trend toward microservices architecture coming from the monolithic applications. This research proposes a decomposition method that extracts microservices’ candidates through analyzing the application programming interface in order to extract the operations and the parameters. Then the operation names are converted into word representations using word embedding models. Next, semantically similar operations are clustered together to provide a microservice’ candidate. Additional step is to evaluate the proposed candidate using cohesion and complexity metrics. The proposed algorithm improved the decomposition approach for big applications but did not affect the decomposition of smaller applications.},
  keywords={},
  doi={10.1109/SoSE50414.2020.9130466},
  ISSN={},
  month={June},}

@INPROCEEDINGS{8399148,
  author={Guaman, Daniel and Yaguachi, Lady and Samanta, Cueva C. and Danilo, Jaramillo H. and Soto, Fernanda},
  booktitle={2018 13th Iberian Conference on Information Systems and Technologies (CISTI)}, 
  title={Performance evaluation in the migration process from a monolithic application to microservices}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Microservices are considered as a software architecture that allows the decomposition of a system, its components or its functionalities into a set of small services, which are implemented, deployed and managed independently. In this study, the models that allow migrating a Monolith to Microservices such as NGINX and IBM are analyzed. From these models, activities that allow such migration are carefully selected and identified. In order to implement and evaluate the activities proposed in those models, an application that initially does not have any structure at the design and coding level (using PHP programming language) is applied. Then, the application's coding language changes to Java and the classes and libraries are distributed into packages. Subsequently, as it is suggested in the models, services are identified and implemented using RESTful Web Services to finally implement the microservices using technologies such as Spring Boot, Eureka, and Zuul. In the migration process, the application under study is modified at the code and design level, including patterns such as Singleton, Façade, Strangler, Single Service per Host, Service Discovery, and API Gateway, which are used to evaluate performance as a quality attribute in each migration phase. In order to obtain the performance related metrics and to analyze the advantages and disadvantages of each migration phase, Apache JMeter as tool is used. This tool is set up to generate results regarding the use of resources such as CPU, memory, network, and database access. Finally, the results show scenarios of several concurrent users who access to consult records in the database that uses the aforementioned application in each migration phase.},
  keywords={},
  doi={10.23919/CISTI.2018.8399148},
  ISSN={},
  month={June},}

@ARTICLE{9507486,
  author={Xu, Rongxu and Jin, Wenquan and Kim, Dohyeun},
  journal={IEEE Access}, 
  title={Enhanced Service Framework Based on Microservice Management and Client Support Provider for Efficient User Experiment in Edge Computing Environment}, 
  year={2021},
  volume={9},
  number={},
  pages={110683-110694},
  abstract={Leveraging the edge computing paradigm, computing resources are deployed in the network edge to provide heterogeneous services. Edge computing delivers sensing and actuating services to the Internet from the constrained Internet of Things (IoT) devices. Meanwhile, management of various elements is provided by offloading sufficient computing and storage to the edge of the networks for the IoT environments such as home, factory, and private spaces without cloud servers. In this paper, we propose an enhanced service framework based on microservice management and client support provider for efficient user experiments in the edge computing environment. For providing the edge computing service and management in the network edge, this paper presents an edge-computing architecture that provides various functions through microservice modules on the edge platform engine. Through the microservices, the interfaces are provided to the client to access the device, data, and additional services. Using Docker, the microservice modules are deployed in the edge platform to provide the services. However, the services and management functions need to be presented to the clients based on the friendly user interfaces. For providing the user interfaces of the services and Docker engine to the clients, the client support service provider is developed and deployed in the network edge based on the edge platform. Therefore, the proposed edge platform provides the services and management to the users for accessing the resources and functions through visualized interfaces in the IoT environment based on edge computing. The performance of our proposed system can be checked through the test result screen and delay time. Compared to controlling edge computing by using a command-line tool for users, we made it easy for general users who are not computer savvy to access edge services through a graphic user interface. And by measuring the delay time and comparing the execution time, it can be seen that the proposed system operates faster.},
  keywords={},
  doi={10.1109/ACCESS.2021.3102595},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7829777,
  author={Parimala, N. and Kohar, Rachna},
  booktitle={2016 Eleventh International Conference on Digital Information Management (ICDIM)}, 
  title={A quality metric for BPEL process under evolution}, 
  year={2016},
  volume={},
  number={},
  pages={197-202},
  abstract={In Service-Oriented Architecture (SOA), behaviour of a business process is specified using Business Process Execution Language (BPEL) which is a XML based language. In today's competitive market, enterprises change their business processes frequently. Changes in BPEL process may affect the quality of BPEL process for the consumer. It is desirable to measure and evaluate the BPEL process quality when changes occur. Metrics are vastly used to provide a quantitative measure for the quality. In this paper, BPEL Process Usefulness Metric under Evolution (BUME) is proposed to measure quality of a BPEL process when it evolves. The applicability of the metric is demonstrated using simulated data for different versions of a BPEL process.},
  keywords={},
  doi={10.1109/ICDIM.2016.7829777},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7562758,
  author={Honamore, Suhas and Kumar, Lov and Rath, Santanu Ku.},
  booktitle={2016 International Conference on Internet of Things and Applications (IOTA)}, 
  title={Analysis of control flow complexity metrics for web service composition}, 
  year={2016},
  volume={},
  number={},
  pages={389-394},
  abstract={In service oriented computing, web services are combined to meet the interoperability demands in different heterogeneous and distributed applications. However, incisively measuring the control flow complexity of Web Service Composition (WSC) is not an easy task due to characteristics of distributed, loose-coupling, and heterogeneity. In Service Oriented Architecture (SOA), Business Process Execution Language (BPEL) is used to describe the combination of web services. This paper mainly focuses on the complexity measurement of web service composition from BPEL. Petri-net is one of the models to represent the work flow. The BPEL of WSC is converted into Petri-net based model and by extracting the information of places, transitions, and their interrelationship; the complexity is measured for that Petri-net model. Two metric sets are considered for analysis of the WSC's complexity, which are identified by studying the workflow's execution dependency relations. The first metric set describes the static features, and second metric set describes about the dynamic complexity of business process.},
  keywords={},
  doi={10.1109/IOTA.2016.7562758},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{9304633,
  author={Cebotari, Vadim and Kugele, Stefan},
  booktitle={2020 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Playground for Early Automotive Service Architecture Design and Evaluation}, 
  year={2020},
  volume={},
  number={},
  pages={1349-1356},
  abstract={Context: We consider the structure of service-oriented architectures in vehicular software. Aim: We aim at evaluating the structure and grouping of service architectures. Method: We propose and discuss architectural metrics tailored towards automotive service-oriented architectures. We apply the metrics on an adaptive cruise control case example extracted from the AUTOSAR standard. Results: The application of the proposed metrics to two different service groupings for ACC points clearly to the same service grouping that we consider, after a thorough analysis, to be better with respect to coupling and cohesion attributes. Conclusion: We demonstrate the usefulness of proposed service group metrics in early design phases of the development process and validate the metrics on the case example of an adaptive cruise control function.},
  keywords={},
  doi={10.1109/IV47402.2020.9304633},
  ISSN={2642-7214},
  month={Oct},}

@INPROCEEDINGS{8614791,
  author={Parekh, Nikunj and Kurunji, Swathi and Beck, Alan},
  booktitle={2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)}, 
  title={Monitoring Resources of Machine Learning Engine In Microservices Architecture}, 
  year={2018},
  volume={},
  number={},
  pages={486-492},
  abstract={Microservices architecture facilitates building distributed scalable software products, usually deployed in a cloud environment. Monitoring microservices deployed in a Kubernetes orchestrated distributed advanced analytics machine learning engines is at the heart of many cloud resource management solutions. In addition, measuring resource utilization at more granular level such as per query or sub-query basis in an MPP Machine Learning Engine (MLE) is key to resource planning and is also the focus of our work. In this paper we propose two mechanisms to measure resource utilization in Teradata Machine Learning Engine (MLE). First mechanism is the Cluster Resource Monitoring (CRM). CRM is a high-level resource measuring mechanism for IT administrators and analytics users to visualize, plot, generates alerts and perform live and historical-analytics on overall cluster usage statistics. Second mechanism is the Query Resource Monitoring (QRM). QRM enables IT administrators and MLE users to measure compute resource utilization per individual query and its sub-queries. When query takes long time, QRM provides insights. This is useful to identify expensive phases within a query that tax certain resources more and skew the work distribution. We show the results of proposed mechanisms and highlight use-cases.},
  keywords={},
  doi={10.1109/IEMCON.2018.8614791},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8972825,
  author={Samir, Areeg and Pahl, Claus},
  booktitle={2019 7th International Conference on Future Internet of Things and Cloud (FiCloud)}, 
  title={DLA: Detecting and Localizing Anomalies in Containerized Microservice Architectures Using Markov Models}, 
  year={2019},
  volume={},
  number={},
  pages={205-213},
  abstract={Container-based microservice architectures are emerging as a new approach for building distributed applications as a collection of independent services that works together. As a result, with microservices, we are able to scale and update their applications based on the load attributed to each service. Monitoring and managing the load in a distributed system is a complex task as the degradation of performance within a single service will cascade reducing the performance of other dependent services. Such performance degradations may result in anomalous behaviour observed for instance for the response time of a service. This paper presents a Detection and Localization system for Anomalies (DLA) that monitors and analyzes performance-related anomalies in container-based microservice architectures. To evaluate the DLA, an experiment is done using R, Docker and Kubernetes, and different performance metrics are considered. The results show that DLA is able to accurately detect and localize anomalous behaviour.},
  keywords={},
  doi={10.1109/FiCloud.2019.00036},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{6986002,
  author={Nik Daud, Nik Marsyahariani and Wan Kadir, Wan M. N.},
  booktitle={2014 8th. Malaysian Software Engineering Conference (MySEC)}, 
  title={Static and dynamic classifications for SOA structural attributes metrics}, 
  year={2014},
  volume={},
  number={},
  pages={130-135},
  abstract={Evaluating qualities of software based on software structural attributes such as coupling and cohesion are frequently done in practice as these attributes directly have impacts on value of higher level quality. Concerning oneself with structural attributes values early on helps developers to predict quality attributes level in the software. Service-Oriented Architecture (SOA) is an architectural concept where services are used as building blocks in developing new software. Lots of structural attributes metrics related to SOA had been proposed these recent years, which triggered an investigation to classify these metrics based on specific criteria. In this paper, we introduce classifications for SOA based structural attributes metrics, where the metrics are restricted to coupling, cohesion and complexity metrics. These metrics are classified based on software static and dynamic aspects with some brief introduction for each metric. By classifying these SOA based structural attributes metrics, it will allow user to avoid redundancy in proposing similar metrics thus increases the reusability of existing metrics.},
  keywords={},
  doi={10.1109/MySec.2014.6986002},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9000664,
  author={Orduz, Juan S. and Orozco, Gabriel D. and Tobar-Arteaga, Carlos H. and Rendon, Oscar Mauricio Caicedo},
  booktitle={2019 IEEE 44th LCN Symposium on Emerging Topics in Networking (LCN Symposium)}, 
  title={μvIMS: A Finer-Scalable Architecture Based on Microservices}, 
  year={2019},
  volume={},
  number={},
  pages={141-148},
  abstract={The steps toward all over IP have defined to the IP Multimedia Subsystem (IMS) as the de facto technology for end-to-end multimedia service provisioning in 5G. However, the unpredictable growth of users in 5G requires to improve IMS scalability to handle dynamic user traffic. Several works have addressed this issue by introducing auto-scaling mechanisms in virtualized IMS (vIMS) architectures. However, the current vIMS deployments use monolithic designs that do not allow finer-scalability. In this paper, we present μvIMS, an architecture that uses microservices to provide finer-scalability and more effective resource usage than regular monolithic design. To test our architecture, we evaluate μvIMS prototype regarding CPU usage, RAM usage, Successful Call Rate (SCR), and latency metrics. Our test results reveal that μvIMS achieves a higher SCR, using the available resources effectively with a negligible latency increasing. Thus, we can state that dividing the monolithic vIMS architecture in microservices allows providing finer-scalability.},
  keywords={},
  doi={10.1109/LCNSymposium47956.2019.9000664},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9527007,
  author={Wu, Li and Tordsson, Johan and Bogatinovski, Jasmin and Elmroth, Erik and Kao, Odej},
  booktitle={2021 IEEE/ACM International Workshop on Cloud Intelligence (CloudIntelligence)}, 
  title={MicroDiag: Fine-grained Performance Diagnosis for Microservice Systems}, 
  year={2021},
  volume={},
  number={},
  pages={31-36},
  abstract={Microservice architecture has emerged as a popular pattern for developing large-scale applications for its benefits of flexibility, scalability, and agility. However, the large number of services and complex dependencies make it difficult and time-consuming to diagnose performance issues. We propose Micro-Diag, an automated system to localize root causes of performance issues in microservice systems at a fine granularity, including not only locating the faulty component but also discovering detailed information for its abnormality. MicroDiag constructs a component dependency graph and performs causal inference on diverse anomaly symptoms to derive a metrics causality graph, which is used to infer root causes. Our experimental evaluation on a microservice benchmark running in a Kubernetes cluster shows that MicroDiag localizes root causes well, with 97% precision of the top 3 most likely root causes, outperforming state-of-the-art methods by at least 31.1%.},
  keywords={},
  doi={10.1109/CloudIntelligence52565.2021.00015},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9582183,
  author={Choochotkaew, Sunyanan and Chiba, Tatsuhiro and Trent, Scott and Amaral, Marcelo},
  booktitle={2021 IEEE 14th International Conference on Cloud Computing (CLOUD)}, 
  title={Run Wild: Resource Management System with Generalized Modeling for Microservices on Cloud}, 
  year={2021},
  volume={},
  number={},
  pages={609-618},
  abstract={Microservice architecture competes with the traditional monolithic design by offering benefits of agility, flexibility, reusability resilience, and ease of use. Nevertheless, due to the increase in internal communication complexity, care must be taken for resource-usage scaling in harmony with placement scheduling, and request balancing to prevent cascading performance degradation across microservices. We prototype Run Wild, a resource management system that controls all mechanisms in the microservice-deployment process covering scaling, scheduling, and balancing to optimize for desirable performance on the dynamic cloud driven by an automatic, united, and consistent deployment plan. In this paper, we also highlight the significance of co-location aware metrics on predicting the resource usage and computing the deployment plan. We conducted experiments with an actual cluster on the IBM Cloud platform. RunWild reduced the 90th percentile response time by 11% and increased average throughput by 10% with more than 30% lower resource usage for widely used autoscaling benchmarks on Kubernetes clusters.},
  keywords={},
  doi={10.1109/CLOUD53861.2021.00079},
  ISSN={2159-6190},
  month={Sep.},}

@INPROCEEDINGS{9680438,
  author={Ramesh, Srinivasan and Ross, Robert and Dorier, Matthieu and Malony, Allen and Carns, Philip and Huck, Kevin},
  booktitle={2021 IEEE 28th International Conference on High Performance Computing, Data, and Analytics (HiPC)}, 
  title={SYMBIOMON: A High-Performance, Composable Monitoring Service}, 
  year={2021},
  volume={},
  number={},
  pages={332-342},
  abstract={High-performance computing (HPC) software is evolving to support an increasingly diverse set of applications and heterogeneous hardware architectures. As part of this evolution, the construction of scientific software has shifted from a traditional monolithic message passing interface executable model to a coupled, services-style model in which simulations run alongside a host of distributed HPC data services within the same batch job allocation. Microservices have emerged as a powerful new way to build these distributed data services through a composition model. However, performance analysis of composed microservices is a daunting challenge. It requires collecting, monitoring, aggre-gating, and exporting performance data from multiple sources. To be effective, the design of such a monitoring solution must allow for seamless integration into HPC applications and distributed services alike, be scalable, operate with a low overhead, and take advantage of the HPC platform. We propose SYMBIOMON, a monitoring service that is built by composing high-performance microservices. We describe its design and implementation within the context of the Mochi framework. SYMBIOMON combines a time-series data model with existing Mochi data services to collect, aggregate, and export performance metrics in a distributed manner. SYMBIOMON enables seamless, low-overhead monitoring and analysis of data services and HPC applications alike. Using HEPnOS, a production-quality Mochi data service, we demonstrate the use of SYMBIOMON to identify better service configurations.},
  keywords={},
  doi={10.1109/HiPC53243.2021.00047},
  ISSN={2640-0316},
  month={Dec},}

@INPROCEEDINGS{7899252,
  author={Do, Nam H. and Van Do, Tien and Thi Tran, Xuan and Farkas, Lóránt and Rotter, Csaba},
  booktitle={2017 20th Conference on Innovations in Clouds, Internet and Networks (ICIN)}, 
  title={A scalable routing mechanism for stateful microservices}, 
  year={2017},
  volume={},
  number={},
  pages={72-78},
  abstract={Scalability is an important requirement in the development and the operation of applications in a cloud environment. To handling heavy concurrency in the input load, many design-related and operational factors should be considered. The microservice architecture patterns provide better means to increase the scalability than traditional software architecture patterns. However, certain aspects of applications such as the need to persist/maintain the application state require additional measures in the design and the supporting mechanism. We propose a scalable routing mechanism for applications designed according to the microservice architecture. In particular, a cloud infrastructure resource reservation application has been designed with some stateful services. The proposed approach maintains a good scalability, which provides a mean to achieve the efficient usage of the infrastructure resources.},
  keywords={},
  doi={10.1109/ICIN.2017.7899252},
  ISSN={2472-8144},
  month={March},}

@INPROCEEDINGS{8494072,
  author={Pulparambil, Supriya and Baghdadi, Youcef and Al-Hamdani, Abdullah and Al-Badawi, Mohammed},
  booktitle={2018 9th International Conference on Computing, Communication and Networking Technologies (ICCCNT)}, 
  title={Service Design Metrics to Predict IT-Based Drivers of Service Oriented Architecture Adoption}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  abstract={The key factors for deploying successful services is centered on the service design practices adopted by an enterprise. The design level information should be validated and measures are required to quantify the structural attributes. The metrics at this stage will support an early discovery of design flaws and help designers to predict the capabilities of service oriented architecture (SOA) adoption. In this work, we take a deeper look at how we can forecast the key SOA capabilities infrastructure efficiency and service reuse from the service designs modeled by SOA modeling language. The proposed approach defines metrics based on the structural and domain level similarity of service operations. The proposed metrics are analytically validated with respect to software engineering metrics properties. Moreover, a tool has been developed to automate the proposed approach and the results indicate that the metrics predict the SOA capabilities at the service design stage. This work can be further extended to predict the business based capabilities of SOA adoption such as flexibility and agility.},
  keywords={},
  doi={10.1109/ICCCNT.2018.8494072},
  ISSN={},
  month={July},}

@INPROCEEDINGS{9525743,
  author={Gamage, Isuru Udara Piyadigama and Perera, Indika},
  booktitle={2021 Moratuwa Engineering Research Conference (MERCon)}, 
  title={Using dependency graph and graph theory concepts to identify anti-patterns in a microservices system: A tool-based approach}, 
  year={2021},
  volume={},
  number={},
  pages={699-704},
  abstract={Microservice architecture (MSA) based application developments are becoming the common trend in implementing large-scale applications. Unlike the traditional monolith applications, MSA applications are composed of many services hence there is an immense possibility of anti-patterns introduced into the system. To identify these design problems, a detailed analysis of the architecture needs to be performed. We see great potential for adopting graph concepts and algorithms in this regard. However, the few tools proposed by existing work to find anti-patterns that adopt graph concepts are not up to providing developers with adequate statistical information such as metrics along with visualization techniques or they are not fully automated. In this research, we present a tool-based solution for this problem which is capable of utilizing traced data of an MSA system to generate dependency graphs and thereby extract metrics using graph theory concepts and algorithms. We analyze a sample MSA system for anti-patterns with the tool. To verify the usability of the tool further, a group of developers also analyze an open-source system with the tool.},
  keywords={},
  doi={10.1109/MERCon52712.2021.9525743},
  ISSN={2691-364X},
  month={July},}

@INPROCEEDINGS{9198750,
  author={Brusakova, I. A.},
  booktitle={2020 XXIII International Conference on Soft Computing and Measurements (SCM)}, 
  title={Metrics for Cognitive Management of IT Services}, 
  year={2020},
  volume={},
  number={},
  pages={259-261},
  abstract={The article presents metrics for managing IT services in a service-oriented architecture of the information system. Cognitive management of the effectiveness of IT services is considered on a variety of ICT infrastructure management metrics, information system management metrics, IT service management metrics. The necessary components of the formation of an analytical platform for the cognitive management of IT services in the EIM environment for SAP BI (Business Objects Business Intelligent) are considered. A model of cognitive management of IT services using key performance indicators (KPIs) for managing IT service metrics is presented.},
  keywords={},
  doi={10.1109/SCM50615.2020.9198750},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8029780,
  author={Wang, Hanzhang and Kessentini, Marouane and Hassouna, Taghreed and Ouni, Ali},
  booktitle={2017 IEEE International Conference on Web Services (ICWS)}, 
  title={On the Value of Quality of Service Attributes for Detecting Bad Design Practices}, 
  year={2017},
  volume={},
  number={},
  pages={341-348},
  abstract={Service-Oriented Architectures (SOAs) successfully evolve over time to update existing exposed features to the users and fix possible bugs. This evolution process may have a negative impact on the design quality of Web services. Recent studies addressed the problem of Web service antipatterns detection (bad design practices). To the best of our knowledge, these studies focused only on the use of metrics extracted from the implementation details (source code) of the interface and the services. However, the quality of service (QoS) metrics, widely used to evaluate the overall performance, are never used in the context of Web service antipatterns detection. We start, in this work, from the hypothesis that these bad design practices may impact several QoS metrics such as the response time. Furthermore, the source code metrics of services may not be always available. Without the consideration of these QoS metrics, the current detection processes of antipatterns will still lack the integration of symptoms that could be extracted from the usage of services. In this paper, we propose an automated approach to generate Web service defect detection rules that consider not only the code/interface level metrics but also the quality of service attributes. Through multi-objective optimization, the proposed approach generates solutions (detection rules) that maximize the coverage of antipattern examples and minimize the coverage of well-designed service examples. An empirical validation is performed with eight different common types of Web design defects to evaluate our approach. We compared our results with three other state of the art techniques which are not using QoS metrics. The statistical analysis of the obtained results confirm that our approach outperforms other techniques and generates detection rules that are more meaningful from the services' user perspective.},
  keywords={},
  doi={10.1109/ICWS.2017.126},
  ISSN={},
  month={June},}

@INPROCEEDINGS{8704556,
  author={Alvarez Q., Juan M. and Sanabria O., John A. and Garcia M., Jose I},
  booktitle={2019 IEEE Latin American Test Symposium (LATS)}, 
  title={Microservices-based architecture for fault diagnosis in tele-rehabilitation equipment operated via Internet}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents the design of a microservices based architecture allows early fault detection and diagnosis on a remote controlled physical rehabilitation machine using the Internet as a communication channel. Aforementioned architecture is composed of three layers: the low layer which collects variables from the rehabilitation machine components, using Internet of Things protocols. The middle layer which analyses the provided variables and diagnoses the component status, using fuzzy logic. And finally, the upper layer which makes decisions depending on the diagnosis data. The proposed architecture is suitable for heterogeneous systems.This paper also shows how this architecture fulfills the specific and rigorous safety measures for critical mission devices like technical aids for health-care.},
  keywords={},
  doi={10.1109/LATW.2019.8704556},
  ISSN={2373-0862},
  month={March},}

@INPROCEEDINGS{6966260,
  author={Gomathy, C. K. and Rajalakshmi, S.},
  booktitle={Second International Conference on Current Trends In Engineering and Technology - ICCTET 2014}, 
  title={A software quality metric performance of professional management in service oriented architecture}, 
  year={2014},
  volume={},
  number={},
  pages={41-47},
  abstract={Service-oriented architecture (SOA) is generally the way of containing and examines to develop the information management needs in order to make dealing responsive and elastic in pace with forceful quality conditions. Adopting, implementing and running SOA require considerable thought and effort in order to distribute high-quality metrics data and become conscious the complete assessment of SOA. In this paper, inspect the sequentially and quality related metrics issues that have been investigated organizations in order to uncover the activities in regard to information quality within their initiatives of implementing SOA. In the succession of quality behavior that solve certain information quality and maintenance, development issues therefore, can be enthusiastically established across any industry to support the building of high quality and then making SOA solutions. In current days service oriented architecture design is also incorporated and potentially distributed with the quality metrics and to perform a superior evaluation of the representation.},
  keywords={},
  doi={10.1109/ICCTET.2014.6966260},
  ISSN={},
  month={July},}

@INPROCEEDINGS{9407977,
  author={Tummalapalli, Sahithi and Kumar, Lov and Neti, Lalita Bhanu Murthy and Krishna, Aneesh},
  booktitle={2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)}, 
  title={An Empirical Analysis on the Role of WSDL Metrics in Web Service Anti-Pattern Prediction}, 
  year={2020},
  volume={},
  number={},
  pages={559-564},
  abstract={Service-Oriented Architecture (SOA) is one of the most well-known models for designing web systems. SOA system evolution and maintenance is challenging because of its distributive nature and secondly due to the demand of designing high-quality, stable interfaces. This evolution leads to a problem called Anti-patterns in web services. It is observed that these anti-patterns negatively impact the evolution and maintenance of software systems, making the early detection and correction of them a primary concern for the software developers. The primary motivation of this work is to investigate the relationship between the Web Service Description Language(WSDL) metrics and anti-patterns in web services. This research aims to develop an automatic method for the detection of web service anti-patterns. The core idea of the methodology defined is to identify the most crucial WSDL metrics with the association of various feature selection techniques for the prediction of anti-patterns. Experimental results show that the model developed by using all the WSDL quantity metrics(AM) shows a bit high performance compared to the models developed with the other metric sets. Experimental results also showed that the performance of the models generated using Decision Tree(DT) and Major Voting Ensemble(MVE) is high compared to the models generated using other classifier techniques.},
  keywords={},
  doi={10.1109/HPCC-SmartCity-DSS50907.2020.00070},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8035019,
  author={Alnahdi, Amany and Liu, Shih-Hsi},
  booktitle={2017 IEEE International Conference on Services Computing (SCC)}, 
  title={Identifying Characteristic Attributes for Estimating Cost of Service in Service Oriented Architecture}, 
  year={2017},
  volume={},
  number={},
  pages={467-470},
  abstract={Web services are software modules that provide interoperability over a network. Web services provide Web service users platform independence while using software. It enables businesses to collaborate by using Web services from Web service providers. Estimating a Cost of Service (CoS) is essential when pricing, selecting, and monitoring a Web service. The concept of cost is not restricted to financial value of technology hardware and software. The cost concept can also include time, usability, and maintenance. Cost of a Web service can be estimated by identifying the attributes of cost from the perspective of different stakeholders such as Web service provider, Web service consumer, Web service repository moderator, and Web service policy maker. In addition, analyzing different roles in Service Oriented Architecture (SOA) will further provide more knowledge about different perspectives of cost concepts in SOA. This paper addresses the essential attributes of estimating cost of a Web service. Moreover, this paper specifies attributes of measuring CoS, defines these attributes, and defines metrics and units of these attributes. Additionally, it provides further hierarchy classification of Web service cost concepts. It also provides a model for evaluating Web service cost based on different cost criteria. By measuring CoS, Web service stakeholders will be able to estimate an accurate value to the CoS.},
  keywords={},
  doi={10.1109/SCC.2017.66},
  ISSN={2474-2473},
  month={June},}

@INPROCEEDINGS{8786277,
  author={Delgado, Andrea},
  booktitle={2018 XLIV Latin American Computer Conference (CLEI)}, 
  title={Monitoring and Analyzing Service Execution from Business Processes: An AXIS Extension}, 
  year={2018},
  volume={},
  number={},
  pages={582-589},
  abstract={Implementing Business Processes (BPs) with services (and microservices) is nowadays the main way to support the execution of automated activities in processes, both within the organization itself, and externally interacting with customers, suppliers and other participants. In order to do so, it is important not only to model and implement services but also to define Quality of Service (QoS) characteristics for services, to monitor and evaluate their execution. Although there are many proposals for services monitoring and evaluation from the services point of view, there are not many from the BPs perspective. In this paper we present a reference architecture for service monitoring tools, along with a prototype implementation as an extension of the web services execution environment AXIS2. We show that existing service measures and new ones can be defined into the monitor to collect execution data and relate this data with BPs execution, to measure BPs and service execution in an integrated manner.},
  keywords={},
  doi={10.1109/CLEI.2018.00075},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9307705,
  author={Camilli, Matteo and Colarusso, Carmine and Russo, Barbara and Zimeo, Eugenio},
  booktitle={2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
  title={Domain Metric Driven Decomposition of Data-Intensive Applications}, 
  year={2020},
  volume={},
  number={},
  pages={189-196},
  abstract={The microservices architectural style is picking up more and more momentum in IT industry for the development of systems as loosely coupled, collaborating services. Companies that undergo the migration of their own applications have aspirations such as increasing maintainability and the scale of operation. Such a process is worthwhile but not easy, since it should ensure atomic improvements to the overall architecture for each migration step. Furthermore, the systematic evaluation of migration steps becomes cumbersome without sensible optimization metrics that take into account performance and scalability under expected operational conditions. Recent lines of research recognize this task as challenging, especially in data-intensive applications where known approaches based, for instance, on Domain Driven Design may not be adequate. In this paper, we introduce an approach to evaluate a migration in an iterative way and recognize whether it represents an improvement in terms of performance and scalability. The approach leverages a Domain Metric-based analysis to quantitatively evaluate alternative architectures. We exemplified the envisioned approach on a data-intensive application case study in the domain of smart mobility. Preliminary results from our controlled experiments show the effectiveness of our approach to support systematic and automated evaluation of migration processes.},
  keywords={},
  doi={10.1109/ISSREW51248.2020.00071},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8986987,
  author={Tripathi, Manish K and Chaubisa, Divyanshu and Kumar, Lov and Murthy Neti, Lalita Bhanu},
  booktitle={2018 15th IEEE India Council International Conference (INDICON)}, 
  title={Prediction of Quality of Service Parameters Using Aggregate Software Metrics and Machine Learning Techniques}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={In todays Service-Oriented Architecture (SOA) world, software systems are built by composing web services offered by Service Providers (SPs). There are different SPs offering services for the same set of functional requirements. Service providers are expected to be highly competitive in their offerings to enhance their market. The quality of web services is an important factor that differentiates one service provider from another. Twelve parameters are identified by which quality of service can be measured. The prediction of these twelve QoS parameters help SPs to enhance the quality of their service. Each web service is realized by several programming files. CK and object oriented metrics of the underlying Java files of the web services are important features for predicting QoS parameters of the web service. The aggregated measure, mean, is chosen to be a feature in predicting the QoS parameters in earlier studies. We propose to build prediction models using 16 aggregate measures and show that there is significant difference between these aggregate measures. We find best feature subset using six feature selection techniques and build prediction models using Extreme Learning Machines with different kernels. We show that feature selection techniques might not enhance prediction accuracies and the ensemble algorithm out performs other learning algorithms.},
  keywords={},
  doi={10.1109/INDICON45594.2018.8986987},
  ISSN={2325-9418},
  month={Dec},}

@INPROCEEDINGS{8767397,
  author={White, Gary and Palade, Andrei and Cabrera, Christian and Clarke, Siobhán},
  booktitle={2019 IEEE International Conference on Pervasive Computing and Communications (PerCom}, 
  title={Autoencoders for QoS Prediction at the Edge}, 
  year={2019},
  volume={},
  number={},
  pages={1-9},
  abstract={In service-oriented architectures, collaborative filtering is a key technique for service recommendation based on QoS prediction. Matrix factorisation has emerged as one of the main approaches for collaborative filtering as it can handle sparse matrices and produces good prediction accuracy. However, this process is resource-intensive and training must take place in the cloud, which can lead to a number of issues for user privacy and being able to update the model with new QoS information. Due to the time-varying nature of QoS it is essential to update the QoS prediction model to ensure that it is using the most recent values to maintain prediction accuracy. The request time, which is the time for a middleware to submit a user's information and receive QoS metrics for a candidate services is also important due to the limited time during dynamic service adaptations to choose suitable replacement services. In this paper we propose a stacked autoencoder with dropout on a deep edge architecture and show how this can be used to reduce training and request time compared to traditional matrix factorisation algorithms, while maintaining predictive accuracy. To evaluate the accuracy of the algorithms we compare the actual and predicted QoS values using standard error metrics such as MAE and RMSE. In addition, we propose an alternative evaluation technique using the predictions as part of a service composition and measuring the impact that the predictions have on the response time and throughput of the final composition. This more clearly shows the direct impact that these algorithms will have in practice.},
  keywords={},
  doi={10.1109/PERCOM.2019.8767397},
  ISSN={2474-249X},
  month={March},}

@INPROCEEDINGS{7434265,
  author={Bora, Abhijit and Bezboruah, Tulshi},
  booktitle={2015 IEEE International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)}, 
  title={Some aspects of QoS for interoperability of multi service multi functional service oriented computing}, 
  year={2015},
  volume={},
  number={},
  pages={363-368},
  abstract={Quality of service is the key indicator for service oriented architectures, because it directly expresses the operability and computational nature of the system. As such, we propose a quality evaluation framework for multi service multi functional hierarchical SOAP based web service. The overall interoperable quality is evaluated through load testing using Mercury Load Runner with Apache Tomcat web server and MySQL database engine. The recorded quality metrics are analyzed statistically. We present here in detail the architecture, observed metrics and analyzed results of the service oriented computing to validate the acceptability of the evaluation framework.},
  keywords={},
  doi={10.1109/ICRCICN.2015.7434265},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8539382,
  author={Park, Youngki and Yang, Hyunsik and Kim, Younghan},
  booktitle={2018 International Conference on Information and Communication Technology Convergence (ICTC)}, 
  title={Performance Analysis of CNI (Container Networking Interface) based Container Network}, 
  year={2018},
  volume={},
  number={},
  pages={248-250},
  abstract={The increasing significance of lightweight cloud infrastructure for microservices cannot be overstated. This has led many researchers to propose container based virtualized computing services. Specifically, for networks, Container Networking Interface technologies are proposed to connect heterogeneous network services between virtual-machine based clouds and containers. In order to improve network performance of cloud systems, a comparison with detailed design and performance verification of network configuration using CNI technologies is required. In this paper, centering on various CNI technologies, we designed network architectures with OpenStack cloud platform and Kubernetes container management environment, and subsequently measured network performance for each design. The results of the evaluation are useful to provide guidelines for containerized cloud system deployment.},
  keywords={},
  doi={10.1109/ICTC.2018.8539382},
  ISSN={2162-1233},
  month={Oct},}

@INPROCEEDINGS{9272016,
  author={Liu, Bo and Betancourt, Victor Pazmino and Zhu, Yimeng and Becker, Jürgen},
  booktitle={2020 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={Towards an On-Demand Redundancy Concept for Autonomous Vehicle Functions using Microservice Architecture}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={More and more functionalities will be deployed on heterogeneous devices in the vehicle for future autonomous driving. These devices will be connected not only within the vehicle but also to the internet to receive information or consume services provided by other vehicles or road-side units. Even some functions could be offloaded to the cloud infrastructure. However, this high connectivity also means more cyber-security issues for future autonomous driving cars. As cyber-attacks become a more serious issue for the future automotive industry, keeping high availability of safety-critical and non-safety-critical vehicle functions when connected devices in the vehicle are being attacked is an important and challenging task. In this paper, we propose an on-demand redundancy concept to get high availability for autonomous vehicle functions using microservice architecture and container technology. We implemented the concept of embedded devices and showed the feasibility of this concept. The results showed that redundancy could be setup dynamically for non-safety-critical vehicle functions in a cost-effective manner using the proposed approach. This approach could be taken as a security measure while certain devices are being attacked, and the system could continue working without being influenced.},
  keywords={},
  doi={10.1109/ISSE49799.2020.9272016},
  ISSN={2687-8828},
  month={Oct},}

@INPROCEEDINGS{9279887,
  author={Băjenaru, Lidia and Dobre, Ciprian and Ciobanu, Radu-Ioan and Dedu, Georgiana and Pantelimon, Silviu-George and Marinescu, Ion Alexandru and Gavrilă, Veronica},
  booktitle={2020 International Conference on e-Health and Bioengineering (EHB)}, 
  title={Depth-based Human Activity Recognition: vINCI Case Study}, 
  year={2020},
  volume={},
  number={},
  pages={1-4},
  abstract={The growing aging of the world's population is leading to the need to take assistance measures and prepare health care systems for the elderly. The innovative vINCI system provides technologies and uses smart devices that can noninvasively monitor the activity of elderly, to intervene in case of alerts, to prevent possible health problems, such as falling, in the same time to keep their life independent and to improve their quality of life. Monitoring physical activity of the elderly with the help of smart cameras is important in identifying one of the most important lifestyle risk factors for many chronic conditions in the older age. In this paper there are presented the microservice-based vINCI architecture and how an Orbbec Persee camera is used to monitor the physical activity as well as to recognize the elderly. The advantages of the monitoring physical activity application consist in detecting a low level of activity or detecting health problems allowing intervention and correction of an unhealthy lifestyle.},
  keywords={},
  doi={10.1109/EHB50910.2020.9279887},
  ISSN={2575-5145},
  month={Oct},}

@INPROCEEDINGS{9628475,
  author={de Faria, Brenno Tondato and Aguzzi, Cristiano and Bates, Travis and Campbell, Colin and Tomei, Fausto and Bittelli, Marco and Roffia, Luca},
  booktitle={2021 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)}, 
  title={Predict soil moisture into the future: on the integration of CRITERIA-1D into ZENTRA cloud}, 
  year={2021},
  volume={},
  number={},
  pages={331-335},
  abstract={This paper presents a case of study of a IoT cloud plat-form composed of a microservices architecture that has been developed to integrate the CRITERIA-1D into the ZENTRA cloud. CRITERIA-1D is an open-source agro-hydrological model developed by ARPAE simulating one-dimensional soil water fluxes, crop development, and crop water needs. CRITERIA-1D comes with a default set of crops and soils that can be used or tuned for a specific scenarios. Taking as input the weather forecasts (i.e., temperatures and precipitations), the model can be used to predict the soil water content and soil water potential at different depths. Along with the design of the implemented solution, this paper presents the process of tuning crop and soil parameters for a specific use case. The results show that the tuned model estimates very well with respect to the measures observed by sensors, paving the way to its application within the larger context of the METER’s ZENTRA cloud.},
  keywords={},
  doi={10.1109/MetroAgriFor52389.2021.9628475},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7062707,
  author={Nuraini, Aminah and Widyani, Yani},
  booktitle={2014 International Conference on Data and Software Engineering (ICODSE)}, 
  title={Software with service oriented architecture quality assessment}, 
  year={2014},
  volume={},
  number={},
  pages={1-6},
  abstract={Service Oriented Architecture (SOA) is becoming popular since its flexibility fulfill the need of rapidly changing enterprise requirement. Therefore, expectation of a good quality software with SOA is getting higher. To address this need, this paper presents a guideline to conduct quality assessment using an existing tool. The quality assessment model is designed by selecting the relevant quality factors, choosing an appropriate quality to metric mapping method, identifying the relevant metrics, and mapping each quality factor to the metrics. Using the model, the quality assessment process is prepared by identifying data and selecting the appropriate tools. The chosen tool may require some modification. The proposed quality assessment guideline can help the software quality assurance team to assess quality of their software with SOA. The proposed guideline has been used to assess the quality of an existing sofware with SOA (Bonita BPM). The result is considered as promising, although several improvement are still needed.},
  keywords={},
  doi={10.1109/ICODSE.2014.7062707},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8622058,
  author={Streiffer, Christopher and Raghavendra, Ramya and Benson, Theophilus and Srivatsa, Mudhakar},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Learning to Simplify Distributed Systems Management}, 
  year={2018},
  volume={},
  number={},
  pages={1837-1845},
  abstract={Managing large-scale distributed systems is a difficult task. System administrators are responsible for the upkeep and maintenance of numerous components with complex dependencies. With the shift to microservices-based architectures, these systems can consist of 100s to 1000s of interconnected nodes. To combat this difficulty, administrators rely on analyzing logs and metrics collected from the different services. However, the number of available metrics for large systems presents complexity and scaling issues. To combat these issues, we present Minerva, an unsupervised Machine Learning (ML) framework for performing network diagnosis analysis. Minerva is composed of a multi-stage pipeline, where each component can act individually or cohesively to perform various management tasks. Our system offers a unified and extensible framework for managing the complexity of large networks, and presents administrators with a swiss-army knife for diagnosing the overall health of their systems. To demonstrate the feasibility of Minerva, we evaluate its performance on a production-scale system. We present use cases for the various management tools made available by Minerva, and show how these tools can be used to make strong inferences about the system using unsupervised techniques.},
  keywords={},
  doi={10.1109/BigData.2018.8622058},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9453517,
  author={Lennick, David and Azim, Akramul and Liscano, Ramiro},
  booktitle={2021 22nd IEEE International Conference on Industrial Technology (ICIT)}, 
  title={A Microservice-Based Architecture for Performance and Energy Benchmarking of Docker-Host Linux Distributions on Internet-of-Things Devices}, 
  year={2021},
  volume={1},
  number={},
  pages={705-711},
  abstract={Containers are rapidly being adopted in several areas of the information technology industry. A major area is edge and embedded Internet-of-Things systems. In this paper, we present a microservice-based architecture for performance analysis and energy consumption of Internet-of-Things "Docker host" Linux distributions. Our methodology builds on previous container benchmarking work, with analysis of performance metrics such as processing, memory, and disk throughput. Furthermore, our methodology introduces container-engine performance metrics related to container lifecycle operations, and concurrent container performance. We demonstrate by comparing four Linux distributions in this domain: BalenaOS, HypriotOS, RancherOS, and Raspbian Lite. All source code is provided.},
  keywords={},
  doi={10.1109/ICIT46573.2021.9453517},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7019405,
  author={Deepiga A S and Senthil Velan S and Babu, Chitra},
  booktitle={2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies}, 
  title={Empirical investigation of introducing Aspect Oriented Programming across versions of an SOA application}, 
  year={2014},
  volume={},
  number={},
  pages={1732-1739},
  abstract={Service Oriented Architecture (SOA) is an architectural style used to provide services to consumers that promotes loose coupling between services. The scattered and tangled functionalities modeled in an SOA application can be redesigned using Aspect Oriented Programming (AOP). This results in two sets of services, the first set having services for the base functionalities and the other modeling cross-cutting functionalities. During compilation, cross-cutting functionalities in the second set are woven to the functionalities modeled in the first set. By introducing AOP in SOA, the quality attributes such as re-usability, extendibility and maintainability can be improved. The objective of this paper is to perform an empirical investigation by quantitatively measuring the effect of introducing Aspect Orientation (AO) in SOA by developing with multiple versions of a given application. An AO based SOA application (University Automation System) for automating the functionalities of a typical University with multiple versions has been developed as an experimental test bed. An equivalent set of versions without introducing aspectization are also developed in parallel. The values of the AOP metrics are measured for the different versions of University Automation System both aspectized and unaspectized. The measured values show that the quality attributes namely maintainability, reusability and extendibility improve whereas the complexity of the application decreases during the evolution of the case study application.},
  keywords={},
  doi={10.1109/ICACCCT.2014.7019405},
  ISSN={},
  month={May},}

@INPROCEEDINGS{6830891,
  author={Alzahmi, Salwa Mohamed and Abu-Matar, Mohammad and Mizouni, Rabeb},
  booktitle={2014 IEEE 8th International Symposium on Service Oriented System Engineering}, 
  title={A Practical Tool for Automating Service Oriented Software Product Lines Derivation}, 
  year={2014},
  volume={},
  number={},
  pages={90-97},
  abstract={Service Oriented Architecture (SOA) is a business driven architecture that supports business strategies and goals. In enterprise systems, it offers flexibility for building IT solutions that can respond rapidly to changing business requirements and technology. The success of a service-oriented application implementation is measured by the level of flexibility, extendibility and customization in the provided services. In effect, it raises variability management concerns that require a good understanding of the business domain and a careful design of the application artifacts to cater for various service consumers' demands and requirements. Many approaches and frameworks have been proposed to realize variability in SOA by applying the concept of Software Product Lines (SPL) where services are the core assets and each member of the service-oriented product line is a possible assembly of those services. However, there are few tools that support these approaches and ease the derivation process of member applications taking into consideration the variability from different perspectives. In this paper we present a tool that facilitates the automatic derivation of SOA applications based on Model Driven Engineering (MDE) as an implementation methodology. The tool is based on the Multiple-Views Service-Oriented Product Line Variability approach. The tool architecture as well as its implemented modules is first described. Then, an example in the e-health domain is presented.},
  keywords={},
  doi={10.1109/SOSE.2014.16},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8009904,
  author={Arcuri, Andrea},
  booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={RESTful API Automated Test Case Generation}, 
  year={2017},
  volume={},
  number={},
  pages={9-20},
  abstract={Nowadays, web services play a major role in the development of enterprise applications. Many such applications are now developed using a service-oriented architecture (SOA), where microservices is one of its most popular kind. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, as inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, as the tested API is a remote service whose code is not available. In this paper, we consider testing from the point of view of the developers, which do have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault finding metrics. We implemented our technique in a tool called EVOMASTER, which is open-source. Experiments on two open-source, yet non-trivial RESTful services and an industrial one, do show that our novel technique did automatically find 38 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such approach are therefore discussed.},
  keywords={},
  doi={10.1109/QRS.2017.11},
  ISSN={},
  month={July},}

@INPROCEEDINGS{8595113,
  author={Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred},
  booktitle={2018 IEEE/ACM International Conference on Technical Debt (TechDebt)}, 
  title={Limiting Technical Debt with Maintainability Assurance – An Industry Survey on Used Techniques and Differences with Service- and Microservice-Based Systems}, 
  year={2018},
  volume={},
  number={},
  pages={125-133},
  abstract={Maintainability assurance techniques are used to control this quality attribute and limit the accumulation of potentially unknown technical debt. Since the industry state of practice and especially the handling of Service-and Microservice-Based Systems in this regard are not well covered in scientific literature, we created a survey to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related treatment of systems based on service-orientation, and c) influences on developer satisfaction w.r.t. maintainability. 60 software professionals responded to our online questionnaire. The results indicate that using explicit and systematic techniques has benefits for maintainability. The more sophisticated the applied methods the more satisfied participants were with the maintainability of their software while no link to a hindrance in productivity could be established. Other important findings were the absence of architecture-level evolvability control mechanisms as well as a significant neglect of service-oriented particularities for quality assurance. The results suggest that industry has to improve its quality control in these regards to avoid problems with long-living service-based software systems.},
  keywords={},
  doi={},
  ISSN={},
  month={May},}

@ARTICLE{9285284,
  author={Rumez, Marcel and Grimm, Daniel and Kriesten, Reiner and Sax, Eric},
  journal={IEEE Access}, 
  title={An Overview of Automotive Service-Oriented Architectures and Implications for Security Countermeasures}, 
  year={2020},
  volume={8},
  number={},
  pages={221852-221870},
  abstract={New requirements from the customers' and manufacturers' point of view such as adding new software functions during the product life cycle require a transformed architecture design for future vehicles. The paradigm of signal-oriented communication established for many years will increasingly be replaced by service-oriented approaches in order to increase the update and upgrade capability. In this article, we provide an overview of current protocols and communication patterns for automotive architectures based on the service-oriented architecture (SOA) paradigm and compare them with signal-oriented approaches. Resulting challenges and opportunities of SOAs with respect to information security are outlined and discussed. For this purpose, we explain different security countermeasures and present a state of the section of automotive approaches in the fields of firewalls, Intrusion Detection Systems (IDSs) and Identity and Access Management (IAM). Our final discussion is based on an exemplary hybrid architecture (signal- and service-oriented) and examines the adaptation of existing security measures as well as their specific security features.},
  keywords={},
  doi={10.1109/ACCESS.2020.3043070},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7959309,
  author={Zavvar, Mohammad and Garavand, Shole and Sabbagh, Esmaeel and Rezaei, Meysam and Khalili, Hajar and Zavvar, Mohammad Hossein and Motameni, Homayun},
  booktitle={2017 3th International Conference on Web Research (ICWR)}, 
  title={Measuring service quality in service-oriented architectures using a hybrid particle swarm optimization algorithm and artificial neural network (PSO-ANN)}, 
  year={2017},
  volume={},
  number={},
  pages={78-83},
  abstract={Web service combination is an important task performed in different phases of the service-oriented architecture lifecycle. Measuring service quality based on the non-functional characteristics is an exceedingly difficult task. Therefore, this paper presents a Multilayer Perceptron Artificial Neural Network (MLPANN) to provide a method for measuring quality of service in a service-oriented architecture. To improve network performance, Particle Swarm Optimization (PSO) is used to optimize the weights of the network. Finally, our results are compared to those of a combination of Different Evolution (DE) algorithm and MLPANN in terms of Mean Square Error (MSE), Root Mean Square Error (RMSE) and Standard Deviation (STD). The results demonstrate the superiority of the proposed method.},
  keywords={},
  doi={10.1109/ICWR.2017.7959309},
  ISSN={},
  month={April},}

@INPROCEEDINGS{9628703,
  author={du Plessis, Shani and Correia, Noélia},
  booktitle={2021 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS)}, 
  title={A Comparative Study of Software Architectures in Constrained Device IoT Deployments}, 
  year={2021},
  volume={},
  number={},
  pages={35-41},
  abstract={The Internet of Things (IoT) is an area that has consistently seen growth and development and will no doubt continue to do so. One group of IoT devices - constrained devices - has seen significant developments in recent years. With the advent of constrained devices in almost every area of life, e.g. industrial, leisure and medical, this group of devices is well worth studying. Clearly, resource management is a critical aspect to ensure optimal use of such devices. A number of factors can have a significant impact on resource management, such as the operating system and the software architecture.This study aimed to compare the power consumption, runtime performance and memory consumption of two software architectures: microservices and monolithic. The study was conducted using a constrained device, and to ensure that the results are not language-specific, three different programming languages were used: Go, Python and C++. It was found that, for smallscale applications, the monolithic architecture performed better across most metrics. These results may provide valuable insights to engineers for the design and implementation of constrained-device IoT applications. It was recommended that additional research be conducted on larger-scale applications.},
  keywords={},
  doi={10.1109/IoTaIS53735.2021.9628703},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8638056,
  author={Ahmed, Abdelmuttlib Ibrahim Abdalla and Khan, Suleman and Gani, Abdullah and Hamid, Siti Hafizah Ab and Guizani, Mohsen},
  booktitle={2018 IEEE 43rd Conference on Local Computer Networks (LCN)}, 
  title={Entropy-based Fuzzy AHP Model for Trustworthy Service Provider Selection in Internet of Things}, 
  year={2018},
  volume={},
  number={},
  pages={606-613},
  abstract={Nowadays, trust and reputation models are used to build a wide range of trust-based security mechanisms and trust-based service management applications on the Internet of Things (IoT). Considering trust as a single unit can result in missing important and significant factors. We split trust into its building-blocks, then we sort and assign weight to these building-blocks (trust metrics) on the basis of its priorities for the transaction context of a particular goal. To perform these processes, we consider trust as a multi-criteria decision-making problem, where a set of trust worthiness metrics represent the decision criteria. We introduce Entropy-based fuzzy analytic hierarchy process (EFAHP) as a trust model for selecting a trustworthy service provider, since the sense of decision making regarding multi-metrics trust is structural. EFAHP gives 1) fuzziness, which fits the vagueness, uncertainty, and subjectivity of trust attributes; 2) AHP, which is a systematic way for making decisions in complex multi-criteria decision making; and 3) entropy concept, which is utilized to calculate the aggregate weights for each service provider. We present a numerical illustration in trust-based Service Oriented Architecture in the IoT (SOA-IoT) to demonstrate the service provider selection using the EFAHP Model in assessing and aggregating the trust scores.},
  keywords={},
  doi={10.1109/LCN.2018.8638056},
  ISSN={0742-1303},
  month={Oct},}

@ARTICLE{6812231,
  author={Hertis, Matej and Juric, Matjaz B.},
  journal={IEEE Transactions on Software Engineering}, 
  title={An Empirical Analysis of Business Process Execution Language Usage}, 
  year={2014},
  volume={40},
  number={8},
  pages={738-757},
  abstract={The current state of executable business process languages allows for and demands optimization of design practices and specifications. In this paper, we present the first empirical study that analyses Web Services Business Process Execution Language (WS-BPEL or BPEL) usage and characteristics of real world executable business processes. We have analysed 1,145 BPEL processes by measuring activity usage and process complexity. In addition, we investigated the occurrence of activity usage patterns. The results revealed that the usage frequency of BPEL activities varies and that some activities have a strong co-occurrence. BPEL activities often appear in activity patterns that are repeated in multiple processes. Furthermore, the current process complexity metrics have proved to be inadequate for measuring BPEL process complexity. The empirical results provide fundamental knowledge on how BPEL specification and process design practices can be improved. We propose BPEL design guidelines and BPEL language improvements for the design of more understandable and less complex processes. The results are of interest to business process language designers, business process tool developers, business process designers and developers, and software engineering researchers, and contribute to the general understanding of BPEL and service-oriented architecture.},
  keywords={},
  doi={10.1109/TSE.2014.2322618},
  ISSN={1939-3520},
  month={Aug},}

@INPROCEEDINGS{8536110,
  author={Langermeier, Melanie and Bauer, Bernhard},
  booktitle={2018 IEEE 22nd International Enterprise Distributed Object Computing Workshop (EDOCW)}, 
  title={A Model-Based Method for the Evaluation of Project Proposal Compliance within EA Planning}, 
  year={2018},
  volume={},
  number={},
  pages={97-106},
  abstract={The business model and IT infrastructure of organizations is continually changing. Trends like microservices and digital transformation demand an adaption of the business models and IT infrastructure in order to stay competitive. It is important to ensure the compliance of these new projects with the current goals and principles. The discipline of Enterprise Architecture Planning provides methods for the structured development of the business and IT of an organization. In this paper we propose a tool-supported method for EA planning to evaluate to the project compliance based on established models. Different analyses are used to support the architect during project planning. Gap and impact analysis are used to ensure the change consistency. The compliance with the current strategy is finally evaluated with view generation and metric calculation. Foundation of the method is a generic generic analysis architecture execution environment (A2E), that provides us with the required flexibility to adapt to different needs and meta models. The method and the proposed analyses are evaluated within a case study from a medium-sized software product company.},
  keywords={},
  doi={10.1109/EDOCW.2018.00024},
  ISSN={2325-6605},
  month={Oct},}

@INPROCEEDINGS{9150502,
  author={Mahajan, Yash and Krishnaswamy, Dilip and Chelliah, Pethuru Raj},
  booktitle={2020 IEEE Conference on Technologies for Sustainability (SusTech)}, 
  title={MiSA - A System for a Microlending Service to Assist Edge Communities}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={In this paper, we propose a distributed edge+cloud system to assist with microlending services to communities, with machine learning catered to that specific community. A combination of technologies including microservices-based architecture and blockchain technology coupled with machine learning is utilized to provide microfinancing services to help sustain businesses in a local community, and to enable the community to grow into a thriving economy. To minimize the widespread expressed risk, in our prototype, the prediction of whether a loan will default or not is based on the various decision-enabling parameters and on any available information about the borrowers' past transaction as well as aggregate metrics related to the community that the borrower resides in. The authors hope that the suggested distributed edge+cloud architecture in the paper can be leveraged for other emerging sustainable edge applications as well.},
  keywords={},
  doi={10.1109/SusTech47890.2020.9150502},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8705779,
  author={Afwani, Royana and Irmawati, Budi and Jatmika, Andy Hidayat and Agitha, Nadiyasari},
  booktitle={2018 5th International Conference on Data and Software Engineering (ICoDSE)}, 
  title={Specialized Mobile Health Design Using the Open Group Architecture Framework (TOGAF): A Case Study in Child and Maternity Health Services Organization}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Mobile health applications are well known effective to provide education materials, receive personalized prompts, as a reminder system, and also create great impacts as early diagnose system and even facilitate a doctor to recommend treatments for patients in rural area as well as in the disaster area. E-health projects failed with the major problem was “no clear definition of the system requirements”. Another challenge for health organization that have specialized units are flexibility, easily expandable, and sustainability information system architecture to be integrated. Therefore, providing a good architecture design for build mobile health application is important. This research have done initial study to particular units in health care organization (Maternal and Child Health Services - PKIA), observation, site interview, and data collection. The research main phase are analyze and design TOGAF architecture for PKIA organization. TOGAF produced some tables and matrices to address detailed requirements in specialized mobile health services. The result from enterprise architecture than becomes reference for the design and development of mobile information system based on service oriented architecture and can be used on mobile devices for multiple platforms. For future work, we will create a model to map the diagrams and tables of enterprise architecture into specific software design, and work for detailed architecture validation using ALMA and object-oriented metrics.},
  keywords={},
  doi={10.1109/ICODSE.2018.8705779},
  ISSN={2640-0227},
  month={Nov},}

@INPROCEEDINGS{9172283,
  author={LaSorda, Maj Michael and Borky, John and Sega, Ron},
  booktitle={2020 IEEE Aerospace Conference}, 
  title={Model-Based Systems Architecting with Decision Quantification for Cybersecurity, Cost, and Performance}, 
  year={2020},
  volume={},
  number={},
  pages={1-13},
  abstract={The architecture selection process early in a major system acquisition is a critical step in determining the success of a program. There are recognized deficiencies that frequently occur in this step such as poor transparency into the final selection decision and excessive focus on lowest cost, which does not necessarily result in best value. This research investigates improvements to this process by integrating Model-Based Systems Engineering (MBSE) techniques; enforcing rigorous, quantitative evaluation metrics with a corresponding understanding of uncertainties; and eliciting stakeholder feedback in order to generate an architecture that is better optimized and trusted to provide improved value for the stakeholders. The proposed methodology presents a decision authority with an integrated assessment of architecture alternatives, to include expected performance evaluated against desired parameters with corresponding uncertainty distributions, and traceable to the concerns of the system's stakeholders. This thus enables a more informed and objective selection of the preferred alternative. We present a case study that analyzes the evaluation of a service-oriented architecture (SOA) providing satellite command and control with cyber security protections. This serves to define and demonstrate a new, more transparent and trusted architecture selection process, and the results show that it consistently achieves the desired improvements. Several excursions are also presented to show how rigorously capturing uncertainty could potentially lead to greater insights in architecture evaluation, which is a robust area for further investigation. The primary contribution of this research then is improved decision support to an architecture selection in the early phases of a system acquisition program.},
  keywords={},
  doi={10.1109/AERO47225.2020.9172283},
  ISSN={1095-323X},
  month={March},}

@INPROCEEDINGS{6917306,
  author={Zhang, Lili and Yu, Shusong and Ding, Xiangqian and Wang, Xiaodong},
  booktitle={2014 Sixth International Conference on Intelligent Human-Machine Systems and Cybernetics}, 
  title={Research on IOT RESTful Web Service Asynchronous Composition Based on BPEL}, 
  year={2014},
  volume={1},
  number={},
  pages={62-65},
  abstract={In recent years, The Internet of Things(IOT) is one of the hottest research topics. It was originally defined as connected all the things through the sensing devices to the Internet. In addition, Service-Oriented methodology has gradually drawn people's attention. Therefore, integrated The IOT with Service-Oriented methodology is very important. But now IOT service composition is mostly synchronous and service model is more complex. RESTful web services have been widely recognized and used because of their lightweight and succinct. RESTful web services introduce a new kind of abstraction, the resource, so that they are hard to compose using the Business Process Execution Language (BPEL). In order to compose asynchronous RESTful web services and make use of various IOT services, this paper proposes an asynchronous RESTful web service recursive measure, which is based on the BPEL extention. First, design the architecture of IOT RESTful web services, the architecture is divided into six layers so that it can integrate The IOT and RESTful web services effectively. Second, we show how to invoke the RESTful web services from the IOT and publish a BPEL process as a RESTful web service by extending BPEL. Finally, through an experiment to verify the correctness and validity of the proposed method in this paper.},
  keywords={},
  doi={10.1109/IHMSC.2014.23},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7153891,
  author={Abid, Kashif Sohail and Abid, Asif Sohail and Ansari, M. Mohsen},
  booktitle={2015 IEEE International Conference on Multimedia Big Data}, 
  title={A Better Approach for Conceptual Readability of WSDL}, 
  year={2015},
  volume={},
  number={},
  pages={260-263},
  abstract={Issues that concerns with the inter-operability on a heterogeneous environment can easily be address using the flexible platform of Service Oriented Architecture (SOA). Web service is an implementation and modeling of Service Oriented Architecture (SOA). Web service description language (WSDL)is a standard describing a web service in XML form. This Description can be categorized in two parts i.e. Structural and non-structural. The readability of a web service helps the consumer to understand it easily, it is suggested to provide sufficient details about functionality scope and limitation of scope in WSDL, so that it can easily be understandable. Readability depends upon interaction of two variables i.e. Text and reader. The maximum details about a web service could lead to it's reproduction by business competitor, and it may helps in maximizing vulnerabilities in it. This paper focuses on a technique for computing readability index by a detail analysis of WSDL document. This readability index obtain using this approach helps the producer of a web service to adjust readability, so that it can easily be understandable by consumer. The better readability index can also leads the provider to a better service discovery. To calculate Readability Index, extraction of WSDL file components was performed. After extraction of key concepts, they were mapped with the Domain Ontology. The words that were not mapped in the ontology, synonyms are employed by consulting the Word Net. Final readability was obtained using Simplified Dale Chall readability index (DaCw). The Web Service Readability can be measure more precisely by considering words that were not found in the mapping process.},
  keywords={},
  doi={10.1109/BigMM.2015.52},
  ISSN={},
  month={April},}

@INPROCEEDINGS{7060895,
  author={Mohamed, Merabet and Mohamed, Benslimane Sidi and El Amine Chergui, Mohamed},
  booktitle={2014 Second World Conference on Complex Systems (WCCS)}, 
  title={A hybrid particle swarm optimization for service identification from business process}, 
  year={2014},
  volume={},
  number={},
  pages={122-127},
  abstract={Service identification - as the first step of Service-Oriented Architecture -holds the main emphasis on the modeling process and has a broad influence on the system development. Selecting appropriate service identification method is essential for the prosperity of any service-oriented architecture project. Existing methods for service identification ignore the automation capability while providing human based prescriptive guidelines, which mostly are not applicable at enterprise scales. In this paper, we propose a top down approach to identify automatically services from business process. We use for clustering a hybrid particle swarm optimization algorithm and several design metrics for produce reusable services with proper granularity and acceptable level of cohesion and coupling. The experimental results show that our method HPSOSI (Hybrid Particle Swarm Algorithm for Service Identification) can achieve a high performance in terms of execution time and convergence speed.},
  keywords={},
  doi={10.1109/ICoCS.2014.7060895},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8537823,
  author={Saadaoui, Alaeddine and Scott, Stephen L.},
  booktitle={2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC)}, 
  title={Lightweight Web Services Migration Framework in Hybrid Clouds}, 
  year={2018},
  volume={},
  number={},
  pages={106-113},
  abstract={Service-oriented architectures allow the deployment of loosely coupled services that are platform independent. An enterprise can take advantage of service-oriented architecture in two different directions. On one side, the abstraction of technology implementation allows the deployment of web services in disparate systems. On the other side, the flexibility and independence of services from each other makes scalability easier to achieve. This paper presents a migration solution of web services in hybrid clouds. The adoption of hybrid cloud solutions is valuable for dynamic workloads to maintain the availability of web services during periods of spikes in demand. The migration solution is a lightweight framework composed of web services to manage cloud instances and the migration task of web services deployed on Java-based web containers. The peak management process is based on Java Management Extensions (JMX) technology to monitor resources and deployed web services. In addition, the framework dynamically integrates a set of JMX metrics to synchronize enterprise demand for resources with the migration process. Finally, a design of the framework prototype is described and a real case of CPU intensive web service is presented to test the migration process and show an improvement of CPU usage and execution time.},
  keywords={},
  doi={10.1109/CIC.2018.00025},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{7275851,
  author={Kumari, Smita and Rath, Santanu Kumar},
  booktitle={2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
  title={Performance comparison of SOAP and REST based Web Services for Enterprise Application Integration}, 
  year={2015},
  volume={},
  number={},
  pages={1656-1660},
  abstract={Web Services are common means to exchange data and information over the network. Web Services make themselves available over the internet, where technology and platform are independent. Once web services are built it is accessed via uniform resource locator (URL) and their functionalities can be utilized in the application domain. Web services are self-contained, modular, distributed and dynamic in nature. These web services are described and then published in Service Registry e.g., UDDI and then they are invoked over the Internet. Web Services are basic Building blocks of Services Oriented Architecture (SOA). These web services can be developed based on two interaction styles such as Simple Object Access Protocol (SOAP) and Representational State Transfer Protocol (REST). It is important to select appropriate interaction styles i.e., either SOAP or REST for building Web Sevices. Choosing service interaction style is an important architectural decision for designers and developers, as it influences the underlying requirements for implementing web service solutions. In this study, the performance of application of web services for Enterprise Application Integration (EAI) based on SOAP and REST is compared. Since web services operate over network throughput and response time are considered as a metrics parameter for evaluation.},
  keywords={},
  doi={10.1109/ICACCI.2015.7275851},
  ISSN={},
  month={Aug},}

@ARTICLE{6517184,
  author={Bianchini, Devis and Cappiello, Cinzia and De Antonellis, Valeria and Pernici, Barbara},
  journal={IEEE Transactions on Services Computing}, 
  title={Service Identification in Interorganizational Process Design}, 
  year={2014},
  volume={7},
  number={2},
  pages={265-278},
  abstract={Service identification is one of the main phases in the design of a service-oriented application. The way in which services are identified may influence the effectiveness of the SOA architecture. More specifically, the granularity of the services is very important in reaching flexibility and reusing them. Such properties are crucial in interorganizational interactions based on collaborative business processes. In fact, collaboration is facilitated by ensuring a homogeneous description of services at the right level of granularity. In this paper, we provide a detailed description of P2S (Process-to-Services), a computer-aided methodology to enable the identification of services that compose a collaborative business process. The methodology is based on metrics defined to setup service granularity, cohesion, coupling, and reuse. A prototype tool based on the methodology is also described with reference to a real case scenario.},
  keywords={},
  doi={10.1109/TSC.2013.26},
  ISSN={1939-1374},
  month={April},}

@INPROCEEDINGS{8621924,
  author={Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example}, 
  year={2018},
  volume={},
  number={},
  pages={5328-5329},
  abstract={Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.},
  keywords={},
  doi={10.1109/BigData.2018.8621924},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8372741,
  author={Al-Shammari, Haider Qays and Lawey, Ahmed and El-Gorashi, Taisir and Elmirghani, Jaafar M. H.},
  booktitle={2018 27th Wireless and Optical Communication Conference (WOCC)}, 
  title={Energy efficient service embedding in IoT networks}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={The Internet of Things (IoT) is anticipated to participate in performing diverse and complex tasks in the near future. IoT objects capable of handling multiple sensing and actuating functions will be the corner stone of future IoT systems in smart cities. In this paper, we present an energy efficient service embedding framework in IoT network by using mixed integer linear programming (MILP). This framework addresses a set of metrics such as scalability, flexible resource allocation, cost reduction, and efficient use of resources. We consider the event-driven paradigm of Service Oriented Architecture (SOA) in our framework in order to provide service abstraction of basic services which can be composed into complex services and exploited by the upper application layer. The results show that our optimized network can save an average of 27% and 36% of the processing and network power consumption, respectively, compared to an energy unaware service embedding scheme.},
  keywords={},
  doi={10.1109/WOCC.2018.8372741},
  ISSN={2379-1276},
  month={April},}

@INPROCEEDINGS{7396198,
  author={Chituc, Claudia-Melania},
  booktitle={2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)}, 
  title={Towards a Methodology for Trade-off Analysis in a Multi-cloud Environment Considering Monitored QoS Metrics and Economic Performance Assessment Results}, 
  year={2015},
  volume={},
  number={},
  pages={479-482},
  abstract={Cloud computing and service-oriented computing brought new opportunities for companies. However, numerous challenges, (e.g., related to application design and deployment, service monitoring) are associated with the cloud and provisioned services. Complex SLAs need to be established and monitored. Current approaches do not sufficiently address the challenges of QoS monitoring in multi-cloud environments in a holistic manner, tackling mainly technical aspects. This paper presents an on-going research project towards the development of a methodology for a trade-off analysis in a multi-cloud environment considering monitored QoS metrics and economic performance assessment results. The research methodology followed and partial results are presented, and directions for future work are discussed. Based on the needs identified, an architecture for SLA monitoring and dynamic runtime adaptations in multi-cloud environments is proposed, tackling technical and business-economic aspects.},
  keywords={},
  doi={10.1109/CloudCom.2015.87},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7068197,
  author={Thirumaran, M. and Jannani, M.},
  booktitle={Proceedings of IEEE International Conference on Computer Communication and Systems ICCCS14}, 
  title={Theoretical foundation to evaluate the change measures for an effective web service change management}, 
  year={2014},
  volume={},
  number={},
  pages={226-232},
  abstract={With the advent in the need for a cost effective and efficient solution which supports the evolution and enhancement of the Enterprise Information Systems, the adoption of Service Oriented Architectures (SOAs) for the automation of business processes and the integration of IT systems is increasing. These SOAs rely on web service standards for the implementation of service invocations across machine boundaries. Web services are software systems designed to support interoperable machine-to-machine interaction over a network. This interoperability is gained through a set of XML-based open standards. These standards provide a common approach for defining, publishing, and using web services. However after a product is introduced in the market, its successful growth against the competitors depends critically on the company's ability to rapidly improve and extend its product in response to customer feedback. These changes must be reflected accordingly in the web service without injecting any disputes. Hence an effective web service Change Management with appropriate change measures is very essential. This paper focuses on such change measures for an effective change management.},
  keywords={},
  doi={10.1109/ICCCS.2014.7068197},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{8259711,
  author={Fethallah, Hadjila and Ismail, Smahi Mohamed and Mohamed, Merzoug and Zeyneb, Torchane},
  booktitle={2017 International Conference on Mathematics and Information Technology (ICMIT)}, 
  title={An outranking model for web service discovery}, 
  year={2017},
  volume={},
  number={},
  pages={162-167},
  abstract={The web service discovery is the cornerstone of the service oriented architecture. To solve this issue, we usually leverage a matching model as well as the operation signature in order to minimize the residual errors. In this paper, we resolve this problem by combining a set of similarity measures through the use of a majority voting model called “outranking”. The Experimental evaluation confirms that this model performs better than the well-known Borda and all input similarity measures.},
  keywords={},
  doi={10.1109/MATHIT.2017.8259711},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{6901158,
  author={Mcheick, Hamid and Mohammad, Atif Farid},
  booktitle={2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
  title={The evident use of evidence theory in big data analytics using cloud computing}, 
  year={2014},
  volume={},
  number={},
  pages={1-6},
  abstract={We live in the world of evidence. This research survey comprises of several research works and has an example implying dempster-shafer theory of evidence. We have witnessed several advances in computational performance, which have brought us the design and development of high-performance computing simulation tools. It is a fact that we have to account for uncertainty, while generating such high-performance systems using such simulation tools can fail in service performance predictions. We have seen that evidence theory is utilized to measure uncertainty in terms of the uncertain measures of belief and plausibility. It is also witnessed in computing community that Cloud computing has provided a flexible and scalable infrastructures to grow beyond contemporary borders to the organizations as wells the users everyday use of services. It also has increased availability of high-performance computing applications to small/ medium-sized businesses as well as academic users to work with. This paper also sheds light on Cloud computing and Service-Oriented Architecture.},
  keywords={},
  doi={10.1109/CCECE.2014.6901158},
  ISSN={0840-7789},
  month={May},}

@INPROCEEDINGS{9217849,
  author={Gehrmann, Tobias and Duplys, Paul},
  booktitle={2020 23rd Euromicro Conference on Digital System Design (DSD)}, 
  title={Intrusion Detection for SOME/IP: Challenges and Opportunities}, 
  year={2020},
  volume={},
  number={},
  pages={583-587},
  abstract={Due to ever increasing complexity and the introduction of more and more connectivity, modern cars have an ever growing attack surface. To cope with this, intrusion detection should be used as an additional layer of defense complementing dedicated security measures. There is, however, very little published work on intrusion detection in cars, in particular for service-oriented communication. In this short paper, we first discuss selected challenges and opportunities for intrusion detection in SOME/IP, a standard protocol for service-oriented communication in cars. We then propose an architecture for a SOME/IP intrusion detection system, discuss its security properties and report preliminary experimental results.},
  keywords={},
  doi={10.1109/DSD51259.2020.00096},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8377634,
  author={Kumar, Lov and Sureka, Ashish},
  booktitle={2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)}, 
  title={An Empirical Analysis on Web Service Anti-pattern Detection Using a Machine Learning Framework}, 
  year={2018},
  volume={01},
  number={},
  pages={2-11},
  abstract={Web Services are application components characterised by interoperability, extensibility, distributed application development and service oriented architecture. A complex distributed application can be developed by combing several third-party web-services. Anti-patterns are counter-productive and poor design and practices. Web-services suffer from a multitude of anti-patterns such as God object Web service and Fine grained Web service. Our work is motivated by the need to build techniques for automatically detecting common web-services anti-patterns by static analysis of the source code implementing a web-service. Our approach is based on the premise that summary values of object oriented source code metrics computed at a web-service level can be used as a predictor for anti-patterns. We present an empirical analysis of 4 data sampling techniques to encounter the class imbalance problem, 5 feature ranking techniques to identify the most informative and relevant features and 8 machine learning algorithms for predicting 5 different types of anti-patterns on 226 real-world web-services across several domains. We conclude that it is possible to predict anti-patterns using source code metrics and a machine learning framework. Our analysis reveals that the best performing classification algorithm is Random Forest, best performing data sampling technique is SMOTE and the best performing feature ranking method is OneR.},
  keywords={},
  doi={10.1109/COMPSAC.2018.00010},
  ISSN={0730-3157},
  month={July},}

@ARTICLE{8726136,
  author={Jin, Hai and Li, Zhi and Zou, Deqing and Yuan, Bin},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={DSEOM: A Framework for Dynamic Security Evaluation and Optimization of MTD in Container-Based Cloud}, 
  year={2021},
  volume={18},
  number={3},
  pages={1125-1136},
  abstract={Due to the lightweight features, the combination of container technology and microservice architecture makes container-based cloud environment more efficient and agile than VM-based cloud environment. However, it also greatly amplifies the dynamism and complexity of the cloud environment and increases the uncertainty of security issues in the system concurrently. In this case, the effectiveness of defense mechanisms with fixed strategies would fluctuate as the updates occur in cloud environment. We refer this problem as effectiveness drift problem of defense mechanisms, which is particularly acute in the proactive defense mechanisms, such as moving target defense (MTD). To tackle this problem, we present DSEOM, a framework that can automatically perceive updates of container-based cloud environment, rapidly evaluate the effectiveness change of MTD and dynamically optimize MTD strategies. Specifically, we establish a multi-dimensional attack graphs model to formalize various complex attack scenarios. Combining with this model, we introduce the concept of betweenness centrality to effectively evaluate and optimize the implementation strategies of MTD. In addition, we present a series of security and performance metrics to quantify the effectiveness of MTD strategies in DSEOM. And we conduct extensive experiments to illustrate the existence of the effectiveness drift problem and demonstrate the usability and scalability of DSEOM.},
  keywords={},
  doi={10.1109/TDSC.2019.2916666},
  ISSN={1941-0018},
  month={May},}

@ARTICLE{9036958,
  author={Herrera, José and Moltó, Germán},
  journal={IEEE Access}, 
  title={Toward Bio-Inspired Auto-Scaling Algorithms: An Elasticity Approach for Container Orchestration Platforms}, 
  year={2020},
  volume={8},
  number={},
  pages={52139-52150},
  abstract={The wide adoption of microservices architectures has introduced an unprecedented granularisation of computing that requires the coordinated execution of multiple containers with diverse lifetimes and with potentially different auto-scaling requirements. These applications are managed by means of container orchestration platforms and existing centralised approaches for auto-scaling face challenges when used for the timely adaptation of the elasticity required for the different application components. This paper studies the impact of integrating bio-inspired approaches for dynamic distributed auto-scaling on container orchestration platforms. With a focus on running self-managed containers, we compare alternative configuration options for the container life cycle. The performance of the proposed models is validated through simulations subjected to both synthetic and real-world workloads. Also, multiple scaling options are assessed with the purpose of identifying exceptional cases and improvement areas. Furthermore, a nontraditional metric for scaling measurement is introduced to substitute classic analytical approaches. We found out connections for two related worlds (biological systems and software container elasticity procedures) and we open a new research area in software containers that features potential self-guided container elasticity activities.},
  keywords={},
  doi={10.1109/ACCESS.2020.2980852},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9251052,
  author={Kesim, Dominik and van Hoorn, André and Frank, Sebastian and Häussler, Matthias},
  booktitle={2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)}, 
  title={Identifying and Prioritizing Chaos Experiments by Using Established Risk Analysis Techniques}, 
  year={2020},
  volume={},
  number={},
  pages={229-240},
  abstract={The prevalence of microservice architectures and container orchestration technologies increases the complexity of assessing such systems' resilience. Chaos engineering is an emerging approach for resilience assessment by testing hypotheses after intentionally injecting faults into a distributed system and observing customer- and business-affecting metrics. As the number of potential risks within a complex system is high, the identification and prioritization of effective and efficient chaos experiments are non-trivial. In the scope of an industrial case study, this work investigates means to identify and prioritize chaos experiments by using established risk analysis techniques known from engineering safety-critical systems, namely i) Fault Tree Analysis, ii) Failure Mode and Effects Analysis, iii) and Computer Hazard and Operability Study. We conducted semi-structured interviews to elicit architectural information and resilience requirements of the case study system. The extracted knowledge was leveraged during the application of the risk analysis techniques. A subset of the identified and prioritized risks was used to create and execute chaos experiments. The risk analysis resulted in over 100 findings and revealed that the system is rather fragile as it comprises a high amount of single points of failure. The chaos experiments revealed further weaknesses for formerly unknown system behavior.},
  keywords={},
  doi={10.1109/ISSRE5003.2020.00030},
  ISSN={2332-6549},
  month={Oct},}

@INPROCEEDINGS{8548336,
  author={Filipe, Ricardo and Correia, Jaime and Araujo, Filipe and Cardoso, Jorge},
  booktitle={2018 IEEE 17th International Symposium on Network Computing and Applications (NCA)}, 
  title={On Black-Box Monitoring Techniques for Multi-Component Services}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={Despite the advantages of microservice and function-oriented architectures, there is an increase in complexity to monitor such highly dynamic systems. In this paper, we analyze two distinct methods to tackle the monitoring problem in a system with reduced instrumentation. Our goal is to understand the feasibility of such approach with one specific driver: simplicity. We aim to determine the extent to which it is possible to characterize the state of two generic tandem processes, using as little information as possible. To answer this question, we resorted to a simulation approach. Using a queue system, we simulated two services, that we could manipulate with distinct operation sets for each module. We used the total response time seen upstream of the system. Having this setup and metric, we applied two distinct methods to analyze the results. First, we used supervised machine learning algorithms to identify where the bottleneck is happening. Secondly, we used an exponential decomposition to identify the occupation in the two components in a more black-box fashion. Results show that both methodologies have their advantages and limitations. The separation of the signal more accurately identifies occupation in low occupied resources, but when a service is totally dominating the overall time, it lacks precision. The machine learning has a more stable error, but needs the training set. This study suggest that a black-box occupation approach with both techniques is possible and very useful.},
  keywords={},
  doi={10.1109/NCA.2018.8548336},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8076798,
  author={Sekar, K. R. and Sethuraman, J. and Srinivasan, Manav and Ravichandran, K.S. and Manikandan, R.},
  booktitle={2017 International Conference on Networks & Advances in Computational Technologies (NetACT)}, 
  title={Concurrent classifier based analysis for climate prediction using service oriented architecture}, 
  year={2017},
  volume={},
  number={},
  pages={370-375},
  abstract={Climate prediction is the essential one for the unforeseen world and reduces the uncertainty. Many research articles are available in bountiful in research arena. In this work the climate prediction will be obtained through concurrent classifiers, usually pronounced as ensemble classifier. Using `N' number of weather forecasting, web sites the training set well called semantic has formulated with a worthy attributes. The interface has created using the concept of Service oriented Architecture (SoA), so that to provide rooms for other applications can also integrated in the future trend. Using the climate prediction, what are the remedial measures to be taken and estimating their budget cost planning can be the one another good application to integrate with the existing application. The homogeneous property of the application can also be verified while integrating with the existing applications. SoA architecture needs umpteen number of services, to accomplish that factor, software components and web services are plays a important role. In the heterogeneous environment the weather forecasting is inevitable for the meter logical department to predict the season of the day.},
  keywords={},
  doi={10.1109/NETACT.2017.8076798},
  ISSN={},
  month={July},}

@INPROCEEDINGS{7030325,
  author={Musavi, Maryam and Pasha, Mohammad Reza and Hamzehnia, Mahnaz and Hoseini, Mahyar},
  booktitle={2014 6th Conference on Information and Knowledge Technology (IKT)}, 
  title={A QoS-based fuzzy model for evaluation service quality parameters in service-oriented architecture}, 
  year={2014},
  volume={},
  number={},
  pages={15-19},
  abstract={Nowadays, service-oriented architecture is developed as a flexible architecture for developing dynamic systems. In consider to the importance of quality of service (QoS), measured parameters in this architectural services such as security, reliability and ... has a special place. Uncertainty of parameters affect service quality is considered as a key challenge in such environments. This requiring measurement of these parameters reveals a consistent and efficient manner. Since fuzzy logic is able to express the relative value of the credit in real-world concepts, this article proposes an approach on fuzzy logic to deal with these challenges and evaluation of quality of service.},
  keywords={},
  doi={10.1109/IKT.2014.7030325},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8987957,
  author={Kumar, T Sathis and Latha, K},
  booktitle={2019 International Conference on Smart Systems and Inventive Technology (ICSSIT)}, 
  title={Interoperability Performance in Adaptive Middleware for Enterprise Business Applications}, 
  year={2019},
  volume={},
  number={},
  pages={652-656},
  abstract={To improve the presentation of B2B (Business to Business) and B2C (Business to Consumer) regarding venture wide Service Oriented Architecture (SOA), we need middleware interoperability particularly with agent building to be specific CORBA (Common Object Request Broker Architecture) proposed by Object Management Group ORB programming named ORBeline. Unmistakable models for client server correspondences have just been created and executed specifically Handle Driven ORB (H-ORB), Forwarding ORB (F-ORB), and the Adaptive ORB (A- ORB). This paper concentrates how to improve the presentation of the interoperability in Adaptive ORB (A-ORB) as for client server collaboration in N-level engineering alongside multithreading condition. We have presented a strategy called linear discriminant interoperable support learning method and how it will in general be used for improving the presentation of interoperability is examined. The outcome gives the framework conduct especially the impact of message measure, between hub deferrals; torpidity and flexibility of solicitation/reaction administration times for the A-ORB engineering are broke down.},
  keywords={},
  doi={10.1109/ICSSIT46314.2019.8987957},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7495852,
  author={Pinarer, Ozgun and Gripay, Yann and Servigne, Sylvie and Ozgovde, Atay},
  booktitle={2016 24th Signal Processing and Communication Application Conference (SIU)}, 
  title={Real-time multi-application based sensor flux management}, 
  year={2016},
  volume={},
  number={},
  pages={765-768},
  abstract={Smart building management systems become very popular research topics due to high energy consumption of buildings in developed countries. Proposed approaches in the literature commonly focus on smart building energy management systems to improve this high consumption and on network communications between deployed devices. However, these approaches are specialized for a single monitoring application, and adopt static wireless sensor device configurations. In this study, we focus on the energy and lifetime of the monitoring architecture itself. We consider a monitoring system as a set of applications that exploit sensor measures in real-time, where these applications are declaratively expressed as (service-oriented) continuous queries over sensor data streams. We tackle the optimization of interactions between application real-time requirements for data and wireless sensor devices that produces those data. In this context, we present a novel approach, an energy-aware dynamic sensor configuration mechanism for a sustainable declarative monitoring architecture that can support multiple applications. We first introduce formalization of application requirements and sensor configuration based on data acquisition/transmission and continuous stream queries. We then propose a self-adaptive energy-aware algorithm that dynamically generates optimized sensor configurations based on real-time query requirements. We also present a Smart-Service Stream-oriented Sensor Management (3SoSM) Gateway that optimizes sensor configurations and manages sensor data streams. Finally, we present a set of experiments we conducted with a wireless sensor network simulator and with a real Smart Building platform.},
  keywords={},
  doi={10.1109/SIU.2016.7495852},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9622915,
  author={Huang, Pei-Shu and Fahmi, Faisal and Wang, Feng-Jian and Yang, Hongji},
  booktitle={2021 8th International Conference on Dependable Systems and Their Applications (DSA)}, 
  title={Constructing A Creative Service Software with Semantic Web}, 
  year={2021},
  volume={},
  number={},
  pages={499-507},
  abstract={In software development, Service Oriented Architecture (SOA) and creative computing can be adopted to utilize multiple-domain knowledges to construct service software possessing creative properties, i.e., novel, useful, and surprising. In the past, several theoretical evaluation metrics have been proposed to measure creativity of a software system. However, a systematic practical method to construct creative service software is rarely considered in current researches. In this paper, we propose a model for creative service software development based on semantic web, which is applied in two phases: domain-creative requirement specification and semantic-based service design. The model can reduce communication work between domain experts and software engineers, improve traceability of the specifications, and improve machine readability during the generation of creativity. After the model of service design is validated for completeness and consistency, the creative service software is well-designed and can be implemented and reused effectively without losing of creativity.},
  keywords={},
  doi={10.1109/DSA52907.2021.00074},
  ISSN={2767-6684},
  month={Aug},}

@INPROCEEDINGS{7829931,
  author={Garusinghe, Asanka and Perera, Indika and Meedeniya, Dulani},
  booktitle={2016 Sixteenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
  title={Managing Service Level Agreements in Service Oriented Product Lines}, 
  year={2016},
  volume={},
  number={},
  pages={274-280},
  abstract={Service Oriented Architecture (SOA) and Software Product Line (SPL) have individually proven to be Software Engineering concepts, which are creating values for developing software systems. While SOA is being used for developing applications from an orchestration of web services, SPL has ability to prepare core sets of assets and manage with variable components. The combination of SOA and SPL has highlighted the term of Service Oriented Product Line (SOPL) which is setting up the application to manage common parts and reuse them without developing from scratch. It helps to manage service component bundles dynamically according to identified commonalities and variabilities. In this paper, we present our implementation approach of SOPL and manage Service Level Agreements (SLAs) in such environments by monitoring Quality of Service (QoS) attributes in bundles of web service components. The designing and developing service bundles for representing core sets of assets in SOPL are followed by the initial feature based analysis and identification of service components. Then, the managing SLAs is handled by detecting the deviation between actual and acceptable pre-defined QoS metrics values in previously analysed web service components via Web Service Level Agreement (WSLA) language specified templates.},
  keywords={},
  doi={10.1109/ICTER.2016.7829931},
  ISSN={2472-7598},
  month={Sep.},}

@INPROCEEDINGS{8118424,
  author={Torkura, Kennedy A. and Sukmana, Muhammad I.H. and Cheng, Feng and Meinel, Christoph},
  booktitle={2017 IEEE International Conference on Smart Cloud (SmartCloud)}, 
  title={Leveraging Cloud Native Design Patterns for Security-as-a-Service Applications}, 
  year={2017},
  volume={},
  number={},
  pages={90-97},
  abstract={This paper discusses a new approach for designing and deploying Security-as-a-Service (SecaaS) applications using cloud native design patterns. Current SecaaS approaches do not efficiently handle the increasing threats to computer systems and applications. For example, requests for security assessments drastically increase after a high-risk security vulnerability is disclosed. In such scenarios, SecaaS applications are unable to dynamically scale to serve requests. A root cause of this challenge is employment of architectures not specifically fitted to cloud environments. Cloud native design patterns resolve this challenge by enabling certain properties e.g. massive scalability and resiliency via the combination of microservice patterns and cloud-focused design patterns. However adopting these patterns is a complex process, during which several security issues are introduced. In this work, we investigate these security issues, we redesign and deploy a monolithic SecaaS application using cloud native design patterns while considering appropriate, layered security counter-measures i.e. at the application and cloud networking layer. Our prototype implementation out-performs traditional, monolithic applications with an average Scanner Time of 6 minutes, without compromising security. Our approach can be employed for designing secure, scalable and performant SecaaS applications that effectively handle unexpected increase in security assessment requests.},
  keywords={},
  doi={10.1109/SmartCloud.2017.21},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9006025,
  author={De Iasio, Antonio and Furno, Angelo and Goglia, Lorenzo and Zimeo, Eugenio},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)}, 
  title={A Microservices Platform for Monitoring and Analysis of IoT Traffic Data in Smart Cities}, 
  year={2019},
  volume={},
  number={},
  pages={5223-5232},
  abstract={The ongoing digitization of cities, enabled by the diffusion of interconnected sensors and devices, makes it possible to continuously collect and analyze huge streams of data at extremely large spatio-temporal scales and fine resolutions. These data can be used to monitor, detect and anticipate different kinds of infrastructure vulnerabilities and anomalies, as well as to implement more personalized services that could improve citizens' life. In this new context, full of opportunities, it is difficult to foresee and develop, in advance, the set of applications and services that can be potentially useful for administrators and citizens to solve the manifold compelling needs a city may have to face. Novel ICT paradigms and technologies can help designing agile, general-purpose smart city platforms aimed at supporting the collection and treatment of large-scale, multi-source (streams of) data and the development of novel applications that could fulfill diverse functional requirements under strict non-functional constraints. This paper presents the reference architecture, a prototype implementation and a city-scale case-study evaluation of PROMENADE, a platform that exploits IoT/Fog/Cloud paradigms, microservices and DevOps infrastructures to guarantee continuous development of robust and reliable applications for real-time monitoring and analysis of traffic data generated by IoT devices in large smart cities. The prototype has been evaluated in a case study concerning the quasi real-time detection of road networks vulnerabilities via centrality measures from on-line traffic conditions, emulated from off-line real datasets available for the city of Lyon, France.},
  keywords={},
  doi={10.1109/BigData47090.2019.9006025},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8089863,
  author={Serth, Sebastian and Podlesny, Nikolai and Bornstein, Marvin and Lindemann, Jan and Latt, Johanna and Selke, Jan and Schlosser, Rainer and Boissier, Martin and Uflacker, Matthias},
  booktitle={2017 IEEE 21st International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={An Interactive Platform to Simulate Dynamic Pricing Competition on Online Marketplaces}, 
  year={2017},
  volume={},
  number={},
  pages={61-66},
  abstract={E-commerce marketplaces are highly dynamic with constant competition. While this competition is challenging for many merchants, it also provides plenty of opportunities, e.g., by allowing them to automatically adjust prices in order to react to changing market situations. For practitioners however, testing automated pricing strategies is time-consuming and potentially hazardously when done in production. Researchers, on the other side, struggle to study how pricing strategies interact under heavy competition. As a consequence, we built an open continuous time framework to simulate dynamic pricing competition called Price Wars. The microservice-based architecture provides a scalable platform for large competitions with dozens of merchants and a large random stream of consumers. Our platform stores each event in a distributed log. This allows to provide different performance measures enabling users to compare profit and revenue of various repricing strategies in real-time. For researchers, price trajectories are shown which ease evaluating mutual price reactions of competing strategies. Furthermore, merchants can access historical marketplace data and apply machine learning. By providing a set of customizable, artificial merchants, users can easily simulate both simple rule-based strategies as well as sophisticated data-driven strategies using demand learning to optimize their pricing strategies.},
  keywords={},
  doi={10.1109/EDOC.2017.17},
  ISSN={2325-6362},
  month={Oct},}

@INPROCEEDINGS{9209674,
  author={Li, Zhuo and Cao, Jiannong and Liu, Xiulong and Zhang, Jiuwu and Hu, Haoyuan and Yao, Didi},
  booktitle={2020 29th International Conference on Computer Communications and Networks (ICCCN)}, 
  title={A Self-Adaptive Bluetooth Indoor Localization System using LSTM-based Distance Estimator}, 
  year={2020},
  volume={},
  number={},
  pages={1-9},
  abstract={In recent years, there is an increasing demand for indoor localization services with the aim to locate people and objects inside buildings. However, localization accuracy is susceptible to inaccurate and high variant sensor measurements due to the unpredictable fluctuations of received wireless signals and the sensitivity of hardware devices. To address this issue, in this paper, we establish a new Bluetooth indoor localization system, whose architecture can be basically decomposed into two parts: the internet-of-things (IoT) framework and the localization module. Concretely, the IoT platform uses the state-of-the-art light weight Spring Boot microservice framework consisting of multi-layer structure. In the localization module, it follows the general process of trilateration but significantly distinguished from it. A set of measures are adopted to strengthen the system's robustness when obtained measurements cannot be fully trusted. Specifically, in the first place, rather than using conventional propagation model to predict the distance between Bluetooth transmitter and receiver, we design a bran-new LSTM-based distance estimator which can better depict the nonlinearity of attenuation characteristics of radio signal. Moreover, we also employ a series of self-adaptive mechanisms, including elastic radius intersecting, multiple weighted centroid localization and self-adaptive Kalman tracking, to make the system robust against inaccurate measurements and unpredictable sudden variation of received wireless signal. A bunch of tests are conducted in both ideal lab environment and Alibaba's large-scale warehouse, and experimental results show our indoor localization system outperforms the state-of-the-art benchmarks by a large margin in both localization accuracy and stability.},
  keywords={},
  doi={10.1109/ICCCN49398.2020.9209674},
  ISSN={2637-9430},
  month={Aug},}

@INPROCEEDINGS{9470894,
  author={Andersen, Nicklas Sindlev and Chiarandini, Marco and Mauro, Jacopo},
  booktitle={2021 IEEE/ACM 3rd International Workshop on Software Engineering for Healthcare (SEH)}, 
  title={Wandering and getting lost: the architecture of an app activating local communities on dementia issues}, 
  year={2021},
  volume={},
  number={},
  pages={36-43},
  abstract={We describe the architecture of Sammen Om Demens (SOD), an application for portable devices aiming at helping persons with dementia when wandering and getting lost through the involvement of caregivers, family members, and ordinary citizens who volunteer.To enable the real-time detection of a person with dementia that has lost orientation, we transfer location data at high frequency from a frontend on the smartphone of a person with dementia to a backend system. The backend system must be able to cope with the high throughput data and carry out possibly heavy computations for the detection of anomalous behavior via artificial intelligence techniques. This sets certain performance and architectural requirements on the design of the backend.In the paper, we discuss our design and implementation choices for the backend of SOD that involve microservices and serverless services to achieve efficiency and scalability. We give evidence of the achieved goals by deploying the SOD backend on a public cloud and measuring the performance on simulated load tests.},
  keywords={},
  doi={10.1109/SEH52539.2021.00014},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9659766,
  author={&#x00DC;nl&#x00FC;, H&#x00FC;seyin and Hacalo&#x011F;lu, Tuna and Leblebici, Onur and Demir&#x00F6;rs, Onur},
  booktitle={2021 15th Turkish National Software Engineering Symposium (UYMS)}, 
  title={Effort Prediction for Microservices: A Case Study}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Software size measurement is critical as an input to perform important project management processes such as effort, cost and schedule estimation. Functional size measurement (FSM) methods are beneficial in terms of being applicable in the early phases of the software life cycle over functional requirements and providing a systematic and repeatable method. However, in agile organizations, it can be challenging to seperate measurement components of FSM methods from requirements in the early phases as the documentation is kept to a minimum compared to traditional methods such as the Waterfall Model and is detailed as the project steps. In addition, the existing FSM methods are not fully compatible with today&#x0027;s architectural structures, which are from being data-driven and to evolve into a behaviour-oriented structure. In this study, we performed a case study which includes a project developed with agile methods and using microservice-based architecture to compare the effectiveness of COSMIC FSM and event-based software size measurement. For this purpose, we measured the size of the project and created effort estimation models based on two methods. The measurers had difficulty in applying both methods due to the limited detail level of the requirements in the project. However, the event-based method was found to estimate effort with less error than the COSMIC FSM method.},
  keywords={},
  doi={10.1109/UYMS54260.2021.9659766},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{6945146,
  author={Triantafyllidis, Andreas K. and Koutkias, Vassilis G. and Chouvarda, Ioanna and Maglaveras, Nicos},
  booktitle={2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 
  title={Development and usability of a personalized sensor-based system for pervasive healthcare}, 
  year={2014},
  volume={},
  number={},
  pages={6623-6626},
  abstract={Although a plethora of remote health monitoring systems have been proposed for chronic conditions, the challenge posed by the changing patient needs and the requirement for personalization in health monitoring to move beyond proprietary, difficult to extend, and unsustainable solutions still pertains. In this direction, we describe a mobile health system based on a smartphone, portable/wearable sensors for measuring the patient's physiological parameters, and back-end platforms for the health professionals to monitor the patient condition and configure monitoring plans in an individualized manner. A prototype system was developed based on a Service-oriented Architecture and integrating commercially available sensing devices. An experimental study has been conducted with 53 patients in order to investigate the usability of the proposed system. The patients were able to perform the majority of the target tasks successfully (Success Rate = 77%), while the perceived usability using the System Usability Scale (SUS) was found to be above average (SUS score = 73%), indicating that the patients overall perceived the system as both easy to use and useful.},
  keywords={},
  doi={10.1109/EMBC.2014.6945146},
  ISSN={1558-4615},
  month={Aug},}

@INPROCEEDINGS{7013150,
  author={Wijayanto, Arie Wahyu and Suhardi},
  booktitle={2014 International Conference on ICT For Smart Society (ICISS)}, 
  title={Service oriented architecture design using SOMA for optimizing public satisfaction in government agency: Case study: BPN - National Land Authority of Indonesia}, 
  year={2014},
  volume={},
  number={},
  pages={49-55},
  abstract={Service oriented architecture (SOA) enables organizations to easily integrate systems, data, and business processes. Implementation of SOA solution in private sector is widely used and successfully proven to increase their profit. But there are different challenge in public sector which is not profit oriented and has different business model. In public sector, user satisfaction on government agencies is one of common indicator to measure quality of public service. This paper presents SOA solution for public sector using SOMA to conduct a service integration for optimizing public satisfaction. We also combined SWOT and Porter's Value Chain to support business modelling analysis. The result shows that there is a simplicity and feasibility for users to access the service after SOA integration, which improves user satisfaction.},
  keywords={},
  doi={10.1109/ICTSS.2014.7013150},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9074950,
  author={Dongre, Yashwant and Ingle, Rajesh},
  booktitle={2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA)}, 
  title={An Investigation of QoS Criteria for Optimal Services Selection in Composition}, 
  year={2020},
  volume={},
  number={},
  pages={705-710},
  abstract={Web service plays a vital role in the service industry to improve the web service applications and it is service oriented architecture. Due to which formation of composite service leads to non-optimal. Service selection task in the composition process is to select the best service for each candidate services out of available services which are a functionally similar but non-functional measures of services are different. This paper, presents the investigation of quality of service parameters and optimality criteria for services selection. The work in the paper provides the analysis of quality of service parameters used in existing works for services selection/composition. Through the survey and analysis it has been revealed that the response time, availability, and reliability are most common used quality of service attributes with minimum/maximum as optimality criteria. However, after analysis of these parameters, the work suggests to use these attributes to solve optimal services selection problem in composition.},
  keywords={},
  doi={10.1109/ICIMIA48430.2020.9074950},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8082679,
  author={Chaudhari, Nikhil and Bhadoria, Robin Singh and Prasad, Siddharth},
  booktitle={2016 8th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={Information Handling and Processing Using Enterprise Service Bus in Service-Oriented Architecture System}, 
  year={2016},
  volume={},
  number={},
  pages={418-421},
  abstract={Information is key factor in delivering service across networks. Messaging is important aspect in handing information using Enterprise Service Bus (ESB) in Service Oriented Architecture (SOA). Such information is generally passes and used as interaction parameters upon communication between two parties that could be carried out amongst multiple services. Integration between multiple application services could be strengthened by adopting this methodology which is important to handle web services over networks. ESB is messaging middleware framework that helps in designing and developing web services through which software intermediary could be possible. It is a kind of depletion layer that efficiently handles various overheads during communication and interaction between multiple application services. This paper details about issues related to application services with message handling and control. Testing and simulation has been carried out on - AdroitLogic UltraESB, WSO2 ESB and Red Hat JBoss Fuse ESBs. Several parameters like total and average message counts, overall bytes measure, overall message received and sent, processing time of messages, and memory allocation.},
  keywords={},
  doi={10.1109/CICN.2016.88},
  ISSN={2472-7555},
  month={Dec},}

@INPROCEEDINGS{9441601,
  author={Sun, Yu and Mao, Shaojie and Huang, Songhua and Mao, Xiaobin},
  booktitle={2021 2nd Information Communication Technologies Conference (ICTC)}, 
  title={Load Balancing Method for Service Scheduling of Command Information System}, 
  year={2021},
  volume={},
  number={},
  pages={297-301},
  abstract={In order to satisfy the capability generation requirement of command information system, a load balancing method for service scheduling is studied. Considering that a work which will be finished by command information system based on service-oriented architecture is composed of several jobs, the paper analyzes the work completion process in detail. Then, a method to measure the load on a service which is scheduled to participate in a work is designed. On that basis, a load balancing mathematical model for service scheduling is established and a model solving algorithm that is based on greedy strategy and has polynomial time is put forward. Simulated experimental results show that the method proposed in this paper can allocate load to the services of command information system in a balanced way.},
  keywords={},
  doi={10.1109/ICTC51749.2021.9441601},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8029043,
  author={Su, Rui and Wan, Bo and Deng, Zhaoyun and Mei, Zheng and Mi, Weimin and Xie, Qiaoyun and Lin, Wenbin},
  booktitle={2017 36th Chinese Control Conference (CCC)}, 
  title={Research and application on integrated maintenance of smart substation and remote control center based on SOA}, 
  year={2017},
  volume={},
  number={},
  pages={10588-10593},
  abstract={The RCC-SS (Remote Control Center — Smart Substation) integrated maintenance technology based on SOA (Service Oriented Architecture) is proposed to solve the following problems: the complexity of debugging and maintenance when connecting the smart substation to the remote control center, the singularity of information exchange, and the difficulty to support advanced interactive application. By the construction of a wide-area distribution service system between the RCC (Remote Control Center) and the SS (Smart Substation), this proposal provides services in modelling, communication interface, real-time and historic data, and information verification as example. This RCC-SS system also unifies the modeling and the configuration, develops integrated maintenance tools, converts the substation SCD (Substation Configuration Description) file to CIM/E (Common Information Model / Efficient model exchange format) file and CIM/G (Common Information Model / Graphic exchange format) file used in remote control center, associates ID (Identification) between model and graphic files, imports all the information and stores in database, and lastly executes the automatic checking of the tele-measuring, tele-signaling and protection signaling. As a result, the smart substation's programmatic and automatic connection to the RCC is achieved, and the information exchange and business collaboration capabilities between the RCC and the SS is enhanced.},
  keywords={},
  doi={10.23919/ChiCC.2017.8029043},
  ISSN={1934-1768},
  month={July},}

@INPROCEEDINGS{8862722,
  author={Ribin, Jones S.B and Kumar, N.},
  booktitle={2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)}, 
  title={Precursory study on varieties of DDoS attacks and its implications in Cloud Systems}, 
  year={2019},
  volume={},
  number={},
  pages={1003-1008},
  abstract={Cloud Computing has emerged into an inevitable platform for computing services by effectively implementing Service Oriented Architecture (SOA) and Virtualization. However it is still vulnerable to traditional security threats and offers scope for innovative security attacks such as EDoS [1]. While it offers platform to generate innumerable Virtual components from a single physical component, it inadvertently provides wide spectrum of possibilities for distributed attacks. Moreover such attacks have adapted to cloud platform and have exploited various inherent vulnerabilities. In an unprecedented manner, they became unpredictable, evasive and challenging to Cloud Security measures. Therefore various versions of DDoS attack that targets the cloud platform have been extensively researched and narrated. The Cloud Security faces unprecedented challenges such as the Single-point-of-Failure occurs when a Cloud Supervisory Component or hypervisor fails due to a security breach. Moreover Cloud requirements often require being liberal to meet the Clients needs. This does not help the CSP to adapt traditional stringent security measures in Cloud System the reasons have been discussed in details.},
  keywords={},
  doi={10.1109/ICOEI.2019.8862722},
  ISSN={},
  month={April},}

@INPROCEEDINGS{6824100,
  author={Alzaghoul, Esra and Bahsoon, Rami},
  booktitle={2014 23rd Australian Software Engineering Conference}, 
  title={Evaluating Technical Debt in Cloud-Based Architectures Using Real Options}, 
  year={2014},
  volume={},
  number={},
  pages={1-10},
  abstract={A Cloud-based Service-Oriented Architecture (CBSOA) is typically composed of web services, which are offered off the cloud marketplace. CB-SOA can improve its utility and add value to its composition by switching among its constituent services. We look at the option to defer the decision of substitution under uncertainty. We exploit Binomial Options to the formulation. We quantify the time-value of the architecture decisions of switching web services and technical debt they can imply on the structure. As CB-SOA are market-sensitive, dynamic and "volatile", the decision of deferral tends to be sensitive to these dynamics. Henceforth, the structural complexity of a CB-SOAcan change over time and so the technical debt as its constituent web services are modified, replaced, upgraded, etc. The method builds on Design Structure Matrix (DSM) and introduces time and complexity aware propagation cost metrics to assess the value of deferral decisions relative to changes in the structure. Architects of CB-SOA can use our method to assess the time value of deferring the decisions to switch web services relative to complexity, technical debt and value creation. We demonstrate the applicability of the method using an illustrative example.},
  keywords={},
  doi={10.1109/ASWEC.2014.27},
  ISSN={2377-5408},
  month={April},}

@INPROCEEDINGS{8585730,
  author={OULMAHDI, Mohamed and CHASSOT, Christophe and VAN WAMBEKE, Nicolas},
  booktitle={2018 International Conference on Smart Communications in Network Technologies (SaCoNeT)}, 
  title={Extensible and Adaptive Architecture for an Evolutive Transport Layer}, 
  year={2018},
  volume={},
  number={},
  pages={102-107},
  abstract={The world of communications and networking knows and important evolution over the years. While this evolution is concretized by a deployment of many modern protocols at most of protocol layers, the Transport one continues to use old TCP and UDP protocols. This despite that an important number of modern protocols and mechanisms have been proposed. In this context, we study in this paper the obstacle of the deployment of new transport protocols and propose a new architecture to support the deployment and the adaptation of new Transport solutions. This was achieved by adding extensibility and adaptability capabilities using service-oriented and component-based paradigms. The architecture performances are studied at the end to measure the impact and the benefits of the new architecture comparing to classical Transport protocol.},
  keywords={},
  doi={10.1109/SaCoNeT.2018.8585730},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{7037678,
  author={Hecht, Geoffrey and Jose-Scheidt, Benjamin and De Figueiredo, Clement and Moha, Naouel and Khomh, Foutse},
  booktitle={2014 IEEE 6th International Conference on Cloud Computing Technology and Science}, 
  title={An Empirical Study of the Impact of Cloud Patterns on Quality of Service (QoS)}, 
  year={2014},
  volume={},
  number={},
  pages={278-283},
  abstract={Cloud patterns are described as good solutions to recurring design problems in a cloud context. These patterns are often inherited from Service Oriented Architectures or Object Oriented Architectures where they are considered good practices. However, there is a lack of studies that assess the benefits of these patterns for cloud applications. In this paper, we conduct an empirical study on a Restful application deployed in the cloud, to investigate the individual and the combined impact of three cloud patterns (i.e., Local Database proxy, Local Sharding-Based Router and Priority Queue Patterns) on Quality of Service (QoS). We measure the QoS using the application's response time, average, and maximum number of requests processed per seconds. Results show that cloud patterns doesn't always improve the response time of an application. In the case of the Local Database proxy pattern, the choice of algorithm used to route requests has an impact on response time, as well as the average and maximum number of requests processed per second. Combinations of patterns can significantly affect the QoS of applications. Developers and software architects can make use of these results to guide their design decisions.},
  keywords={},
  doi={10.1109/CloudCom.2014.141},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{7509323,
  author={Pulparambil, Supriya and Baghdadi, Youcef},
  booktitle={2016 IEEE Students' Conference on Electrical, Electronics and Computer Science (SCEECS)}, 
  title={SOA maturity model a frame of reference}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={Service Oriented Architecture (SOA) is an architectural style that supports service orientation. In reality, SOA is much more than architecture. SOA adoption is prerequisite for organization to excel their service deliveries, as the delivery platforms are shifting to mobile, cloud and social media. A maturity model is a tool to accelerate enterprise SOA adoption, however it depends on how it should be applied. This paper presents a literature review of existing maturity models and proposes 5 major aspects that a maturity model has to address to improve SOA practices of an enterprise. A maturity model can be used as: (i) a roadmap for SOA adoption, (ii) a reference guide for SOA adoption, (iii) a tool to gauge maturity of process execution, (iv) a tool to measure the effectiveness of SOA motivations, and (v) a review tool for governance framework. This paper also sheds light on how SOA maturity assessment can be modeled. A model for SOA process execution maturity and perspective maturity assessment has been proposed along with a framework to include SOA scope of adoption.},
  keywords={},
  doi={10.1109/SCEECS.2016.7509323},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8560743,
  author={Marmsoler, Diego},
  booktitle={2018 International Symposium on Theoretical Aspects of Software Engineering (TASE)}, 
  title={On Syntactic and Semantic Dependencies in Service-Oriented Architectures}, 
  year={2018},
  volume={},
  number={},
  pages={132-137},
  abstract={In service oriented architectures, components provide services on their output ports and consume services from other components on their input ports. Thereby, a component is said to depend on another component if the former consumes a service provided by the latter. This notion of dependency (which we call syntactic dependency) is used by many architecture analysis tools as a measure for system maintainability. With this paper, we introduce a weaker notion of dependency, still sufficient, however, to guarantee semantic independence between components. Thereby, we discover the concepts of weak and strong semantic dependency and prove that strong semantic dependency indeed implies syntactic dependency. Our alternative notion of dependency paves the way to more precise dependency analysis tools. Moreover, our results about the different types of dependencies can be used for the verification of semantic independence.},
  keywords={},
  doi={10.1109/TASE.2018.00025},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9402637,
  author={Yang, Meixia and Yang, JingJing and Xiao, Zhe and Huang, Ming},
  booktitle={2021 International Conference on Computer Communication and Informatics (ICCCI)}, 
  title={A modular spectrum sensing node for Resources-Oriented Radio Monitoring}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={The existing radio monitoring practices suffer from two critical limitations: 1) current practices have been limited to isolated measuring and are unable to integrate data from different spectrum sensors; 2) these practices still require human operators to interpret signals, which prevent automated and intelligent processing. To address these limitations, we present a novel design of a spectrum sensing node based on the Representational State Transfer (REST) architecture. The spectrum sensing node exploits the REST architecture to integrate resources (data) into the web and to make it effortless to collect and share resources by exposing resources as service-oriented Web APIs. In addition, the spectrum sensing node introduces standardization and automation to radio monitoring. We outline the procedures of spectrum analysis and signal analysis for radio monitoring. Furthermore, to automate spectrum sensing node functionalities, we implement spectrum analysis and signal analysis using machine learning methods, which reliably extract and learn intrinsic features of complex data. The spectrum sensing node has been packaged and deployed in Honghe Hani and Yi Autonomous Prefecture of China for radio spectrum management. We describe and discuss our implemented prototype of the spectrum sensing node to demonstrate its practical advantages.},
  keywords={},
  doi={10.1109/ICCCI50826.2021.9402637},
  ISSN={2329-7190},
  month={Jan},}

@ARTICLE{6530636,
  author={Li, Shancang and Zhao, Shanshan and Wang, Xinheng and Zhang, Kewang and Li, Ling},
  journal={IEEE Systems Journal}, 
  title={Adaptive and Secure Load-Balancing Routing Protocol for Service-Oriented Wireless Sensor Networks}, 
  year={2014},
  volume={8},
  number={3},
  pages={858-867},
  abstract={Service-oriented architectures for wireless sensor networks (WSNs) have been proposed to provide an integrated platform, where new applications can be rapidly developed through flexible service composition. In WSNs, the existing multipath routing schemes have demonstrated the effectiveness of traffic distribution over multipaths to fulfill the quality of service requirements of applications. However, the failure of links might significantly affect the transmission performance, scalability, reliability, and security of WSNs. Thus, by considering the reliability, congestion control, and security for multipath, it is desirable to design a reliable and service-driven routing scheme to provide efficient and failure-tolerant routing scheme. In this paper, an evaluation metric, path vacant ratio, is proposed to evaluate and then find a set of link-disjoint paths from all available paths. A congestion control and load-balancing algorithm that can adaptively adjust the load over multipaths is proposed. A threshold sharing algorithm is applied to split the packets into multiple segments that will be delivered via multipaths to the destination depending on the path vacant ratio. Simulations demonstrate the performance of the adaptive and secure load-balance routing scheme.},
  keywords={},
  doi={10.1109/JSYST.2013.2260626},
  ISSN={1937-9234},
  month={Sep.},}

@ARTICLE{8119814,
  author={Wang, Chao and Gong, Lei and Li, Xi and Yu, Qi and Wang, Aili and Hung, Patrick and Zhou, Xuehai},
  journal={IEEE Transactions on Services Computing}, 
  title={SOLAR: Services-Oriented Deep Learning Architectures-Deep Learning as a Service}, 
  year={2021},
  volume={14},
  number={1},
  pages={262-273},
  abstract={Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data size have posed significant challenge to construct a flexible and high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. To leverage the trade-offs between the metrics among performance, power, energy, and efficiency, we present a multitarget design space exploration. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability.},
  keywords={},
  doi={10.1109/TSC.2017.2777478},
  ISSN={1939-1374},
  month={Jan},}

@INPROCEEDINGS{7558028,
  author={Li, Zhinan and Yang, Xiaodong},
  booktitle={2016 IEEE International Conference on Web Services (ICWS)}, 
  title={A Reliability-Oriented Web Service Discovery Scheme with Cross-Layer Design in MANET}, 
  year={2016},
  volume={},
  number={},
  pages={404-411},
  abstract={Web service technologies are playing an increasingly important role in service-oriented architecture design and application convergence over Mobile ad hoc networks (MANET). Due to the decentralized administration and dynamic wireless connectivity problems, accomplishing reliable service discovery in MANET faces a large number of challenges. In order to relieve the communication inefficiency among service providers and clients caused mainly by the unpredictable node mobility, this paper proposes a cross-layer service discovery scheme which enables improved network efficiency and reduced resource consumption. Firstly a network-layer based underlay framework is presented. It specifically establishes a reliability-oriented source routing mechanism which is equipped with a novel reliability-maximized path selection metric and a backup path support fast route recovery strategy. The cross-layer design is prudentially realized by piggybacking service discovery procedures on the reliability enhanced underlay routing mechanism. Simulation analysis verifies that the proposed scheme improves service discovery reliability by achieving low rediscovery frequency, and guarantees high network efficiency by providing reduced service discovery delay and control overhead.},
  keywords={},
  doi={10.1109/ICWS.2016.59},
  ISSN={},
  month={June},}

@ARTICLE{9274356,
  author={Chiu, Kai-Cheng and Liu, Chien-Chang and Chou, Li-Der},
  journal={IEEE Access}, 
  title={CAPC: Packet-Based Network Service Classifier With Convolutional Autoencoder}, 
  year={2020},
  volume={8},
  number={},
  pages={218081-218094},
  abstract={The Internet has been evolving from a traditional mechanism to a modern service-oriented architecture, such as quality-of-service (QoS) policies, to meet users’ various requirements for high service quality. An instant and effective network traffic classification method is indispensable to identify network services to enforce QoS policies on the corresponding service. Network managers can easily flexibly deploy traffic classification modules and configure the network policies with the help of the emerging software-defined networking. However, most existing traffic classification solutions, such as port-based methods or deep packet inspection, cannot handle real-time and encrypted traffic classification. In this research, a Convolutional Autoencoder Packet Classifier (CAPC) has been proposed to immediately classify incoming packets in fine-grained and coarse-grained manners, that is, classifying a service to a single application and a rough genre, respectively. The CAPC is a packet-based deep learning model consisting of a 1D convolutional neural network and an autoencoder, which can handle dynamic-port and encrypted traffic and even cluster similar applications. This classifier is verified on not only the private self-captured traffic but also a public VPN dataset to demonstrate its performance. Moreover, the CAPC classifies different types of service traffic with an accuracy of over 99.9% on the private dataset of 16 services and over 97% on the public dataset of 24 services, thereby outperforming other deep learning classifiers. Experimental results also show other performance metrics, including stability, average precision, and recall and the highest F1-score values of 15 and 18 services on the private and public datasets, respectively.},
  keywords={},
  doi={10.1109/ACCESS.2020.3041806},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9529361,
  author={Huang, Pei-Shu and Fahmi, Faisal and Wang, Feng-Jian},
  booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={A Model to Helping the Construction of Creative Service-Based Software}, 
  year={2021},
  volume={},
  number={},
  pages={1235-1242},
  abstract={With the advent of the Service Oriented Architecture (SOA) in system design, various domain knowledges are included in a service-based application, such as the design of Artificial Intelligence (AI) or augmented reality (AR) systems. While merging one or multiple domains into computation systems, the computation systems can be widely applied in various domain usages with novelty, useful, and surprising properties, which are defined as systems of creative computing. In creative computing, several theoretical evaluation metrics and verification approaches have been proposed for system design in several domains. However, a solid practical design environment for creative service-based systems is rarely considered in current researches. In this paper, we propose a model for creative service software development based on semantic web, which is applied in two phases: (1) requirement specification and (2) service design. In order to bridge the knowledge gap between domain experts and software engineers, and provide a machine-readable format for creative computing, two sub-models, Requirement Specification and Service Structure Models, are constructed in both phases, sequentially. After the latter sub-model is validated, the creative service software is well-constructed based on the services definition and composition represented by the model.},
  keywords={},
  doi={10.1109/COMPSAC51774.2021.00171},
  ISSN={0730-3157},
  month={July},}

@INPROCEEDINGS{7009432,
  author={Mishra, Siba and Kumar, Chiranjeev},
  booktitle={The 2014 2nd International Conference on Systems and Informatics (ICSAI 2014)}, 
  title={Estimating development size and effort of business process service-oriented architecture applications}, 
  year={2014},
  volume={},
  number={},
  pages={1006-1011},
  abstract={Service-oriented Architecture (SOA) is adopted by many industrial and business organizations, as an efficient means for designing, developing and integrating enterprise business processes applications. With the built of Web Services, the developed business processes can be easily combined to achieve a composite business solution. Generally, any business applications are mixture of processes and some tasks corresponding to the processes. In the planning phase of software project management, estimation of development effort is very critical and crucial for software/business organizations. Having an accurate effort estimate guarantees the managers that the projects are completed within time and budget. So, estimation techniques/models need to be very efficient and should highlights all important cost drivers. The estimation of development effort for business processes primarily depends on the number of processes and it's associated tasks. The development effort of business applications primarily depends on the size of integrated applications. In this paper, some metrics for predicting the size of integrated business process SOA applications have been proposed. After estimating size, the development effort calculated by using the popular COCOMO model. A comparison of various performance evaluation criterion for assessing the accuracy of proposed model has been computed and shown.},
  keywords={},
  doi={10.1109/ICSAI.2014.7009432},
  ISSN={},
  month={Nov},}

@ARTICLE{7103341,
  author={Anta, Antonio Fernández and Gramoli, Vincent and Jiménez, Ernesto and Kermarrec, Anne-Marie and Raynal, Michel},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Distributed Slicing in Dynamic Systems}, 
  year={2016},
  volume={27},
  number={4},
  pages={1030-1043},
  abstract={Peer to peer (P2P) systems have moved from application specific architectures to a generic service oriented design philosophy. This raised interesting problems in connection with providing useful P2P middleware services capable of dealing with resource assignment and management in a large-scale, heterogeneous and unreliable environment. The slicing problem consists of partitioning a P2P network into $k$  groups (slices) of a given portion of the network nodes that share similar resource values. As the network is large and dynamic this partitioning is continuously updated without any node knowing the network size. In this paper, we propose the first algorithm to solve the slicing problem. We introduce the metric of slice disorder and show that the existing ordering algorithm cannot nullify this disorder. We propose a new algorithm that speeds up the existing ordering algorithm but that suffers from the same inaccuracy. Then, we propose another algorithm based on ranking that is provably convergent under reasonable assumptions. In particular, we notice experimentally that ordering algorithms suffer from resource-correlated churn while the ranking algorithm can cope with it. These algorithms are proved viable theoretically and experimentally.},
  keywords={},
  doi={10.1109/TPDS.2015.2430856},
  ISSN={1558-2183},
  month={April},}

@INPROCEEDINGS{8618153,
  author={Aljawawdeh, Hamzeh and Odeh, Mohammed and Simons, Christopher and Lebzo, Nawras},
  booktitle={2018 1st International Conference on Cancer Care Informatics (CCI)}, 
  title={A Metaheuristic Search Framework to Derive Cancer Care Services from Business Process Models}, 
  year={2018},
  volume={},
  number={},
  pages={142-151},
  abstract={Cancer Care involves not only handling patients' medical or physical needs but also other services to facilitate patient needs which are underpinned by appropriate software systems that assist in patient care processes. The Service-Oriented Architecture (SOA) model of computing has become widely adopted and can provide efficient and agile business solutions in the face of rapid changes to business requirements. Instead of adopting a more traditional way of building an IT system for Cancer Care by rigidly piecing together a collection of hardware, software and networking, SOA offers the opportunity to build the IT systems in an increasingly flexible and reconfigurable way. However, current service identification methods can suffer from shortcomings such as a lack of computational support, and not being able to address all the necessary activities of the service identification. To address these shortcomings, this paper presents a comprehensive metaheuristic search framework for deriving SOA-based services applied to Cancer Care business process models. This framework is evaluated using both quantitative and qualitative methods with the help of domain experts at King Hussein Cancer Centre (KHCC), Jordan. Evaluation by domain experts confirmed that the resulting services are feasible (i.e., valid services that can be practically applied for real-life projects) that the domain experts might not have arrived at manually. Statistical analysis shows candidate services produced by the search-based framework are superior to the services produced manually by domain experts at KHCC with respect to metrics for coupling and cohesion.},
  keywords={},
  doi={10.1109/CANCERCARE.2018.8618153},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7077302,
  author={Kim, Yukyong and Choi, Jong-Seok and Shin, Yongtae},
  booktitle={2014 4th World Congress on Information and Communication Technologies (WICT 2014)}, 
  title={A decision model for optimizing the service portfolio in SOA governance}, 
  year={2014},
  volume={},
  number={},
  pages={57-62},
  abstract={Effective service-oriented architecture (SOA) governance requires an appropriate process in place by which services described by a service model become candidates to enter the service portfolio. This is a planning for the appropriate identified services to create business agility and maximize reuse. Not all services in the service model can be realized in the form of IT solutions, so if our intended use of the service portfolio is to drive IT development planning, we must first decide which services are potentially realizable and which services are not. In this paper, we present a decision model to evaluate the services based on the proposed metrics. Comparing the relative value of each service with its development or maintenance cost should make the prioritization. The decision model is useful to support an approach to identifying the optimum portfolio of services based on the prioritization of business needs, followed by an estimation of the technical feasibility for each candidate service.},
  keywords={},
  doi={10.1109/WICT.2014.7077302},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8030635,
  author={Azarmi, Mehdi and Bhargava, Bharat},
  booktitle={2017 IEEE 10th International Conference on Cloud Computing (CLOUD)}, 
  title={An End-to-End Dynamic Trust Framework for Service-Oriented Architecture}, 
  year={2017},
  volume={},
  number={},
  pages={568-575},
  abstract={Service-oriented architecture (SOA) is an architectural paradigm that advocates composition of loosely-coupled services in order to construct more complex applications. The agility and complexity of modern web services on one hand and the arbitrary interconnections among them on the other hand, make it difficult to maintain a sustainable trustworthiness in long-running SOA-based applications. Moreover, the chain of participating services in a specific SOA invocation may not be visible to the service consumers, which leads to a lack of accountability. To address these challenges in SOA, we propose the following contributions. First, we design a new dynamic and flexible trust model based on graph abstraction that uses multiple trust strategies to calculate trust across SOA. This trust model keeps track of three trust metrics: individual service trust, session trust, and composite trust. We further design a trust engine component that implements the proposed trust model and that continuously maintains the quantitative end-to-end trust based on processing actual execution of services. Second, to prove the practicality and usefulness of the proposed framework, we have implemented an adaptive and secure service composition engine (ASSC) which takes advantage of an efficient algorithm to generate service compositions with near-optimal trustworthiness under predefined QoS constraints. Finally, we have developed a tool that is able to automatically deploy SOA testbeds from arbitrary directed acyclic graphs (created in the GUI). This tool enables the researcher to study the dynamics of new trust algorithms and strategies under different scenarios (e.g., arbitrary SOA topologies and attacks). We have extensively studied the effectiveness and performance of the proposed solutions using testbeds in the Amazon EC2 cloud.},
  keywords={},
  doi={10.1109/CLOUD.2017.78},
  ISSN={2159-6190},
  month={June},}

@INPROCEEDINGS{9592453,
  author={Fahmi, Faisal and Huang, Pei-Shu and Wang, Feng-Jian and Yang, Hongji},
  booktitle={2021 IEEE International Conference on Services Computing (SCC)}, 
  title={Constructing a Creative Software with Services}, 
  year={2021},
  volume={},
  number={},
  pages={134-144},
  abstract={Service Oriented Architecture (SOA) and Creative Computing can be applied to construct a creative service software by utilizing various domain knowledges, where the software contains a solution that not only effective, but also novel, useful and surprising. In creative computing, several theoretical evaluation metrics and verification approaches have been proposed for system design in several domains. However, a solid methodology for development of creative service software is rarely considered in current researches. In this paper, we propose a method composed of requirement specification and service design phases to develop creative software with SOA, where each phase applies a specification model based on semantic web. Inside the development, the models containing XML structures and the associated directed graphs are constructed in both phases to improve machine readability for automatic information processing in creative computing and reduce communication work among development participants with different knowledges, respectively. The graph models defined also can improve the traceability of the specifications and support machine processing. After the model resulted in the second phase is validated for consistency and completeness, the creative service software is well-constructed and can be implemented and reused effectively. Besides, a real example is adopted to demonstrate the workings of the method.},
  keywords={},
  doi={10.1109/SCC53864.2021.00026},
  ISSN={2474-2473},
  month={Sep.},}

@INPROCEEDINGS{7521492,
  author={Gomes, Luiza Barcelos Gualberto and Farias, Pedro Porfírio Muniz and Bessa Albuquerque, Adriano and Herden, Adriana},
  booktitle={2016 11th Iberian Conference on Information Systems and Technologies (CISTI)}, 
  title={Software measure based on BPMN activity points}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={BPMN usage has been extended beyond the workflow systems and service-oriented architecture. Development methodologies have been proposed using BPMN to describe use cases, to specify the activities flow that forms each scenario of general purpose systems and business process execution with web services. This article proposes a metric to estimate software size, called BPMN Activity Points based on activities counting from three different perspectives, in which these scores are increasingly detailed and refined.},
  keywords={},
  doi={10.1109/CISTI.2016.7521492},
  ISSN={},
  month={June},}

@ARTICLE{8288619,
  author={Akbar, Adnan and Kousiouris, George and Pervaiz, Haris and Sancho, Juan and Ta-Shma, Paula and Carrez, Francois and Moessner, Klaus},
  journal={IEEE Access}, 
  title={Real-Time Probabilistic Data Fusion for Large-Scale IoT Applications}, 
  year={2018},
  volume={6},
  number={},
  pages={10015-10027},
  abstract={Internet of Things (IoT) data analytics is underpinning numerous applications, however, the task is still challenging predominantly due to heterogeneous IoT data streams, unreliable networks, and ever increasing size of the data. In this context, we propose a two-layer architecture for analyzing IoT data. The first layer provides a generic interface using a service oriented gateway to ingest data from multiple interfaces and IoT systems, store it in a scalable manner and analyze it in real-time to extract high-level events; whereas second layer is responsible for probabilistic fusion of these high-level events. In the second layer, we extend state-of-the-art event processing using Bayesian networks in order to take uncertainty into account while detecting complex events. We implement our proposed solution using open source components optimized for large-scale applications. We demonstrate our solution on real-world use-case in the domain of intelligent transportation system where we analyzed traffic, weather, and social media data streams from Madrid city in order to predict probability of congestion in real-time. The performance of the system is evaluated qualitatively using a web-interface where traffic administrators can provide the feedback about the quality of predictions and quantitatively using F-measure with an accuracy of over 80%.},
  keywords={},
  doi={10.1109/ACCESS.2018.2804623},
  ISSN={2169-3536},
  month={},}

@ARTICLE{8643377,
  author={Macis, Silvia and Loi, Daniela and Ulgheri, Andrea and Pani, Danilo and Solinas, Giuliana and Manna, Serena La and Cestone, Vincenzo and Guerri, Davide and Raffo, Luigi},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Design and Usability Assessment of a Multi-Device SOA-Based Telecare Framework for the Elderly}, 
  year={2020},
  volume={24},
  number={1},
  pages={268-279},
  abstract={Telemonitoring is a branch of telehealth that aims at remotely monitoring vital signs, which is important for chronically ill patients and the elderly living alone. The available standalone devices and applications for the self-monitoring of health parameters largely suffer from interoperability problems; meanwhile, telemonitoring medical devices are expensive, self-contained, and are not integrated into user-friendly technological platforms for the end user. This paper presents the technical aspects and usability assessment of the telemonitoring features of the HEREiAM platform, which supports heterogeneous information technology systems. By exploiting a service-oriented architecture, the measured parameters collected by off-the-shelf Bluetooth medical devices are sent as XML documents to a private cloud that implements an interoperable health service infrastructure, which is compliant with the most recent healthcare standards and security protocols. This Android-based system is designed to be accessible both via TV and portable devices, and includes other utilities designed to support the elderly living alone. Four usability assessment sessions with quality-validated questionnaires were performed to accurately understand the ease of use, usefulness, acceptance, and quality of the proposed system. The results reveal that our system achieved very high usability scores even at its first use, and the scores did not significantly change over time during a field trial that lasted for four months, reinforcing the idea of an intuitive design. At the end of such a trial, the user-experience questionnaire achieved excellent scores in all aspects with respect to the benchmark. Good results were also reported by general practitioners who assessed the quality of their remote interfaces for telemonitoring.},
  keywords={},
  doi={10.1109/JBHI.2019.2894552},
  ISSN={2168-2208},
  month={Jan},}

@INPROCEEDINGS{7357508,
  author={Manso, Marco and Alcaraz Calero, Jose Maria and Barz, Christoph and Bloebaum, Trude Hafsøe and Chan, Kevin and Jansen, Norman and Johnsen, Frank Trethan and Markarian, Garik and Meiler, Peter-Paul and Owens, Ian and Sliwa, Joanna and Wang, Qi},
  booktitle={MILCOM 2015 - 2015 IEEE Military Communications Conference}, 
  title={SOA and Wireless Mobile Networks in the tactical domain: Results from experiments}, 
  year={2015},
  volume={},
  number={},
  pages={593-598},
  abstract={The NATO research task group IST-118 titled “SOA recommendations for disadvantaged grids in the tactical domain” is addressing the challenge of implementing the Service Oriented Architecture (SOA) paradigm at the tactical level by providing guidance and best practices in the form of a Tactical SOA Profile. The group will conduct identification and feasibility assessments of possible improvements of the Tactical SOA Profile, over a series of live and emulated experiments. In this paper, we describe our first experiments in applying SOA Web services to mobile nodes that are connected using Wireless Broadband Mobile Networks (WBMN) in the tactical domain. The experiments involved components provided by various nations, including radio hardware equipment, the Publish/Subscribe messaging service and NATO Friendly Force Information (NFFI) (as our functional service). We measured the system performance at service and physical (radio) levels in the presence of network disruption. We conclude by presenting the results of the experiments and a view of future work.},
  keywords={},
  doi={10.1109/MILCOM.2015.7357508},
  ISSN={},
  month={Oct},}

@ARTICLE{7192623,
  author={Trang, Mai Xuan and Murakami, Yohei and Ishida, Toru},
  journal={IEEE Transactions on Services Computing}, 
  title={Policy-Aware Service Composition: Predicting Parallel Execution Performance of Composite Services}, 
  year={2018},
  volume={11},
  number={4},
  pages={602-615},
  abstract={With the increasing volume of data to be analysed, one of the challenges in Service Oriented Architecture (SOA) is to make web services efficient in processing large-scale data. Parallel execution and cloud technologies are the keys to speed-up the service invocation. In SOA, service providers typically employ policies to limit parallel execution of the services based on arbitrary decisions. In order to attain optimal performance improvement, users need to adapt to the services policies. A composite service is a combination of several atomic services provided by various providers. To use parallel execution for greater composite service efficiency, the degree of parallelism (DOP) of the composite services need to be optimized by considering the policies of all atomic services. We propose a model that embeds service policies into formulae to calculate composite service performance. From the calculation, we predict the optimal DOP for the composite service, where it attains the best performance. Extensive experiments are conducted on real-world translation services. We use several measures such as mean prediction error (MPE), mean absolute deviation (MAD) and tracking signal (TS) to evaluate our model. The analysis results show that our proposed model has good prediction accuracy in identifying optimal DOPs for composite services.},
  keywords={},
  doi={10.1109/TSC.2015.2467330},
  ISSN={1939-1374},
  month={July},}

@INPROCEEDINGS{8599786,
  author={Johnsen, Frank T. and Landmark, Lars and Hauge, Mariann and Larsen, Erlend and Kure, Øivind},
  booktitle={MILCOM 2018 - 2018 IEEE Military Communications Conference (MILCOM)}, 
  title={Publish/Subscribe Versus a Content-Based Approach for Information Dissemination}, 
  year={2018},
  volume={},
  number={},
  pages={1-9},
  abstract={NATO has identified the WS-Notification standard from OASIS to support event-driven communication in the NATO enterprise and when building coalition networks. Using this standard promotes interoperability. However, there is significant overhead associated with WS-Notification since it is built on SOAP Web services (WS). Overhead can be problematic in networks with scarce resources. In this paper we perform a small-scale comparative evaluation of overhead of WS-Notification with another publish/subscribe standard: Message Queuing Telemetry Transport (MQTT). We also measure how these standards compare to the novel approach of content-based networking under the same networking conditions. We use the Named Data Networking (NDN) flavor of content-based networking for our experiment. Though fundamentally different, these approaches can be used to realize the Service-Oriented Architecture (SOA) paradigm. The drawback of standard publish/subscribe approaches is that they usually rely on a broker, which constitutes a single point of failure. NDN, on the other hand, has no broker which makes it interesting to consider for tactical networks. We use NATO Friendly Force Information (NFFI), which is much used for friendly force tracking, as the data format for the payload in all our tests. In the paper we focus on the respective approaches' network resource consumption. Based on the results we argue that the content-based approach seems promising and should be investigated further.},
  keywords={},
  doi={10.1109/MILCOM.2018.8599786},
  ISSN={2155-7586},
  month={Oct},}

@INPROCEEDINGS{9080034,
  author={Soomro, Arif Hussain and Jilani, Muhammad Taha},
  booktitle={2020 International Conference on Information Science and Communication Technology (ICISCT)}, 
  title={Application of IoT and Artificial Neural Networks (ANN) for Monitoring of Underground Coal Mines}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Explosions in coal mines during the work time is a one of major cause of casualties in the coal mines. Thus possess a life threaten situation for coal miners. In this paper we propose a system in which sensors sense concentration of gases (Methane and carbon monoxide) in the air, measures the mine temperature and humidity and heartbeat of miner. In response it generate the alerts, and identifies the location of miners. We propose ZigBee based wireless sensor network (WSN) for communication between sensors and coal mine safety monitoring system. The iBeacons are proposed for identification of miners. A service oriented architecture (SOA) has used to develop the system. The main purpose of this research paper is to ensure miners safety, by predicting the methane has with artificial neural network (ANN). The application of ANN seems more viable than others, the calculated values shows that its prove a negligible relative error that is around 0.05.. than the actual measurements. The proposed work is then compared with the state-of-the-art methods that overcomes the limitations form the existing systems.},
  keywords={},
  doi={10.1109/ICISCT49550.2020.9080034},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{8695920,
  author={Yunofri and Suhardi and Kurniawan, Novianto Budi},
  booktitle={2018 International Conference on Information Technology Systems and Innovation (ICITSI)}, 
  title={Designing Service Computing Platform for Statistical Project Management Based on SOA}, 
  year={2018},
  volume={},
  number={},
  pages={99-104},
  abstract={Statistical are identical to conducting surveys. The process of conducting a survey is still not in accordance with the stages of planning, so that the impact on business processes is not maximized from the entire survey. The implementation of the project management system in statistical is expected to improve the control function in the survey. As information technology develops, service concepts can improve project management systems. Project management services make every component in project management work effectively and efficiently. Project management services develop in the addition of features, technology and resources. So there needs to be a system that can accommodate these needs by having functions that can be expanded. The service computing platform is the answer to this problem. A service computing platform is an architecture designed to support the process of preparing web services, and can provide tools and techniques for modeling, simulating, analyzing, planning, providing and monitoring service-oriented applications in real time. This platform can also be used as a basis for implementing various surveys. Statistics Indonesia (SI) needs to make improvements in conducting surveys. In line with the increasing quality of data produced by SI, it is necessary to develop a service computing platform for statistical project management. This study proposes a service computing platform using the Service Computing System Engineering (SCSE) methodology. After getting the service computing platform design, the proposed design is evaluated. Design evaluation is measured by Coupling Factor 0.0034 (loose coupling), Cohesion Factor 0.9198 (high cohesion), Complexity Factor 0.00368 (low complexity) and Reusability Factor 5.28571 (reusable) indicating that the design value is quite good.},
  keywords={},
  doi={10.1109/ICITSI.2018.8695920},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8035002,
  author={Radwan, Wafaa and Hassouneh, Yousef and Sayyad, Abdel Salam and Ammar, Nariman},
  booktitle={2017 IEEE International Conference on Services Computing (SCC)}, 
  title={YAFA-SOA: A GA-Based Optimizer for Optimizing Security and Cost in Service Compositions}, 
  year={2017},
  volume={},
  number={},
  pages={330-337},
  abstract={This paper studies heuristic search-based optimization of service compositions. We have investigated applying Genetic Algorithms (GA) to optimize service-oriented architectures (SOA) in terms of security goals and cost, we help software Engineers to map the optimized service composition to the business process model based on security and cost. Service composition security risk is measured by implementing the aggregation rules from the local security risk values of the aggregated services in the composition. We adapt the DREAD model for Security risk assessment by suggesting new categorizations for calculating DREAD factors based on a proposed service structure and service attributes. We implemented the YAFA-SOA Optimizer as an extension of an existing GA implementation to solve multi-objective optimization problems for varying number of objectives in the context of SOA. We evaluated the tool in a case study. The study results show that applying multi-objective GA is feasible to find the optimized security and cost in SOA-based systems. We were able to approve that adding security services to the generated composition reduces the risk severity of the generated composition and enhances its security in terms of confidentiality, integrity and availability (CIA). We found that the generated service composition risk severity is less than 0.5, which matches the validation results obtained from a security expert.},
  keywords={},
  doi={10.1109/SCC.2017.49},
  ISSN={2474-2473},
  month={June},}

@INPROCEEDINGS{7133511,
  author={Harrer, Simon and Geiger, Matthias and Preißinger, Christian R. and Bimamisa, David and Schuberth, Stephan J.A. and Wirtz, Guido},
  booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
  title={Improving the Static Analysis Conformance of BPEL Engines with BPELlint}, 
  year={2015},
  volume={},
  number={},
  pages={31-39},
  abstract={Today, process-aware systems are ubiquitous. They are built by leveraging process languages for both business and implementation perspectives. In the typical context of a Web Services-based Service-oriented Architecture, the obvious choice to implement service orchestrations is still the Business Process Execution Language (BPEL). For BPEL, a variety of open source and commercial engines have emerged. Although the BPEL standard document defines a set of static analysis rules which should be checked by engines prior to deployment to be standard conformant, previous work revealed that most engines are not capable of revealing all violations of these constraints, resulting in costly runtime errors later on. In this paper, we aim to improve the static analysis conformance of BPEL engines. We implement the tool BPELlint that validates 71 static analysis rules of the BPEL specification, show that the tool can be easily integrated into the deployment process of existing engines, and evaluate its performance to measure the effect on the time to deploy. The results demonstrate that BPELlint can improve the static analysis conformance of BPEL engines with an acceptable performance overhead.},
  keywords={},
  doi={10.1109/SOSE.2015.21},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8776974,
  author={Barnwal, Anil and Jangade, Rajesh and Pugla, Satyakam},
  booktitle={2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)}, 
  title={Analyzing and Predicting the Allocation and Utilization of Resources in Cloud Computing System}, 
  year={2019},
  volume={},
  number={},
  pages={56-62},
  abstract={The increasing use of cloud computing, constructed on good research in utility computing, networking, virtualization and web services provides some important benefits such as flexibility, cost reduction and easy availability for people using the system. These advantages are expected to increase the demand for more cloud services which further increase the installation of more clouds and its customer base. These demands lead to many technical issues such as applications of internet services, service oriented architecture including high scalability and availability, fault tolerance. So the core issue is to develop techniques for balancing of load effectively. It is clear from the fact that the measure and complexity makes these systems infeasible for assignment of centralized jobs to specific servers. So there is need of productive distributed solutions. In the current paper three proposed load balancing solutions for distributed environment is investigated. They are biased Random Sampling, Honeybee Foraging and Active Clustering.},
  keywords={},
  doi={10.1109/CONFLUENCE.2019.8776974},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{9779690,
  author={Zaragoza, Pascal and Seriai, Abdelhak-Djamel and Seriai, Abderrahmane and Shatnawi, Anas and Derras, Mustapha},
  booktitle={2022 IEEE 19th International Conference on Software Architecture (ICSA)}, 
  title={Leveraging the Layered Architecture for Microservice Recovery}, 
  year={2022},
  volume={},
  number={},
  pages={135-145},
  abstract={The microservice-oriented architecture (MSA) is an architectural style which involves organizing an application as of small independent services, each oriented towards one business functionality while being data autonomous. In pursuit of modernizing their software to take advantage of the Cloud, companies have been eager to migrate their monolithic legacy software towards an MSA. This migration necessitates an identification phase to reorganize classes around the monolith’s functionalities as a set of microservice candidates. However, most identification approaches fail to utilize the monolith’s internal multilayered architecture to identify those functionalities, and thus the microservices. As a consequence, ignoring the internal multilayered architecture increases the risk of identifying microservice by their technical layer which is recognized as a conceptual anti-pattern. In this paper, we explore the impact of the multi-layer architecture in monolithic applications during the identification to develop a semi-automatic approach that relies on it to identify an MSA. Particularly, we analyze the presentation layer to determine the endpoints of each business functionality of the monolith. From these endpoints, we apply a vertical decomposition to identify the necessary classes to implement each feature as a microservice. In the process, we also define the bounded context of each microservice during the vertical decomposition of the data-access layer. For the evaluation, we implemented a model-driven process and applied it on a set of varying open-source applications commonly used in the literature. We compared the results of approach with and without the reverse-engineering of the internal architecture to measure the impact of our approach on the identification of quality microservices. Using decomposition metrics (e.g., MoJoFM, c2ccvg), we were able to measure a significant positive impact.},
  keywords={},
  doi={10.1109/ICSA53651.2022.00021},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10043222,
  author={Wang, Yu-Te and Ma, Shang-Pin and Lai, Yue-Jun and Liang, Yan-Cih},
  booktitle={2022 29th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Analyzing and Monitoring Kubernetes Microservices based on Distributed Tracing and Service Mesh}, 
  year={2022},
  volume={},
  number={},
  pages={477-481},
  abstract={The microservice system architecture (MSA) outperforms the monolithic system architecture in terms of maintainability, extensibility, scalability, and fault tolerance. This is prompting a widescale migration of software systems from existing monolith systems to MSA. Most microservice systems utilize container technology for deployment. The fact that Kubernetes (K8s) provides a fully-fledged toolchain for managing container-based applications is prompting many organizations to adopt the K8s protocol for microservice system deployment and operations. Microservice monitoring is essential to the success of any service operation. The collection of logs and aggregation of metrics by most existing microservice monitoring systems is somewhat intrusive. Furthermore, the heterogeneity of Kubernetes technology means that most monitoring methods are inapplicable in situations where microservices are developed for a system using a variety of underlying languages and platforms. In the current study, we developed a monitoring mechanism that provides various metrics specific to microservice systems in a nonintrusive way. The proposed K8s-based microservice monitoring system, referred to as KMamiz (Kubernetes-based Microservice Analysis and Monitoring using Istio and Zipkin), enables the construction and visualization for service-level/endpoint-level dependency graphs and endpoint request chains, and the service cohesion/coupling analysis to enhance system quality for the development team.},
  keywords={},
  doi={10.1109/APSEC57359.2022.00066},
  ISSN={2640-0715},
  month={Dec},}

@INPROCEEDINGS{9955300,
  author={Joyce, Josephine Eskaline and Sebastian, Shoney},
  booktitle={2022 IEEE 4th PhD Colloquium on Emerging Domain Innovation and Technology for Society (PhD EDITS)}, 
  title={Reinforcement Learning based Autoscaling for Kafka-centric Microservices in Kubernetes}, 
  year={2022},
  volume={},
  number={},
  pages={1-2},
  abstract={Microservices and Kafka have become a perfect match for enabling the Event-driven Architecture and this encourages microservices integration with various opensource platforms in the world of Cloud Native applications. Kubernetes is an opensource container orchestration platform, that can enable high availability, and scalability for Kafkacentric microservices. Kubernetes supports diverse autoscaling mechanisms like Horizontal Pod Autoscaler (HPA), Vertical Pod Autoscaler (VPA) and Cluster Autoscaler (CA). Among others, HPA automatically scales the number of pods based on the default Resource Metrics, which includes CPU and memory usage. With Prometheus integration, custom metrics for an application can be monitored. In a Kafkacentric microservices, processing time and speed depends on the number of messages published. There is a need for auto scaling policy which can be based on the number of messages processed. This paper proposes a new autoscaling policy, which scales Kafka-centric microservices deployed in an eventdriven deployment architecture, using a Reinforcement Learning model.},
  keywords={},
  doi={10.1109/PhDEDITS56681.2022.9955300},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{10074842,
  author={Yang, Xikang and Wang, Juan and Zhou, Biyu and Wang, Wang and Liu, Wantao and Dong, Yangchen},
  booktitle={2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={Fine-grained Spatiotemporal Features-Based for Anomaly Detection in Microservice Systems}, 
  year={2022},
  volume={},
  number={},
  pages={847-856},
  abstract={More and more applications use microservice architecture. Protecting the reliability of the microservice system is very important for the stable operation of applications. However, the complexity of microservice systems poses a great challenge to operation and maintenance. Researchers have proposed a series of anomaly detection algorithms, which can automatically detect the anomalies of cloud systems in time. However, for the microservice system with a complex spatial structure, there is no effective method to represent the fine-grained features of the internal metric level of the microservice. To solve this problem, we propose a fine-grained metric-level spatial feature graph TopoMetrics and use a spatiotemporal neural network STAD to obtain the spatiotemporal features of microservices, which can accurately detect the anomalies of complex microservices. We compare STAD with the most advanced algorithms in three open microservice workloads. The experimental results show that the average precision of STAD is significantly higher than that of the most advanced baseline method.},
  keywords={},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00138},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9836297,
  author={Pulnil, Sermsook and Senivongse, Twittie},
  booktitle={2022 19th International Joint Conference on Computer Science and Software Engineering (JCSSE)}, 
  title={A Microservices Quality Model Based on Microservices Anti-patterns}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Microservices architecture is becoming popular as many software organizations have the need to transform large complex systems into small-sized software units whose functions are separated by business capabilities. Microservices architecture is preferable since it promotes independence of software modification, maintenance, and deployment. However, anti-patterns or poor development patterns of microservices can decrease the software quality. Nonetheless, quality measurement of microservices design based on anti-patterns has not been found in existing research. Using the QMOOD method for quality model design, this paper proposes a microservices quality model based on 11 microservices anti-patterns and ISO/IEe 25010 as a standard reference for quality attributes. Also, a microservices quality measurement tool called MSA Nose+ is developed to measure the quality of microservices applications. In an experiment to validate the proposed model, the result shows that the quality values obtained from the proposed model improve consistently with the refactorings that are applied to a microservice-based system. Thus, development teams can use the proposed model and quality measurements as part of the decision making on quality improvement and maintenance of microservices applications.},
  keywords={},
  doi={10.1109/JCSSE54890.2022.9836297},
  ISSN={2642-6579},
  month={June},}

@INPROCEEDINGS{9820743,
  author={Wang, Xinkai and Li, Chao and Zhang, Lu and Hou, Xiaofeng and Chen, Quan and Guo, Minyi},
  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Exploring Efficient Microservice Level Parallelism}, 
  year={2022},
  volume={},
  number={},
  pages={223-233},
  abstract={The microservice architecture has recently become a driving trend in the cloud by disaggregating a monolithic application into many scenario-oriented service blocks (microservices). The decomposition process results in a highly dynamic execution scenario, in which various chained microservices contend for computing resources in different ways. While parallelism has been exploited at both the instruction/thread level and the task/request level, very limited work has been done with the grain-size of a microservice. Current parallel processing solutions are sub-optimal as they neither capture the unique characteristics of microservices nor consider the uncertainty arises in the microservice environment. In this work we introduce microservice level parallelism (MLP), a technique that aims to precisely coalesce and align parallel microservice chains for better system performance and resource utilization. We identify major issues that prevent servers from effectively exploiting MLP and we define metrics that can guide MLP optimization. We propose v-MLP, a volatility-aware MLP that is able to adapt to a highly heterogeneous and dynamic microservice environment. We show that v-MLP can reduce tail latency by up to 50% and improve resource utilization by up to 15 % under various scenarios.},
  keywords={},
  doi={10.1109/IPDPS53621.2022.00030},
  ISSN={1530-2075},
  month={May},}

@INPROCEEDINGS{10350596,
  author={Li, Yang and Zhang, Yang and Yang, Yilong and Wang, Weiru and Yin, Yongfeng},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={RM2MS: A Tool for Automatic Identification of Microservices from Requirements Models}, 
  year={2023},
  volume={},
  number={},
  pages={50-54},
  abstract={Microservices identification is the key development process of cloud-native applications. It focuses on decomposing system into decoupling autonomous components to support development and deployment independently. This process requires sophisticated human efforts for careful requirements analysis and validation to identify the appropriate microservices boundary inside system modules. Our previous work RM2PT can help to achieve a validated requirements model through automatically generating prototypes from original requirements models. The validated requirements model contains the precise definitions of functionality and data structure that can help in microservices identification. In this paper, we present a tool named RM2MS to further alleviate the problem of cloud-native application development to support automatic identification of microservices from the validated requirements model. RM2MS can automatically analyse the relationship between functionality and data structure, and trade-off non-functional factors for microservices identification. We demonstrate that the microservice architecture solution generated by RM2MS demonstrates a average gain of 27.1% over the manual approach in three key metrics(Function-Cohesion, Modularity, and Instability), while exhibiting efficiency that surpasses the manual process by more than 10-fold through five case studies. The proposed approach can be further extended and applied for the cloud-native application development in the software industry. The tool can be downloaded at https://rm2pt.com/advs/rm2ms, and a demo video casting its features is at https://www.youtube.com/watch?v=T71vQDasOSw},
  keywords={},
  doi={10.1109/MODELS-C59198.2023.00018},
  ISSN={},
  month={Oct},}

@ARTICLE{10332462,
  author={Xu, Yueshen and Qiu, Zhibo and Gao, Honghao and Zhao, Xinkui and Wang, Lu and Li, Rui},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Heterogeneous Data-Driven Failure Diagnosis for Microservice-Based Industrial Clouds Towards Consumer Digital Ecosystems}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  abstract={Consumer digital ecosystems include a large volume of different types of applications, and those applications are usually deployed in industrial cloud computing systems. Currently, microservices are one of the most prevailing architectures for industrial clouds. Similar to other architectures, microservices may also produce failures, so failure diagnosis for microservices becomes an inevitable problem in industrial clouds. A majority of existing methods focus on statistical analysis for monitoring data or system topological structure. However, because these methods usually only harness service-level or machine-level metrics, they cannot complete fine-grained failure diagnosis, increasing the running risk of microservice-based industrial clouds. To tackle this issue, in this paper, we design a novel graph structure to represent failure dependencies, especially the heterogeneity, and name it as the heterogeneous failure dependence graph (HFDG). We propose a framework to inform engineers which type of and where failures occur in industrial clouds. The HFDG can be used to mine the propagation of failures between different types of components. We also propose a novel neural network model based on attention mechanism and heterogeneous graph neural network, to fully leverage the metric data and HFDG. We performed experiments on three large-scale public datasets from real-world microservices-based systems. The experimental results demonstrate the superior performance of our model compared to well-known baselines.},
  keywords={},
  doi={10.1109/TCE.2023.3337351},
  ISSN={1558-4127},
  month={},}

@INPROCEEDINGS{10256409,
  author={Jack, Chang Hoong and Teck, See Kwee and Ming, Lim Tong and Hong, Ding Ying},
  booktitle={2023 IEEE 8th International Conference On Software Engineering and Computer Systems (ICSECS)}, 
  title={An Overview Analysis of Authentication Mechanism in Microservices-Based Software Architecture: A Discussion Paper}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Microservices-based software architecture promotes scalability and flexibility by breaking down a software application into smaller modules and making it more independent and loosely coupled services compared to monolith systems. However, securing microservices in a distributed nature has become one of the challenges. Authentication is one of the most critical components that should be focused in the microservices security measures. It helps to identify that only authenticated personnel and services can access sensitive information and secure the trust between microservices. This discussion paper aims to provide an overview analysis and extensive understanding on the authentication mechanism in microservices-based software architecture. In this study, we explore different authentication mechanisms including Mutual Transport Layer Security (mTLS), Token based authentication and API Gateway authentication. This study examines the strengths and limitations of different authentication mechanisms in microservices-based software architecture. It also emphasizes the importance of authentication and the need for having a well-designed authentication mechanism to ensure the integrity and security of microservices-based software architecture is crucial.},
  keywords={},
  doi={10.1109/ICSECS58457.2023.10256409},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9779811,
  author={Speth, Sandro and Stieß, Sarah and Becker, Steffen},
  booktitle={2022 IEEE 19th International Conference on Software Architecture Companion (ICSA-C)}, 
  title={A Saga Pattern Microservice Reference Architecture for an Elastic SLO Violation Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={116-119},
  abstract={Reference architectures are becoming increasingly popular for industry and researchers as benchmark solutions to test their novel concepts and tools. While many reference architectures exist in the microservice domain, they are often not built on state-of-the-art technologies. Furthermore, many existing reference architectures do not use lightweight and asynchronous communications, such as messaging, do not have out-of-the-box self-adaptation and do not consider state-of-the-art microservice patterns. Therefore, this paper proposes a self-adaptive microservice reference architecture that implements the microservice saga pattern. The architecture is implemented in Java Spring Boot and uses the Eventuate Tram framework for the saga orchestration. Moreover, the architecture is instrumented to export performance metrics for monitoring and data for system-wide tracing to check for correct execution of the system and its adaptations. The objective of this reference architecture is to provide a benchmark for explaining self-adaptation and propagation of service-level objective (SLOs) violations across an architecture with complex patterns. In addition to the architecture, we provide defined SLOs and load profiles to stress the architecture.},
  keywords={},
  doi={10.1109/ICSA-C54293.2022.00029},
  ISSN={2768-4288},
  month={March},}

@ARTICLE{9090324,
  author={Ma, Meng and Lin, Weilan and Pan, Disheng and Wang, Ping},
  journal={IEEE Transactions on Services Computing}, 
  title={Self-Adaptive Root Cause Diagnosis for Large-Scale Microservice Architecture}, 
  year={2022},
  volume={15},
  number={3},
  pages={1399-1410},
  abstract={The emergence of microservice architecture in Cloud systems poses a new challenges for the reliability operation and maintenance. Due to numerous services and diverse types of metrics, it is time-consuming and challenging to identify the root cause of anomaly in large-scale microservice architecture. To solve this issue, this article presents a multi-metric and self-adaptive root cause diagnosis framework, named MS-Rank. MS-Rank decomposes the task into four phases: impact graph construction, random walk diagnosis, result precision evaluation, metrics weight update. Initially, we introduce the concept of implicit metrics and propose a composite impact graph construction algorithm, using multiple types of metrics to discover causal relationships between services. Afterwards, we propose a diagnostic algorithm in which forward, selfward and backward transitions are designed to heuristically identify the root cause services. In addition, we establish a self-adaptive mechanism to update the confidence of different metrics dynamically according to their diagnostic precision. Lastly, we develop a prototype system and integrate MS-Rank into real production system - IBM Cloud. Experimental results show that MS-Rank has a high diagnostic precision and its performance outperforms several selected benchmarks. Through multiple rounds of diagnosis, MS-Rank can optimize itself effectively. MS-Rank can be rapidly deployed in various microservice-based systems and applications, requiring no predefined knowledge. MS-Rank also allows us to introduce expert experiences into its framework to improve the diagnostic efficiency and precision.},
  keywords={},
  doi={10.1109/TSC.2020.2993251},
  ISSN={1939-1374},
  month={May},}

@INPROCEEDINGS{10336221,
  author={Sarda, Komal},
  booktitle={2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)}, 
  title={Leveraging Large Language Models for Auto-remediation in Microservices Architecture}, 
  year={2023},
  volume={},
  number={},
  pages={16-18},
  abstract={Microservices architecture is popular due to its scalability and flexibility. However, managing and troubleshooting distributed microservices-based systems can be challenging and time consuming. Auto-remediation of anomalies, that is the automated detection and root-causes generation and execution of repair scripts, can reduce the down-times and increase the availability of systems. This thesis will explore the potential and effectiveness of using large language models (LLMs) in auto-remediation. It will develop an auto-remediation framework to mitigate the effects of performance-based anomalies in self-adaptive microservice architectures. Multiple sample microservice applications as test-bed will be rigorously studied, and a dataset will be created to evaluate LLM-based codegeneration models using semantic, lexical, and correctness metrics in zero-shot and few-shot scenarios. Additionally, we will develop reliable prompts for automated Ansible runbook generation and assess their efficiency for orchestrating the auto-remediation process, including deployment, configuration changes, and system recovery to improve application reliability and operational efficiency.},
  keywords={},
  doi={10.1109/ACSOS-C58168.2023.00025},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10062468,
  author={Frank, Sebastian and Wagner, Lion and Hakamian, Alireza and Straesser, Martin and van Hoorn, André},
  booktitle={2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={MiSim: A Simulator for Resilience Assessment of Microservice-Based Architectures}, 
  year={2022},
  volume={},
  number={},
  pages={1014-1025},
  abstract={Increased resilience compared to monolithic architectures is both one of the key promises of microservice-based architectures and a big challenge, e.g., due to the systems’ distributed nature. Resilience assessment through simulation requires fewer resources than the measurement-based techniques used in practice. However, there is no existing simulation approach that is suitable for a holistic resilience assessment of microservices comprised of (i) representative fault injections, (ii) common resilience mechanisms, and (iii) time-varying workloads. This paper presents MiSim — an extensible simulator for resilience assessment of microservice-based architectures. It overcomes the stated limitations of related work. MiSim fits resilience engineering practices by supporting scenario-based experiments and requiring only lightweight input models. We demonstrate how MiSim simulates (1) common resilience mechanisms — i.e., circuit breaker, connection limiter, retry, load balancer, and autoscaler — and (2) fault injections — i.e., instance/service killing and latency injections. In addition, we use TeaStore, a reference microservice-based architecture, aiming to reproduce scaling behavior from an experiment by using simulation. Our results show that MiSim allows for quantitative insights into microservice-based systems’ complex transient behavior by providing up to 25 metrics.},
  keywords={},
  doi={10.1109/QRS57517.2022.00105},
  ISSN={2693-9177},
  month={Dec},}

@INPROCEEDINGS{10123637,
  author={Li, Yuewei and Lu, Yan and Wang, Jingyu and Qi, Qi and Wang, Jing and Wang, Yingying and Liao, Jianxin},
  booktitle={2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={TADL: Fault Localization with Transformer-based Anomaly Detection for Dynamic Microservice Systems}, 
  year={2023},
  volume={},
  number={},
  pages={718-722},
  abstract={Due to the complexity of microservice architecture, it is difficult to accomplish efficient microservice anomaly detection and localization tasks and achieve the target of high system reliability. For rapid failure recovery and user satisfaction, it is significant to detect and locate anomalies fast and accurately in microservice systems. In this paper, we propose an anomaly detection and localization model based on Transformer, named TADL (Transformer-based Anomaly Detector and Locator), which models the temporal features and dynamically captures container relationships using Transformer with sandwich structure. TADL uses readily available container performance metrics, making it easy to implement in already-running container clusters. Evaluations are conducted on a sock-shop dataset collected from a real microservice system and a publicly available dataset SMD. Empirical studies on the above two datasets demonstrate that TADL can outperform baseline methods in the performance of anomaly detection, the latency of anomaly detection, and the effect of anomalous container localization, which indicates that TADL is useful in maintaining complex and dynamic microservice systems in the real world.},
  keywords={},
  doi={10.1109/SANER56733.2023.00078},
  ISSN={2640-7574},
  month={March},}

@INPROCEEDINGS{10015475,
  author={Pearce, Glen and Pflaum, Alexis and Balasoiu, Dumitru Alin and Szabo, Claudia},
  booktitle={2022 Winter Simulation Conference (WSC)}, 
  title={Jeopardy Assessment for Dynamic Configuration of Collaborative Microservice Architectures}, 
  year={2022},
  volume={},
  number={},
  pages={2070-2081},
  abstract={Microservice architectures, which are lightweight, flexible, and adapt easily to changes, have recently been considered for system development in military operations in contested and dynamic environments. However, in a military setting, the dynamic configuration of collaborative microservices execution becomes critical, and testing that microservice configurations behave as expected becomes paramount. In this paper, we propose a complex jeopardy metric and reconfiguration process that dynamically configures collaborative algorithms running on multiple nodes. Our metric and proposed scenarios will allow for the automated evaluation of microservice configurations and their re-configuration to suit operational needs. We evaluate our proposed scenario, metric, and various reconfiguration algorithms to show the benefits of this approach.},
  keywords={},
  doi={10.1109/WSC57314.2022.10015475},
  ISSN={1558-4305},
  month={Dec},}

@INPROCEEDINGS{9978950,
  author={Bi, Tingzhu and Pan, Yicheng and Jiang, Xinrui and Ma, Meng and Wang, Ping},
  booktitle={2022 IEEE 33rd International Symposium on Software Reliability Engineering (ISSRE)}, 
  title={VECROsim: A Versatile Metric-oriented Microservice Fault Simulation System (Tools and Artifact Track)}, 
  year={2022},
  volume={},
  number={},
  pages={297-308},
  abstract={Automated fault diagnosis of microservice systems has been a hot topic in recent years. As most incidents in real commercial cloud systems are not publicly available, we have witnessed researchers putting considerable effort into developing various experimental systems. However, previous tools cannot quickly refactor their functionality, scale the architecture, and customize fault characteristics. Given this, we develop VECROsim, a versatile metric-oriented microservice fault simulation system, and release the VECROsim benchmark dataset. VECROsim works delicately as a highly-customizable toolkit to generate abnormal performance metrics datasets of microservice systems on demand and automatically. Validation of representative services from the benchmark dataset confirms the capability of VECROsim to generate realistic performance metrics for diverse real-world systems. Our case studies on root cause analysis and dynamic correlation discovery demonstrated the superiority of VECROsim. We also witnessed that the VECROsim dataset brings new research challenges to state-of-the-art fault diagnosis schemes. VECROsim concretely supports microservice developers from the industry, as well as academic researchers working on fault diagnosis or broader research topics in many ways.},
  keywords={},
  doi={10.1109/ISSRE55969.2022.00037},
  ISSN={2332-6549},
  month={Oct},}

@ARTICLE{10034937,
  author={Gu, Shenghui and Rong, Guoping and Ren, Tian and Zhang, He and Shen, Haifeng and Yu, Yongda and Li, Xian and Ouyang, Jian and Chen, Chunan},
  journal={IEEE Transactions on Software Engineering}, 
  title={TrinityRCL: Multi-Granular and Code-Level Root Cause Localization Using Multiple Types of Telemetry Data in Microservice Systems}, 
  year={2023},
  volume={49},
  number={5},
  pages={3071-3088},
  abstract={The microservice architecture has been commonly adopted by large scale software systems exemplified by a wide range of online services. Service monitoring through anomaly detection and root cause analysis (RCA) is crucial for these microservice systems to provide stable and continued services. However, compared with monolithic systems, software systems based on the layered microservice architecture are inherently complex and commonly involve entities at different levels of granularity. Therefore, for effective service monitoring, these systems have a special requirement of multi-granular RCA. Furthermore, as a large proportion of anomalies in microservice systems pertain to problematic code, to timely troubleshoot these anomalies, these systems have another special requirement of RCA at the finest code-level. Microservice systems rely on telemetry data to perform service monitoring and RCA of service anomalies. The majority of existing RCA approaches are only based on a single type of telemetry data and as a result can only support uni-granular RCA at either application-level or service-level. Although there are attempts to combine metric and tracing data in RCA, their objective is to improve RCA's efficiency or accuracy rather than to support multi-granular RCA. In this article, we propose a new RCA solution TrinityRCL that is able to localize the root causes of anomalies at multiple levels of granularity including application-level, service-level, host-level, and metric-level, with the unique capability of code-level localization by harnessing all three types of telemetry data to construct a causal graph representing the intricate, dynamic, and nondeterministic relationships among the various entities related to the anomalies. By implementing and deploying TrinityRCL in a real production environment, we evaluate TrinityRCL against two baseline methods and the results show that TrinityRCL has a significant performance advantage in terms of accuracy at the same level of granularity with comparable efficiency and is particularly effective to support large-scale systems with massive telemetry data.},
  keywords={},
  doi={10.1109/TSE.2023.3241299},
  ISSN={1939-3520},
  month={May},}

@ARTICLE{9215019,
  author={Khazaei, Hamzeh and Mahmoudi, Nima and Barna, Cornel and Litoiu, Marin},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Performance Modeling of Microservice Platforms}, 
  year={2022},
  volume={10},
  number={4},
  pages={2848-2862},
  abstract={Microservice architecture has transformed the way developers are building and deploying applications in the nowadays cloud computing centers. This new approach provides increased scalability, flexibility, manageability, and performance while reducing the complexity of the whole software development life cycle. The increase in cloud resource utilization also benefits microservice providers. Various microservice platforms have emerged to facilitate the DevOps of containerized services by enabling continuous integration and delivery. Microservice platforms deploy application containers on virtual or physical machines provided by public/private cloud infrastructures in a seamless manner. In this article, we study and evaluate the provisioning performance of microservice platforms by incorporating the details of all layers (i.e., both micro and macro layers) in the modeling process. To this end, we first build a microservice platform on top of Amazon EC2 cloud and then leverage it to develop a comprehensive performance model to perform what-if analysis and capacity planning for microservice platforms at scale. In other words, the proposed performance model provides a systematic approach to measure the elasticity of the microservice platform by analyzing the provisioning performance at both the microservice platform and the back-end macroservice infrastructures.},
  keywords={},
  doi={10.1109/TCC.2020.3029092},
  ISSN={2168-7161},
  month={Oct},}

@ARTICLE{10160171,
  author={Abgaz, Yalemisew and McCarren, Andrew and Elger, Peter and Solan, David and Lapuz, Neil and Bivol, Marin and Jackson, Glenn and Yilmaz, Murat and Buckley, Jim and Clarke, Paul},
  journal={IEEE Transactions on Software Engineering}, 
  title={Decomposition of Monolith Applications Into Microservices Architectures: A Systematic Review}, 
  year={2023},
  volume={49},
  number={8},
  pages={4213-4242},
  abstract={Microservices architecture has gained significant traction, in part owing to its potential to deliver scalable, robust, agile, and failure-resilient software products. Consequently, many companies that use large and complex software systems are actively looking for automated solutions to decompose their monolith applications into microservices. This paper rigorously examines 35 research papers selected from well-known databases using a Systematic Literature Review (SLR) protocol and snowballing method, extracting data to answer the research questions, and presents the following four contributions. First, the Monolith to Microservices Decomposition Framework (M2MDF) which identifies the major phases and key elements of decomposition. Second, a detailed analysis of existing decomposition approaches, tools and methods. Third, we identify the metrics and datasets used to evaluate and validate monolith to microservice decomposition processes. Fourth, we propose areas for future research. Overall, the findings suggest that monolith decomposition into microservices remains at an early stage and there is an absence of methods for combining static, dynamic, and evolutionary data. Insufficient tool support is also in evidence. Furthermore, standardised metrics, datasets, and baselines have yet to be established. These findings can assist practitioners seeking to understand the various dimensions of monolith decomposition and the community's current capabilities in that endeavour. The findings are also of value to researchers looking to identify areas to further extend research in the monolith decomposition space.},
  keywords={},
  doi={10.1109/TSE.2023.3287297},
  ISSN={1939-3520},
  month={Aug},}

@INPROCEEDINGS{9919941,
  author={Yang, Linwei and Li, Jing and Shi, Kuanzhi and Yang, Songlin and Yang, Qingfu and Sun, Jiangang},
  booktitle={2022 23rd Asia-Pacific Network Operations and Management Symposium (APNOMS)}, 
  title={MicroMILTS: Fault Location for Microservices Based Mutual Information and LSTM Autoencoder}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Driven by the development of cloud computing and artificial intelligence, architecture has dramatically improved in terms of flexibility and scalability in software development. Therefore, it is increasingly being used to build large-scale applications for agile development. However, along with the technology heterogeneity, the dynamics of running instances, and the complexity of service dependencies, fault localization is extraordinarily difficult. In this paper, we present MicroMILTS, a microservice fault location method based on mutual information and an LSTM Autoencoder. MicroMILTS first uses BIRCH for anomaly detection based on the analysis of the performance metrics data correlated to microservice anomalies. Once anomalies are detected, a service dependency property graph is constructed based on the real-time microservice invocation relationships and the reconstructed deviations of performance metrics with the LSTM Autoencoder. Next, MicroMILTS dynamically updates the weight of each node in the service dependency property graph. Then, a PageRank-based random walk is applied for further ranking root causes. Finally, a Sock-shop microservice system is built on the Huawei Cloud to evaluate the performance of MicroMILTS. The experiment shows that MicroMILTS achieves a good root cause location result, with 90.4 % in precision and 91.6% in mean average precision, outperforming state-of-the-art methods.},
  keywords={},
  doi={10.23919/APNOMS56106.2022.9919941},
  ISSN={2576-8565},
  month={Sep.},}

@ARTICLE{10125010,
  author={Zdun, Uwe and Queval, Pierre-Jean and Simhandl, Georg and Scandariato, Riccardo and Chakravarty, Somik and Jelić, Marjan and Jovanović, Aleksandar},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Detection Strategies for Microservice Security Tactics}, 
  year={2023},
  volume={},
  number={},
  pages={1-17},
  abstract={Microservice architectures are widely used today to implement distributed systems. Securing microservice architectures is challenging because of their polyglot nature, continuous evolution, and various security concerns relevant to such architectures. This article proposes a novel, model-based approach providing detection strategies to address the automated detection of security tactics (or patterns and best practices) in a given microservice architecture decomposition model. Our novel detection strategies are metrics-based rules that decide conformance to a security recommendation based on a statistical predictor. The proposed approach models this recommendation using Architectural Design Decisions (ADDs). We apply our approach for four different security-related ADDs on access management, traffic control, and avoiding plaintext sensitive data in the context of microservice systems. We then apply our approach to a model data set of 10 open-source microservice systems and 20 variants of those systems. Our results are detection strategies showing a very low bias, a very high correlation, and a low prediction error in our model data set.},
  keywords={},
  doi={10.1109/TDSC.2023.3276487},
  ISSN={1941-0018},
  month={},}

@INPROCEEDINGS{10092594,
  author={Filippone, Gianluca and Qaisar Mehmood, Nadeem and Autili, Marco and Rossi, Fabrizio and Tivoli, Massimo},
  booktitle={2023 IEEE 20th International Conference on Software Architecture (ICSA)}, 
  title={From monolithic to microservice architecture: an automated approach based on graph clustering and combinatorial optimization}, 
  year={2023},
  volume={},
  number={},
  pages={47-57},
  abstract={Migrating from a legacy monolithic system to a microservice architecture is a complex and time-consuming process. Software engineers may strongly benefit from automated support to identify a high-cohesive and loose-coupled set of microservices with proper granularity. The automated approach proposed in this paper extracts microservices by using graph clustering and combinatorial optimization to maximize cohesion and minimize coupling. The approach performs static analysis of the code to obtain a graph representation of the monolithic system. Then, it uses graph clustering to detect high-cohesive communities of nodes using the Louvain community algorithm. In parallel, the tool clusters the domain entities (i.e., classes representing uniquely identifiable concepts in a system domain) within bounded contexts to identify the required service granularity. Finally, it uses combinatorial optimization to minimize the coupling, hence deriving the microservice architecture. The approach is fully implemented. We applied it over four different monolithic systems and found valuable results. We evaluated the identified architectures through cohesion and coupling metrics, along with a comparison with other state-of-the-art approaches based on features such as granularity level, number of produced services, and methods applied. The approach implementation and the experimental results are publicly available.},
  keywords={},
  doi={10.1109/ICSA56044.2023.00013},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10371619,
  author={Ünlü, Hüseyin and Hacaloğlu, Tuna and Ömüral, Neslihan Küçükateş and Çalişkanel, Neslihan and Leblebici, Onur and Demirörs, Onur},
  booktitle={2023 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={An Exploratory Case Study on Effort Estimation in Microservices}, 
  year={2023},
  volume={},
  number={},
  pages={215-218},
  abstract={Software project management plays an important role in producing high-quality software, and effort estimation can be considered as a backbone for successful project management. Size is a very significant attribute of software by being the only input to perform early effort estimation. Even though functional size measurement methods showed successful results in effort estimation of traditional data-centric architectures such as monoliths, they were not designed for today’s architectures which are more service-based and decentralized such as microservices. In these new systems, the event concept is highly used specifically for communication among different services. By being motivated by this fact, in this study, we looked for more microservice-compatible ways of sizing microservices using events and developed a method accordingly. Then, we conducted an exploratory case study in an organization using agile methods and measured the size of 17 Product Backlog Items (PBIs) to assess how this proposed method can be useful in effort estimation in microservices. The implication from the case study is that despite performing a more accurate effort estimation using the proposed size measurement than COSMIC, we were unable to significantly outperform using the total number of events. However, our suggested approach demonstrated to us a different way to use software size in terms of events, namely, to determine the coupling complexity of the project. This finding can be beneficial specifically when evaluating the change requests.},
  keywords={},
  doi={10.1109/SEAA60479.2023.00040},
  ISSN={2376-9521},
  month={Sep.},}

@INPROCEEDINGS{10295809,
  author={Adrio, Kendricko and Tanzil, Clementius Nichklaus and Lianto, Michael Christian and Rasjid, Zulfany Erlisa},
  booktitle={2023 10th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)}, 
  title={Comparative Analysis of Monolith, Microservice API Gateway and Microservice Federated Gateway on Web-based application using GraphQL API}, 
  year={2023},
  volume={},
  number={},
  pages={654-660},
  abstract={The purpose of this research is to provide a detailed explanation regarding the characteristics as well as the pros and cons offered by various software development architecture, such as monolithic and Microservice architecture implemented with graph-based API called GraphQL. Monolithic architecture offers a centralized software development pattern with relatively simpler integration and development process. Conversely, Microservices architecture such as Gateway Aggregation and Federated Gateway will divide independent components of the application into smaller modules. Gateway Aggregation utilizes a single Gateway which acts as the main entry point for data exchange between the client and the application. In this research aims an application is developed using the three different architectures to measure the quality, both qualitative and quantitative performances of each architecture. There are several different parameters that are going to be used to measure the architecture’s performance such as response time and data throughput which become an essential criterion in conducting load and stress testing. The result is that the Monolithic architecture offers some advantages in its quantitative performance measurement due to better efficiency in collecting and processing requested data in a single application which utilizes fewer resources and shorter time. In contrast, the Gateway Aggregation architecture and Federated Gateway architecture also have some significant performance differences because it costs resources to combine several subgraphs together into a valid graph.},
  keywords={},
  doi={10.1109/EECSI59885.2023.10295809},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10031648,
  author={Kleftakis, Spyridon and Mavrogiorgou, Argyro and Zafeiropoulos, Nikolaos and Mavrogiorgos, Konstantinos and Kiourtis, Athanasios and Kyriazis, Dimosthenis},
  booktitle={2022 IEEE International Conference on Computing (ICOCO)}, 
  title={A Comparative Study of Monolithic and Microservices Architectures in Machine Learning Scenarios}, 
  year={2022},
  volume={},
  number={},
  pages={352-357},
  abstract={Choosing the most suitable architecture for applications is not an easy decision. While the software giants have almost all put in place the microservices architecture, on smaller platforms such decision it is not so obvious. In the healthcare domain and specifically when accomplishing Machine Learning (ML) tasks in this domain, considering its special characteristics, the decision should be made based on specific metrics. In the context of the beHEALTHIER platform, a platform that is able to handle heterogeneous healthcare data towards their successful management and analysis by applying various ML tasks, such research gap was fully investigated. There has been conducted an experiment by installing the platform in three (3) different architectural ways, referring to the monolithic architecture, the clustered microservices architecture exploiting docker compose, and the microservices architecture exploiting Kubernetes cluster. For these three (3) environments, time-based measurements were made for each Application Programming Interface (API) of the diverse platform’s functionalities (i.e., components) and useful conclusions were drawn towards the adoption of the most suitable software architecture.},
  keywords={},
  doi={10.1109/ICOCO56118.2022.10031648},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9885761,
  author={Chen, Yufu and Yan, Meng and Yang, Dan and Zhang, Xiaohong and Wang, Ziliang},
  booktitle={2022 IEEE International Conference on Web Services (ICWS)}, 
  title={Deep Attentive Anomaly Detection for Microservice Systems with Multimodal Time-Series Data}, 
  year={2022},
  volume={},
  number={},
  pages={373-378},
  abstract={Software architecture is undergoing a transition from monolithic architectures to microservices to achieve resilience, agility, and scalability in the software life circle. However, microservice architecture is not perfect and suffers from intermittent faults, leading to economic and user losses. Therefore, it is essential to detect anomalies in microservice systems accurately. The key limitation of current approaches lies in a lack of ability to detect multitype anomalies, excessive resource overhead, and requirements of expert knowledge. In this paper, we present a Deep Attentive anomaly detection approach with Multimodal data named DAM. With multimodal fusion, attentive LSTM, and a dynamic threshold selecting algorithm, DAM could detect anomalies accurately and efficiently in an unsupervised manner. We evaluate our approach by injecting six types of anomalies on a widely used microservice system, Train-Ticket. The result shows that DAM could detect multitype anomalies well, with 80.46% F-measure, achieving 16.76% and 29.52% improvement over two state-of-the-art baselines (Donut and DAGMM), respectively.},
  keywords={},
  doi={10.1109/ICWS55610.2022.00062},
  ISSN={},
  month={July},}

@INPROCEEDINGS{10211993,
  author={Abbasi, Maryam and Melo, Pedro and Saraiva, Luzia and Pereira, Pedro and Martins, Pedro and Sá, Filipe and Cardoso, Filipe},
  booktitle={2023 18th Iberian Conference on Information Systems and Technologies (CISTI)}, 
  title={Enhancing Banking Operations with Microservices and Mobile Technology}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a novel architecture for enhancing the banking experience by combining microservices and a mobile application. The use of microservices provides scalability and flexibility in the development process, making it easier to add new features or modify existing ones. The results of the study shows that the proposed architecture is capable of handling high volume of transactions and requests while providing high quality of service. The mobile application provides a user-friendly interface for accessing financial information, and the use of microservices ensures efficient management of data and transactions. The architecture also has the potential to improve security through the use of security measures to protect sensitive data. As a future research direction, the proposed architecture can be evaluated in real-world settings and its security can be further tested. The field of technology in the banking sector is constantly evolving, and it is important to stay updated with new advancements that can potentially improve the proposed architecture.},
  keywords={},
  doi={10.23919/CISTI58278.2023.10211993},
  ISSN={2166-0727},
  month={June},}

@ARTICLE{10183809,
  author={Zeb, Shah and Rathore, Muhammad Ahmad and Hassan, Syed Ali and Raza, Saleem and Dev, Kapal and Fortino, Giancarlo},
  journal={IEEE Wireless Communications}, 
  title={Toward AI-Enabled NextG Networks with Edge Intelligence-Assisted Microservice Orchestration}, 
  year={2023},
  volume={30},
  number={3},
  pages={148-156},
  abstract={Network agility, automation, and intelligence are at the forefront of the next-generation networks (NGNs) vision, which aims to provide zero-touch service management and self-optimizing networks. In this article, we give an overview of the significance of artificial intelligence (Ali-enabled NGNs, their projected benefits, design requirements, and critical challenges for evolving heterogeneous softwarized networks where microservices can be autonomously orchestrated, scaled, and maintained. The convergence of emerging disruptive technologies, for example, AI, network softwarization, hybrid cloud/edge-native computing architecture, with NGNs accelerates the enhanced service-oriented architecture at the network core/edge level to support on-demand microservices, such as visibility services for intelligent network management. In addition, we present a use case study and conduct experiments based on a novel design of an edge intelligence framework that orchestrates and deploys AI microservices utilizing the testbed resources of a multisite cloud/edge-native NGNs. We use a deep learning-based forecaster model to predict near real-time edge network flow between a centralized service orchestrator hub and multiple edge devices, geographically apart. The obtained results show that the deployed forecaster model accurately predicts the throughput and latency of edge network flow (verified against the groundtruth observations), which is additionally validated through two performance metrics obtained, low root-mean-square error, and high coefficient of determination values. Finally, we outline some of the potential future prospects for AI-enabled NGNs research.},
  keywords={},
  doi={10.1109/MWC.015.2200461},
  ISSN={1558-0687},
  month={June},}

@ARTICLE{9744560,
  author={Rossi, Fabiana and Cardellini, Valeria and Presti, Francesco Lo and Nardelli, Matteo},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Dynamic Multi-Metric Thresholds for Scaling Applications Using Reinforcement Learning}, 
  year={2023},
  volume={11},
  number={2},
  pages={1807-1821},
  abstract={Cloud-native applications increasingly adopt the microservices architecture, which favors elasticity to satisfy the application performance requirements in face of variable workloads. To simplify the elasticity management, the trend is to create an auto-scaler instance per microservice, which controls its horizontal scalability by using the classic threshold-based policy. Although easy to implement, setting manually the scaling thresholds, which are usually statically-defined on a single metric, may lead to poor scaling decisions when applications are heterogeneous in terms of resource consumption. In this article, we study dynamic multi-metric threshold-based scaling policies, that exploit Reinforcement Learning (RL) to autonomously update the scaling thresholds, one per controlled resource (CPU and memory). The proposed RL approaches (i.e., QL, MB, and DQL Threshold) use different degrees of knowledge about the system dynamics. To model the thresholds’ adaptation actions, we consider two RL-based architectures. In the single-agent architecture, one agent drives the updates of both scaling thresholds. To speed-up the learning, the multi-agent architecture adopts a distinct agent per threshold. Simulation- and prototype-based results show the benefits of the proposed solutions when compared to the state-of-the-art policies and highlight the advantages of multi-agent MB Threshold and DQL Threshold approaches, in terms of deployment objectives and execution times.},
  keywords={},
  doi={10.1109/TCC.2022.3163357},
  ISSN={2168-7161},
  month={April},}

@INPROCEEDINGS{10279802,
  author={Raghunandan, Arpitha and Kalasapura, Deepti and Caesar, Matthew},
  booktitle={ICC 2023 - IEEE International Conference on Communications}, 
  title={Digital Twinning for Microservice Architectures}, 
  year={2023},
  volume={},
  number={},
  pages={3018-3023},
  abstract={Digital twins have been designed and implemented for diverse applications like smart manufacturing, healthcare, supply chain and retail management. They provide monitoring, remote prognostics and health management capabilities for the various physical assets used in these domains. Many of these capabilities would be beneficial to microservice architectures as well, given the need for lightweight monitoring solutions in multitenant environments. In particular, twins can provide operators with real-time resource usage metrics which help with operational objectives such as resource planning, anomaly detection, rewind and replay and so on. In this work, we propose a design for building digital twins for microservice architectures. As a proof of concept, we focus on modelling the resource utilization as that is a key requirement for monitoring system reliability and security. In general, digital twins require a real world counterpart, a virtual model and a mechanism for consistently keeping both synchronized. We focus on the two latter aspects of the digital twin. Our approach involves converting a formal model of a microservice architecture into a digital twin that can capture and execute an actual cluster's state. We present an extensible architecture connecting the various components of the system and the twin and evaluate the twin's ability to capture the real-time state of a real Kubernetes cluster. We also discuss future extensions which can enhance the system's security by detecting a broad range of attacks.},
  keywords={},
  doi={10.1109/ICC45041.2023.10279802},
  ISSN={1938-1883},
  month={May},}

@INPROCEEDINGS{10037281,
  author={Raharjo, Agus Budi and Andyartha, Putu Krisna and Wijaya, William Handi and Purwananto, Yudhi and Purwitasari, Diana and Juniarta, Nyoman},
  booktitle={2022 International Conference on Computer Engineering, Network, and Intelligent Multimedia (CENIM)}, 
  title={Reliability Evaluation of Microservices and Monolithic Architectures}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Software is continuously evolving as business processes that needed to be solved become increasingly complex. Software architecture is an important aspect during software design, with monolithic and microservices being two of the most common with their own advantages and disadvantages. Monolithic is a unified system with a relatively fast development time. Meanwhile, microservices facilitates low coupling and high cohesion, prioritizing maintenance, and ease of modification post-development. This research compares microservices and monolithic API-based thesis monitoring systems. Implementations are done using PHP, Redis, PostgreSQL, Docker, and Heroku. Reliability evaluations are done through automated tests with Apache JMeter. Metrics used are maturity, availability, fault tolerance, and recoverability based on the ISO/IEC 25010 reliability quality characteristics. The conclusion section showed that microservices are more reliable than the monolithic by demonstrating much better fault tolerance and recoverability, with comparable maturity and availability.},
  keywords={},
  doi={10.1109/CENIM56801.2022.10037281},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9984530,
  author={Hettiarachchi, Lasal Sandeepa and Jayadeva, Senura Vihan and Bandara, Rusiru Abhisheak Vikum and Palliyaguruge, Dilmi and Arachchillage, Udara Srimath S. Samaratunge and Kasthurirathna, Dharshana},
  booktitle={2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Artificial Intelligence-Based Centralized Resource Management Application for Distributed Systems}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Due to the decentralized nature and emergence of new practices, tools, and platforms, microservices have become one of the most widely spread software architectures in the modern software industry. Furthermore, the advancement of software packaging tools like Docker and orchestration platforms such as Kubernetes enable developers and operation engineers to deploy and manage microservice applications more effectively and efficiently. However, establishing and managing microservice applications are still cumbersome due to the infrastructure configuration and array of disjoint tools that fail to understand the application’s dynamic behavior. As a result, developers need to configure multiple tools and platforms to automate the deployment and monitoring process to provide the optimal deployment strategy for microservices. Even though many tools are available in the industry, the fully automated product which comprises deployment, monitoring, resiliency evaluation and optimization were not developed yet. In response to this issue, we propose an artificial intelligence (AI)-based centralized resource management tool, that provides an automated low latency container management, cluster metrics gathering, resiliency evaluation and optimal deployment strategy behave in dynamic nature.},
  keywords={},
  doi={10.1109/ICCCNT54827.2022.9984530},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10371525,
  author={Daniel, João and Guerra, Eduardo and Rosa, Thatiane and Goldman, Alfredo},
  booktitle={2023 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={Towards the Detection of Microservice Patterns Based on Metrics}, 
  year={2023},
  volume={},
  number={},
  pages={132-139},
  abstract={Microservices is a popular architectural approach for complex systems in companies, despite its nature of decentralization. There is a comprehensive set of microservices architectural patterns that guides implementations and helps developers to overcome issues. However, the community still scarcely adopts these patterns and only has a theoretical understanding of them. In this work, in order to increase awareness of such patterns and provide aid to developers to better understand an architecture based on microservices, we propose a detection approach based on metrics for microservices patterns. We focused on structural or architectural patterns, and implemented detection for five of them. We conducted two case studies with real-world applications and evaluated the accuracy and applicability of our approach with the developers of those applications.},
  keywords={},
  doi={10.1109/SEAA60479.2023.00029},
  ISSN={2376-9521},
  month={Sep.},}

@INPROCEEDINGS{10298321,
  author={Huang, Jun and Yang, Yang and Yu, Hang and Li, Jianguo and Zheng, Xiao},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Twin Graph-Based Anomaly Detection via Attentive Multi-Modal Learning for Microservice System}, 
  year={2023},
  volume={},
  number={},
  pages={66-78},
  abstract={Microservice architecture has sprung up over recent years for managing enterprise applications, due to its ability to independently deploy and scale services. Despite its benefits, ensuring the reliability and safety of a microservice system remains highly challenging. Existing anomaly detection algorithms based on a single data modality (i.e., metrics, logs, or traces) fail to fully account for the complex correlations and interactions between different modalities, leading to false negatives and false alarms, whereas incorporating more data modalities can offer opportunities for further performance gain. As a fresh attempt, we propose in this paper a semi-supervised graph-based anomaly detection method, MSTGAD, which seamlessly integrates all available data modalities via attentive multi-modal learning. First, we extract and normalize features from the three modalities, and further integrate them using a graph, namely MST (microservice system twin) graph, where each node represents a service instance and the edge indicates the scheduling relationship between different service instances. The MST graph provides a virtual representation of the status and scheduling relationships among service instances of a real-world microservice system. Second, we construct a transformer-based neural network with both spatial and temporal attention mechanisms to model the inter-correlations between different modalities and temporal dependencies between the data points. This enables us to detect anomalies automatically and accurately in real-time. Extensive experiments on two real-world datasets verify the effectiveness of our proposed MSTGAD method, achieving competitive performance against state-of-the-art approaches, with a 0.961 F1-score and an average increase of 4.85%. The source code of MST-GAD is publicly available at https://github.com/ant-research/microservice_system_twin_graph_based_anomaly_detection.},
  keywords={},
  doi={10.1109/ASE56229.2023.00138},
  ISSN={2643-1572},
  month={Sep.},}

@INPROCEEDINGS{9932943,
  author={Pramesti, Annisa Ayu and Kistijantoro, Achmad Imam},
  booktitle={2022 9th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)}, 
  title={Autoscaling Based on Response Time Prediction for Microservice Application in Kubernetes}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Containerized application are evolving along with the microservice architectures in distributed application development. This trend shows the importance of managing and orchestrating containerized applications thus applications can operate properly. One of the aspects of container orchestration is scaling or increasing the application’s ability to handle more requests. In this study, an autoscaler based on response time prediction is developed for microservice applications in Kubernetes environment. The prediction function is developed using a machine learning model that features performance metrics at the microservice and node levels. The response time prediction is then used to calculate the number of pods required by the application to meet the target response time. Our experiment shows that the proposed autoscaler can serve more requests that match the target response time compare with the Kubernetes Horizontal Pod Autoscaler (HPA) that are using CPU usage as the target. However, as the consequence, the proposed autoscaler consumes more resources than the Kubernetes HPA.},
  keywords={},
  doi={10.1109/ICAICTA56449.2022.9932943},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10279721,
  author={Kalinagac, Onur and Soussi, Wissem and Anser, Yacine and Gaber, Chrystel and Gür, Gürkan},
  booktitle={ICC 2023 - IEEE International Conference on Communications}, 
  title={Root Cause and Liability Analysis in the Microservices Architecture for Edge IoT Services}, 
  year={2023},
  volume={},
  number={},
  pages={3277-3283},
  abstract={In this work, we present a liability analysis frame-work for root cause analysis (RCA) in the microservices architecture with IoT-oriented containerized network services. We keep track of the performance metrics of microservices, such as service response time, memory usage and availability, to detect anomalies. By injecting faults in the services, we construct a Causal Bayesian Network (CBN) which represents the relation between service faults and metrics. Service Level Agreement (SLA) data obtained from a descriptor named TRAILS (sTakeholder Responsibility, AccountabIlity and Liability deScriptor) is also used to flag service providers which have failed their commitments. In the case of SLA violation, the constructed CBN is used to predict the fault probability of services under given metric readings and to identify the root cause.},
  keywords={},
  doi={10.1109/ICC45041.2023.10279721},
  ISSN={1938-1883},
  month={May},}

@INPROCEEDINGS{9960012,
  author={Ivanov, Rosen and Yordanov, Stanimir and Dinev, Dinko},
  booktitle={2022 International Conference Automatics and Informatics (ICAI)}, 
  title={Internet of Things–based pregnancy tracking and monitoring service}, 
  year={2022},
  volume={},
  number={},
  pages={298-302},
  abstract={This paper presents the architecture and implementation of a service for pregnancy tracking and monitoring. The main goal of the service is to analyze the behavior of pregnant women in order to proactively decide to notify medical staff when symptoms are detected that are risky for the normal pregnancy. This is achieved by (1) providing the necessary pregnancy-related information for each of the gestational weeks (nutrition, physical activity, normal and risk symptoms, necessary screening tests, etc.), (2) analysis of physical activity of pregnant women, (3) measurement and analysis of basic biological indicators using a wireless sensor network (pulse oximeter, human body temperature, biopotential channel to obtain electrocardiogram (ECG) and bioimpedance channel to measure respiration), and (4) receiving push notifications about important events related to the pregnancy; scheduled medical examinations; risk factors; and messages from the obstetrician under whose supervision the pregnant woman is. The service has a distributed architecture - it uses multiple microservices. The communication between clients (mobile app), wireless sensor network and microservices is realized through a message broker. Microservices use its own MongoDB databases hosted in the Azure cluster. Experiments were conducted to prove the feasibility of the service on simulated wireless sensor network.},
  keywords={},
  doi={10.1109/ICAI55857.2022.9960012},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10248319,
  author={Castro, Jessica and Laranjeiro, Nuno and Vieira, Marco},
  booktitle={2023 IEEE International Conference on Web Services (ICWS)}, 
  title={Exploring Logic Scoring of Preference for DoS Attack Detection in Microservice Applications}, 
  year={2023},
  volume={},
  number={},
  pages={573-584},
  abstract={Microservice architectures allow the development of highly scalable, flexible, and manageable systems. However, such architectures raise new security problems and exacerbate the challenge of monitoring applications at runtime due to their high service granularity and distributed nature. Developing effective monitoring and security strategies is thus crucial to effectively detect potential attacks. This paper explores the applicability of Logic Scoring of Preference (LSP), a multi-criteria decision-making method to compute a score based on a set of preferences, for attack detection in microservice applications. We present an extensive experimental study and define a model based on LSP and application-level metrics to characterize the impact of DoS attacks. The output of the model is a unique score used to determine whether a microservice is under a DoS attack. The results of the experimental study show precision, recall, and f1-score rates of more than 80%, indicating that LSP could effectively characterize the application under attack, opening several possibilities for future work.},
  keywords={},
  doi={10.1109/ICWS60048.2023.00076},
  ISSN={2836-3868},
  month={July},}

@INPROCEEDINGS{10036308,
  author={Basciftci, Fatih and Aydemir, Fikri},
  booktitle={2022 IEEE 20th Jubilee International Symposium on Intelligent Systems and Informatics (SISY)}, 
  title={Strategies for Request-Response Logging in Microservices Architecture}, 
  year={2022},
  volume={},
  number={},
  pages={000121-000126},
  abstract={Microservices Architecture is the prevailing architectural choice today for building distributed software systems in various business sectors, such as telecommunications, e-commerce, and finance. It is often necessary to log the full content of request and response messages (i.e., the entire body of these messages) that are entering and leaving such distributed systems due to legal book-keeping requirements. In this work, two systematic design strategies were used to realize the structured logging of request-response messages including their entire message body, particularly in a microservices architecture-based distributed software system. As a case study, a prototype per strategy was implemented and deployed into an existing Microservices Architecture-based banking system, which was provided by a commercial bank for the research presented in this paper in the Kubernetes cluster. Load tests were performed against this banking system to measure average request throughputs and average response times per logging strategy for analysis purposes. The results that are presented in this paper are expected to be helpful for both researchers and practitioners in the software industry who need a similar logging solution.},
  keywords={},
  doi={10.1109/SISY56759.2022.10036308},
  ISSN={1949-0488},
  month={Sep.},}

@INPROCEEDINGS{10262956,
  author={Yang, Yunhao and Jiang, Ying},
  booktitle={2023 IEEE 9th International Conference on Cloud Computing and Intelligent Systems (CCIS)}, 
  title={Microservice Indicator Prediction Method Based on STE and CNN-BiLSTM}, 
  year={2023},
  volume={},
  number={},
  pages={511-515},
  abstract={Due to the extensibility and continuous evolution of microservice architecture, there are a lot of uncertainties in the microservice system, which brings great risks to the reliability of the service. Indicator prediction plays an important role in service reliability. If the predicted value exceeds the safe range, alarms are generated and measures are taken to prevent faults. Therefore, a microservice indicator prediction method based on SET and CNN-BiLSTM is proposed. Symbolic transfer entropy (STE) is used to analyze the nonlinear causality, and a prediction model based on CNN-BiLSTM is established. The simulation results show that this method can capture the causal relationship between the indicators with nonlinear relationship effectively and improve the prediction accuracy.},
  keywords={},
  doi={10.1109/CCIS59572.2023.10262956},
  ISSN={2376-595X},
  month={Aug},}

@INPROCEEDINGS{10303332,
  author={Jhingran, Sushant and Rakesh, Nitin},
  booktitle={2023 International Conference on Sustainable Emerging Innovations in Engineering and Technology (ICSEIET)}, 
  title={Application Deployment and Performance Measurement in Serverless Cloud for Microservices}, 
  year={2023},
  volume={},
  number={},
  pages={173-177},
  abstract={The effectiveness of Cloud technology relies heavily on its ability to perform at a high level. To measure this performance, it is necessary to conduct a performance evaluation based on specific aims and applications and assess the capabilities of the cloud services. In the case of enterprise applications deployed on the cloud, the service provider must consider the application's deployment model, security, networking, and operational constraints. This evaluation involves identifying benchmarks, configuring the system, running tests, analyzing results, and providing recommendations. There are various performance metrics that can be applied to different aspects of the cloud services to evaluate their performance. The figures below display data on resource utilization and the impact of the load on the application. Microservices offer organizations the opportunity to deploy applications on the cloud by providing web service functions and an architecture that enables scaling and updating of applications with minimal inconsistency. Through public cloud technology such as Amazon Web Services, organizations can deploy secure and valuable applications to the cloud.},
  keywords={},
  doi={10.1109/ICSEIET58677.2023.10303332},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10136018,
  author={Lin, Zhichao and Wang, Qingsheng and Yang, Shifeng and Luo, Busheng and Ma, Qiujie and Yu, Chuankun},
  booktitle={2023 8th Asia Conference on Power and Electrical Engineering (ACPEE)}, 
  title={Modeling and Performance Analysis of Cloud-Based Active Distribution Networks Based on EdgeCloudSim}, 
  year={2023},
  volume={},
  number={},
  pages={1682-1687},
  abstract={In order to realise the performance + analysis of the cloud-based active distribution network, this paper proposes the modeling and performance analysis method of the cloud-based active distribution network. Firstly, based on the task processing requirements of the active distribution network, the corresponding task modeling method based on microservices is proposed. Then, the cloud-based active distribution network architecture modeling is proposed, and the corresponding dynamic resource allocation process is realised. In addition, through the professional edge computing simulation software EdgeCloudSim, the modeling of the cloud-based active distribution network is realised. The modeling includes specific scenarios, resource allocation and two task spatio-temporal logics. Task delay and resource load rate are as performance metrics. Finally, with microservices as the research granularity, the performance differences of different task spatio-temporal logics are analysed.},
  keywords={},
  doi={10.1109/ACPEE56931.2023.10136018},
  ISSN={},
  month={April},}

@INPROCEEDINGS{9788687,
  author={Xu, Beibei and Zhao, Yanqing and Kuzminykh, Valeriy and Zhu, Shiwei and Yu, Junfeng and Zhang, Mingjun and Li, Sisi},
  booktitle={ICETIS 2022; 7th International Conference on Electronic Technology and Information Science}, 
  title={Research on the Evaluation System of International S&T Cooperation Based on Microservice Architecture}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={The development of the world has benefited from advances in science and technology, and the destiny of mankind has become closer due to scientific and technological cooperation. International scientific and technological innovation cooperation is one of the important indicators to measure the potential and technological innovation of a country or region. Scientific evaluation and performance evaluation of international scientific and technological cooperation have become important for effectively improving the management level of international scientific and technological cooperation projects and promoting scientific and technological output. Means, the construction of a scientific cooperation evaluation and performance evaluation system has become a realistic demand for promoting international scientific and technological cooperation and strengthening performance management of international scientific and technological cooperation in the new era. Based on the analysis of the data sources, data structure, index evaluation system and system functions of the international scientific and technological cooperation evaluation system, the article proposes the system logic and hierarchical structure under the microservice architecture, and designs and implements the international scientific and technological cooperation evaluation system based on the microservice architecture.},
  keywords={},
  doi={},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{9874065,
  author={Chinthavali, Supriya and Hasan, S.M.Shamimul and Yoginath, Srikanth and Xu, Haowen and Nugent, Phil and Jones, Terry and Engebretsen, Cozmo and Olatt, Joseph and Tansakul, Varisara and Christopher, Carter and Polsky, Yarom},
  booktitle={2022 IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI)}, 
  title={An Alternative Timing and Synchronization Approach for Situational Awareness and Predictive Analytics}, 
  year={2022},
  volume={},
  number={},
  pages={172-177},
  abstract={Accurate and synchronized timing information is required by power system operators for controlling the grid infrastructure (relays, Phasor Measurement Units (PMUs), etc.) and determining asset positions. Satellite-based global positioning system (GPS) is the primary source of timing information. However, GPS disruptions today (both intentional and unintentional) can significantly compromise the reliability and security of our electric grids. A robust alternate source for accurate timing is critical to serve both as a deterrent against malicious attacks and as a redundant system in enhancing the resilience against extreme events that could disrupt the GPS network. To achieve this, we rely on the highly accurate, terrestrial atomic clock-based network for alternative timing and synchronization. In this paper, we discuss an experimental setup for an alternative timing approach. The data obtained from this experimental setup is continuously monitored and analyzed using various time deviation metrics. We also use these metrics to compute deviations of our clock with respect to the National Institute of Standards and Technologys (NIST) GPS data. The results obtained from these metric computations are elaborately discussed. Finally, we discuss the integration of the procedures involved, like real-time data ingestion, metric computation, and result visualization, in a novel microservices-based architecture for situational awareness.},
  keywords={},
  doi={10.1109/IRI54793.2022.00047},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9973518,
  author={Chapman, Martin and G-Medhin, Abigail and Sassoon, Isabel and Kökciyan, Nadin and Sklar, Elizabeth I. and Curcin, Vasa},
  booktitle={2022 IEEE 18th International Conference on e-Science (e-Science)}, 
  title={Using Microservices to Design Patient-facing Research Software}, 
  year={2022},
  volume={},
  number={},
  pages={44-54},
  abstract={With a significant amount of software now being developed for use in patient-facing studies, there is a pressing need to consider how to design this software effectively in order to support the needs of both researchers and patients. We posit that a microservice architecture-which offers a large amount of flexibility for development and deployment, while at the same time ensuring certain quality attributes, such as scalability, are present-provides an effective mechanism for designing such software. To explore this proposition, in this work we show how the paradigm has been applied to the design of Consult, a decision support system that provides autonomous support to stroke patients and is characterised by its use of a data-backed AI reasoner. We discuss the impact that the use of this software architecture has had on the teams developing Consult and measure the performance of the system produced. We show that the use of microservices can deliver software that is able to facilitate both research and effective patient interactions. However, we also conclude that the impact of the approach only goes so far, with additional techniques needed to address its limitations.},
  keywords={},
  doi={10.1109/eScience55777.2022.00019},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9808684,
  author={Hrusto, Adha and Engström, Emelie and Runeson, Per},
  booktitle={2022 IEEE/ACM 10th International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems (SESoS)}, 
  title={Optimization of Anomaly Detection in a Microservice System Through Continuous Feedback from Development}, 
  year={2022},
  volume={},
  number={},
  pages={13-20},
  abstract={Monitoring a microservice system may bring a lot of benefits to development teams such as early detection of run-time errors and various performance anomalies. In this study, we explore deep learning (DL) solutions for detection of anomalous system’s behavior based on collected monitoring data that consists of applications’ and systems’ performance metrics. The study is conducted in a collaboration with a Swedish company responsible for ticket and payment management in public transportation. Moreover, we specifically address a shortage of approaches for evaluating DL models without any ground truth data. Hence, we propose a solution design for anomaly detection and reporting alerts inspired by state-of-the-art DL solutions. Furthermore, we propose a plan for its in-context implementation and evaluation empowered by feedback from the development team. Through continuous feedback from development, the labeled data is generated and used for optimization of the DL model. In this way, a microservice system may leverage DL solutions to address rising challenges within its architecture. CCS CONCEPTS • Software and its engineering → Software post-development issues; • Information systems → Data mining; Computing platforms; • Computing methodologies → Machine learning.},
  keywords={},
  doi={},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10254995,
  author={Garbi, Giulio and Incerto, Emilio and Tribastone, Mirco},
  booktitle={2023 IEEE 16th International Conference on Cloud Computing (CLOUD)}, 
  title={μP: A Development Framework for Predicting Performance of Microservices by Design}, 
  year={2023},
  volume={},
  number={},
  pages={178-188},
  abstract={Microservice (MS) architecture has become a popular paradigm in software engineering and has been embraced in the industry (e.g., Amazon, Netflix) for cloud-based applications with crucial performance requirements. Surprisingly, assessing how the MS designs affect performance is still a challenging issue, which is generally tackled by extensive and expensive profiling. In this paper, we propose $\mu \mathbf{P}$, a novel development framework for MS applications where performance can be predicted $by$ design. $\mu \mathbf{P}$ offers an API that automatically generates a per-formance model based on Layered Queuing Networks (LQNs) without requiring any development effort beyond writing the actual system code. The model can then be queried to predict performance metrics such as response time and utilization of individual microservices. We validate $\mu \mathbf{P}$ on four benchmarks taken from the literature. The results show the effectiveness of $\mu \mathbf{P}$ in accurately predicting performance due to increasing user load, vertical and horizontal scaling. We report prediction errors for response times consistently lower than 10% across a wide range of operating conditions.},
  keywords={},
  doi={10.1109/CLOUD60044.2023.00029},
  ISSN={2159-6190},
  month={July},}

@INPROCEEDINGS{10092637,
  author={Pinciroli, Riccardo and Aleti, Aldeida and Trubiani, Catia},
  booktitle={2023 IEEE 20th International Conference on Software Architecture (ICSA)}, 
  title={Performance Modeling and Analysis of Design Patterns for Microservice Systems}, 
  year={2023},
  volume={},
  number={},
  pages={35-46},
  abstract={The adoption of design patterns in the microservice architecture and cloud-native development scope was recently reviewed to investigate the industry practice. Interestingly, when considering performance-related aspects, practitioners focus on specific metrics (e.g., the time taken to handle requests) to identify sources of performance hindrance. This paper investigates a subset of seven design patterns that industrial practitioners indicate as relevant for system performance. We are interested to quantify the impact of these patterns while considering heterogeneous workloads, thus supporting software architects in understanding the root causes of performance issues. We use queuing networks to build the performance models of the seven design patterns and extract quantitative insights from model-based performance analysis. Our performance models are flexible in their input parameterization and reusable in different application contexts. We find that most design patterns confirm the expectation of practitioners, and our experimental results assess the identified performance gains and pains. One design pattern (i.e., Gateway Offloading) shows the peculiar characteristic of contributing to performance pains in some cases, leading to novel insights about the impact of design patterns in microservice systems.},
  keywords={},
  doi={10.1109/ICSA56044.2023.00012},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10074951,
  author={Li, Gongliang and Wen, Zepeng and Xie, Xin},
  booktitle={2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={Unsupervised Anomaly Detection Based on CNN-VAE with Spectral Residual for KPIs}, 
  year={2022},
  volume={},
  number={},
  pages={1307-1313},
  abstract={Current large-scale applications, such as trading systems, blockchain, social software, etc, are increasingly adopting microservice architecture, which bring challenges to manual operation and maintenance, intrusion detection. In both operations and intrusion detection, there are a common characteristic that service metrics and network traffic are normal for most of the time, but anomaly data is more important. In this paper, we propose an unsupervised anomaly detection algorithm based on convolutional neural network with Spectral Residual, which is verified experimentally and has potential application capability with 19.2% f1-score improvement compared to the Variational AutoEncoder.},
  keywords={},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00204},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{10216673,
  author={Jaival, Madhavi and Mkrtchyan, Katya and Kaplan, Adam},
  booktitle={2022 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Serverless Cloud Functions - Opportunity in Chaos}, 
  year={2022},
  volume={},
  number={},
  pages={1330-1335},
  abstract={Due to its cost-effectiveness and limited scope of administration, Serverless Computing has fast become a favorite cloud computing execution model. Meanwhile, with the rise of distributed cloud architectures and microservices in the last decade, many development teams have adopted the principles of Chaos Engineering. This allows them to assess the effects of random failures or delays on an application. In prior literature, serverless developers measured and reported cold-start penalties and transaction latency, whereas Chaos Engineers have studied security and resiliency in cloud infrastructure. In this work, we combine these approaches to measure the performance of a set of serverless cloud functions which implement common server-side file and database operations. We study each function's performance response under a set of controlled chaos experiments, wherein we emulate various client load conditions, as well as inject random delays into the function execution. We find that under heavy 1000-client load, the longest-latency operations can provide as much as 36.5% improvement to overall response time by failing early.},
  keywords={},
  doi={10.1109/CSCI58124.2022.00239},
  ISSN={2769-5654},
  month={Dec},}

@INPROCEEDINGS{10175431,
  author={Centofanti, C. and Tiberti, W. and Marotta, A. and Graziosi, F. and Cassioli, D.},
  booktitle={2023 IEEE 9th International Conference on Network Softwarization (NetSoft)}, 
  title={Latency-Aware Kubernetes Scheduling for Microservices Orchestration at the Edge}, 
  year={2023},
  volume={},
  number={},
  pages={426-431},
  abstract={Network and computing infrastructures are nowadays challenged to meet the increasingly stringent requirements of novel applications. One of the most critical aspect is optimizing the latency perceived by the end-user accessing the services. New network architectures offer a natural framework for the efficient orchestration of microservices. However, how to incorporate accurate latency metrics into orchestration decisions still represents an open challenge.In this work we propose a novel architectural approach to perform scheduling operations in Kubernetes environment. Existing approaches proposed the collection of network metrics, e.g. latency between nodes in the cluster, via purposely-built external measurement services deployed in the cluster. Compared to other approaches the proposed one: (i) collects performance metrics at the application layer instead of network layer; (ii) relies on latency measurements performed inside the service of interest instead of utilizing external measurement services; (iii) takes scheduling decisions based on effective end-user perceived latency instead of considering the latency between cluster nodes.We show the effectiveness of our approach by adopting an iterative discovery strategy able to dynamically determine which node operates with the lowest latency for the Kubernetes pod placement.},
  keywords={},
  doi={10.1109/NetSoft57336.2023.10175431},
  ISSN={2693-9789},
  month={June},}

@INPROCEEDINGS{9908059,
  author={Leiter, Ákos and Huszti, Dániel and Galambosi, Nándor and Lami, Edina and Salah, Mohamad Saleh and Kulics, Péter and Bokor, László},
  booktitle={2022 13th International Symposium on Communication Systems, Networks and Digital Signal Processing (CSNDSP)}, 
  title={Cloud-native IP-based mobility management: a MIPv6 Home Agent standalone microservice design}, 
  year={2022},
  volume={},
  number={},
  pages={252-257},
  abstract={The ever-increasing traffic and mobility events impose an unprecedented load on mobile networks. Meanwhile, the number of connected users and devices has been growing continuously; hence IPv6 is necessary to serve them. The mobility extension of IPv6 (Mobile IPv6) can also support and handle the rising demand for mobility management in the IP layer. At the same time, concepts like Network Function Virtualization, Software Defined Networks, and microservice architectures have changed the landscape of telecommunication services. In this paper, our prototype implementation is measured and evaluated: what containerization causes in case of different MIPv6-re1ated traffic types on the top of Kubernetes. Additionally, Kubernetes Container Network Interface types are compared for a microservice and container-based standalone Home Agent entity of a cloud-native Mobile IPv6 implementation.},
  keywords={},
  doi={10.1109/CSNDSP54353.2022.9908059},
  ISSN={},
  month={July},}

@INPROCEEDINGS{10118016,
  author={Schindewolf, Marc and Grimm, Daniel and Lingor, Christian and Sax, Eric},
  booktitle={2022 IEEE 1st International Conference on Cognitive Mobility (CogMob)}, 
  title={Toward a Resilient Automotive Service-Oriented Architecture by using Dynamic Orchestration}, 
  year={2022},
  volume={},
  number={},
  pages={000147-000154},
  abstract={Modern software development in vehicles is focusing on a service-oriented approach. Structuring software systems into self-sufficient software components that provide specific capabilities to the overall system allow software engineers to make changes to vehicle functions more granularly. The decentralized SOA approach offers advantages, as it enables loose coupling between components instead of statically implementing their relationships. But with the increasing degree of autonomy and dynamism of the vehicle's software, the system's safety and security requirements are also growing. Preventive measures will no longer suffice here; instead, resilient systems are required that provide a minimum level of safety even in the event of an unexpected problem. Today, a SOA's services are assigned to a hardware platform during development and executed there, which lacks being able to react to problems or changing requirements. One possibility for being more flexible at runtime, is the use of an orchestrator, which dynamically allocates resources to services while retaining the advantages of a loosely coupled architecture. This paper proposes a methodology for implementing a resilient vehicular electronic architecture based on orchestrating containerized software. To avoid a single point of failure, a distributed approach for a dynamic orchestrator that deploys the software to appropriate execution platforms is proposed. The orchestrator makes its deployment decisions based on specifiable parameters (e.g., required RAM, GPU) and dependencies between services. The decision process adapts to changes in these factors dynamically, making the system able to react to external influences. The concept differentiates itself from other approaches by tracking dynamic changes to specified parameters and easily extensible interfaces for new parameters or requirements. In addition, the concept introduces a priority metric to describe the impact of services in the system and models how this metric is inherited through dependencies. The concept is evaluated qualitatively by three exemplary use cases, demonstrating the effect of dynamic orchestration on the resilience of the vehicle.},
  keywords={},
  doi={10.1109/CogMob55547.2022.10118016},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9779839,
  author={Serbout, Souhaila and Lauro, Fabio Di and Pautasso, Cesare},
  booktitle={2022 IEEE 19th International Conference on Software Architecture Companion (ICSA-C)}, 
  title={Web APIs Structures and Data Models Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={84-91},
  abstract={Microservice architectures emphasize keeping components small, to foster autonomy, low coupling and independent evolution. In this large-scale empirical study we measure the size of Web API specifications mined from open source repositories. These APIs are modeled using the OpenAPI Specification (OAS), which, in addition to documenting the offered operations, also contain schemas definitions for the data exchanged with the API request and response message payloads. This study has as a goal to build empirical knowledge about: (1) How big and diverse are real-world web APIs both in terms of their operations and data, (2) How different API structures use and reuse schema definitions. By mining public software repositories on Github, we gathered 42,194 valid OAS specifications published between 2014-2021. These specifications include descriptions of Web APIs of well-known services providers such as Google, VMware (Avi Networks), Twilio, Amazon. After measuring the size of API structures and their data model schemas, we found that most APIs are rather small. Also there is a medium correlation between the size of the APIs’ functional structures and their data models. API developers do reuse schema definitions within the same API model.},
  keywords={},
  doi={10.1109/ICSA-C54293.2022.00059},
  ISSN={2768-4288},
  month={March},}

@INPROCEEDINGS{10011525,
  author={Ünlü, Hüseyin and Hacaloglu, Tuna and Büber, Fatma and Berrak, Kıvılcım and Leblebici, Onur and Demirörs, Onur},
  booktitle={2022 48th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={Utilization of Three Software Size Measures for Effort Estimation in Agile World: A Case Study}, 
  year={2022},
  volume={},
  number={},
  pages={239-246},
  abstract={Functional size measurement (FSM) methods, by being systematic and repeatable, are beneficial in the early phases of the software life cycle for core project management activities such as effort, cost, and schedule estimation. However, in agile projects, requirements are kept minimal in the early phases and are detailed over time as the project progresses. This situation makes it challenging to identify measurement components of FSM methods from requirements in the early phases, hence complicates applying FSM in agile projects. In addition, the existing FSM methods are not fully compatible with today’s architectural styles, which are evolving into event-driven decentralized structures. In this study, we present the results of a case study to compare the effectiveness of different size measures: functional -COSMIC Function Points (CFP)-, event-based - Event Points-, and code length-based - Line of Code (LOC)-on projects that were developed with agile methods and utilized a microservice-based architecture. For this purpose, we measured the size of the project and created effort estimation models based on three methods. It is found that the event-based method estimated effort with better accuracy than the CFP and LOC-based methods.},
  keywords={},
  doi={10.1109/SEAA56994.2022.00045},
  ISSN={},
  month={Aug},}

@ARTICLE{9758767,
  author={Surantha, Nico and Utomo, Oei K. and Lionel, Earlicha M. and Gozali, Isabella D. and Isa, Sani M.},
  journal={IEEE Access}, 
  title={Intelligent Sleep Monitoring System Based on Microservices and Event-Driven Architecture}, 
  year={2022},
  volume={10},
  number={},
  pages={42069-42080},
  abstract={Sleep monitoring using polysomnography (PSG) in hospitals can be considered expensive, so the preferable way is to use contactless and wearable sensors to monitor sleep daily by patients at home. In this study, the Internet-of-Things (IoT) platform was utilized for sleep monitoring with contactless or wearable sensors as an integrated system developed based on an event-driven and microservice architecture. Multiple services that respond to events are provided within the system. Electrocardiogram (ECG) data were used as the input in the sleep monitoring system. The combination of the weighted extreme learning machine (WELM) algorithm with particle swarm optimization (PSO) was used to process the ECG data, followed by fuzzy logic to measure sleep quality, then display the data on the dashboard. Based on the experimental results, the proposed architecture increased throughput by 34.76%, decreased response time by 55.85%, and reduced memory consumption by 37.26% per instance replication compared to the non-event-driven architecture. The accuracies of the sleep stage classification were 78.78% and 73.09% for the three and four classes, respectively, and the area under a receiver operating characteristic (ROC) curve (AUC) reached 0.89 for both the three and four class classifications.},
  keywords={},
  doi={10.1109/ACCESS.2022.3167637},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9912639,
  author={El Malki, Amine and Zdun, Uwe and Pautasso, Cesare},
  booktitle={2022 IEEE International Conference on Service-Oriented System Engineering (SOSE)}, 
  title={Impact of API Rate Limit on Reliability of Microservices-Based Architectures}, 
  year={2022},
  volume={},
  number={},
  pages={19-28},
  abstract={Many API patterns and best practices have been developed around microservices-based architectures, such as Rate Limiting and Circuit Breaking, to increase quality properties such as reliability, availability, scalability, and performance. Even though estimates on such properties would be beneficial, especially during the early design of such architectures, the real impact of the patterns on these properties has not been rigorously studied yet. This paper focuses on API Rate Limit and its impact on reliability properties from the perspective of API clients. We present an analytical model that considers specific workload configurations and predefined rate limits and then accurately predicts the success and failure rates of the back-end services. The model also presents a method for adaptively fine-tuning rate limits. We performed two extensive data experiments to validate the model and measured Rate Limiting impacts, firstly on a private cloud to minimize latency and other biases, and secondly on the Google Cloud Platform to test our model in a realistic cloud environment. In both experiments, we observed a low percentage of prediction errors. Thus, we conclude that our model can provide distributed system engineers and architects with insights into an acceptable value for the rate limits to choose for a given workload. Very few works empirically studied the impact of Rate Limit or similar API-related patterns on reliability.},
  keywords={},
  doi={10.1109/SOSE55356.2022.00009},
  ISSN={2642-6587},
  month={Aug},}

@INPROCEEDINGS{9861873,
  author={Vosteen, Lars and John, Fabian and Schuljak, Joerg and Sievers, Bjoern and Hanemann, Andreas and Hellbrueck, Horst},
  booktitle={Mobile Communication - Technologies and Applications; 26th ITG-Symposium}, 
  title={Practical Security Analysis and Measures for 5G Private Industrial Standalone (SA) Deployments}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={The standardization of the fifth generation of mobile communications has been completed, and the expansion of the 5G system is currently being driven forward. In addition to public mobile networks, the 5G mobile network standard foresees privately operated systems. Private 5G systems are started to be deployed and operated in industrial and academic environments using off-the-shelf components like standard computing hardware, software-defined radios, and open-source software with costs below 10k EUR. 5G systems are extensible and scalable due to the service-oriented architecture of the distributed 5G system. Especially in industrial deployment, the demand for security of networks is high, for example, to protect in-house data. In this paper, we present a security analysis for 5G systems from different possible attack points from the operator’s perspective. We conduct selected attacks to highlight and demonstrate weakness on our private indoor 5G testbed at the University of Applied Sciences in Lübeck. From the results of the security analysis and attacks, we derive measures to improve the security of the 5G system. Finally, we verify the effectiveness of the measures by additional tests.},
  keywords={},
  doi={},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10334274,
  author={Beingolea, Jorge R. and Zegarra, Milagros and Bolivar, Renzo and Rendulich, Jorge and Borja-Murillo, Juan},
  booktitle={2023 IEEE Colombian Conference on Communications and Computing (COLCOM)}, 
  title={Heterogeneous Devices: Network Layer Integration Experience}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The work proposes the development of an integration architecture for highly heterogeneous sensor network ecosystems. The implementation is carried out on a device called the “integration device”. The device functions as a management and abstraction layer, integrated with the communication and data layer of a service-oriented middleware. The integration device controls real-time events through the programming of thread groups, which have the role of managing and abstracting the heterogeneity of data and communication protocols of wireless sensor devices. A part of the integration device implementation is presented, and data transfer rate experiments are conducted to measure its performance.},
  keywords={},
  doi={10.1109/COLCOM59909.2023.10334274},
  ISSN={2771-568X},
  month={July},}

@INPROCEEDINGS{10044743,
  author={Yensabai, Chavapol and Ngoenthai, Waranyu and Leangarun, Teema and Koolpiruck, Diew},
  booktitle={2023 Third International Symposium on Instrumentation, Control, Artificial Intelligence, and Robotics (ICA-SYMP)}, 
  title={Digital Retail Shop Services in Cyber-Physical Retail System: A Case Study of Food Business}, 
  year={2023},
  volume={},
  number={},
  pages={61-64},
  abstract={Food demand is expected to grow substantially as a result of major factors such as population. It necessitates that food manufacturers streamline their supply chain to accommodate shorter product life cycles. To manage sustainable food solutions and successful supply chain management, cyber-physical systems at the supply chain level attempt to challenge the integration of data from suppliers, manufacturing, logistics, and retail. The implementation of Cyber-Physical Retail Systems (CPRS) was developed to sense and analyze dynamic market environments to modify sales and shop operation activities. The data were collected from several ERP modules and operational technology (OT) data. The shop CPS was managed using the OSIsoft-PI platform, which is based on service-oriented architecture (SOA) and then integrated into the Enterprise Cloud. The customer analytics service in CPRS was used as an example of a self-aware concept to notify the sales function and was implemented on the Azure platform. The results show that churn prediction in retail shops can be detected monthly for warning sales staff based on the customer object goal. The models that were used are RF, LR and GBC. The overall performance of GBC outperforms all measures with 78.05% accuracy. While the remaining were around 65%.},
  keywords={},
  doi={10.1109/ICA-SYMP56348.2023.10044743},
  ISSN={},
  month={Jan},}

@ARTICLE{8851303,
  author={Cabrera, Christian and Clarke, Siobhán},
  journal={IEEE Transactions on Services Computing}, 
  title={A Self-Adaptive Service Discovery Model for Smart Cities}, 
  year={2022},
  volume={15},
  number={1},
  pages={386-399},
  abstract={City services are frequently supported by software services that are managed by service-oriented architectures. However, a large number of software services is likely to cause performance issues when discovering software services. The distributed organisation of services information improves discovery performance. Existing research proposes to organise services information according to service location, domains, or city context, keeping that organisation constant under an assumption that cities do not change. However, cities are dynamic environments where entities interact, causing events that in turn, effect changes in the city. The organisation of services information must evolve or it will become outdated, negatively impacting discovery performance. We propose a self-adaptive service model for smart cities to support service discovery. This model adapts the organisation of services information according to city events. We introduce a self-adaptive architecture that keeps track of the discovery metrics and moves information about services between registries to maintain the discovery efficiency. We evaluate the proposed model in simulated environments and a real IoT testbed. Results show that our model outperforms competitors when reactive adaptation is triggered by a specific event. However, proactive adaptation needs further research. Results from the real IoT testbed present the costs of the proposed model.},
  keywords={},
  doi={10.1109/TSC.2019.2944356},
  ISSN={1939-1374},
  month={Jan},}

@ARTICLE{8936375,
  author={Sun, Chang-ai and Dai, Hepeng and Wang, Guan and Towey, Dave and Chen, Tsong Yueh and Cai, Kai-Yuan},
  journal={IEEE Transactions on Services Computing}, 
  title={Dynamic Random Testing of Web Services: A Methodology and Evaluation}, 
  year={2022},
  volume={15},
  number={2},
  pages={736-751},
  abstract={In recent years, service oriented architecture (SOA) has been increasingly adopted to develop distributed applications in the context of the Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of web services. In this article, we propose a dynamic random testing (DRT) technique for web services, which is an improvement over the widely-practiced random testing (RT) and partition testing (PT) approaches. We examine key issues when adapting DRT to the context of SOA, including a framework, guidelines for parameter settings, and a prototype for such an adaptation. Empirical studies are reported where DRT is used to test three real-life web services, and mutation analysis is employed to measure the effectiveness. Our experimental results show that, compared with the three baseline techniques, RT, Adaptive Testing (AT) and Random Partition Testing (RPT), DRT demonstrates higher fault-detection effectiveness with a lower test case selection overhead. Furthermore, the theoretical guidelines of parameter setting for DRT are confirmed to be effective. The proposed DRT and the prototype provide an effective and efficient approach for testing web services.},
  keywords={},
  doi={10.1109/TSC.2019.2960496},
  ISSN={1939-1374},
  month={March},}

@ARTICLE{9222262,
  author={Wang, Chen and Ma, Hui and Chen, Gang and Hartmann, Sven and Branke, J&#x00FC;rgen},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Robustness Estimation and Optimisation for Semantic Web Service Composition With Stochastic Service Failures}, 
  year={2022},
  volume={6},
  number={1},
  pages={77-92},
  abstract={Service-oriented architecture (SOA) is a widely adopted software engineering paradigm that encourages modular and reusable applications. One popular application of SOA is web service composition, which aims to loosely couple web services to accommodate complex goals not achievable through any individual web service. Many approaches have been proposed to construct composite services with optimized Quality of Service (QoS), assuming that QoS of web services never changes. However, the constructed composite services may not perform well and may not be executable later due to its component services' failure. Therefore, it is important to build composite services that are robust to stochastic service failures. Two challenges of building robust composite services are to efficiently generate service composition with near-optimal quality in a large search space of available services and to accurately measure the robustness of composite services considering all possible failure scenarios. This article proposes a novel two-stage GA-based approach to robust web service composition with an adaptive evolutionary control and an efficient robustness measurement. This approach can generate robust composite service at the design phase, which can cope with stochastic service failures and maintain high quality at the time of execution. We have conducted experiments with benchmark datasets to evaluate the performance of our proposed approach. Our experiments show that our method can produce highly robust composite services, achieving outstanding performance consistently in the event of stochastic service failures, on service repositories with varying sizes.},
  keywords={},
  doi={10.1109/TETCI.2020.3027870},
  ISSN={2471-285X},
  month={Feb},}

@ARTICLE{9463396,
  author={Chen, Jeng-Chung and Chen, Chun-Chih and Shen, Chih-Hsiung and Chen, Ho-Wen},
  journal={IEEE Internet of Things Journal}, 
  title={User Integration in Two IoT Sustainable Services by Evaluation Grid Method}, 
  year={2022},
  volume={9},
  number={3},
  pages={2242-2252},
  abstract={To meet the need for sustainable development, Taiwan has been spreading a network of micro-monitoring stations to measure the environmental quality in rivers and air to protect people from environmental pollution. As a result, more and more information technology companies develop Internet of Things (IoT) services for this job. However, most IoT services are screened out of the market because lacking design thinking. Therefore, including users’ desires in IoT products and services is a critical determinant for their survival in this permanently changing market. Thus, this study proposed a systematic framework to identify the users’ desires from different stakeholders to determine technological development. For this, we use the evaluation grid method (EGM) to explore the users’ desires by a series of in-depth interviews and visualize the user’s response as a hierarchical evaluation map of attraction. After that, an IoT prototype is built and used to capture the insightful feedback of respondents. Meanwhile, we adopt the minimum viable product (MVP) design principles to develop two prototypes that manage a wastewater treatment plant and household environment. Overall, this study proposes an applicable user integration procedure to help IT engineers develop the IoT for sustainable service. This study also confirms that the MVP method can help to accelerate user integration. We propose a service-oriented IoT architecture in technology development and develop a decision-making service of human dispatch in operating environmental facilities and a context-awareness service for environmental control.},
  keywords={},
  doi={10.1109/JIOT.2021.3091688},
  ISSN={2327-4662},
  month={Feb},}

@INPROCEEDINGS{8477218,
  author={Li, Keqin},
  booktitle={2018 IEEE 16th International Conference on Software Engineering Research, Management and Applications (SERA)}, 
  title={Quantitative Modeling and Analytical Calculation of Elasticity in Cloud Computing}, 
  year={2018},
  volume={},
  number={},
  pages={3-3},
  abstract={Elasticity is a fundamental feature of cloud computing and can be considered as a great advantage and a key benefit of cloud computing. Our research makes the following significant contributions. First, we present a new, quantitative, and formal definition of elasticity in cloud computing, i.e., the probability that the computing resources provided by a cloud platform match the current workload. Our definition is applicable to any cloud platform and can be easily measured and monitored. Furthermore, we develop an analytical model to study elasticity by treating a cloud platform as a queueing system, and use a continuous-time Markov chain (CTMC) model to precisely calculate the elasticity value of a cloud platform by using an analytical and numerical method based on just a few parameters, namely, the task arrival rate, the service rate, the virtual machine start-up and shut-down rates. In addition, we formally define auto-scaling schemes and point out that our model and method can be easily extended to handle arbitrarily sophisticated scaling schemes. Second, we apply our model and method to predict many other important properties of an elastic cloud computing system, such as average task response time, throughput, quality of service, average number of VMs, average number of busy VMs, utilization, cost, cost-performance ratio, productivity, and scalability. In fact, from a cloud consumer's point of view, these performance and cost metrics are even more important than the elasticity metric. Ourperformance and cost guarantee using the results developed in this talk. On the other hand, a cloud service provider can optimize its elastic scaling scheme to deliver the best cost-performance ratio. study in this talk has two significance. On one hand, a cloud service provider can predict its To the best of our knowledge, this is the first work that analytically and comprehensively studies elasticity, performance, and cost in cloud computing. Our model and method significantly contribute to the understanding of cloud elasticity and management of elastic cloud computing systems.},
  keywords={},
  doi={10.1109/SERA.2018.8477218},
  ISSN={},
  month={June},}

@INPROCEEDINGS{8477227,
  author={Li, Keqin},
  booktitle={2018 IEEE 16th International Conference on Software Engineering Research, Management and Applications (SERA)}, 
  title={Quantitative Modeling and Analytical Calculation of Elasticity in Cloud Computing}, 
  year={2018},
  volume={},
  number={},
  pages={3-3},
  abstract={Elasticity is a fundamental feature of cloud computing and can be considered as a great advantage and a key benefit of cloud computing. Our research makes the following significant contributions. First, we present a new, quantitative, and formal definition of elasticity in cloud computing, i.e., the probability that the computing resources provided by a cloud platform match the current workload. Our definition is applicable to any cloud platform and can be easily measured and monitored. Furthermore, we develop an analytical model to study elasticity by treating a cloud platform as a queueing system, and use a continuous-time Markov chain (CTMC) model to precisely calculate the elasticity value of a cloud platform by using an analytical and numerical method based on just a few parameters, namely, the task arrival rate, the service rate, the virtual machine start-up and shut-down rates. In addition, we formally define auto-scaling schemes and point out that our model and method can be easily extended to handle arbitrarily sophisticated scaling schemes. Second, we apply our model and method to predict many other important properties of an elastic cloud computing system, such as average task response time, throughput, quality of service, average number of VMs, average number of busy VMs, utilization, cost, cost-performance ratio, productivity, and scalability. In fact, from a cloud consumer's point of view, these performance and cost metrics are even more important than the elasticity metric. Ourperformance and cost guarantee using the results developed in this talk. On the other hand, a cloud service provider can optimize its elastic scaling scheme to deliver the best cost-performance ratio. study in this talk has two significance. On one hand, a cloud service provider can predict its To the best of our knowledge, this is the first work that analytically and comprehensively studies elasticity, performance, and cost in cloud computing. Our model and method significantly contribute to the understanding of cloud elasticity and management of elastic cloud computing systems.},
  keywords={},
  doi={10.1109/SERA.2018.8477227},
  ISSN={},
  month={June},}

@INPROCEEDINGS{7207388,
  author={Zhou, Nianjun and Mohindra, Ajay},
  booktitle={2015 IEEE International Conference on Services Computing}, 
  title={Causality-Driven Performance Monitoring and Scaling Automation for Managed Solutions}, 
  year={2015},
  volume={},
  number={},
  pages={467-474},
  abstract={A key feature of Cloud computing is its agility and flexibility to support the scalability needs of business solutions. Currently, the agility is only limited to the scalability of the compute, memory and storage. To improve an application's agility, we need to monitor & measure solution level metrics and associate the performance of the metrics to the business agility needs of the solution by making real-time scalability or change decisions. In this paper, we illustrate a scaling decision mechanism utilizing the monitoring data from infrastructure, middleware, and business level metrics. We use these performance metrics as input to a causality analysis model to make architecture changes or scalability decisions. Mathematically, we define the causality as a graph to link the changes in the measured metric values to the action of the solution change. The causality analysis follows scalability principles as best practices. They are a) the principle of performance scalability b) principle of contribution margin for scalability, and c) principle of the least cost of SLA compliance. We define these scalability principles as the rules to ensure that the business stakeholder of the solution can maintain or improve their business quality or profit margins as the computing capability scales up or down. To implement those principles, we need to establish the linkages of the business metrics to the decision of changes. To make such linkage, we first utilize causality analysis to identify feasible scaling actions, and then associate those actions with the system, application, and business performance metrics. With the help of causality analysis, we implement a performance monitoring and scaling automation framework for managed solutions using an Open Source Monitoring system.},
  keywords={},
  doi={10.1109/SCC.2015.70},
  ISSN={},
  month={June},}

@ARTICLE{9139920,
  author={Guerron, Ximena and Abrahão, Silvia and Insfran, Emilio and Fernández-Diego, Marta and González-Ladrón-De-Guevara, Fernando},
  journal={IEEE Access}, 
  title={A Taxonomy of Quality Metrics for Cloud Services}, 
  year={2020},
  volume={8},
  number={},
  pages={131461-131498},
  abstract={A large number of metrics with which to assess the quality of cloud services have been proposed over the last years. However, this knowledge is still dispersed, and stakeholders have little or no guidance when choosing metrics that will be suitable to evaluate their cloud services. The objective of this paper is, therefore, to systematically identify, taxonomically classify, and compare existing quality of service (QoS) metrics in the cloud computing domain. We conducted a systematic literature review of 84 studies selected from a set of 4333 studies that were published from 2006 to November 2018. We specifically identified 470 metric operationalizations that were then classified using a taxonomy, which is also introduced in this paper. The data extracted from the metrics were subsequently analyzed using thematic analysis. The findings indicated that most metrics evaluate quality attributes related to performance efficiency (64%) and that there is a need for metrics that evaluate other characteristics, such as security and compatibility. The majority of the metrics are used during the Operation phase of the cloud services and are applied to the running service. Our results also revealed that metrics for cloud services are still in the early stages of maturity - only 10% of the metrics had been empirically validated. The proposed taxonomy can be used by practitioners as a guideline when specifying service level objectives or deciding which metric is best suited to the evaluation of their cloud services, and by researchers as a comprehensive quality framework in which to evaluate their approaches.},
  keywords={},
  doi={10.1109/ACCESS.2020.3009079},
  ISSN={2169-3536},
  month={},}

@ARTICLE{7153530,
  author={Zhao, Feng and Nian, Guodong and Jin, Hai and Yang, Laurence T. and Zhu, Yajun},
  journal={IEEE Systems Journal}, 
  title={A Hybrid eBusiness Software Metrics Framework for Decision Making in Cloud Computing Environment}, 
  year={2017},
  volume={11},
  number={2},
  pages={1049-1059},
  abstract={Developing high-quality software is essential for eBusiness organizations to cope with drastic market competition. With the development of cloud computing technologies, eBusiness systems and applications pay more attention to open endedness. In a cloud computing environment, eBusiness systems have the ability to provide information technology resources on demand. Traditional software metric methods in distributed systems and applications are technical and project driven, making the market demand and internal practical operation not perfectly balanced within a cloud-computing-based eBusiness corporation. To address this issue, this paper presents a hybrid framework based on the goal/question/metric paradigm to evaluate the quality and efficiency of previous software products, projects, and development organizations in a cloud computing environment. In our approach, to support decision making at the project and organization levels, three angular metrics are used, i.e., project metrics, product metrics, and organization metrics. Furthermore, an improved radial-basis-function-based model is also provided to manage existing projects and design new projects. Experimental results on a well-known eBusiness organization show that the proposed framework is effective, efficient, and operational. Moreover, using the described decision-making algorithm, the predicted data are very close to actual results on the software cost, the fault rate, the development workload, etc., which are greatly helpful in achieving high-quality software.},
  keywords={},
  doi={10.1109/JSYST.2015.2443049},
  ISSN={1937-9234},
  month={June},}

@ARTICLE{7845614,
  author={Li, Keqin},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Quantitative Modeling and Analytical Calculation of Elasticity in Cloud Computing}, 
  year={2020},
  volume={8},
  number={4},
  pages={1135-1148},
  abstract={Elasticity is a fundamental feature of cloud computing and can be considered as a great advantage and a key benefit of cloud computing. One key challenge in cloud elasticity is lack of consensus on a quantifiable, measurable, observable, and calculable definition of elasticity and systematic approaches to modeling, quantifying, analyzing, and predicting elasticity. Another key challenge in cloud computing is lack of effective ways for prediction and optimization of performance and cost in an elastic cloud platform. The present paper makes the following significant contributions. First, we present a new, quantitative, and formal definition of elasticity in cloud computing, i.e., the probability that the computing resources provided by a cloud platform match the current workload. Our definition is applicable to any cloud platform and can be easily measured and monitored. Furthermore, we develop an analytical model to study elasticity by treating a cloud platform as a queueing system, and use a continuous-time Markov chain (CTMC) model to precisely calculate the elasticity value of a cloud platform by using an analytical and numerical method based on just a few parameters, namely, the task arrival rate, the service rate, the virtual machine start-up and shut-down rates. In addition, we formally define auto-scaling schemes and point out that our model and method can be easily extended to handle arbitrarily sophisticated scaling schemes. Second, we apply our model and method to predict many other important properties of an elastic cloud computing system, such as average task response time, throughput, quality of service, average number of VMs, average number of busy VMs, utilization, cost, cost-performance ratio, productivity, and scalability. In fact, from a cloud consumer's point of view, these performance and cost metrics are even more important than the elasticity metric. Our study in this paper has two significance. On one hand, a cloud service provider can predict its performance and cost guarantee using the results developed in this paper. On the other hand, a cloud service provider can optimize its elastic scaling scheme to deliver the best cost-performance ratio. To the best of our knowledge, this is the first paper that analytically and comprehensively studies elasticity, performance, and cost in cloud computing. Our model and method significantly contribute to the understanding of cloud elasticity and management of elastic cloud computing systems.},
  keywords={},
  doi={10.1109/TCC.2017.2665549},
  ISSN={2168-7161},
  month={Oct},}

@INPROCEEDINGS{8613297,
  author={Al-Said Ahmad, Amro and Andras, Peter},
  booktitle={2018 Fifth International Symposium on Innovation in Information and Communication Technology (ISIICT)}, 
  title={Measuring and Testing the Scalability of Cloud-based Software Services}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Performance and scalability testing and measurements of cloud-based software services are critically important in the context of rapid growth of cloud computing and supporting the delivery of these services. Cloud-based software services performance aspects are interrelated, both elasticity and efficiency are depending on the delivery of a sufficient level of scalability performance. In this work, we focused on testing and measuring the scalability of cloud-based software services in technical terms. This paper uses technical scalability metrics that address both volume and quality scaling, that inspired by earlier technical metrics of elasticity. We show how our technical scalability metrics can be integrated into an earlier utility oriented metric of scalability. We demonstrate the application of the metrics using a practical example and discuss the importance of them.},
  keywords={},
  doi={10.1109/ISIICT.2018.8613297},
  ISSN={},
  month={Oct},}

@ARTICLE{8391708,
  author={Xu, Han and Qiu, Xiwei and Sheng, Yongpan and Luo, Liang and Xiang, Yanping},
  journal={IEEE Access}, 
  title={A Qos-Driven Approach to the Cloud Service Addressing Attributes of Security}, 
  year={2018},
  volume={6},
  number={},
  pages={34477-34487},
  abstract={Recently, cloud computing has been widely used by relying on its powerful resource integration and computing abilities. In the cloud computing system (CCS), the quality of service (QoS) is an important service evaluation criterion from provider and client perspectives, which directly affects the client experience and profit of the cloud providers. Thus, a precise evaluation of the QoS can help the cloud provider develop reasonable resource allocation strategies for improving the client experience. The performance metric is usually adopted to quantify QoS. Many approaches and methods for evaluating performance have been widely studied. However, another important metric, i.e., security, does not receive adequate attention in the evaluation of QoS. More importantly, security also has serious effects on the performance metric, that is, complex security-performance (S-P) correlations. To address these issues, this paper first builds a Markov model to analyze and assess the security of the CCS that captures two critical security factors, i.e., malicious attacks and the security protection mechanism. Then, a hierarchical modeling approach is presented to flexibly build the connection between security and the service performance. Finally, we propose a correlation metric to quantify random service performance. This correlation metric comprehensively considers the effect of the security factors and thus becomes more realistic and precise. The experimental results reveal the dynamic change of performance caused by the security factors and demonstrate the important S-P correlation. Therefore, security cannot be ignored in the modeling and evaluation of the QoS metric.},
  keywords={},
  doi={10.1109/ACCESS.2018.2849594},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{6968746,
  author={Poggi, Nicolas and Carrera, David and Ayguadé, Eduard and Torres, Jordi},
  booktitle={2014 IEEE International Conference on Cluster Computing (CLUSTER)}, 
  title={POSTER: Profit-aware cloud resource provisioner for ecommerce}, 
  year={2014},
  volume={},
  number={},
  pages={274-275},
  abstract={In recent years, the Cloud Computing paradigm has proven effective in scaling dynamically the number of servers according to simple performance metrics and the incoming workload. However while some applications are able to scale-out, as current scaling metrics do not relate system performance to sales, hosting costs and profits are not optimized completely. The following article proposes a novel technique for dynamic resource provisioning based on revenue and cost metrics, to optimize profits for online retailers in the Cloud. The proposal relies on user behavior models that relate Quality-of-Service (QoS) to service capacity, and to the intention of users to buy a product on an Ecommerce site. We show how such metrics can enable profit-aware resource management by setting an optimal number of servers at each time of the day. Experiments are performed on custom, real-life datasets from an Ecommerce retailer contain over two years of access, performance, and sales data from popular travelWeb applications.},
  keywords={},
  doi={10.1109/CLUSTER.2014.6968746},
  ISSN={2168-9253},
  month={Sep.},}

@INPROCEEDINGS{9719517,
  author={Nayak, Samaleswari Prasad and Rout, Suchismita and Das, Surajit and Patra, Sudhansu Shekhar},
  booktitle={2021 19th OITS International Conference on Information Technology (OCIT)}, 
  title={Error rate reduction of Air Quality Parameters in Health Care Industry using SD-IoT Environment}, 
  year={2021},
  volume={},
  number={},
  pages={454-459},
  abstract={The air quality index has a major impact on the health of a person. This parameter must be carefully monitored to take all the necessary arrangements to improve the environmental conditions. The error rate must be minimized for accurate data collection and processing. In this paper, an IoT -based air quality platform is designed with the name ‘'IAQM (Industry Air Quality Monitoring)”. This platform relies on IoT, SDN, and Cloud computing technology to monitor the air quality of the health care industry. Apart from the health care sector, this system can be used anywhere and anytime. We measure its performance metrics with existing AQMS. IAQM gives better performance than the existing approach as per the resultant graphs.},
  keywords={},
  doi={10.1109/OCIT53463.2021.00094},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8343082,
  author={Ibrahim, Abdallah A. Z. A. and Varrette, Sebastien and Bouvry, Pascal},
  booktitle={2018 International Conference on Information Networking (ICOIN)}, 
  title={PRESENCE: Toward a novel approach for performance evaluation of mobile cloud SaaS Web Services}, 
  year={2018},
  volume={},
  number={},
  pages={50-55},
  abstract={Cloud Services Providers (CSPs) deliver cloud services to cloud customers on a pay-per-use model. The quality of the provided services are defined using Service Level Agreements (SLAs). The recent developments around edge computing and the advent of mobile cloud computing platforms contribute to the success of this approach and the multiplication of offers. Unfortunately, despite the projections foreseeing a growing market for the coming years, there is no standard mechanism which exists to verify and assure that delivered services satisfy the signed SLA agreement. Accurate measures of the provided Quality of Service (QoS) is also missing most of time. In this context, we aim at offering an automatic framework named PRESENCE, to evaluate the QoS and SLA compliance of Web Services (WSs) offered across several CSPs. PRESENCE aims at quantifying in a fair and by stealth way the performance and scalability of the delivered WS. By stealthiness, we refer to the capacity of evaluating a given Cloud service by orchestrating multiple workload patterns that making them indistinguishable from a regular user traffic from the provider point of view. PRESENCE defines a set of Common performance metrics handled by a set of agents within a customized client (called the Auditor) for measuring the behaviour of cloud applications on top of a given CSP. This position paper offers a description of the PRESENCE framework, and the way each modules are foreseen to be designed. This opens novel perspectives for assessing the SLA compliance of Cloud providers using the PRESENCE framework.},
  keywords={},
  doi={10.1109/ICOIN.2018.8343082},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{7913123,
  author={Khurana, Ravi and Bawa, Rajesh Kumar},
  booktitle={2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)}, 
  title={Quality based cloud simulators: State-of-the-art & road ahead}, 
  year={2016},
  volume={},
  number={},
  pages={101-106},
  abstract={Cloud Computing is an emerging technology nowadays. It has been used by many leading organisations. They deploy their critical information onto the cloud. Several challenges are associated with it like quality issues, security, energy consumption etc. Continuous research is going on to cater these issues. It is not easy to setup a cloud for any researcher or group of researchers, it needs huge investment. Big organisations having huge budgets can only afford that. To address these issues cloud simulators are really helpful. Cloud simulator is a simulating environment through which one can realize actual cloud environment. Data centers, virtual machines, hosts and networks can be setup virtually. Numbers of cloud simulators are there in the literature each offering different scenario. Cloudsim, a well known cloud simulator calculates start time, finish time and total time for execution of cloudlet. Another cloud simulator GreenCloud calculates energy consumption by data centers, hosts, switches and other network equipments. In the present paper, we focus on quality metrics addressed by cloud simulators. With each simulator, we will enlist quality metrics discussed by them. At the end, we conclude that there is a need to develop simulator which will address relevant quality metrics.},
  keywords={},
  doi={10.1109/PDGC.2016.7913123},
  ISSN={},
  month={Dec},}

@ARTICLE{9220139,
  author={Feng, Jie Xu and Si, Guannan and Zhou, Fengyu},
  journal={IEEE Access}, 
  title={Overview and Framework of Quality Service Metrics for Cloud-Based Robotics Platforms}, 
  year={2020},
  volume={8},
  number={},
  pages={185885-185898},
  abstract={With the rapid development of big data, cloud computing and other technologies, Cloud-based robotic has become one of the key research directions for service robot, such as used in hospitals. A framework and set of metrics for evaluating the quality of service (QOS) of a cloud robotic platform would be greatly facilitate research into and actual practice of service robots. In this paper, a QOS metrics framework of cloud robotic computing is summarized and the research of components and metrics of a cloud robotic platform is reviewed. QOS metrics are organized into software, network, and robotic services. By summarizing and analyzing the above three groups of metrics, a QOS framework or index system is proposed. Finally, future research towards open source and standardization of components of robotic cloud platform is discussed.},
  keywords={},
  doi={10.1109/ACCESS.2020.3030069},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8058324,
  author={Upadhyaya, Jolly and Ahuja, Neelu Jyoti},
  booktitle={2017 International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Quality of service in cloud computing in higher education: A critical survey and innovative model}, 
  year={2017},
  volume={},
  number={},
  pages={137-140},
  abstract={Cloud Computing, an emerging trend, in the e-learning sector has attracted number of service providers to the market in very less time, providing users with several applications at their disposal. However, while providing such service, not sufficient importance is given to the quality of the service, especially from the user's point of view. Hence it becomes necessary to monitor, track and quantify the QoS of the cloud computing e-learning applications in order to provide the right information to both the customers and the service providers. This information would help both the parties in terms of the comparison between the expectations and the capacity to meet them, but in this sector there is no standard model which defines the QoS parameters from the user's point of view. Thus, the need arises for developing a metrics model for enhancing the quality of service in cloud computing e-learning applications for higher education sector. In the current work, Quality of Service models are studied and comprehensive review of work done in this field is presented. Additionally an innovative QoS model for resolving this issue has been suggested.},
  keywords={},
  doi={10.1109/I-SMAC.2017.8058324},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{8389549,
  author={Geetha, P. and Robin, C.R. Rene},
  booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)}, 
  title={A comparative-study of load-cloud balancing algorithms in cloud environments}, 
  year={2017},
  volume={},
  number={},
  pages={806-810},
  abstract={The enrichment knowledge of Cloud Computing is Green Cloud Computing. The term of Cloud Computing is a globally inter — connected networks of Computing Resources( Servers, Networks, Applications, Hardwares, Softwares). The Green Computing is an Environmental Benefits of eco-friendly usage of Computing Resources. The combination of Green Computing and Cloud computing is Green Cloud Computing. GCC performs both performance and efficiency. The combination of Mobile Computing and Cloud Computing is known as Mobile Cloud Computing. Now, the Computational science is changing to be data-intensive. So, Load balancing is a technique to distribute the load across a given Green Cloud Network Vs Mobile Cloud Network. In this proposed system, the in-depth analysis of Load Balancing Algorithms. The Load of Cloud Balancing is a process of reassigning the total load to the individual nodes in a given network. Then the Comparative study of load balancing algorithms with its quality metrics are summarized.},
  keywords={},
  doi={10.1109/ICECDS.2017.8389549},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8467224,
  author={Dhirani, Lubna Luxmi and Newe, Thomas and Nizamani, Shahzad},
  booktitle={2018 5th International Multi-Topic ICT Conference (IMTIC)}, 
  title={Hybrid Cloud Computing QoS Glitches}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={The Hybrid Cloud Computing model has been growing extensively due to its Infrastructure as a Service (IaaS) architecture, customisation and cost benefits. The hybrid cloud services are measured based on the Quality of Service parameters defined by the public cloud vendors. These parameters (i.e. availability, scalability, latency etc.) vary from vendor-to-vendor, developing complexity and confusion on the grounds of methods of service assessments. A Cloud Service Level Agreement (SLA) lists the QoS provisions to be provided to the tenant, the objectives, and exclusions. Regardless of vendors promised uptimes and service metrics, the tenants are susceptible to the following threats: data governance, Denial of Services, multi-tenancy, etc. Cloud computing has often been compared as a utility, but the basic different between a utility and the cloud is the amount of risk involved with data protection, provisioning and control. Few cloud standards have been developed for standardizing the hybrid cloud model but since each public cloud vendor provides different applications and services, these standards do not resolve the existing cloud QoS issue. Since each enterprise implementing the cloud and vendor supplying the services is diverse, a customized Trio (Cloud-IT-Business) QoS model is required to resolve the business need. The authors have designed a model to resolve this existing cloud QoS issue, the abstraction of the model is detailed in this paper.},
  keywords={},
  doi={10.1109/IMTIC.2018.8467224},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8611562,
  author={Shin, Young-Rok and Son, A-Young and Jo, Hyeok Kyun and Huh, Eui-Nam},
  booktitle={2018 Second World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)}, 
  title={Cloud Service Broker Based Quality Metrics Integration Model for Mobile Environment}, 
  year={2018},
  volume={},
  number={},
  pages={254-259},
  abstract={Mobile cloud computing is high technology that extends existing IT capabilities and requirements. And it can also access to shareable remote computing resources pool through the network. As the concept of mobile cloud, many providers have served the mobile cloud services using their own service policies. In other words, there is no formal definition of quality criteria for mobile cloud service evaluation. To solve this problem, some quality models are proposed for cloud service evaluation. However, those did not include many metrics to evaluate the services. Even if the model included a number of criteria, it is difficult to identify whether the metrics are proper or not. Furthermore, most existing models were not concerned about mobile characteristics. Therefore, we propose a cloud service integration model to solve the problem as we mentioned above. First, we select additional metrics to satisfy the mobile characteristics. Second, we present an extended SLA model for modeling complex service-dependencies in mobile cloud services. Third, we describe a method of discovering relations between the metrics of service belonging to mobile cloud services and then using these relations for establishing newly generated SLA.},
  keywords={},
  doi={10.1109/WorldS4.2018.8611562},
  ISSN={},
  month={Oct},}

@ARTICLE{9178326,
  author={Ahamed Ahanger, Tariq and Tariq, Usman and Ibrahim, Atef and Ullah, Imdad and Bouteraa, Yassine},
  journal={IEEE Access}, 
  title={ANFIS-Inspired Smart Framework for Education Quality Assessment}, 
  year={2020},
  volume={8},
  number={},
  pages={175306-175318},
  abstract={In the education sector, the Internet of Things (IoT) technology, integrated with fog-cloud computing, has offered productive services. Motivated by this, the smart recommender system offers the facility to the students to opt for the course and college based on the education quality. This research provides an IoT-fog-cloud paradigm for evaluating the academic environment with a perspective to enhance quality education. Specifically, IoT technology is incorporated to gather data about the academic environment that directly and indirectly influence the quality of education. Using the Bayesian Modeling Technique, the data collected is analyzed utilizing a fog-cloud computing framework to quantify the measure of the probability of education quality (PoEQ). Moreover, the Education Quality Assurance Index (EQAI) is calculated to analyze the quality assessment over a temporal scale. Furthermore, predictive decision-making is performed for quality estimation using the Adaptive Neuro-Fuzzy Inference System (ANFIS). The experimental simulation on 4 challenging datasets namely C1 (2124 instances), C2 (2112), C3 (2139), and C4 (2109) shows the effectiveness of the proposed framework. Simulation findings are compared with state-of-the-art techniques to measure the overall performance enhancement of the proposed system. Also, the mathematical analysis was carried out to assess the analytical performance of the proposed framework.},
  keywords={},
  doi={10.1109/ACCESS.2020.3019682},
  ISSN={2169-3536},
  month={},}

@ARTICLE{8207422,
  author={Noormohammadpour, Mohammad and Raghavendra, Cauligi S.},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={Datacenter Traffic Control: Understanding Techniques and Tradeoffs}, 
  year={2018},
  volume={20},
  number={2},
  pages={1492-1525},
  abstract={Datacenters provide cost-effective and flexible access to scalable compute and storage resources necessary for today's cloud computing needs. A typical datacenter is made up of thousands of servers connected with a large network and usually managed by one operator. To provide quality access to the variety of applications and services hosted on datacenters and maximize performance, it deems necessary to use datacenter networks effectively and efficiently. Datacenter traffic is often a mix of several classes with different priorities and requirements. This includes user-generated interactive traffic, traffic with deadlines, and long-running traffic. To this end, custom transport protocols and traffic management techniques have been developed to improve datacenter network performance. In this tutorial paper, we review the general architecture of datacenter networks, various topologies proposed for them, their traffic properties, general traffic control challenges in datacenters and general traffic control objectives. The purpose of this paper is to bring out the important characteristics of traffic control in datacenters and not to survey all existing solutions (as it is virtually impossible due to massive body of existing research). We hope to provide readers with a wide range of options and factors while considering a variety of traffic control mechanisms. We discuss various characteristics of datacenter traffic control, including management schemes, transmission control, traffic shaping, prioritization, load balancing, multipathing, and traffic scheduling. Next, we point to several open challenges as well as new and interesting networking paradigms. At the end of this paper, we briefly review inter-datacenter networks that connect geographically dispersed datacenters, which have been receiving increasing attention recently and pose interesting and novel research problems. To measure the performance of datacenter networks, different performance metrics have been used, such as flow completion times, deadline miss rate, throughput, and fairness. Depending on the application and user requirements, some metrics may need more attention. While investigating different traffic control techniques, we point out the tradeoffs involved in terms of costs, complexity, and performance. We find that a combination of different traffic control techniques may be necessary at particular entities and layers in the network to improve the variety of performance metrics. We also find that despite significant research efforts, there are still open problems that demand further attention from the research community.},
  keywords={},
  doi={10.1109/COMST.2017.2782753},
  ISSN={1553-877X},
  month={Secondquarter},}

@INPROCEEDINGS{7336367,
  author={Zhou, Ping and Wang, Zhipeng and Li, Wenjing and Jiang, Ning},
  booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems}, 
  title={Quality Model of Cloud Service}, 
  year={2015},
  volume={},
  number={},
  pages={1418-1423},
  abstract={In recent years, services based on cloud computing have been used more and more widely. Stakeholders have paid more and more attention on the quality of cloud service. But most of them don't know how to evaluate the quality of cloud service. This paper proposes a comprehensive, structurized, and hierarchical quality model of cloud service, which concerned not only the IT features but also the service features of cloud service. The quality model was constructed by 6 characteristics, i.e., usability, security, reliability, tangibility, responsiveness, and empathy. We divided each characteristic into several subcharacteristics. In order to apply the cloud service model better, and to evaluate the service quality systematically, we provide a metrics framework for those subcharacteristics, which was made up of objective and subjective metrics. We give a brief intro to the methodology on evaluating the cloud service quality. We also illustrate the evaluation process with a case study.},
  keywords={},
  doi={10.1109/HPCC-CSS-ICESS.2015.134},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9711797,
  author={Padma, P. and Akshaya, RS. and Akshaya, H. and Harini, R.},
  booktitle={2021 4th International Conference on Computing and Communications Technologies (ICCCT)}, 
  title={Perlustrate Study on Cloud Security and Vulnerabilities}, 
  year={2021},
  volume={},
  number={},
  pages={293-296},
  abstract={In this modern technology,cloud computing plays an integral role which is also the fastest emerging technology. Cloud computing refers to manipulating,configuring and accessing the applications online. It offers online data storage,infrastructure and application. It is both a combination of software and hardware based computing resources delivered as a network service. High Quality services with improved performance and with reduced cost made the cloud computing a popular paradigm. Cloud computing has numerous advantages to the customer, like its ability to scale and recover from various problems agility and flexibility. As every technology emerges with its own pros and cons cloud computing is vulnerable to certain threats regarding security issues which makes the clients a lack of confidence to adopt cloud technologies. The main reason why companies are leaving the cloud is due to security concerns. Cloud security measures are often inadequate to protect sensitive data. This work aims at presenting a survey of various security issues faced by clients and the necessary measures to counter these threats.},
  keywords={},
  doi={10.1109/ICCCT53315.2021.9711797},
  ISSN={},
  month={Dec},}

@ARTICLE{6740846,
  author={Zheng, Xianrong and Martin, Patrick and Brohman, Kathryn and Xu, Li Da},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={CLOUDQUAL: A Quality Model for Cloud Services}, 
  year={2014},
  volume={10},
  number={2},
  pages={1527-1536},
  abstract={Cloud computing is an important component of the backbone of the Internet of Things (IoT). Clouds will be required to support large numbers of interactions with varying quality requirements. Service quality will therefore be an important differentiator among cloud providers. In order to distinguish themselves from their competitors, cloud providers should offer superior services that meet customers' expectations. A quality model can be used to represent, measure, and compare the quality of the providers, such that a mutual understanding can be established among cloud stakeholders. In this paper, we take a service perspective and initiate a quality model named CLOUDQUAL for cloud services. It is a model with quality dimensions and metrics that targets general cloud services. CLOUDQUAL contains six quality dimensions, i.e., usability, availability, reliability, responsiveness, security, and elasticity, of which usability is subjective, whereas the others are objective. To demonstrate the effectiveness of CLOUDQUAL, we conduct empirical case studies on three storage clouds. Results show that CLOUDQUAL can evaluate their quality. To demonstrate its soundness, we validate CLOUDQUAL with standard criteria and show that it can differentiate service quality.},
  keywords={},
  doi={10.1109/TII.2014.2306329},
  ISSN={1941-0050},
  month={May},}

@INPROCEEDINGS{7427071,
  author={Khan, Hassan Mahmood and Chan, Gaik-Yee and Chua, Fang-Fang},
  booktitle={2016 International Conference on Information Networking (ICOIN)}, 
  title={An adaptive monitoring framework for ensuring accountability and quality of services in cloud computing}, 
  year={2016},
  volume={},
  number={},
  pages={249-253},
  abstract={Cloud computing platform has gained popularity among service providers and consumers to perform business operations due to the ease of communication and transaction convenience in terms of accessibility and availability. However, due to the vulnerability of this dynamic open environment, it is crucial to have a binding agreement between all the service parties for ensuring trust while fulfilling the expected Quality of Services (QoS). There is a need to improve on the current Service Level Agreements (SLAs) practice which does not focus on the QoS and accountability assurance. In this paper, we propose an adaptive monitoring framework to dynamically monitor QoS metrics and performance measures to verify compliances to the respective SLAs. The framework is validated with scenarios on response time and availability which shown to provide adaptive remedy action to rectify violation situation. Besides, any service party which establishes non-compliance to SLAs shall be penalized in monetary terms.},
  keywords={},
  doi={10.1109/ICOIN.2016.7427071},
  ISSN={},
  month={Jan},}

@INBOOK{9116755,
  author={Wu, Chu‐ge and Wang, Ling},
  booktitle={Fog Computing: Theory and Practice: Theory and Practice}, 
  title={An Estimation of Distribution Algorithm to Optimize the Utility of Task Scheduling Under Fog Computing Systems}, 
  year={2020},
  volume={},
  number={},
  pages={371-384},
  abstract={The Internet of Things (IoT) is realized initially today. A large amount of data is produced and a range of IoT services are settled down. Based on it, a range of responsive IoT applications arise. To satisfy the quality of experience (QoE) of users, the applications are needed to be processed in a timely manner. Compared with traditional cloud computing systems, fog computing is one of the promising solutions to processing the huge amount of local data and decreasing the end‐to‐end latency. Different time‐dependent functions are adopted to measure the utility of different tasks and in this work, the resource allocation and task scheduling problem under the fog system is considered to maximize the sum of the utility of tasks. And an estimation of distributed algorithm to maximum the task utility (uEDA) with a repair procedure and local search is adopted to determine the task processing order and computing node allocation. The comparative results show that the performance of our algorithm exceeds significantly the heuristic method on the utility metrics.},
  keywords={},
  doi={10.1002/9781119551713.ch14},
  ISSN={},
  publisher={Wiley},
  isbn={},
  url={https://ieeexplore.ieee.org/document/9116755},}

@INPROCEEDINGS{6974100,
  author={Brilhante, Jonathan and Silva, Bruno and Maciel, Paulo and Zimmermann, Armin},
  booktitle={2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Dependability models for Eucalyptus infrastructure clouds considering VM life-cycle}, 
  year={2014},
  volume={},
  number={},
  pages={1336-1341},
  abstract={Managing a cloud computing provider is a difficult task, which involves the control and maintenance of several components, such as computers, network infrastructures and software components. In these environments, availability, security and low costs are important requirements to achieve high quality of service. Therefore, the evaluation of these systems is important to find a configuration that meets the constraints of users and provider. A widely adopted strategy to evaluate cloud computing systems consists by the utilization of stochastic models (e.g., stochastic Petri nets - SPN) to assess the concern metrics. In this context, Eucalyptus is an open source private cloud software for building private and hybrid clouds. This work presents dependability models for evaluation on Eucalyptus clouds. These models focus on the user point of view metrics (e.g., number of running virtual machines) to assess the dependability metrics. In order to demonstrate the feasibility of the proposed models, we evaluate a real world environment and validate the presented models by using Eucabomber tool version 2.0.},
  keywords={},
  doi={10.1109/SMC.2014.6974100},
  ISSN={1062-922X},
  month={Oct},}

@ARTICLE{8131852,
  author={Hussain, Omar Khadeer and Rahman, Zia-ur- and Hussain, Farookh Khadeer and Singh, Jaipal and Janjua, Naeem Khalid and Chang, Elizabeth},
  journal={The Computer Journal}, 
  title={A User-Based Early Warning Service Management Framework in Cloud Computing}, 
  year={2015},
  volume={58},
  number={3},
  pages={472-496},
  abstract={Cloud computing is a very attractive option for service users and service providers for their businesses because of the benefits it provides. A major concern among service users regarding cloud adoption, however, is the unpredictability of performance in relation to the services provided. Even though guarantees in the form of service-level agreements are provided to users by service providers, real-time service-level degradability remains a critical concern; hence, there is a need for an approach that assists users to manage a service before it fails. The approaches proposed in the literature assess and evaluate the performance of the cloud infrastructure of providers, but this does not guarantee that a given service instance will meet the desired quality level because there may be factors other than the provider's infrastructure that will affect the level of quality of the service instance. In this paper, we present an approach that measures the quality of a service instance in real time and provides important analysis for service users as to whether they will achieve their desired objectives. This analysis also constitutes an important input for service users in the assessment and management of a service to avoid the failure to achieve objectives.},
  keywords={},
  doi={10.1093/comjnl/bxu064},
  ISSN={1460-2067},
  month={March},}

@INPROCEEDINGS{8251864,
  author={Gustamas, R. Gargista and Shidik, Guruh Fajar},
  booktitle={2017 International Seminar on Application for Technology of Information and Communication (iSemantic)}, 
  title={Analysis of network infrastructure performance on cloud computing}, 
  year={2017},
  volume={},
  number={},
  pages={169-174},
  abstract={Cloud Computing offers more convenience than conventional that provide custom Virtual Machine (VM) for any computation requirements. Network connectivity is closely related to the quality of cloud infrastructure itself. This paper focus in preliminary study to test the performance of cloud infrastructure with two type test. First test to measure Network performance and the second to measure cloud computation performance. OpenStack was used as cloud computing software infrastructure. We perform simple cloud infrastructure topology which is divided into three zones, there are Internal Zone, External Zone and Outside Cloud Infrastructure Zone. The parameter tested in this research are quality of bandwidth, latency, jitter and also Processing time during rendering process. The results show VM from simple topology cloud computing which is used to render video, able to perform processing time that slightly longer than using personal computer (PC) with same specification. The network side has been considering as a key of degradation render performance in cloud computing.},
  keywords={},
  doi={10.1109/ISEMANTIC.2017.8251864},
  ISSN={},
  month={Oct},}

@ARTICLE{9070142,
  author={Xu, Jianwen and Ota, Kaoru and Dong, Mianxiong},
  journal={China Communications}, 
  title={A real plug-and-play fog: Implementation of service placement in wireless multimedia networks}, 
  year={2019},
  volume={16},
  number={10},
  pages={191-201},
  abstract={Initially as an extension of cloud computing, fog computing has been inspiring new ideas about moving computing tasks to the edge of networks. In fog, we often repeat the procedure of placing services because of the geographical distribution of mobile users. We may not expect a fixed demand and supply relationship between users and service providers since users always prefer nearby service with less time delay and transmission consumption. That is, a plug-and-play service mode is what we need in fog. In this paper, we put forward a dynamic placement strategy for fog service to guarantee the normal service provision and optimize the Quality of Service (QoS). The simulation results show that our strategy can achieve better performance under metrics including energy consumption and end-to-end latency. Moreover, we design a real Plug-and-Play Fog (PnPF) based on Raspberry Pi and OpenWrt to provide fog services for wireless multimedia networks.},
  keywords={},
  doi={10.23919/JCC.2019.10.012},
  ISSN={1673-5447},
  month={Oct},}

@ARTICLE{8130579,
  author={Kumar, Neeraj and Chilamkurti, Naveen and Zeadally, Sherali and Jeong, Young-Sik},
  journal={The Computer Journal}, 
  title={Achieving Quality of Service (QoS) Using Resource Allocation and Adaptive Scheduling in Cloud Computing with Grid Support}, 
  year={2014},
  volume={57},
  number={2},
  pages={281-290},
  abstract={In the past few years, cloud computing has emerged as a new reliable, scalable and flexible virtual computing environment (VCE). In this new VCE, users can use the available resources as a service by paying for that service according to the time for which these resources are used. It remains a significant challenge to achieve quality of service (QoS) in a VCE with the available resources. The main goal is to schedule the available resources so that the overall QoS delivered by the VCE can be improved. Resources are assumed to be located both at local and global sites. We propose a three-step scheme: resource selection, scheduling of users requests with shared resources and a new Resource Allocation and Adaptive Job Scheduling algorithm, which improves the QoS delivered by the cloud. For job scheduling, we define a new weight metric that is used to efficiently schedule jobs competing for available resources. Our proposed strategy increases the reliability of resource availability for a job and reduces the job completion time, which in turn increases the QoS delivered to end-users. We evaluate our proposed scheme using well-known heuristics. The results obtained show that our proposed scheme considerably reduces the job execution time, and increases the reliability of resource availability for job execution and throughput.},
  keywords={},
  doi={10.1093/comjnl/bxt024},
  ISSN={1460-2067},
  month={Feb},}

@INPROCEEDINGS{7852595,
  author={Rodziah binti Atan},
  booktitle={2016 2nd International Conference on Science in Information Technology (ICSITech)}, 
  title={Enhancing service quality through Service Level Agreement (SLA) full implementation}, 
  year={2016},
  volume={},
  number={},
  pages={1-1},
  abstract={Various SLA monitoring systems are proposed by different features and abilities to evaluate the agreed SLA. The current SLA monitoring systems in cloud computing for its structural, behavioral characteristics and situation are also in place. The systematic reviews of a well-known methods and approaches shows a significant numbers of researches been done in this area. Based on the number of effort and researches, the quality of services should proportionately increase alongside them. We look this matter from the perspectives of enforcement, that evident the stand of quality of services. Service Level Agreement (SLA) enforcement impact measures is a potential research area to be explored. Assumptions that this study is making are, SLA management will become better by a firm enforcement, where every customers are responsible to launch report of bugs or mischief of services such as unsatisfactory quality or service unavailability to a collection pool, and the provider will react immediately to the complaints so that the total downtime not exceeding the SLA value, with efficient enforcement. This study establishes fundamental theory to measure enforcement impact to SLA monitoring and management. We proposed eight activity phases from formulating until analyzing and decision formation. Descriptive statistics is utilized to analyze the extracted data. The SLA validation detection is the most frequent purpose of SLA monitoring systems in cloud by 58% and throughput is checked as an attribute target by 28%. The self-monitoring SLA, self-healing system, hierarchical structure are recognized points of SLA monitoring systems which need improvement before the enforcement could be based upon.},
  keywords={},
  doi={10.1109/ICSITech.2016.7852595},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{7193080,
  author={Vijayakumar, N and Ramya, R},
  booktitle={2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)}, 
  title={The real time monitoring of water quality in IoT environment}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  abstract={In order to ensure the safe supply of the drinking water the quality needs to be monitor in real time. In this paper we present a design and development of a low cost system for real time monitoring of the water quality in IOT(internet of things). The system consist of several sensors is used to measuring physical and chemical parameters of the water. The parameters such as temperature, PH, turbidity, conductivity, dissolved oxygen of the water can be measured. The measured values from the sensors can be processed by the core controller. The raspberry PI B+ model can be used as a core controller. Finally, the sensor data can be viewed on internet using cloud computing.},
  keywords={},
  doi={10.1109/ICIIECS.2015.7193080},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7159459,
  author={Vijayakumar, N and Ramya, R},
  booktitle={2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]}, 
  title={The real time monitoring of water quality in IoT environment}, 
  year={2015},
  volume={},
  number={},
  pages={1-4},
  abstract={In order to ensure the safe supply of the drinking water the quality needs to be monitor in real time. In this paper we present a design and development of a low cost system for real time monitoring of the water quality in IOT(internet of things).the system consist of several sensors is used to measuring physical and chemical parameters of the water. The parameters such as temperature, PH, turbidity, conductivity, dissolved oxygen of the water can be measured. The measured values from the sensors can be processed by the core controller. The raspberry PI B+ model can be used as a core controller. Finally, the sensor data can be viewed on internet using cloud computing.},
  keywords={},
  doi={10.1109/ICCPCT.2015.7159459},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9302797,
  author={Flinck Lindström, Sebastian and Wetterberg, Markus and Carlsson, Niklas},
  booktitle={2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)}, 
  title={Cloud Gaming: A QoE Study of Fast-paced Single-player and Multiplayer Gaming}, 
  year={2020},
  volume={},
  number={},
  pages={34-45},
  abstract={Cloud computing offers an attractive solution for modern computer games. By moving the increasingly demanding graphical calculations (e.g., generation of real-time video streams) to the cloud, consumers can play games using small, cheap devices. While cloud gaming has many advantages and is increasingly deployed, not much work has been done to understand the underlying factors impacting players' user experience when moving the processing to the cloud. In this paper, we study the impact of the quality of service (QoS) factors most affecting the players' quality of experience (QoE) and in-game performance. In particular, these relationships are studied from multiple perspectives using complementing analysis methods applied on the data collected via instrumented user tests. During the tests, we manipulated the players' network conditions and collected low-level QoS metrics and in-game performance, and after each game, the users answered questions capturing their QoE. New insights are provided using different correlation/auto-correlation/cross-correlation statistics, regression models, and a thorough breakdown of the QoS metric most strongly correlated with the users' QoE. We find that the frame age is the most important QoS metric for predicting in-game performance and QoE, and that spikes in the frame age caused by large frame transfers can have extended negative impact as they can cause processing backlogs. The study emphasizes the need to carefully consider and optimize the parts making up the frame age, including dependencies between the processing steps. By lowering the frame age, more enjoyable gaming experiences can be provided.},
  keywords={},
  doi={10.1109/UCC48980.2020.00023},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9702517,
  author={Chauhan, Rishabh and Kumar, Sunil},
  booktitle={2021 5th International Conference on Information Systems and Computer Networks (ISCON)}, 
  title={Packet Loss Prediction Using Artificial Intelligence Unified with Big Data Analytics, Internet of Things and Cloud Computing Technologies}, 
  year={2021},
  volume={},
  number={},
  pages={01-06},
  abstract={Big Data Analytics, Artificial Intelligence and cloud computing all together has emerged with an ultimate goal of automating and changing human life by providing their services. These incredibly strong technologies have huge potential by working together, making human life simpler and advanced. To increase the popularity of any of these services, Quality of Service metrics are needed to be defined clear. One of those quality metrics is packet loss or packet delivery, which is the main research idea of this paper. With advancement in Intelligent Network there exists a scope to predict packet loss, by analyzing the recorded network traffic and processing said data under certain machine learning algorithms to create a model to either predict packet loss or tell which variable is responsible for packet loss. This paper includes the study of packet loss behavior of networks. The Analytics techniques applied successfully by analyzing big network traffic data, processing of data, using AI and Machine Learning classifier “XGBoost” and hence designed a model to predict Packet loss which is a QoS metric with an accuracy of 90 percent. The model is personalized to work on WireShark data.},
  keywords={},
  doi={10.1109/ISCON52037.2021.9702517},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9284541,
  author={Martins, Wictor Souza and Tardiole Kuehne, Bruno and Sobrinho, Rafael Ferreira and Preti, Fábio},
  booktitle={2020 IEEE International Conference on Services Computing (SCC)}, 
  title={A Reference Method for Performance Evaluation in Big Data Architectures}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper presents a reference method for performance evaluation in Big Data architectures, called by Improvement Method for Big Data Architectures (IMBDA) aiming to increase the performance, and consequently raising the quality of service provided. The method will contribute to small businesses and startups that have limited financial re-sources (impossible to invest in market solutions). The proposed approach considers the relationship of the processes in a data processing flow to find possible bottlenecks and optimization points. To this end, IMBDA collects system logs to compose functional metrics (e.g., processing time) and non-functional metrics (e.g., CPU and memory utilization, and other cloud computing infrastructure resources). The system stores these metrics in an external data analysis tool that investigates the correlation of performance between processes. The reference method applies to the architecture of a Big Data application, which provides solutions in fleet logistics. With the use of IMBDA, it was possible to identify performance bottlenecks, allowing the reconfiguration of the architecture to increase service quality at the lowest possible cost.},
  keywords={},
  doi={10.1109/SCC49832.2020.00044},
  ISSN={2474-2473},
  month={Nov},}

@INPROCEEDINGS{7536961,
  author={Chaemin Seong and Minsoo Jang and Kyungshik Lim},
  booktitle={2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN)}, 
  title={Context-aware HTTP Adaptive Streaming in mobile cloud environments}, 
  year={2016},
  volume={},
  number={},
  pages={1062-1067},
  abstract={With advances of cloud computing, seamless video streaming from video server to cloud client has been one of technical challenges for multimedia cloud applications. Especially in case that Desktop-as-a-Service (DaaS) as a major cloud application is deployed via wireless networks, it could raise a new set of issues to be addressed. To solve the problem, we propose a Cloud-based Context-aware HTTP Adaptive Streaming (C2HAS) agent located at cloud server. The goal of the agent is to maximize the video quality of seamless streaming perceived by cloud client, given a dynamically changing network context. From network context we derive a major metric for adapting and maximizing the video quality perceived by cloud clients, which is the throughput ratio of backbone networks and access networks. Based on the metric, we can provide a maximal quality of seamless video streaming to cloud users who might be connected via distant and/or lossy wireless links. The experimental performance analysis shows that the C2HAS agent could be a viable solution for cloud-based multimedia applications.},
  keywords={},
  doi={10.1109/ICUFN.2016.7536961},
  ISSN={2165-8536},
  month={July},}

@INPROCEEDINGS{9333865,
  author={Han, Jaehyun and Zhu, Guangyu and Lee, Eunseo and Lee, Sangmook and Son, Yongseok},
  booktitle={2021 International Conference on Information Networking (ICOIN)}, 
  title={An Empirical Evaluation and Analysis of Performance of Multiple Optane SSDs}, 
  year={2021},
  volume={},
  number={},
  pages={541-545},
  abstract={Cloud Computing as a service-on-demand architecture has grown in importance over the previous few years. The storage subsystem in cloud computing has undergone enormous innovation in order to provide high-quality cloud services. Emerging non-volatile memory express (NVMe) technology has a considerable attraction in cloud computing by delivering high I/O performance in terms of latency and bandwidth. Especially, multiple NVMe SSDs can provide higher performance, fault tolerance, and storage capacity in the cloud computing environment. In this paper, we perform an empirical evaluation study of performance on recent NVMe SSDs (i.e., Intel Optane SSDs) with different RAID environments. We analyze the performance of the multiple NVMe SSDs with RAID in terms of different performance metrics via micro and macro benchmarks. We anticipate that the experimental results and performance analysis will provide the implications on various storage systems.},
  keywords={},
  doi={10.1109/ICOIN50884.2021.9333865},
  ISSN={1976-7684},
  month={Jan},}

@INPROCEEDINGS{8883458,
  author={Biondi, Katalina and Al-Masri, Eyhab and Baiocchi, Orlando and Jeyaraman, Suganya and Pospisil, Eric and Boyer, Graham and de Souza, Cleonilson Protasio},
  booktitle={2019 International Conference in Engineering Applications (ICEA)}, 
  title={Air Pollution Detection System using Edge Computing}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Existing solutions to measuring air quality can be expensive and potentially mutes high air pollution events. The IoT Pollution Project is exploring how IoT concepts can be applied with smart systems to detect pollution in real-time. Using a network of Raspberry Pi prototypes, the project aims to measure heavily populated areas around the City of Tacoma, while building a real-time interface measuring current air quality. The project also explores the use of edge computing as an alternative to cloud computing. The vast expansion of IoT devices poses threats to the infrastructure of cloud computing as more devices process and store data to the cloud. The project demonstrates how edge devices can alleviate the work done on the cloud by calculating rolling averages over a time interval on the edge device and then deploying the data to the cloud. The project uses Microsoft Azure Framework, IoT concepts and edge computing concepts to build the project architecture.},
  keywords={},
  doi={10.1109/CEAP.2019.8883458},
  ISSN={},
  month={July},}

@INPROCEEDINGS{8073634,
  author={Muralitharan, D. Boobala and Reebha, S. Arockia Babi and Saravanan, D.},
  booktitle={2017 International Conference on IoT and Application (ICIOT)}, 
  title={Optimization of performance and scheduling of HPC applications in cloud using cloudsim and scheduling approach}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={Cloud computing is emerging as a promising alternative to supercomputers for some High-Performance Computing (HPC) applications. Cloud computing is an essential component of the back bone of the Internet of Things (IoT). Clouds are needed to support huge numbers of interactions with varying quality requirements. Hence, Service quality will be a vital differentiator among cloud providers. In order to differentiate themselves from their competitors, cloud providers should offer best services that meet customers' expectations. A quality model can be used to represent, measure and compare the quality of the providers, such that a mutual understanding can be established among clouds take holders. With cloud as an additional deployment option, HPC users and providers faces the challenges of dealing with highly heterogeneous resources, where the variability spans across a wide range of processor configurations, interconnects, virtualization environments, and pricing models. HPC applications are increasingly being used in academia and laboratories for scientific research and in industries for business and analytics. Cloud computing offers the benefits of virtualization, elasticity of resources and elimination of cluster setup cost and time to HPC applications users. Effort was taken for holistic viewpoint to answer the questions - why and who should choose cloud for HPC, for what applications and how the cloud can be used for HPC? Comprehensive performance and cost evaluation and analysis of running a set of HPC applications on a range of platforms, varying from supercomputers to clouds was carried out. Further, performance of HPC applications is improved in cloud by optimizing HPC applications' characteristics for cloud and cloud virtualization mechanisms for HPC. In this paper, a novel heuristics for online application-aware job scheduling in multi-platform environments is presented. Experimental results and Simulations using CloudSim show that current clouds cannot substitute supercomputers but can effectively complement them.},
  keywords={},
  doi={10.1109/ICIOTA.2017.8073634},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8920865,
  author={Hlaing, Yamin Thet Htar and Yee, Tin Tin},
  booktitle={2019 International Conference on Advanced Information Technologies (ICAIT)}, 
  title={Static Independent Task Scheduling on Virtualized Servers in Cloud Computing Environment}, 
  year={2019},
  volume={},
  number={},
  pages={55-59},
  abstract={Cloud Computing is the advanced design of client-server computing, cluster computing and grid computing. The cloud providers provide cloud services mainly as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS) to the users who can access publicly, privately or hybrid via the Internet. In Cloud computing, there are many research areas like task scheduling, allocation of resource, security and privacy etc. Task scheduling is a vital area in the cloud computing, and it must be optimized by considering different parameters. Nowadays, there are a lot of different scheduling algorithms to minimize execution time and cost, to improve the quality of service, system performance and to maximize resource utilization and load balancing, etc. This paper proposed a Static Independent Task Scheduling on Virtualized Servers in Cloud Computing Environment in which tasks are allocated to the suitable VM by measuring the availability of each resource with respect to its processing power, cost and the number of available processing elements and by grouping tasks according to their instruction length. This method is simulated on Cloud Simulator (Cloudsim toolkit) and results show the proposed method that maximizes total execution time and minimizes execution cost for all tasks than scheduling algorithms such as Shortest Job First (SJF) and First Come First Serve (FCFS) algorithms.},
  keywords={},
  doi={10.1109/AITC.2019.8920865},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7566494,
  author={Chandu P.M.S.S and Kata, Divyasree},
  booktitle={2016 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)}, 
  title={Integrating and enhancing the quality of services in cloud computing with software testing}, 
  year={2016},
  volume={},
  number={},
  pages={2008-2010},
  abstract={Cloud computing involves to delivering the hosted services throughout the internet. Testing tools are used to test the desktop applications, web applications and the cloud based software systems that are used to address the quality of the cloud infrastructure such as tremendous extensibility and aggressive composition. In the existing paper it is not providing the quality of services in the effective manner. In this paper we focused on integrating the software metrics for getting the quality of services, in terms of speed, memory size, RAM, ROM size and we are also using the D-cloud and prefail testing tools to perform the fault tolerance and recovery testing. By using OVMP algorithm we are minimizing the cost spending for services and load prediction algorithm and it is also used to reduce the load. The aim is to extend the above framework with cross cloud testing scenario involving communications between heterogeneous cloud hosts. The results shows that the cloud environment ensures more flexible and quality of services.},
  keywords={},
  doi={10.1109/WiSPNET.2016.7566494},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8097142,
  author={Jelassi, Mariem and Ghazel, Cherif and Saïdane, Leila Azzouz},
  booktitle={2017 3rd International Conference on Frontiers of Signal Processing (ICFSP)}, 
  title={A survey on quality of service in cloud computing}, 
  year={2017},
  volume={},
  number={},
  pages={63-67},
  abstract={The quality of service is one of challenges posed by the Cloud Computing. This issue plays an important role in making the Cloud services acceptable to customers, denotes the levels of performance, reliability, and availability offered by Cloud services. Literature has reported many implementations for measuring and ensuring QoS in Cloud Computing systems to achieve better results and meet the needs of producers and consumers. In this paper, we have presented a survey on QoS in Cloud Computing, the mechanisms and methods to guarantee quality of service (QoS) used to Cloud Computing services.},
  keywords={},
  doi={10.1109/ICFSP.2017.8097142},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7924927,
  author={Yazhou Hu and Bo Deng and Fuyang Peng},
  booktitle={2016 2nd IEEE International Conference on Computer and Communications (ICCC)}, 
  title={Autoscaling prediction models for cloud resource provisioning}, 
  year={2016},
  volume={},
  number={},
  pages={1364-1369},
  abstract={The elasticity mechanism of cloud computing can auto scale cloud resources to meet users' need. Elastic adding or removing virtual machines is the most common method to achieve the auto scaling. But the elastic scaling often takes tens of minutes, which is inefficient for the running workload. To reduce the latency and improve the quality of service (QoS), the new virtual machine should be provisioned when the request arrives. In this paper, we present a prediction framework for virtual machines provisioning. This prediction framework includes three main modules: monitor, filter and predictor. This framework aims to predict the upcoming workload and provision the virtual machines in advance. To get the reasonable monitored metrics, we propose the Kalman filter method to preprocess the raw data. Moreover, we present five different prediction models as the based predictor. These prediction models include moving average (MA), auto regression (AR), auto regression integrated moving average (ARIMA), neural networks (NN) and support vector machine (SVM). Meanwhile, we propose four evaluation metrics, including the prediction error, the time saving, under-prediction resource and over-prediction resource, to evaluate the performance of prediction framework. In addition, we use Alicloud as the experimental infrastructure. Experimental results demonstrate that the prediction framework can reduce the latency of provisioning cloud resource and improve the cloud service quality.},
  keywords={},
  doi={10.1109/CompComm.2016.7924927},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9397031,
  author={Haque, Halima and Labeeb, Kashshaf and Riha, Rabea Basri and Khan, Md. Nasfikur R.},
  booktitle={2021 International Conference on Emerging Smart Computing and Informatics (ESCI)}, 
  title={IoT Based Water Quality Monitoring System By Using Zigbee Protocol}, 
  year={2021},
  volume={},
  number={},
  pages={619-622},
  abstract={This paper dictates the damages caused by water and what can be done to resolve those issues by involving the Internet of things (IoT). Keeping the quality of water in check is today's ultimate objective. Thereby, to guarantee safe drinking water supply, the quality of water should be observed regularly. The use of IoT based solution, focused mainly on water quality monitoring has therefore been suggested. In order to support the issue, an IoT-based water quality checking network has been introduced that continuously monitors and evaluates the quality of water and tries to distinguish whether it is up to the mark for general use. This paper includes the use of specific sensors that calculates the various parameters of the quality of water which includes conductivity and dissolved oxygen (DO), turbidity, pH, and temperature. The values from the sensors have been measured and calculated using the microcontrollers. Then these processed remote values have been transmitted to the raspberry pi, the central controller which uses the Zigbee protocol. Lastly, all the data from the sensors are then accessible via cloud computing through any browser, on request.},
  keywords={},
  doi={10.1109/ESCI50559.2021.9397031},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9604373,
  author={Wang, Lei and He, Qiang and Gao, Demin and Wan, Jing and Zhang, Yunqiu},
  booktitle={2021 IEEE World Congress on Services (SERVICES)}, 
  title={Temporal-Perturbation aware Reliability Sensitivity Measurement for Adaptive Cloud Service Selection}, 
  year={2021},
  volume={},
  number={},
  pages={8-8},
  abstract={Benefiting from the pay-as-you-go business model, cloud computing has significantly promoted service computing techniques in real-world industrial applications. Software applications based on cloud computing are becoming more and more popular. By integrating existing component cloud services through the internet, composite cloud systems can be built to meet sophisticated application logic. Stable execution of such systems is desirable in the long term so that the service-level agreements (SLAs), as well as users’ quality of experience (QoE), can be fulfilled. To achieve this goal, it is critical to identify and fault-tolerate system components at high risks of failing. This is extremely challenging due to the dynamic and uncertainty of the cloud environment that hosts the component cloud services. Nevertheless, existing approaches pay little attention to the modeling and analysis of system components’ reliability time series. To address the above issues, we first present a reliability evaluation method for component cloud services based on the reliability model and their failure probability under continuous client-side invocation tests. Then, we propose a perturbation-aware reliability sensitivity measurement approach (named PARS) for measuring the reliability sensitivity of component cloud services. It first analyzes the negative perturbations in component cloud services’ historical reliability time series based on the Markov chain rule. Then, it calculates the reliability sensitivity of component cloud services by analyzing how their reliability perturbations impact the reliability of the entire cloud system. To guarantee the execution quality of the composite cloud system, we further propose a proactive adaptation approach named PA-PARS that enables 1-out-of-2 N-version Programming fault-tolerance for composite cloud systems based on PARS. PA-PARS takes the reliability sensitivity of component cloud services estimated by PARS as input to assure the reliability of the cloud system. It consists of four parts: 1) risky system component identification; 2) adaptation trigger; 3) candidate component cloud service selection; and 4) NVP-based system construction as the proactive adaptation for the composite cloud system. The results of experiments conducted on two widely-used datasets demonstrate the effectiveness and efficiency of the proposed approaches in ensuring the reliability of composite cloud systems.},
  keywords={},
  doi={10.1109/SERVICES51467.2021.00019},
  ISSN={2642-939X},
  month={Sep.},}

@ARTICLE{7355287,
  author={Zuo, Liyun and Shu, Lei and Dong, Shoubin and Zhu, Chunsheng and Hara, Takahiro},
  journal={IEEE Access}, 
  title={A Multi-Objective Optimization Scheduling Method Based on the Ant Colony Algorithm in Cloud Computing}, 
  year={2015},
  volume={3},
  number={},
  pages={2687-2699},
  abstract={For task-scheduling problems in cloud computing, a multi-objective optimization method is proposed here. First, with an aim toward the biodiversity of resources and tasks in cloud computing, we propose a resource cost model that defines the demand of tasks on resources with more details. This model reflects the relationship between the user's resource costs and the budget costs. A multi-objective optimization scheduling method has been proposed based on this resource cost model. This method considers the makespan and the user's budget costs as constraints of the optimization problem, achieving multi-objective optimization of both performance and cost. An improved ant colony algorithm has been proposed to solve this problem. Two constraint functions were used to evaluate and provide feedback regarding the performance and budget cost. These two constraint functions made the algorithm adjust the quality of the solution in a timely manner based on feedback in order to achieve the optimal solution. Some simulation experiments were designed to evaluate this method's performance using four metrics: 1) the makespan; 2) cost; 3) deadline violation rate; and 4) resource utilization. Experimental results show that based on these four metrics, a multi-objective optimization method is better than other similar methods, especially as it increased 56.6% in the best case scenario.},
  keywords={},
  doi={10.1109/ACCESS.2015.2508940},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7976761,
  author={Bashar, Abul},
  booktitle={2017 IEEE 7th International Advance Computing Conference (IACC)}, 
  title={BN-Based Approach for Predictive Admission Control of Cloud Services}, 
  year={2017},
  volume={},
  number={},
  pages={59-64},
  abstract={A phenomenal growth in the demand for Cloud Computing services by the cloud consumers has necessitated the efficient and proactive management of the data center hosted services having varied characteristics. One of the major issues concerning both the cloud service providers and consumers is the provisioning of highest level of Quality of Service (QoS) under unpredictable service demands, while maintaining required revenue targets. Traditional Admission Control (AC) approaches which are usually mathematical or analytical in nature, have limited performance levels in the situations where service types, QoS parameters and user demands become highly unpredictable. To this end, an opportunity exists to utilize the self-learning capabilities of Machine Learning (ML) approaches to incorporate predictive and adaptive Admission Control of service requests without violating the Service Level Agreements (SLA) and simultaneously ensuring targeted revenue to the providers. This paper proposes, implements and evaluates a Bayesian Networks based predictive modeling framework (termed as BNSAC) to provide an autonomic Admission Control of cloud service requests. In summary, the BN-based model learns the historical behavior of the system involving various performance metrics (indicators) and predicts the desired unknown metric (e.g. SLA parameter) for making admission control decisions. It presents simulated experimental results involving various service demand scenarios which provide insights into the feasibility and applicability of the proposed approach for improving the QoS in the cloud computing setup.},
  keywords={},
  doi={10.1109/IACC.2017.0027},
  ISSN={2473-3571},
  month={Jan},}

@INPROCEEDINGS{9510130,
  author={Bhonde, Aparna and Devane, Satish},
  booktitle={2021 International Conference on Communication information and Computing Technology (ICCICT)}, 
  title={Impact of Cloud Attacks on Service Level Agreement}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Cloud computing has taken center stage in the current business due to reduced cost, high performance and zero infrastructure. Cloud computing paradigm is yet unable to provide quality of service (QoS) by complying service level agreement (SLA) because of lack of analysis on the unaddressed security issues, challenges, threats which has paved the path for researchers to overcome the lapses in order to improve the QoS. In this paper, we present a survey on the cloud service model attacks and threat analysis. It is been observed after thorough analysis of attacks on all the service models that there is a need to have stronger security for infrastructure as a service level attack. Trust can be ensured among the cloud users with the introduction of security metrics in the service level agreement. Cloud service providers can mention the security metrics in SLA only if it can confidently address these attacks by adding security for infrastructure as a service.},
  keywords={},
  doi={10.1109/ICCICT50803.2021.9510130},
  ISSN={},
  month={June},}

@INPROCEEDINGS{8509050,
  author={Kumar Koditala, Nikhil and Shekar Pandey, Purnendu},
  booktitle={2018 International Conference on Research in Intelligent and Computing in Engineering (RICE)}, 
  title={Water Quality Monitoring System Using IoT and Machine Learning}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={World Economic Forum ranked drinking water crisis as one of the global risk, due to which around 200 children are dying per day. Drinking unsafe water alone causes around 3.4 million deaths per year. Despite the advancements in technology, sufficient quality measures are not present to measure the quality of drinking water. By focusing on the above issue, this paper proposes a low cost water quality monitoring system using emerging technologies such as IoT, Machine Learning and Cloud Computing which can replace traditional way of quality monitoring. This helps in saving people of rural areas from various dangerous diseases such as fluorosis, bone deformities etc. The proposed model also has a capacity to control temperature of water and adjusts it so as to suit environment temperature. Based on our model we have achieved R-squared score of 0.933.},
  keywords={},
  doi={10.1109/RICE.2018.8509050},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7207386,
  author={Cedillo, Priscila and Jimenez-Gomez, Javier and Abrahao, Silvia and Insfran, Emilio},
  booktitle={2015 IEEE International Conference on Services Computing}, 
  title={Towards a Monitoring Middleware for Cloud Services}, 
  year={2015},
  volume={},
  number={},
  pages={451-458},
  abstract={Cloud Computing represents a new trend in the development and use of software. Many organizations are currently adopting the use of services that are hosted in the cloud by employing the Software as a Service (SaaS) model. Services are typically accompanied by a Service Level Agreement (SLA), which defines the quality terms that a provider offers to its customers. Many monitoring tools have been proposed to report compliance with the SLA. However, they have some limitations when changes to monitoring requirements must be made and because of the complexity involved in capturing low-level raw data from services at runtime. In this paper, we propose the design of a platform-independent monitoring middleware for cloud services, which supports the monitoring of SLA compliance and provides a report containing SLA violations that may help stakeholders to make decisions regarding how to improve the quality of cloud services. Moreover, our middleware definition is based on the use of models@run.time, which allows the dynamic change of quality requirements and/or the dynamic selection of different metric operationalizations (i.e., Calculation formulas) with which to measure the quality of services. In order to demonstrate the feasibility of our approach, we show the instantiation of the proposed middleware that can be used to monitor services when deployed on the Microsoft Azure© platform.},
  keywords={},
  doi={10.1109/SCC.2015.68},
  ISSN={},
  month={June},}

@INPROCEEDINGS{7436036,
  author={Gholami, Atoosa and Arani, Mostafa Ghobaei},
  booktitle={2015 2nd International Conference on Knowledge-Based Engineering and Innovation (KBEI)}, 
  title={A trust model for resource selection in cloud computing environment}, 
  year={2015},
  volume={},
  number={},
  pages={144-151},
  abstract={In recent years, cloud computing technology has been increasingly embraced by people and most organizations tend to use this technology in their business processes. On the other hand, the use of this technology is not so easy and many organizations are concerned about the storage of their sensitive data in their data centers instead of storing them in the cloud storage centers. Today, one of the most important factors for the success of cloud computing is to create trust and security. Cloud computing will face a lot of challenges when the key element trust is absent. Trust is one of the most important ways to improve the reliability of cloud computing resources provided in the cloud environment and plays an important role in business carried out in the cloud business environments. User trust contributes to selection of appropriate sources in heterogeneous cloud infrastructure. In this paper, we present the trust model based on standards of appropriate service quality and speed of implementation for choose the best source. The proposed approach, in addition to taking into account criteria of quality of service such as cost, response time, bandwidth, and processor speed. Simulation results show that the proposed approach compared with similar approaches, in addition to taking into account measures of the quality of service, selects the most reliable source in a cloud environment by taking into account the speed of things.},
  keywords={},
  doi={10.1109/KBEI.2015.7436036},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8251862,
  author={Tirta, Manggiardi B.W. and Shidik, Guruh Fajar},
  booktitle={2017 International Seminar on Application for Technology of Information and Communication (iSemantic)}, 
  title={Evaluation performance of cloud computing with network attached storage for video render}, 
  year={2017},
  volume={},
  number={},
  pages={157-163},
  abstract={One of the benefits of Cloud Computing is the use of virtual machines for efficiency and resource utilization. The study utilizes a virtual machine on cloud computing technology for video rendering needs and is integrated with Network Attached Storage (NAS) storage methods, a centralized storage method that uses network media to connect storage media with users. The rendering process is then analyzed using several metering tools to measure the rendering time frame, VM Utilization, network performance, and NAS Network Performance. The results show that rendering takes longer, then CPU Utilization shows a maximum of 77%, Memory Utilization 55%, and Network Utilization 10%. The bandwidth available between NAS and VM storage in a cloud computing system only generates a maximum of 295.1 Mbps, which should reach 1 Gbps. The quality of video rendering in VM cloud computing shows similar results with rendering on physical computers, the results obtained from testing between frames using mean-square error algorithm.},
  keywords={},
  doi={10.1109/ISEMANTIC.2017.8251862},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8005362,
  author={Brataas, Grunnar and Herbst, Nikolas and Ivansek, Simon and Polutnik, Jure},
  booktitle={2017 IEEE International Conference on Autonomic Computing (ICAC)}, 
  title={Scalability Analysis of Cloud Software Services}, 
  year={2017},
  volume={},
  number={},
  pages={285-292},
  abstract={Cloud computing theoretically offers its customers unlimited cloud resources. However, the scalability of software services is often limited by their underlying architecture. In contrast to current scalability analysis approaches, we make work parameters, quality thresholds, as well as the resource space explicit in a conceptually consistent set of equations. We propose two scalability metric functions based on these equations. The resource scalability metric function describes the relation between the capacity of the multi-tier cloud software service and its use of cloud resources, whereas the cost scalability metric function replaces cloud resources with cost. We validate using the Cloud-Store application. CloudStore follows the TPC-W specification, representing an online book store. We have experimented with 21 different public Amazon Web Service configurations and two private OpenStack configurations.},
  keywords={},
  doi={10.1109/ICAC.2017.34},
  ISSN={2474-0756},
  month={July},}

@INPROCEEDINGS{7404740,
  author={Vallone, Joël and Birke, Robert and Chen, Lydia Y. and Falsafi, Babak},
  booktitle={2015 IEEE 23rd International Symposium on Quality of Service (IWQoS)}, 
  title={Contention detection by throttling: A black-box on-line approach}, 
  year={2015},
  volume={},
  number={},
  pages={237-242},
  abstract={Visualization technology powers up the cloud computing paradigm and inevitably raises concerns about performance isolation of collocated virtual machines (VM). It is imperative for public cloud providers to guarantee performance targets for tenants' VMs while respecting strict business confidentiality, e.g., having no information on applications nor their performance. A large body of related work addresses the challenges of detecting performance interferences by leveraging client's quality of service (QoS) metrics, such as latency, and additional profiling servers. Whereas to assist cloud providers, we resort to an on-line blackbox approach based on throttling that detects a wide range of resource contentions with no cooperation need from the virtual machines. We focus on different resource metrics and actively monitor them from the hypervisor in fine time granularity at low cost. To detect resource contention, we propose a three-phase algorithm: an alarm phase, to identify statistical outliers in the victim's VM resource metrics; a passive diagnosis phase, to match the current sample to historical behaviors; and, an active learning phase, to discern contentions from application phase changes via throttling. We evaluate our algorithm on a prototype running Wikimedia as victim application across a set of VMs collocated with neighboring VMs running resource hoggers, i.e. PARSEC and Cachebench. Our extensive experimental results show that we can reach an average detection accuracy above 90% while limiting the performance degradation experienced by offender workloads to short learning phases.},
  keywords={},
  doi={10.1109/IWQoS.2015.7404740},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9080012,
  author={Kotteswari, K. and Bharathi, A.},
  booktitle={2019 International Conference on Advances in Computing and Communication Engineering (ICACCE)}, 
  title={Spectral Expansion Method for Cloud Reliability Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={Cloud Computing is a computing hypothesis, where a huge group of systems linked together in private, public or hybrid network, to offer dynamically amendable infrastructure for data storage, file storage and application. With this emerging technology, application hosting, delivery, content storage, and reduced computation cost, and it acts as an essential module for backbone of the Internet of Things (IOT). The efficiency of cloud Service providers (CSP) could be improved by considering significant factors such as availability, reliability, usability, security, responsiveness, and elasticity. Assessment of these factors leads to efficiency in designing scheduler for CSP. This metrics also improved the Quality of Service (QoS) in cloud. Many existing model and approaches evaluate this metrics. But these existing approaches doesn't offer efficient outcome. In this paper, a prominent performance model named as Spectral Expansion Method (SPM) evaluates cloud reliability. Spectral expansion Method (SPM) is a huge technique useful in reliability and performance modelling of computing system. This approach solves the Markov model of Cloud service Provider (CSP) to predict the reliability. The SPM is better compared to matrix geometric methods.},
  keywords={},
  doi={10.1109/ICACCE46606.2019.9080012},
  ISSN={},
  month={April},}

@INPROCEEDINGS{7016607,
  author={Abdeladim, Alfath and Baina, Salah and Baina, Karim},
  booktitle={2014 Third IEEE International Colloquium in Information Science and Technology (CIST)}, 
  title={Elasticity and scalability centric quality model for the cloud}, 
  year={2014},
  volume={},
  number={},
  pages={135-140},
  abstract={Cloud computing seems to be the most logical shift in terms of Information Technology after Internet, Social Networking. Despite the potential benefits that cloud computing offers, the model brings new issues, challenges, and needs in term of SLA formalization, Quality of Service (QoS) evaluation due to the heterogeneous resources and to the special features it implies, such as Elasticity and Scalability. In the scope of this paper we focus on the Elasticity and Scalability attributes to assess their impact on the QoS. The paper provides a multi-lenses overview that can help both cloud consumers and potential business application's owners to understand, analyze, and evaluate important aspects related to Scalability and Elasticity capabilities. We determine and analyze the key features of these characteristics and derive metrics that evaluate the cloud elasticity-centric capabilities. We present a specific quality model for those two characteristics derived from their sub-attributes.},
  keywords={},
  doi={10.1109/CIST.2014.7016607},
  ISSN={2327-1884},
  month={Oct},}

@INPROCEEDINGS{7182659,
  author={Young-Rok Shin and Eui-Nam Huh},
  booktitle={2015 Seventh International Conference on Ubiquitous and Future Networks}, 
  title={QoE metrics aggregation for hierarchical Service Level Agreement in Cross-Layered SLA architecture}, 
  year={2015},
  volume={},
  number={},
  pages={831-836},
  abstract={Numerous services are developed using cloud computing technology. It is possible to use service from remote location, not in place of local computer. Accordingly, the research groups predict that the scale of cloud service also will be grown. One of cloud computing's advantage is scalability. It can extend its service scale and range using collaboration between cloud service providers. To make it possible, however, interoperability is required in that environment. Cross-Layered SLA architecture is the cloud service environment that supports interoperability. In this paper, we propose aggregation functions and quality model for QoE metrics and newly generating Service Level Agreement in cross-layered SLA architecture. We expect that this aggregation function and quality model will solve the possible problems in the cloud service area.},
  keywords={},
  doi={10.1109/ICUFN.2015.7182659},
  ISSN={2165-8536},
  month={July},}

@INPROCEEDINGS{7474163,
  author={Bousselmi, Khadija and Brahmi, Zaki and Gammoudi, Mohamed Mohsen},
  booktitle={2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA)}, 
  title={QoS-Aware Scheduling of Workflows in Cloud Computing Environments}, 
  year={2016},
  volume={},
  number={},
  pages={737-745},
  abstract={Cloud Computing has emerged as a service model that enables on-demand network access to a large number of available virtualized resources and applications with a minimal management effort and a minor price. The spread of Cloud Computing technologies allowed dealing with complex applications such as Scientific Workflows, which consists of a set of intensive computational and data manipulation operations. Cloud Computing helps such Workflows to dynamically provision compute and storage resources necessary for the execution of its tasks thanks to the elasticity asset of these resources. However, the dynamic nature of the Cloud incurs new challenges, as some allocated resources may be overloaded or out of access during the execution of the Workflow. Moreover, for data intensive tasks, the allocation strategy should consider the data placement constraints since data transmission time can increase notably in this case which implicates the increase of the overall completion time and cost of the Workflow. Likewise, for intensive computational tasks, the allocation strategy should consider the type of the allocated virtual machines, more specifically its CPU, memory and network capacities. Yet, a critical challenge is how to efficiently schedule the Workflow tasks on Cloud resources to optimize its overall quality of service. In this paper, we propose a QoS-aware algorithm for Scientific Workflows scheduling that aims to improve the overall quality of service (QoS) by considering the metrics of execution time, data transmission time, cost, resources availability and data placement constraints. We extended the Parallel Cat Swarm Optimization (PCSO) algorithm to implement our proposed approach. We tested our algorithm within two sample Workflows of different scales and we compared the results to those given by the standard PSO, the CSO and the PCSO algorithms. The results show that our proposed algorithm improves the overall quality of service of the tested Workflows.},
  keywords={},
  doi={10.1109/AINA.2016.72},
  ISSN={1550-445X},
  month={March},}

@ARTICLE{8204552,
  author={Sun, Chang-ai and Pan, Lin and Wang, Qiaoling and Liu, Huai and Zhang, Xiangyu},
  journal={The Computer Journal}, 
  title={An Empirical Study on Mutation Testing of WS-BPEL Programs}, 
  year={2017},
  volume={60},
  number={1},
  pages={143-158},
  abstract={Nowadays, applications are increasingly deployed as Web services in the globally distributed cloud computing environment. Multiple services are normally composed to fulfill complex functionalities. Business Process Execution Language for Web Services (WS-BPEL) is an XML-based service composition language that is used to define a complex business process by orchestrating multiple services. Compared with traditional applications, WS-BPEL programs pose many new challenges to the quality assurance, especially testing, of service compositions. A number of techniques have been proposed for testing WS-BPEL programs, but only a few studies have been conducted to systematically evaluate the effectiveness of these techniques. Mutation testing has been widely acknowledged as not only a testing method in its own right but also a popular technique for measuring the fault-detection effectiveness of other testing methods. Several previous studies have proposed a family of mutation operators for generating mutants by seeding various faults into WS-BPEL programs. In this study, we conduct a series of empirical studies to evaluate the applicability and effectiveness of various mutation operators for WS-BPEL programs. The experimental results provide insightful and comprehensive guidance for mutation testing of WS-BPEL programs in practice. In particular, our work is the systematic study in the selection of effective mutation operators specifically for WS-BPEL programs.},
  keywords={},
  doi={10.1093/comjnl/bxw076},
  ISSN={1460-2067},
  month={Jan},}

@INPROCEEDINGS{8350688,
  author={Indrawati and Puspita, Fitri Maya and Erlita, Sri and Nadeak, Inosensius and Arisha, Bella},
  booktitle={2018 International Conference on Information and Communications Technology (ICOIACT)}, 
  title={LINGO-based optimization problem of cloud computing of bandwidth consumption in the Internet}, 
  year={2018},
  volume={},
  number={},
  pages={436-441},
  abstract={Optimization problem is an important issue in the network Internet. With the dynamic approach in modeling networks, we can strengthen network performance and ensure that the cost will be minimized and profit of provider can be maximized. This research aims to study, analyze the scheme for cloud networking and formulate a plan of new models of dynamic networks and can work under a cloud of wireless networks. Mixed Integer Non Linear Programming (MINLP) is an integer linear programming model to optimize a particular purpose. In MINLP process, the objective function is determined beforehand. The optimal solution of MINLP lies in the majority of decision variables that can be an integer, Boolean or fractions. Model Cloud computing is one of the areas that is most discussed and promising in modern computer science. Cloud computing is a computing model in which resources such as processors, storage, network and software information that can be accessed by customers via the Internet. In the cloud computing implementation, we require a good traffic for performance and reliability of the system is maintained. QoS (Quality of Services) refers to the distribution of bandwidth. QoS is used as a measure of whether or not the characteristics of the network to meet the needs of different services that use the same infrastructure. Tests carried out on the quality of service parameters, namely, delay, packet loss, throughput and bandwidth. To formulate and solve optimization problems used LINGO software applications. The results show that by designing the optimization problem, the cost of consumption of the demand of the internet can be reduced; the maximum profit for the provider can be increased.},
  keywords={},
  doi={10.1109/ICOIACT.2018.8350688},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9732606,
  author={Le, Van Thanh and El Ioini, Nabil and Pahl, Claus and Barzegar, Hamid R. and Ardagna, Claudio},
  booktitle={2021 Sixth International Conference on Fog and Mobile Edge Computing (FMEC)}, 
  title={A Distributed Trust Layer for Edge Infrastructure}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Recently, Mobile Edge Cloud computing (MEC) has attracted attention both from academia and industry. The idea of moving a part of cloud resources closer to users and data sources can bring many advantages in terms of speed, data traffic, security and context-aware services. The MEC infrastructure does not only host and serves applications next to the end-users, but services can be dynamically migrated and reallocated as mobile users move in order to guarantee latency and performance constraints. This specific requirement calls for the involvement and collaboration of multiple MEC providers, which raises a major issue related to trustworthiness. Two main challenges need to be addressed: i) trustworthiness needs to be handled in a manner that does not affect latency or performance, ii) trustworthiness is considered in different dimensions - not only security metrics but also performance and quality metrics in general. In this paper, we propose a trust layer for public MEC infrastructure that handles establishing and updating trust relations among all MEC entities, making the interaction withing a MEC network transparent. First, we define trust attributes affecting the trusted quality of the entire infrastructure and then a methodology with a computation model that combines these trust attribute values. Our experiments showed that the trust model allows us to reduce latency by removing the burden from a single MEC node, while at the same time increase the network trustworthiness.},
  keywords={},
  doi={10.1109/FMEC54266.2021.9732606},
  ISSN={},
  month={Dec},}

@ARTICLE{6595652,
  author={Xia, Yunni and Zhou, MengChu and Luo, Xin and Zhu, Qingsheng and Li, Jia and Huang, Yu},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Stochastic Modeling and Quality Evaluation of Infrastructure-as-a-Service Clouds}, 
  year={2015},
  volume={12},
  number={1},
  pages={162-170},
  abstract={Cloud computing is a recently developed new technology for complex systems with massive service sharing, which is different from the resource sharing of the grid computing systems. In a cloud environment, service requests from users go through numerous provider-specific steps from the instant it is submitted to when the requested service is fully delivered. Quality modeling and analysis of clouds are not easy tasks because of the complexity of the automated provisioning mechanism and dynamically changing cloud environment. This work proposes an analytical model-based approach for quality evaluation of Infrastructure-as-a-Service cloud by considering expected request completion time, rejection probability, and system overhead rate as key quality metrics. It also features with the modeling of different warm-up and cool-down strategies of machines and the ability to identify the optimal balance between system overhead and performance. To validate the correctness of the proposed model, we obtain simulative quality-of-service (QoS) data and conduct a confidence interval analysis. The result can be used to help design and optimize industrial cloud computing systems.},
  keywords={},
  doi={10.1109/TASE.2013.2276477},
  ISSN={1558-3783},
  month={Jan},}

@INPROCEEDINGS{8394855,
  author={Bouzidi, Mohammed Ridha and Soltani, Abdelghani and Bouhank, Asma and Daoudi, Mourad},
  booktitle={2018 5th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
  title={New Search Based Methods to Solve Workflow Scheduling Problem in Cloud Computing}, 
  year={2018},
  volume={},
  number={},
  pages={647-652},
  abstract={Scheduling has a big influence on the performance of the cloud computing environment, and still remains of big interest for researchers, in particular when a certain level of quality should be maintained in order to satisfy the customer. We consider the particular scheduling multiobjective optimization problem of allocating workflows to the resources of a cloud computing environment, by managing four QoS metrics: makespan, cost, reliability and the availability. It belongs to a category of NP Hard problems. A particular metaheuristic, BBO is investigated. New workflow scheduling BBO based methods are proposed. Further, two multiobjective pareto based optimization methods MOHEFT and NSGA-II are considered in solving our problem. Tests are performed on well-known benchmarks, showing a good behavior of the different methods.},
  keywords={},
  doi={10.1109/CoDIT.2018.8394855},
  ISSN={2576-3555},
  month={April},}

@INPROCEEDINGS{9627219,
  author={Patel, Jatin and Halabi, Talal},
  booktitle={2021 IEEE 6th International Conference on Smart Cloud (SmartCloud)}, 
  title={Optimizing the Performance of Web Applications in Mobile Cloud Computing}, 
  year={2021},
  volume={},
  number={},
  pages={33-37},
  abstract={Cloud computing adoption is on the rise. Many organizations have decided to shift their workload to the cloud to benefit from the scalability, resilience, and cost reduction characteristics. Mobile Cloud Computing (MCC) is an emerging computing paradigm that also provides many advantages to mobile users. Mobile devices function on wireless internet connectivity, which entails issues of limited bandwidth and network congestion. Hence, the primary focus of Web applications in MCC is on improving performance by quickly fulfilling customer's requests to improve service satisfaction. This paper investigates a new approach to caching data in these applications using Redis, an in-memory data store, to enhance Quality of Service. We highlight the two implementation approaches of fetching the data of an application either directly from the database or from the cache. Our experimental analysis shows that, based on performance metrics such as response time, throughput, latency, and number of hits, the caching approach achieves better performance by speeding up the data retrieval by up to four times. This improvement is of significant importance in mobile devices considering their limitation of network bandwidth and wireless connectivity.},
  keywords={},
  doi={10.1109/SmartCloud52277.2021.00013},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7000073,
  author={Ravanello, Anderson and Desharnais, Jean-Marc and Bautista Villalpando, Luis Eduardo and April, Alain and Gherbi, Abdelouahed},
  booktitle={2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement}, 
  title={Performance Measurement for Cloud Computing Applications Using ISO 25010 Standard Characteristics}, 
  year={2014},
  volume={},
  number={},
  pages={41-49},
  abstract={Measuring the performance of cloud computing-based applications using ISO quality characteristics is a complex activity for various reasons, among them the complexity of the typical cloud computing infrastructure on which an application operates. To address this issue, the authors use Bautista's proposed performance measurement framework [1] on log data from an actual data centre to map and statistically analyze one of the ISO quality characteristics: time behavior. This empirical case study was conducted on an industry private cloud. The results of the study demonstrate that it is possible to use the proposed performance measurement framework in a cloud computing context. They also show that the framework holds great promise for expanding the experimentation to other ISO quality characteristics, larger volumes of data, and other statistical techniques that could be used to analyze performance.},
  keywords={},
  doi={10.1109/IWSM.Mensura.2014.33},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9581580,
  author={Zolait, Ali Hussein and Alalas, Sumaya and Ali, Noor and Showaiter, Aya},
  booktitle={2021 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)}, 
  title={Quality of Life Integrated Framework: Perspective of Cloud Computing Usage}, 
  year={2021},
  volume={},
  number={},
  pages={537-544},
  abstract={This research aims to measure the impact of cloud computing on people's quality of life in the Kingdom of Bahrain and recognize factors that could impact people's intention to use cloud computing services. An online survey has been used to collect primary data for the research. It was distributed to a random sample of 443 respondents in the Kingdom of Bahrain. The achievable sample comprised 394 represent people of different ages and educational levels. The researchers adapted selected factors from the diffusion of innovation (DOI) theory, including relative advantage, complexity, and compatibility. In addition to the quality of life factors consisting of education, healthcare, wellbeing, and entertainment. These factors are used to establishing the framework of this research. The research limitation was in examining only the variables proposed in the framework. Also, as a consequence of the coronavirus's current situation (COVID-19), collecting data was restricted to the quantitative approach using an online survey. Findings show that administrability of cloud computing usage is the most impacting factor on people's quality of life and, more specifically, on people's education.},
  keywords={},
  doi={10.1109/3ICT53449.2021.9581580},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9582214,
  author={Aggarwal, Pooja and Nagar, Seema and Gupta, Ajay and Shwartz, Larisa and Mohapatra, Prateeti and Wang, Qing and Paradkar, Amit and Mandal, Atri},
  booktitle={2021 IEEE 14th International Conference on Cloud Computing (CLOUD)}, 
  title={Causal Modeling based Fault Localization in Cloud Systems using Golden Signals}, 
  year={2021},
  volume={},
  number={},
  pages={124-135},
  abstract={In cloud-native applications, a large fraction of operational failures, known as outages, result in violations of Service Level Objectives (SLOs). SLOs are defined around specific measurable characteristics: availability, throughput, frequency, response time, and quality. Four metrics, latency, traffic, errors, and saturation, ensure coverage for most outages of an application. These are often called golden signals. The dynamicity and complexity of cloud-native applications complicate Site Reliability Engineers’ (SREs) efforts in problem determination, in particular in its fault localization. The fault localization is often a try-and-error process in which SREs rely on their domain knowledge and experience. It is laborious and frequently results in long Mean Time To Resolution (MTTR) for outages. This paper describes a lightweight fault localization system, that establishes causal relationships among the golden signal service errors and error logs, and further leverages PageRank centrality of the derived causal graph for generating a ranked list of faulty microservices.},
  keywords={},
  doi={10.1109/CLOUD53861.2021.00026},
  ISSN={2159-6190},
  month={Sep.},}

@INPROCEEDINGS{7982313,
  author={Althani, B. and Khaddaj, S. and Makoond, B.},
  booktitle={2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)}, 
  title={A Quality Assured Framework for Cloud Adaptation and Modernization of Enterprise Applications}, 
  year={2016},
  volume={},
  number={},
  pages={634-637},
  abstract={Cloud Computing has emerged as a viable alternative to in-house computing resources for many organisations. It offers an alternative solution for many enterprise applications, particularly large-scale legacy applications. In addition, it can offer a cost effective strategy for small and medium-sized enterprises (SMEs) where the high set-up and maintenance cost of computing resources can be prohibiting. Thus, in this paper a System Migration Life Cycle (SMLC) framework is proposed, which includes a step by-stepmigration strategy that is descriptive at the business analyst level and based on quality metrics modelling at the technical level, to estimate the potential computational needs, risks, and costs for an organisation. The proposed framework is generic and adaptable in order to accommodate various organisational requirements, thus covering a wide range of enterprise applications and following a number of novel software requirements and quality engineering principles.},
  keywords={},
  doi={10.1109/CSE-EUC-DCABES.2016.251},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8821787,
  author={Mahenge, Michael P. J. and Li, Chunlin and Sanga, Camilius A.},
  booktitle={2019 IEEE 4th International Conference on Computer and Communication Systems (ICCCS)}, 
  title={Collaborative Mobile Edge and Cloud Computing: Tasks Unloading for Improving Users’ Quality of Experience in Resource-Intensive Mobile Applications}, 
  year={2019},
  volume={},
  number={},
  pages={322-326},
  abstract={The advancement in resource-intensive and latency-sensitive applications challenge the legacy systems in Mobile Cloud Computing (MCC) in terms of network congestion, bandwidth utilization, performance and Quality of Service (QoS) metrics. Such challenges emanate from first, limited energy sources and resource poverty of mobile devices. Second, multi-hop connection between user devices and the cloud. To address such challenges, mobile edge computing is a promising solution. This study proposes an architecture that considers unloading resource-intensive tasks from clients' devices to more resourceful edge servers which exploit cooperative approach for tasks processing. Thus, it is essential for minimizing delay, bandwidth usage, congestion to the core network and guarantees cost-effective approach for meeting user's demands. The simulation results show that the proposed approach through unloading, it reduces response time and energy usage. This in turn improves performance, system utility and Quality of Experience (QoE).},
  keywords={},
  doi={10.1109/CCOMS.2019.8821787},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{6897195,
  author={Baliyan, Niyati and Kumar, Sandeep},
  booktitle={2014 Seventh International Conference on Contemporary Computing (IC3)}, 
  title={Towards software engineering paradigm for software as a service}, 
  year={2014},
  volume={},
  number={},
  pages={329-333},
  abstract={The Software as a Service model of Cloud Computing offers economies of scale through the pay per use model; however, it renders the modern software very different from traditional software. Hence, there is a need to adapt Software Engineering approach in a manner that will make the development process and delivery of Software as a Service more efficient and of high quality. After performing literature review, a classification of ongoing research in this direction of adaptation is presented. Various research gaps in the areas of software development process, software reengineering, measurement, metrics, and quality models targeted at Software as a Service are identified, which can be a first step towards the definition of standards and guidelines for Software as a Service development.},
  keywords={},
  doi={10.1109/IC3.2014.6897195},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7170460,
  author={Armando Cabrera, S. and Abad, E. Marco and Danilo Jaramillo, H. and Poma, G. Ana and Verdúm, José Carrillo},
  booktitle={2015 10th Iberian Conference on Information Systems and Technologies (CISTI)}, 
  title={Incidence of software quality attributes in the design, construction and deployment of Cloud architectural environments}, 
  year={2015},
  volume={},
  number={},
  pages={1-7},
  abstract={Cloud Computing (CC) is a new paradigm in the world of computing, it includes several service and deployment models. This project is part of a general study that starts from the software engineering process and the software development life cycle (SDLC), this study is based on the ISO / IEC / IEEE 12207 standard, to evaluate the software implementation process and then we review some software quality aspects by applying the standard ISO / IEC 9126; finally we review some key terms, features, models, architecture, taxonomy, deployment scenarios and scope statements of Cloud Computing. After that we proceed to identify the key characteristics of private and public SaaS environments and obtained a quality model of service (QoS) using quality attributes and their corresponding metrics derived from the ISO / IEC 9126 Standard.},
  keywords={},
  doi={10.1109/CISTI.2015.7170460},
  ISSN={2166-0727},
  month={June},}

@ARTICLE{9097181,
  author={Belgaum, Mohammad Riyaz and Musa, Shahrulniza and Alam, Muhammad Mansoor and Su’ud, Mazliham Mohd},
  journal={IEEE Access}, 
  title={A Systematic Review of Load Balancing Techniques in Software-Defined Networking}, 
  year={2020},
  volume={8},
  number={},
  pages={98612-98636},
  abstract={The traditional networks are facing difficulties in managing the services offered by cloud computing, big data, and the Internet of Things as the users have become more dependent on their services. Software-Defined Networking (SDN) has pulled enthusiasm in the integration process of technologies and function as per the user's requirements for both academia and industry, and it has begun to be embraced in actual framework usage. The emergence of SDN has given another idea to empower the focal programmability of the system. Because of the increasing demand and the scarcity of resources, the load balancing issue needs to be addressed efficiently to manage the incoming traffic and resources and to improve network performance. One of the most critical issues is the role of the controller in SDN to balance the load for having a better Quality of Service (QoS). Though there are few survey articles written on load balancing, there is no detail and systematic review conducted in load balancing in SDN. Hence, this paper extends and reviews the discussion with a taxonomy of current emerging load balancing techniques in SDN systematically by categorizing the techniques as conventional and artificial intelligence-based techniques to improve the service quality. The review also includes the study of metrics and parameters which have been used to measure the performance. This review would allow gaining more information on load balancing approaches in SDN and enables the researchers to fill the current research gaps.},
  keywords={},
  doi={10.1109/ACCESS.2020.2995849},
  ISSN={2169-3536},
  month={},}

@ARTICLE{8737926,
  author={Liu, Ying and Wang, Ke and Ge, Liang and Ye, Lei and Cheng, Jingde},
  journal={IEEE Access}, 
  title={Adaptive Evaluation of Virtual Machine Placement and Migration Scheduling Algorithms Using Stochastic Petri Nets}, 
  year={2019},
  volume={7},
  number={},
  pages={79810-79824},
  abstract={More and more mobile applications rely on the combination of both mobile and cloud computing technology to bring out their full potential. The cloud is usually used for providing additional computing resources that cannot be handled efficiently by the mobile devices. Cloud usage, however, results in several challenges related to the management of virtualized resources. A large number of scheduling algorithms are proposed to balance between performance and cost of data center. Due to huge cost and time consuming of measure-based and simulation method, this paper proposes an adaptive method to evaluate scheduling algorithms. In this method, the virtual machine placement and migration process are modeled by using Stochastic Reward Nets. Different scheduling methods are described as reward functions to perform the adaptive evaluation. Two types of performance metrics are also discussed: one is about quality of service, such as system availability, mean waiting time, and mean service time, and the other is the cost of runtime, such as energy consumption and cost of migration. Compared to a simulation method, the analysis model in this paper only modifies the reward function for different scheduling algorithms and does not need to reconstruct the process. The numeric results suggest that it also has a good accuracy and can quantify the influence of scheduling algorithms on both quality of service and cost of runtime.},
  keywords={},
  doi={10.1109/ACCESS.2019.2923592},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7312289,
  author={Zhang, Xiaodong and Dechen, Zhan and Nie, Lanshun and Zhao, Tianqi and Xiong, Xiao},
  booktitle={2014 International Conference on Service Sciences}, 
  title={An Optimal Service-Selection Model Based on Capability and Quality of Resource Service}, 
  year={2014},
  volume={},
  number={},
  pages={47-52},
  abstract={Most of the researches on optimal service selection are based on the assumption that the capabilities of the services fully meet the requirements. Their limitation is the ignorance of the resources which is the basic factor supporting the implementation of services and it may cause a waste of resources. In cloud computing environment which benefits from its large-scale, there are a large number of resources. Therefore, the waste of resources in it would be a big problem. This paper introduces 'service equivalent' as the basic metric to measure the capabilities of service resources and proposes an optimal service selection model based on capability and quality of service resources and algorithm, in order to solve the issues about the matching capability of service resource and the optimal selection of service resource based on quality. Finally it proves that the model can effectively reduce the waste of resources by the test, which achieves the expected goal.},
  keywords={},
  doi={10.1109/ICSS.2014.39},
  ISSN={2165-3836},
  month={May},}

@INPROCEEDINGS{9412544,
  author={Paolanti, Marina and Mameli, Marco and Frontoni, Emanuele and Gioacchini, Giorgia and Giorgini, Elisabetta and Notarstefano, Valentina and Zacà, Carlotta and Carnevali, Oliana and Borini, Andrea},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Automatic Classification of Human Granulosa Cells in Assisted Reproductive Technology using vibrational spectroscopy imaging}, 
  year={2021},
  volume={},
  number={},
  pages={209-216},
  abstract={In the field of reproductive technology, the biochemical composition of female gametes has been successfully investigated with the use of vibrational spectroscopy. Currently, in assistive reproductive technology (ART), there are no shared criteria for the choice of oocyte, and automatic classification methods for the best quality oocytes have not yet been applied. In this paper, considering the lack of criteria in Assisted Reproductive Technology (ART), we use Machine Learning (ML) techniques to predict oocyte quality for a successful pregnancy. To improve the chances of successful implantation and minimize any complications during the pregnancy, Fourier transform infrared microspectroscopy (FTIRM) analysis has been applied on granulosa cells (GCs) collected along with the oocytes during oocyte aspiration, as it is routinely done in ART, and specific spectral biomarkers were selected by multivariate statistical analysis. A proprietary biological reference dataset (BRD) was successfully collected to predict the best oocyte for a successful pregnancy. Personal health information are stored, maintained and backed up using a cloud computing service. Using a user-friendly interface, the user will evaluate whether or not the selected oocyte will have a positive result. This interface includes a dashboard for retrospective analysis, reporting, real-time processing, and statistical analysis. The experimental results are promising and confirm the efficiency of the method in terms of classification metrics: precision, recall, and F1-score (F1) measures.},
  keywords={},
  doi={10.1109/ICPR48806.2021.9412544},
  ISSN={1051-4651},
  month={Jan},}

@INPROCEEDINGS{8230005,
  author={Kumar, Somansh and Jasuja, Ashish},
  booktitle={2017 International Conference on Computing, Communication and Automation (ICCCA)}, 
  title={Air quality monitoring system based on IoT using Raspberry Pi}, 
  year={2017},
  volume={},
  number={},
  pages={1341-1346},
  abstract={Air pollution is the largest environmental and public health challenge in the world today. Air pollution leads to adverse effects on Human health, climate and ecosystem. Air is getting polluted because of release of Toxic gases by industries, vehicular emissions and increased concentration of harmful gases and particulate matter in the atmosphere. Particulate matter is one of the most important parameter having the significant contribution to the increase in air pollution. This creates a need for measurement and analysis of real-time air quality monitoring so that appropriate decisions can be taken in a timely period. This paper presents a real-time standalone air quality monitoring system which includes various parameters: PM 2.5, carbon monoxide, carbon dioxide, temperature, humidity and air pressure. Internet of Things is nowadays finding profound use in each and every sector, plays a key role in our air quality monitoring system too. Internet of Things converging with cloud computing offers a novel technique for better management of data coming from different sensors, collected and transmitted by low power, low cost ARM based minicomputer Raspberry pi. The system is tested in Delhi and the measurements are compared with the data provided by the local environment control authority and are presented in a tabular form. The values of the parameters measured are shown in IBM Bluemix Cloud.},
  keywords={},
  doi={10.1109/CCAA.2017.8230005},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9080705,
  author={Hussain, Mujahid and Aleem, Sadaf and Karim, Arif and Ghazanfar, Faisal and Hai, Mansoor and Hussain, Kashif},
  booktitle={2020 International Conference on Emerging Trends in Smart Technologies (ICETST)}, 
  title={Design of Low Cost, Energy Efficient, IoT Enabled, Air Quality Monitoring System with Cloud Based Data Logging, Analytics and AI}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a design of real-time Air Quality Monitoring System (AQMS) which incorporates Internet of Things (IoT) and cloud computing. AQMS utilizes solar panel and battery pack for independent and autonomous operation, thus, making it self-powered and sustainable. AQMS is based on AVR Microcontroller (Atmega32) and GSM modem (Sim900) for connectivity with the cloud application. The design is made low cost and scalable so that around 50nos. of such systems can be installed on roundabouts of market places, residential and industrial areas. The AQMS monitors the air quality with the help of a miniature suction pump (5volt DC) which establishes a controlled and constant stream of air-flow through a manifold that encapsulates electromechanical sensors, thus measuring the concentration of O2, CO, CO2, SO / SO2 (SOx), NO/ NO2 (NOx), Hydrocarbon (CxHx), temperature, humidity and noise. By default, the air sampling is carried out once in an hour which may be changed depending on the change in air quality, i.e. making it adoptive for energy conservation and extending the sensor's life. The data collected at the cloud application will be processed using data analytics and Artificial Intelligence (AI) for getting insights of data (data mining) regarding the potential locations where the emissions are critical and disastrous for environmental, thus, leading to prevent any mishap. The design is mapped over a metropolitan city of Pakistan, i.e. Karachi, thus initiating the transformation of Karachi to a smart city.},
  keywords={},
  doi={10.1109/ICETST49965.2020.9080705},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7883186,
  author={Shan Luo and Yanhui Zhou},
  booktitle={2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
  title={How to guarantee the cloud services quality}, 
  year={2016},
  volume={},
  number={},
  pages={791-795},
  abstract={At present, cloud computing is used widely. Cloud services through the cloud platform provide various services to users. And cloud services will play an increasingly important role in the economy and society. Service level agreement (SLA) is an indispensable part of information service in the cloud environment. It is not only the legal protection of the quality of the cloud services, but also provides the service terms and services between the providers and users. Cloud services quality is directly reflected in the user's satisfaction, and it is still a problem to be solved. In this paper, a simple cloud SLA model is proposed to solve the problem of cloud service quality which cannot be measured by the SLA case.},
  keywords={},
  doi={10.1109/ICSESS.2016.7883186},
  ISSN={2327-0594},
  month={Aug},}

@INPROCEEDINGS{7349726,
  author={Shi, Peichang and Gangopadhyay, Aryya},
  booktitle={2015 International Conference on Healthcare Informatics}, 
  title={Personalized Health Plan Ranking - One Application of Cloud Computing to Health Care Data}, 
  year={2015},
  volume={},
  number={},
  pages={447-447},
  abstract={Health plan ranking is one important factor when people are considering their health plan selection. The current health plan ranking is done by National Committee for Quality Assurance, which calculates the ratings based on three types of quality measures and gives an overall ranking of health plans. Individual consumers may be more interested in the ranks of health plans for people with similar conditions. For example, what is the plan ranking for people with both asthma and diabetes? This paper will explore how to combine some data mining techniques and cloud computing to provide a personalized health plan ranking based on each individual's physical conditions.},
  keywords={},
  doi={10.1109/ICHI.2015.65},
  ISSN={},
  month={Oct},}

@ARTICLE{8016558,
  author={Mubeen, Saad and Asadollah, Sara Abbaspour and Papadopoulos, Alessandro Vittorio and Ashjaei, Mohammad and Pei-Breivold, Hongyu and Behnam, Moris},
  journal={IEEE Access}, 
  title={Management of Service Level Agreements for Cloud Services in IoT: A Systematic Mapping Study}, 
  year={2018},
  volume={6},
  number={},
  pages={30184-30207},
  abstract={Cloud computing and Internet of Things (IoT) are computing technologies that provide services to consumers and businesses, allowing organizations to become more agile and flexible. Therefore, ensuring quality of service (QoS) through service-level agreements (SLAs) for such cloud-based services is crucial for both the service providers and service consumers. As SLAs are critical for cloud deployments and wider adoption of cloud services, the management of SLAs in cloud and IoT has thus become an important and essential aspect. This paper investigates the existing research on the management of SLAs in IoT applications that are based on cloud services. For this purpose, a systematic mapping study (a well-defined method) is conducted to identify the published research results that are relevant to SLAs. This paper identifies 328 primary studies and categorizes them into seven main technical classifications: SLA management, SLA definition, SLA modeling, SLA negotiation, SLA monitoring, SLA violation and trustworthiness, and SLA evolution. This paper also summarizes the research types, research contributions, and demographic information in these studies. The evaluation of the results shows that most of the approaches for managing SLAs are applied in academic or controlled experiments with limited industrial settings rather than in real industrial environments. Many studies focus on proposal models and methods to manage SLAs, and there is a lack of focus on the evolution perspective and a lack of adequate tool support to facilitate practitioners in their SLA management activities. Moreover, the scarce number of studies focusing on concrete metrics for qualitative or quantitative assessment of QoS in SLAs urges the need for in-depth research on metrics definition and measurements for SLAs.},
  keywords={},
  doi={10.1109/ACCESS.2017.2744677},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7037641,
  author={Kritikos, Kyriakos and Domaschka, Jörg and Rossini, Alessandro},
  booktitle={2014 IEEE 6th International Conference on Cloud Computing Technology and Science}, 
  title={SRL: A Scalability Rule Language for Multi-cloud Environments}, 
  year={2014},
  volume={},
  number={},
  pages={1-9},
  abstract={The benefits of cloud computing have led to a proliferation of infrastructures and platforms covering the provisioning and deployment requirements of many cloud-based applications. However, the requirements of an application may change during its life cycle. Therefore, its provisioning and deployment should be adapted so that the application can deliver its target quality of service throughout its entire life cycle. Existing solutions typically support only simple adaptation scenarios, whereby scalability rules map conditions on fixed metrics to a single scaling action targeting a single cloud environment (e.g., Scale out an application component). However, these solutions fail to support complex adaptation scenarios, whereby scalability rules could map conditions on custom metrics to multiple scaling actions targeting multi-cloud environments. In this paper, we propose the Scalability Rule Language (SRL), a language for specifying scalability rules that support such complex adaptation scenarios of multi-cloud applications. SRL provides Eclipse-based tool support, thus allowing modellers not only to specify scalability rules but also to syntactically and semantically validate them. Moreover, SRL is well integrated with the Cloud Modelling Language (Cloud ML), thus allowing modellers to associate their scalability rules with the components and virtual machines of provisioning and deployment models.},
  keywords={},
  doi={10.1109/CloudCom.2014.170},
  ISSN={},
  month={Dec},}

@ARTICLE{9121263,
  author={Junaid, Muhammad and Sohail, Adnan and Ahmed, Adeel and Baz, Abdullah and Khan, Imran Ali and Alhakami, Hosam},
  journal={IEEE Access}, 
  title={A Hybrid Model for Load Balancing in Cloud Using File Type Formatting}, 
  year={2020},
  volume={8},
  number={},
  pages={118135-118155},
  abstract={Maintaining accuracy in load balancing using metaheuristics is a difficult task even with the help of recent hybrid approaches. In the existing literature, various optimized metaheuristic approaches are being used to achieve their combined benefits for proper load balancing in the cloud. These approaches often adopt multi-objective QoS metrics, such as reduced SLA violations, reduced makespan, high throughput, low overload, low energy consumption, high optimization, minimum migrations, and higher response time. The cloud applications are generally computation-intensive and can grow exponentially in memory with the increase in size if no proper effective and efficient load balancing technique is adopted resulting in poor quality solutions. To provide a better load balancing solution in cloud computing, with extensive data, a new hybrid model is being proposed that performs classification on the number of files present in the cloud using file type formatting. The classification is performed using Support Vector Machine (SVM) considering various file formats such as audio, video, text maps, and images in the cloud. The resultant data class provides high classification accuracy which is further fed into a metaheuristic algorithm namely Ant Colony Optimization (ACO) using File Type Formatting FTF for better load balancing in the cloud. Frequently used QoS metrics, such as SLA violations, migration time, throughput time, overhead time, and optimization time are evaluated in the cloud environment and comparative analysis is performed with recent metaheuristics, such as Ant Colony Optimization-Particle Swarm Optimization (ACOPS), Chaotic Particle Swarm Optimization (CPSO), Q- learning Modified Particle Swarm Optimization (QMPSO), Cat Swarm Optimization (CSO) and D-ACOELB. The proposed algorithm outperforms them and provides good performance with scalability and robustness.},
  keywords={},
  doi={10.1109/ACCESS.2020.3003825},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7558009,
  author={Meng, Shunmei and Zhou, Zuojian and Huang, Taigui and Li, Duanchao and Wang, Song and Fei, Fan and Wang, Wenping and Dou, Wanchun},
  booktitle={2016 IEEE International Conference on Web Services (ICWS)}, 
  title={A Temporal-Aware Hybrid Collaborative Recommendation Method for Cloud Service}, 
  year={2016},
  volume={},
  number={},
  pages={252-259},
  abstract={With the rapid development of cloud computing, large scale of cloud services are provided to users. Recommender systems have been proven to be valuable tools to deal with information overload and be able to provide appropriate recommendations to users. The cloud environment is dynamic and uncertain, which makes the quality of cloud services time-sensitive. However, most existing recommender systems did not take temporal influence into consideration, therefore could not accommodate the dynamic cloud environment. In view of this challenge, we propose a temporal-aware hybrid collaborative recommendation method for cloud service. It aims at providing users with appropriate recommendations from time-sensitive cloud services. In our method, by distinguishing temporal QoS metrics from stable QoS metrics, temporal influence is integrated into classical neighborhood-based collaborative recommender algorithm. Besides, to get an optimal recommendation, a temporal-aware latent factor model based on tensor decomposition is proposed and combined to improve the recommendation performance. Finally, experiments are designed and conducted to demonstrate the efficiency of our method.},
  keywords={},
  doi={10.1109/ICWS.2016.40},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9500259,
  author={Ramos, Felipe and Viegas, Eduardo and Santin, Altair and Horchulhack, Pedro and dos Santos, Roger R. and Espindola, Allan},
  booktitle={ICC 2021 - IEEE International Conference on Communications}, 
  title={A Machine Learning Model for Detection of Docker-based APP Overbooking on Kubernetes}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Resource allocation overbooking is an approach used by cloud providers that allocates more virtual resources than available on physical hardware, which may imply service quality degradation. Docker in cloud computing environments is being increasingly used due to their fast provisioning and deployment, while the impact of overbooking of resources allocation due to multi-tenancy remains overlooked. This paper proposes a machine learning model to detect overbooking in Kubernetes environments within the docker container. The proposed model continuously monitors distributed container OS usage and application performance metrics. The collected metrics are used as input to a machine learning model that identifies multi-tenancy interference incurring in application performance degradation. Experiments performed on a Kubernetes cluster with a Docker-based Big Data processing application showed that our proposed model could detect resource overbooking with up to 98% accuracy. This implies an overbooking on a resource of up to 1.2 in the client’s domain.},
  keywords={},
  doi={10.1109/ICC42927.2021.9500259},
  ISSN={1938-1883},
  month={June},}

@INPROCEEDINGS{7564810,
  author={Chatterjee, Subarna and Misra, Sudip},
  booktitle={2016 IEEE Wireless Communications and Networking Conference}, 
  title={QoS estimation and selection of CSP in oligopoly environment for Internet of Things}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={This work focuses on an automated selection of Cloud Service Provider (CSP) for a naive end-user in an IoT scenario. In traditional cloud computing model, the end-users are knowledgeable about the Virtual Machines (VMs) and are technically aware of their requirements in terms of the computing cores, processing abilities, and storage requirements. In case of IoT, the users are envisioned to be widespread from naive, unsophisticated people to even objects or things who are devoid of the required knowledge and expertise. Further, in IoT technology, multiple Cloud Service Providers (CSPs) may possess the potential of serving an IoT application. Therefore, it is required for the end-user to judiciously select a single CSP based on the maximum obtainable Quality of Service (QoS) from a CSP. This work proposes an algorithm QoS based Automated Selection of CSP (QASeC) for automated selection of a CSP from a set of nominated CSPs based on the maximum achievable QoS. The work identifies and models the QoS parameters for every CSP and defines a QoS utility metric for each CSP. Based on the metric, the work proposes an optimization for selection of the appropriate CSP and the cloud gateway associated with it. From the obtained results, we infer the suitability of QASeC in real-life IoT scenarios.},
  keywords={},
  doi={10.1109/WCNC.2016.7564810},
  ISSN={1558-2612},
  month={April},}

@INPROCEEDINGS{9310816,
  author={Firdhous, M.F.M. and Budiarto, Rahmat},
  booktitle={2020 5th International Conference on Information Technology Research (ICITR)}, 
  title={BTDM: A QoS-based Trust Distribution Mechanism for Cloud Computing}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Cloud computing makes the delivery of computing resources over the Internet as services. As there are many providers in the market, it is necessary to monitor their performance. Several mechanisms for monitoring service quality of providers have been reported in the literature. But, it is not possible to monitor the entire cloud system by a single monitor. Hence, there is a need for a mechanism to share the performance metrics across a large geographical area. In this paper, the authors propose a mechanism called Bayesian Trust Distribution Mechanism (BTDM) for sharing the performance metrics as trust scores across an extended geographical area. The proposed BTDM also checks the reliability of the received scores based on their previous experience and adjusts them based on the reliability of sender. BTDM was tested using simulations and the results show that it performs better than the other mechanisms reported in the literature.},
  keywords={},
  doi={10.1109/ICITR51448.2020.9310816},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8114483,
  author={Maiyama, Kabiru Muhammad and Kouvatsos, Demetres and Mohammed, Bashir and Kiran, Mariam and Kamala, Mumtaz Ahmed},
  booktitle={2017 IEEE 5th International Conference on Future Internet of Things and Cloud (FiCloud)}, 
  title={Performance Modelling and Analysis of an OpenStack IaaS Cloud Computing Platform}, 
  year={2017},
  volume={},
  number={},
  pages={198-205},
  abstract={Performance is one of the main aspects that should be taken into consideration during the design, development, tuning and optimisation of computer networks supported by cloud computing platforms (CCPs). Queueing network models (QNMs) of CCPs constitute essential quantitative tools of investigation towards identifying acceptable levels of quality-of-service (QoS), whether for upgrading an existing CCP or designing a new one. In this paper, a new stable open QNM with either single or multiple server queueing stations, first-come-first-served (FCFS) scheduling and random routing is proposed for the performance modelling and analysis of an OpenStack Infrastructure as a Service (IaaS) CCP. In this context, it is assumed that the external arrival process is Poisson and the queueing stations provide exponentially distributed service times. Based on Jackson's Theorem, the open QNM is decomposed into individual M/M/c queues with c server(s) (c≥ 1) and exponential inter-arrival and service times, each of which can be analysed in isolation. Consequently, closed form expressions for key performance metrics of the QNM are determined, such as those for the mean response time, throughput, server (resource) utilisation and the probability of the number of requests by clients at each queueing station during waiting for and/or receiving resource provisioning. The evaluation of these metrics identifies the bottlenecks of the CCP that are causing the worst network delays and associated performance degradation and thus, provides insights into the capacity planning of networks with OpenStack IaaS solutions for CSPs.},
  keywords={},
  doi={10.1109/FiCloud.2017.54},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7930199,
  author={Haupt, Florian and Leymann, Frank and Scherer, Anton and Vukojevic-Haupt, Karolina},
  booktitle={2017 IEEE International Conference on Software Architecture (ICSA)}, 
  title={A Framework for the Structural Analysis of REST APIs}, 
  year={2017},
  volume={},
  number={},
  pages={55-58},
  abstract={Today, REST APIs have established as a means for realizing distributed systems and are supposed to gain even more importance in the context of Cloud Computing, Internet of Things, and Microservices. Nevertheless, many existing REST APIs are known to be not well-designed, resulting in the absence of desirable quality attributes that truly RESTful systems entail. Although existing analysis show, that many REST APIs are not fully REST compliant, it is still an open issue how to improve this deficit and where to start. In this work, we introduce a framework for the structural analysis of REST APIs based on their description documents, as this allows for a comprehensive, well-structured analysis approach that also includes analyzing the corresponding API description languages. A first validation builds on a set of 286 real world API descriptions available as Swagger documents, and comprises their transformation into a canonical metamodel for REST APIs as well as a metrics-based analysis and discussion of their structural characteristics with respect to compliance with the REST architectural style.},
  keywords={},
  doi={10.1109/ICSA.2017.40},
  ISSN={},
  month={April},}

@INPROCEEDINGS{7116128,
  author={Al-Jawad, Ahmed and Trestian, Ramona and Shah, Purav and Gemikonakli, Orhan},
  booktitle={Proceedings of the 2015 1st IEEE Conference on Network Softwarization (NetSoft)}, 
  title={BaProbSDN: A probabilistic-based QoS routing mechanism for Software Defined Networks}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  abstract={Over the past decade there has been an exponential increase in the Internet traffic especially with the proliferation of cloud computing and other distributed data services. This explosion of data traffic with its dynamically changing traffic patterns and flows might result in degradation of the network performance. In this context, there is a need for an intelligent and efficient network management system that delivers guaranteed services. To this extent, this paper proposes BaProbSDN, a probabilistic Quality of Service (QoS) routing mechanism for Software Defined Networks (SDN). The QoS routing algorithm employs the bandwidth availability metric as a QoS routing constraint for unicast data delivery. BaProbSDN makes use of Bayes' theorem and Bayesian network model to determine the link probability in order to select the route that satisfies the given bandwidth constraint. The performance of the proposed probabilistic QoS routing algorithm was tested in a simulation-based environment and compared against the widest-shortest path routing (WSR) algorithm. The results demonstrate that BaProbSDN can achieve up to 8.02% decrease in the bandwidth blocking rate when compared to WSR in the presence of link update inaccuracies of threshold and time delay.},
  keywords={},
  doi={10.1109/NETSOFT.2015.7116128},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8079983,
  author={Gabi, Danlami and Ismail, Abdul Samad and Zainal, Anazida and Zakaria, Zalmiyah and Al-Khasawneh, Ahmad},
  booktitle={2017 8th International Conference on Information Technology (ICIT)}, 
  title={Cloud scalable multi-objective task scheduling algorithm for cloud computing using cat swarm optimization and simulated annealing}, 
  year={2017},
  volume={},
  number={},
  pages={1007-1012},
  abstract={In cloud computing, customers-desired Quality of Service (QoS) expectations are quite superficial due to lack of scalable task scheduling solutions that can adjust to long-time changes. Researchers in the literature have put forward several task scheduling algorithms to account for customers' QoS expectations. Unfortunately, most of these algorithms need improvements to ensure the provisioning of better consumers' QoS expectation. In this study, a Multi-Objective QoS model to address customers' expectation based on execution time and execution cost criteria is presented. A Cloud Scalable Multi-Objective Cat Swarm Optimization (CSO) based Simulated Annealing (SA) (CSM-CSOSA) algorithm is then proposed to solve the model. In this method, the Taguchi Orthogonal approach is used to enhanced the SA and incorporated into the local search of the proposed algorithm for enhancing it exploration capability. Implementation of the algorithm is carried out on CloudSim tool and evaluated using one dataset (Normal distributed) and one Parallel Workload (High-Performance Computing Center North(HPC2N)). Quantitative analysis of the algorithm performance is taken based on metrics of execution time, execution cost, QoS and percentage improvement. Result obtained is compared with that of Multi-Objective Genetic Algorithm (MOGA), Multi-Objective Ant Colony (MOSACO) and Multi-Objective Particle Swarm Optimization (MOPSO), where proposed method is able to return substantial performance with improved QoS.},
  keywords={},
  doi={10.1109/ICITECH.2017.8079983},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8646562,
  author={Zhu, Hongbin and Wang, Haifeng and Luo, Xiliang and Qian, Hua},
  booktitle={2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)}, 
  title={AN ONLINE LEARNING APPROACH TO WIRELESS COMPUTATION OFFLOADING}, 
  year={2018},
  volume={},
  number={},
  pages={678-682},
  abstract={Fog computing extends cloud computing and services to the edge of networks, bringing advantages of the cloud closer to where data is created and acted upon. To support real time applications, latency performance is a crucial metric in fog computing. In this paper, we consider a sequential decision-making problem for computation offloading with unknown dynamics in which a mobile user offloads its arrival tasks to associated fog nodes (FNs) at each time slot. The queue of arrival tasks at each FN is modeled as a Markov chain. In order to provide satisfactory quality of experience, the network latency, which is directly associated with the queue condition, needs to be minimized. Taking advantage of reinforcement learning, the sequential decision-making problem is formulated as a restless multi-armed bandit problem. We construct a policy with interleaved exploration and exploitation stages, which achieves a regret with sub-linear order. Both analytical and simulation results validate the effectiveness of the proposed method in dealing with sequential decision-making problem.},
  keywords={},
  doi={10.1109/GlobalSIP.2018.8646562},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8936206,
  author={Lima, Diana Bezerra Correia and da Silva Lima, Rubens Matheus Brasil and de Farias Medeiros, Douglas and Pereira, Renata Imaculada Soares and de Souza, Cleonilson Protasio and Baiocchi, Orlando},
  booktitle={2019 IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)}, 
  title={A Performance Evaluation of Raspberry Pi Zero W Based Gateway Running MQTT Broker for IoT}, 
  year={2019},
  volume={},
  number={},
  pages={0076-0081},
  abstract={The Internet of Things (IoT) has become widely used in recent years in a wide range of applications, such as, weather condition monitoring, transportation, smart homes, smart cities, smart farm, etc. The ecosystem of the IoT is also vast, including from sensor and hardware devices up to cloud-computing. An approach that is getting more and more attention in the IoT ecosystem is the edge-computing and one of its fundamental pieces of equipment is the edge-computing gateway (GTW), which can working as a data-processing device nearer to the things and as a bridge to the Internet, as well. The most important features for these GTWs must be robustness and efficiency and a very popular solution is to use low-cost Raspberry Pi card-size computers. Considering protocol solution, Message Queue Telemetry Transport (MQTT) communication protocol has been considered one of the most applicable to IoT because of its low-power capability. In this context, this paper describes a study about the performance evaluation of a low-power member of the Raspberry Pi family, the Raspberry Pi Zero W, working as an IoT gateway and running MQTT. The experimental results show its performance using as metrics: the processor temperature, the CPU usage level, and rate of MQTT received messages under different Quality of Services (QoS).},
  keywords={},
  doi={10.1109/IEMCON.2019.8936206},
  ISSN={2644-3163},
  month={Oct},}

@INPROCEEDINGS{8641090,
  author={Xu, Jianwen and Ota, Kaoru and Dong, Mianxiong},
  booktitle={2018 IEEE/CIC International Conference on Communications in China (ICCC)}, 
  title={Plug-and-Play for Fog: Dynamic Service Placement in Wireless Multimedia Networks}, 
  year={2018},
  volume={},
  number={},
  pages={490-494},
  abstract={Initially as an extension of cloud computing, fog computing has been inspiring new ideas about moving computing tasks to the edge of a network. In fog, we often repeat the procedure of placing service because of the geographical distribution of mobile users. We may not expect a fixed demand and supply relationship between users and service providers since users always prefer nearby service with less time delay and transmission consumption. That is, a plug-and-play service mode is what we need in fog. In this paper, we put forward a dynamic placement strategy for fog service in a three-tier wireless multimedia network to guarantee the normal service provision and optimize the Quality of Service (QoS). The simulation results show that our strategy can achieve better performance under metrics including energy consumption and end-to-end latency compared with existed methods.},
  keywords={},
  doi={10.1109/ICCChina.2018.8641090},
  ISSN={2377-8644},
  month={Aug},}

@INPROCEEDINGS{6927024,
  author={Mdhaffar, Afef and Halima, Riadh Ben and Jmaiel, Mohamed and Freisleben, Bernd},
  booktitle={2014 IEEE 23rd International WETICE Conference}, 
  title={CEP4Cloud: Complex Event Processing for Self-Healing Clouds}, 
  year={2014},
  volume={},
  number={},
  pages={62-67},
  abstract={This paper presents a cross-layer self-healing approach for Cloud computing environments, based on the Complex Event Processing method. It analyzes monitored events to detect performance-related problems and performs action to fix them without human intervention. Our proposal makes use of novel analysis rules, derived from a comprehensive study of the relationships between monitored metrics across multiple Cloud layers. The results of our study are used to define and optimize the analysis rules and identify the causes of performance-related problems. The results of several experiments demonstrate the benefits of the proposed approach in terms of speeding up the analysis without affecting the quality of the diagnosis.},
  keywords={},
  doi={10.1109/WETICE.2014.56},
  ISSN={1524-4547},
  month={June},}

@INPROCEEDINGS{7173479,
  author={Techio, Leila Regina and Misaghi, Mehran},
  booktitle={Fifth International Conference on the Innovative Computing Technology (INTECH 2015)}, 
  title={EMSCLOUD – an evaluative model of cloud services cloud service management}, 
  year={2015},
  volume={},
  number={},
  pages={100-105},
  abstract={Cloud computing is considered a paradigm both technology and business. Its widespread adoption is an increasingly effective trend. However, the lack of quality metrics and audit of services offered in the cloud slows its use, and it stimulates the increase in focused discussions with the adaptation of existing standards in management services for cloud services offered. This article describes the EMSCloud, that is an Evaluative Model of Cloud Services following interoperability standards, risk management and audit of cloud IT services. Aims to present that is possible to assess the life cycle of services offered in the cloud in the technical dimensions of usability, good practices and economic viability.},
  keywords={},
  doi={10.1109/INTECH.2015.7173479},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8080065,
  author={Gabi, Danlami and Ismail, Abdul Samad and Zainal, Anazida and Zakaria, Zalmiyah and Al-Khasawneh, Ahmad},
  booktitle={2017 8th International Conference on Information Technology (ICIT)}, 
  title={Cloud scalable multi-objective task scheduling algorithm for cloud computing using cat swarm optimization and simulated annealing}, 
  year={2017},
  volume={},
  number={},
  pages={599-604},
  abstract={In cloud computing, customers-desired Quality of Service (QoS) expectations are quite superficial due to lack of scalable task scheduling solutions that can adjust to long-time changes. Researchers in the literature have put forward several task scheduling algorithms to account for customers' QoS expectations. Unfortunately, most of these algorithms need improvements to ensure the provisioning of better consumers' QoS expectation. In this study, a Multi-Objective QoS model to address customers profit based on execution time and execution cost criteria is presented. A Cloud Scalable Multi-Objective Cat Swarm Optimization (CSO) based Simulated Annealing (SA) (CSM-CSOSA) algorithm is then proposed to solve the model. In this method, the Taguchi Orthogonal approach is used to enhanced the SA and incorporated into the local search of the proposed algorithm for enhancing it exploration capability. Implementation of the algorithm is carried out on CloudSim tool and evaluated using one dataset (Normal distributed) and one Parallel Workload (High-Performance Computing Center North(HPC2N)). Quantitative analysis of the algorithm performance is taken based on metrics of execution time, execution cost, QoS and percentage improvement. Result obtained is compared with that of Multi-Objective Genetic Algorithm (MOGA), Multi-Objective Ant Colony (MOSACO) and Multi-Objective Particle Swarm Optimization (MOPSO), where proposed method is able to returned substantial performance with improved QoS.},
  keywords={},
  doi={10.1109/ICITECH.2017.8080065},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8109266,
  author={Gonçalves, Charles Ferreira},
  booktitle={2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
  title={Benchmarking the Security of Virtualization Infrastructures: Motivation and Approach}, 
  year={2017},
  volume={},
  number={},
  pages={100-103},
  abstract={With the growing adoption of cloud computing for business systems, the efforts to keep those environments secure are also increasing. Virtualization infrastructures are key to support such systems, but engineers lack means to help them in selecting the best solutions according to their security requirements. The goal of this work is to define and develop a benchmarking approach to assess and compare the security of virtualization infrastructures. The approach allows the benchmark user to define his usage scenario, which will influence the assessment metrics and quality model. Well established performance benchmarks will be used as workload. The evaluation procedure comprises two key phases: i) security qualification to make sure that detectable/known vulnerabilities are not present; ii) trustworthiness assessment to gather further evidences of the system security. We believe this approach will allow assessing and comparing systems in terms of security, thus helping IaaS providers to select the best infrastructure for their specific needs.},
  keywords={},
  doi={10.1109/ISSREW.2017.70},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9269114,
  author={Papakonstantinou, Ioannis and Kalafatidis, Sarantis and Mamatas, Lefteris},
  booktitle={2020 16th International Conference on Network and Service Management (CNSM)}, 
  title={A Techno-Economic Assessment of Microservices}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={The microservices design paradigm enables applications, usually based on containers, exploiting the flexibility of cloud computing and bringing unique scalability, fault-tolerance and resource-allocation benefits. A number of orchestration facilities, including Kubernetes, target the efficient deployment and operation of containers and are mainly focusing on the maintenance of server resource allocation under predefined thresholds, i.e., through scaling up or down containers to mitigate dynamic changes in the workload. In this work, we highlight the technical capabilities and cost-saving impact of microservices in contrast to traditional monolithic applications, based on a techno-economic analysis. We also investigate the service performance vs resource allocation trade-off, uncovering interesting dynamics when elasticity is driven from service quality metrics. This approach allows the Service Providers (SPs) to balance their profit margins with the customer satisfaction, i.e., reducing the infrastructure cost while keeping the service performance at an acceptable level.},
  keywords={},
  doi={10.23919/CNSM50824.2020.9269114},
  ISSN={2165-963X},
  month={Nov},}

@INPROCEEDINGS{7018544,
  author={Nodehi, Tahereh and Ghimire, Sudeep and Jardim-Gonçalves, Ricardo},
  booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
  title={Toward a unified intercloud interoperability conceptual model for IaaS cloud service}, 
  year={2014},
  volume={},
  number={},
  pages={673-681},
  abstract={The concept of interoperation between cloud providers is a recent research challenging objective. Current cloud systems have been developed without concerns of seamless cloud interconnection, and actually they do not support intercloud interoperability. The paper proposes a conceptual model for Intercloud Interoperability, to enable schedule dynamic operation for Infrastructure as a Service (IaaS) between different clouds. The paper is providing a better understanding of elaborates on the cloud computing architecture, appropriate metrics for Service Level Agreements (SLA) and Quality of Service (QoS) models that are required for seamless integration and interoperability between cloud environments. Then, a conceptual model for the Intercloud Interoperability Framework for Workload Migration is proposed. The novel component of the framework that provides interoperability is the Transformation Engine that maps workload between heterogeneous cloud providers, whilst Model Driven Architecture (MDA) is adopted as an applicable method for developing the Transformation Engine module.},
  keywords={},
  doi={},
  ISSN={},
  month={Jan},}

@ARTICLE{7403967,
  author={Wang, Lujia and Liu, Ming and Meng, Max Q.-H.},
  journal={IEEE Transactions on Cybernetics}, 
  title={A Hierarchical Auction-Based Mechanism for Real-Time Resource Allocation in Cloud Robotic Systems}, 
  year={2017},
  volume={47},
  number={2},
  pages={473-484},
  abstract={Cloud computing enables users to share computing resources on-demand. The cloud computing framework cannot be directly mapped to cloud robotic systems with ad hoc networks since cloud robotic systems have additional constraints such as limited bandwidth and dynamic structure. However, most multirobotic applications with cooperative control adopt this decentralized approach to avoid a single point of failure. Robots need to continuously update intensive data to execute tasks in a coordinated manner, which implies real-time requirements. Thus, a resource allocation strategy is required, especially in such resource-constrained environments. This paper proposes a hierarchical auction-based mechanism, namely link quality matrix (LQM) auction, which is suitable for ad hoc networks by introducing a link quality indicator. The proposed algorithm produces a fast and robust method that is accurate and scalable. It reduces both global communication and unnecessary repeated computation. The proposed method is designed for firm real-time resource retrieval for physical multirobot systems. A joint surveillance scenario empirically validates the proposed mechanism by assessing several practical metrics. The results show that the proposed LQM auction outperforms state-of-the-art algorithms for resource allocation.},
  keywords={},
  doi={10.1109/TCYB.2016.2519525},
  ISSN={2168-2275},
  month={Feb},}

@INPROCEEDINGS{9504780,
  author={Saemi, Behzad and Sadeghilalimi, Mehdi and Rahmani Hosseinabadi, Ali Asghar and Mouhoub, Malek and Sadaoui, Samira},
  booktitle={2021 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={A New Optimization Approach for Task Scheduling Problem Using Water Cycle Algorithm in Mobile Cloud Computing}, 
  year={2021},
  volume={},
  number={},
  pages={530-539},
  abstract={Mobile devices are used by numerous applications that continuously need computing power to grow. Due to limited resources for complex computing, offloading, a service offered for mobile devices, is commonly used in cloud computing. In Mobile Cloud Computing (MCC), offloading decides where to execute the tasks to efficiently maximize the benefits. Hence, we represent offloading as a Task Scheduling Problem (TSP). This latter is a Multi-Objective Optimization (MOO) problem where the goal is to find the best schedule for processing mobile source tasks, while minimizing both the average processor energy consumption and the average task processing time. Owing to the combinatorial nature of the problem, the TSP in MCC is known as NP-hard. To overcome this difficulty in practice, we adopt meta-heuristic search techniques as they offer a good trade-off between solution quality and scalability. More precisely, we introduce a new optimization approach, that we call Multi-objective Discrete Water Cycle Algorithm (MDWCA), to schedule tasks from mobile source nodes to processor resources in a hybrid MCC architecture, including public cloud, cloudlets, and mobile devices. To evaluate the performance of our proposed approach, we conducted several comparative experiments on many generated TSP instances in MCC. The simulation results show that MDWCA outperforms the state-of-the-art optimization algorithms for several quality metrics.},
  keywords={},
  doi={10.1109/CEC45853.2021.9504780},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9171079,
  author={Hans, Manoj and Jagtap, Nilesh and Deokate, Jivan Balasaheb and Jogi, Vivek Kant},
  booktitle={2020 Fourth International Conference on Inventive Systems and Control (ICISC)}, 
  title={Peak Load Management in Smart Grid – Integration of Rescheduling & Cloud Computing}, 
  year={2020},
  volume={},
  number={},
  pages={861-865},
  abstract={Ever-increasing demand for power has motivated researchers to come up with methodologies for meeting the demand. A smart grid is one of the solutions to minimizing the issue. A smart grid has become a proven way to optimize the load flow, hence the balance between the demand and supply of power is maintained. Though the problem of demand and supply is resolved to some extent still problems persist. The efficiency of the system i.e. end to end must be high. The quality of power must be intact. Considering above mentioned factors there can be checkpoints at three different levels. Remedial measures can be at the utility or the consumer end. As much can’t be done at the utility side due to several constraints hence there is a need for implementation of remedial measures on the consumer end, also known as the Demand Side Management (DSM). The demand-side management must be given emphasis because of several advantages it serves to the consumer as well as to the utility. DSM has been implemented at the Institute premises with the application of cloud computing. Communication of data between the cloud and the microgrid at the institute has been monitored and analyzed in the experimentation. Through analysis has been presented in the paper.},
  keywords={},
  doi={10.1109/ICISC47916.2020.9171079},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{9684637,
  author={Udomsripaiboon, Thana and Chaewsuwan, Chutiphan and Chumpoowang, Thanatip and Saetoen, Natthachai and Rojanavasu, Pornthep and Chaewsuwan, Thitirath},
  booktitle={2021 25th International Computer Science and Engineering Conference (ICSEC)}, 
  title={The Atmospheric Ozone Monitoring System by using Internet of Things Technology for Nanosatellites (3U CubeSat)}, 
  year={2021},
  volume={},
  number={},
  pages={325-329},
  abstract={This paper presents the system for monitoring the condition of the earth's ozone layer using Internet of Things (IoT) technology. The proposed system deploys a 3U CubeSat satellite to measure ultraviolet intensity to compare the intensity value of ultraviolet radiation at each terrestrial base station through the web application. The measured data is computed and classified by cloud computing into five levels of the regional ozone layer's quality: excellent, good, average, fair, and poor. These data can also be gathered as statistics and retrieved on a daily, monthly and annual basis. Consequently, by this information, campaigning for people to reduce the destructive behaviours of the ozone layer in each area would be incredibly advantageous. In addition, each base station can be built for less than $150 using commonly available electronics and sensors, allowing the proposed system to be implemented and deployed globally. As a result, this project would be of great benefit to humanity to have information on the health of the ozone layer in each region around the world. This paper could be better inform monitoring strategies to reduce the greenhouse effect that brings unfamiliar and unpredictable impacts of climate change to the world.},
  keywords={},
  doi={10.1109/ICSEC53205.2021.9684637},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7498925,
  author={Karadimce, Aleksandar and Davcev, Danco},
  booktitle={2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX)}, 
  title={Perception of quality in cloud computing based services}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={Cloud computing consists of hardware and software resources, available on the Internet as a set of services for users. This technology aims to provide stable, reliable and encapsulated dynamic information and communication environment for end users to be able to simultaneously access shared resources that are available anywhere and at any time. The major benefit of cloud computing is used to improve the perception of quality for the client requests. Commonly in the communications industry, the term Quality of Experience (QoE) is used as a measure for the user perception of service from the user's point of view. In this research, we propose a classification of cloud-based services based on objective and subjective characteristics for perception of quality. The main contribution in this paper is a novel approach based on Bayesian modeling for efficient assessment of QoE perception for cloud-based services considering the level of interactivity, service complexity, usage domain, and multimedia-intensity.},
  keywords={},
  doi={10.1109/QoMEX.2016.7498925},
  ISSN={},
  month={June},}

@ARTICLE{7501820,
  author={Chen, Yunliang and Wang, Lizhe and Chen, Xiaodao and Ranjan, Rajiv and Zomaya, Albert Y. and Zhou, Yuchen and Hu, Shiyan},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Stochastic Workload Scheduling for Uncoordinated Datacenter Clouds with Multiple QoS Constraints}, 
  year={2020},
  volume={8},
  number={4},
  pages={1284-1295},
  abstract={Cloud computing is now a well-adopted computing paradigm. With unprecedented scalability and flexibility, the computational cloud is able to carry out large scale computing tasks in parallel. The datacenter cloud is a new cloud computing model that uses multi-datacenter architectures for large scale massive data processing or computing. In datacenter cloud computing, the overall efficiency of the cloud depends largely on the workload scheduler, which allocates clients' tasks to different Cloud datacenters. Developing high performance workload scheduling techniques in Cloud computing imposes a great challenge which has been extensively studied. Most previous works aim only at minimizing the completion time of all tasks. However, timeliness is not the only concern, reliability and security are also very important. In this work, a comprehensive Quality of Service (QoS) model is proposed to measure the overall performance of datacenter clouds. An advanced Cross-Entropy based stochastic scheduling (CESS) algorithm is developed to optimize the accumulative QoS and sojourn time of all tasks. Experimental results show that our algorithm improves accumulative QoS and sojourn time by up to 56.1 and 25.4 percent respectively compared to the baseline algorithm. The runtime of our algorithm grows only linearly with the number of Cloud datacenters and tasks. Given the same arrival rate and service rate ratio, our algorithm steadily generates scheduling solutions with satisfactory QoS without sacrificing sojourn time.},
  keywords={},
  doi={10.1109/TCC.2016.2586048},
  ISSN={2168-7161},
  month={Oct},}

@INPROCEEDINGS{9091376,
  author={Zhang, Ziyi and Guo, Caishan and Sun, Yuyan and Hu, Kaiqiang and Wang, Qinghai and Wu, Yuzhao and Cai, Zexiang},
  booktitle={2019 IEEE International Conference on Smart Cloud (SmartCloud)}, 
  title={Cloud Computing Placement Optimization Under Ubiquitous Power Internet of Things Background}, 
  year={2019},
  volume={},
  number={},
  pages={13-18},
  abstract={With the development of power system and the introduction of the Energy Internet, the implementation of Ubiquitous Power Internet of Things (UPIoT) is necessary for power utilities to meet the demands of Integrated Energy Applications. Massive heterogeneous data from various devices surge into power system via UPIoT, which puts heavy burden on data processing capabilities of power system. Cloud computing is an effective measure to provide big data processing capabilities and the establishment of cloud computing for power system is of great significance. Firstly, the architecture of UPIoT and the cloud computing system based on UPIoT background are analyzed. Considering the characteristics of power system, a distributed cloud computing architecture for power system is proposed. A coordinated placement optimization strategy based on minimum cost and satisfaction of quality of service for the proposed architecture is formulated. Based on a given case, the placement optimization simulations are studied. The simulation results prove that the proposed architecture is cost-efficient and the proposed optimization strategy is effective and efficient.},
  keywords={},
  doi={10.1109/SmartCloud.2019.00012},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8422956,
  author={Skourletopoulos, Georgios and Mavromoustakis, Constandinos X. and Mastorakis, George and Batalla, Jordi Mongay and Song, Houbing and Sahalos, John N. and Pallis, Evangelos},
  booktitle={2018 IEEE International Conference on Communications (ICC)}, 
  title={Elasticity Debt Analytics Exploitation for Green Mobile Cloud Computing: An Equilibrium Model}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Mobile cloud computing is being accepted as the model for mobile users to ubiquitously access a shared pool of cloud computing resources, data and services on-demand. In this context, elasticity debt analytics can be harnessed as a measure for efficient scheduling of cloud resources and guarantee of quality of service requirements. This paper proposes a novel green-driven, game theoretic approach to minimizing the elasticity debt on mobile cloud-based service level, investigating the case when a task is offloaded, scheduled and executed on a mobile cloud computing system. The decision to offload a mobile device user's task on cloud affects the level of elasticity debt minimization for the provided services. The research problem is formulated as an elasticity debt quantification game, elaborating on an incentive mechanism to: (a) predict elasticity debt and mitigate the risk of service overutilization, (b) achieve scalability as the number of mobile device user requests for cloud resources increases or decreases accordingly, and (c) optimize cloud resource provisioning, parameterizing the current pool of active users per service. The experimental results prove the effectiveness of the equilibrium model, which allocates the mobile device user requests to high elasticity debt-level services and facilitate elasticity debt minimization for greener mobile cloud computing environments.},
  keywords={},
  doi={10.1109/ICC.2018.8422956},
  ISSN={1938-1883},
  month={May},}

@ARTICLE{8752013,
  author={Yang, Xiao and Pavelsky, Tamlin M. and Allen, George H. and Donchyts, Gennadii},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={RivWidthCloud: An Automated Google Earth Engine Algorithm for River Width Extraction From Remotely Sensed Imagery}, 
  year={2020},
  volume={17},
  number={2},
  pages={217-221},
  abstract={The wetted width of a river is one of the most important hydraulic parameters that can be readily measured using remote sensing. Remotely sensed river widths are used to estimate key attributes of river systems, including changes in their surface area, channel storage, and discharge. Although several published algorithms automate river network and width extraction from remote sensing images, they are limited by only being able to run on local computers and do not automatically manage cloudy images as input. Here we present RivWidthCloud, a river width software package developed on the Google Earth Engine cloud computing platform. RivWidthCloud automatically extracts river centerline and widths from optical satellite images with the ability to flag observations that are obstructed by features like clouds, cloud shadows, and snow based on existing quality band classification. Because RivWidthCloud is built on a popular cloud computing platform, it allows users to easily apply the algorithm to the platform's vast archive of remote sensing images, thereby reducing the users' overhead for computing hardware and data storage. By comparing RivWidthCloud-derived widths from Landsat images to in situ widths from the U.S. and Canada, we show that RivWidthCloud can estimate widths with high accuracy (root mean square error: 99 m; mean absolute error: 43 m; mean bias: -21 m). By making RivWidthCloud publicly available, we anticipate that it will be used to address both river science questions and operational applications of water resource management.},
  keywords={},
  doi={10.1109/LGRS.2019.2920225},
  ISSN={1558-0571},
  month={Feb},}

@INPROCEEDINGS{7328126,
  author={Al-Shammari, Shaymaa and Al-Yasiri, Adil},
  booktitle={2015 IEEE 9th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Environments (MESOCA)}, 
  title={MonSLAR: a middleware for monitoring SLA for RESTFUL services in cloud computing}, 
  year={2015},
  volume={},
  number={},
  pages={46-50},
  abstract={Measuring the quality of cloud computing provision from the client's point of view is important in order to ensure that the service conforms to the level specified in the service level agreement (SLA). With a view to avoid SLA violation, the main parameters should be determined in the agreement and then used to evaluate the fulfillment of the SLA terms at the client's side. Current studies in cloud monitoring only handle monitoring the provider resources with little or no consideration to the client's side. This paper presents MonSLAR, a User-centric middleware for Monitoring SLA for Restful services in SaaS cloud computing environments. MonSLAR uses a distributed architecture that allows SLA parameters and the monitored data to be embedded in the requests and responses of the REST protocol.},
  keywords={},
  doi={10.1109/MESOCA.2015.7328126},
  ISSN={2326-6937},
  month={Oct},}

@INPROCEEDINGS{8279168,
  author={Shibu, Sini and Naik, Archana},
  booktitle={2017 International Conference on Information, Communication, Instrumentation and Control (ICICIC)}, 
  title={An approach to increase the awareness of e-governance initiatives based on cloud computing}, 
  year={2017},
  volume={},
  number={},
  pages={1-4},
  abstract={E-governance is being adopted by the governments of every country for its operations through the ICT (Information and Communication Technology) i.e. incorporating its operations through IT model so that the schemes can be reached to the masses. In India too, as of now, nearly every state government has its own e-Governance model. Cloud computing is now being widely used in e-governance. With the help of the features of Cloud computing, e-Governance operations can be built up as cost effective technology solutions and can be geographically distributed to heterogeneous resources thereby increasing the quality of service to the users. In fact, G-cloud (Governance on Cloud) is designed for using Government services. It is not merely enough to set up e-governance models but its awareness amongst masses is equally important. This paper analyses the cloud based model of e-governance and suggests measures to increase awareness among people regarding the various e-governance initiatives taken by the Government of Madhya Pradesh.},
  keywords={},
  doi={10.1109/ICOMICON.2017.8279168},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7453380,
  author={Singh, Sarbjeet and Sidhu, Jagpreet},
  booktitle={2015 2nd International Conference on Recent Advances in Engineering & Computational Sciences (RAECS)}, 
  title={A collaborative trust calculation scheme for cloud computing systems}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  abstract={One of the major hurdles in the widespread use of cloud computing systems is the lack of trust between consumer and service provider. Lack of trust can put consumer's sensitive data and applications at risk. Consumers need assurance that service providers will provide services as per agreement and will not deviate from agreed terms and conditions. Though trust is a subjective term, it can be measured objectively also. In this paper we present the design and simulation of a collaborative trust calculation scheme in which trust on a service provider is build by participants in a collaborative way. Each collaborator shares its experience of service provider with the coordinator and then shared experiences are aggregated by coordinator to compute final trust value which represents the trustworthiness of service provider. The scheme makes use of fuzzy logic to aggregate responses and to handle uncertain and imprecise information. Collaborative trust calculation scheme makes it difficult for untrustworthy service provider to build its reputation in the system by providing quality services only to a selected set of participants. A service provider has to provide agreed services to all participants uniformly in order to build reputation in the environment. Simulation has been done using MATLAB toolkit. Simulation results show that the scheme is workable and can be adopted for use in collaborative cloud computing systems to determine trustworthiness of service providers.},
  keywords={},
  doi={10.1109/RAECS.2015.7453380},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8666838,
  author={Mengge, YUAN and Rui, LI and Ning, ZHAO},
  booktitle={2018 International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
  title={Optimization of the Number of Servers in Cloud Computing Centers}, 
  year={2018},
  volume={},
  number={},
  pages={269-273},
  abstract={To improve the service quality and save the system cost of the cloud computing center, this paper studies the joint optimization problem of energy consumption and performance of cloud computing centers with a batch Markovian arrival process. The system has multiple parallel processors and the processing time of each processor follows phase type distribution. The system has finite buffer. We construct the system as a BMAP/PH/N/M queueing system and analyze the performance of the system based on queuing theory. An optimization model is established by combing the energy consumption and system performance measures. The optimal number of servers is analyzed. Some managerial insights are given by numerical analysis.},
  keywords={},
  doi={10.1109/ICISCAE.2018.8666838},
  ISSN={},
  month={July},}

@INPROCEEDINGS{9084987,
  author={Chen, Zhijia and Di, Yanqiang and Yuan, Hongli and Feng, Shaochong},
  booktitle={2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)}, 
  title={Intelligent Cloud Training System based on Edge Computing and Cloud Computing}, 
  year={2020},
  volume={1},
  number={},
  pages={1550-1553},
  abstract={Equipment simulation training based on cloud computing is emerging. However, the latency between cloud center and client is long, and the energy consumption management is difficult, which are influencing the development of cloud training. Intelligent cloud training system based on edge computing and cloud computing is introduced in this paper. Intelligent gateway is introduced, through which the task and resources are scheduled and managed together. The popularity of training resources is analyzed. The management of servers in cloud center and edge is intelligently switched between timing sleep and task-activation. Intelligent training service provisioning is achieved through above measures. The simulation results show that the system and management methods are effective on improving training service quality and lower the energy consumption.},
  keywords={},
  doi={10.1109/ITNEC48623.2020.9084987},
  ISSN={},
  month={June},}

@ARTICLE{8852632,
  author={Ala’anzy, Mohammed and Othman, Mohamed},
  journal={IEEE Access}, 
  title={Load Balancing and Server Consolidation in Cloud Computing Environments: A Meta-Study}, 
  year={2019},
  volume={7},
  number={},
  pages={141868-141887},
  abstract={The data-center is considered the heart of cloud computing. Recently, the growing demand for cloud computing services has caused a growing load on data centers. In terms of system behavior and workload, patterns of cloud computing are very dynamic; and that might serve to imbalance the load among data center resources. Eventually, some data-center resources could come to be over-loaded/under-loaded, which leads to an increase in energy consumption in addition to decreased functioning and wastage of resources. Just considering energy-efficiency (that can be attained efficiently by consolidate the servers) may not be enough for real applications because it may cause problems such as unbalanced load for each Physical Machine (PM). Therefore, this paper surveys published load balancing algorithms that achieved by server consolidation via a meta-analysis. Load balancing with server consolidation enriches the exploitation of resource utilization and can enhance Quality of Service (QoS) metrics, since data-centers and their applications are increasing exponentially. This meta-study, reviews the literature on load balancing and server consolidation and presents a ready reference taxonomy on the most efficient algorithms that achieve load balancing and server consolidation. This work attempts to present a taxonomy with a new classification for load balancing and server consolidation, such as migration overhead, hardware threshold, network traffic, and reliability.},
  keywords={},
  doi={10.1109/ACCESS.2019.2944420},
  ISSN={2169-3536},
  month={},}

@ARTICLE{9261243,
  author={Bui, Khiet Thanh and Van Vo, Len and Nguyen, Canh Minh and Pham, Tran Vu and Tran, Hung Cong},
  journal={Journal of Communications and Networks}, 
  title={A fault detection and diagnosis approach for multi-tier application in cloud computing}, 
  year={2020},
  volume={22},
  number={5},
  pages={399-414},
  abstract={Ensuring the availability of cloud computing services always concerns both service providers and end users. Therefore, the system always needs precautions for unexpected cases. Accordingly, cloud computing services must be capable of identifying faults and behaving appropriately when it is abnormal to ensure the smoothness as well as the service quality. In this study, we propose a fault detection method for multi-tier web application in cloud computing deployment environment based on the Fuzzy One-class support vector machine and Exponentially Weighted Moving Average method. And then, the suspicious metrics are located by using feature selection method which based on Random Forest algorithm. To evaluate our approach, a multi-tier application is deployed by a transnational web e-Commerce benchmark by using TPC-W (TPC Benchmark™ W, simulates the activities of a business oriented transaction web server in a controlled internet commerce environment) in private cloud and then it is injected typical faults. The effectiveness of the fault detection and diagnosis are demonstrated in experiment results.},
  keywords={},
  doi={10.1109/JCN.2020.000023},
  ISSN={1976-5541},
  month={Oct},}

@INPROCEEDINGS{8422909,
  author={Kong, Cuiyu and Rimal, Bhaskar Prasad and Bhattarai, Bishnu P. and Devetsikiotis, Michael},
  booktitle={2018 IEEE International Conference on Communications (ICC)}, 
  title={Cloud-Based Charging Management of Electric Vehicles in a Network of Charging Stations}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={A large scale of electric vehicles (EVs) and the operation of smart grid requires the support of a reliable and robust communication infrastructure. Cloud computing has gained popularity in smart grid for reducing computational and communication complexity. Based on cloud computing services, this paper considers the issues of high charging demand in fast charging stations (FCSs) during peak hours and communication among a large-scale of EVs, a network of FCSs, and system operator (SO). More specifically, we propose a novel cloud-based hierarchical charging management model of EVs, whereby two levels of cloud computing infrastructures are considered to meet different latency requirements of customers in highway exits and parking lots. Considering the quality of service (QoS) metrics (average waiting time in the queue, and blocking probability), the model is composed of: server planning in the cloud, capacity planning in FCSs, and profit maximization. Meanwhile, a price incentive mechanism is applied to shift the heavy load from peak hours to off-peak hours. Numerical results demonstrate the effectiveness of the proposed method, which can guarantee QoS and system profit, thereby more customers can satisfy their charging demand.},
  keywords={},
  doi={10.1109/ICC.2018.8422909},
  ISSN={1938-1883},
  month={May},}

@INPROCEEDINGS{8229905,
  author={Baghel, Dinesh Kumar and Singh, Arun and Deka, Pratyush Kumar},
  booktitle={2017 International Conference on Computing, Communication and Automation (ICCCA)}, 
  title={Agricultural management using cloud computing in India}, 
  year={2017},
  volume={},
  number={},
  pages={801-806},
  abstract={Although agriculture now accounts for only 14 percent of Gross Domestic Product (GDP), rapid growth of agriculture in India is critical for inclusiveness. Information Communication and Technology (ICT) provides greater role in offering greater expertise to producers regarding pricing, good quality seed information, fertilizers, disease detail, sharing new discoveries of scientists working at various Agricultural Institutes. An effective implementation of cloud computing in agricultural sector is encouraging and required for overall development of agricultural sector of India. There are potential risks in cloud computing which if properly addressed can be a potent ICT tool in agricultural sector in India. Considering the benefits of cloud computing, a design is proposed for Indian agricultural sector and two performance metrics are discussed which can be used to assess any cloud based application.},
  keywords={},
  doi={10.1109/CCAA.2017.8229905},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8390054,
  author={Gopavanitha, K. and Nagaraju, S.},
  booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)}, 
  title={A low cost system for real time water quality monitoring and controlling using IoT}, 
  year={2017},
  volume={},
  number={},
  pages={3227-3229},
  abstract={Water is a prerequisite element required for humans and therefore there must be mechanisms put in place to vigorously test the quality of drinking water in real time. This paper proposes a low cost system for real time water quality monitoring and controlling using IoT. The system consist of physiochemical sensors which can measures the physical and chemical parameters of the water such as Temperature, Turbidity, Conductivity, pH and Flow. By these sensors, water contaminants are detected. The sensor values processed by Raspberry pi and send to the cloud. Finally the sensed data is visible on the cloud using cloud computing and the flow of the water in the pipeline is controlled through IoT.},
  keywords={},
  doi={10.1109/ICECDS.2017.8390054},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7974607,
  author={Li, Suyou and Guo, Zhigang and Shou, Guochu and Hu, Yihong and Li, Hongxing},
  booktitle={2016 IEEE International Conference on Network Infrastructure and Digital Content (IC-NIDC)}, 
  title={QoE analysis of NFV-based mobile edge computing video application}, 
  year={2016},
  volume={},
  number={},
  pages={411-415},
  abstract={Mobile Edge Computing (MEC) provides mobile and cloud computing capabilities within the access network. Network Functions Virtualization (NFV) leverages standard IT Virtualization technology to decouple the network functions from the underlying physical infrastructure. Basing on the ICT demand, MEC can be consolidated into NFV, as a network element within access network. This paper presents an architecture of NFV-based MEC platform and analyzes its Quality of Service (QoS) compared with the remote servers (Shenzhen and Qingdao). Then, this paper measures the Quality of Experience (QoE) of HTTP videos deployed in the servers. The result shows MEC can offer a service environment with higher bandwidth, which supports 10-fold gains, and ultra-low latency, jitter and packet loss rate. Moreover, along with the higher resolution and bitrates, the range of the video QoE improvement on this platform rises compared with the remote servers. In a word, the NFV-based MEC can achieve better performance than the remote servers.},
  keywords={},
  doi={10.1109/ICNIDC.2016.7974607},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9242798,
  author={Abdulwahid, Ali Hadi},
  booktitle={2020 9th International Conference on Renewable Energy Research and Application (ICRERA)}, 
  title={IoT Based Water Quality Monitoring System for Rural Areas}, 
  year={2020},
  volume={},
  number={},
  pages={279-282},
  abstract={To ensure that safety is guaranteed, it is essential to implement monitoring in real-time for the quality of potable water. This work is about the use of Internet of Things (IoT) technology to develop an affordable system to control water quality in real-time. Several sensors are integrated into the system to measure various chemical and physical water properties, such as conductivity, pH, turbidity, and temperature. The core controller, which can also be the microprocessor, manages the processing of data captured by the sensor. The visualization of data can be accomplished on cloud computing via the Internet.},
  keywords={},
  doi={10.1109/ICRERA49962.2020.9242798},
  ISSN={2572-6013},
  month={Sep.},}

@INPROCEEDINGS{8310143,
  author={Puteaux, Pauline and Puech, William},
  booktitle={2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)}, 
  title={Reversible data hiding in encrypted images based on adaptive local entropy analysis}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={With the development of cloud computing, the growth in information technology has led to serious security issues. For this reason, a lot of multimedia files are stored in encrypted forms. Methods of reversible data hiding in encrypted images (RDHEI) have been designed to provide authentication and integrity in the encrypted domain. The original image is firstly encrypted to ensure confidentiality, by making the content unreadable. A secret message is then embedded in the encrypted image, without the need of the encryption key or any access to the clear content. The challenge lies in finding the best trade-off between embedding capacity and quality of the reconstructed image. In 2008, Puech et al. suggested using the AES algorithm to encrypt an original image and to embed one bit in each block of 16 pixels (payload = 0.0625 bpp) [12]. During the decryption phase, the original image is reconstructed by measuring the standard deviation into each block. In this paper, we propose an improvement to this method, by performing an adaptive local entropy measurement. We can achieve a larger payload without altering the recovered image quality. Our obtained results are very good and better than most of the modern state-of-the-art methods, whilst offering an improved security level with the use of the AES algorithm, defined as the encryption standard by the NIST.},
  keywords={},
  doi={10.1109/IPTA.2017.8310143},
  ISSN={2154-512X},
  month={Nov},}

@INPROCEEDINGS{8364051,
  author={Ramos da Paixão, Ermínio Augusto and Vieira, Rafael Fogarolli and Araújo, Welton Vasconcelos and Cardoso, Diego Lisboa},
  booktitle={2018 Third International Conference on Fog and Mobile Edge Computing (FMEC)}, 
  title={Optimized load balancing by dynamic BBU-RRH mapping in C-RAN architecture}, 
  year={2018},
  volume={},
  number={},
  pages={100-104},
  abstract={Cloud Radio Access Network (C-RAN) is one of the key architectures for the next generation of mobile networks (5G) that aim at centralized processing and management, collaborative radios and cloud computing in real time. These features enable the architectures to make a rational adjustment to the connection between remote radio heads (RRHs) and baseband units (BBUs) dynamically. This is important since if this feature is neglected, it can cause difficulties such as blocked calls and low-quality connections. This study investigates this area and proposes an optimized mapping model for RRH-BBU that seeks a more equitable and effective balancing. The Key Performance Indicator (KPI) of blocked calls was used for this to measure the quality of service (QoS). A particle Swarm algorithm (PSO) was created to minimize the number of blocked calls and additionally balancing the processing load between the BBUs. Scenario from literature was employed that consists of 19 RRHs distributed in a geographic area, which can be mapped in a BBU pool that manages two BBUs with three sectors each. The initial configuration on average, led to 80 blocked calls. The results obtained by the PSO show that there was a reduction of up to 100% of blocked calls, as well as a more equitable load distribution between the BBUs. Additionally, realistic scenarios with different user profiles were also included, since they demonstrate that these factors have a direct impact on the load generated in the BBUs and hence, affect their balance.},
  keywords={},
  doi={10.1109/FMEC.2018.8364051},
  ISSN={},
  month={April},}

@INPROCEEDINGS{9178684,
  author={Wang, Chengrong and Zhang, Xiaodong and Chu, Dianhui},
  booktitle={2020 5th International Conference on Computational Intelligence and Applications (ICCIA)}, 
  title={Research on Service Composition Optimization Method Based on Composite Services QoS}, 
  year={2020},
  volume={},
  number={},
  pages={206-210},
  abstract={With the development of Cloud Computing, Internet of Things, and the advent of the era of Big Data, the types and scale of services are getting larger and larger, and the problem space of service composition is exploding. In order to measure the composite services quality of different combination schemes, this paper shows the calculation method of composite services QoS (Quality of Service), and improves the Ant Colony Algorithm by introducing Skyline calculation to further improve the efficiency of service composition and respond to user quickly. Finally, it is verified on the real QoS data set, and the feasibility and effectiveness of the method are proved through experiments.},
  keywords={},
  doi={10.1109/ICCIA49625.2020.00046},
  ISSN={},
  month={June},}

@ARTICLE{7517217,
  author={Lyu, Xinchen and Tian, Hui and Sengul, Cigdem and Zhang, Ping},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Multiuser Joint Task Offloading and Resource Optimization in Proximate Clouds}, 
  year={2017},
  volume={66},
  number={4},
  pages={3435-3447},
  abstract={Proximate cloud computing enables computationally intensive applications on mobile devices, providing a rich user experience. However, remote resource bottlenecks limit the scalability of offloading, requiring optimization of the offloading decision and resource utilization. To this end, in this paper, we leverage the variability in capabilities of mobile devices and user preferences. Our system utility metric is a measure of quality of experience (QoE) based on task completion time and energy consumption of a mobile device. We propose a heuristic offloading decision algorithm (HODA), which is semidistributed and jointly optimizes the offloading decision, and communication and computation resources to maximize system utility. Our main contribution is to reduce the problem to a submodular maximization problem and prove its NP-hardness by decomposing it into two subproblems: 1) optimization of communication and computation resources solved by quasiconvex and convex optimization and 2) offloading decision solved by submodular set function optimization. HODA reduces the complexity of finding the local optimum to O(K3), where K is the number of mobile users. Simulation results show that HODA performs within 5% of the optimal on average. Compared with other solutions, HODA's performance is significantly superior as the number of users increases.},
  keywords={},
  doi={10.1109/TVT.2016.2593486},
  ISSN={1939-9359},
  month={April},}

@ARTICLE{8082532,
  author={Abdul-Rahman, Omar Arif and Aida, Kento},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Google Users as Sequences: A Robust Hierarchical Cluster Analysis Study}, 
  year={2020},
  volume={8},
  number={1},
  pages={167-179},
  abstract={In this era of cloud computing, users encounter the challenging task of effectively composing and running their applications on the cloud. By understanding user behavior in constructing applications and interacting with typical cloud infrastructures, cloud managers can develop better systems that improve the users' experience. In this paper, we analyze a large dataset of a Google cluster to characterize the users into distinct groups of similar usage behavior. We used a wide range of measured metrics to model user behavior in composing applications from the perspective of actions around application architecting, capacity planning, and workload type planning and to model user interaction behavior around the session view. The trajectories of users' actions are represented as sequences using categorical and proportional encoding schemes. We used techniques from the sequence analysis paradigm to quantify dissimilarity among users. We employed a robust cluster analysis procedure based on the agglomerative hierarchical methods to optimally classify users into 12 classes. We used a variety of formal indices and visual aids to confirm the quality and stability of the outcomes. By visual inspection, we regrouped the obtained clusters into 5 main groups that reveal interesting insights about the characteristics which underline different groups' utilization behavior.},
  keywords={},
  doi={10.1109/TCC.2017.2766227},
  ISSN={2168-7161},
  month={Jan},}

@INPROCEEDINGS{7152489,
  author={Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
  booktitle={2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing}, 
  title={Modeling Cross-Architecture Co-Tenancy Performance Interference}, 
  year={2015},
  volume={},
  number={},
  pages={231-240},
  abstract={Cloud computing has become a dominant computing paradigm to provide elastic, affordable computing resources to end users. Due to the increased computing power of modern machines powered by multi/many-core computing, data centers often co-locate multiple virtual machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing and competition. Applications or VMs co-locating in one physical machine can interfere with each other despite of the promise of performance isolation through virtualization. Modelling and predicting co-run interference therefore becomes critical for data center job scheduling and QoS (Quality of Service) assurance. Co-run interference can be categorized into two metrics, sensitivity and pressure, where the former denotes how an application's performance is affected by its co-run applications, and the latter measures how it impacts the performance of its co-run applications. This paper shows that sensitivity and pressure are both application-and architecture dependent. Further, we propose a regression model that predicts an application's sensitivity and pressure across architectures with high accuracy. This regression model enables a data center scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate with another VMs/applications.},
  keywords={},
  doi={10.1109/CCGrid.2015.152},
  ISSN={},
  month={May},}

@INPROCEEDINGS{7744161,
  author={He, Fei-Long and Chen, Wei-Neng and Hu, Xiao-Min},
  booktitle={2016 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Differential evolution with double-level archives for bi-objective cloud task scheduling}, 
  year={2016},
  volume={},
  number={},
  pages={2942-2949},
  abstract={In cloud computing, scheduling plays a critical role for quality of service (QoS) and provider efficiency which are generally measured by several metrics and make the scheduling a multiobjective problem (MOP). In this paper, we propose a differential evolution algorithm with double-level archives (DE-DLA) for bi-objective cloud task scheduling. The proposed algorithm is based on the newly-developed framework, multiobjective evolutionary algorithm with double-level archives (MOEA-DLA), and uses differential evolution to implement this framework. Global Archive is used to save Pareto-optimal individuals for the whole problem and Sub-archive is used to save several comparatively good individuals for the corresponding sub-problem formed by decomposition. So the algorithm takes advantages of both whole multiobjective problem optimization and decomposition based optimization. Precedence constraint in user's application is considered in the scheduling model of this paper. To minimize cost and makespan simultaneously, the proposed algorithm tries to find optimal resource allocation and optimal order of task executing. In the experiment, compared with two other algorithms, DE-DLA has shown competitive advantages.},
  keywords={},
  doi={10.1109/CEC.2016.7744161},
  ISSN={},
  month={July},}

@ARTICLE{6856153,
  author={Shuja, Junaid and Bilal, Kashif and Madani, Sajjad A. and Othman, Mazliza and Ranjan, Rajiv and Balaji, Pavan and Khan, Samee U.},
  journal={IEEE Systems Journal}, 
  title={Survey of Techniques and Architectures for Designing Energy-Efficient Data Centers}, 
  year={2016},
  volume={10},
  number={2},
  pages={507-519},
  abstract={Cloud computing has emerged as the leading paradigm for information technology businesses. Cloud computing provides a platform to manage and deliver computing services around the world over the Internet. Cloud services have helped businesses utilize computing services on demand with no upfront investments. The cloud computing paradigm has sustained its growth, which has led to increase in size and number of data centers. Data centers with thousands of computing devices are deployed as back end to provide cloud services. Computing devices are deployed redundantly in data centers to ensure 24/7 availability. However, many studies have pointed out that data centers consume large amount of electricity, thus calling for energy-efficiency measures. In this survey, we discuss research issues related to conflicting requirements of maximizing quality of services (QoSs) (availability, reliability, etc.) delivered by the cloud services while minimizing energy consumption of the data center resources. In this paper, we present the concept of inception of data center energy-efficiency controller that can consolidate data center resources with minimal effect on QoS requirements. We discuss software- and hardware-based techniques and architectures for data center resources such as server, memory, and network devices that can be manipulated by the data center controller to achieve energy efficiency.},
  keywords={},
  doi={10.1109/JSYST.2014.2315823},
  ISSN={1937-9234},
  month={June},}

@ARTICLE{8683979,
  author={Gamal, Marwa and Rizk, Rawya and Mahdi, Hani and Elnaghi, Basem E.},
  journal={IEEE Access}, 
  title={Osmotic Bio-Inspired Load Balancing Algorithm in Cloud Computing}, 
  year={2019},
  volume={7},
  number={},
  pages={42735-42744},
  abstract={Cloud computing is increasing rapidly as a successful paradigm presenting on-demand infrastructure, platform, and software services to clients. Load balancing is one of the important issues in cloud computing to distribute the dynamic workload equally among all the nodes to avoid the status that some nodes are overloaded while others are underloaded. Many algorithms have been suggested to perform this task. Recently, worldview is turning into a new paradigm for optimization search by applying the osmosis theory from chemistry science to form osmotic computing. Osmotic computing is aimed to achieve balance in highly distributed environments. The main goal of this paper is to propose a hybrid metaheuristics technique which combines the osmotic behavior with bio-inspired load balancing algorithms. The osmotic behavior enables the automatic deployment of virtual machines (VMs) that are migrated through cloud infrastructures. Since the hybrid artificial bee colony and ant colony optimization proved its efficiency in the dynamic environment in cloud computing, the paper then exploits the advantages of these bio-inspired algorithms to form an osmotic hybrid artificial bee and ant colony (OH_BAC) optimization load balancing algorithm. It overcomes the drawbacks of the existing bio-inspired algorithms in achieving load balancing between physical machines. The simulation results show that OH_BAC decreases energy consumption, the number of VMs migrations and the number of shutdown hosts compared to existing algorithms. In addition, it enhances the quality of services (QoSs) which is measured by service level agreement violation (SLAV) and performance degradation due to migrations (PDMs).},
  keywords={},
  doi={10.1109/ACCESS.2019.2907615},
  ISSN={2169-3536},
  month={},}

@ARTICLE{7066939,
  author={Lillo-Castellano, J. M. and Mora-Jiménez, I. and Santiago-Mozos, R. and Chavarría-Asso, F. and Cano-González, A. and García-Alberola, A. and Rojo-Álvarez, J. L.},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Symmetrical Compression Distance for Arrhythmia Discrimination in Cloud-Based Big-Data Services}, 
  year={2015},
  volume={19},
  number={4},
  pages={1253-1263},
  abstract={The current development of cloud computing is completely changing the paradigm of data knowledge extraction in huge databases. An example of this technology in the cardiac arrhythmia field is the SCOOP platform, a national-level scientific cloud-based big data service for implantable cardioverter defibrillators. In this scenario, we here propose a new methodology for automatic classification of intracardiac electrograms (EGMs) in a cloud computing system, designed for minimal signal preprocessing. A new compression-based similarity measure (CSM) is created for low computational burden, so-called weighted fast compression distance, which provides better performance when compared with other CSMs in the literature. Using simple machine learning techniques, a set of 6848 EGMs extracted from SCOOP platform were classified into seven cardiac arrhythmia classes and one noise class, reaching near to 90% accuracy when previous patient arrhythmia information was available and 63% otherwise, hence overcoming in all cases the classification provided by the majority class. Results show that this methodology can be used as a high-quality service of cloud computing, providing support to physicians for improving the knowledge on patient diagnosis.},
  keywords={},
  doi={10.1109/JBHI.2015.2412175},
  ISSN={2168-2208},
  month={July},}

@INPROCEEDINGS{8046685,
  author={Wang, Paul and Takizawa, Shigeyuki and He, David and Ge, Fred and Wang, Orson and Ye, Fred and Liang, Park and Tan, KG},
  booktitle={2017 18th International Conference on Electronic Packaging Technology (ICEPT)}, 
  title={DDR4 dual-contact interconnect methodology, component, and board level reliability}, 
  year={2017},
  volume={},
  number={},
  pages={1337-1344},
  abstract={This article is a series of study on new generation of electronic contact challenges and component interconnects technology for high-end computing products. These products include server and data storage for cloud computing applications at the data center as well as core routers for service providers, edge and branch routers for enterprise networking companies, and small switch and wireless router for commercial and small and home office. All these cloud computing products require high data speed in terabytes per second and high signal integrity for the massive mobile users and loT application whenever and wherever they connected To achieve such mobility and signal integrity the major focus is the electrical interconnections between the CPUlGPU and component in the system. Due to the large number of edge-card connections such as DIMM, PCle, etc. in modern computer systems and due to their relatively low reliability, in previous Part 2 of the study a test vehicle with daisy chain was used to assess the contact interconnect failure related to factors such as soldering flux residue, plating quality, contact interface cleaning and doubt insertion, and particulate control and management. As concluded in Part 2, the heavy flux residue and vibration preconditioning have medium effect on contact failure, however contact interface cleaning and particulate control show no significant contribution and not able to eliminate the last thousands DPPM of DIMM contact failure. The purpose of current study is to look into a new generation of dual-contact interconnect methodology and assess component level contact configuration and interconnect reliability. First, the contact pin configuration and plating morphology such as homogeneity and thickness are carefully examined to ensure contact integrity between DDR4 connect and DIMM module can be achieved. Then normal force of dual-pin and individual first and second contact were measured to benchmark to existing conventional single-contacts. Furthermore the JEDEC Raptor test vehicle was adapted to assess characteristic impedance and four signal integrity tests, RL (return loss), IL (insertion loss), NEXT (near-end cross talk), and FEXT (far-end cross talk) to ensure signal integrity requirements are fulfilled. Finally, board level reliability test is proposed for Raptor test board and trial run on real product. The overall goal of Part 3 of the study is to ensure a smooth migration from conventional single-contact to a new interconnect mechanism with robust board and system reliability for high signal integrity requirement in cloud computing and loT application.},
  keywords={},
  doi={10.1109/ICEPT.2017.8046685},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9667831,
  author={Ishak, Md and Rahman, Raiyan and Mahmud, Tahasin},
  booktitle={2021 5th International Conference on Electrical Engineering and Information Communication Technology (ICEEICT)}, 
  title={Integrating Cloud Computing in E-healthcare: System Design, Implementation and Significance in Context of Developing Countries}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Cloud computing in the medical sector refers to the method of storing, maintaining, and processing electronic health records and relevant services on cloud servers that are accessible over the internet. The flexibility of cloud computing makes it a practical approach for enhancing the quality, dependability, and efficiency of medical services, as well as increasing patient-doctor interaction and safeguarding patient anonymity if proper measures are taken. Furthermore, cloud strategies facilitate healthcare technologies such as computerized healthcare records, remote appointments, mobile applications, patient portals, IoT devices, and big data analytics, enabling trouble-free scalable solutions. Integrating cloud computing technologies can especially be beneficial in increasing the efficiency of healthcare services in developing counties where physical health infrastructure is usually limited. As such, the objective of this work is to explore the feasibility of incorporating cloud and distributed computing in e-healthcare through an extensive requirement analysis and user study. Then, the smart healthcare system will be compared with traditional database-centric healthcare systems and a prototype system will be designed and implemented based on the findings. Finally, we focus on finding the usability and user acceptance of such systems and challenges that lie with integrating cloud services to e-healthcare systems for the general user demographic of developing countries through extensive usability evaluation.},
  keywords={},
  doi={10.1109/ICEEICT53905.2021.9667831},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7335004,
  author={Dai, Yangyang and Lou, Yuansheng and Lu, Xin},
  booktitle={2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics}, 
  title={A Task Scheduling Algorithm Based on Genetic Algorithm and Ant Colony Optimization Algorithm with Multi-QoS Constraints in Cloud Computing}, 
  year={2015},
  volume={2},
  number={},
  pages={428-431},
  abstract={Task scheduling problem in cloud computing environment is NP-hard problem, which is difficult to obtain exact optimal solution and is suitable for using intelligent optimization algorithms to approximate the optimal solution. Meanwhile, quality of service (QoS) is an important indicator to measure the performance of task scheduling. In this paper, a novel task scheduling algorithm MQoS-GAAC with multi-QoS constraints is proposed, considering the time-consuming, expenditure, security and reliability in the scheduling process. The algorithm integrates ant colony optimization algorithm (ACO) with genetic algorithm (GA). To generate the initial pheromone efficiently for ACO, GA is invoked. With the designed fitness function, 4-dimensional QoS objectives are evaluated. Then, ACO is utilized to seek out the optimum resource. The experiment indicates that the proposed algorithm has preferable performance both in balancing resources and guaranteeing QoS.},
  keywords={},
  doi={10.1109/IHMSC.2015.186},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{6958835,
  author={Wen, Zi-Yi and Hsiao, Hsu-Feng},
  booktitle={2014 IEEE 16th International Workshop on Multimedia Signal Processing (MMSP)}, 
  title={QoE-driven performance analysis of cloud gaming services}, 
  year={2014},
  volume={},
  number={},
  pages={1-6},
  abstract={With the popularity of cloud computing services and the endorsement from the video game industry, cloud gaming services have emerged promisingly. In a cloud gaming service, the contents of games can be delivered to the clients through either video streaming or file streaming. Due to the strict constraint on the end-to-end latency for real-time interaction in a game, there are still challenges in designing a successful cloud gaming system, which needs to deliver satisfying quality of experience to the customers. In this paper, the methodology for subjective and objective evaluation as well as the analysis of cloud gaming services was developed. The methodology is based on a nonintrusive approach, and therefore, it can be used on different kinds of cloud gaming systems. There are challenges in such objective measurements of important QoS factors, due to the fact that most of the commercial cloud gaming systems are proprietary and closed. In addition, satisfactory QoE is one of the crucial ingredients in the success of cloud gaming services. By combining subjective and objective evaluation results, cloud gaming system developers can infer possible results of QoE levels based on the measured QoS factors. It can also be used in an expert system for choosing the list of games that customers can appreciate at a given environment, as well as for deciding the upper bound of the number of users in a system.},
  keywords={},
  doi={10.1109/MMSP.2014.6958835},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7037229,
  author={Mazza, Daniela and Tarchi, Daniele and Corazza, Giovanni E.},
  booktitle={2014 IEEE Global Communications Conference}, 
  title={A user-satisfaction based offloading technique for smart city applications}, 
  year={2014},
  volume={},
  number={},
  pages={2783-2788},
  abstract={The Smart cities applications are gaining an increasing interest among administrations, citizens and technologists for their suitability in managing the everyday life. One of the major challenges is regarding the possibility of managing in an efficient way the presence of multiple applications in a Wireless Heterogeneous Network (HetNet) environment, alongside the presence of a Mobile Cloud Computing (MCC) infrastructure. In this context we propose a utility function model derived from the economic world aiming to measure the Quality of Service (QoS), in order to choose the best access point in a HetNet to offload part of an application on the MCC, aiming to save energy for the Smart Mobile Devices (SMDs) and to reduce computational time. We distinguish three different types of application, considering different offloading percentage of computation and analyzing how the cell association algorithm allows energy saving and shortens computation time. The results show that when the network is overloaded, the proposed utility function allows to respect the target values by achieving higher throughput values, and reducing the energy consumption and the computational time.},
  keywords={},
  doi={10.1109/GLOCOM.2014.7037229},
  ISSN={1930-529X},
  month={Dec},}

@INPROCEEDINGS{9306518,
  author={Forcan, M. and Maksimović, M. and Forcan, J. and Jokić, S.},
  booktitle={2020 28th Telecommunications Forum (TELFOR)}, 
  title={5G and Cloudification to Enhance Real-Time Electricity Consumption Measuring in Smart Grid}, 
  year={2020},
  volume={},
  number={},
  pages={1-4},
  abstract={The number of smart devices in Smart Grid (SG) increases continuously and the presence of big data demands more efficient communication architectures. It is anticipated that the full potential of the SG vision in terms of better performances, reliability, and quality of service, can be achieved by incorporating the fifth generation of cellular network technology (5G) and Cloudification into the SG. In order to demonstrate their potential in SG, this paper presents the enhancement of real-time electricity consumption measuring with the help of 5G and Cloud computing. 5G-based communication model supporting Advanced Metering Infrastructure (AMI) in SG is built and validated on the example of real-time communication between the SM model and Cloud platform ThingSpeak.},
  keywords={},
  doi={10.1109/TELFOR51502.2020.9306518},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9004389,
  author={Anita, J. Mary and Raina, Roma},
  booktitle={2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE)}, 
  title={Review on Smart Grid Communication Technologies}, 
  year={2019},
  volume={},
  number={},
  pages={215-220},
  abstract={Smart grid (SG) has given a better vision for electricity infrastructure. The quality, quantity of power transmitted and the usage of available data from smart sensing, metering and communication has dramatically increased with the introduction of smart grid to power systems. SG also has empowered customer participation by managing their load pattern to take advantage of choosing their supply and pricing options. The heart of the SG lies on the communication between the consumers and the grid operators. Grids operators need the real time customer meter data to schedule their supply and pricing policies and the consumers need the same to manage their loads. The Wireless Sensor Network (WSN) uses Aggregation Protocol with Error Detection (APED) to improve the security of data. The SG with SCADA is facilitated by data acquisitions which includes the meter reading, system conditions, etc. that are monitored and transmitted at regular intervals in real time. The security of data transfer is assured by the introduction of improvised Ciphertext Policy_ Attribute Based Encryption (CP-ABE) is used to achieve the security parameters like confidentiality, integrity, and availability in cloud computing. Block chain-based systems combine distributed register and cryptographic security measures. Introduction of block chain in SG has revolutionized the functioning of SG with smart contracts, and transaction of huge amount of data in a fully decentralized market platform.This paper reviews the modern technologies used in smart grid communication based on IEEE 802.15.4 standard to the SG and how it is modified to ensure effective, efficient and economical and secured communication of the huge real time data from the smart meters.},
  keywords={},
  doi={10.1109/ICCIKE47802.2019.9004389},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9292150,
  author={Gavra, Vlad-Dacian and Pop, Ovidiu Aurel},
  booktitle={2020 IEEE 26th International Symposium for Design and Technology in Electronic Packaging (SIITME)}, 
  title={Usage of ZigBee and LoRa wireless technologies in IoT systems}, 
  year={2020},
  volume={},
  number={},
  pages={221-224},
  abstract={IoT systems are based sensors and actuators to enable ubiquitous sensing to measure environment parameters from delicate ecologies and natural environments to urban environments. By connecting these sensors and actuators to a big network, like internet, an automatization can be performed, and repetitive actions can be done in background by the IoT ecosystem and save a lot of time. IoT can do such things in Home Automation, Smart Cities and even in Air Quality analysis. IoT solution are dependent on the way sensors are transmitting data to cloud or up to the internet. This paper will present the benefits of using Zig Bee instead of using traditional Wi-Fi sensor and present some of the characteristics of LoRa sensors. Cloud computing contributed to the expansion of the IoT systems by offloading local IoT devices of computation intensive tasks. Fog computing brings Cloud closer to the sensors and by doing this minimize communication latencies.},
  keywords={},
  doi={10.1109/SIITME50350.2020.9292150},
  ISSN={2642-7036},
  month={Oct},}

@INPROCEEDINGS{7979932,
  author={Lin, Cho-Chin and Kuo, Yuan-Han and Xie, Dong-Ye and Goh, Wei-Ping and Wu, Shyi-Tsong},
  booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)}, 
  title={A Practical Model for Analyzing Push-Based Virtual Machine Live Migration}, 
  year={2016},
  volume={},
  number={},
  pages={347-352},
  abstract={Virtual Technology has been employed by cloud computing to satisfy service requests from the customers. Virtual machine live migration provides non-stop services while an unexpected event impacts the service quality of the host. The cost of performing live migration is measured by the total number of transferred pages and the service suspension time. In this paper, a practical model for analysing push-based live migration is proposed. The model abstracts live migration strategy into trend and sanction functions. Based on the model, the patterns on the numbers of transferred memory frames in the iterations have been analysed for various dirty frequencies and push rules. Furthermore, it is useful for developing a formal method for conducting complex analysis.},
  keywords={},
  doi={10.1109/CCBD.2016.074},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8892932,
  author={Tri, Nguyen Minh and Nagata, Syunya and Tsuru, Masato},
  booktitle={2019 20th Asia-Pacific Network Operations and Management Symposium (APNOMS)}, 
  title={Locating Delay Fluctuation-Prone Links by Packet Arrival Intervals in OpenFlow Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={In cloud computing and content delivery networking, OpenFlow-based centrally managed networks to connect distributed servers are becoming popular these days. To maintain service quality and availability in such networks by flexible and dynamic traffic engineering, detecting and locating deteriorated (e.g., congested) links in an efficient manner is essential. Following our previous study that actively monitors packet loss rate to find deteriorated links, in this paper, we actively estimate packet delay variance on each link (note both up and down directions of each full-duplex link are distinguished) in an OpenFlow network. A notable feature is that packet delay variance is estimated based on monitoring arrival time intervals of probe packets without directly measuring packet delay time over a link. In the proposed scheme, a series of probe packets is launched from a measurement host and traverses each direction of each link once and only once by multicasting, while arrival time intervals of those packets at each input port of OpenFlow switches are monitored. Then the OpenFlow controller collects the arrival time interval statistics from those switches to locate delay fluctuation-prone links, i.e., links with a high packet delay variance, which are likely congested or physically unstable. In addition, to minimize the necessary number of accesses to switch ports, an appropriate order of collecting statistics from switches is dynamically controlled. The results of numerical simulation on large-scale network topologies demonstrate the effectiveness of our proposed scheme. A prototype implementation which requires an extension of OpenFlow is also presented on Mininet.},
  keywords={},
  doi={10.23919/APNOMS.2019.8892932},
  ISSN={2576-8565},
  month={Sep.},}

@INPROCEEDINGS{8494151,
  author={Prathibha, K and Hegde, Pawan},
  booktitle={2018 9th International Conference on Computing, Communication and Networking Technologies (ICCCNT)}, 
  title={A Real-Time System for Environmental Study Based on Cloud Computing}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Following the ecological parameters variation is important keeping in mind the end goal to decide on the nature of our environment. This paper aims at detecting and detailing the changes in the environmental parameters. Environmental monitoring applications based on cloud computing makes use of sensors to help in protecting environmental conditions by checking parameters like temperature, air quality, earthquake and so on. The utilization of present day advances such as the single-board computers can encourage and give much more functionalities to cloud. The significant areas that cover the above said applications are home, industries, buildings and so forth. This is the way toward observing some of the modules of environment and thus providing the features to the admins and clients. It causes the specialists to screen the states of the work in an organization or industry from remote areas and to take prompt measures.},
  keywords={},
  doi={10.1109/ICCCNT.2018.8494151},
  ISSN={},
  month={July},}

@INPROCEEDINGS{8998716,
  author={Munadi, Rendy and Irawan, Arif Indra and Romiadi, Yuman Fariz},
  booktitle={2019 International Conference on Mechatronics, Robotics and Systems Engineering (MoRSE)}, 
  title={Security System ATM Machine with One-Time Passcode on M-Banking Application}, 
  year={2019},
  volume={},
  number={},
  pages={92-96},
  abstract={Automated Teller Machine (ATM) security system currently still uses magnetic cards and static PIN as its security system, which create many security holes. This security hole in many cases caused many bank customers to lose money mysteriously. In this paper a two-factor authentication system which uses ATM card and dynamic PIN is proposed to overcome this security hole. In this paper, a prototype of an ATM and m-banking application were built. The ATM prototype uses several components such as the Raspberry Pi 3B, smart card, smart card reader / writer, keypad number and LCD monitor. Dynamic PINs are generated using the CSPRNG-SHA1-MWC random number generator. In developing the prototypes, the framework used in this study is based on mobile applications and cloud computing. To evaluate the quality of the prototype, we performed qualitative and quantitative tests. Qualitatively we tested the prototype using a questionnaire using 165 sample respondents to provide an opinion about the safety and comfort of our prototype and quantitatively we measured the prototype to find out the level of randomness of the generated PIN and the QoS of the designed prototype.},
  keywords={},
  doi={10.1109/MoRSE48060.2019.8998716},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8976772,
  author={Rana, Prateek and Sharma, Monika},
  booktitle={2019 2nd International Conference on Power Energy, Environment and Intelligent Control (PEEIC)}, 
  title={Less Energy Consumption Framework for Fog Computing With IoT}, 
  year={2019},
  volume={},
  number={},
  pages={41-46},
  abstract={IOT applications nowadays have quickly expanded and the basic standard centralized models of cloud computing have faced numerous challenging situations which has excessiveness in latency; have low capacity and the failure of network, less capacity of storing and excessive use of power. Fog computing has brought the cloud nearer to the devices of IOT, which deals with the challenges. The services being provided by the fog have quicker response moreover more better quality, in comparison to the cloud. The choices which are best, allow the IOT to offer services which are efficient and secured for most of the users of IOT, that is being measured by the fog computing. In this paper, we are focusing on the fog computing furthermore the incorporation of fog computing with the IOT by specially focusing on the implementation of the challenges is being presented. We have proposed architecture for the less consumption of energy and power.},
  keywords={},
  doi={10.1109/PEEIC47157.2019.8976772},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9355356,
  author={Yang, Yi},
  booktitle={2020 International Conference on Advance in Ambient Computing and Intelligence (ICAACI)}, 
  title={Simulation Analysis of Standardized Management Measures of Enterprise Accounting Based on Cloud Computing}, 
  year={2020},
  volume={},
  number={},
  pages={143-146},
  abstract={In the current corporate governance, accounting management is an indispensable part, in which the quality of accounting directly affects the prosperity of enterprises. Accounting is a basic work in the related work of enterprise accounting, which is the accounting of economic activities of enterprises under the constraints of relevant laws and regulations, and is a summary of previous economic activities. Accounting, as a basic function and the core content of enterprise accounting work, has always been in a very important position. Accounting information feeds back the problems existing in the production and operation of enterprises, so that relevant personnel can continuously optimize their work, and ultimately enhance the competitiveness of products or services provided by enterprises in the market. This paper discusses the connotation and importance of enterprise accounting standardization management, analyzes the problems existing in the process of enterprise accounting standardization management, and puts forward specific measures of enterprise accounting standardization management based on cloud computing.},
  keywords={},
  doi={10.1109/ICAACI50733.2020.00036},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9602504,
  author={Li, Xu and Gao, Guanbin and Na, Jing and Chen, Xin},
  booktitle={2021 33rd Chinese Control and Decision Conference (CCDC)}, 
  title={Design of an Automatic T-beam Erection System Based on NB-IoT for Bridge-Erecting Cranes}, 
  year={2021},
  volume={},
  number={},
  pages={684-689},
  abstract={Bridge-erecting cranes are often used for beam erection in the construction of expressways. To improve the speed and quality of T-beam erection and ensure the safety of bridge-erecting cranes, an automatic T-beam erection system based on the Internet of Things (IoT) is designed in this paper. Narrow Band Internet of Things (NB-IoT) communication technology is used to integrate laser-ranging sensors, batteries, and communication modules into base station subsystems, which are installed in specific locations of the bridge-erecting crane. The position of the T-beam can be measured in real-time by the laser ranging sensors, with which a closed-loop control system is constructed for the T-beam erection system. The information of the running state including the position of the T-beam, the installation progress, and the position of the bridge-erecting crane is transferred to the cloud computing platform by NB-IoT, which can be viewed by mobile terminals. The experimental tests show that the distance measurement range of the system is 0.045m~30m, and the measurement accuracy is 2mm. Compared with the manual operation, the automatic T-beam erection system can reduce the risk of the T-beam erection and improve efficiency.},
  keywords={},
  doi={10.1109/CCDC52312.2021.9602504},
  ISSN={1948-9447},
  month={May},}

@INPROCEEDINGS{9556059,
  author={Jin, Xuan and Xie, Yunlong and Yin, Yitao},
  booktitle={2021 13th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)}, 
  title={BotCatcher:A Complementary Advantages and Deep Learning Based Scheme for Intrusion Detection}, 
  year={2021},
  volume={},
  number={},
  pages={95-98},
  abstract={In today’s society, computer network is one of the most important and advanced infrastructures. The emerging technologies, represented by cloud computing, have brought many new situations and challenges to network security. This paper presents a novel scheme, called BotCatcher, which is based on the fusion and optimization of two different types of existing methods for intrusion detection. There are two models in the scheme: 1) The high-speed mode based on complementary advantages improves the F-Measure by about 0.06 without reducing the detection rate and 2) The high-quality mode based on deep learning combines the historical behavior of hosts and optimizes the detection index for greater performance improvement.},
  keywords={},
  doi={10.1109/IHMSC52134.2021.00030},
  ISSN={},
  month={Aug},}

@ARTICLE{9129768,
  author={Azizi, Sadoon and Shojafar, Mohammad and Abawajy, Jemal and Buyya, Rajkumar},
  journal={IEEE Systems Journal}, 
  title={GRVMP: A Greedy Randomized Algorithm for Virtual Machine Placement in Cloud Data Centers}, 
  year={2021},
  volume={15},
  number={2},
  pages={2571-2582},
  abstract={Cloud computing efficiency greatly depends on the efficiency of the virtual machines (VMs) placement strategy used. However, VM placement has remained one of the major challenging issues in cloud computing mainly because of the heterogeneity in both virtual and physical machines (PMs), the multidimensionality of the resources, and the increasing scale of the cloud data centers (CDCs). An inefficiency in VM placement strategy has a significant influence on the quality of service provided, the amount of energy consumed, and the running costs of the CDCs. To address these issues, in this article, we propose a greedy randomized VM placement (GRVMP) algorithm in a large-scale CDC with heterogeneous and multidimensional resources. GRVMP inspires the “power of two choices” model and places VMs on the more power-efficient PMs to jointly optimize CDC energy usage and resource utilization. The performance of GRVMP is evaluated using synthetic and real-world production scenarios (Amazon EC2) with several performance matrices. The results of the experiment confirm that GRVMP jointly optimizes power usage and the overall wastage of resource utilization. The results also show that GRVMP significantly outperforms the baseline schemes in terms of the performance metrics used.},
  keywords={},
  doi={10.1109/JSYST.2020.3002721},
  ISSN={1937-9234},
  month={June},}

@INPROCEEDINGS{6903503,
  author={Lim, Erbin and Thiran, Philippe},
  booktitle={2014 IEEE International Conference on Cloud Engineering}, 
  title={Communication of Technical QoS among Cloud Brokers}, 
  year={2014},
  volume={},
  number={},
  pages={403-409},
  abstract={Service brokers are commonly used in the cloud computing paradigm to represent service requesters to select a service provider. They act as an intermediary between the two parties. One model of the cloud computing paradigm involves 3 layers, the user, the SaaS provider and the Cloud provider. The selection of service requesters is challenging due to the different levels of Quality of Service that each service provider can provide. In this paper we propose a unique mechanism that allows communication between service brokers in different layers in order to further improve this selection. In addition, we introduce a metric, efficiency, which service brokers can use to deterministically compare service providers with each other.},
  keywords={},
  doi={10.1109/IC2E.2014.92},
  ISSN={},
  month={March},}

@INPROCEEDINGS{6927759,
  author={Sang-Ho Na and Eui-Nam Huh},
  booktitle={Fourth edition of the International Conference on the Innovative Computing Technology (INTECH 2014)}, 
  title={A methodology of assessing security risk of cloud computing in user perspective for security-service-level agreements}, 
  year={2014},
  volume={},
  number={},
  pages={87-92},
  abstract={underlying cloud computing feature, outsourcing of resources, makes the Service Level Agreement (SLA) is a critical factor for Quality of Service (QoS), and many researchers have addressed the question of how a SLA can be evaluated. Lately, security-SLAs have also received much attention with the Security-as-a-Service mode in cloud computing. The quantitative measurement of security metrics is a considerably difficult problem and might be considered the multi-dimensional aspects of security threats and user requirements. To address these issues, we provide a novel a methodology of security risk assessment for security-service-level agreements in the cloud service based on a multi-dimensional approach depending on services type, probabilities of threats, and network environments to reach a security-SLA evaluation.},
  keywords={},
  doi={10.1109/INTECH.2014.6927759},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9537262,
  author={Reddy, Gangireddy Narendra Kumar and Manikandan, M. Sabarimalai and Murty, N. V. L. Narasimha},
  booktitle={2021 IEEE International Conference on Health, Instrumentation & Measurement, and Natural Sciences (InHeNce)}, 
  title={Lightweight Compressed Sensing (CS) and Partial DCT Based Compression Schemes for Energy-Efficient Wearable PPG Monitoring Devices}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Most wearable medical devices are designed to continuously acquire photoplethysmography (PPG) for measuring vital signs and transmitting acquired PPG data wirelessly to edge-computing device or cloud-computing server. These devices are constrained with limited battery power and data-rate capacity. Therefore, in this paper, we present a lightweight effective data-reduction method by investigating the performance of compressed sensing (CS)-based and and partial discrete cosine transform (DCT)-based compression methods with major objectives of achieving higher compression ratio (CR) with minimal waveform distortion with low reconstruction time. By using both normal and abnormal PPG signals, the performance of the CS-based and DCT-based compression methods is evaluated in terms of CR, global and local distortion measures and processing time. Evaluation results showed that CR values of the partial-DCT based method are 3 times higher (CR ranging from 7.50 to 9.38) without distorting fiducial points and shapes of the PPG signal (percentage root-mean-square difference (PRD) ranging from 1% to 2%) as compared to the CS-based data method (CR from 2.50 to 3.13 for PRD from 2% of 4%). The higher data reduction with acceptable level of reconstruction quality demonstrates that the partial DCT-based method can lead to provide better overall energy consumption reduction solution for resource-constrained wearable devices.},
  keywords={},
  doi={10.1109/InHeNce52833.2021.9537262},
  ISSN={},
  month={July},}

@ARTICLE{7226862,
  author={Peng, Kuan-Li and Huang, Chin-Yu},
  journal={IEEE Transactions on Services Computing}, 
  title={Reliability Analysis of On-Demand Service-Based Software Systems Considering Failure Dependencies}, 
  year={2017},
  volume={10},
  number={3},
  pages={423-435},
  abstract={Service-based software systems (SBSSs) are widely deployed due to the growing trend of distributed computing and cloud computing. It is important to ensure high quality of an SBSS, especially in a strongly competitive market. Existing works on SBSS reliability usually assumed independence of service failures. However, the fact that resource sharing exists in different levels of SBSS operations invalidates this assumption. Ignorance of failure dependencies have been discussed as potentially affecting system reliability predictions and lowering the benefits of design diversity, as typically seen in high-reliability systems. In this paper, we propose a reliability framework that incorporates failure dependence modeling, system reliability modeling, as well as reliability analysis for individual services and for failure sources. The framework is also capable of analyzing the internal structures of popular software fault tolerant (FT) schemes. The proposed method is applied to a travel agency system based upon a real-world practice for verifying its accuracy of reliability modeling and effectiveness of varied reliability measures. The results show that failure dependence of the services is an essential factor for analyzing any valuable SBSS system. Further, a set of reliability measures with different capabilities and complexities are available for assisting SBSS engineers with system improvements.},
  keywords={},
  doi={10.1109/TSC.2015.2473843},
  ISSN={1939-1374},
  month={May},}

@INPROCEEDINGS{9210369,
  author={Rico-Bautista, Dewar and Maestre-Gongora, Gina and Guerrero, Cesar D.},
  booktitle={2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)}, 
  title={Smart University:IoT adoption model}, 
  year={2020},
  volume={},
  number={},
  pages={821-826},
  abstract={Higher education organizations see in new technologies an opportunity to improve the quality of their academic and administrative processes. However, the existence and application of new technologies in this type of institutions does not imply that it really has the expected impact on their processes, so many adoption initiatives fail generating losses and discouragement towards change. This happens mainly because more work is done on technology than on the realities of the institutions. One way to prevent this type of problem and redirect efforts is to align the adoption process itself. As artificial intelligence, cloud computing, IoT (Internet of Things) and Big Data technologies become stronger, it is necessary to have tools at hand that have the capacity to measure the level of adoption by institutions. The objective of measuring adoption by the processes and realities of higher education institutions, with a focus on their users. Many models have been proposed to understand why users accept or use technologies. Among those that have emerged for the study of technology adoption are TAM, UTAUT, UTAUT2, DOI, TPB, TRA, among others. This characterization allows us to conclude about the need for alignment and integration of technology with the organization's processes, calling for greater interaction with senior management.},
  keywords={},
  doi={10.1109/WorldS450073.2020.9210369},
  ISSN={},
  month={July},}

@INPROCEEDINGS{8711239,
  author={Navamani, Beaulah A and Yue, Chuan and Zhou, Xiaobo},
  booktitle={2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC)}, 
  title={Discover and Secure (DaS): An Automated Virtual Machine Security Management Framework}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Cloud computing is very appealing for its convenient central management, the elasticity of resource provisioning and its economic benefits. Undoubtedly, the non-transparent nature of the Cloud infrastructure introduces significant security concerns. Naively, Virtual Machine (VM) migration can weaken or even nullify the security protection on a VM. Attackers compromise such vulnerable hosts and can either take control over their resources or use them as a channel for future attacks. To overcome the hidden security risk, this paper proposes Discover and Secure (DaS) framework for automated VM security management. This framework accomplishes two qualities: 1) to discover whether the VM is an inadvertent security victim 2) to secure the VM and the mission-critical applications running inside them. Modules in this framework detect, extract and measures the new identifiers assigned to the VM. Comparing the new identifiers to the reference table containing the old measured identifier values, verifies the identifier/s status. Transformed identifiers are perceived and replaced with new valid ones, hence, restoring the nullified security. This framework is implemented as VM-Internal security, self-supplied by the user and VM-introspection security, host-supplied by the cloud provider. Experimental results show that DaS framework can armor the VM from obscured security problems and seal the hidden door against attackers.},
  keywords={},
  doi={10.1109/PCCC.2018.8711239},
  ISSN={2374-9628},
  month={Nov},}

@INPROCEEDINGS{9732730,
  author={Shrivastava, Ritu and Tiwary, Abhigyan and Yadav, Pranay},
  booktitle={2021 International Conference on Advances in Technology, Management & Education (ICATME)}, 
  title={Challenges Block Chain Technology Using IOT for Improving Personal and Physical Safety - Review}, 
  year={2021},
  volume={},
  number={},
  pages={238-243},
  abstract={Web of Things (IoT) is an interconnection of clever actual articles called things and People. IoT permits each “Thing” to associate and impart in this manner creating and sending a colossal measure of Data like a monster Information System. Because of the immense measure of Data dealing with by IoT gadgets it got fundamental to incorporate Cloud Computing, Machine Learning and Information Modelling into The IoT stage. The enormous development in the field of IoT is causing increment in Information and Communication Technology (ICT) business too. It is anticipated that before the finish of 2020 95% of the new items will have IoT as its canter. As can be seen there will be an expanding presence of IoT objects and thus their perceivability from the Internet and lawful admittance to assets is a subject of grave concern. IoT gives and empowers the formation of creative applications that improved the physical and individual existence of an individual, yet the absence of security and weakness of individuals may prompt basic issues like the wellbeing of our homes might be undermined, and Centralized organizations utilizing delicate information are consistently at a danger hacking. Block-chain innovation is increasing a ton of consideration from different associations and parcel of examination is being done as it gives extraordinary answers for the issues related with the traditional incorporated design of IoT. Since, there are so numerous IoT gadgets which are on the lookout for the upgrade of an individual's physical, mental development and improving the personal satisfaction by and large, a conveyed trust innovation, guaranteeing versatility, security, and unwavering quality, is the need of great importance for the development of IoT conditions. The joining of IoT and BC represents a ton of difficulties. The Objective of this exploration paper is to build up an incorporated IOT and Block-chain Application Environment to upgrade the individual and Physical existence of a being. Consequently, our center will be to build up a safe and safe climate for a Smart home. Our proposed design depends on progressive structure and disseminated foundation of Block-chain to keep up security and protection and make it appropriate for explicit prerequisites of IoT. Subsequently we have coordinated the mechanization of the home alongside the actual soundness of an individual in this manner guaranteeing wellbeing, accommodation and strength of an individual.},
  keywords={},
  doi={10.1109/ICATME50232.2021.9732730},
  ISSN={},
  month={Jan},}

@ARTICLE{9153148,
  author={Abdel-Basset, Mohamed and El-Shahat, Doaa and Elhoseny, Mohamed and Song, Houbing},
  journal={IEEE Internet of Things Journal}, 
  title={Energy-Aware Metaheuristic Algorithm for Industrial-Internet-of-Things Task Scheduling Problems in Fog Computing Applications}, 
  year={2021},
  volume={8},
  number={16},
  pages={12638-12649},
  abstract={In Industrial-Internet-of-Things (IIoT) applications, fog computing (FC) has soared as a means to improve the Quality of Services (QoSs) provided to users through cloud computing, which has become overwhelmed by the massive flow of data. Transmitting all these amounts of data to the cloud and coming back with a response can cause high latency and requires high network bandwidth. The availability of sustainable energy sources for FC servers is one of the difficulties that the service providers can face in IIoT applications. The most important factor contributing to energy consumption on fog servers is task scheduling. In this article, we suggest an energy-aware metaheuristic algorithm based on a Harris Hawks optimization algorithm based on a local search strategy (HHOLS) for task scheduling in FC (TSFC) to improve the QoSs provided to the users in IIoT applications. First, we describe the high virtualized layered FC model taking into account its heterogeneous architecture. The normalization and scaling phase aids the standard Harris hawks algorithm to solve the TSFC, which is discrete. Moreover, the swap mutation ameliorates the quality of the solutions due to its ability to balance the workloads among all virtual machines. For further improvements, a local search strategy is integrated with HHOLS. We compare HHOLS with other metaheuristics using various performance metrics, such as energy consumption, makespan, cost, flow time, and emission rate of carbon dioxide. The proposed algorithm gives superior results in comparison with other algorithms.},
  keywords={},
  doi={10.1109/JIOT.2020.3012617},
  ISSN={2327-4662},
  month={Aug},}

@INPROCEEDINGS{7063421,
  author={Ran, Yongyi and Shi, Youkang and Yang, Enzhong and Chen, Shuangwu and Yang, Jian},
  booktitle={2014 IEEE Globecom Workshops (GC Wkshps)}, 
  title={Dynamic resource allocation for video transcoding with QoS guaranteeing in cloud-based DASH system}, 
  year={2014},
  volume={},
  number={},
  pages={144-149},
  abstract={Due to diverse network conditions and heterogeneous devices, there may be various video demands with different video qualities and formats from the client side. Compared to keeping all necessary copies for the same video, video transcoding in real-time should be an essential solution. The complex nature of video transcoding enables cloud computing to be uniquely suitable for dynamically providing transcoding resource. However, due to the fluctuation and uncertainty of the future transcoding demand, it is still a challenge to dynamically determine the optimal resource allocation to save cost while guaranteeing the Quality of Service (QoS). Overload may result in the transcoding jitter and increase the lateness which directly affects video freezes while over-provisioning naturally increases the cost. To address this problem, in this paper, by defining the transcoding jitter probability as a metric of QoS, we proposed a dynamic resource allocation algorithm based on the large deviation principle, which is capable of proactive calculating the optimal number of transcoding nodes for the upcoming transcoding demand subject to the transcoding jitter probability below a desired threshold. Finally, the experiments are performed on a cloud-based prototype system to show the attainable performance of the proposed resource allocation algorithm and verify that the proposed algorithm can make a good tradeoff between cost saving and QoS guaranteeing.},
  keywords={},
  doi={10.1109/GLOCOMW.2014.7063421},
  ISSN={2166-0077},
  month={Dec},}

@INPROCEEDINGS{7027516,
  author={Keller, Matthias and Robbert, Christoph and Karl, Holger},
  booktitle={2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing}, 
  title={Template Embedding: Using Application Architecture to Allocate Resources in Distributed Clouds}, 
  year={2014},
  volume={},
  number={},
  pages={387-395},
  abstract={In distributed cloud computing, application deployment across multiple sites can improve quality of service. Recent research developed algorithms to find optimal locations for virtual machines. However, those algorithms assume to have either single-tier applications or a fixed number of virtual machines -- a strong simplification of reality. This paper investigates the placement and scaling of complex application architectures. An application is dynamically scaled to fit both the current demand situation and the currently available infrastructure resources. We compare two approaches: The first one is based on virtual network embedding. The second approach is a novel method called Template Embedding. It is based on a hierarchical 1-allocation hub flow problem and combines application scaling and embedding in one step. Extensive experiments on 43200 network configurations showed that Template Embedding outperforms virtual network embedding in all cases in three metrics: success rate, solution quality, and runtime. This positive result shows that template embedding is a promising approach for distributed cloud resource allocation.},
  keywords={},
  doi={10.1109/UCC.2014.49},
  ISSN={},
  month={Dec},}

@ARTICLE{9395576,
  author={Cedillo, Priscila and Insfran, Emilio and Abrahão, Silvia and Vanderdonckt, Jean},
  journal={IEEE Access}, 
  title={Empirical Evaluation of a Method for Monitoring Cloud Services Based on Models at Runtime}, 
  year={2021},
  volume={9},
  number={},
  pages={55898-55919},
  abstract={Cloud computing is being adopted by commercial and governmental organizations driven by the need to reduce the operational cost of their information technology resources and search for a scalable and flexible way to provide and release their software services. In this computing model, the Quality of Services (QoS) is agreed between service providers and their customers through Service Level Agreements (SLA). There is thus a need for systematic approaches with which to assess the quality of cloud services and their compliance with the SLA. In previous work, we introduced a generic method for Monitoring cloud Services using models at RunTime (MoS@RT), which allows the monitoring requirements or the metric operationalizations of these requirements to be changed at runtime without the modification of the underlying infrastructure. In this paper, we present the design of a monitoring infrastructure that supports the proposed method with its instantiation to a specific platform and reports the results of an experiment carried out to evaluate the perceived efficacy of 58 undergraduate students when using the infrastructure to configure the monitoring of cloud services deployed on the Microsoft Azure platform. The results show that the participants perceived MoS@RT to be easy to use, useful, and they also expressed their intention to use the method in the future. Although further experiments must be carried out to strengthen these results, MoS@RT has proved to be a promising monitoring method for cloud services.},
  keywords={},
  doi={10.1109/ACCESS.2021.3071417},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9242580,
  author={Albur, Nageshwar and Handigol, Sonal and Naik, Sonali and Mulla, Mohammed Moin and Narayan, D. G.},
  booktitle={2020 12th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={QoS-aware Flow Management in Software Defined Network}, 
  year={2020},
  volume={},
  number={},
  pages={215-220},
  abstract={Software Defined Networking is a developing pattern in a computer network that authorize a controller to control and keep track of the entire network state of a system. Traditional networks are not compatible for business ventures since they are manually configured (static) and are inflexible. To solve the problems related to traditional network framework, SDN is considered as an effective solution because SDN is dynamic and easily manageable. Quality-of-Service (QoS) is a list of network requirements that is mentioned in Service Level Agreement (SLA) and this requirements has to be satisfied when a packet is being streamed from one node to another node in a network. QoS metrics provide a sophisticated way to prioritise the network traffic over a network to guarantee better performance. QoS assures that the priorities of routing does not change because change-in prioritise may lead to jitter, packet loss and delay. With rapid development in the cloud computing environment for hosting various virtual applications, Quality Of Service (QoS) has to be maintained while delivering the services that were specified in the Software Level Agreement (SLA). Network Traffic Management helps the administrator to reduce the congestion, packet loss, latency, and also ensures smooth network operations with the help of traffic monitoring tools and techniques. The proposed work presents the QoS aware routing using the OpenDayLight controller. To implement the routing algorithm the bandwidth and the delay parameters are considered. These help to manage the resources of the network by prioritizing specific types of packets on the network. Packet switching not only forwards the data packets but also focuses on selecting the optimal path available for routing of these packets based on certain parameters.},
  keywords={},
  doi={10.1109/CICN49253.2020.9242580},
  ISSN={2472-7555},
  month={Sep.},}

@INPROCEEDINGS{9284567,
  author={Zhang, Yilei and Zhang, Xiao and Zhang, Peiyun and Luo, Jun},
  booktitle={2020 IEEE International Conference on Services Computing (SCC)}, 
  title={Credible and Online QoS Prediction for Services in Unreliable Cloud Environment}, 
  year={2020},
  volume={},
  number={},
  pages={272-279},
  abstract={With the widespread adoption of cloud computing, Service-Orientated Architecture (SOA) facilitates the deployment of large-scale online applications in many key areas where quality and reliability are critical. In order to ensure the performance of cloud applications, Quality of Service (QoS) is widely used as a key metric to enable QoS-driven service selection, composition, adaption, etc. Since QoS data observed by users is sparse due to technical constraints, previous studies have proposed prediction approaches to solve this problem. However, the dynamic nature of the cloud environment requires timely prediction of time-varying QoS values. In addition, unreliable QoS data from untrustworthy users may significantly affect the prediction accuracy. In this paper, we propose a credible online QoS prediction approach to address these challenges. We evaluate user credibility through a reputation mechanism and employ online learning techniques to provide QoS prediction results at runtime. The proposed approach is evaluated on a large-scale real-world QoS dataset, and the experimental results demonstrate its effectiveness and efficiency in unreliable cloud environment.},
  keywords={},
  doi={10.1109/SCC49832.2020.00043},
  ISSN={2474-2473},
  month={Nov},}

@INPROCEEDINGS{7761131,
  author={O'Donncha, Fearghal and Venugopal, Srikumar and James, Scott C. and Ragnoli, Emanuele},
  booktitle={OCEANS 2016 MTS/IEEE Monterey}, 
  title={Deploying and optimizing performance of a 3D hydrodynamic model on cloud}, 
  year={2016},
  volume={},
  number={},
  pages={1-7},
  abstract={Container-based cloud computing, as standardised and popularised by the open-source docker project has many potential opportunities for scientific application in highperformance computing. It promises highly flexible and available compute capabilities via cloud, without the resource overheads of traditional virtual machines. Further, productivity gains can be made by easy repackaging of images with additional developments, automated deployments, and version-control integrations. Nevertheless, the impact of container overhead and overlay network implementation and performance are areas that requires detailed study to allow for well-defined quality of service for typical HPC applications. This papers presents details on deploying the Environmental Fluid Dynamics Code (EFDC) on a container-based cloud environment. Results are compared to a bare metal deployment. Application-specific benchmarking tests are complemented by detailed network tests that evaluate isolated MPI communication protocols both at intra-node and inter-node level with varying degrees of self-contention. Cloud-based simulations report significant performance loss in mean run-times. A containerised environment increases simulation time by up to 50%. More detailed analysis demonstrates that much of this performance penalty is a result of large variance in MPI communciation times. This manifests as simulation runtime variance on container cloud that hinders both simulation run-time and collection of well-defined quality-of-service metrics.},
  keywords={},
  doi={10.1109/OCEANS.2016.7761131},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8390400,
  author={Silva, Helber and Barbalho, Felipe and Neto, Augusto},
  booktitle={2018 International Conference on Computing, Networking and Communications (ICNC)}, 
  title={Cross-layer Multiuser Session Control for Improved SDN Cloud Communications}, 
  year={2018},
  volume={},
  number={},
  pages={377-382},
  abstract={The integration of Cloud Computing and Internet of Things (IoT) is foreseen as an enabler to suit a plethora of novel latency critical applications (e.g, e-health, intelligent transportation, safety, energy, smart cities, and many others). These applications require multimedia (mainly video) flows to be handled by the underlying network in an efficient and scalable way, as they expect to consume a massive data produced by billions of things. In view of this, we propose a dynamic multiuser session control plane which leverages 5G's support of Software-Defined Networking (SDN) substrate to advance beyond todays limited, per-flow IP-based communication systems. We handle such limitations by proposing CLASSICO, a Cross-LAyer Sdn SessIon COntrol architecture that exploits SDN to offload the flow streaming computation operations from the IoT cloud platform to the network edge, affording high timeliness and scalability for the IoT-cloudified system. CLASSICO dynamically builds Application Layer multiuser data sessions and maps them into enhanced group-enabled data paths featuring SDN replication at branching nodes. We applied our solution to multimedia-alike use case, and results show that CLASSICO outperforms typical SDN-enabled IoT systems in terms to Quality of Service (QoS) and Quality of Experience (QoE) video metrics.},
  keywords={},
  doi={10.1109/ICCNC.2018.8390400},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7847681,
  author={Pendlebury, John and Emeakaroha, Vincent C. and O'Shea, David and Cafferkey, Neil and Morrison, John P. and Lynn, Theo},
  booktitle={2016 2nd International Conference on Cloud Computing Technologies and Applications (CloudTech)}, 
  title={SOMBA - automated anomaly detection for Cloud quality of service}, 
  year={2016},
  volume={},
  number={},
  pages={71-79},
  abstract={Cloud computing has transformed the standard model of service provisioning, allowing the delivery of on-demand services over the Internet. With its inherent requirements for elastic scalability and a pay-as-you-go pricing model, an additional level of complexity is added to its Quality of Service (QoS) management. This has made service provisioning more prone to performance anomalies due to the large-scale and evolving nature of Clouds. Existing methods for anomaly detection based on QoS monitoring in the Cloud rely on probabilistic methods, which are not computationally easy and are often valid for very short times before system dynamics change. We posit that more minimalistic approaches including automated techniques are needed for effective anomaly detection to support QoS enforcement in Clouds. In this paper, we present an automated anomaly detection scheme that recognises and adapts to changes in Clouds for efficient multi-metric performance anomaly detection to guarantee service quality. It includes a monitoring tool for collating performance data in real time for analysis and an anomaly detection technique based on an unsupervised machine learning strategy. Based on a Cloud service provisioning use case scenario, we evaluate our anomaly detection technique and compare it against two statistical anomaly detection approaches to demonstrate its efficiency.},
  keywords={},
  doi={10.1109/CloudTech.2016.7847681},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9454002,
  author={Estrela, Vania V. and Andreopoulos, Nikolaos and Sroufer, Robert and de Jesus, Maria A. and Mamani, Wilma Dora Huacasi and Peixoto, Aruquia},
  booktitle={2021 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Transmedia Ecosystems, Quality of Experience and Quality of Service in Fog Computing for Comfortable Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1003-1009},
  abstract={This paper looks at the concepts of Quality of Service (QoS) and Quality of Experience (QoE) for the valuation of Transmedia Ecosystems (TEs) services in fog networking. Fog computing (FC) has delivered new services that cloud computing (CC) cannot make available, particularly those calling for QoE warranties. The suggested model is advantageous in an FC that comprises cloud-like services to sustain users with low latency response necessities. This TE model for QoE/QoS relies on objective and subjective QoE metrics. These assessment mechanisms will capture evidence automatically employing agent technology with user feedback. This proposed structure observes, investigates, yields reports and policy alterations without administrators' intervention.},
  keywords={},
  doi={10.1109/EDUCON46332.2021.9454002},
  ISSN={2165-9567},
  month={April},}

@ARTICLE{7018938,
  author={Lu, Yao and Liu, Yao and Dey, Sujit},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Cloud Mobile 3D Display Gaming User Experience Modeling and Optimization by Asymmetric Graphics Rendering}, 
  year={2015},
  volume={9},
  number={3},
  pages={517-532},
  abstract={With the arrival of auto-stereoscopic 3D displays for mobile devices, and emergence of more 3D content, there is much anticipation for 3D mobile multimedia experiences, including 3D display gaming. Simultaneously, with the emergence of cloud computing, more mobile applications are being developed to take advantage of the elastic cloud resources. In this paper, we explore the possibility of developing Cloud Mobile 3D Display Gaming, where the 3D video rendering and encoding is performed on cloud servers, with the resulting 3D video streamed over wireless networks to mobile devices with 3D displays for a true 3D mobile gaming experience. However, with the significantly higher bitrate requirement for 3D video, ensuring user experience may be a challenge, both in terms of 3D video quality and network delay (response time), considering the bandwidth constraints and fluctuations of wireless networks. In this paper, we propose a new asymmetric graphics rendering approach which can significantly reduce the video encoding bitrate needed for a certain video quality, thereby making it easier to transmit the video over wireless network. However, since asymmetric graphics rendering may also impair the graphics quality, we need to be able to understand and measure its impact. We conduct subjective tests to study and model the impairments due to asymmetric graphics rendering and network delay, thereby developing a user experience model for cloud based mobile 3D display gaming. By conducting subsequent subjective tests, we prove the correctness of the impairment functions and the resulting user experience model. Furthermore, given any network condition, we propose to solve the problem of selecting the optimal graphics rendering factors for the left and right views so as to maximize user experience of cloud mobile 3D display gaming. In order to solve this problem, we first develop a model to estimate the resulting video bitrate of the rendered 3D video when certain graphics rendering factors are used. Next, we derive a model to predict the delay given the available network bandwidth and the video bitrate of the rendered 3D video. We use the above two models together with a branch and bound algorithm to solve the optimization problem and determine the optimal values for the left and right view rendering factors. Experiments conducted using real 4G-LTE network profiles on commercial cloud service demonstrate the feasibility of significant improvement in user experience when the proposed optimization algorithm is used to dynamically select optimal rendering factors according to changing network conditions.},
  keywords={},
  doi={10.1109/JSTSP.2015.2396475},
  ISSN={1941-0484},
  month={April},}

@INPROCEEDINGS{8230340,
  author={Laghari, Asif Ali and He, Hui and Shafiq, Muhammad and Khan, Asiya},
  booktitle={2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN)}, 
  title={Impact of storage of mobile on quality of experience (QoE) at user level accessing cloud}, 
  year={2017},
  volume={},
  number={},
  pages={1402-1409},
  abstract={Quality of Experience (QoE) is referred as level of user's satisfaction, enjoyment, learning and evaluation about the services or products. Recently quality of experience is used for improvement in product development life cycle after getting feedback from end users and service providers also use QoE to measure quality of services (QoS) of their services. Cloud computing provides services such as storage, web hosting, operating system environment, application development platform and CPU resources pay per use. End users are accessing cloud services via mobile apps, but enormous amount of temporary/cache data is generated by apps, so internal storage of mobile devices are filled quickly. Mobile device without any space in internal storage has huge impact on the performance when accessing the cloud services, which degrade the QoE of end users for particular cloud app and services. This paper presents the results of experiments conducted using two mobile devices HTC and Samsung to analyze the impact on end user's QoE during accessing cloud, when internal storage of HTC mobile device is filled and Samsung having 10 GB free space. Finally, on the basis of experimental results future changes in cloud apps are suggested for service provider to improve end user's QoE.},
  keywords={},
  doi={10.1109/ICCSN.2017.8230340},
  ISSN={2472-8489},
  month={May},}

@INPROCEEDINGS{9415574,
  author={Younis, Ayman and Qiu, Brian and Pompili, Dario},
  booktitle={2021 16th Annual Conference on Wireless On-demand Network Systems and Services Conference (WONS)}, 
  title={QLRan: Latency-Quality Tradeoffs and Task Offloading in Multi-node Next Generation RANs}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Next-Generation Radio Access Network (NG-RAN) is an emerging paradigm that provides flexible distribution of cloud computing and radio capabilities at the edge of the wireless Radio Access Points (RAPs). Computation at the edge bridges the gap for roaming end users, enabling access to rich services and applications. In this paper, we propose a multi-edge node task offloading system, i.e., QLRan, a novel optimization solution for latency and quality tradeoff task allocation in NG-RANs. Considering constraints on service latency, quality loss, and edge capacity, the problem of joint task offloading, latency, and Quality Loss of Result (QLR) is formulated in order to minimize the User Equipment (UEs) task offloading utility, which is measured by a weighted sum of reductions in task completion time and QLR cost. The QLRan optimization problem is proved as a Mixed Integer Nonlinear Program (MINLP) problem, which is a NP-hard problem. To efficiently solve the QLRan optimization problem, we utilize Linear Programming (LP)-based approach that can be later solved by using convex optimization techniques. Additionally, a programmable NG-RAN testbed is presented where the Central Unit (CU), Distributed Unit (DU), and UE are virtualized using the OpenAirInterface (OAI) software platform to characterize the performance in terms of data input, memory usage, and average processing time with respect to QLR levels. Simulation results show that our algorithm performs significantly improves the network latency over different conflgurations.},
  keywords={},
  doi={10.23919/WONS51326.2021.9415574},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8545546,
  author={Štofová, Lenka and Szaryszova, Petra and Bosák, Martin and Tarča, Alexander and Hajduová, Zuzana},
  booktitle={2018 XIV International Scientific-Technical Conference on Actual Problems of Electronics Instrument Engineering (APEIE)}, 
  title={Ambient Intelligence for Increasing Innovation Performance of Enterprises}, 
  year={2018},
  volume={},
  number={},
  pages={452-458},
  abstract={The new development of the changing global environment brings to the attention of ideas in the form of trends that show the future environment very differently from the current state. Technical experts are discussing the growing presence of internet of things and technologies, the involvement of all devices, and the prediction of how to create ambient intelligence. This type of intelligence refers to an electronic environment that is sensitive and responsive to the presence of users. Within these ambient systems, we can identify what the user needs and at the same time obtain it without asking for it. Ambient intelligence is a combination of neural networking, smart technologies, cloud computing, big data, websites, bearers, and user interface to services that can automate processes and make recommendations to improve the quality of users' lives. On a wider scale, there are modern high-tech possibilities to closely monitor the impact on the quality and safety of the current market environment by ambient intelligence and sensory management solutions. The aim of the paper is to highlight the selected trends of ambient intelligence system applications as an innovative paradigm to support smart innovation that will further enhance the quality of the automotive industry's technological solution in Slovak republic with advancing developments inspired by other successful companies' results acting in this sector. The reliability of their most important measures were obtained by correlation analysis and multiple linear regression.},
  keywords={},
  doi={10.1109/APEIE.2018.8545546},
  ISSN={2473-8573},
  month={Oct},}

@INPROCEEDINGS{6785362,
  author={Yao Lu and Liu, Yao and Dey, Sujit},
  booktitle={2014 International Conference on Computing, Networking and Communications (ICNC)}, 
  title={Enhancing Cloud Mobile 3D display gaming user experience by asymmetric graphics rendering}, 
  year={2014},
  volume={},
  number={},
  pages={368-374},
  abstract={With the arrival of auto-stereoscopic 3D displays for mobile devices, and emergence of more 3D content, there is much anticipation for 3D mobile multimedia experiences, including 3D display gaming. Simultaneously, with the emergence of cloud computing, more mobile applications are being developed to take advantage of the elastic cloud resources. In this paper, we explore the possibility of developing Cloud-based 3D Mobile Gaming, where the 3D video rendering and encoding is performed on cloud servers, with the resulting 3D video streamed over wireless networks to mobile devices with 3D displays for a true 3D mobile gaming experience. However, with the significantly higher bit rate requirement for 3D video, ensuring user experience may be a challenge, both in terms of 3D video quality and network delay (response time), considering the bandwidth constraints and fluctuations of wireless networks. In this paper, we propose a new asymmetric graphics rendering approach which can significantly reduce the video encoding bit rate needed for a certain video quality, thereby making it easier to transmit the video over wireless network. However, since asymmetric rendering may also impair the graphics quality, we need to be able to understand and measure its impact. We conduct subjective tests to study and model the impairments due to asymmetric rendering and network delay, thereby developing a user experience model for cloud based mobile 3D display gaming. By conducting subsequent subjective tests, we prove the correctness of the impairment functions and the resulting user experience model. We also conduct experiments using real 4G-LTE network profile. Experimental results show that by making use of the user experience model, it is possible to set appropriate graphics rendering parameters according to network constraints, such that the user experience can be maintained to a high level.},
  keywords={},
  doi={10.1109/ICCNC.2014.6785362},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{8053360,
  author={Fang, Dongfeng and Ye, Feng and Qian, Yi and Sharif, Hamid},
  booktitle={2017 IEEE International Conference on Electro Information Technology (EIT)}, 
  title={An efficient incentive mechanism for cloud-based mobile sensor network}, 
  year={2017},
  volume={},
  number={},
  pages={229-234},
  abstract={Mobile sensor networks (MSNs) enable extensive applications of data collection, such as accident report in transportation and health prediction in public health. Incentive mechanism (IM) is applied for sensing user (SU) recruitment. However, the IM used in traditional MSN is not efficient due to limited information of SU used for recruitment. With the development of cloud computing technology, cloud-based MSN is the trend to use more information of SUs for IM design to improve its efficiency. In this paper, a novel cloud-based MSN model is presented. Three parties are considered, including data request party, cloud-based platform and SUs. A data quality model is proposed to measure the credit level of SUs. In addition, with consideration of social connections of SUs, a SU recruitment strategy is presented. SUs are divided into first and second degrees based on how they join the sensing task. The utility functions of first degree SUs and cloud-based platform are presented, respectively. At last, an efficient IM is proposed by formulating a Stackelberg game. The performance of the proposed IM on data quality and SU recruitment time comparing with other method are presented and discussed. The simulation results illustrate that the proposed IM ensures data quality for data request party and recruits SUs more efficiently.},
  keywords={},
  doi={10.1109/EIT.2017.8053360},
  ISSN={2154-0373},
  month={May},}

@INPROCEEDINGS{7396225,
  author={Heidari, Parisa and Boucheneb, Hanifa and Shami, Abdallah},
  booktitle={2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)}, 
  title={A Formal Approach for QoS Assurance in the Cloud}, 
  year={2015},
  volume={},
  number={},
  pages={629-634},
  abstract={Cloud computing is an attractive business model offering cost-efficiency and business agility. Recently, the trend is that small and large businesses are moving their services to cloud environments. The quality of service is always negotiated between the cloud users and the cloud providers and documented in the service level agreement (SLA). Yet assuring -- or even measuring -- the quality of the provided service can be challenging. This paper proposes a formal approach for quantifying the quality of service in the cloud systems as promised in the SLA. The proposed approach uses controller synthesis to find a system configuration that meets the SLA requirement. The formal approach suggested in this paper is based on, but not limited to, %the controller synthesis of Time Petri Nets (TPN). As a case study, we focus on service availability as a key performance indicator in the SLA and for a sample set of resources providing a service, we determine the system configuration satisfying the SLA.},
  keywords={},
  doi={10.1109/CloudCom.2015.36},
  ISSN={},
  month={Nov},}

@ARTICLE{7807337,
  author={Singh, Sukhpal and Chana, Inderveer and Buyya, Rajkumar},
  journal={IEEE Transactions on Cloud Computing}, 
  title={STAR: SLA-aware Autonomic Management of Cloud Resources}, 
  year={2020},
  volume={8},
  number={4},
  pages={1040-1053},
  abstract={Cloud computing has recently emerged as an important service to manage applications efficiently over the Internet. Various cloud providers offer pay per use cloud services that requires Quality of Service (QoS) management to efficiently monitor and measure the delivered services through Internet of Things (IoT) and thus needs to follow Service Level Agreements (SLAs). However, providing dedicated cloud services that ensure user's dynamic QoS requirements by avoiding SLA violations is a big challenge in cloud computing. As dynamism, heterogeneity and complexity of cloud environment is increasing rapidly, it makes cloud systems insecure and unmanageable. To overcome these problems, cloud systems require self-management of services. Therefore, there is a need to develop a resource management technique that automatically manages QoS requirements of cloud users thus helping the cloud providers in achieving the SLAs and avoiding SLA violations. In this paper, we present SLA-aware autonomic resource management technique called STAR which mainly focuses on reducing SLA violation rate for the efficient delivery of cloud services. The performance of the proposed technique has been evaluated through cloud environment. The experimental results demonstrate that STAR is efficient in reducing SLA violation rate and in optimizing other QoS parameters which effect efficient cloud service delivery.},
  keywords={},
  doi={10.1109/TCC.2017.2648788},
  ISSN={2168-7161},
  month={Oct},}

@ARTICLE{7852434,
  author={Mei, Jing and Li, Kenli and Li, Keqin},
  journal={IEEE Transactions on Sustainable Computing}, 
  title={Customer-Satisfaction-Aware Optimal Multiserver Configuration for Profit Maximization in Cloud Computing}, 
  year={2017},
  volume={2},
  number={1},
  pages={17-29},
  abstract={Along with the development of cloud computing, an increasing number of enterprises start to adopt cloud service, which promotes the emergence of many cloud service providers. For cloud service providers, how to configure their cloud service platforms to obtain the maximum profit becomes increasingly the focus that they pay attention to. In this paper, we take customer satisfaction into consideration to address this problem. Customer satisfaction affects the profit of cloud service providers in two ways. On one hand, the cloud configuration affects the quality of service which is an important factor affecting customer satisfaction. On the other hand, the customer satisfaction affects the request arrival rate of a cloud service provider. However, few existing works take customer satisfaction into consideration in solving profit maximization problem, or the existing works considering customer satisfaction do not give a proper formalized definition for it. Hence, we first refer to the definition of customer satisfaction in economics and develop a formula for measuring customer satisfaction in cloud computing. And then, an analysis is given in detail on how the customer satisfaction affects the profit. Lastly, taking into consideration customer satisfaction, service-level agreement, renting price, energy consumption, and so forth, a profit maximization problem is formulated and solved to get the optimal configuration such that the profit is maximized.},
  keywords={},
  doi={10.1109/TSUSC.2017.2667706},
  ISSN={2377-3782},
  month={Jan},}

@ARTICLE{8412190,
  author={El Kassabi, Hadeel T. and Serhani, Mohamed Adel and Dssouli, Rachida and Benatallah, Boualem},
  journal={IEEE Access}, 
  title={A Multi-Dimensional Trust Model for Processing Big Data Over Competing Clouds}, 
  year={2018},
  volume={6},
  number={},
  pages={39989-40007},
  abstract={Cloud computing has emerged as a powerful paradigm for delivering data-intensive services over the Internet. Cloud computing has enabled the implementation and success of big data, a recent phenomenon handling huge data being generated from different sources. Competing clouds have made it challenging to select a cloud provider that guarantees quality of cloud service (QoCS). Also, cloud providers' claims of guaranteeing QoCS are exaggerated for marketing purposes; hence, they cannot often be trusted. Therefore, a comprehensive trust model is necessary to evaluate the QoCS prior to making any selection decision. In this paper, we propose a multi-dimensional trust model for big data workflow processing over different clouds. It evaluates the trustworthiness of cloud providers based on: the most up-to-date cloud resource capabilities, the reputation evidence measured by neighboring users, and a recorded personal history of experiences with the cloud provider. The ultimate goal is to ensure an efficient selection of trustworthiness cloud provider who eventually will guarantee high QoCS and fulfills key big data workflow requirements. Various experiments were conducted to validate our proposed model. The results show that our model captures the different components of trust, ensures high QoCS, and effectively adapts to the dynamic nature of the cloud.},
  keywords={},
  doi={10.1109/ACCESS.2018.2856623},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9148980,
  author={Ma, Kun and Bagula, Antoine and Ajayi, Olasupo and Nyirenda, Clement},
  booktitle={ICC 2020 - 2020 IEEE International Conference on Communications (ICC)}, 
  title={Aiming at QoS: A Modified DE Algorithm for Task Allocation in Cloud Computing}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={The Cloud computing system is characterized by large scale servers being utilized by an even larger number of users. It is a system where there is the need to frequently and efficiently schedule and manage different application tasks, with varied service requirements. One of the challenges of Cloud computing is managing the quality of service (QoS) rendered to users, specifically scheduling tasks between users and Cloud resources in a timely manner. Cloud users usually have widely diverse QoS requirements and meeting these simultaneously is also a challenge. In this paper, in order to improve on Cloud resource allocation and specifically to tailor it towards meeting varied QoS requirements of users, we proposed a new algorithm which combines Differential Evolution with the Shapley Value economic mode. This combination allows us measure the contribution of each virtual machine (VM), so as to improve the probability of obtaining a better tasks-to-resource allocation thereby improving user satisfaction. From results of conducted experiments, when compared with the traditional DE (Differential Evolution) algorithm and the conventional task-VM binding policy in CloudSim, both for allocations where special QoS requirements are required and in instances of multiple QoS requirements; the modified Shapley value based DE algorithm (SVBDA) shows significant improvement.},
  keywords={},
  doi={10.1109/ICC40277.2020.9148980},
  ISSN={1938-1883},
  month={June},}

@INPROCEEDINGS{7904283,
  author={Bruschi, Gustavo C. and Spolon, Roberta and Pauro, Leandro L. and Lobato, Renata S. and Manacero, Aleardo and Cavenaghi, Marcos A.},
  booktitle={2016 15th International Symposium on Parallel and Distributed Computing (ISPDC)}, 
  title={StackAct: Performance Evaluation in an IaaS Cloud Multilayer}, 
  year={2016},
  volume={},
  number={},
  pages={149-156},
  abstract={Cloud Computing has become synonymous of quality, efficiency, and return of investment in Information Technology, creating new challenges for processing and data integrations. This paper presents the StackAct, a mechanism that allows performing monitoring and obtaining data on the consumption of computing resources of a solution in three layers using orchestrator IaaS Apache CloudStack, XenServer hypervisor and data storage on the NAS OpenFiler system. Performance tests were conducted using three different instances profile in a private cloud computing, allowing measuring CPU consumption, I/O disk and memory in three layers with different service offerings. The tests resulted in a comparison between the layers, it is possible to note the high consumption of disk in the data storage layer, in particular I/O data recording and the high memory consumption on the hypervisor layer, which is justified by the hypervisor itself to allocation of VMs being created and used in the process.},
  keywords={},
  doi={10.1109/ISPDC.2016.27},
  ISSN={},
  month={July},}

@INPROCEEDINGS{9463991,
  author={Toka, Laszlo and Dobreff, Gergely and Haja, David and Szalay, Mark},
  booktitle={2021 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
  title={Predicting cloud-native application failures based on monitoring data of cloud infrastructure}, 
  year={2021},
  volume={},
  number={},
  pages={842-847},
  abstract={The quality of service provided by cloud-deployed online applications is often affected by faults in the underlying cloud platform and infrastructure. In order to discover the cause and effect at application failures, a cloud monitoring system must be in place. The sheer amount of the produced monitoring data calls for smart and automatic handling in order to find the patterns that can be used for fault management. In this paper we present an open source, cloud-native, lightweight cloud monitoring system, and a data analytics pipeline that efficiently processes the gathered data and is able to discover useful inference between infrastructure-, and application-level metrics. We apply time series clustering steps within the pipeline to compress the collected data for fast and lightweight data mining. We show the capabilities of our proposed system in a reactive and a proactive use case. The results prove that the proposed system brings precious insights for root-cause analysis and proactive fault management frameworks of cloud applications.},
  keywords={},
  doi={},
  ISSN={1573-0077},
  month={May},}

@INPROCEEDINGS{7565173,
  author={Mushtaq, M. Sajid and Fowler, Scott and Augustin, Brice and Mellouk, Abdelhamid},
  booktitle={2016 IEEE Wireless Communications and Networking Conference}, 
  title={QoE in 5G cloud networks using multimedia services}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={The 4G standard Long Term Evolution-Advanced (LTE-A) has been deployed in many countries. Now, technology is evolving towards the 5G standard since it is expecting to start its service in 2020. The 5G cellular networks will mainly contain in cloud computing and primarily Quality of Service (QoS) parameters (e.g. delay, loss rate, etc.) influence the cloud network performance. The impact of user perceived Quality of Experience (QoE) using multimedia services, and application significantly relies on the QoS parameters. The key challenge of 5G technology is to reduce the delay less than one millisecond. In this paper, we have described a method that minimizes the overall network delay for multimedia services; which are constant bit rate (VoIP) and variable bit rate (video) traffic model. We also proposed a method that measures the user's QoE for video streaming traffic using the network QoS parameters, i.e. delay and packet loss rate. The performance of proposed QoE method is compared with QoV method, and our proposed QoE method performs best by carefully handle the impact of QoS parameters. The results show that our described method successfully reduces the overall network delays, which result to maximize the user's QoE.},
  keywords={},
  doi={10.1109/WCNC.2016.7565173},
  ISSN={1558-2612},
  month={April},}

@ARTICLE{7445250,
  author={Shi, Lei and Shi, Yi and Wei, Xing and Ding, Xu and Wei, Zhenchun},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Cost Minimization Algorithms for Data Center Management}, 
  year={2017},
  volume={28},
  number={1},
  pages={60-71},
  abstract={Due to the increasing usage of cloud computing applications, it is important to minimize energy cost consumed by a data center, and simultaneously, to improve quality of service via data center management. One promising approach is to switch some servers in a data center to the idle mode for saving energy while to keep a suitable number of servers in the active mode for providing timely service. In this paper, we design both online and offline algorithms for this problem. For the offline algorithm, we formulate data center management as a cost minimization problem by considering energy cost, delay cost (to measure service quality), and switching cost (to change servers's active/idle mode). Then, we analyze certain properties of an optimal solution which lead to a dynamic programming based algorithm. Moreover, by revising the solution procedure, we successfully eliminate the recursive procedure and achieve an optimal offline algorithm with a polynomial complexity. For the online algorithm, We design it by considering the worst case scenario for future workload. In simulation, we show this online algorithm can always provide near-optimal solutions.},
  keywords={},
  doi={10.1109/TPDS.2016.2549016},
  ISSN={1558-2183},
  month={Jan},}

@ARTICLE{9144208,
  author={Vatalaro, Francesco and Ciccarella, Gianfranco},
  journal={IEEE Access}, 
  title={A Network Paradigm for Very High Capacity Mobile and Fixed Telecommunications Ecosystem Sustainable Evolution}, 
  year={2020},
  volume={8},
  number={},
  pages={135075-135090},
  abstract={The main objective for Very High Capacity (VHC) fixed and mobile networks is improving end-user Quality of Experience (QoE), i.e., meeting the Key Performance Indicators (KPIs) - throughput, download time, round trip time, and video delay - required by the applications. KPIs depend on the end-to-end connection between the server and the end-user device. Not only Telco operators must provide the quality needed for the different applications, but also they must address economic sustainability objectives for VHC networks. Today, both goals are often not met, mainly due to the push to increase the access networks bitrate without considering the end-to-end applications KPIs. This paper's main contribution deals with the definition of a VHC network deployment framework able to address performance and cost issues. We show that three are the interventions on which it is necessary to focus: i) the reduction of bit-rate through video compression, ii) the reduction of packet loss rate through artificial intelligence algorithms for access lines stabilization, and iii) the reduction of latency (i.e., the round-trip time) with edge-cloud computing and content delivery platforms, including transparent caching. The concerted and properly phased action of these three measures can allow a Telco to get out of the Ultra Broad Band access network “trap”as defined in the paper. We propose to work on the end-to-end optimization of the bandwidth utilization ratio (i.e., the ratio between the throughput and the bit-rate that any application can use). It leads to better performance experienced by the end-user, enables new business models and revenue streams, and provides a sustainable cost for the Telco operators. To make such a perspective more precise, the case of MoVAR (Mobile Virtual and Augmented Reality), one of the most challenging future services, is finally described.},
  keywords={},
  doi={10.1109/ACCESS.2020.3010348},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8766409,
  author={Brunschwiler, T. and Weiss, J. and Paredes, S. and Sridhar, A. and Pluntke, U. and Chau, S. Mai and Gerke, S. and Barroso, J. and Loertscher, E. and Temiz, Y. and Ruch, P. and Michel, B. and Zafar, S. and van Kessel, T.},
  booktitle={2019 Global IoT Summit (GIoTS)}, 
  title={Internet of the Body - Wearable Monitoring and Coaching}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Wearables that acquire relevant vital and contextual parameters improve work safety as well as quality of life of elderly citizens or patients with chronic diseases. A scalable architecture connects wearables via a hub to the cloud and combines edge with cloud computing to provide optimal user interaction and allow analytics on multi-stream data. The functionality was expanded to enable demonstrations of physiological and psychological stress classification in firemen and mobile health interventions in patients with lung diseases. Following an initial table-top edge demonstrator a hemi-spherical display improves emotional contact to users. A first use case tested an integrated acquisition and inference system that was trained to differentiate physical and emotional stress. The system measured stress in firemen during training in a cage maze and in hot training locations and provided functions to acquire expert labels. A second use case focused on mobile-health intervention for patients suffering from Chronic-Obstructive-Pulmonary-Disease (COPD), to improve their quality-of-life. Patient-physician conversations are extended through a communication channel and a virtual assistant provides disease related information, reminders, and alerts.},
  keywords={},
  doi={10.1109/GIOTS.2019.8766409},
  ISSN={},
  month={June},}

@INPROCEEDINGS{8718133,
  author={Wong, Tong-Sheng and Chan, Gaik-Yee and Chua, Fang-Fang},
  booktitle={2019 International Conference on Information Networking (ICOIN)}, 
  title={Adaptive Preventive and Remedial Measures in Resolving Cloud Quality of Service Violation}, 
  year={2019},
  volume={},
  number={},
  pages={473-479},
  abstract={Cloud Computing acts as a paradigm to support on-demand computing services, from applications to storage, manage and processing capabilities. One of the major challenges in delivering and accessing cloud applications is the management of Quality of Service (QoS) and cloud service providers are mandated to adhere to Service Level Agreement (SLA) in providing quality cloud services to the users. The agreement matching is important for both parties to ensure satisfaction and expectation level. This proposed work aims to resolve cloud QoS violation with the implementation of adaptive preventive and remedial mechanisms. Preventive measure such as horizontal scaling is used to optimize the performance of a running cloud service in order to prevent the cloud service to downgrade to QoS violation condition. Remedial action on the other hand, is to provide fault tolerance using replication for faulty cloud service to recover from failure incidents or already violation condition. Experimental results have demonstrated the feasibility and effectiveness of applying horizontal scaling in preventing and replication in rectifying cloud QoS violations based on response time and throughput.},
  keywords={},
  doi={10.1109/ICOIN.2019.8718133},
  ISSN={1976-7684},
  month={Jan},}

@INPROCEEDINGS{7175787,
  author={Hong Shen and Jinglei Meng and Licheng Yu and Xuefeng Fang and Tianzhou Chen and Hui Yan and Honglun Hou},
  booktitle={2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems}, 
  title={A quantitative quality control method of big data in cancer patients using artificial neural network}, 
  year={2014},
  volume={},
  number={},
  pages={499-504},
  abstract={Nonstandard treatments for cancer patients are commonly seen in hospitals of developing countries like China. So it is crucial to standardize the treatments for cancer with technological means in order to supervise the process of treatments. Widespread of electronic health records (EHRs) has generated massive data sets which are far beyond the capability of traditional computing model. Although there are process and measures about quality control, but automatic computerized Quantitative Control (QC) and quantization methods are still lack. In this paper, we propose a quantitative quality control method of radiotherapy and chemotherapy based on artificial neural network to automatically analysis and rate the compliance with standard treatment process. The quantitative QC items are established and the artificial neural network is constructed accordingly. Then the selected cases are evaluated by experts for corresponding QC grades to train the artificial neural network. After that, the trained artificial neural network can be used to grade new cases for their QC score. To meet the high requirement of computation and accommodate massive data sets, we adopt our proposal in the cloud. With massive data distributed on computing nodes in the cloud, computing capability of nodes are dynamically allocated to homogenization and ANN computing, each node work both homogenization medical records and ANN computing according to system load balance resulting the quantitative QC process perform in high parallelism.},
  keywords={},
  doi={10.1109/CCIS.2014.7175787},
  ISSN={2376-595X},
  month={Nov},}

@ARTICLE{6473795,
  author={Bruneo, Dario},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={A Stochastic Model to Investigate Data Center Performance and QoS in IaaS Cloud Computing Systems}, 
  year={2014},
  volume={25},
  number={3},
  pages={560-569},
  abstract={Cloud data center management is a key problem due to the numerous and heterogeneous strategies that can be applied, ranging from the VM placement to the federation with other clouds. Performance evaluation of cloud computing infrastructures is required to predict and quantify the cost-benefit of a strategy portfolio and the corresponding quality of service (QoS) experienced by users. Such analyses are not feasible by simulation or on-the-field experimentation, due to the great number of parameters that have to be investigated. In this paper, we present an analytical model, based on stochastic reward nets (SRNs), that is both scalable to model systems composed of thousands of resources and flexible to represent different policies and cloud-specific strategies. Several performance metrics are defined and evaluated to analyze the behavior of a cloud data center: utilization, availability, waiting time, and responsiveness. A resiliency analysis is also provided to take into account load bursts. Finally, a general approach is presented that, starting from the concept of system capacity, can help system managers to opportunely set the data center parameters under different working conditions.},
  keywords={},
  doi={10.1109/TPDS.2013.67},
  ISSN={1558-2183},
  month={March},}

@ARTICLE{8306964,
  author={Neghabi, Ali Akbar and Jafari Navimipour, Nima and Hosseinzadeh, Mehdi and Rezaee, Ali},
  journal={IEEE Access}, 
  title={Load Balancing Mechanisms in the Software Defined Networks: A Systematic and Comprehensive Review of the Literature}, 
  year={2018},
  volume={6},
  number={},
  pages={14159-14178},
  abstract={With the expansion of the network and increasing their users, as well as emerging new technologies, such as cloud computing and big data, managing traditional networks is difficult. Therefore, it is necessary to change the traditional network architecture. Lately, to address this issue, a notion named software-defined network (SDN) has been proposed, which makes network management more conformable. Due to limited network resources and to meet the requirements of quality of service, one of the points that must be considered is load balancing issue that serves to distribute data traffic among multiple resources in order to maximize the efficiency and reliability of network resources. Load balancing is established based on the local information of the network in the conventional network. Hence, it is not very precise. However, SDN controllers have a global view of the network and can produce more optimized load balances. Although load balancing mechanisms are important in the SDN, to the best of our knowledge, there exists no precise and systematic review or survey on investigating these issues. Hence, this paper reviews the load balancing mechanisms which have been used in the SDN systematically based on two categories, deterministic and non-deterministic. Also, this paper represents benefits and some weakness regarded of the selected load balancing algorithms and investigates the metrics of their algorithms. In addition, the important challenges of these algorithms have been reviewed, so better load balancing techniques can be applied by the researchers in the future.},
  keywords={},
  doi={10.1109/ACCESS.2018.2805842},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8030623,
  author={Al-Dhuraibi, Yahya and Paraiso, Fawaz and Djarallah, Nabil and Merle, Philippe},
  booktitle={2017 IEEE 10th International Conference on Cloud Computing (CLOUD)}, 
  title={Autonomic Vertical Elasticity of Docker Containers with ELASTICDOCKER}, 
  year={2017},
  volume={},
  number={},
  pages={472-479},
  abstract={Elasticity is the key feature of cloud computing to scale computing resources according to application workloads timely. In the literature as well as in industrial products, much attention was given to the elasticity of virtual machines, but much less to the elasticity of containers. However, containers are the new trend for packaging and deploying microservices-based applications. Moreover, most of approaches focus on horizontal elasticity, fewer works address vertical elasticity. In this paper, we propose ELASTICDOCKER, the first system powering vertical elasticity of Docker containers autonomously. Based on the well-known IBM's autonomic computing MAPE-K principles, ELASTICDOCKER scales up and down both CPU and memory assigned to each container according to the application workload. As vertical elasticity is limited to the host machine capacity, ELASTICDOCKER does container live migration when there is no enough resources on the hosting machine. Our experiments show that ELASTICDOCKER helps to reduce expenses for container customers, make better resource utilization for container providers, and improve Quality of Experience for application end-users. In addition, based on the observed migration performance metrics, the experiments reveal a high efficient live migration technique. As compared to horizontal elasticity, ELASTICDOCKER outperforms Kubernetes elasticity by 37.63%.},
  keywords={},
  doi={10.1109/CLOUD.2017.67},
  ISSN={2159-6190},
  month={June},}

@ARTICLE{7045510,
  author={Xia, YunNi and Zhou, MengChu and Luo, Xin and Pang, ShanChen and Zhu, QingSheng},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Stochastic Modeling and Performance Analysis of Migration-Enabled and Error-Prone Clouds}, 
  year={2015},
  volume={11},
  number={2},
  pages={495-504},
  abstract={Cloud computing is a promising paradigm capable of rationalizing the use of computational resources by means of outsourcing and virtualization. Virtualization allows to instantiate virtual machines (VMs) on top of fewer physical systems managed by a VM manager. Performance evaluation of clouds is required to evaluate and quantify the cost-benefit of a strategy portfolio and the quality of service (QoS) experienced by end-users. Such evaluation is not feasible by means of simulation or on-the-field measurement, due to the great scale of parameter spaces that have to be traversed. In this study, we present a stochastic-queuing-network-based approach to performance analysis of migration-enabled clouds in error-prone environment. Several performance metrics are defined and evaluated: utilization, expected task completion time, and task rejection rate under different load conditions and error intensities. To validate the proposed approach, we obtain experimental performance data through a real-world cloud and conduct a confidence-interval analysis. The analysis results suggest the perfect coverage of theoretical performance results by corresponding experimental confidence intervals.},
  keywords={},
  doi={10.1109/TII.2015.2405792},
  ISSN={1941-0050},
  month={April},}

@ARTICLE{9046283,
  author={Zhu, Zongwei and Han, Guangjie and Jia, Gangyong and Shu, Lei},
  journal={IEEE Internet of Things Journal}, 
  title={Modified DenseNet for Automatic Fabric Defect Detection With Edge Computing for Minimizing Latency}, 
  year={2020},
  volume={7},
  number={10},
  pages={9623-9636},
  abstract={As an essential step in quality control, fabric defect detection plays an important role in the textile manufacturing industry. The traditional manual detection method is inaccurate and incurs a high cost; as a result, it is gradually being replaced by deep learning algorithms based on cloud computing. However, a high data transmission latency between end devices and the cloud has a significant impact on textile production efficiency. In contrast, edge computing, which provides services near end devices by deploying network, computing and storage facilities at the edge of the Internet, can effectively solve the above-mentioned problem. In this article, we propose a deep-learning-based fabric defect detection method for edge computing scenarios. First, this article modifies the structure of DenseNet to better suit a resource-constrained edge computing scenario. To better assess the proposed model, an optimized cross-entropy loss function is also formulated. Afterward, six feasible expansion schemes are utilized to enhance the data set according to the characteristics of various defects in fabric samples. To balance the distribution of samples, proportions of various defect types are used to determine the number of enhancements. Finally, a fabric defect detection system is established to test the performance of the optimized model used on edge devices in a real-world textile industry scenario. Experimental results demonstrate that compared with the conventional convolutional neural network (CNN), the proposed optimized model attains an average improvement of 18% in the area under the curve (AUC) metric for 11 defects. Data transmission is reduced by approximately 50% and latency is reduced by 32% in the Cambricon 1H8 platform compared with a cloud platform.},
  keywords={},
  doi={10.1109/JIOT.2020.2983050},
  ISSN={2327-4662},
  month={Oct},}

@ARTICLE{8976136,
  author={Farid, Mazen and Latip, Rohaya and Hussin, Masnida and Abdul Hamid, Nor Asilah Wati},
  journal={IEEE Access}, 
  title={Scheduling Scientific Workflow Using Multi-Objective Algorithm With Fuzzy Resource Utilization in Multi-Cloud Environment}, 
  year={2020},
  volume={8},
  number={},
  pages={24309-24322},
  abstract={The provision of resources and services for scientific workflow applications using a multi-cloud architecture and a pay-per-use rule has recently gained popularity within the cloud computing research domain. This is because workflow applications are computation intensive. Most of the existing studies on workflow scheduling in the cloud mainly focus on finding an ideal makespan or cost. Nevertheless, there are other important quality of service metrics that are of critical concern in workflow scheduling such as reliability and resource utilization. In this respect, this paper proposes a new multi-objective scheduling algorithm with Fuzzy resource utilization (FR-MOS) for scheduling scientific workflow based on particle swarm optimization (PSO) method. The algorithm minimizes cost and makespan while considering reliability constraint. The coding scheme jointly considers task execution location and data transportation order. Simulation experiments reveal that FR-MOS outperforms the basic MOS over the PSO algorithm.},
  keywords={},
  doi={10.1109/ACCESS.2020.2970475},
  ISSN={2169-3536},
  month={},}

@ARTICLE{7010388,
  author={Nesmachnow, Sergio and Iturriaga, Santiago and Dorronsoro, Bernabe},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Efficient Heuristics for Profit Optimization of Virtual Cloud Brokers}, 
  year={2015},
  volume={10},
  number={1},
  pages={33-43},
  abstract={This article introduces a new kind of broker for cloud computing, whose business relies on outsourcing virtual machines (VMs) to its customers. More specifically, the broker owns a number of reserved instances of different VMs from several cloud providers and offers them to its customers in an on-demand basis, at cheaper prices than those of the cloud providers. The essence of the business resides in the large difference in price between on-demand and reserved VMs. We define the Virtual Machine Planning Problem, an optimization problem to maximize the profit of the broker. We also propose a number of efficient smart heuristics (seven two-phase list scheduling heuristics and a reordering local search) to allocate a set of VM requests from customers into the available pre-booked ones, that maximize the broker earnings. We perform experimental evaluation to analyze the profit and quality of service metrics for the resulting planning, including a set of 400 problem instances that account for realistic workloads and scenarios using real data from cloud providers.},
  keywords={},
  doi={10.1109/MCI.2014.2369893},
  ISSN={1556-6048},
  month={Feb},}

@ARTICLE{9140398,
  author={Fantacci, Romano and Picano, Benedetta},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Performance Analysis of a Delay Constrained Data Offloading Scheme in an Integrated Cloud-Fog-Edge Computing System}, 
  year={2020},
  volume={69},
  number={10},
  pages={12004-12014},
  abstract={The recent growth in intensive services and applications demand has triggered the functional integration of cloud computing with edge computing capabilities. One of the main goals is to allow a fast processing to tasks with strict real time constraints in order to lower the task dropping probability due to expiration of the associated deadlines. This paper deals with the performance evaluation and optimization of a three layers cloud-fog-edge computing infrastructure by resorting to the use of queueing theory results. In particular, a Markov queueing system model with reneging is proposed for the cloud subsystem, in order to consider the premature computation requests departure due to their deadline expiration. Furthermore, a computational resources allocation method is proposed with the aim at maximizing the social welfare metric, constrained to specific quality of service requirements. Finally, the proposed queueing theory analysis as well as of the computational resources allocation approach is validated by comparing the obtained analytical predictions with simulation results.},
  keywords={},
  doi={10.1109/TVT.2020.3008926},
  ISSN={1939-9359},
  month={Oct},}

@ARTICLE{7042798,
  author={Ayoubi, Sara and Assi, Chadi and Shaban, Khaled and Narayanan, Lata},
  journal={IEEE Transactions on Communications}, 
  title={MINTED: Multicast VIrtual NeTwork Embedding in Cloud Data Centers With Delay Constraints}, 
  year={2015},
  volume={63},
  number={4},
  pages={1291-1305},
  abstract={Network virtualization is regarded as the pillar of cloud computing, enabling the multi-tenancy concept where multiple Virtual Networks (VNs) can cohabit the same substrate network. With network virtualization, the problem of allocating resources to the various tenants, commonly known as the Virtual Network Embedding problem, emerges as a challenge. Its NP-Hard nature has drawn a lot of attention from the research community, many of which however overlooked the type of communication that a given VN may exhibit, assuming that they all exhibit a one-to-one (unicast) communication only. In this paper, we motivate the importance of characterizing the mode of communication in VN requests, and we focus our attention on the problem of embedding VNs with a one-to-many (multicast) communication mode. Throughout this paper, we highlight the unique properties of multicast VNs and its distinct Quality of Service (QoS) requirements, most notably the end-delay and delay-variation constraints for delay-sensitive multicast services. Further, we showcase the limitations of handling a multicast VN as unicast. To this extent, we formally define the VNE problem for Multicast VNs (MVNs) and prove its NP-Hard nature. We propose two novel approach to solve the Multicast VNE (MVNE) problem with end-delay and delay variation constraints: A 3-Step MVNE technique, and a Tabu-Search algorithm. We motivate the intuition behind our proposed embedding techniques, and provide a competitive analysis of our suggested approaches over multiple metrics and against other embedding heuristics.},
  keywords={},
  doi={10.1109/TCOMM.2015.2404440},
  ISSN={1558-0857},
  month={April},}

@INPROCEEDINGS{6903296,
  author={Cao, Fei and Zhu, Michelle M. and Wu, Chase Q.},
  booktitle={2014 IEEE World Congress on Services}, 
  title={Energy-Efficient Resource Management for Scientific Workflows in Clouds}, 
  year={2014},
  volume={},
  number={},
  pages={402-409},
  abstract={The elastic resource provision, non-interfering resource sharing and flexible customized configuration provided by the Cloud infrastructure has shed light on efficient execution of many scientific applications. Due to the increasing deployment of data centers and computer servers around the globe escalated by the higher electricity price, the energy cost on running the computing, communication and cooling together with the amount of CO2 emissions have skyrocketed. In order to maintain sustainable Cloud computing facing with ever-increasing problem complexity and big data size in the next decades, we design and develop energy-aware scientific workflow scheduling algorithm to minimize energy consumption and CO2 emission while still satisfying certain Quality of Service (QoS) such as response time specified in Service Level Agreement (SLA). We also apply Dynamic Voltage and Frequency Scaling (DVFS) and DNS scheme to further reduce energy consumption within acceptable performance bounds. Our multiple-step resource provision and allocation algorithm achieves the response time requirement in the step of forwarding task scheduling and minimizes the VM overhead for reduced energy consumption and higher resource utilization rate in the backward task scheduling step. The effectiveness of our algorithm is evaluated under various performance metrics and experimental scenarios using software adapted from open source CloudSim simulator.},
  keywords={},
  doi={10.1109/SERVICES.2014.76},
  ISSN={2378-3818},
  month={June},}

@ARTICLE{8641327,
  author={Zhou, Xijia and Li, Kenli and Liu, Chubo and Li, Keqin},
  journal={IEEE Access}, 
  title={An Experience-Based Scheme for Energy-SLA Balance in Cloud Data Centers}, 
  year={2019},
  volume={7},
  number={},
  pages={23500-23513},
  abstract={The proliferation of cloud computing has resulted in the establishment of large-scale data centers containing thousands of computing nodes and consuming enormous amounts of electrical energy. However, the low-cost and high-efficiency slogans are getting louder and louder, and the IT industry is also striving for this pursuit. Therefore, it is vital to minimizing the energy consumption for cloud providers while ensuring the quality of service for cloud users. In this paper, we propose several heuristic strategies to optimize these two metrics based on a two-level management model under a heterogeneous cloud environment. First, to detect whether a physical node is continuously overloaded, we propose an empirical forecast algorithm, which predicts the future state of the host by statistically analyzing the historical utilization data of the host. Second, we propose a weighted priority virtual machine (VM) selection algorithm. For each VM on the overloaded host, we weight several utilization factors and calculate its migration priority. Then, we simulate the proposed approach and compare it with the existing overloaded hosts detection algorithms with different VM selection policies under different workloads.},
  keywords={},
  doi={10.1109/ACCESS.2019.2899101},
  ISSN={2169-3536},
  month={},}

@ARTICLE{9178309,
  author={He, Xingqiu and Wang, Sheng},
  journal={IEEE Internet of Things Journal}, 
  title={Peer Offloading in Mobile-Edge Computing With Worst Case Response Time Guarantees}, 
  year={2021},
  volume={8},
  number={4},
  pages={2722-2735},
  abstract={Mobile-edge computing (MEC) is a new paradigm that provides cloud computing services at the edge of networks. To achieve better performance with limited computing resources, peer offloading between cooperative edge servers (e.g., MEC-enabled base stations) has been proposed as an effective technique to handle bursty and spatially imbalanced arrival of computation tasks. While various performance metrics of peer offloading policies have been considered in the literature, the worst case response time, a common quality of service (QoS) requirement in real-time applications, yet receives much less attention. To fill the gap, we formulate the peer offloading problem based on a stochastic arrival model and propose two online algorithms for cases with and without prior knowledge of task arrival rate. Our goal is to maximize the utility function of time-average throughput under constraints of energy consumption and worst case response time. Both theoretical analysis and numerical results show that our algorithms are able to produce close to optimal performance.},
  keywords={},
  doi={10.1109/JIOT.2020.3019492},
  ISSN={2327-4662},
  month={Feb},}

@INPROCEEDINGS{8514452,
  author={Alqahtani, Awatif and Li, Yinhao and Patel, Pankesh and Solaiman, Ellis and Ranjan, Rajiv},
  booktitle={2018 International Conference on High Performance Computing & Simulation (HPCS)}, 
  title={End-to-End Service Level Agreement Specification for IoT Applications}, 
  year={2018},
  volume={},
  number={},
  pages={926-935},
  abstract={The Internet of Things (IoT) promises to help solve a wide range of issues that relate to our wellbeing within applica¬tion domains that include smart cities, healthcare monitoring, and environmental monitoring. IoT is bringing new wireless sensor use cases by taking advantage of the computing power and flexibility provided by Edge and Cloud Computing. However, the software and hardware resources used within such applications must perform correctly and optimally. Especially in applications where a failure of resources can be critical. Service Level Agreements (SLA) where the performance requirements of such applications are defined, need to be specified in a standard way that reflects the end-to-end nature of IoT application domains, accounting for the Quality of Service (QoS) metrics within every layer including the Edge, Network Gateways, and Cloud. In this paper, we propose a conceptual model that captures the key entities of an SLA and their relationships, as a prior step for end-to-end SLA specification and composition. Service level objective (SLO) terms are also considered to express the QoS constraints. Moreover, we propose a new SLA grammar which considers workflow activities and the multi-layered nature of IoT applications. Accordingly, we develop a tool for SLA specification and composition that can be used as a template to generate SLAs in a machine-readable format. We demonstrate the effectiveness of the proposed specification language through a literature survey that includes an SLA language comparison analysis, and via reflecting the user satisfaction results of a usability study.},
  keywords={},
  doi={10.1109/HPCS.2018.00147},
  ISSN={},
  month={July},}

@INPROCEEDINGS{8969716,
  author={Maliszewski, Anderson M. and Vogel, Adriano and Griebler, Dalvan and Roloff, Eduardo and Fernandes, Luiz G. and Philippe O. A., Navaux},
  booktitle={2019 IEEE Symposium on Computers and Communications (ISCC)}, 
  title={Minimizing Communication Overheads in Container-based Clouds for HPC Applications}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Although the industry has embraced the cloud computing model, there are still significant challenges to be addressed concerning the quality of cloud services. Network-intensive applications may not scale in the cloud due to the sharing of the network infrastructure. In the literature, performance evaluation studies are showing that the network tends to limit the scalability and performance of HPC applications. Therefore, we proposed the aggregation of Network Interface Cards (NICs) in a ready-to-use integration with the OpenNebula cloud manager using Linux containers. We perform a set of experiments using a network microbenchmark to get specific network performance metrics and NAS parallel benchmarks to analyze the performance impact on HPC applications. Our results highlight that the implementation of NIC aggregation improves network performance in terms of throughput and latency. Moreover, HPC applications have different patterns of behavior when using our approach, which depends on communication and the amount of data transferring. While network-intensive applications increased the performance up to 38%, other applications with aggregated NICs maintained the same performance or presented slightly worse performance.},
  keywords={},
  doi={10.1109/ISCC47284.2019.8969716},
  ISSN={2642-7389},
  month={June},}

@INPROCEEDINGS{7214120,
  author={Bruneo, Dario and Longo, Francesco and Ghosh, Rahul and Scarpa, Marco and Puliafito, Antonio and Trivedi, Kishor S.},
  booktitle={2015 IEEE 8th International Conference on Cloud Computing}, 
  title={Analytical Modeling of Reactive Autonomic Management Techniques in IaaS Clouds}, 
  year={2015},
  volume={},
  number={},
  pages={797-804},
  abstract={Cloud computing infrastructures provide services to a wide number of users whose behavior can deeply change at the occurrence of particular events. To correctly handle such situations a cloud infrastructure have to be reconfigured in a way that does not cause degradation in the overall performance. Otherwise, the quality of service specified in the service level agreement could be violated. To prevent such situations, the infrastructure could be organized as an autonomic system where self-adaptation and self-configuration techniques are implemented. Appropriate design choices become important in order not to fail in this goal. We propose a technique, based on a Petri net model and a specific analytical analysis approach, to represent Infrastructure-as-a-Service (IaaS) systems in the case in which the load conditions can suddenly change and reactive autonomic management techniques are applied to mitigate the consequences of the change. The model we propose is able to appropriately evaluate performance metrics in such critical situations making it suitable as a design tool for IaaS cloud systems.},
  keywords={},
  doi={10.1109/CLOUD.2015.110},
  ISSN={2159-6190},
  month={June},}

@INPROCEEDINGS{6969013,
  author={Tchernykh, Andrei and Lozano, Luz and Schwiegelshohn, Uwe and Bouvry, Pascal and Pecero, Johnatan E. and Nesmachnow, Sergio},
  booktitle={2014 IEEE 3rd International Conference on Cloud Networking (CloudNet)}, 
  title={Bi-objective online scheduling with quality of service for IaaS clouds}, 
  year={2014},
  volume={},
  number={},
  pages={307-312},
  abstract={This paper focuses on the bi-objective experimental analysis of online scheduling in the Infrastructure as a Service model of Cloud computing. In this model, customer have the choice between different service levels. Each service level is associated with a price per unit of job execution time and a slack factor that determines the maximal time span to deliver the requested amount of computing resources. It is responsibility of the system and its scheduling algorithm to guarantee the corresponding quality of service for all accepted jobs. We do not consider any optimistic scheduling approach, that is, a job cannot be accepted if its service guarantee will not be observed assuming that all accepted jobs receive the requested resources. We analyze several scheduling algorithms with different cloud configurations and workloads and use the maximization of the provider income and minimization of the total power consumption of a schedule as additional objectives. Therefore, we cannot expect finding a unique solution to a given problem but a set of nondominated solutions also known as Pareto optima. Then we assess the performance of different scheduling algorithms by using a set coverage metric to compare them in terms of Pareto dominance. Based on the presented case study, we claim that a simple algorithm can provide the best energy and income trade-offs. This scheduling algorithm performs well in different scenarios with a variety of workloads and cloud configurations.},
  keywords={},
  doi={10.1109/CloudNet.2014.6969013},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9092335,
  author={Meyer, Vinícius and Kirchoff, Dionatrã F. and da Silva, Matheus L. and De Rose, César A.F.},
  booktitle={2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)}, 
  title={An Interference-Aware Application Classifier Based on Machine Learning to Improve Scheduling in Clouds}, 
  year={2020},
  volume={},
  number={},
  pages={80-87},
  abstract={To maximize resource utilization and system throughput in cloud platforms, hardware resources are often shared across multiple virtualized services or applications. In such a consolidated scenario, performance of applications running concurrently in the same physical host can be negatively affected due to interference caused by resource contention. This should be taken into account for efficient scheduling of such applications and performance prediction at user level. Nevertheless, resource scheduling in cloud computing is usually based solely on resource capacity, implemented by heuristics such as bin-packing. Our previous work has introduced an interference-aware scheduling model for web-applications considering their resource utilization profile, and to classify applications we applied fixed interference intervals based on common utilization patters. Although this resulted in placements with better overall results, we observed that some applications with more dynamic workload patterns were wrongly classified with intervals. In this paper, we propose an alternative to the use of intervals and present an interference-aware application classifier for cloud-based applications that deals better with dynamic workloads. Our classifier defines automatically interference levels ranges combining two well-known machine learning techniques: Support Vector Machines and K-Means. Preliminary experiments evaluated the applied machine learning techniques in three quality metrics: Accuracy, F1-Score and Rand Index, observing rates over 80%. The proposed solution creates a workload-aware fine-grained classification that was compared with previous work over different workload scenarios. The results demonstrate that our classification approach improves the placement efficiency by 23% on average.},
  keywords={},
  doi={10.1109/PDP50117.2020.00019},
  ISSN={2377-5750},
  month={March},}

@INPROCEEDINGS{8458040,
  author={Ma, Kun and Bagula, Antoine and Mauwa, Hope and Celesti, Antonio},
  booktitle={2018 IEEE 6th International Conference on Future Internet of Things and Cloud (FiCloud)}, 
  title={Modelling Cloud Federation: A Fair Profit Distribution Strategy Using the Shapley Value}, 
  year={2018},
  volume={},
  number={},
  pages={393-398},
  abstract={Cloud computing provides software (Software as a Service), platform (Platform as a Service) and infrastructure (Infrastructure as a Service) services to its users by integrating IT resources into a large-scale and scalable resource pool through the virtualisation technology. However, the single cloud resource provider model currently implemented by many providers may fail short to meet the dynamic nature of cloud users' requirements. Cloud federation can mitigate this issue by optimising cloud resource allocation through sharing and re-usability of available resources. This paper revisit the problem of cloud engineering by tackling the key issue of the fair distribution of profit between cloud resource providers, which, to the best of our knowledge, has only been scarcely addressed by the research and practitioners' community. We propose a method that enables the cloud federation to map the contribution of resources of the participants to the federations into a quality of service metric used to achieve a cloud federation. Building upon a federation game implementation, we reveal the possibilities and benefits of different federation compositions using the Shapley value of each resource provider as a way of implementing a fair profit sharing strategy. Using extensions of the CloudSim package, we present simulation results that demonstrate the fairness of our proposed method and strategy.},
  keywords={},
  doi={10.1109/FiCloud.2018.00063},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9207151,
  author={Wang, Binyang and Li, Huifang and Lin, Zhiwei and Xia, Yuanqing},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Temporal Fusion Pointer network-based Reinforcement Learning algorithm for Multi-Objective Workflow Scheduling in the cloud}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Cloud computing is emerging as a deployment promising environment for hosting exponentially increasing scientific and social media applications, but how to manage and execute these applications efficiently depends mainly on workflow scheduling. However, scheduling workflows in the cloud is an NP-hard problem and its existing solutions have certain limitations when applied to real-world scenarios. In this paper, a Temporal Fusion Pointer network-based Reinforcement Learning algorithm for multi-objective workflow scheduling (TFP-RL) is proposed. Through adopting reinforcement learning, our algorithm can discover its heuristics over time by continuous learning according to the rewards resulting from good scheduling solutions. To make more comprehensive scheduling decisions as the influence of historical actions, a novel temporal fusion pointer network (TFP) is designed for the reinforcement learning agent, which can improve the quality of our resulting solutions and the ability of our algorithm in dealing with versatile workflow applications. To decrease convergence time, we train the proposed TFP-RL model independently by the Asynchronous Advantage Actor-Critic method and use its resulting model for scheduling workflows. Finally, under a multi-agent reinforcement learning framework, a Pareto dominance-oriented criterion for reasonable action selection is established for a multi-objective optimization scenario. We first train our TFP-RL model by taking randomly generated workflows as inputs to validate its effectiveness in scheduling, then compare our trained model with other existing scheduling approaches through practical compute- and data-intensive workflows. Experimental results demonstrate that our proposed algorithm outperforms the benchmarking ones in terms of different metrics.},
  keywords={},
  doi={10.1109/IJCNN48605.2020.9207151},
  ISSN={2161-4407},
  month={July},}

@INPROCEEDINGS{9149347,
  author={Alam, A B M Bodrul and Halabi, Talal and Haque, Anwar and Zulkernine, Mohammad},
  booktitle={ICC 2020 - 2020 IEEE International Conference on Communications (ICC)}, 
  title={Multi-Objective Interdependent VM Placement Model based on Cloud Reliability Evaluation}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={Virtual Machine (VM) placement is considered as one of the crucial problems in Cloud Computing environments. From the perspective of Cloud Service Providers (CSPs), finding the optimal VM placement strategy is often related to optimal resource utilization, revenue maximization, and energy efficiency. However, to ensure the continuity of customer services, CSPs should also consider the reliability of deployed applications when placing VMs on their infrastructures. Existing research in this area either do not focus on the Cloud reliability evaluation aspect or do not account for the trade-off between reliability and performance in the VM placement process. In this paper, we propose a multi-objective placement model for interdependent VMs in the Cloud that considers both reliability and workload. Reliability in our model is quantitatively evaluated through a set of metrics that we propose. The model involves an Integer Linear Programming problem that aims at maximizing the reliability of the Cloud while minimizing network delay. A multi-objective genetic algorithm is then used to solve the problem heuristically. The proposed model introduces a level of flexibility and its parameters could be adjusted depending on the requirements of the infrastructure and services. The results show that our model achieves high Cloud reliability and allows to effectively control the trade-off between reliability and Quality of Service.},
  keywords={},
  doi={10.1109/ICC40277.2020.9149347},
  ISSN={1938-1883},
  month={June},}

@INPROCEEDINGS{9407931,
  author={Hussain, Razin Farhan and Pakravan, Alireza and Salehi, Mohsen Amini},
  booktitle={2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)}, 
  title={Analyzing the Performance of Smart Industry 4.0 Applications on Cloud Computing Systems}, 
  year={2020},
  volume={},
  number={},
  pages={11-18},
  abstract={Cloud-based Deep Neural Network (DNN) applications that make latency-sensitive inference are becoming an indispensable part of Industry 4.0. Due to the multi-tenancy and resource heterogeneity, both inherent to the cloud computing environments, the inference time of DNN-based applications are stochastic. Such stochasticity, if not captured, can potentially lead to low Quality of Service (QoS) or even a disaster in critical sectors, such as Oil and Gas industry. To make Industry 4.0 robust, solution architects and researchers need to understand the behavior of DNN-based applications and capture the stochasticity exists in their inference times. Accordingly, in this study, we provide a descriptive analysis of the inference time from two perspectives. First, we perform an application-centric analysis and statistically model the execution time of four categorically different DNN applications on both Amazon and Chameleon clouds. Second, we take a resource-centric approach and analyze a rate-based metric in form of Million Instruction Per Second (MIPS) for heterogeneous machines in the cloud. This non-parametric modeling, achieved via Jackknife and Bootstrap re-sampling methods, provides the confidence interval of MIPS for heterogeneous cloud machines. The findings of this research can be helpful for researchers and cloud solution architects to develop solutions that are robust against the stochastic nature of the inference time of DNN applications in the cloud and can offer a higher QoS to their users and avoid unintended outcomes.},
  keywords={},
  doi={10.1109/HPCC-SmartCity-DSS50907.2020.00003},
  ISSN={},
  month={Dec},}

@ARTICLE{6671599,
  author={Shen, Haiying and Lin, Yuhua and Li, Ting},
  journal={IEEE Transactions on Computers}, 
  title={Combining Efficiency, Fidelity, and Flexibility in Resource Information Services}, 
  year={2015},
  volume={64},
  number={2},
  pages={353-367},
  abstract={A large-scale resource sharing system (e.g., collaborative cloud computing and grid computing) creates a virtual supercomputer by providing an infrastructure for sharing tremendous amounts of resources (e.g., computing, storage, and data) distributed over the Internet. A resource information service, which collects resource data and provides resource search functionality for locating desired resources, is a crucial component of the resource sharing system. In addition to resource discovery speed and cost (i.e., efficiency), the ability to accurately locate all satisfying resources (i.e., fidelity) is also an important metric for evaluating service quality. Previously, a number of resource information service systems have been proposed based on Distributed Hash Tables (DHTs) that offer scalable key-based lookup functions. However, these systems either achieve high fidelity at low efficiency, or high efficiency at low fidelity. Moreover, some systems have limited flexibility by only providing exact-matching services or by describing a resource using a pre-defined list of attributes. This paper presents a resource information service that offers high efficiency and fidelity without restricting resource expressiveness, while also providing a similar-matching service. Extensive simulation and PlanetLab experimental results show that the proposed service outperforms other services in terms of efficiency, fidelity, and flexibility; it dramatically reduces overhead and yields significant enhancements in efficiency and fidelity.},
  keywords={},
  doi={10.1109/TC.2013.222},
  ISSN={1557-9956},
  month={Feb},}

@INPROCEEDINGS{9659505,
  author={Klinaku, Floriment and Hakamian, Alireza and Becker, Steffen},
  booktitle={2021 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)}, 
  title={Architecture-based Evaluation of Scaling Policies for Cloud Applications}, 
  year={2021},
  volume={},
  number={},
  pages={151-157},
  abstract={The cloud computing model enables organizations to employ policies for the automated provisioning of computing resources. The impact on the quality, such as performance or cost, of such policies is often unknown for complex, large, and highly distributed cloud applications. Software architects lack a feasible approach to evaluate scaling policies for their cloud application quantitatively. While approaches exist in the literature, they are costly and require a high effort. We propose an approach that utilizes modeling and terminating simulations to evaluate alternative styles and configurations for cloud scaling policies. The approach aids the architect in understanding and explaining their dynamic behavior and the existing trade-offs. Third, we conduct simulation experiments on a representative case study model to show the approach&#x0027;s feasibility. We evaluate the performance, cost, efficiency, and complexity of three scaling policies of different styles (e.g., centralized vs. decentralized) on a model. Results show that the policies improve the performance for the selected scenario. However, no significant difference among them exists in terms of performance. Other metrics highlight the present trade-offs across policies. All in all, the case shows that the approach helps architects refine the style and find an appropriate policy for their context.},
  keywords={},
  doi={10.1109/ACSOS52086.2021.00035},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9563355,
  author={Li, Dian and Wang, Weidong and Kang, Yujian},
  booktitle={2021 IEEE International Conference on Electronic Technology, Communication and Information (ICETCI)}, 
  title={A Hierarchical Approach for QoS-Aware Edge Service Scheduling and Composition}, 
  year={2021},
  volume={},
  number={},
  pages={677-681},
  abstract={Edge computing is a new computing paradigm after cloud computing. Edge service scheduling and composition (ESC) has been well recognized as a convenient and flexible way of services sharing and integrating in industrial application fields. The ESC aims at selecting a set of existing edge service candidates with different Quality of Service (QoS) attributes (e.g., price), then compositing them to accomplish a complex task to meet users' QoS requirements, where each edge service may have multiple functionally equivalent, but different QoS metrics. A grand research challenge of the ESC based on QoS is to select proper service candidates to maximize QoS of the composited edge service and meanwhile meet the global QoS requirements. In this article, we focus on this challenge and propose a hierarchical solution for ESC. Specifically, we classify service candidates into specified grades based on their QoS attributes, and use hierarchical model to address service selection problem. Furthermore, in order to expand the flexibility of our approach, we design a near-optimal solution by decomposing the global end-to-end QoS constraints into local QoS constraints and adopting local service selecting and updating strategy based on the hierarchical model. The results of simulation-based experiments are conducted to show our approach is more efficient and valid compared to other existing approaches.},
  keywords={},
  doi={10.1109/ICETCI53161.2021.9563355},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8614209,
  author={Wangsom, Peerasak and Bouvry, Pascal and Lavangnananda, Kittichai},
  booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Extreme Solutions NSGA-III (E-NSGA-III) for Scientific Workflow Scheduling on Cloud}, 
  year={2018},
  volume={},
  number={},
  pages={1139-1146},
  abstract={The execution of scientific workflows on dynamic environments such as cloud computing has become multi-objective scheduling in order to satisfy user demands from several perspectives. Among these objectives, Cost and Makespan are probably the most common. This research also includes Data Movement as an additional objective as it has significant effect to network utilization and energy consumption in network equipment in cloud data center. This paper proposes a multi-objective scheduling, Extreme Nondominated Sorting Genetic Algorithm (E-NSGA-III). It is an extension of the Nondominated Sorting Genetic Algorithm (NSGA-III). E-NSGA-III utilizes extreme solutions in the population generation module in order improve quality of solutions. Five well-known scientific workflows are selected as testbeds. Hypervolume and the Pareto front are chosen as the performance metrics. E-NSGA-III is evaluated by comparing its performance against the two previous versions (NSGA-II and NSGA-III). The comparison reveals that E-NSGA-III yields the best performance among them in multi-objective scheduling of the five scientific workflows.},
  keywords={},
  doi={10.1109/ICMLA.2018.00184},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9678460,
  author={Noureddine, Staifi and Meriem, Belguidoum},
  booktitle={2021 International Conference on Information Systems and Advanced Technologies (ICISAT)}, 
  title={ML-SLA-IoT: an SLA Specification and Monitoring Framework for IoT applications}, 
  year={2021},
  volume={},
  number={},
  pages={1-12},
  abstract={Service level agreement (SLA) is a formal contract between a service provider and a service consumer to guarantee the Quality of service (QoS) expectations, it is used in all areas of information technology, such as Cloud Computing, Internet of Things (IoT), networks and Web services. For IoT applications, the main challenges are: (1) how to describe the SLA terms, such as QoS properties, service levels, penalties in SLA violation, (2) how to monitor these terms, and (3) how to integrate an SLA into all the IoT application layers. Therefore, in this paper, we propose ML-SLA-IoT, a framework for SLA specification and monitoring. It covers the entire layers of an IoT application. It describes precisely QoS levels, provided services, and obligations. It differs from other SLA specification languages in the specification of user preferences, the use of microservices (for reusability, dynamism and ease of integration) and ML-SLA (multi-level metrics and QoS according to existing constraints and user preferences). Furthermore, ML-SLA-IoT monitors SLA terms in an automatic and decentralised manner using smart contracts and blockchain technologies without the intervention of the third-party. Finally, we present a comparative study and experiments with existing solutions regarding to some criteria for SLA specification and monitoring. Our results show that ML-SLA-IoT gives better performance in terms of dynamic pricing, obligation combination, and SLA monitoring.},
  keywords={},
  doi={10.1109/ICISAT54145.2021.9678460},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{8260045,
  author={Afify, Yasmine M. and Badr, Nagwa L. and Moawad, Ibrahim F. and Tolba, Mohamed F.},
  booktitle={2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)}, 
  title={Evaluation of cloud service ontologies}, 
  year={2017},
  volume={},
  number={},
  pages={144-153},
  abstract={It is the cloud computing era. Substantial number of Cloud Services (CS) have emerged. The automation of the cloud services life cycle can be enhanced using domain ontologies in the cloud environment. Ontologies allow more efficient CS publication, discovery, selection, composition and recommendations. However, no broad cloud ontology for this purpose has dominated yet. This paper presents an in-depth analysis of existing cloud taxonomies and service ontologies. We present a comparison of the cloud service ontologies implementation features. Moreover, a semiotic metrics suite of ontology quality is used for the assessment process.},
  keywords={},
  doi={10.1109/INTELCIS.2017.8260045},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9719678,
  author={Gong, Yanqi and Hao, Fei and Sun, Yifei and Guo, Longjiang},
  booktitle={2021 20th International Conference on Ubiquitous Computing and Communications (IUCC/CIT/DSCI/SmartCNS)}, 
  title={Joint Optimization of Latency and Reward for Offloading Dependent Tasks in Mobile Edge Computing}, 
  year={2021},
  volume={},
  number={},
  pages={68-75},
  abstract={In the 5G era with the explosive growth of data, offloading tasks to edge servers that are executed closer to it becomes one of the most popular computational paradigms. Different from cloud computing, mobile edge computing (MEC) significantly addresses the problem of latency-sensitive applications execution, such as online gaming and VR/AR applications. In order to improve the quality of experience of end-users, those applications are often divided into multiple tasks with dependencies. Regarding the problem of offloading tasks with dependencies, the existing researches focus on either latency or reward optimization that leads to the practical difficulty of this problem. Towards this end, this paper first introduces a novel metric reward per unit time which integrates the latency and reward for better optimization of tasks offloading strategy. Then, the reward per unit time is viewed as the objective function, and further addressing the above problem is equivalent to finding the optimal offloading strategy to maximize the value of the objective function. The simulation experiments are conducted for demonstrating that the proposed offloading strategy is feasible and effective.},
  keywords={},
  doi={10.1109/IUCC-CIT-DSCI-SmartCNS55181.2021.00025},
  ISSN={},
  month={Dec},}

@ARTICLE{7437245,
  author={Maia, Delano Jose Holanda and Moreira, Leonardo Oliveira and Coutinho, Emanuel Ferreira and Gomes, Danielo Goncalves},
  journal={IEEE Latin America Transactions}, 
  title={MILENA: a model for implementing distributed multiplayer games in computing clouds}, 
  year={2016},
  volume={14},
  number={2},
  pages={951-957},
  abstract={Cloud computing environments are characteristically composed of distributed infrastructure, heterogeneous and virtualized resources, besides serving concurrently a wide range of customers whose service level agreements may require different levels of Quality of Service (QoS). Distributed multiplayer games, in turn, are collaborative distributed applications that must have an acceptable level of QoS to maintain justice among its players. This article proposes MILENA, a model for implementing distributed multiplayer games on computer clouds. The proposed model meets the requirements of QoS, fault tolerance and scalability. As a case study for verification and validation of the proposal, we have developed a game and evaluated its performance by SLA violations and response time of requests metrics. The results show that the MILENA scales up to 100 players without QoS loss, thus improving justice in distributed multiplayer games.},
  keywords={},
  doi={10.1109/TLA.2016.7437245},
  ISSN={1548-0992},
  month={Feb},}

@ARTICLE{7972945,
  author={He, Jianhua and Wei, Jian and Chen, Kai and Tang, Zuoyin and Zhou, Yi and Zhang, Yan},
  journal={IEEE Internet of Things Journal}, 
  title={Multitier Fog Computing With Large-Scale IoT Data Analytics for Smart Cities}, 
  year={2018},
  volume={5},
  number={2},
  pages={677-686},
  abstract={Analysis of Internet of Things (IoT) sensor data is a key for achieving city smartness. In this paper a multitier fog computing model with large-scale data analytics service is proposed for smart cities applications. The multitier fog is consisted of ad-hoc fogs and dedicated fogs with opportunistic and dedicated computing resources, respectively. The proposed new fog computing model with clear functional modules is able to mitigate the potential problems of dedicated computing infrastructure and slow response in cloud computing. We run analytics benchmark experiments over fogs formed by Rapsberry Pi computers with a distributed computing engine to measure computing performance of various analytics tasks, and create easy-to-use workload models. Quality of services (QoS) aware admission control, offloading, and resource allocation schemes are designed to support data analytics services, and maximize analytics service utilities. Availability and cost models of networking and computing resources are taken into account in QoS scheme design. A scalable system level simulator is developed to evaluate the fog-based analytics service and the QoS management schemes. Experiment results demonstrate the efficiency of analytics services over multitier fogs and the effectiveness of the proposed QoS schemes. Fogs can largely improve the performance of smart city analytics services than cloud only model in terms of job blocking probability and service utility.},
  keywords={},
  doi={10.1109/JIOT.2017.2724845},
  ISSN={2327-4662},
  month={April},}

@ARTICLE{8489955,
  author={Nguyen, Tien-Dung and Huh, Eui-Nam and Jo, Minho},
  journal={IEEE Internet of Things Journal}, 
  title={Decentralized and Revised Content-Centric Networking-Based Service Deployment and Discovery Platform in Mobile Edge Computing for IoT Devices}, 
  year={2019},
  volume={6},
  number={3},
  pages={4162-4175},
  abstract={Mobile edge computing (MEC) is used to offload services (tasks) from cloud computing in order to deliver those services to mobile Internet of Things (IoT) devices near mobile edge nodes. However, even though there are advantages to MEC, we face many significant problems, such as how a service provider (SP) deploys requested services efficiently on a destination MEC node, and how to discover existing services in neighboring MEC nodes to save edge resources. In this paper, we present a decentralized and revised content-centric networking (CCN)-based MEC service deployment/discovery protocol and platform. We organized a gateway in every area according to a three-tiered hierarchical MEC network topology to reduce computing overhead at the centralized controller. We revised CCN to introduce a protocol to help SP deploy their service on MEC node and assist MEC node discover services in neighboring nodes. By using our proposed protocol, MEC nodes can deploy or discover the requested service instances in the proximity of IoT devices to reduce transmission delay. We also present a mathematical model to calculate the round trip time to guarantee quality of service. Numerical experiments measure the performance of our proposed method with various mobile IoT device services. The results show that the proposed service deployment protocol and platform reduce the average service delay by up to 50% compared to legacy cloud. In addition, the proposed method outperforms the legacy protocol of the MEC environment in service discovery time.},
  keywords={},
  doi={10.1109/JIOT.2018.2875489},
  ISSN={2327-4662},
  month={June},}

@ARTICLE{9489314,
  author={Cheikhrouhou, Omar and Mahmud, Redowan and Zouari, Ramzi and Ibrahim, Muhammad and Zaguia, Atef and Gia, Tuan Nguyen},
  journal={IEEE Access}, 
  title={One-Dimensional CNN Approach for ECG Arrhythmia Analysis in Fog-Cloud Environments}, 
  year={2021},
  volume={9},
  number={},
  pages={103513-103523},
  abstract={Cardiovascular diseases are considered the number one cause of death across the globe which can be primarily identified by the abnormal heart rhythms of the patients. By generating electrocardiogram (ECG) signals, wearable Internet of Things (IoT) devices can consistently track the patient's heart rhythms. Although Cloud-based approaches for ECG analysis can achieve some levels of accuracy, they still have some limitations, such as high latency. Conversely, the Fog computing infrastructure is more powerful than edge devices but less capable than Cloud computing for executing compositionally intensive data analytic software. The Fog infrastructure can consist of Fog-based gateways directly connected with the wearable devices to offer many advanced benefits, including low latency and high quality of services. To address these issues, a modular one-dimensional convolution neural network (1D-CNN) approach is proposed in this work. The inference module of the proposed approach is deployable over the Fog infrastructure for analysing the ECG signals and initiating the emergency countermeasures within a minimum delay, whereas its training module is executable on the computationally enriched Cloud data centers. The proposed approach achieves the F1-measure score ≈1 on the MIT-BIH Arrhythmia database when applying GridSearch algorithm with the cross-validation method. This approach has also been implemented on a single-board computer and Google Colab-based hybrid Fog-Cloud infrastructure and embodied to a remote patient monitoring system that shows 25% improvement in the overall response time.},
  keywords={},
  doi={10.1109/ACCESS.2021.3097751},
  ISSN={2169-3536},
  month={},}

@ARTICLE{7090976,
  author={Candeia, David and Santos, Ricardo Araújo and Lopes, Raquel},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Business-Driven Long-Term Capacity Planning for SaaS Applications}, 
  year={2015},
  volume={3},
  number={3},
  pages={290-303},
  abstract={Capacity Planning is one of the activities developed by Information Technology departments over the years, it aims at estimating the amount of resources needed to offer a computing service. This activity contributes to achieving high Quality of Service levels and also to pursuing better economic results for companies. In the Cloud Computing context, one plausible scenario is to have Software-as-a-Service (SaaS) providers that build their IT infrastructure acquiring resources from Infrastructure-as-a-Service (IaaS) providers. SaaS providers can reduce operational costs and complexity by buying instances from a reservation market, but then need to predict the number of instances needed in the long-term. This work investigates how important is the capacity planning in this context and how simple business-driven heuristics for long-term capacity planning impact on the profit achieved by SaaS providers. Simulation experiments were performed using synthetic e-commerce workloads. Our analysis show that proposed heuristics increase SaaS provider profit, on average, at 9.6501 percent per year. Analysing such results we demonstrate that capacity planning is still an important activity, contributing to the increase of SaaS providers profit. Besides, a good capacity planning may also avoid bad reputation due to unacceptable performance, which is a gain very hard to measure.},
  keywords={},
  doi={10.1109/TCC.2015.2424877},
  ISSN={2168-7161},
  month={July},}

@ARTICLE{9165739,
  author={Gao, Zihan and Hao, Wanming and Han, Zhuo and Yang, Shouyi},
  journal={IEEE Access}, 
  title={Q-Learning-Based Task Offloading and Resources Optimization for a Collaborative Computing System}, 
  year={2020},
  volume={8},
  number={},
  pages={149011-149024},
  abstract={Mobile edge computing (MEC) can effectively overcome the shortcomings of high-latency in mobile cloud computing (MCC) by deploying the cloud resources, e.g., storage and computational capability, to the edge. However, the limited computation capability of the MEC restricts the scalability of offloading. Therefore, the basic requirements of the MEC system are to explore effective offloading decisions and resource allocation methods. To address it, we develop a collaborative computing system composed of local computing (mobile device), MEC (edge cloud) and MCC (central cloud). Based on the proposed collaborative computing system, we design a novel Q-learning based computation offloading (QLCOF) policy to achieve the optimal resource allocation and offloading scheme by prescheduling the computation side for each task from a global perspective. Specifically, we first model the offloading decision process as a Markov decision process (MDP) and design a state loss function (STLF) to measure the quality of experience (QoE). After that, we define the cumulation of STLFs as the system loss function (SYLF) and formulate an SYLF minimization problem. Due to the difficulty to directly solve the formulated problem, we decompose it into multiple subproblems and preferentially optimize the transmission power and computation frequency of the edge cloud by the quasi-convex bisection and polynomial analysis method, respectively. Based on the precalculated offline transmission power and edge cloud computation frequency, we develop a Q-learning based offloading (QLOF) scheme to minimize the SYLF by optimizing offloading decisions. Finally, the numeral results show that the proposed QLOF scheme effectively reduces the SYLF under different parameters.},
  keywords={},
  doi={10.1109/ACCESS.2020.3015993},
  ISSN={2169-3536},
  month={},}

@ARTICLE{9097295,
  author={Rahman, Sabidur and Ahmed, Tanjila and Huynh, Minh and Tornatore, Massimo and Mukherjee, Biswanath},
  journal={IEEE Transactions on Network and Service Management}, 
  title={Auto-Scaling Network Service Chains Using Machine Learning and Negotiation Game}, 
  year={2020},
  volume={17},
  number={3},
  pages={1322-1336},
  abstract={Network Function Virtualization (NFV) enables Network Operators (NOs) to efficiently respond to the increasing dynamicity of network services. Virtual Network Functions (VNFs) running on commercial off-the-shelf servers are easy to deploy, update, monitor, and manage. Such virtualized services are often deployed as Service Chains (SCs), which require in-sequence placement of computing and memory resources as well as routing of traffic flows. Due to the ongoing migration towards cloudification of networks, the concept of auto-scaling which originated in Cloud Computing, is now receiving attention from networks professionals too. Prior studies on auto-scaling use measured load to dynamically react to traffic changes. Moreover, they often focus on only one of the resources (e.g., compute only, or network capacity only). In this study, we consider three different resource types: compute, memory, and network bandwidth. In prior studies, NO takes auto-scaling decisions, assuming tenants are always willing to auto-scale, and Quality of Service (QoS) requirements are homogeneous. Our study proposes a negotiation-game-based auto-scaling method where tenants and NO both engage in the auto-scaling decision, based on their willingness to participate, heterogeneous QoS requirements, and financial gain (e.g., cost savings). In addition, we propose a proactive Machine Learning (ML) based prediction method to perform SC auto-scaling in dynamic traffic scenario. Numerical examples show that our proposed SC auto-scaling methods powered by ML present a win-win situation for both NO and tenants (in terms of cost savings).},
  keywords={},
  doi={10.1109/TNSM.2020.2995900},
  ISSN={1932-4537},
  month={Sep.},}

@INPROCEEDINGS{8366932,
  author={Zhou, Peipei and Ruan, Zhenyuan and Fang, Zhenman and Shand, Megan and Roazen, David and Cong, Jason},
  booktitle={2018 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Doppio: I/O-Aware Performance Analysis, Modeling and Optimization for In-memory Computing Framework}, 
  year={2018},
  volume={},
  number={},
  pages={22-32},
  abstract={In conventional Hadoop MapReduce applications, I/O used to play a heavy role in the overall system performance. More recently, a study from the Apache Spark community- state-of-the-art in-memory cluster computing framework- reports that I/O is no longer the bottleneck and has a marginal performance impact on applications like SQL processing. However, we observe that simply replacing HDDs with SSDs in a Spark cluster can have over 10x performance improvement for certain stages in large-scale production-quality genome processing. Therefore, one key question arises: How does I/O quanti- tatively impact the performance of today's big data applications developed using in-memory cluster computing frameworks like Apache Spark? In this paper we select an important yet complex application- the Spark-based Genome Analysis ToolKit (GATK4)-to guide our modeling. We first use different combinations of HDDs and SSDs to measure the I/O impact on GATK4 and change the CPU core number to discover the relation between computation and I/O access. By combining with Spark's underlying implementations, we further analyze the inherent cause of the above observations and build our model based on the analysis. Although we are building upon GATK4, our model maintains generality to other applications. Experimental results show that we can achieve a performance prediction error rate within 10% for typical Spark applications of both iterative and shuffle-heavy algorithms. Finally, we further extend our model to a broader area-that of optimal configuration selection in the public cloud. In Google Cloud, our model enables us to save 38% to 57% of cost for genome sequencing compared with its recommended default configurations. Currently, more and more companies are adopting cloud computing for specific workloads. Our proposed model can have a huge impact on their choices, while also enabling them to significantly reduce their costs.},
  keywords={},
  doi={10.1109/ISPASS.2018.00011},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8057143,
  author={Toka, Lászlíó and Lajtha, Balázs and Hosszu, Éva and Formanek, Bence and Géhberger, Dániel and Tapolcai, János},
  booktitle={IEEE INFOCOM 2017 - IEEE Conference on Computer Communications}, 
  title={A resource-aware and time-critical IoT framework}, 
  year={2017},
  volume={},
  number={},
  pages={1-9},
  abstract={Internet of Things (IoT) systems produce great amount of data, but usually have insufficient resources to process them in the edge. Several time-critical IoT scenarios have emerged and created a challenge of supporting low latency applications. At the same time cloud computing became a success in delivering computing as a service at affordable price with great scalability and high reliability. We propose an intelligent resource allocation system that optimally selects the important IoT data streams to transfer to the cloud for processing. The optimization runs on utility functions computed by predictor algorithms that forecast future events with some probabilistic confidence based on a dynamically recalculated data model. We investigate ways of reducing specifically the upload bandwidth of IoT video streams and propose techniques to compute the corresponding utility functions. We built a prototype for a smart squash court and simulated multiple courts to measure the efficiency of dynamic allocation of network and cloud resources for event detection during squash games. By continuously adapting to the observed system state and maximizing the expected quality of detection within the resource constraints our system can save up to 70% of the resources compared to the naive solution.},
  keywords={},
  doi={10.1109/INFOCOM.2017.8057143},
  ISSN={},
  month={May},}

@ARTICLE{9458258,
  author={Liu, Li and Lu, Caiwu and Xiao, Fengjun and Liu, Ruimin and Xiong, Neal Naixue},
  journal={IEEE Access}, 
  title={A Practical, Integrated Multi-Criteria Decision-Making Scheme for Choosing Cloud Services in Cloud Systems}, 
  year={2021},
  volume={9},
  number={},
  pages={88391-88404},
  abstract={Currently, with the rapid development and broad application of cloud computing technology, companies tend to use cloud services to build their applications or business systems. Selecting a trustworthy cloud service is a challenging multi-criteria decision-making (MCDM) problem. Moreover, decision makers are more inclined to use linguistic descriptions to assess the quality of service (QoS) for cloud services due to the limitation of the decision makers' knowledge and the vagueness of criteria information. Therefore, we propose a practical, integrated MCDM scheme for cloud service evaluation and selection of cloud systems, allowing decision makers to compare cloud services based on QoS criteria. First, to more accurately and effectively express the uncertainty of qualitative concepts, the cloud model is used as a conversion tool for qualitative and quantitative information to quantify linguistic terms. Second, given the shortcomings of traditional differentiating measures between cloud models, a more comprehensive distance measurement algorithm using cloud droplet distribution is proposed for the cloud model. The new distance measurement algorithm is applied to the calculation of cloud model similarity and the gray correlation coefficient. The dynamic expertise weights are determined by calculating the similarity between the expert evaluation cloud model and the arithmetic mean cloud model. Then, we propose a technique for order preference by similarity to an ideal solution (TOPSIS) improved by the grey relational analysis (GRA) to calculate the relative closeness of alternatives to the positive and negative ideal solutions and establish a multi-objective optimization model that maximizes the relative closeness of all alternatives to determine the weights of the criteria. Finally, we reconstructed the QoS evaluation criteria for cloud services from both application and service perspectives, and the classical TOPSIS is applied to generate alternative rankings. The practicability and robustness of the scheme were tested through the cloud service selection problem experienced by a real mining company's scheduling platform, which can provide practical references with the theoretical basis for the selection and evaluation of cloud services.},
  keywords={},
  doi={10.1109/ACCESS.2021.3089991},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9058016,
  author={Bhardwaj, Tushar and Upadhyay, Himanshu and Sharma, Subhash Chander},
  booktitle={2020 10th International Conference on Cloud Computing, Data Science & Engineering (Confluence)}, 
  title={Framework for Quality Ranking of Components in Cloud Computing: Regressive Rank}, 
  year={2020},
  volume={},
  number={},
  pages={598-604},
  abstract={As the popularity of cloud computing is increasing there is an urgent requirement of developing highly efficient and highly qualitative cloud applications (CA). Hence, it be-comes a big research problem. A recommender system recommends the suitable item to the user and almost all the systems provide a rating score for preference. Traditionally, algorithms predicts the ratings that a user should give to the unrated components to queue the item in recommended list. To select an optimal candidate from a set of function-ally equivalent candidates is crucial through approaches that follow a framework for component quality ranking. More-over, such framework helps in detecting the poor performing candidates from a highly distributed cloud applications. In this paper a novel technique is proposed to provide personalized component ranking for designers by employing the past usage of components by different users. In this approach the similarity between the users is measured based on their rankings for functionally equivalent components set instead of their rating values. In this approach no additional invocation of cloud component is required. Experimental results on real world web-service invocations data set shows that the proposed approach outperforms the previous approaches.},
  keywords={},
  doi={10.1109/Confluence47617.2020.9058016},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{6883911,
  author={Liu, Meng and Dou, Wanchun and Yu, Shui and Zhang, Zhensheng},
  booktitle={2014 IEEE International Conference on Communications (ICC)}, 
  title={A clusterized firewall framework for cloud computing}, 
  year={2014},
  volume={},
  number={},
  pages={3788-3793},
  abstract={Cloud computing is becoming popular as the next infrastructure of computing platform. However, with data and business applications outsourced to a third party, how to protect cloud data centers from numerous attacks has become a critical concern. In this paper, we propose a clusterized framework of cloud firewall, which characters performance and cost evaluation. To provide quantitative performance analysis of the cloud firewall, a novel M/Geo/1 analytical model is established. The model allows cloud defenders to extract key system measures such as request response time, and determine how many resources are needed to guarantee quality of service (QoS). Moreover, we give an insight into financial cost of the proposed cloud firewall. Finally, our analytical results are verified by simulation experiments.},
  keywords={},
  doi={10.1109/ICC.2014.6883911},
  ISSN={1938-1883},
  month={June},}

@ARTICLE{9354861,
  author={Sacco, Alessio and Flocco, Matteo and Esposito, Flavio and Marchetto, Guido},
  journal={IEEE Transactions on Network and Service Management}, 
  title={Supporting Sustainable Virtual Network Mutations With Mystique}, 
  year={2021},
  volume={18},
  number={3},
  pages={2714-2727},
  abstract={The abiding attempt of automation has also permeated the networks, with the ability to measure, analyze, and control themselves in an automated manner, by reacting to changes in the environment (e.g., demand). When provided with these features, networks are often labeled as “self-driving” or “autonomous”. In this regard, the provision and orchestration of physical or virtual resources are crucial for both Quality of Service (QoS) guarantees and cost management in the edge/cloud computing environment. To effectively manage the lifecycle of these resources, an auto-scaling mechanism is essential. However, traditional threshold-based and recent Machine Learning (ML)-based policies are often unable to address the soaring complexity of networks due to their centralized approach. By relying on multi-agent reinforcement learning, we propose Mystique, a solution that learns from the load on links to establish the minimal set of active network resources. As traffic demands ebb and flow, our adaptive and self-driving solution can scale up and down and also react to failures in a fully automated, flexible, and efficient manner. Our results demonstrate that the presented solution can reduce network energy consumption while providing an adequate service level, outperforming other benchmark auto-scaling approaches.},
  keywords={},
  doi={10.1109/TNSM.2021.3059647},
  ISSN={1932-4537},
  month={Sep.},}

@INPROCEEDINGS{7396178,
  author={Rahulamathavan, Yogachandran and Rajarajan, Muttukrishnan and Rana, Omer F. and Awan, Malik S. and Burnap, Pete and Das, Sajal K.},
  booktitle={2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)}, 
  title={Assessing Data Breach Risk in Cloud Systems}, 
  year={2015},
  volume={},
  number={},
  pages={363-370},
  abstract={The emerging cloud market introduces a multitude of cloud service providers, making it difficult for consumers to select providers who are likely to be a low risk from a security perspective. Recently, significant emphasis has arisen on the need to specify Service Level Agreements that address security concerns of consumers (referred to as SecSLAs) -- these are intended to clarify security support in addition to Quality of Service characteristics associated with services. It has been found that such SecSLAs are not consistent among providers, even though they offer services with similar functionality. However, measuring security service levels and the associated risk plays an important role when choosing a cloud provider. Data breaches have been identified as a high priority threat influencing the adoption of cloud computing. This paper proposes a general analysis framework which can compute risk associated with data breaches based on pre-agreed SecSLAs for different cloud providers. The framework exploits a tree based structure to identify possible attack scenarios that can lead to data breaches in the cloud and a means of assessing the use of potential mitigation strategies to reduce such breaches.},
  keywords={},
  doi={10.1109/CloudCom.2015.58},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{6883661,
  author={Si, Pengbo and Yu, F. Richard and Zhang, Yanhua},
  booktitle={2014 IEEE International Conference on Communications (ICC)}, 
  title={Joint cloud and radio resource management for video transmissions in mobile cloud computing networks}, 
  year={2014},
  volume={},
  number={},
  pages={2270-2275},
  abstract={In mobile cloud computing (MCC) systems, the resource in both the cloud and the mobile network should be carefully managed. Cloud resource management and radio resource management have traditionally been addressed separately in previous works. In this paper, we propose to jointly study dynamic cloud and radio resource management so as to improve end-to-end performance of adaptive video transmissions in MCC systems. Video application quality of service performance, distortion, is adopted as the performance measure. An important video application layer parameter, intra-refreshing rate, is optimized to improve the video distortion performance. We formulate the problem as a stochastic restless bandits optimization problem, which facilitates the distributed MCC architecture and simplifies the computation and implementation due to its “indexibility” property. Simulation results are presented to show the effectivenes of the proposed scheme.},
  keywords={},
  doi={10.1109/ICC.2014.6883661},
  ISSN={1938-1883},
  month={June},}

@INPROCEEDINGS{9183589,
  author={Winzinger, Stefan and Wirtz, Guido},
  booktitle={2020 IEEE International Conference on Service Oriented Systems Engineering (SOSE)}, 
  title={Applicability of Coverage Criteria for Serverless Applications}, 
  year={2020},
  volume={},
  number={},
  pages={49-56},
  abstract={Serverless computing is a popular trend in cloud computing based on serverless functions. These functions are stateless which can be utilized by the cloud platform provider to scale functions dynamically. While these small functions are easy to test in isolation, integrating them with other resources provided by the cloud platform provider or third parties creates a complex system whose emerging behavior is hard to test. Integration tests help test the interaction of the serverless functions with other resources and their environment. However, it is hard to decide if a test case is adequate and focuses on the critical parts of the system. Therefore, coverage criteria can be used to measure the coverage of the relevant software components and help assess the quality of the test suite. In this paper, we identified serverless applications based on serverless functions on GitHub and used them to investigate which coverage criteria can be used to capture the interaction of serverless functions with other resources. Furthermore, we show a general approach to implement the measurement of the coverage on FaaS platforms. Thus, developers have means to test the adequacy of their applications on any FaaS platform.},
  keywords={},
  doi={10.1109/SOSE49046.2020.00013},
  ISSN={2642-6587},
  month={Aug},}

@INPROCEEDINGS{7288394,
  author={Xie, Xiongwei and Wang, Weichao and Qin, Tuanfa},
  booktitle={2015 24th International Conference on Computer Communication and Networks (ICCCN)}, 
  title={Detection of Service Level Agreement (SLA) Violation in Memory Management in Virtual Machines}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  abstract={In cloud computing, quality of services is often enforced through Service Level Agreement (SLA) between end users and cloud providers. While SLAs on hardware resources such as CPU cycles or bandwidth can be monitored by low layer sensors, the enforcement of security SLAs stays a very challenging problem. Several high level architectures for security SLAs have been proposed. However, details still need to be filled before they can be deployed. In this paper, we propose to design mechanisms to detect violations of security SLAs. Specifically, we focus on unauthorized accesses to memory pages of a virtual machine and violation of the memory deduplication policies. Through measuring the accumulated memory access latency, we try to derive out whether or not the memory pages have been swapped out and the order of accesses to them. These events will then be compared to access commands issued by the local VM. In this way, unauthorized memory accesses or violation of deduplication policies can be detected. Compared to existing approaches, our mechanisms do not need explicit help from the hypervisor or third parties. Therefore, it can detect SLA violations even when they are initiated by the hypervisor. We implement our approaches under VMWare with Windows virtual machines. Our experiment results show that the VM can effectively detect the violations with small increases in overhead.},
  keywords={},
  doi={10.1109/ICCCN.2015.7288394},
  ISSN={1095-2055},
  month={Aug},}

@INPROCEEDINGS{7435462,
  author={Chen, Wei and Chen, Jiming and Tang, Jine and Wang, Liangmin},
  booktitle={2015 Third International Conference on Advanced Cloud and Big Data}, 
  title={A QoS Guarantee Framework for Cloud Services Based on Bayesian Prediction}, 
  year={2015},
  volume={},
  number={},
  pages={117-124},
  abstract={With the quality of service (QoS) of cloud services becoming increasingly concerned, how to ensure that the QoS of cloud services can meet the users' QoS requirement has become a focus of the study on cloud computing. However, the QoS of cloud services is dynamic regularly, we propose a QoS guarantee framework for cloud services and the Bayesian prediction method is used to predict QoS of cloud service. In our framework, cloud system can monitor and predict the QoS of cloud services in real-time, not only during the selection of cloud services, but also during the execution of cloud services. Once QoS prediction results show that some QoS violations will occur, cloud system will take measures to avoid the occurrence of QoS violations. We use the cloud simulation software called CloudSim for the experiment and the results demonstrate that compared with ARIMA and other prediction methods, our Bayesian prediction method has higher accuracy. Moreover, our QoS guarantee framework can greatly reduce the probability of QoS violation.},
  keywords={},
  doi={10.1109/CBD.2015.28},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{7410295,
  author={Abtahizadeh, S. Amirhossein and Khomh, Foutse and Guéhéneuc, Yann-Gaël},
  booktitle={2015 IEEE 34th International Performance Computing and Communications Conference (IPCCC)}, 
  title={How green are cloud patterns?}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  abstract={Cloud Patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. Yet, energy consumption is the biggest challenge that cloud computing systems (the backbone of today's high-tech economy) face today. In fact, 10% of the world's electricity is now being consumed by servers, laptops, tablets and smartphones. Energy consumption has complex dependencies on the hardware platform, and the multiple software layers. The hardware, its firmware, the operating system, and the various software components used by a cloud application, all contribute to determining the energy footprint. Hence, even though increasing a data center efficiency will eventually improve energy efficiency, the internal design of cloud-based applications can be improved to lower energy consumption. In this paper, we conduct an empirical study on a RESTful multi-threaded application deployed in the cloud, to investigate the individual and the combined impact of three cloud patterns (e.g., Local Database proxy, Local Sharding Based Router and Priority Queue) on the energy consumption of cloud based applications. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Developers and software architects can make use of these results to guide their design decisions.},
  keywords={},
  doi={10.1109/PCCC.2015.7410295},
  ISSN={2374-9628},
  month={Dec},}

@INPROCEEDINGS{9084760,
  author={Shan, Nanliang and Cui, Xiaolong and Gao, Zhiqiang and Li, Yu},
  booktitle={2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)}, 
  title={Multi-User Multi-Server Multi-Channel Computation Offloading Strategy for Mobile Edge Computing}, 
  year={2020},
  volume={1},
  number={},
  pages={1389-1400},
  abstract={Mobile edge computing is a new computing paradigm that can extend cloud computing capabilities to the edge network, supporting computation-intensive applications such as face recognition, natural language processing, augmented reality. Notably, computation offloading is a key technology of mobile edge computing to improve mobile devices performance and user experience by offloading local tasks to edge servers. In this paper, we study the problem of computation offloading under multi-user, multi-server, and multi-channel scenarios, and propose a computation offloading strategy considering the quality of service (QoS) of users, server resources, and channel interference. This strategy consists of three stages: (1) In offloading decision stage, the offloading decision is made based on the beneficial degree of computation offloading, which is measured by the total cost of local computing of mobile device in comparison with the edge-side server. (2) In the server selection stage, the candidate is comprehensively evaluated and selected by a multi-objective decision based on Cov-AHP for computation offloading. (3) In the channel selection stage, a multi-user and multi-channel distributed computation offloading model based on potential game is proposed by considering the influence of channel interference on the user's overall overhead. The corresponding multi-user and multi-channel task scheduling algorithm is designed to maximize the overall benefit by finding the Nash equilibrium point of the potential game. Amounts of experimental results show that the proposed method can greatly increase the number of beneficial computation offloading users, and effectively reduce the energy consumption and time delay.},
  keywords={},
  doi={10.1109/ITNEC48623.2020.9084760},
  ISSN={},
  month={June},}

@ARTICLE{6740925,
  author={Nam, Yunyoung and Park, Hyung Ju and Cho, Chae Ho and Park, Jong Hyuk},
  journal={IEEE Systems Journal}, 
  title={An Interactive IPTV System With Community Participation in Cloud Computing Environments}, 
  year={2014},
  volume={8},
  number={1},
  pages={174-183},
  abstract={This paper presents a video communication system that provides real-time participating services to audiences using Internet Protocol television (IPTV) and mobile devices on cloud computing environments. High-quality video processing and bidirectional multimedia communication technologies are combined for videoconferencing and interactive user participation. The P-module has been developed to encode and decode 1080i high-definition video data on a system simultaneously. A video communication protocol is applied to exchanging information between distributed and heterogeneous devices. The developed system has been deployed in a public service center in Seoul, Korea. In the experiments, we will show the implemented system using IPTV and a mobile phone, as well as the experiment results of the measured CPU overhead and mixing time in our system.},
  keywords={},
  doi={10.1109/JSYST.2013.2258745},
  ISSN={1937-9234},
  month={March},}

@INPROCEEDINGS{8418099,
  author={Kyriazis, Dimosthenis},
  booktitle={2018 32nd International Conference on Advanced Information Networking and Applications Workshops (WAINA)}, 
  title={BYOS: Bring Your Own Security in Clouds and Service Oriented Infrastructures}, 
  year={2018},
  volume={},
  number={},
  pages={374-379},
  abstract={Cloud computing is widely being used by users, tenants and enterprises. However, the major concern and barrier for its adoption are the security and privacy concerns of end users. To this end, in this paper an approach is presented that proposes the use of security mechanisms, as plugins that are custom / tailored and potentially developed by the end users themselves. The latter is proposed as a means to overcome users concerns about the quality of security offered by the providers through their deployed security and privacy measures. As a concept, it builds on top of the well-established Bring Your Own Device (BYOD) paradigm and adopts it in the context of security and privacy. The potential challenges and an architecture with the corresponding key building blocks that address these challenges are presented.},
  keywords={},
  doi={10.1109/WAINA.2018.00114},
  ISSN={},
  month={May},}

@INPROCEEDINGS{7391921,
  author={Sudipta Singha Roy and Tamjid Haque Sarker and Hashem, M. M. A.},
  booktitle={2015 2nd International Conference on Electrical Information and Communication Technologies (EICT)}, 
  title={A novel trust measurement system for cloud-based marketplace}, 
  year={2015},
  volume={},
  number={},
  pages={49-54},
  abstract={Cloud Computing is an enormously growing phenomenon in the present days enabling IT related services to run in a more dynamic and scalable way than the previous days and cloud marketplace is becoming more competitive with the entrance of new cloud service providers (CSP) offering similar functionalities. The basic obstacles in the way of success of cloud marketplace are the numerous shortcomings in reliable monitoring and identifying reliable cloud service provider based on consumers' preferred services. In this paper, a multi-faceted Entrusted Trust Management (ETM) system architecture is introduced that can support the customers in reliably choosing the trustworthy cloud service provider (CSP) as well as properly weight the information sources that provide feedbacks about the quality of services (QoS) of the CSPs. This ETM system works on several issues to measure the trust value of the providers on specific domain and overall trust value. Firstly, measurement of the trust value of the specific domain of provider on the basis of certainty and uncertainty. Secondly, measurement of overall trust value of the provider from these domain specific trust values. Thirdly, measurement of “Degree of Conflict” between the rating/feedback of consumers and experts. And finally, measurement of trustworthiness of the information sources which provide the rating of the provider on the basis of the SLA between the provider and the consumer. At last, our proposed system is experimented using real datasets.},
  keywords={},
  doi={10.1109/EICT.2015.7391921},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9569577,
  author={Li, Guo and Liu, Ling and Liang, Zhengping and Ma, Xiaoliang and Zhu, Zexuan},
  booktitle={2021 IEEE 32nd Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)}, 
  title={Memetic Algorithm Based on Community Detection for Energy-Efficient Service Migration Optimization in 5G Mobile Edge Computing}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={Mobile edge computing (MEC) can supplement cloud computing by helping to overcome the limitations of long physical transmission distances and accelerating the responsiveness of edge computing servers. In 5G (fifth generation) cellular networks, adopting MEC can guarantee ultralow latency. To enhance the MEC quality, optimization of the user service profile migration according to the user mobility is essential. However, this optimization establishes an NP-hard problem. Moreover, high-speed 5G base stations with MEC servers often experience high energy consumption. As conventional service migration algorithms such as those based on profile tracking and game theory tend to fall in local optima and neglect energy consumption constraints, we propose a memetic algorithm based on community detection local search (MA-CDLS) to continuously optimize the service migration in 5G MEC scenarios. During busy periods or in crowded areas, MA-CDLS adopts a single-objective optimization of user-perceived latency to achieve high-performance 5G services. During light-load periods or in uncrowded areas, MA-CDLS uses two measures, namely the user-perceived latency and energy consumption, to realize energy-efficient 5G services. MA-CDLS effectively reduces the search space and speeds up the elite selection in the meme operator. Experiments in simulated scenarios show that MA-CDLS achieves a lower user-perceived latency and energy consumption, than the traditional profile tracking and game theory methods, especially during congestion.},
  keywords={},
  doi={10.1109/PIMRC50174.2021.9569577},
  ISSN={2166-9589},
  month={Sep.},}

@INPROCEEDINGS{7983102,
  author={Ekanayake, Wijaya and Amarasinghe, Heli and Karmouch, Ahmed},
  booktitle={2017 14th IEEE Annual Consumer Communications & Networking Conference (CCNC)}, 
  title={SDN-based IaaS for mobile computing}, 
  year={2017},
  volume={},
  number={},
  pages={179-184},
  abstract={Mobile Cloud Computing enables resource limited mobile devices to support rich application services. Among three types of cloud services, Infrastructure-as-a-Service (IaaS) clouds provides compute infrastructure for mobile applications on demand. In IaaS-based mobile clouds, latency and bandwidth requirements can considered as critical factors impacting Quality of Service (QoS). Opposed to centralized clouds, geographically distributed clouds realize higher QoS benefiting the proximity to the end user. In this paper, we propose an IaaS framework with regional datacenters for mobile clouds. With the benefits of software-defined networking (SDN), we address impacts on QoS during mobility by serving mobile user via the optimum datacenter. A test-bed was developed to measure the performance of service allocation and relocation in proposed framework.},
  keywords={},
  doi={10.1109/CCNC.2017.7983102},
  ISSN={2331-9860},
  month={Jan},}

@INPROCEEDINGS{7816920,
  author={Falasi, Asma Al and Serhani, Mohamed Adel},
  booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)}, 
  title={SLA Specification and Negotiation Model for a Network of Federated Clouds: CloudLend}, 
  year={2016},
  volume={},
  number={},
  pages={772-779},
  abstract={The Cloud computing paradigm is remarkably evolving. Cloud customers have become more conscious about the QoS expectation from Cloud providers' services. Accordingly, Cloud providers are required to be responsive to customers' demands, be open to establishing federations with other providers in order to retain their shares in the competitive Cloud market. Cloud customers are always searching for optimized services, irrespective of which providers are taking part in a federation to deliver these services. They seek federated Cloud services to attain the maximum satisfaction level, which is measured by the degree of adherence to customers' quality of service (QoS). Such federations of Cloud services are typically governed by service level agreements (SLAs) that are negotiated between the Cloud customer, provider prior to service provisioning. This paper tackles the challenges related to SLA specification, negotiation in a federated network of Clouds, CloudLend. We first propose a weighted SLA specification model that captures customers' QoS, manages multi-level SLAs specification. We then introduce an autonomous SLA negotiation model that adopts an enhanced fair division game. The model enables federated Cloud services to examine SLAs, react to SLA offers, eventually sign an SLA contract. It autonomously detects changes in Clouds federations, revises SLA specifications accordingly. The proposed model is evaluated using a CloudLend simulator, which we developed for this purpose. Several test cases were executed,, the results we have achieved verified the fairness, efficiency of our proposed SLA specification, negotiation models in CloudLend.},
  keywords={},
  doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0124},
  ISSN={},
  month={July},}

@INPROCEEDINGS{9443978,
  author={Yu, Zhixing and He, Kejing and Chen, Chao and Wang, Jian},
  booktitle={2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)}, 
  title={Live Container Migration via Pre-restore and Random Access Memory}, 
  year={2020},
  volume={},
  number={},
  pages={102-109},
  abstract={Container technology is increasingly being used for virtualization due to its ability to isolate the operating environment of the program. In cloud computing environment, we need to migrate containers between different hosts for load balancing or downtime maintenance. However, during the migration process, the container will be temporarily shut down, and the service will be unavailable. Therefore, the time cost is an essential indicator to measure the quality of the migration process. To achieve live container migration, we propose a pre-restore method and a complete random access memory (RAM) based method to migrate containers. Extensive experiments validate the effectiveness of our methods in reducing downtime and improving the efficiency of container migration.},
  keywords={},
  doi={10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00039},
  ISSN={},
  month={Dec},}

@ARTICLE{7343819,
  author={Liu, Jiangchuan and Zhu, Wenwu and Ebrahimi, Touradj and Apostolopoulos, John and Hua, Xian-Sheng and Wu, Chuan},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Introduction to the Special Section on Visual Computing in the Cloud: Fundamentals and Applications}, 
  year={2015},
  volume={25},
  number={12},
  pages={1885-1887},
  abstract={Cloud computing involves a large number of terminals connected through a real-time high-speed network (such as the Internet). The adoption rates for private and hybrid cloud services increased to 40% in 2013, with computing shifting from on-premise infrastructure to the cloud. To keep pace with the ever-accelerating rate of innovation, companies are moving to the cloud. However, visual computing in the cloud brings great challenges, such as how to measure and then improve the quality of experience in cloud computing. This Special Section provides the image/video community a forum to present new academic research and industrial development in running visual computing services in the cloud. This Special Section aims to address fundamental and practical aspects of visual computing in the cloud, such as how to build cloud platforms that can cope with seemingly unlimited supply of content coming from traditional media sources as well as new media uploaded to the Internet (YouTube, Facebook, etc.); how to leverage cloud technology to build high-quality image/video browsing and delivery experiences for a global audience; how to ingest, encode, process, adapt, as well as protect contents and privacy of users; how to provide both on-demand and live-streaming capabilities; how to tag image/video and allow consumers to access the image/video contents with high availability; how to support image/video services in mobile devices; and how to perform real-time image/video analytics in the cloud, to mention a few among a diverse range of challenges.},
  keywords={},
  doi={10.1109/TCSVT.2015.2472955},
  ISSN={1558-2205},
  month={Dec},}

@INPROCEEDINGS{10355514,
  author={Samarakoon, Sahan and Bandara, Shashika and Jayasanka, Nishan and Hettiarachchi, Chathuranga},
  booktitle={2023 Moratuwa Engineering Research Conference (MERCon)}, 
  title={Self-Healing and Self-Adaptive Management for IoT-Edge Computing Infrastructure}, 
  year={2023},
  volume={},
  number={},
  pages={473-478},
  abstract={Containerized micro-service oriented computing deployment strategies have proven to possess resilience, selfadaptive, and self-healing properties in cloud computing environments. The rapid growth of smart Internet of Things (IoT) deployments necessitates a similar approach to mitigate the challenges associated with manually managing large fleets of IoT devices. To address these challenges, we propose a novel software framework that extends Kubernetes(K8s) to collect and integrate IoT device performance metrics. By leveraging this framework, a set of self-healing and self-adaptive strategies can be deployed, taking into account the status of IoT devices. In our research, we evaluate the impact of IoT device-to-edge compute latency, bandwidth, and jitter information using the proposed software framework, including a metrics collection plugin and a custom scheduler. The results demonstrate significant enhancements in Quality of Service measures for a benchmark application scenario, emphasizing the framework’s ability to reduce manual intervention efforts through extended adaptation strategies.},
  keywords={},
  doi={10.1109/MERCon60487.2023.10355514},
  ISSN={2691-364X},
  month={Nov},}

@INPROCEEDINGS{10329061,
  author={Nguyen, Michael and Sood, Kanika and Avery, Kenytt and Bein, Doina},
  booktitle={2023 5th International Conference on Robotics and Computer Vision (ICRCV)}, 
  title={Deep Learning-based Super-Resolution on the Cloud: Focus on Face and Text Enhancement}, 
  year={2023},
  volume={},
  number={},
  pages={124-129},
  abstract={Real-ESRGAN and SwinIR are two deep learning models for Single-Image Super-Resolution (SISR), which attempt to address real-world scenarios for image enhancement. However, the pre-trained models do not effectively handle LR images containing human faces and text. An experiment is conducted to expand upon the training performed in their respective studies and improve the image enhancement using a cloud computing environment. Traditional image quality metrics, Peak Signal-toNoise Ratio (PSNR), and Structural Similarity (SSIM), are used to objectively evaluate the image quality. Three learning-based perceptual metrics, the Blind / Referenceless Image Spatial Quality Evaluator (BRISQUE), Naturalness Image Quality Evaluator (NIQE), and the Learned Perceptual Image Patch Similarity (LPIPS), are also incorporated to assess how images would be subjectively perceived based on human perception. To evaluate the model performance of Real-ESRGAN and SwinIR, specifically for face and text images, both traditional and perceptual metrics are taken into consideration, in addition to the cost associated with model training using Microsoft Azure. The findings show that with additional fine-tuning, SwinIR has slightly improved PSNR and SSIM values while taking less training time compared to Real-ESRGAN at the cost of perceptual quality.},
  keywords={},
  doi={10.1109/ICRCV59470.2023.10329061},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9744289,
  author={Volkov, A. O. and Korobkina, A. V. and Stepanov, S. N.},
  booktitle={2022 Systems of Signals Generating and Processing in the Field of on Board Communications}, 
  title={Development of a Model and Algorithms for Servicing Real-Time and Data Traffic in a Cloud Computing System}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={Today, modern low earth orbit (LEO) mobile satellite systems require constant data exchange with cloud computing centers located on Earth to work correctly. Moreover, there is both real-time data transmission and data that allows for a delay, this, for example, may be telemetry data. In the described model the key performance features of the conjoint traffic processing were defined. They are defined using values of probabilities of the model’s stationary states. The numerical algorithm of measuring of designed performance metrics by solving the state equations system with the help of the Gauss-Zeidel algorithm is presented. The model and the resulting algorithms can be used to assess the service quality and load levels of a small cloud computing node. One of the possible options for future research on this topic may be to restrict the access of applications for data processing to prioritize the processing of real-time applications.},
  keywords={},
  doi={10.1109/IEEECONF53456.2022.9744289},
  ISSN={2768-0118},
  month={March},}

@INPROCEEDINGS{9972509,
  author={Rajesh, K.N.V.S.S.K. and Redd, K. Thammi and Murthy, N.V.E.S. and Sarma, M. Subrahmanya},
  booktitle={2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon)}, 
  title={Augmenting Additional Quality of Services for Adaptability and Reliability of Cloud Infrastructure}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Adoption of Cloud computing has been increased by multi fold in the recent past in order to cope up with business requirements. There are few Quality of Service (QoS) metrics which augment the availability of cloud hosted applications. However, still there are quite a few challenges to be addressed in order to improve degree of trust in cloud computing. One of the key challenges observed is, there were no quality of services (QoS) metrics augmented with the PaaS which checks certain parameters covering Disk space utilization and Memory utilization etc., which are the root cause for any priority incidents pertaining to cloud hosted service availability. This paper aims to study the current quality of services available and augment additional quality of services to scale up the services for need of the hour requirements covering site reliability engineering (SRE). Experimentation has been done with the help of major cloud services platforms such as AWS and results were analyzed and attached to this paper.},
  keywords={},
  doi={10.1109/MysuruCon55714.2022.9972509},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10346675,
  author={Alkhazali, Abdel Rahman Mohammad and Khasawneh, Ahmad M. and Alzoubi, Sharaf and Magableh, Murad and Mohamed, Rajina R. and Pandey, Bishwajeet},
  booktitle={2023 International Conference on Computer Science and Emerging Technologies (CSET)}, 
  title={Cloud Computing in Smart Cities: Privacy, Ethical and Social Issues}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The rapid development of cloud computing technologies has revolutionized various sectors, and smart cities are no exception. Smart cities leverage cloud computing to optimize urban services, enhance resource management, and improve the overall quality of life for their inhabitants. However, as these technological advancements proliferate, concerns about privacy, ethical considerations, and social implications have emerged. This research paper critically examines the multifaceted challenges associated with the integration of cloud computing in smart cities, shedding light on the potential risks and highlighting the need for comprehensive solutions. The research employs a mixed-methods approach, combining quantitative data analysis and qualitative case studies to offer a comprehensive perspective on the identified issues. The primary focus lies in identifying privacy risks, ethical dilemmas, and social disparities that arise due to the extensive use of cloud-based systems and data in smart cities. Furthermore, the study investigates the role of key stakeholders, including governments, technology providers, and citizens, in mitigating or exacerbating these challenges. Key findings reveal that while cloud computing empowers smart cities with unparalleled capabilities, it also exposes residents' personal information to potential breaches and misuse. Ethical concerns arise from the handling of sensitive data, data ownership, and algorithmic biases that could perpetuate discrimination. Moreover, social issues like the digital divide and access disparities may further exacerbate existing inequalities in smart city implementation. This research paper concludes by proposing a comprehensive framework of guidelines and best practices to address the identified issues effectively. These recommendations encompass enhanced data privacy measures, transparent and accountable data governance, the promotion of ethical data usage, and inclusive strategies to bridge social disparities. By adopting these measures, smart cities can harness the full potential of cloud computing while safeguarding individual rights and fostering a more equitable and inclusive urban environment. Overall, this study underscores the critical importance of addressing privacy, ethical, and social challenges in the context of cloud computing in smart cities. By adopting a holistic and proactive approach, city planners, policymakers, and technology providers can build sustainable and responsible smart cities that ensure the well-being and dignity of their residents in the digital era.},
  keywords={},
  doi={10.1109/CSET58993.2023.10346675},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10078974,
  author={Li, Qixiu},
  booktitle={2022 2nd International Conference on Networking, Communications and Information Technology (NetCIT)}, 
  title={Construction of Agricultural Economic Data Management and Service Platform Based on Improved Genetic Algorithm}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={As a large agricultural country, agriculture is the foundation of China's national economy. Agricultural informatization construction is a major measure to strengthen the basic position of agriculture and promote the new agricultural scientific and technological revolution. It is of great significance to develop agricultural economy and promote the construction of a new socialist countryside. With the rise and continuous improvement of cloud computing, it has brought new inspiration to the construction of agricultural informatization. Cloud computing is the latest network information technology at present. It creates a new model of serving computing resources, storage resources and information resources under the condition of rapid network development, so that information can spread at a high speed, so that farmers can get better agricultural technology and more agricultural information resources in the shortest time, so as to greatly improve the agricultural economy. In recent years, the new cloud computing technology provides a new method and feeling for the construction of agricultural economic management information service platform. In order to realize the rapid acquisition of a variety of information resources and the sharing of information data, and improve the service level and service quality of agricultural information, taking the new IT resource model “cloud computing” as the starting point, computing and big data are distributed on computers all over the country. People can easily obtain applications, data and resources through mobile phones, computers and other services. Relying on cloud computing technology, the agricultural economic management information service platform can not only give full play to the township Internet and call rate with high coverage in China, but also realize unlimited capacity storage, fast and high-speed operation and fast and efficient information transmission that cannot be realized by traditional technology without pressure. So as to realize the convenient access and comprehensive sharing of a variety of information resources, and improve the level and quality of agricultural information services.},
  keywords={},
  doi={10.1109/NetCIT57419.2022.00105},
  ISSN={},
  month={Dec},}

@ARTICLE{8937836,
  author={Lin, Weiwei and Wu, Wentai and He, Ligang},
  journal={IEEE Transactions on Services Computing}, 
  title={An On-Line Virtual Machine Consolidation Strategy for Dual Improvement in Performance and Energy Conservation of Server Clusters in Cloud Data Centers}, 
  year={2022},
  volume={15},
  number={2},
  pages={766-777},
  abstract={As data centers are consuming massive amount of energy, improving the energy efficiency of cloud computing has emerged as a focus of research. However, it is challenging to reduce energy consumption while maintaining system performance without increasing the risk of Service Level Agreement violations. Most of the existing consolidation approaches for virtual machines (VMs) consider system performance and Quality of Service (QoS) metrics as constraints, which usually results in large scheduling overhead and impossibility to achieve effective improvement in energy efficiency without sacrificing some system performance and cloud service quality. In this article, we first define the metrics of peak power efficiency and optimal utilization for heterogeneous physical machines (PMs). Then we propose Peak Efficiency Aware Scheduling (PEAS), a novel strategy of VM placement and reallocation for achieving dual improvement in performance and energy conservation from the perspective of server clusters. PEAS allocates and reallocates VMs in an on-line manner and always attempts to maintain PMs working in their peak power efficiency via VM consolidation. Extensive experiments on Cloudsim show that PEAS outperforms several energy-aware consolidation algorithms with regard to energy consumption, system performance as well as multiple QoS metrics.},
  keywords={},
  doi={10.1109/TSC.2019.2961082},
  ISSN={1939-1374},
  month={March},}

@INPROCEEDINGS{9964896,
  author={Jônatas dos Passos, Edenilson and Fiorese, Adriano},
  booktitle={2022 18th International Conference on Network and Service Management (CNSM)}, 
  title={Monitoring Metrics for Load Balancing over Video Content Distribution Servers}, 
  year={2022},
  volume={},
  number={},
  pages={247-253},
  abstract={Cloud computing and video streaming services have been in constant expansion in recent years. Along with it, the demand for computing resources has also increased significantly. In this context, monitoring the use of these resources is crucial to maintain a satisfactory level of Quality of Service and, consequently, Quality of Experience, especially in video transmission services. This work discusses a new method of monitoring resources and quality of service metrics on content servers involving CPU utilization and server throughput, which is obtained in a distributed way. For that, a distributed collector system that is based on a modified version of the ring election algorithm is developed to retrieve the Quality of Service metrics in each server. Evaluation experiment results show that there are no performance gains on the system such as the content loading faster for the user, there are however, improvements in terms of the whole system scalability. The greater the number of servers for monitoring, the better the approach is compared to the traditional method of monitoring resources through request and response.},
  keywords={},
  doi={10.23919/CNSM55787.2022.9964896},
  ISSN={2165-963X},
  month={Oct},}

@INPROCEEDINGS{9793442,
  author={Mangalampalli, Sudheer and Pokkuluri, Kiran Sree and Satish, G. Naga and Swain, Sangram Keshari},
  booktitle={2022 International Conference on Computing, Communication and Power Technology (IC3P)}, 
  title={Effective VM Placement Mechanism in Cloud Computing using Cuckoo Search Optimization}, 
  year={2022},
  volume={},
  number={},
  pages={238-241},
  abstract={Effective VM placement mechanism is needed for Cloud Computing as incoming flow of tasks were dynamic when they are coming onto cloud console. Incoming requests for any cloud console were large in number then there is a chance of decay in quality of service. Quality of service will be degraded when these number of requests were not properly handled i.e. SLA violations and more number of migrations. Quality of Service will be directly affected by these parameters. Many of the authors addressed these metrics but still there is a chance to improve the VM placement in cloud computing. In this paper, we have used a nature inspired algorithm i.e. cuckoo search to design the VM placement strategy and we have used a threshold value to identify the physical host whet her it is overloaded, under loaded or balanced based on the utilization of CPU. Cloudsim toolkit is used as a simulator to conduct simulation and our proposed mechanism minimizes violation of SLA and there is a great improvement in makespan when it was compared with PSO and GA algorithms.},
  keywords={},
  doi={10.1109/IC3P52835.2022.00057},
  ISSN={},
  month={Jan},}

@ARTICLE{9238484,
  author={Mahmoudi, Nima and Khazaei, Hamzeh},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Performance Modeling of Serverless Computing Platforms}, 
  year={2022},
  volume={10},
  number={4},
  pages={2834-2847},
  abstract={Analytical performance models have been leveraged extensively to analyze and improve the performance and cost of various cloud computing services. However, in the case of serverless computing, which is projected to be the dominant form of cloud computing in the future, we have not seen analytical performance models to help with the analysis and optimization of such platforms. In this work, we propose an analytical performance model that captures the unique details of serverless computing platforms. The model can be leveraged to improve the quality of service and resource utilization and reduce the operational cost of serverless platforms. Also, the proposed performance model provides a framework that enables serverless platforms to become workload-aware and operate differently for different workloads to provide a better trade-off between the cost and performance depending on the user's preferences. The current serverless offerings require the user to have extensive knowledge of the internals of the platform to perform efficient deployments. Using the proposed analytical model, the provider can simplify the deployment process by calculating the performance metrics for users even before physical deployments. We validate the applicability and accuracy of the proposed model by extensive experimentation on AWS Lambda. We show that the proposed model can calculate essential performance metrics such as average response time, probability of cold start, and the average number of function instances in the steady-state. Also, we show how the performance model can be used to tune the serverless platform for each workload, which will result in better performance or lower cost without scarifying the other. The presented model assumes no non-realistic restrictions, so that it offers a high degree of fidelity while maintaining tractability at large scale.},
  keywords={},
  doi={10.1109/TCC.2020.3033373},
  ISSN={2168-7161},
  month={Oct},}

@INPROCEEDINGS{10294685,
  author={Bai, Yiming and Chen, Lei and Lei, Ying and Xie, Hongjuan},
  booktitle={2023 5th International Conference on Data-driven Optimization of Complex Systems (DOCS)}, 
  title={A Deep Learning Prediction Approach for Machine Workload in Cloud Computing}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Accurate workload prediction for cloud computing clusters is essential to ensure Quality of Service (QoS), meet Service Level Agreements (SLAs), and minimize energy consumption. In a cloud computing environment, cloud servers collect and store large sets of time series data, including metrics such as CPU usage, network traffic, and disk I/O at each point in time. The fluctuations in these metrics show noticeable temporal correlations. Therefore, these data can be used in time series data prediction models to predict upcoming workload scenarios. However, traditional statistical methods have limitations such as requiring manual feature extraction, high data requirements, and limited generalizability, leading to inaccurate predictions. In addition, most standard deep learning models ignore the problem of sequence noise. To overcome these hurdles, we have created a hybrid deep learning model utilizing advanced techniques. This model integrates Time Convolutional Networks (TCN), Gated Recurrent Units (GRU), and self-attention mechanism to achieve a more accurate workload prediction. Additionally, we perform a comparative analysis of different methods during the preprocessing stage and select the optimal one to effectively remove noise from the original sequences. Experimental results demonstrate that our proposed model outperforms other workload prediction algorithms in terms of prediction accuracy.},
  keywords={},
  doi={10.1109/DOCS60977.2023.10294685},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10140844,
  author={Mishra, Praveen Kumar and Chaturvedi, Amit Kumar},
  booktitle={2023 International Conference on Computational Intelligence, Communication Technology and Networking (CICTN)}, 
  title={Research Challenges in Job Scheduling and Resource Distribution Methodology for Cloud Fog Atmosphere: An Organized Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={292-299},
  abstract={The IoT technology applications have become the most important technologies used internationally to encourage communication between humans and objects and enhance quality of life in recent years. This will lead to an increase in the variety of devices used in such applications, which will produce massive amounts of data. In 2012, Cisco made the first mention of fog computing, which sits between individual consumers (IoT devices) and cloud computing. Fog computing enhances cloud computing’s effectiveness, lessens its drawbacks, and offers storage and processing capacities at the edge, despite the fact that it is not a cloud computing solution. Resource management has a significant role in determining how well fog computing performs. Scheduling is essential to manage supplies in fog layer, which is the capability to match jobs to the suitable useful resources. As we know that the job is a minor component with a bigger undertaking with a deadline. Fog computing leverages distributed and heterogeneous resources, making task scheduling challenging. Numerous recommended scheduling algorithms have been produced in previous years; the maximum of them were engaged in scalability to fog computing. This study’s major goal is to extensively examine and evaluate the most important existing scheduling approaches in terms of their main context, advantages, limitations and performance measures. The analysis is presented in proportion of categories of algorithm and usage of parameters for measure of performance in fog.},
  keywords={},
  doi={10.1109/CICTN57981.2023.10140844},
  ISSN={},
  month={April},}

@INPROCEEDINGS{10196549,
  author={Mokhtari, Ali and Rawls, Drake and Huynh, Tony and Green, Jeremiah and Salehi, Mohsen Amini},
  booktitle={2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={E2C: A Visual Simulator to Reinforce Education of Heterogeneous Computing Systems}, 
  year={2023},
  volume={},
  number={},
  pages={270-277},
  abstract={Heterogeneity has been an indispensable aspect of distributed computing throughout the history of these systems. ln particular, with the increasing popularity of accelerator technologies (eg., GPUs and TPUs) and the emergence of domain-specific computing via ASlCs and FPGA, the matter of heterogeneity and understanding its ramifications on the system performance has become more critical than ever before. However, it is challenging to effectively educate students about the potential impacts of heterogeneity on: (a) the performance of distributed systems; and (b) the logic of resource allocation methods to efficiently utilize the resources. Making use of the real infrastructure (such as those offered by the public cloud providers) for benchmarking the performance of heterogeneous machines, for different applications, with respect to different obiectives, and under various workload intensities is cost- and time-prohibitive. Moreover, not all students (globally and nationally) have access or can afford such real infrastructure. To reinforce the quality of learning about various dimensions of heterogeneity, and to decrease the widening gap in education, we develop an open-source simulation tool, called E2C, that can help students researchers and practitioners to study any type of heterogeneous (or homogeneous) computing system and measure its performance under various system configurations. To make the learning curve shallow, E2C is equipped with an intuitive graphical user interface (GU1) that enables its users to easily examine system-level solutions (scheduling, load balancing, scalability, etc.) in a controlled environment within a short time and at no cost. In particular, E2C is a discrete event simulator that offers the following features: (i) simulating a heterogeneous computing system; (ii) implementing a newly developed scheduling method and plugging it into the system, (iii) measuring energy consumption and other output-related metrics; and (iv) powerful visual aspects to ease the learning curve for students. We used E2C as an assignment in the Distributed and Cloud Computing course. Our anonymous survey study indicates that students rated E2C with the score of 87 out of 10 for its usefulness in understanding the concepts of scheduling in heterogeneous computing. Moreover, our pre- and post-evaluations indicate that E2C has improved the students’ understanding of heterogeneous computing systems by around 18%.},
  keywords={},
  doi={10.1109/IPDPSW59300.2023.00052},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10008330,
  author={Al-Shammare, Haifa and Al-Otaiby, Nehal},
  booktitle={2022 14th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={An Implementation of a New Proposed Round-Robin Algorithm with Smart Time Quantum in Cloud Computing Environment}, 
  year={2022},
  volume={},
  number={},
  pages={289-296},
  abstract={The popularity of cloud computing platforms has risen dramatically in recent years. As cloud computing serves millions of users at the same time, it must be able to handle all those users' demands efficiently. Thus, choosing a suitable scheduling algorithm is crucial in the cloud computing environment in order to ensure efficient performance with a reasonable degree of quality of service (QoS). The primary goal of this research is to empirically implement and evaluate a recently proposed Round-Robin algorithm with smart time quantum (RR-STQ) in a cloud computing environment, as well as, to enhance the RR-STQ with a dynamic smart time quantum. The CloudSim tool was used to simulate the cloud computing platform to implement RR-STQ and evaluate it with several algorithms using different scenarios. In addition, three scheduling performance metrics were used in the evaluation process. In all comparison scenarios, the (RR-STQ) achieved a significant improvement rate in terms of average response time (RT). Moreover, (RR-STQ) has a better performance in the average turnaround time (TAT), waiting time (WT), and response time (RT) than the traditional RR algorithm. Also, the implemented algorithm (RR-STQ) with dynamic time quantum has a better performance than static time quantum. Based on the evaluation results, it is beneficial to integrate the RR algorithm with other scheduling models such as shortest job first (SJF) to enhance the WT and TAT. Furthermore, the investigations revealed that the dynamic time quantum improves the performance of the RR algorithm.},
  keywords={},
  doi={10.1109/CICN56167.2022.10008330},
  ISSN={2472-7555},
  month={Dec},}

@ARTICLE{10274948,
  author={Al-Eidi, Shorouq and Amsaad, Fathi and Darwish, Omar and Tashtoush, Yahya and Alqahtani, Ali and Niveshitha, Niveshitha},
  journal={IEEE Access}, 
  title={Comparative Analysis Study for Air Quality Prediction in Smart Cities Using Regression Techniques}, 
  year={2023},
  volume={11},
  number={},
  pages={115140-115149},
  abstract={In smart cities, air pollution has detrimental impacts on human physical health and the quality of living environment. Therefore, correctly predicting air quality plays an important effective action plan to mitigate air pollution and create healthier and more sustainable environments. Monitoring and predicting air pollution is crucial to empower individuals to make informed decisions that protect their health. This research presents a comprehensive comparative analysis focused on air quality prediction using three distinct regression techniques- Random Forest regression, Linear regression, and Decision Tree regression. The main goal of this study is to discern the most effective model by considering a range of evaluation criteria, including Mean Absolute Error and  $R^{2}$  measures. Moreover, it considers the crucial aspects of minimizing prediction errors and enhancing computational efficiency by evaluating the regression models within two frameworks. The findings of this study underscore the superiority of the Decision Tree regression approach over the other models, demonstrating its exceptional accuracy with a high  $R^{2}$  score and a minimal error rate. Moreover, integrating cloud computing technology has resulted in substantial improvements in the execution time of these approaches. This technology enhancement significantly affects the overall efficiency of the air quality prediction process. By leveraging distributed computing resources, real-time air quality forecasting becomes feasible, enabling timely decision-making and proactive measures to address air pollution episodes effectively.},
  keywords={},
  doi={10.1109/ACCESS.2023.3323447},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{10302780,
  author={Yιlmaz, Ömer Zekvan and Alagöz, Fatih},
  booktitle={2023 14th International Conference on Network of the Future (NoF)}, 
  title={Shared Network Function Placement for Cloud Native 5G Core Networks}, 
  year={2023},
  volume={},
  number={},
  pages={103-107},
  abstract={Providing adequate operating resources for Network Functions (NFs) with minimal costs has been one of the primary concerns for cloud providers. The adequate operating resources definition has to cover the quality of service (QoS) constraints, which result in ‘underutilized resources’. Studies in the NF placement domain that make use of these resources through sharing, employ NP-Hard algorithms. In this study, we propose an NF placement scheme called Cloud Native Network Function Sharing (CNFSH) using a priority-based load balancer and a polynomial time algorithm for dynamic 5G settings. Using CNFSH, the number of satisfied slice requests increases by 36%, and the average resource consumption per network slice drops by 28% compared to the NoShare model. Besides being dynamic and sustainable, CNFSH outperforms a previous sharing scheme [1], with 5% success rate for both of the metrics.},
  keywords={},
  doi={10.1109/NoF58724.2023.10302780},
  ISSN={2833-0072},
  month={Oct},}

@ARTICLE{8758137,
  author={Azadi, Majid and Emrouznejad, Ali and Ramezani, Fahimeh and Hussain, Farookh Khadeer},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Efficiency Measurement of Cloud Service Providers Using Network Data Envelopment Analysis}, 
  year={2022},
  volume={10},
  number={1},
  pages={348-355},
  abstract={An increasing number of organizations and businesses around the world use cloud computing services to improve their performance in the competitive marketplace. However, one of the biggest challenges in using cloud computing services is performance measurement and the selection of the best cloud service providers (CSPs) based on quality of service (QoS) requirements [13]. To address this shortcoming in this article we propose a network data envelopment analysis (DEA) method in measuring the efficiency of CSPs. When network dimensions are taken into consideration, a more comprehensive analysis is enabled where divisional efficiency is reflected in overall efficiency estimates. This helps managers and decision makers in organizations to make accurate decisions in selecting cloud services. In the current study, the non-oriented network slacks-based measure (SBM) model and conventional SBM model with the assumptions of constant returns to scale (CRS) and variable returns to scale (VRS) are applied to measure the performance of 18 CSPs. The obtained results show the superiority of the network DEA model and they also demonstrate that the proposed model can evaluate and rank CSPs much better than compared to traditional DEA models.},
  keywords={},
  doi={10.1109/TCC.2019.2927340},
  ISSN={2168-7161},
  month={Jan},}

@INPROCEEDINGS{10176237,
  author={B K, Jeevitha and J, Thriveni},
  booktitle={2022 IEEE International Conference for Women in Innovation, Technology & Entrepreneurship (ICWITE)}, 
  title={Data Storage Security and Privacy in Cloud Computing}, 
  year={2022},
  volume={},
  number={},
  pages={1-10},
  abstract={The world has seen a quick transition from hard devices for local storage to massive virtual data centers, all possible because of cloud storage technology. Businesses have grown to be scalable, meeting consumer demands on every turn. Cloud computing has transforming the way we do business making IT more efficient and cost effective that leads to new types of cybercrimes. Securing the data in cloud is a challenging task. Cloud security is a mixture of art and science. Art is to create your own technique and technologies in such a way that the user should be authenticated. Science is because you have to come up with ways of securing your application. Data security refers to a broad set of policies, technologies and controls deployed to protect data application and the associated infrastructure of cloud computing. It ensures that the data has not been accessed by any unauthorized person. Cloud storage systems are considered to be a network of distributed data centers which typically uses cloud computing technologies like virtualization and offers some kind of interface for storing data. Virtualization is the process of grouping the physical storage from multiple network storage devices so that it looks like a single storage device.Storing the important data in the cloud has become an essential argument in the computer territory. The cloud enables the user to store the data efficiently and access the data securely. It avoids the basic expenditure on hardware, software and maintenance. Protecting the cloud data has become one of the burdensome tasks in today’s environment. Our proposed scheme "Certificateless Compressed Data Sharing in Cloud through Partial Decryption" (CCDSPD) makes use of Shared Secret Session (3S) key for encryption and double decryption process to secure the information in the cloud. CC does not use pairing concept to solve the key escrow problem. Our scheme provides an efficient secure way of sharing data to the cloud and reduces the time consumption nearly by 50 percent as compared to the existing mCL-PKE scheme in encryption and decryption process.Distributed Cloud Environment (DCE) has the ability to store the da-ta and share it with others. One of the main issues arises during this is, how safe the data in the cloud while storing and sharing. Therefore, the communication media should be safe from any intruders residing between the two entities. What if the key generator compromises with intruders and shares the keys used for both communication and data? Therefore, the proposed system makes use of the Station-to-Station (STS) protocol to make the channel safer. The concept of encrypting the secret key confuses the intruders. Duplicate File Detector (DFD) checks for any existence of the same file before uploading. The scheduler as-signs the work of generating keys to the key manager who has less task to complete or free of any task. By these techniques, the proposed system makes time-efficient, cost-efficient, and resource efficient compared to the existing system. The performance is analysed in terms of time, cost and resources. It is necessary to safeguard the communication channel between the entities before sharing the data. In this process of sharing, what if the key manager’s compromises with intruders and reveal the information of the user’s key that is used for encryption. The process of securing the key by using the user’s phrase is the key concept used in the proposed system "Secure Storing and Sharing of Data in Cloud Environment using User Phrase" (S3DCE). It does not rely on any key managers to generate the key instead the user himself generates the key. In order to provide double security, the encryption key is also encrypted by the public key derived from the user’s phrase. S3DCE guarantees privacy, confidentiality and integrity of the user data while storing and sharing. The proposed method S3DCE is more efficient in terms of time, cost and resource utilization compared to the existing algorithm DaSCE (Data Security for Cloud Environment with Semi Trusted Third Party) and DACESM (Data Security for Cloud Environment with Scheduled Key Managers).For a cloud to be secure, all of the participating entities must be secure. The security of the assets does not solely depend on an individual's security measures. The neighbouring entities may provide an opportunity to an attacker to bypass the user's defences. The data may compromise due to attacks by other users and nodes within the cloud. Therefore, high security measures are required to protect data within the cloud. Cloudsim allows to create a network that contains a set of Intelligent Sense Point (ISP) spread across an area. Each ISPs will have its own unique position and will be different from other ISPs. Cloud is a cost-efficient solution for the distribution of data but has the challenge of a data breach. The data can be compromised of attacks of ISPs. Therefore, in OSNQSC (Optimized Selection of Nodes for Enhanced in Cloud Environment), an optimized method is proposed to find the best ISPs to place the data fragments that considers the channel quality, distance and the remaining energy of the ISPs. The fragments are encrypted before storing. OSNQSC is more efficient in terms of total upload time, total download time, throughput, storage and memory consumption of the node with the existing Betweenness centrality, Eccentricity and Closeness centrality methods of DROPS (Division and Replication of Data in the Cloud for Optimal Performance and Security).},
  keywords={},
  doi={10.1109/ICWITE57052.2022.10176237},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9985529,
  author={Thirunavukkarasu, M. and Shanmugapriya, P.},
  booktitle={2022 4th International Conference on Inventive Research in Computing Applications (ICIRCA)}, 
  title={Energy Efficient Mobile Cloud offloading for Image Processing Applications using Transfer Learning}, 
  year={2022},
  volume={},
  number={},
  pages={910-916},
  abstract={Processing real-time images in the digital era is a complex task that requires the use of more sophisticated tools and consumes more energy. With the introduction of Hybrid model, Mobile and Cloud Computing, called as Mobile and Cloud Computing (MCC), this becomes an easy task in which the end user connects using the virtual instance in the Cloud environment. MCC enable the execution of cloud applications through mobile terminals. However, the mobile devices cannot process high end applications as they have limited energy. We propose Mobile Cloud offloading technique that excludes the execution on the Mobile terminal by deploying the applications on Cloud. Identification and decision on the energy aware applications are playing a vital role to improve Quality of Services (QoS). The proposed model introduces the Transfer Learning- based Energy efficient Mobile Cloud offloading framework. The proposed model predicts whether to move the applications towards the Mobile Cloud environment. The proposed model uses InceptionResNetV2 as pre-trained transfer learning approach.. The CNN based classifier used to classify the type of application and the respective energy consumption ratio based on the pre-trained model. The proposed model reduces training period to identify the application and effectively handle the offload process to improve the QoS metrics.},
  keywords={},
  doi={10.1109/ICIRCA54612.2022.9985529},
  ISSN={},
  month={Sep.},}

@ARTICLE{10247345,
  author={Singh, Jaspreet and Walia, Navpreet Kaur},
  journal={IEEE Access}, 
  title={A Comprehensive Review of Cloud Computing Virtual Machine Consolidation}, 
  year={2023},
  volume={11},
  number={},
  pages={106190-106209},
  abstract={In the last decade, users have been able to access their applications, data, and services via the cloud from any location with an internet connection. The scale of heterogeneous cloud environments is continuously growing due to the development of computing-intensive smart devices. The cloud computing system is managed by a data center, which consists of physical machines (PMs) or servers and software-based emulation of PMs called virtual machines(VMs). The deployment of a huge number of physical servers as a result of the exponential development in demand for cloud services has resulted in high energy consumption and ineffective resource usage. Efficient utilization of resources and minimizing power consumption in any data center have become crucial challenges. Virtual machine consolidation (VMC) is a method of optimizing computing resources by consolidating multiple VMs onto a reduced number of PMs. By consolidating VMs and running fewer physical servers, VM consolidation can reduce power consumption and improve resource utilization. This review paper presents a comprehensive analysis of cloud computing virtual machine consolidation, exploring various strategies, benefits, challenges and future trends in this domain. By examining a wide range of literature from the year 2015 to 2023, this review attempts to provide insight into the current state of VM consolidation and its possible effects on the performance and sustainability of cloud computing. The main flaw in the articles is that the various authors focused on different assessment metrics when the emphasis should have been on increasing cloud system service quality and energy efficiency. Future research can be aimed at developing a multi-objective system that emphasizes minimizing cloud energy usage without sacrificing service quality and preventing service level agreements with cloud users from being compromised.},
  keywords={},
  doi={10.1109/ACCESS.2023.3314613},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9936294,
  author={Das, Souptik and Chakraborty, Sourish and Jana, Dipanjan and Nandy, Rahul and Bhattacharya, Srijan},
  booktitle={2022 Second International Conference on Computer Science, Engineering and Applications (ICCSEA)}, 
  title={IoT Based Industrial Air Quality Monitoring System}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={In the past two decades, researchers have accelerated a number of different methods to monitor and reduce air contamination leading to the development of efficient and effective air quality measuring systems using air purifiers. Although, recent technological advancements such as IoT (Internet of Things) with Cloud Computing have allowed researchers to obtain and monitor real-time data. In this report, an IoT-enabled industrial air quality monitoring device is proposed. This device is enabled with an MQ-135 gas sensor for precisely monitoring the air quality and detecting the presence of foreign contaminants such as alcohol. The proposed device also uses a Node MCU ESP S266 Wi-Fi module to efficiently transmit real time data to a smart device (E.g. Smartphone) using an IoT platform.},
  keywords={},
  doi={10.1109/ICCSEA54677.2022.9936294},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9935836,
  author={Widodo, Doni Iksan and Hartono, Ambran and Yuniarti, Elvan},
  booktitle={2022 10th International Conference on Cyber and IT Service Management (CITSM)}, 
  title={Design and Build End-to-End Device as User Recommendations for Indoor Air Quality Monitoring}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={The development of integrated end-to-end technology is increasingly reaching all fields. The presence of this technology can answer one of the challenges discussed in this study, namely as a recommendation feature to users on indoor air quality and useful as an effort to prevent respiratory disease symptoms. This device is specially designed by implementing several technologies such as microcontrollers, sensors, internet of things, cloud computing, and progressive web apps. The parameters used to measure the level of indoor air quality are temperature, humidity, fine particles (PM10) and carbon monoxide with a predetermined range, if these parameters are outside the range then it is categorized as poor or unhealthy air, otherwise if the parameters are within range, the air is categorized as good or healthy air. Based on the collection and processing of data samples at several different locations, the air is in the good or healthy category and does not cause symptoms of respiratory disease. The results of this study also produced an end-to-end device that can monitor indoor air quality levels in real-time, as well as provide recommendations for making health efforts if the detected air quality parameters are outside the range that has been determined based on the Ministry of Health regulations Republic of Indonesia.},
  keywords={},
  doi={10.1109/CITSM56380.2022.9935836},
  ISSN={2770-159X},
  month={Sep.},}

@INPROCEEDINGS{10047689,
  author={Vidhya, M. and Devi, R.},
  booktitle={2022 11th International Conference on System Modeling & Advancement in Research Trends (SMART)}, 
  title={Comparative Analysis of Scheduling Algorithms in Cloud Computing using CloudSim}, 
  year={2022},
  volume={},
  number={},
  pages={7-12},
  abstract={Cloud computing restructured the entire world of IT by providing shared scalable resources to the organization. Cloud establishes a huge path to find the solution for many major problems found in the industry. Cloud offers virtualized resources such as applications, software, networks, servers, and storage services. Cloud enables all the virtualized resources to clients on a pay-per-use basis. Because of handling the vast request sent by multiple clients and providing more versatile services to the organization, the cloud faces many critical problems such as security issues, dissatisfied Quality-Of-Services, and sometimes an unbalanced load arises. Among those, the most serious problem is balancing the load across the network. Load balancing issues can be handled by scheduling the work eventually to all nodes without overloading any single node. This paper gives an overview idea of different load balancing algorithms and provides comparative results on its quality metrics.},
  keywords={},
  doi={10.1109/SMART55829.2022.10047689},
  ISSN={2767-7362},
  month={Dec},}

@INPROCEEDINGS{9995753,
  author={Al-Sit, Waleed T. and Ateeq, Karamath and Moinuddin, Syed Quadir and Aslam, Shoukat and Rehman, Abdur and Asgher, Tayba and Thawabeh, Ossma Ali},
  booktitle={2022 International Conference on Cyber Resilience (ICCR)}, 
  title={Direct Trust in Cloud Computing Based on Fuzzy Logic}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Trust is a powerful aspect, particularly for services using processes in the fields of cybersecurity and information technology. Concerns concerning the dependability of the cloud platform have been raised by individuals and creativity on a number of occasions. In cloud computing, it is advantageous for the customer to select a cloud provider's service for the storing and distribution of their thoughtful content. In this research, a trust model is put forth that evaluates trust using Quality of Service (QoS) metrics. The trust-fuzzy environment has inspired us to employ fuzzy logic to estimate a beneficiary's trustworthiness in a cloudy environment, hence increasing the scheme's effectiveness. Reliability, security, accessibility, response time, and cost make up Quality of Service. All work was completed in MATLAB.},
  keywords={},
  doi={10.1109/ICCR56254.2022.9995753},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9987392,
  author={Kumar, G Suvarna and Priyadarshini, R. and Parmenas, Naik Henokh and Tannady, Hendy and Rabbi, Fazle and Andiyan, Andiyan},
  booktitle={2022 Sixth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Design of Optimal Service Scheduling based Task Allocation for Improving CRM in Cloud Computing}, 
  year={2022},
  volume={},
  number={},
  pages={438-445},
  abstract={Cloud computing is a service level computing that provide various service to the customers in order to establish an effective customer Relationship management (CRM). This offers various services like virtual machine, self-service provisioning, elasticity computing and storage (pay-as-you-go). Cloud computing provides shared services by enhancing resource management scalability, interoperability and prediction resources as a key to realize the resource utilization with high-performance management metrics. However, when the number of users increase, the process of task scheduling and service allocation using a traditional computing environment will degrade the CRM. In order to address this challenge, this research study proposes an Optimal Service Level Scheduling (OSLS) based task allocation design for improving CRM in cloud computing environment. The Adaptive Service Level Scheduling Algorithm (ASLSA) and the Support Level Load Balancer (SLLB) will reduce the workload in cloud computing environment in order to improve the Quality of Service (QoS) of CRM. This process will optimize the resource utilization in cloud platform based on the service requirement. It provides optimal scheduling features to CRM in order to improve the service optimality based on task and enhance the computational processes such as service load management, heterogeneous service delivery, pricing, resource pools and elasticity. The proposed system leverages high performance when compared to the existing models.},
  keywords={},
  doi={10.1109/I-SMAC55078.2022.9987392},
  ISSN={2768-0673},
  month={Nov},}

@INPROCEEDINGS{10105030,
  author={Sandhiya, B and Canessane, R.Aroul},
  booktitle={2023 International Conference on Sustainable Computing and Data Communication Systems (ICSCDS)}, 
  title={An Extensive Study of Scheduling the Task using Load Balance in Fog Computing}, 
  year={2023},
  volume={},
  number={},
  pages={1586-1593},
  abstract={The proliferation of IoT has resulted in a rise in the demand for services provided by the fog layer, a novel dispersed computing pattern that supplements cloud computing. The fog system enables location awareness and mobility assistance by extending storage and multiplication to the network’s edge, dramatically reducing the issue of service computing in delay-sensitive applications. More requests from more users means more stress for the VMs running in the fog layer. When it comes to fog networks, Load Balancing (LB) is crucial since it prevents some fog nodes from being under- or overworked. Fairly dividing up the fog layer’s burden across the available virtual machines (VMs) is now an absolute must. LB can enhance quality-of-service metrics including cost, response time, performance, and energy ingesting. Although there has been limited investigation of load complementary techniques in fog networks in recent years, no comprehensive analysis has been conducted to compile this information. This article takes a systematic look at the various load-balancing procedures in fog computing, categorizing it as either approximate, precise, fundamental, or hybrid. In addition, the study explores (Load Balancing) LB metrics, including the benefits and drawbacks of the techniques used for fog networks. There is also an examination of the methods and instruments used in the aforementioned evaluations of each research under consideration. The most unanswered questions and emerging tendencies for these algorithms are also covered. In the final section, the study suggests potential avenues for further research.},
  keywords={},
  doi={10.1109/ICSCDS56580.2023.10105030},
  ISSN={},
  month={March},}

@ARTICLE{10041775,
  author={Singh, Jagdeep and Singh, Parminder and Hedabou, Mustapha and Kumar, Neeraj},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={An Efficient Machine Learning-Based Resource Allocation Scheme for SDN-Enabled Fog Computing Environment}, 
  year={2023},
  volume={72},
  number={6},
  pages={8004-8017},
  abstract={Fog computing is an emerging technology which enables computing resources accessibility close to the end-users. It overcomes the drawbacks of available network bandwidth and delay in accessing the computing resources as observed in cloud computing environment. Resource allocation plays an important role in resource management in a fog computing environment. However, the existing traditional resource allocation techniques in fog computing do not guarantee less execution time, reduced energy consumption, and low latency requirements which is a pre-requisite for most of the modern fog computing-based applications. The complex fog computing environment requires a robust resource allocation technique to ensure the quality and optimal resource usage. Motivated from the aforementioned challenges and constraints, in this article, we propose a resource allocation technique for SDN-enabled fog computing with Collaborative Machine Learning (CML). The proposed CML model is integrated with the resource allocation technique for the SDN-enabled fog computing environment. The FogBus and iFogSim are deployed to test the results of the proposed technique using various performance evaluation metrics such as bandwidth usage, power consumption, latency, delay, and execution time. The results obtained are compared with other existing state-of-the-art techniques using the aforementioned performance evaluation metrics. The results obtained show that the proposed scheme reduces 19.35% processing time, 18.14% response time, and 25.29% time delay. Moreover, compared to the existing techniques, it reduces 21% execution time, 9% network usage, and 7% energy consumption.},
  keywords={},
  doi={10.1109/TVT.2023.3242585},
  ISSN={1939-9359},
  month={June},}

@INPROCEEDINGS{10070735,
  author={Kong, Xiangyu and Gao, Xuesong and Pan, Shibao and Zhou, Yizhi and Yang, Yanan and Zhao, Laiping and Qi, Heng},
  booktitle={2022 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)}, 
  title={TailCmp - A Tail Latency Evaluation Solution of Public Cloud and Labeled von Neumann Architecture based Cloud Prototype}, 
  year={2022},
  volume={},
  number={},
  pages={888-895},
  abstract={Tail latency is the key performance metric for cloud computing systems (CCSs). When latency-critical (LC) applications and best-effort batch (BE) jobs are co-located on CCSs, the unmanaged contention for computing resources usually leads to significant fluctuations in tail latency, which have a severe negative impact to the end user experience. Therefore, some promising computing systems and effective scheduling solutions are proposed to guarantee better quality of service (QoS), e.g., La-beled von Neumann Architecture (LvNA) based cloud prototype system. However, relatively few work focuses on the tail latency observation and evaluation of public CCSs and newest prototype systems. Therefore, we introduce an evaluation framework named TailCmp which includes nine representative workloads ranging over different tail latency requirements and application domains. With the TailCmp, we can collect and analyze four evaluation metrics including tail latency entropy. In this paper, we conduct a large number of experiments with TailCmp on three CCSs (two public CCSs and one LvNA-based prototype system). The results show that the co-location on existing public CCSs can seriously affect the QoS of LC workloads, while the labeling mechanism in Lv Na - based prototype can improve the performance of LC workloads in co-location.},
  keywords={},
  doi={10.1109/ISPA-BDCloud-SocialCom-SustainCom57177.2022.00118},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{10085328,
  author={Tiwana, Pardeep Singh and Singh, Jaspreet},
  booktitle={2023 International Conference on Artificial Intelligence and Smart Communication (AISC)}, 
  title={Load Optimization Accessions, Ramification the QoS in Software Defined Networking}, 
  year={2023},
  volume={},
  number={},
  pages={1013-1017},
  abstract={It is difficult to manage traditional networks due to network growth, an increase in user numbers, and the emergence of new technologies like big data and cloud computing. Consequently, it is required to alter the current network design. SDN is a well-liked architecture because it offers customizable control and centralized management in data centers. To achieve scalability and dependability, it was necessary to propose the geographical dispersal of a logically centralized control plane due to the huge scale of networks. Software-defined networking (SDN) load-balancing optimization has been studied for a long time. Many approaches to the load-balancing conundrum have been put forth by researchers, but very few have taken the impact of transmission delay between controllers and switches under heavy network load into account. In this article we elaborate the various techniques of load balancing in SDN those working on different areas like multi-controller, switch migration, multi-agent, strategy, time sharing, prediction, reinforcement learning etc. We discuss the methods used along with the advantages and disadvantages of various costs, service quality, and network performance metrics.},
  keywords={},
  doi={10.1109/AISC56616.2023.10085328},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{10112423,
  author={Srivastava, Manoj Kumar and Joshi, Vijay K.},
  booktitle={2023 10th International Conference on Computing for Sustainable Global Development (INDIACom)}, 
  title={Efficient Consolidation of VMs Systems in the Cloud to Reduce Energy Use}, 
  year={2023},
  volume={},
  number={},
  pages={1510-1514},
  abstract={The energy needs of cloud computing systems are very high. Cloud suppliers, in order to keep their services available, need to reduce the amount of power their platforms need without sacrificing quality of service. As a result, studies have recommended a cloud-specific architecture that optimizes energy use throughout the whole computer infrastructure. The suggested method was developed in the CloudSim simulator, and the results of the associated simulations suggest that power consumption may be substantial and varies depending on parameters like the quantum variable, data size, and the number of VMs operating on a host. It is possible to establish a variety of resource allocation and planning methods for amassing virtual machines (VMs) on fewer hosts while still maintaining critical metrics, making cloud technology the first step towards sustainable energy. In this study, we first outline the taxonomy of VM placement approaches before proposing new iterative placement strategies that dynamically adjust their placement judgments based on host load Swarm Bee inspired improved Threshold.},
  keywords={},
  doi={},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10250648,
  author={Kavitha, J and Rao, P S V Srinivasa and Babu, G Charles},
  booktitle={2023 Second International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)}, 
  title={Energy Efficient Resource Utilization of Cloud Computing Environments for Deployment Models}, 
  year={2023},
  volume={},
  number={},
  pages={1111-1119},
  abstract={This research study focuses on optimizing resource allocation to reduce energy consumption in cloud-based systems. This study explores various techniques and algorithms employed to achieve energy efficiency, such as dynamic workload consolidation, virtual machine migration, and power-aware scheduling. This study emphasizes the importance of considering energy efficiency alongside performance metrics while making resource management decisions. The proposed methods intend to minimize energy wastage and carbon footprint while maintaining Quality of Service (QoS) and cost-effectiveness. Ultimately, this research contributes to the sustainable development and greener operation of cloud computing infrastructures.},
  keywords={},
  doi={10.1109/ICAISS58487.2023.10250648},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{10366486,
  author={Saadouni, Rafika and Khacha, Amina and Harbi, Yasmine and Gherbi, Chirihane and Harous, Saad and Aliouat, Zibouda},
  booktitle={2023 15th International Conference on Innovations in Information Technology (IIT)}, 
  title={Secure IIoT networks with hybrid CNN-GRU model using Edge-IIoTset}, 
  year={2023},
  volume={},
  number={},
  pages={150-155},
  abstract={Industrial Internet of Things (IIoT), or Industry 4.0, is an application of IoT in the industrial sector. Its main objective is to enhance product quality and optimize production costs by leveraging advanced technologies such as edge/fog/cloud computing, 5G/6G, and artificial intelligence. In the context of Industry 4.0, numerous devices and systems are interconnected to provide seamless services to users. However, with this interconnection comes the need to protect these devices and the information they transmit from cyberthreats and intrusions. In order to tackle this challenge, our proposed solution involves the utilization of deep learning (DL) models to develop an anomaly-based detection system. Our approach involves two powerful DL models, namely Convolutional Neural Networks (CNN) and Gated Recurrent Units (GRU). The proposed model’s performance is studied within binary and multiclass classification using a new real-world industrial traffic dataset called Edge-IIoTset. The outcomes of our experiments showcased the efficacy of the CNN-GRU model that we proposed, surpassing the performance of recent related works in terms of performance metrics, including accuracy, precision, false positive rate, and detection cost. The combination of the two models CNN and GRU outperforms the GRU model with 88% of detection cost in multiclass classification for one traffic flow.},
  keywords={},
  doi={10.1109/IIT59782.2023.10366486},
  ISSN={2473-2052},
  month={Nov},}

@INPROCEEDINGS{10087523,
  author={Khayyat, Mashael M. and Aboulola, Omar A.},
  booktitle={2023 International Conference on Smart Computing and Application (ICSCA)}, 
  title={Implementing an Ambient Air Quality Monitoring System in Spaces with Inadequate Ventilation using the Internet of Things}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Improving quality of life can be one of the most important outcomes of health care policies. Researchers have proven the role of technology in enhancing the quality of life. The Internet of Things (IoT) concept employs technology in an integrated manner by connecting smart devices, sensors, data, and channels, in order to enhance communication between things and to facilitate life in a smart way. Thus, IoT utilizes a great amount of data coming from these devices and produces stories and clear pictures about what is going on especially since the emergence of big data and cloud computing paradigms. The objective of this paper is to introduce IoT, explain how to calculate the Air Quality Index for Gulf Cooperation Council countries, and to provide evidence of the importance of implementing air quality monitoring systems in tunnels since their architecture limits airflow; therefore it can be assumed that pollutants are trapped. Results from the data generated by the system implemented measures air quality in the tunnel on King Abdulaziz Road in Makkah that shows air quality in such spaces should be monitored and proper actions need to be considered to avoid health problems to ensure quality of life.},
  keywords={},
  doi={10.1109/ICSCA57840.2023.10087523},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{9928755,
  author={Li, Shuo and Baştuğ, Ejder and Di Renzo, Marco},
  booktitle={2022 IEEE International Mediterranean Conference on Communications and Networking (MeditCom)}, 
  title={On the Modelling and Analysis of Edge-Serverless Computing}, 
  year={2022},
  volume={},
  number={},
  pages={250-254},
  abstract={The exponential increase in data generated by mobile devices and the rapid growth of cloud computing traffic are pushing services to the edge of the network. Serverless computing is an emerging cloud computing paradigm that provides stateless and event-driven services, called serverless services, independent of network infrastructure. Further improvements in quality of service (QoS) can be achieved by utilizing novel concepts like edge-serverless computing. Analyzing such systems as in existing works requires network-level simulations which could be costly and time-consuming, thus analytical models to obtain rapid insights are also needed. In this paper, we propose a stochastic geometry-based mathematical model in a tiered network, characterizing average end-to-end delay as performance measure, impacted by various crucial network parameters. The spatial distribution of server nodes, as well as computation requests are randomly and independently distributed according to homogeneous Poisson point processes (PPPs), allowing us to study the performance gains from multiple aspects. Two distinct network topologies are investigated numerically, namely communication and computation-oriented, validating the accuracy of our theoretical approximations.},
  keywords={},
  doi={10.1109/MeditCom55741.2022.9928755},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10326297,
  author={Yekta, Mohammad and Shahhoseini, Hadi Shahriar},
  booktitle={2023 13th International Conference on Computer and Knowledge Engineering (ICCKE)}, 
  title={A Review on Machine Learning Methods for Workload Prediction in Cloud Computing}, 
  year={2023},
  volume={},
  number={},
  pages={306-311},
  abstract={Workload prediction is one of the critical parts of resource provisioning in cloud computing and its evolved branches such as serverless and edge computing. Effective resource provisioning stands as a crucial element within the realm of edge-cloud computing. Accurate prediction of cloud workloads is essential for the effective allocation of resources. Workload prediction plays a crucial role in enhancing efficiency, reducing costs, optimizing cloud performance, maintaining a high level of quality of service, and minimizing energy consumption. In this paper, we conduct a comprehensive review of state-of-the-art Machine Learning (ML) and Deep Learning (DL) algorithms employed in workload prediction in cloud computing and other similar platforms such as edge computing. We compared the selected papers in terms of utilized methods and techniques, predicted factors, accuracy metrics, and the dataset. Additionally, to facilitate usability and comparison, articles sharing similar advantages and disadvantages are organized into a table. Finally, the paper concludes by addressing current challenges and future research directions.},
  keywords={},
  doi={10.1109/ICCKE60553.2023.10326297},
  ISSN={2643-279X},
  month={Nov},}

@INPROCEEDINGS{10054247,
  author={M, Gayathri Hegde and M, Shrishti Bekal and Prasad, Srinidhi S and Shenoy, P Deepa and K R, Venugopal},
  booktitle={2022 IEEE 7th International Conference on Recent Advances and Innovations in Engineering (ICRAIE)}, 
  title={Analysis of Secure EHR Storage Methods on Blockchain and Integrating ML to Predict Chronic Kidney Disease}, 
  year={2022},
  volume={7},
  number={},
  pages={250-255},
  abstract={An Electronic Health Record(EHR) is the digital form of the patient’s health data, including vital signs, demographic details, clinical notes, X-rays, medical images, etc. When discussing EHR processing and management, storing this vast information is the main challenge. Although cloud computing’s centralized storage model is one of the best ways to store large amounts of data, it is not secure. Blockchain technology has the potential to revolutionize EHR storage systems and provide data security. Directly keeping the vast EHR data on blockchain may be costly and inefficient. The best method to store the data is off-chain in a Blockchain-based EHR system. Implementing machine learning (ML) in the healthcare industry aims to anticipate diseases earlier so patients can receive higher-quality medical care. Integrating these two disruptive data-driven technologies can highly improve the quality of healthcare. First, this paper analyzes three different off-chain EHR storage methods: Storj, InterPlanetary File System(IPFS), and CosmosDB, based on their Storage and Access Time. From the experimental results, IPFS has the fastest Storage and Access Time. Second, we integrate blockchain and ML technology to predict Chronic Kidney Disease(CKD) from the CKD dataset stored on the IPFS. Finally, using IPFS storage, a record is accessed, and the prediction time is 0.4 sec for detecting CKD or NOCKD is measured.},
  keywords={},
  doi={10.1109/ICRAIE56454.2022.10054247},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{10176696,
  author={Sefati, Seyed Salar and Halunga, Simona},
  booktitle={2023 12th International Conference on Modern Circuits and Systems Technologies (MOCAST)}, 
  title={Service Recommendation for a Group of Users on the Internet of Things Using the Most Popular Service}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={With the emergence of the Internet of Things (IoT), advanced technologies like fog and cloud computing have been harnessed to create dynamic, real-time platforms addressing the needs of modern decision-makers. Crucial to this process is the recommendation of services tailored to each user's requirements in IoT settings, with the potential for improved Quality of Service (QoS) and Quality of Experience (QoE). The presented method in this paper leverages sensors, services, and fog computing within IoT systems to enhance QoS and adapt to user feedback. The approach involves ranking QoS of services based on Reliability, Availability, and Cost (RAC), and identifying the Most Popular Service (MPS) previously selected by the user. Comparison with Co-Scheduling System for Fog-node Recommendation and Load Management (CoS_FRLM) and User Characteristics-Collaborative Filtering (UCCF) demonstrates our method's effectiveness in maximizing recall, precision, and f-measure, as tested with the Network Simulator (NS3).},
  keywords={},
  doi={10.1109/MOCAST57943.2023.10176696},
  ISSN={},
  month={June},}

@ARTICLE{9508125,
  author={Qin, Zhenquan and Ye, Jin and Meng, Jie and Lu, Bingxian and Wang, Lei},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Privacy-Preserving Blockchain-Based Federated Learning for Marine Internet of Things}, 
  year={2022},
  volume={9},
  number={1},
  pages={159-173},
  abstract={The marine Internet of things (MIoT) is the application of the Internet of things technology in the marine field. Nowadays, with the arrival of the era of big data, the MIoT architecture has been transformed from cloud computing architecture to edge computing architecture. However, due to the lack of trust among edge computing participants, new solutions with higher security need to be proposed. In the current solutions, some use blockchain technology to solve data security problems while some use federated learning technology to solve privacy problems, but these methods neither combine with the special environment of the ocean nor consider the security of task publishers. In this article, we propose a secure sharing method of MIoT data under an edge computing framework based on federated learning and blockchain technology. Combining its special distributed architecture with the MIoT edge computing architecture, federated learning ensures the privacy of nodes. The blockchain serves as a decentralized way, which stores federated learning workers to achieve nontampering and security. We propose a concept of quality and reputation as the metrics of selection for federated learning workers. Meanwhile, we design a quality proof mechanism [proof of quality (PoQ)] and apply it to the blockchain, making the edge nodes recorded in the blockchain more high-quality. In addition, a marine environment model is built in this article, and the analysis based on this model makes the method proposed in this article more applicable to the marine environment. The numerical results obtained from the simulation experiments clearly show that the proposed scheme can significantly improve the learning accuracy under the premise of ensuring the safety and reliability of the marine environment.},
  keywords={},
  doi={10.1109/TCSS.2021.3100258},
  ISSN={2329-924X},
  month={Feb},}

@INPROCEEDINGS{10011594,
  author={Abbasi, Rabiya and Martinez, Pablo and Ahmad, Rafiq},
  booktitle={2022 10th International Conference on Control, Mechatronics and Automation (ICCMA)}, 
  title={Data Acquisition and Monitoring Dashboard for IoT Enabled Aquaponics Facility}, 
  year={2022},
  volume={},
  number={},
  pages={168-172},
  abstract={Aquaponics is an emerging field of agriculture that has great potential to ensure food security and sustainability. It presents a closed-loop ecosystem that synergizes fish farming (aquaculture) and soilless plant growing (hydroponics). As aquaponics is a simulation of a natural ecosystem, a significant number of factors are involved in its management. For instance, constant monitoring of water quality, environmental parameters, and illumination is required to achieve healthy growth of fish and plants. In combination with cloud computing technology, the internet of things (IoT) provides a platform to collect data, monitor systems, detect abnormal conditions, and rectify problems in the system without human intervention. This research project presents a framework that involves the development of a cloud-based dashboard for data acquisition and monitoring of an aquaponics facility. The data from a wireless sensing module (measuring pH, water temperature, electroconductivity, light intensity, air humidity, and air temperature) and several cameras installed in an aquaponics farm are uploaded wirelessly to the dashboard. These images are saved on the dashboard with their relevant sensor measurements. The user (farmer or aquaponics practitioner) can get real-time insights regarding the farm’s performance with this information. It can also be used for the creation of future smart applications that can perform and showcase short and long-term predictions.},
  keywords={},
  doi={10.1109/ICCMA56665.2022.10011594},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9820438,
  author={Chango - Cañaveral, Patricia Marisol and Esperanza Jaya- Jaramillo, Doris and Quezada- Sarmiento, Pablo Alejandro and Teodomiro Salas - Álvarez, Wilson},
  booktitle={2022 17th Iberian Conference on Information Systems and Technologies (CISTI)}, 
  title={Analysis of the Quality Service of the Hotel Villa Colonial through the Servqual method and Cloud Computing tools}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={The objective of this article was to analyze the quality service through the Servqual model at the Villa Colonial Hotel in the Malacatos parish. The methodology used established as an object of study the application of the Servqual model, which allowed measuring customer service satisfaction, which is made up of five dimensions such as: reliability, responsiveness, security, empathy, and tangible elements. The information was collected through the application of structured interviews to 218 guests in two phases: in the first phase, data was acquired regarding the expectation of the services offered by the hotel and in the second phase, data was collected on the perception of the services provided. In the evaluation of the gap, it was possible to identify 11 shortcomings in customer service, ¿the most punctuated deficiency is item 16 Does the staff care about satisfying the needs of the clients? the perception does not exceed the expectations of the client and is qualified as dissatisfied. The results obtained were used to design an improvement plan to overcome the shortcomings identified in the hotel. Finally, it should be noted that the entire development and implementation process was supported with cloud computing tools.},
  keywords={},
  doi={10.23919/CISTI54924.2022.9820438},
  ISSN={2166-0727},
  month={June},}

@INPROCEEDINGS{9915888,
  author={Wen, Shilin and Deng, Hongjie and Qiu, Ke and Han, Rui},
  booktitle={2022 IEEE International Conference on Sensing, Diagnostics, Prognostics, and Control ( SDPC)}, 
  title={EdgeCloudBenchmark: A Benchmark Driven by Real Trace to Generate Cloud-Edge Workloads}, 
  year={2022},
  volume={},
  number={},
  pages={377-382},
  abstract={With the rapid development of 5G and IoT technology, edge computing, as an extension of the cloud computing paradigm, has been widely used to handle some latency-sensitive tasks. Due to insufficient and limited resource of edge devices, when the edge handles some complex tasks, it is often necessary to cooperate with the cloud, which forms the cloud-edge collaboration scenarios. In real cloud-edge collaboration cluster, different scheduling algorithms will greatly affect the resource allocation and workload completion time. Therefore, how to measure the quality of a scheduling algorithm has become critical. However, there is no existing benchmark test sets for such scenarios at present. Based on this problem, this paper proposes EdgeCloudBenchmark, which is a benchmark generation system driven by real Alibaba cluster trace. In this system, we can generate two different benchmark test sets for CPU cluster and GPU cluster, respectively. The experimental results show that these workloads generated from the proposed system can maintain the consistency with the characteristics of the real cluster workloads, and are highly available. Therefore, our proposed system has high concurrency, availability and fault tolerance.},
  keywords={},
  doi={10.1109/SDPC55702.2022.9915888},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9779020,
  author={Duan, Danping and Xiang, Chaoyang},
  booktitle={2022 10th International Conference on Information and Education Technology (ICIET)}, 
  title={The Design and Implementation of Virtual Simulation Teaching Resource Management and Sharing Platform}, 
  year={2022},
  volume={},
  number={},
  pages={11-15},
  abstract={In China, the construction and sharing of virtual simulation teaching resources is an important measure to improve the quality of practical teaching. However, due to various factors, there are still many difficulties in the integrated management and open sharing of virtual simulation teaching resources. Therefore, this paper analyzes the problems existing in the management and sharing of virtual simulation teaching resources, and puts forward the mode of virtual simulation teaching resources management and sharing based on the network support platform. Then, the design idea of virtual simulation teaching resource management sharing platform is elaborated, and the layered architecture is adopted, and the overall framework of the platform is designed by integrating virtual reality, cloud computing, big data and other technologies. Finally, the platform is realized and the functions and applications of the four core modules, portal service, VR resource service, VR project teaching and statistical analysis, are described in detail. This paper can provide reference for other universities to develop the sharing platform of virtual simulation teaching resources management.},
  keywords={},
  doi={10.1109/ICIET55102.2022.9779020},
  ISSN={},
  month={April},}

@INPROCEEDINGS{10212187,
  author={Thakur, Rahul},
  booktitle={2023 2nd International Conference on Edge Computing and Applications (ICECAA)}, 
  title={Bioinspired Sensory Gating in the Artificial Vision System}, 
  year={2023},
  volume={},
  number={},
  pages={1163-1167},
  abstract={When viewed as an enterprise, the healthcare system has important security and privacy requirements, such as protecting patient medical records from unauthorized access, tracking protected drugs, securely connecting to transportation such as ambulances, and conducting secure and intelligent e-health surveillance. Blockchain has introduced new ideas in medical data security and safety, and with the right security measures, it may eliminate the tension between sharing data and maintaining anonymity. In this study, the benefits of cloud computing are integrated with blockchain to offer a secrecy technique for blockchain and IoT. In addition to incorporating IoT and providing IoT solutions to blockchain nodes, this method also collects, analyzes, uses, and maintains identity authentication for health information. Interface and overcomes the insufficient computer power of particular blockchain chain routers to verify the data's feasibility and authenticity order to verify the feasibility and authenticity of the data. The simulation experiment shows that the suggested technique is effective. It may handle problems like high computer intricacy, data interchange, and privacy protection while maintaining and confirming the quality of medical data.},
  keywords={},
  doi={10.1109/ICECAA58104.2023.10212187},
  ISSN={},
  month={July},}

@INPROCEEDINGS{10263944,
  author={Srivastava, Arun Pratap and Madan, Parul and Sharma, Gunjan and Shrivastava, Anurag and Singh, Yograj and Salim, Mohd.},
  booktitle={2023 IEEE World Conference on Applied Intelligence and Computing (AIC)}, 
  title={Bioinspired Sensory Gating in the Artificial Vision System}, 
  year={2023},
  volume={},
  number={},
  pages={735-739},
  abstract={When viewed as an enterprise, the healthcare system has important security and privacy requirements, such as protecting patient medical records from unauthorized access, tracking protected drugs, connecting to transportation such as ambulances securely, and conducting secure and intelligent e-health surveillance. Block chain has introduced new ideas in medical data security and safety, and with the right security measures, it may eliminate the tension between sharing data and maintaining anonymity. In this study, we integrate the benefits of cloud computing with block chain to offer a secrecy technique for block chain and IoT. In addition to incorporating IoT and providing Iot solutions to block chain nodes, this method also collects, analyzes, uses, and maintains identity authentication for health information. Interface and overcomes the insufficient computer power of particular blockchainchain routers to verify the data's feasibility and authenticity order to verify the feasibility and authenticity of the data. The simulation experiment shows that the suggested technique is effective. It may handle problems like high computer intricacy, data interchange, and privacy protection while maintaining and confirming the quality of medical data.},
  keywords={},
  doi={10.1109/AIC57670.2023.10263944},
  ISSN={},
  month={July},}

@ARTICLE{9707879,
  author={Huang, Siqi and Xie, Jiang and Muslam, Muhana Magboul Ali},
  journal={IEEE Transactions on Cloud Computing}, 
  title={A Cloud Computing Based Deep Compression Framework for UHD Video Delivery}, 
  year={2023},
  volume={11},
  number={2},
  pages={1562-1574},
  abstract={Ultra-high-definition (UHD) videos are enjoying increased popularity in people's daily usage because of the good visual experience. However, the data size of UHD videos is 4-16 times larger of HD videos. This will bring many challenges to existing video delivery systems, such as the shortage of network bandwidth resources and longer network transmission latency. In this article, we propose a cloud computing based deep compression framework named Pearl, which utilizes the power of deep learning and cloud computing to compress UHD videos. Pearl compresses UHD videos from two respects: the frame resolution and the colorful information. In pearl, an optimal compact representation of the original UHD video is learned with two deep convolutional neural networks (DCNNs): super resolution CNN (SR-CNN) and colorization CNN (CL-CNN). SR-CNN is used to reconstruct a high resolution video from a low resolution video while CL-CNN is adopted to preserve the color information of the video. Pearl focuses on video content compression in two new directions. Thus, it can be integrated with any existing video compression system. With Pearl, the data size of UHD videos can be significantly reduced. We evaluate the performance of Pearl with a wide variety of network conditions, quality of experience (QoE) metrics, and video properties. In all considered scenarios, Pearl can further compress 84% of video size and reduce 73% of network transmission latency.},
  keywords={},
  doi={10.1109/TCC.2022.3149420},
  ISSN={2168-7161},
  month={April},}

@INPROCEEDINGS{10212311,
  author={Lokesh, Gudivada and Baseer, K. K.},
  booktitle={2023 2nd International Conference on Edge Computing and Applications (ICECAA)}, 
  title={An Architecture for Dynamic Load Balancing in Cloud Environment}, 
  year={2023},
  volume={},
  number={},
  pages={84-91},
  abstract={Clouds are highly customizable infrastructures that offer a platform as a service and let customers subscribe on a pay-as-you-go basis to their requirements. The straightforward service-oriented cloud computing model is gaining popularity around the world. The number of people using the Cloud is constantly growing. Clouds use modern data centers to manage a massive number of users. The reliability of the Cloud depends on load balancing. Balancing virtual machine loads lowers energy consumption and task rejections by optimizing resource utilization. One can increase performance while using fewer resources using load balancing, resource management, quality of service, etc. The difficulty of overloading and underloading virtual machines in cloud computing can be lessened by load balancing in the Cloud.This research study thoroughly examined the load-balancing algorithms found in the literature. First, the traditional approaches are analyzed before moving on to more recent work on load balancing with heterogeneous techniques. Along with the tools available for the current investigation, various metrics are used to evaluate the load-balancing algorithms. The proposed article will primarily serve to assist in the development of new algorithms in the future.},
  keywords={},
  doi={10.1109/ICECAA58104.2023.10212311},
  ISSN={},
  month={July},}

@INPROCEEDINGS{10189468,
  author={Chen, Fan and Du, Yugen and Zhong, Wenhao and Wang, Hanting},
  booktitle={2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)}, 
  title={Web Service QoS Prediction Based on Reputation and Location Aware Matrix Factorization}, 
  year={2022},
  volume={},
  number={},
  pages={1722-1729},
  abstract={With the development of cloud computing and Internet technologies, the number of Web services has increased dramatically. It is increasingly difficult for users to locate applicable services among a large number of functionally equivalent candidates. Considering the high cost of time and resources, users cannot invoke all Web services to obtain the desired quality of service (QoS). Therefore, the problem of QoS prediction of Web services has attracted much attention in recent years. Although QoS is often used as a measure of Web service performance, the value of QoS may vary significantly between users depending on their network and geographical location. Furthermore, most traditional approaches perform QoS prediction directly based on historical QoS values provided by users. However, these historical QoS data may contain unreliable values from unreliable users, resulting in significantly lower prediction accuracy. To overcome the above limitations, we propose a reputation and location aware matrix factorization (RLMF) approach for QoS prediction of Web services in this paper. First, we cluster the users and calculate their reputation based on the clustering information through Dirichlet distribution. Then, we integrate the user’s reputation and location information into the matrix factorization model to obtain more accurate prediction results. Additionally, we use Cauchy loss to measure the difference between the observed and predicted QoS values, which makes our approach robust to even outliers. We conducted experiments on a largescale dataset of 1,974,675 QoS values to evaluate our approach. The experimental results show that our approach performs better than state-of-the-art baseline approaches.},
  keywords={},
  doi={10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00245},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{10114347,
  author={Cao, Junling and Xu, Yichuan and Qian, Yuxuan and Chai, Tingyi and Liu, Chang},
  booktitle={2023 5th Asia Energy and Electrical Engineering Symposium (AEEES)}, 
  title={Multidimensional Full State Water Quality - Electrical Data Mining Method for Intelligent Fishing Grounds Based on Cloud Edge Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={1117-1123},
  abstract={Aiming at the problem that the intelligent analysis technology of aquaculture characteristics is difficult to meet the practical application needs of the market, a multi-dimensional full state water quality-electrical data mining method of intelligent fishing grounds based on cloud edge collaboration is proposed. First, based on the cloud edge collaboration architecture, an intelligent fishing ground awareness operation and maintenance system is designed. Through the collaboration of edge computing and cloud computing, the real-time performance of system data monitoring and processing is improved. Then, in the system edge layer, the weighted least square method and BP neural network are used for multidimensional data fusion to ensure the accuracy of monitoring data. Finally, the grey correlation analysis method is applied to the fuzzy comprehensive evaluation, and an improved fuzzy association rule analysis and evaluation model is constructed for the deep mining of full state data, so as to obtain the water quality and electrical safety level. Based on the experimental analysis of the proposed method in Taizhou Jiangyan Fishing Light Complementary Fishing Ground, the results show that the error of the heterogeneous data fusion method is smaller, and its water quality electrical comprehensive evaluation value is between 0.751-0.928, which is the closest to the measured value.},
  keywords={},
  doi={10.1109/AEEES56888.2023.10114347},
  ISSN={},
  month={March},}

@ARTICLE{9645168,
  author={Balcão-Filho, Amandio and Ruiz, Natasha and Rosa, Ferrucio de Franco and Bonacin, Rodrigo and Jino, Mario},
  journal={IEEE Transactions on Services Computing}, 
  title={Applying a Consumer-Centric Framework for Trust Assessment of Cloud Computing Service Providers}, 
  year={2023},
  volume={16},
  number={1},
  pages={95-107},
  abstract={Cloud computing services consumers do not have enough reliable information about critical characteristics of their providers, such as performance, security, trust and privacy, compliance with laws and regulations, among others. Our proposal addresses these problems presenting a trust assessment framework that integrates three domains: Governance, Transparency, and Security Information. Our approach is consumer-centric and deals with trust aspects from the end-user’s perspective. We use Indicators to communicate the outcomes, which aim to represent the expression of cybersecurity, manageability, and transparency of services under assessment. This paper includes an implementation proposal, prototype, and proof of concept, in which the framework was applied in a real scenario and executed over a long-term (18 months) usage simulation to verify its applicability, sensitivity, and robustness. Our study is intended for use by consumers of cloud computing who seek to know and measure levels of cybersecurity, protection of privacy, transparency of security, and high levels of quality in their services and infrastructure.},
  keywords={},
  doi={10.1109/TSC.2021.3134125},
  ISSN={1939-1374},
  month={Jan},}

@ARTICLE{10309856,
  author={Singhal, Saurabh and Gupta, Nakul and Berwal, Parveen and Naveed, Quadri Noorulhasan and Lasisi, Ayodele and Wodajo, Anteneh Wogasso},
  journal={IEEE Access}, 
  title={Energy Efficient Resource Allocation in Cloud Environment Using Metaheuristic Algorithm}, 
  year={2023},
  volume={11},
  number={},
  pages={126135-126146},
  abstract={Utility-based computing popularly known as “cloud computing” offers several computing services to the users. Due to the proliferation in the users of cloud computing, there is an unprecedented increase in the demand for computation resources to execute cloud services. Thus, there is a requirement to investigate currently available resources like virtual machines, CPU, RAM, and storage to allocate cloud services. The allocation and QoS of cloud services are highly dependent on allocation schemes. The optimized solutions allocate resources to submitted jobs to reduce the overall cost to the end-users/service provider without degrading the performance of virtual machines. The allocation techniques also consider the harvesting of energy consumption required for running the cloud services. In this paper, we have utilized a Rock Hyrax-based optimization technique to allocate resources to the submitted jobs with reduced energy consumption. The proposed Rock Hyrax algorithm has been simulated on the CloudSim simulator for various scenarios. The performance of the proposed algorithm has been measured over various Quality of Service (QoS) parameters such as makespan, energy efficiency, response time, throughput, and cost. The gathered results validate the proposed algorithm that improves the QoS parameters by 3%-8% as compared to algorithms when both jobs and resources are considered to be dynamic in nature.},
  keywords={},
  doi={10.1109/ACCESS.2023.3330434},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{10051112,
  author={Zalokostas-Diplas, Vasileios and Makris, Nikos and Passas, Virgilios and Korakis, Thanasis},
  booktitle={2022 IEEE Conference on Standards for Communications and Networking (CSCN)}, 
  title={Experimental Evaluation of ML Models for Dynamic VNF Autoscaling}, 
  year={2022},
  volume={},
  number={},
  pages={157-162},
  abstract={Network Functions Virtualization (NFV) is a key aspect deeply integrated in the latest 5G networks, allowing for the provisioning of elastic resources that adapt in a flexible manner based on the overall network demand. The adoption of NFV architectures is empowered through the evolution of cloud-native and hypervisor tools to support service monitoring, and orchestrate the appropriate decisions for provisioning the scale of the network. Such decisions may directly impact the overall quality of service and experience for users, as well as the energy consumption that the resources use. To this aim, machine learning (ML) - driven optimization for these decisions, relying on inferring the values of future monitored metrics, can assist in deciding proactively on the network scale. In this work, we employ three different candidate solutions (statistical, tree- and CNN-based) for determining the scale of network functions deployed within a cluster of resources, subject to the user demand. We compare and evaluate the different schemes in a real testbed environment, and discuss the benefits of ML-driven optimizations against existing state-of-the-art approaches.},
  keywords={},
  doi={10.1109/CSCN57023.2022.10051112},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{10151287,
  author={Pandey, Shivani and Mishra, Satanand and Jain, Ravi Kant},
  booktitle={2023 International Conference on Recent Advances in Electrical, Electronics & Digital Healthcare Technologies (REEDCON)}, 
  title={IoT Interface Device for Sensing Arsenic in Contaminated Water}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Water pollution is a serious problem in different parts of the world. In addition, water quality must be monitored to ensure that the water is provided safely for drinking and other purposes. Too high a concentration of Arsenic ions in drinking water is the cause of many health problems, including heart problems, neurological problems, etc. Water sampling and laboratory analysis are required for traditional water quality monitoring. In this paper, we discussed an IoT-based interfacing sensor device for sensing arsenic contaminants in water where IoT cloud computing networks enable the integration of a variety range of mechanical and electronic devices. A Node MCU device is used for data transmission which emphasizes on Wi-Fi-controlled interface devices and IoT-enabled communication protocol for the detection of water contaminants. This system is connected to an IoT cloud platform to store the data for analyzing purposes where Red-Green-Blue (RGB) color detection occurs by identifying the wavelength of contaminants. The system makes use of IoT to display the output in real-time for on-site and off-site monitoring via mobile phone. The system makes use of IoT to display the output in real-time for on-site and off-site monitoring via mobile phone, The major advantage of IoT technology is that it easily connects devices and stores the generated data in the cloud. With the help of command control systems, data can be used for appropriate applications to make human life easier and safer while considering Industry's impact. The acceptable limits set by WHO and the Bureau of Indian Standards for Arsenic are 0.05 mg/litres and 0.01 mg/l respectively. Therefore, a smart and intelligent device that can be used for measuring Arsenic content which is very necessary today to ensure the health of human life in society.},
  keywords={},
  doi={10.1109/REEDCON57544.2023.10151287},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9872806,
  author={Manova, Rd Yovi and Sukmadirana, Edi and Nurmanah, Nurma Siti},
  booktitle={2022 1st International Conference on Information System & Information Technology (ICISIT)}, 
  title={Comparative Analysis of Quality of Service and Performance of MPLS, EoIP and SD-WAN}, 
  year={2022},
  volume={},
  number={},
  pages={403-408},
  abstract={Software Defined – Wide Area Network (SDWAN) implementation is growing each year as one of the options for enterprise to have hybrid and redundant connection between traditional WAN and Internet. The cloud computing services, whether it is IaaS, PaaS, or SaaS has attracted most of enterprise to separate the corporate data connection from private WAN to public internet securely. This dual data traffic still can be managed by enterprise router, but it will require manual routing or at lease delay with complicated rule to mitigate any link problem. SD-WAN as the development of SDN in wide area network, have the solution to solve the manual routing data, by putting the control plane in a software environment to manage the data traffic virtually. In Indonesia, the SD-WAN technology has been introduced by several vendors and operators, some enterprise still reluctance considering the security of enterprise data through public internet service, and some of them still questioning the Quality of Services compare to legacy or traditional WAN services. Therefore, this research will perform the Quality of Service and Performance of SD-WAN, compare to traditional Multiprotocol Label Switching (MPLS) link and Ethernet over Internet Protocol (EoIP) as one of the contenders. The object of the research is an active WAN of one of Indonesian company, that having those three connections between Jakarta and Surabaya. The QoS and performance are measured using ITU- T G.1010 standard as the reference.},
  keywords={},
  doi={10.1109/ICISIT54091.2022.9872806},
  ISSN={},
  month={July},}

@ARTICLE{9772936,
  author={Kashani, Mostafa Haghi and Mahdipour, Ebrahim},
  journal={IEEE Transactions on Services Computing}, 
  title={Load Balancing Algorithms in Fog Computing}, 
  year={2023},
  volume={16},
  number={2},
  pages={1505-1521},
  abstract={Recently, fog computing has been introduced as a modern distributed paradigm and complement to cloud computing to provide services. The fog system extends storing and computing to the edge of the network, which can remarkably solve the problem of service computing in delay-sensitive applications besides enabling location awareness and mobility support. Load balancing is an important aspect of fog networks that avoids a situation with some under-loaded or overloaded fog nodes. Quality of service parameters such as resource utilization, throughput, cost, response time, performance, and energy consumption can be improved by load balancing. In recent years, some research in load balancing algorithms in fog networks has been carried out, but there is no systematic study to consolidate these works. This article investigates the load-balancing algorithms systematically in fog computing in four classifications, including approximate, exact, fundamental, and hybrid algorithms. Also, this article investigates load balancing metrics with all advantages and disadvantages related to chosen load balancing algorithms in fog networks. The evaluation techniques and tools applied for each reviewed study are explored as well. Additionally, the essential open challenges and future trends of these algorithms are discussed.},
  keywords={},
  doi={10.1109/TSC.2022.3174475},
  ISSN={1939-1374},
  month={March},}

@ARTICLE{9140382,
  author={Kong, Cuiyu and Rimal, Bhaskar Prasad and Reisslein, Martin and Maier, Martin and Bayram, Islam Safak and Devetsikiotis, Michael},
  journal={IEEE Transactions on Services Computing}, 
  title={Cloud-Based Charging Management of Heterogeneous Electric Vehicles in a Network of Charging Stations: Price Incentive Versus Capacity Expansion}, 
  year={2022},
  volume={15},
  number={3},
  pages={1693-1706},
  abstract={This article presents a novel cloud-based charging management system for electric vehicles (EVs). Two levels of cloud computing, i.e., local and remote clouds, are employed to meet the different latency requirements of the heterogeneous EVs while exploiting the lower-cost computing in remote clouds. Specifically, we consider time-sensitive EVs at highway exit charging stations and EVs with relaxed timing constraints at parking lot charging stations. We propose algorithms for the interplay among EVs, charging stations, system operator, and clouds. Considering the contention-based random access for EVs to a 4G Long-Term Evolution network, and the quality of service metrics (average waiting time and blocking probability), the model is composed of: queuing-based cloud server planning, capacity planning in charging stations, delay analysis, and profit maximization. We propose and analyze a price-incentive method that shifts heavy load from peak to off-peak hours, a capacity expansion method that accommodates the peak demand by purchasing additional electricity, and a hybrid method of price incentives and capacity expansion that balances the immediate charging needs of customers with the alleviation of the peak power grid load through price-incentive based demand control. Numerical results demonstrate the effectiveness of the proposed methods and elucidate the tradeoffs between the methods.},
  keywords={},
  doi={10.1109/TSC.2020.3009084},
  ISSN={1939-1374},
  month={May},}

@ARTICLE{9760072,
  author={Okegbile, Samuel D. and Maharaj, Bodhaswar T. and Alfa, Attahiru S.},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={A Multi-User Tasks Offloading Scheme for Integrated Edge-Fog-Cloud Computing Environments}, 
  year={2022},
  volume={71},
  number={7},
  pages={7487-7502},
  abstract={This paper presents a multi-user, multi-class and multi-layer edge computing-based framework for effective task offloading and computation processes. Important system requirements that were not captured in the existing multi-layer solutions such as offloading, computations and deadline requirements were captured in the system modeling, while both wireless communications and task computation constraints were considered. We considered three layers system, where each device offloads its generated tasks in each time slot to any selected layer for computation. On its arrival at such a selected layer, the task is only accepted if the queue size is below the pre-defined threshold, otherwise, such a task is offloaded to the next layer. Tasks were classified into class 1 and class 2 tasks following tasks’ quality of service requirements. We adopted stochastic geometry, parallel computing and queueing theory techniques to model the performance of the considered integrated edge-fog-cloud computing environment and obtained analysis for various performance metrics of interest. The obtained analyses demonstrate the importance of multi-layer and multi-class edge computing systems towards improving the experience of both delay-sensitive and mission-critical applications in any task offloading environment.},
  keywords={},
  doi={10.1109/TVT.2022.3167892},
  ISSN={1939-9359},
  month={July},}

@INPROCEEDINGS{10074183,
  author={Morel, Alicia Esquivel and Calyam, Prasad and Qu, Chengyi and Gafurov, Durbek and Wang, Cong and Thareja, Komal and Mandal, Anirban and Lyons, Eric and Zink, Michael and Papadimitriou, George and Deelman, Ewa},
  booktitle={2023 International Conference on Computing, Networking and Communications (ICNC)}, 
  title={Network Services Management using Programmable Data Planes for Visual Cloud Computing}, 
  year={2023},
  volume={},
  number={},
  pages={130-136},
  abstract={Visual Cloud Computing (VCC) applications provide highly efficient solutions in video data processing pipelines on edge/cloud infrastructures. These applications and their infrastructures demand end-to-end monitoring and fine-grained application traffic control to meet user quality of experience requirements. In this paper, we propose a novel network services management methodology for VCC applications by leveraging the advantages of programmable data planes enabled by Protocol-independent Packet Processors (P4). Specifically, we define a custom fixed-length application header and use it to improve the performance of a video streaming application through congestion avoidance using Multi-Hop Route Inspection (MRI), a variant of In-band Network Telemetry (INT), and switch port forwarding (tunneling) capabilities. For evaluation experiments, we use P4 per-packet telemetry metadata for routing paths, ingress/egress timestamps, queue occupancy in a given node, and egress port link utilization in a VCC testbed on the NSF-supported FABRIC infrastructure. Our experiment results demonstrate performance improvement obtained with our methodology in terms of both packet loss and throughput metrics.},
  keywords={},
  doi={10.1109/ICNC57223.2023.10074183},
  ISSN={},
  month={Feb},}

@ARTICLE{9829255,
  author={Fu, Shaojing and Huang, XueLun and Liu, Lin and Luo, Yuchuan},
  journal={IEEE Transactions on Cloud Computing}, 
  title={BFCRI: A Blockchain-Based Framework for Crowdsourcing With Reputation and Incentive}, 
  year={2023},
  volume={11},
  number={2},
  pages={2158-2174},
  abstract={With the rapid development of cloud computing and the sharing economy, crowdsourcing aroused widespread interest and adoption in providing intelligent and efficient services for humans. The majority of existing works focus on effective crowdsourcing task assignment and privacy protection, mostly relying on central servers and assuming that participants are $honest$honest-$and$and-$curious$curious and proactive. However, in reality, workers may be unwilling to participate, and there may be malicious behavior among participants, thus harming the enthusiasm and interests of other participants. The central server has weaknesses such as single point of failure. To address above problems, we propose a blockchain-based framework for crowdsourcing with reputation and incentive. We first design a worker selection scheme to select credible and capable workers. We leverage reputation as a metric of workers’ credibility, which is calculated through the improved subjective logic model. Then we utilize contract theory to design incentive mechanisms to attract more workers, especially high-quality workers to participate. Experimental results show that our proposed method can detect and prevent malicious participants and resist malicious collusion when the proportion of malicious participants is no more than 1/3. And encourage more workers to actively, honestly and continuously participate in crowdsourcing.},
  keywords={},
  doi={10.1109/TCC.2022.3190275},
  ISSN={2168-7161},
  month={April},}

@INPROCEEDINGS{10047622,
  author={Gritto, D. and Muthulakshmi, P.},
  booktitle={2022 11th International Conference on System Modeling & Advancement in Research Trends (SMART)}, 
  title={Scheduling Cloudlets in a Cloud Computing Environment: A Priority-based Cloudlet Scheduling Algorithm (PBCSA)}, 
  year={2022},
  volume={},
  number={},
  pages={80-86},
  abstract={Cloud computing is a service model that has evolved in its stature beyond its traditional bounds of infrastructure, platform and software as a service. As the surge in resource demand may hit the cloud service provider at any time, a ceaseless monitoring system is vital. The allocation of an appropriate virtual machine for the cloudlet i.e., the user workload and maintaining the work load equilibrium among the resources is the most challenging operation in the cloud environment. The proper utilization of the cloud resources can be ensured by selecting the right cloudlet scheduling and load balancing algorithm(s). The cloudlet scheduling algorithm selection is based on the combination of two or more Quality of Service (QoS) and performance metrics like makespan, throughput, cost, power consumption, virtual machine or resource utilization and load balancing etc. The load balancer module takes the responsibility of dispersing the cloudlets evenly among the virtual machines by considering various features like CPU utilization, number of processing elements, bandwidth, memory and the load limit of the virtual machines. In this paper, an effort has been made to comprehend the most persisting cloudlet scheduling and load balancing algorithms that have been proposed by the researchers. Compiling the load balancing technologies that are integrated with the contemporary cloud platforms such as Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP) has also been prioritized. This study suggests a Priority Based Cloudlet Scheduling Algorithm (PBCSA) that schedules the cloudlet according to the user priority. The Min-Min scheduler is used to schedule the high priority cloudlets and the Max-Min scheduler is used to schedule the low priority cloudlets. The experimental findings reveals that, in the majority of scenarios, the proposed algorithm outperforms the Min-Min and Max-Min scheduling in terms of makespan and virtual machine utilization ratio.},
  keywords={},
  doi={10.1109/SMART55829.2022.10047622},
  ISSN={2767-7362},
  month={Dec},}

@ARTICLE{10172271,
  author={Chang, Jiaxin and Wang, Jian and Li, Bing and Zhao, Yuqi and Li, Duantengchuan},
  journal={IEEE Transactions on Network and Service Management}, 
  title={Attention-Based Deep Reinforcement Learning for Edge User Allocation}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  abstract={Edge computing, a recently developed computing paradigm, seeks to extend cloud computing by providing users minimal latency. In a mobile edge computing (MEC) environment, edge servers are placed close to edge users to offer computing resources, and the coverage of adjacent edge servers may partially overlap. Because of the restricted resource and coverage of each edge server, edge user allocation (EUA), i.e., determining the optimal way to allocate users to different servers in the overlapping area, has emerged as a major challenge in edge computing. Despite the NP-hardness of obtaining an optimal solution, it is possible to evaluate the quality of a solution in a short amount of time with given metrics. Consequently, deep reinforcement learning (DRL) can be used to solve EUA by attempting numerous allocations and optimizing the allocation strategy depending on the rewards of those allocations. In this study, we propose the Dual-sequence Attention Model (DSAM) as the DRL agent, which encodes users using self-attention mechanisms and directly outputs the probability of matching between users and servers using an attention-based pointer mechanism, enabling the selection of the most suitable server for each user. Experimental results show that our method outperforms the baseline approaches in terms of allocated users, required servers, and resource utilization, and its running speed meets real-time requirements.},
  keywords={},
  doi={10.1109/TNSM.2023.3292272},
  ISSN={1932-4537},
  month={},}

@INPROCEEDINGS{9900564,
  author={Sankaran, Lakshmi and Saleema, J S and Suleiman, Basem},
  booktitle={2022 IEEE/ACIS 7th International Conference on Big Data, Cloud Computing, and Data Science (BCD)}, 
  title={Analysis of Workloads for Cloud Services}, 
  year={2022},
  volume={},
  number={},
  pages={117-123},
  abstract={Capturing best quality datasets for a study is the first evidence for better outcomes of research. If the analysis are based on such datasets, then the metrics, the characteristics and few factors determines proof point for well proven theories. Hence it is obvious that we rely on the best possible ways to arrive at such data acquiring sources. It can be either based on historical techniques or from the innovations in application of it to industry. This paper introduces a mapping framework for analyzing, and characterizing data previously used by research community and how they are made to fit for Cloud systems, i.e. using “workloads” and “datasets” as the “refined definitions”. It was contributed in the past two decades within the scientific community setting their own workflow analysis mechanisms. The framework thus is validated by acquiring a sample workload per layer of cloud. The sources are form the literature that are available from existing scientific theories. These workloads are then experimented against the three tiers of the cloud computing ie., IaaS(Infrastructure as a Service), PaaS(Platform as a Service), & SaaS(Software as a Service). The selected data is analyzed by the authors for an offline model presented here based on the Machine Learning tool-kits. There are future studies planned for and to be experimented in a cloud auto scaled environment with online model as well.},
  keywords={},
  doi={10.1109/BCD54882.2022.9900564},
  ISSN={},
  month={Aug},}

@ARTICLE{10227869,
  author={Deebak, Bakkiam David and Hwang, Seong Oun},
  journal={IEEE Systems Journal}, 
  title={A Cloud-Assisted Medical Cyber-Physical System Using a Privacy-Preserving Key Agreement Framework and a Chebyshev Chaotic Map}, 
  year={2023},
  volume={17},
  number={4},
  pages={5543-5554},
  abstract={At present, communication networks like the Internet of Things (IoT) are emerging with some of the latest technologies, such as artificial intelligence, big data, and cloud computing, to enable wireless edge infrastructures and seamless data migration. In particular, edge-based wireless communications help to gain new insights into the mitigation of potential communicable infections, e.g., COVID-19, via the use of IoT sensing devices. Of late, an evolving technology known as the cloud-assisted medical cyber-physical system (MCPS) has explored various key agreement protocols to examine security weaknesses between sensing devices and medical experts. Unfortunately, the existing schemes address vulnerabilities such as impersonation, privileged insider, and password guessing, whereby the behavior of the system becomes nondeterministic when guarding against malicious intent. Also, failures, faults, and attacks may vary in the characteristic forms of the IoT and the MCPS, causing unforeseen impairments in the system and for users. Thus, this article develops a privacy-preserving key agreement framework (PP-KAF) using the Chebyshev chaotic map mechanism to avoid privacy data disclosures and to protect session keys. The proposed PP-KAF exploits a strategy of two-way authentication not only to protect user identities but also to achieve untraceability from the remote server. Finally, a formal analytical model is applied to examine the properties of the key agreement protocol. Simulation results demonstrate that the proposed PP-KAF can offer better security efficiencies and can mitigate computation and communication overhead to guarantee improved quality metrics, namely, throughput rate and energy consumption.},
  keywords={},
  doi={10.1109/JSYST.2023.3303460},
  ISSN={1937-9234},
  month={Dec},}

@INPROCEEDINGS{9860577,
  author={Tuli, Shreshth and Casale, Giuliano and Jennings, Nicholas R.},
  booktitle={2022 IEEE 15th International Conference on Cloud Computing (CLOUD)}, 
  title={MetaNet: Automated Dynamic Selection of Scheduling Policies in Cloud Environments}, 
  year={2022},
  volume={},
  number={},
  pages={331-341},
  abstract={Task scheduling is a well-studied problem in the context of optimizing the Quality of Service (QoS) of cloud computing environments. In order to sustain the rapid growth of computational demands, one of the most important QoS metrics for cloud schedulers is the execution cost. In this regard, several data-driven deep neural networks (DNNs) based schedulers have been proposed in recent years to allow scalable and efficient resource management in dynamic workload settings. However, optimal scheduling frequently relies on sophisticated DNNs with high computational needs implying higher execution costs. Further, even in non-stationary environments, sophisticated schedulers might not always be required and we could briefly rely on low-cost schedulers in the interest of cost-efficiency. Therefore, this work aims to solve the non-trivial meta problem of online dynamic selection of a scheduling policy using a surrogate model called MetaNet. Unlike traditional solutions with a fixed scheduling policy, MetaNet on-the-fly chooses a scheduler from a large set of DNN based methods to optimize task scheduling and execution costs in tandem. Compared to state-of-the-art DNN schedulers, this allows for improvement in execution costs, energy consumption, response time and service level agreement violations by up to 11, 43, 8 and 13 percent, respectively.},
  keywords={},
  doi={10.1109/CLOUD55607.2022.00056},
  ISSN={2159-6190},
  month={July},}

@INPROCEEDINGS{10322008,
  author={Malti, Arslan Nedhir and Benmammar, Badr and Hakem, Mourad},
  booktitle={2023 5th International Conference on Pattern Analysis and Intelligent Systems (PAIS)}, 
  title={Task Scheduling Optimization in Cloud Computing: A Comparative Study Between Flower Pollination and Butterfly Optimization Algorithms}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The No Free Lunch theorem states that no single algorithm can universally serve as the best solution for all optimization purposes. Consequently, it becomes imperative to adapt or combine the fundamental structures of metaheuristic optimization algorithms to fit with specific problem-solving requirements. In this paper, we conduct a comparative analysis of two widely recognized metaheuristic algorithms, namely the Butterfly Optimization Algorithm (BOA) and the Flower Pollination Algorithm (FPA) to deal with the task scheduling problem in cloud computing systems. Our objective is to orchestrate the most effective assignment of tasks across the available virtual machines in the system. Performance evaluation encompasses standard and synthetic workloads, focusing on conflicting quality of service metrics, namely time makespan or schedule length and resource utilization. The experiments conducted through the CloudSim framework demonstrate compelling outcomes, highlighting the importance of this study as a basis for future optimization research.},
  keywords={},
  doi={10.1109/PAIS60821.2023.10322008},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10254974,
  author={Caon, Cristiano E. and Li, Jie and Chen, Yong},
  booktitle={2023 IEEE 16th International Conference on Cloud Computing (CLOUD)}, 
  title={Effective Management of Time Series Data}, 
  year={2023},
  volume={},
  number={},
  pages={408-414},
  abstract={Cloud computing systems, consisting of numerous nodes and components, require constant monitoring to satisfy the Quality-of-Service (QoS), making the management of large-scale time series data challenging. To address this issue, age threshold retention policies have been implemented to remove historical data, but this eliminates valuable information from older periods. In this paper, we proposed an alternative approach that applies time series deduplication with metric-based tolerance to discard readings that stabilize within a calculated tolerance window. This approach can reduce the data volume by 70.38% on average. Once the data-reduced interval is queried, the readings can be reconstructed to retrieve the original granularity with low query runtime overhead and a Mean Absolute Percentage Error of 0.74%.},
  keywords={},
  doi={10.1109/CLOUD60044.2023.00055},
  ISSN={2159-6190},
  month={July},}

@ARTICLE{10236903,
  author={Tuli, Shreshth and Casale, Giuliano and Jennings, Nicholas R.},
  journal={IEEE Transactions on Computers}, 
  title={SciNet: Codesign of Resource Management in Cloud Computing Environments}, 
  year={2023},
  volume={72},
  number={12},
  pages={3590-3602},
  abstract={The rise of distributed cloud computing technologies has been pivotal for the large-scale adoption of Artificial Intelligence (AI) based applications for high fidelity and scalable service delivery. Systematic resource management is central in maintaining optimal Quality of Service (QoS) in cloud platforms and is divided into three fundamental types: resource provisioning, AI model deployment and workload placement. To exploit the synergy among these decision types, it becomes imperative to concurrently design (co-design) the provisioning, deployment and placement decisions for optimal QoS. As users and cloud service providers shift to non-stationary AI-based workloads, frequent decision making imposes severe time constraints on the resource management models. Existing AI-based solutions often optimize decision types independently and tend to ignore the dependencies across various system performance aspects such as energy consumption and CPU utilization, making them perform poorly in large-scale cloud systems. To address this, we propose a novel method, called SciNet, that leverages a co-simulated digital-twin of the infrastructure to capture inter-metric dependencies and accurately estimate QoS scores. To avoid expensive simulation overheads at test time, SciNet trains a neural network based imitation learner that aims to mimic an oracle, which takes optimal decisions based on co-simulated QoS estimates. Offline model training and online decision making based on the imitation learner, enables SciNet to take optimal decisions while being time-efficient. Experiments with real-life AI-based benchmark applications on a public cloud testbed show that SciNet gives up to 48% lower execution cost, 79% higher inference accuracy, 71% lower energy consumption and 56% lower response times compared to the current state-of-the-art methods.},
  keywords={},
  doi={10.1109/TC.2023.3310678},
  ISSN={1557-9956},
  month={Dec},}

@INPROCEEDINGS{9988009,
  author={Sripavithra, C K and Kirubanand, V B},
  booktitle={2022 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)}, 
  title={ESSA Scheduling Algorithm for Optimizing Budget-Constrained Workflows}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Workflows are a systematic approach for defining various scientific applications of distributed systems. They break down complicated, data-intensive processes into minor activities that can be executed serially or in parallel according to the type of application. Cloud systems need to allocate resources and schedule workflows efficiently. Despite many studies on job scheduling and resource provisioning, an efficient solution isn't found. Therefore, techniques are required to enhance resource utilization for optimal cloud computing platforms. Hence, user and provider quality of service (QoS) goals, like shortening workflows and ensuring budget limits with low energy utilization, must be considered. Enhanced Salp Swarm Optimization (ESSA) is designed to optimize makespan and QoS metrics in cloud systems. A Virtual Machine (VM's) compute capacity is related to Central Processing Unit (CPU) and memory. Size and memory demand is considered for tasks in the workflow, and task execution time is evaluated using both CPU and memory. The collated experimental outcomes convey that the newly presented technique boosts the workflows' energy utilization (up to 89%) and pushes the normalized makespan results to 3.2ms.},
  keywords={},
  doi={10.1109/ICECCME55909.2022.9988009},
  ISSN={},
  month={Nov},}

@ARTICLE{8762170,
  author={Abdelbaky, Moustafa and Parashar, Manish},
  journal={IEEE Transactions on Services Computing}, 
  title={A General Performance and QoS Model for Distributed Software-Defined Environments}, 
  year={2022},
  volume={15},
  number={1},
  pages={228-240},
  abstract={The landscape for cloud services and cyberinfrastructure offerings has increased drastically over the past few years. Initially, users moved their applications to the cloud to take advantage of a pay-per-usage model and on-demand access. However, as more cloud providers joined the market, users shifted their goals for using cloud computing from cost reduction to resilience, agility, and optimization. These goals can be achieved by dynamically combining services from multiple providers, for example, to avoid data center or cloud zone outages or to take advantage of extensive offerings with different price points. However, to efficiently support application deployment in this dynamic environment, new models and tools that can measure the application performance and the Quality of Service (QoS) of different configurations are required. The goal of this work is to evaluate the application performance and the QoS of a distributed Software-Defined Environment as well as calculate the QoS of alternative configurations from the underlying pool of services. In particular, we present a mathematical model and a tool for evaluating the performance and QoS of batch application workflows in a distributed environment. We experimentally evaluate the proposed model using a bioinformatics workflow running on infrastructure services from multiple cloud providers.},
  keywords={},
  doi={10.1109/TSC.2019.2928300},
  ISSN={1939-1374},
  month={Jan},}

@ARTICLE{9625857,
  author={Hidayetoğlu, Mert and Biçer, Tekin and de Gonzalo, Simon Garcia and Ren, Bin and Gürsoy, Doğa and Kettimuthu, Rajkumar and Foster, Ian T. and Hwu, Wen-Mei W.},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={MemXCT: Design, Optimization, Scaling, and Reproducibility of X-Ray Tomography Imaging}, 
  year={2022},
  volume={33},
  number={9},
  pages={2014-2031},
  abstract={This work extends our previous research entitled “MemXCT: Memory-centric X-ray CT Reconstruction with Massive Parallelization” that was originally published at SC19 conference (Hidayetoğlu et al., 2019) with reproducibility of the computational imaging performance. X-ray computed tomography (XCT) is regularly used at synchrotron light sources to study the internal morphology of materials at high resolution. However, experimental constraints, such as radiation sensitivity, can result in noisy or undersampled measurements. Further, depending on the resolution, sample size and data acquisition rates, the resulting noisy dataset can be in the order of terabytes. Advanced iterative reconstruction techniques can produce high-quality images from noisy measurements, but their computational requirements have made their use an exception rather than the rule. We propose a novel memory-centric approach that avoids redundant computations at the expense of additional memory complexity. We develop a memory-centric iterative reconstruction system, MemXCT, that uses an optimized SpMV implementation with two-level pseudo-Hilbert ordering and multi-stage input buffering. We evaluate MemXCT on various supercomputer architectures involving KNL and GPU. MemXCT can reconstruct a large (11K×11K) mouse brain tomogram in 10 seconds using 4096 KNL nodes (256K cores). The results presented in our original article at the SC19 were based on large-scale supercomputing resources. The MemXCT application was selected for the Student Cluster Competition (SCC) Reproducibility Challenge and evaluated on a variety of cloud computing resources by universities around the world in the SC20 conference. We summarize the results of the top-ranked SCC Reproducibility Challenge teams and identify the most pertinent measures for ensuring the reproducibility of our experiments in this article.},
  keywords={},
  doi={10.1109/TPDS.2021.3128032},
  ISSN={1558-2183},
  month={Sep.},}

@ARTICLE{9785848,
  author={Rui, Lanlan and Song, Dai and Chen, Shiyou and Yang, Yingtai and Yang, Yang and Gao, Zhipeng},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Content Collaborative Caching Strategy in the Edge Maintenance of Communication Network: A Joint Download Delay and Energy Consumption Method}, 
  year={2022},
  volume={33},
  number={12},
  pages={4148-4163},
  abstract={With the development of Big Data technology and Internet, the surge of data in the network will cause network congestion and untimely task processing. Additionally, caching content in the core network may cause redundant access of content and backhaul bottlenecks. Due to the increasing requirements of users for task processing efficiency, the centralized maintenance system based on traditional cloud computing cannot meet the current computing requirements. In view of these problems, we propose a content collaborative caching mechanism based on joint decision of download delay and energy consumption. By integrating network coding and content caching technology, the work content maintained in the communication network is deployed near the edge of the network in the form of coding to reduce the redundant transmission of content and acquisition time of content. This article establishes a user QoE satisfaction model, which consists of two indexes that measure time delay and energy consumption. This article proposes a $\varepsilon$ɛ-hybrid Q-learning algorithm to optimize the placement of cache files, and made the cache action selection based on the combination of improved heuristic greedy algorithm and simulated annealing algorithm. The experimental results show that the proposed cache strategy can reduce the delay of users downloading content and the energy consumption of content cache, so as to improve the quality of field maintenance work in communication network.},
  keywords={},
  doi={10.1109/TPDS.2022.3179271},
  ISSN={1558-2183},
  month={Dec},}

@INPROCEEDINGS{10017214,
  author={Sharma, P. and Saini, K. S. and Sidhu, P. K.},
  booktitle={The 3rd International Conference on Distributed Sensing and Intelligent Systems (ICDSIS 2022)}, 
  title={Authentication mechanisms used in Wireless Body Area Networks: a study}, 
  year={2022},
  volume={2022},
  number={},
  pages={282-291},
  abstract={The WBANs (Collection of light-weight body sensor) of IOT and Cloud Computing provides a substantial ability to upgrade the quality of on-demand health care system. Body Sensors are implanted within or outside the human being body to remotely measure the real-time parameters such as physiology signal and activity. The biggest issue originates from obtaining imprecise and inconsistent measurements when collecting these data from sensor nodes. Moreover, Privacy and Security of human sensitive information are other biggest hurdles of WBANs. In this chapter we describe some wearable devices, security requirements and some security attacks. Moreover, we also discussed various existing authentication mechanisms. Different authentication mechanisms have their own Pros and Cons. Further, based on the study we will performed some comprehensive overview between all existing authentication mechanisms and find some future scope. We also highlight some future research scope in the last section.},
  keywords={},
  doi={10.1049/icp.2022.2478},
  ISSN={},
  month={Oct},}

@ARTICLE{9933021,
  author={Lu, Ke and Du, Zhekai and Li, Jingjing and Min, Geyong},
  journal={IEEE Transactions on Network and Service Management}, 
  title={Resource-Efficient Distributed Deep Neural Networks Empowered by Intelligent Software-Defined Networking}, 
  year={2022},
  volume={19},
  number={4},
  pages={4069-4081},
  abstract={Contemporary machine learning methods have evolved from conventional algorithms to deep neural networks (DNNs) that are computation- and data- intensive. Thus, they are suitable to be deployed in the cloud that can offer high computational capacity and scalable resources. However, the cloud computing paradigm is not optimal for delay- and energy-sensitive applications. To mitigate these problems, a battery of distributed DNNs have been proposed to allow a fast inference with device-edge-cloud synergy. Furthermore, although distributed deployment of DNNs on real communication networks is an important research topic, the legacy network architecture cannot meet the requirements of these distributed deep neural networks due to the complicated management and manual configuration, etc. To cope with these requirements, we develop a novel and explicit Intelligent Software Defined Networking (ISDN) that aims to manage the bandwidth and computing resources across the network via the SDN paradigm. We first identify the difficulties of deploying distributed intelligent computing in the current network architecture. Then, we explain how to address these problems by introducing the ISDN architecture. Specifically, we develop a dynamic routing method to enable Quality-of-Service (QoS) communication based on the SDN paradigm and propose a Markov Decision Process (MDP) based dynamic task offloading model to achieve the optimal offloading policy of DNN tasks. We develop a simulation platform based on Mininet to measure its performance advantages over traditional architectures. Extensive experimental results show that compared with the traditional network architecture, our architecture based on the SDN paradigm can perform better in terms of both network throughput and resource utilization.},
  keywords={},
  doi={10.1109/TNSM.2022.3218173},
  ISSN={1932-4537},
  month={Dec},}

@INPROCEEDINGS{10001602,
  author={Zhu, Jing and Wang, Dan and Qin, Shuxin and Tao, Gaofeng and Gui, Hongxin and Li, Fang and Ou, Liang},
  booktitle={GLOBECOM 2022 - 2022 IEEE Global Communications Conference}, 
  title={Towards Network Dynamics: Adaptive Buffer Management with Deep Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={4935-4940},
  abstract={The prosperity of cloud computing and 5G/B5G is bringing a wide range of delay jitter-sensitive applications (e.g., professional audio/video streaming and industrial automation) to large-scale IP networks. Although various network-side techniques have been proposed to guarantee the quality of service (QoS), the client-side technique by introducing a receive buffer should never be neglected from the applications' perspective. In this paper, we revisit the buffer management problem to address the disadvantages of state-of-the-art studies, which as-sumed network characteristics known a prior with simplified or inaccurate network models and failed to adapt to network dynamics. Specifically, we propose adaptive buffer management with deep reinforcement learning, i.e., DRL-ABM. We first define a tradeoff value to measure the buffer management performance in terms of the start-up delay, underflow frequency and packet losses. Then we formulate the DRL model and design deep neural networks (DNNs) based on the advantage actor critic (A2C) algorithm. To evaluate the performance of DRL-ABM, we perform extensive simulations. Simulation results show that D RL-ABM can achieve better buffer management performance, i.e., reducing the tradeoff value by at least 20% when compared with the benchmarks TBM and ABM. Moreover, DRL-ABM reduces packet losses to approximately 0, indicating that a smaller receive buffer is sufficient if managed with DRL-ABM.},
  keywords={},
  doi={10.1109/GLOBECOM48099.2022.10001602},
  ISSN={},
  month={Dec},}

@ARTICLE{10192547,
  author={Zhou, Jun and Kondo, Masaaki},
  journal={IEEE Transactions on Emerging Topics in Computing}, 
  title={An Edge-Cloud Collaboration Framework for Graph Processing in Smart Society}, 
  year={2023},
  volume={11},
  number={4},
  pages={985-1001},
  abstract={Due to the limitations of cloud computing on latency, bandwidth and data confidentiality, edge computing has emerged as a novel location-aware way to provide the capacity-constrained portable terminals with more processing capacity to improve the computing performance and quality of service (QoS) in several typical domains of the human activity in smart society, such as social networks, medical diagnosis, telecommunications, recommendation systems, internal threat detection, transportation, Internet of Things (IoT), etc. These application domains often manage a vast collection of entities with various relationships, which can be naturally represented by the graph data structure. Graph processing is a powerful tool to model and optimize complex problems where graph-based data is involved. In consideration of the relatively insufficient resource provisioning of the edge devices, in this article, for the first time to our knowledge, we propose a reliable edge-cloud collaboration framework that facilitates the graph primitives based on a lightweight interactive graph processing library (GPL), especially for shortest path search (SPS) operations as the demonstrative example. Two types of different practical cases are also presented to show the typical application scenarios of our graph processing strategy. Experimental evaluations indicate that the acceleration rate of performance can reach 6.87x via graph reduction, and less than 3% and 20% extra latency is required for much better user experiences for navigation and pandemic control, respectively, while the online security measures merely consume about 1% extra time of the overall data transmission. Our framework can efficiently execute the applications with considering of user-friendliness, low-latency response, interactions among edge devices, collaboration between edge and cloud, and privacy protection at an acceptable overhead.},
  keywords={},
  doi={10.1109/TETC.2023.3297066},
  ISSN={2168-6750},
  month={Oct},}

 @article{Raj_2021, title={Evaluation of SOA-Based Web Services and Microservices Architecture Using Complexity Metrics}, volume={2}, ISSN={2661-8907}, url={http://dx.doi.org/10.1007/s42979-021-00767-6}, DOI={10.1007/s42979-021-00767-6}, number={5}, journal={SN Computer Science}, publisher={Springer Science and Business Media LLC}, author={Raj, Vinay and Sadam, Ravichandra}, year={2021}, month=jul }

 @article{Parvizi_Mosaed_2014, title={Towards a self-adaptive service-oriented methodology based on extended SOMA}, volume={16}, ISSN={2095-9230}, url={http://dx.doi.org/10.1631/FITEE.1400040}, DOI={10.1631/fitee.1400040}, number={1}, journal={Frontiers of Information Technology &amp; Electronic Engineering}, publisher={Zhejiang University Press}, author={Parvizi-Mosaed, Alireza and Moaven, Shahrouz and Habibi, Jafar and Beigi, Ghazaleh and Naser-Shariat, Mahdieh}, year={2014}, month=dec, pages={43–69} }

 @article{Bogner_2021, title={Industry practices and challenges for the evolvability assurance of microservices: An interview study and systematic grey literature review}, volume={26}, ISSN={1573-7616}, url={http://dx.doi.org/10.1007/s10664-021-09999-9}, DOI={10.1007/s10664-021-09999-9}, number={5}, journal={Empirical Software Engineering}, publisher={Springer Science and Business Media LLC}, author={Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred}, year={2021}, month=jul }

 @article{Slimani_2020, title={Service-oriented replication strategies for improving quality-of-service in cloud computing: a survey}, volume={24}, ISSN={1573-7543}, url={http://dx.doi.org/10.1007/s10586-020-03108-z}, DOI={10.1007/s10586-020-03108-z}, number={1}, journal={Cluster Computing}, publisher={Springer Science and Business Media LLC}, author={Slimani, Sarra and Hamrouni, Tarek and Ben Charrada, Faouzi}, year={2020}, month=may, pages={361–392} }

 @article{Siavvas_2021, title={A hierarchical model for quantifying software security based on static analysis alerts and software metrics}, volume={29}, ISSN={1573-1367}, url={http://dx.doi.org/10.1007/s11219-021-09555-0}, DOI={10.1007/s11219-021-09555-0}, number={2}, journal={Software Quality Journal}, publisher={Springer Science and Business Media LLC}, author={Siavvas, Miltiadis and Kehagias, Dionysios and Tzovaras, Dimitrios and Gelenbe, Erol}, year={2021}, month=may, pages={431–507} }

 @article{Rajput_2020, title={Multi-agent architecture for fault recovery in self-healing systems}, volume={12}, ISSN={1868-5145}, url={http://dx.doi.org/10.1007/s12652-020-02443-8}, DOI={10.1007/s12652-020-02443-8}, number={2}, journal={Journal of Ambient Intelligence and Humanized Computing}, publisher={Springer Science and Business Media LLC}, author={Rajput, Pushpendra Kumar and Sikka, Geeta}, year={2020}, month=aug, pages={2849–2866} }

 @article{von_Detten_2013, title={Reengineering component-based software systems with Archimetrix}, volume={13}, ISSN={1619-1374}, url={http://dx.doi.org/10.1007/s10270-013-0341-9}, DOI={10.1007/s10270-013-0341-9}, number={4}, journal={Software &amp; Systems Modeling}, publisher={Springer Science and Business Media LLC}, author={von Detten, Markus and Platenius, Marie Christin and Becker, Steffen}, year={2013}, month=apr, pages={1239–1268} }

 @article{Mohsin_2018, title={A review and future directions of SOA-based software architecture modeling approaches for System of Systems}, volume={12}, ISSN={1863-2394}, url={http://dx.doi.org/10.1007/s11761-018-0245-1}, DOI={10.1007/s11761-018-0245-1}, number={3–4}, journal={Service Oriented Computing and Applications}, publisher={Springer Science and Business Media LLC}, author={Mohsin, Ahmad and Janjua, Naeem Khalid}, year={2018}, month=oct, pages={183–200} }

 @article{Rajaram_2018, title={A novel computational knowledge-base framework for visualization and quantification of geospatial metadata in spatial data infrastructures}, volume={22}, ISSN={1573-7624}, url={http://dx.doi.org/10.1007/s10707-018-0317-6}, DOI={10.1007/s10707-018-0317-6}, number={2}, journal={GeoInformatica}, publisher={Springer Science and Business Media LLC}, author={Rajaram, Gangothri and Karnatak, Harish Chandra and Venkatraman, Swaminathan and Manjula, K. R. and Krithivasan, Kannan}, year={2018}, month=feb, pages={269–305} }

 @article{Haupt_2017, title={API governance support through the structural analysis of REST APIs}, volume={33}, ISSN={1865-2042}, url={http://dx.doi.org/10.1007/s00450-017-0384-1}, DOI={10.1007/s00450-017-0384-1}, number={3–4}, journal={Computer Science - Research and Development}, publisher={Springer Science and Business Media LLC}, author={Haupt, Florian and Leymann, Frank and Vukojevic-Haupt, Karolina}, year={2017}, month=sep, pages={291–303} }

 @article{Moens_2013, title={Cost-Effective Feature Placement of Customizable Multi-Tenant Applications in the Cloud}, volume={22}, ISSN={1573-7705}, url={http://dx.doi.org/10.1007/s10922-013-9265-5}, DOI={10.1007/s10922-013-9265-5}, number={4}, journal={Journal of Network and Systems Management}, publisher={Springer Science and Business Media LLC}, author={Moens, Hendrik and Truyen, Eddy and Walraven, Stefan and Joosen, Wouter and Dhoedt, Bart and De Turck, Filip}, year={2013}, month=feb, pages={517–558} }

 @article{Rajesh_2017, title={Esteemed software patterns for banking system}, volume={22}, ISSN={1573-7543}, url={http://dx.doi.org/10.1007/s10586-017-1304-7}, DOI={10.1007/s10586-017-1304-7}, number={S5}, journal={Cluster Computing}, publisher={Springer Science and Business Media LLC}, author={Rajesh, Sudha and Chandrasekar, A.}, year={2017}, month=nov, pages={11087–11099} }

 @article{Sikri_2019, title={An adaptive and scalable framework for automated service discovery}, volume={13}, ISSN={1863-2394}, url={http://dx.doi.org/10.1007/s11761-019-00255-z}, DOI={10.1007/s11761-019-00255-z}, number={1}, journal={Service Oriented Computing and Applications}, publisher={Springer Science and Business Media LLC}, author={Sikri, Monika}, year={2019}, month=mar, pages={67–79} }

 @article{Alaeddini_2016, title={Leveraging business-IT alignment through enterprise architecture—an empirical study to estimate the extents}, volume={18}, ISSN={1573-7667}, url={http://dx.doi.org/10.1007/s10799-016-0256-6}, DOI={10.1007/s10799-016-0256-6}, number={1}, journal={Information Technology and Management}, publisher={Springer Science and Business Media LLC}, author={Alaeddini, Morteza and Asgari, Hamed and Gharibi, Arash and Rashidi Rad, Mona}, year={2016}, month=mar, pages={55–82} }

 @article{Wijerathna_2021, title={Mining and relating design contexts and design patterns from Stack Overflow}, volume={27}, ISSN={1573-7616}, url={http://dx.doi.org/10.1007/s10664-021-10034-0}, DOI={10.1007/s10664-021-10034-0}, number={1}, journal={Empirical Software Engineering}, publisher={Springer Science and Business Media LLC}, author={Wijerathna, Laksri and Aleti, Aldeida and Bi, Tingting and Tang, Antony}, year={2021}, month=oct }

 @article{Garriga_2016, title={A structural-semantic web service selection approach to improve retrievability of web services}, volume={20}, ISSN={1572-9419}, url={http://dx.doi.org/10.1007/s10796-016-9731-1}, DOI={10.1007/s10796-016-9731-1}, number={6}, journal={Information Systems Frontiers}, publisher={Springer Science and Business Media LLC}, author={Garriga, Martin and Renzis, Alan De and Lizarralde, Ignacio and Flores, Andres and Mateos, Cristian and Cechich, Alejandra and Zunino, Alejandro}, year={2016}, month=dec, pages={1319–1344} }

 @article{Haoues_2016, title={A guideline for software architecture selection based on ISO 25010 quality related characteristics}, volume={8}, ISSN={0976-4348}, url={http://dx.doi.org/10.1007/s13198-016-0546-8}, DOI={10.1007/s13198-016-0546-8}, number={S2}, journal={International Journal of System Assurance Engineering and Management}, publisher={Springer Science and Business Media LLC}, author={Haoues, Mariem and Sellami, Asma and Ben-Abdallah, Hanêne and Cheikhi, Laila}, year={2016}, month=nov, pages={886–909} }

 @article{Gerpheide_2015, title={Assessing and improving quality of QVTo model transformations}, volume={24}, ISSN={1573-1367}, url={http://dx.doi.org/10.1007/s11219-015-9280-8}, DOI={10.1007/s11219-015-9280-8}, number={3}, journal={Software Quality Journal}, publisher={Springer Science and Business Media LLC}, author={Gerpheide, Christine M. and Schiffelers, Ramon R. H. and Serebrenik, Alexander}, year={2015}, month=jun, pages={797–834} }

 @article{Zhang_2015, title={Task-driven manufacturing cloud service proactive discovery and optimal configuration method}, volume={84}, ISSN={1433-3015}, url={http://dx.doi.org/10.1007/s00170-015-7731-9}, DOI={10.1007/s00170-015-7731-9}, number={1–4}, journal={The International Journal of Advanced Manufacturing Technology}, publisher={Springer Science and Business Media LLC}, author={Zhang, Yingfeng and Xi, Dong and Li, Rui and Sun, Shudong}, year={2015}, month=sep, pages={29–45} }

 @article{Gao_2019, title={Transformation-based processing of typed resources for multimedia sources in the IoT environment}, volume={27}, ISSN={1572-8196}, url={http://dx.doi.org/10.1007/s11276-019-02200-6}, DOI={10.1007/s11276-019-02200-6}, number={5}, journal={Wireless Networks}, publisher={Springer Science and Business Media LLC}, author={Gao, Honghao and Duan, Yucong and Shao, Lixu and Sun, Xiaobing}, year={2019}, month=nov, pages={3377–3393} }

 @article{Hou_2018, title={Design and achievement of cloud geodatabase for a sponge city}, volume={25}, ISSN={2227-5223}, url={http://dx.doi.org/10.1007/s11771-018-3926-1}, DOI={10.1007/s11771-018-3926-1}, number={10}, journal={Journal of Central South University}, publisher={Springer Science and Business Media LLC}, author={Hou, Jing-wei and Sun, Shi-qin and Liu, Ren-tao and Li, Jian-hua and Zhang, Ming-xin}, year={2018}, month=oct, pages={2423–2437} }

 @article{Bento_2021, title={Automated Analysis of Distributed Tracing: Challenges and Research Directions}, volume={19}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-021-09551-5}, DOI={10.1007/s10723-021-09551-5}, number={1}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={Bento, Andre and Correia, Jaime and Filipe, Ricardo and Araujo, Filipe and Cardoso, Jorge}, year={2021}, month=feb }

 @article{Nogueira_2016, title={Issues on developing interoperable cloud applications: definitions, concepts, approaches, requirements, characteristics and evaluation models}, volume={4}, ISSN={2195-1721}, url={http://dx.doi.org/10.1186/s40411-016-0033-6}, DOI={10.1186/s40411-016-0033-6}, number={1}, journal={Journal of Software Engineering Research and Development}, publisher={Sociedade Brasileira de Computacao - SB}, author={Nogueira, Elias and Moreira, Ana and Lucrédio, Daniel and Garcia, Vinícius and Fortes, Renata}, year={2016}, month=dec }

 @article{Calvaresi_2016, title={Exploring the ambient assisted living domain: a systematic review}, volume={8}, ISSN={1868-5145}, url={http://dx.doi.org/10.1007/s12652-016-0374-3}, DOI={10.1007/s12652-016-0374-3}, number={2}, journal={Journal of Ambient Intelligence and Humanized Computing}, publisher={Springer Science and Business Media LLC}, author={Calvaresi, Davide and Cesarini, Daniel and Sernani, Paolo and Marinoni, Mauro and Dragoni, Aldo Franco and Sturm, Arnon}, year={2016}, month=may, pages={239–257} }

 @article{Yan_2019, title={A novel distributed Social Internet of Things service recommendation scheme based on LSH forest}, volume={25}, ISSN={1617-4917}, url={http://dx.doi.org/10.1007/s00779-019-01283-4}, DOI={10.1007/s00779-019-01283-4}, number={6}, journal={Personal and Ubiquitous Computing}, publisher={Springer Science and Business Media LLC}, author={Yan, Biwei and Yu, Jiguo and Yang, Meihong and Jiang, Honglu and Wan, Zhiguo and Ni, Lina}, year={2019}, month=oct, pages={1013–1026} }

 @article{Ciancone_2013, title={The KlaperSuite framework for model-driven reliability analysis of component-based systems}, volume={13}, ISSN={1619-1374}, url={http://dx.doi.org/10.1007/s10270-013-0334-8}, DOI={10.1007/s10270-013-0334-8}, number={4}, journal={Software &amp; Systems Modeling}, publisher={Springer Science and Business Media LLC}, author={Ciancone, Andrea and Drago, Mauro Luigi and Filieri, Antonio and Grassi, Vincenzo and Koziolek, Heiko and Mirandola, Raffaela}, year={2013}, month=mar, pages={1269–1290} }

 @article{Kim_2014, title={Recent advanced applications and services for intelligent ubiquitous environments}, volume={14}, ISSN={1572-9362}, url={http://dx.doi.org/10.1007/s10660-014-9142-7}, DOI={10.1007/s10660-014-9142-7}, number={3}, journal={Electronic Commerce Research}, publisher={Springer Science and Business Media LLC}, author={Kim, Jongsung and Chao, Han-Chieh and Nguyen, Uyen Trang}, year={2014}, month=sep, pages={217–221} }

 @article{Zhao_2015, title={Perceptual rate-distortion optimization for H.264/AVC video coding from both signal and vision perspectives}, volume={75}, ISSN={1573-7721}, url={http://dx.doi.org/10.1007/s11042-015-2533-5}, DOI={10.1007/s11042-015-2533-5}, number={5}, journal={Multimedia Tools and Applications}, publisher={Springer Science and Business Media LLC}, author={Zhao, Pinghua and Liu, Yanwei and Liu, Jinxia and Yao, Ruixiao and Ci, Song}, year={2015}, month=mar, pages={2781–2800} }

 @article{Liu_2014, title={Requirements model driven adaption and evolution of Internetware}, volume={57}, ISSN={1869-1919}, url={http://dx.doi.org/10.1007/s11432-014-5064-1}, DOI={10.1007/s11432-014-5064-1}, number={6}, journal={Science China Information Sciences}, publisher={Springer Science and Business Media LLC}, author={Liu, Lin and Yang, Chen and Wang, JianMin and Ye, XiaoJun and Liu, YingBo and Yang, HongJi and Liu, XiaoDong}, year={2014}, month=jan, pages={1–19} }

 @article{Semenov_2018, title={Patients Decision Aid System Based on FHIR Profiles}, volume={42}, ISSN={1573-689X}, url={http://dx.doi.org/10.1007/s10916-018-1016-4}, DOI={10.1007/s10916-018-1016-4}, number={9}, journal={Journal of Medical Systems}, publisher={Springer Science and Business Media LLC}, author={Semenov, Ilia and Kopanitsa, Georgy and Denisov, Dmitry and Alexandr, Yakovenko and Osenev, Roman and Andreychuk, Yury}, year={2018}, month=jul }

 @article{Previtali_2018, title={A brokered Virtual Hub approach for the generation of web applications based on historical maps}, volume={10}, ISSN={1866-928X}, url={http://dx.doi.org/10.1007/s12518-018-0235-1}, DOI={10.1007/s12518-018-0235-1}, number={4}, journal={Applied Geomatics}, publisher={Springer Science and Business Media LLC}, author={Previtali, Mattia and Latre, Miguel Ángel}, year={2018}, month=sep, pages={453–472} }

 @article{Oliveira_2015, title={Self-adaptation by coordination-targeted reconfigurations}, volume={3}, ISSN={2195-1721}, url={http://dx.doi.org/10.1186/s40411-015-0021-2}, DOI={10.1186/s40411-015-0021-2}, number={1}, journal={Journal of Software Engineering Research and Development}, publisher={Sociedade Brasileira de Computacao - SB}, author={Oliveira, Nuno and Barbosa, Luís S}, year={2015}, month=may }

 @article{Pang_2020, title={An efficient approach for multi-user multi-cloud service composition in human–land sustainable computational systems}, volume={76}, ISSN={1573-0484}, url={http://dx.doi.org/10.1007/s11227-019-03140-w}, DOI={10.1007/s11227-019-03140-w}, number={7}, journal={The Journal of Supercomputing}, publisher={Springer Science and Business Media LLC}, author={Pang, Beibei and Hao, Fei and Yang, Yixuan and Park, Doo-Soon}, year={2020}, month=jan, pages={5442–5459} }

 @article{Cognini_2016, title={Business process flexibility - a systematic literature review with a software systems perspective}, volume={20}, ISSN={1572-9419}, url={http://dx.doi.org/10.1007/s10796-016-9678-2}, DOI={10.1007/s10796-016-9678-2}, number={2}, journal={Information Systems Frontiers}, publisher={Springer Science and Business Media LLC}, author={Cognini, Riccardo and Corradini, Flavio and Gnesi, Stefania and Polini, Andrea and Re, Barbara}, year={2016}, month=jul, pages={343–371} }

 @article{Suarez_Meza_2013, title={Fully automated resource retrieval in telecommunications and internet converged environments}, volume={16}, ISSN={1572-9419}, url={http://dx.doi.org/10.1007/s10796-013-9457-2}, DOI={10.1007/s10796-013-9457-2}, number={1}, journal={Information Systems Frontiers}, publisher={Springer Science and Business Media LLC}, author={Suarez-Meza, Luis Javier and Zúñiga, Julián Andrés and Pedraza, Edgar Camilo and Corrales, Juan Carlos}, year={2013}, month=oct, pages={77–93} }

 @article{Vahidnia_2019, title={Crowdsource mapping of target buildings in hazard: the utilization of smartphone technologies and geographic services}, volume={12}, ISSN={1866-928X}, url={http://dx.doi.org/10.1007/s12518-019-00280-9}, DOI={10.1007/s12518-019-00280-9}, number={1}, journal={Applied Geomatics}, publisher={Springer Science and Business Media LLC}, author={Vahidnia, Mohammad H. and Hosseinali, Farhad and Shafiei, Maryam}, year={2019}, month=jul, pages={3–14} }

 @article{Redelinghuys_2019, title={A six-layer architecture for the digital twin: a manufacturing case study implementation}, volume={31}, ISSN={1572-8145}, url={http://dx.doi.org/10.1007/s10845-019-01516-6}, DOI={10.1007/s10845-019-01516-6}, number={6}, journal={Journal of Intelligent Manufacturing}, publisher={Springer Science and Business Media LLC}, author={Redelinghuys, A. J. H. and Basson, A. H. and Kruger, K.}, year={2019}, month=dec, pages={1383–1402} }

 @article{DeFranco_2017, title={The nonfunctional requirement focus in medical device software: a systematic mapping study and taxonomy}, volume={13}, ISSN={1614-5054}, url={http://dx.doi.org/10.1007/s11334-017-0301-6}, DOI={10.1007/s11334-017-0301-6}, number={2–3}, journal={Innovations in Systems and Software Engineering}, publisher={Springer Science and Business Media LLC}, author={DeFranco, J. and Kassab, M. and Laplante, P. and Laplante, N.}, year={2017}, month=aug, pages={81–100} }

 @article{Anastasopoulos_2015, title={Optical wireless network convergence in support of energy-efficient mobile cloud services}, volume={29}, ISSN={1572-8188}, url={http://dx.doi.org/10.1007/s11107-015-0494-2}, DOI={10.1007/s11107-015-0494-2}, number={3}, journal={Photonic Network Communications}, publisher={Springer Science and Business Media LLC}, author={Anastasopoulos, Markos P. and Tzanakaki, Anna and Rofoee, Bijan Rahimzadeh and Peng, Shuping and Yan, Yan and Simeonidou, Dimitra and Landi, Giada and Bernini, Giacomo and Ciulli, Nicola and Riera, Jordi Ferrer and Escalona, Eduard and Katsalis, Kostas and Korakis, Thanasis}, year={2015}, month=apr, pages={269–281} }

 @article{Bertolino_2017, title={A tour of secure software engineering solutions for connected vehicles}, volume={26}, ISSN={1573-1367}, url={http://dx.doi.org/10.1007/s11219-017-9393-3}, DOI={10.1007/s11219-017-9393-3}, number={4}, journal={Software Quality Journal}, publisher={Springer Science and Business Media LLC}, author={Bertolino, Antonia and Calabro’, Antonello and Di Giandomenico, Felicita and Lami, Giuseppe and Lonetti, Francesca and Marchetti, Eda and Martinelli, Fabio and Matteucci, Ilaria and Mori, Paolo}, year={2017}, month=nov, pages={1223–1256} }

 @inbook{Apel_2019, title={Towards a Metrics-Based Software Quality Rating for a Microservice Architecture: Case Study for a Measurement and Processing Infrastructure}, ISBN={9783030224820}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-22482-0_15}, DOI={10.1007/978-3-030-22482-0_15}, booktitle={Communications in Computer and Information Science}, publisher={Springer International Publishing}, author={Apel, Sebastian and Hertrampf, Florian and Späthe, Steffen}, year={2019}, pages={205–220} }

 @inbook{Avritzer_2018, title={A Quantitative Approach for the Assessment of Microservice Architecture Deployment Alternatives by Automated Performance Testing}, ISBN={9783030007614}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-00761-4_11}, DOI={10.1007/978-3-030-00761-4_11}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Avritzer, Alberto and Ferme, Vincenzo and Janes, Andrea and Russo, Barbara and Schulz, Henning and van Hoorn, André}, year={2018}, pages={159–174} }

 @inbook{Engel_2018, title={Evaluation of Microservice Architectures: A Metric and Tool-Based Approach}, ISBN={9783319929019}, ISSN={1865-1356}, url={http://dx.doi.org/10.1007/978-3-319-92901-9_8}, DOI={10.1007/978-3-319-92901-9_8}, booktitle={Information Systems in the Big Data Era}, publisher={Springer International Publishing}, author={Engel, Thomas and Langermeier, Melanie and Bauer, Bernhard and Hofmann, Alexander}, year={2018}, pages={74–89} }

 @inbook{Ntentos_2021, title={Evaluating and Improving Microservice Architecture Conformance to Architectural Design Decisions}, ISBN={9783030914318}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-91431-8_12}, DOI={10.1007/978-3-030-91431-8_12}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Ntentos, Evangelos and Zdun, Uwe and Plakidas, Konstantinos and Geiger, Sebastian}, year={2021}, pages={188–203} }

 @inbook{Branzov_2019, title={Service-Microservice Architecture for Context-Aware Content Delivery in National Geoinformation Center of Bulgaria}, ISBN={9783030349745}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-34974-5_4}, DOI={10.1007/978-3-030-34974-5_4}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Branzov, Todor and Ivanova, Krassimira and Georgiev, Mladen}, year={2019}, pages={40–50} }

 @inbook{Athanasopoulos_2021, title={Stability Metrics for Continuous Integration of Service-Oriented Systems}, ISBN={9783030742966}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-74296-6_11}, DOI={10.1007/978-3-030-74296-6_11}, booktitle={Web Engineering}, publisher={Springer International Publishing}, author={Athanasopoulos, Dionysis and Keenan, Daniel}, year={2021}, pages={139–147} }

 @inbook{Ntentos_2020, title={Assessing Architecture Conformance to Coupling-Related Patterns and Practices in Microservices}, ISBN={9783030589233}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-58923-3_1}, DOI={10.1007/978-3-030-58923-3_1}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Ntentos, Evangelos and Zdun, Uwe and Plakidas, Konstantinos and Meixner, Sebastian and Geiger, Sebastian}, year={2020}, pages={3–20} }

 @inbook{Oliveira_2014, title={Towards a Process to Design Architectures of Service-Oriented Robotic Systems}, ISBN={9783319099705}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-09970-5_20}, DOI={10.1007/978-3-319-09970-5_20}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Oliveira, Lucas Bueno Ruas and Leroux, Elena and Felizardo, Katia Romero and Oquendo, Flavio and Nakagawa, Elisa Yumi}, year={2014}, pages={218–225} }

 @inbook{Zdun_2017, title={Ensuring and Assessing Architecture Conformance to Microservice Decomposition Patterns}, ISBN={9783319690353}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-69035-3_29}, DOI={10.1007/978-3-319-69035-3_29}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Zdun, Uwe and Navarro, Elena and Leymann, Frank}, year={2017}, pages={411–429} }

 @inbook{Ntentos_2019, title={Supporting Architectural Decision Making on Data Management in Microservice Architectures}, ISBN={9783030299835}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-29983-5_2}, DOI={10.1007/978-3-030-29983-5_2}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Ntentos, Evangelos and Zdun, Uwe and Plakidas, Konstantinos and Schall, Daniel and Li, Fei and Meixner, Sebastian}, year={2019}, pages={20–36} }

 @inbook{Bogner_2020, title={Collecting Service-Based Maintainability Metrics from RESTful API Descriptions: Static Analysis and Threshold Derivation}, ISBN={9783030591557}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-59155-7_16}, DOI={10.1007/978-3-030-59155-7_16}, booktitle={Software Architecture}, publisher={Springer International Publishing}, author={Bogner, Justus and Wagner, Stefan and Zimmermann, Alfred}, year={2020}, pages={215–227} }

 @inbook{Oliveira_2015, title={RoboSeT: A Tool to Support Cataloging and Discovery of Services for Service-Oriented Robotic Systems}, ISBN={9783662481349}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-662-48134-9_7}, DOI={10.1007/978-3-662-48134-9_7}, booktitle={Robotics}, publisher={Springer Berlin Heidelberg}, author={Oliveira, Lucas Bueno Ruas and Amaral, Felipe Augusto and Martins, Diogo B. and Oquendo, Flavio and Nakagawa, Elisa Yumi}, year={2015}, pages={114–132} }

 @inbook{Wu_2018, title={Software Architecture Measurement—Experiences from a Multinational Company}, ISBN={9783030007614}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-00761-4_20}, DOI={10.1007/978-3-030-00761-4_20}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Wu, Wensheng and Cai, Yuanfang and Kazman, Rick and Mo, Ran and Liu, Zhipeng and Chen, Rongbiao and Ge, Yingan and Liu, Weicai and Zhang, Junhui}, year={2018}, pages={303–319} }

 @inbook{Schnoor_2019, title={Comparing Static and Dynamic Weighted Software Coupling Metrics}, ISBN={9783030302757}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-30275-7_22}, DOI={10.1007/978-3-030-30275-7_22}, booktitle={Information and Software Technologies}, publisher={Springer International Publishing}, author={Schnoor, Henning and Hasselbring, Wilhelm}, year={2019}, pages={285–298} }

 @inbook{Chauhan_2016, title={A Process Framework for Designing Software Reference Architectures for Providing Tools as a Service}, ISBN={9783319490946}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-49094-6_8}, DOI={10.1007/978-3-319-49094-6_8}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Chauhan, Muhammad Aufeef and Babar, Muhammad Ali and Probst, Christian W.}, year={2016}, pages={111–126} }

 @inbook{Salgado_2016, title={A Three-Dimensional Approach for a Quality-Based Alignment Between Requirements and Architecture}, ISBN={9783319326894}, ISSN={1865-1356}, url={http://dx.doi.org/10.1007/978-3-319-32689-4_9}, DOI={10.1007/978-3-319-32689-4_9}, booktitle={Exploring Services Science}, publisher={Springer International Publishing}, author={Salgado, Carlos E. and Machado, Ricardo J. and Maciel, Rita S. P.}, year={2016}, pages={112–125} }

 @inbook{Kirstein_2020, title={Piveau: A Large-Scale Open Data Management Platform Based on Semantic Web Technologies}, ISBN={9783030494612}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-49461-2_38}, DOI={10.1007/978-3-030-49461-2_38}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Kirstein, Fabian and Stefanidis, Kyriakos and Dittwald, Benjamin and Dutkowski, Simon and Urbanek, Sebastian and Hauswirth, Manfred}, year={2020}, pages={648–664} }

 @inbook{Deepika_2021, title={A Software Reusability Paradigm for Assessing Software-as-a-Service for Cloud Computing}, ISBN={9789813369818}, ISSN={2194-5365}, url={http://dx.doi.org/10.1007/978-981-33-6981-8_37}, DOI={10.1007/978-981-33-6981-8_37}, booktitle={Congress on Intelligent Systems}, publisher={Springer Singapore}, author={Deepika and Sangwan, Om Prakash}, year={2021}, pages={463–473} }

 @inbook{Bani_Ismail_2018, title={A Survey of Existing Evaluation Frameworks for Service Identification Methods: Towards a Comprehensive Evaluation Framework}, ISBN={9783319952048}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-319-95204-8_17}, DOI={10.1007/978-3-319-95204-8_17}, booktitle={Knowledge Management in Organizations}, publisher={Springer International Publishing}, author={Bani-Ismail, Basel and Baghdadi, Youcef}, year={2018}, pages={191–202} }

 @inbook{Manuali_2015, title={A Trial User, Resources and Services Quality Evaluation for Grid Communities Sustainability}, ISBN={9783319214078}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-21407-8_24}, DOI={10.1007/978-3-319-21407-8_24}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Manuali, Carlo and Laganà, Antonio}, year={2015}, pages={324–338} }

 @inbook{Buchgeher_2015, title={Making the Case for Centralized Software Architecture Management}, ISBN={9783319270333}, ISSN={1865-1356}, url={http://dx.doi.org/10.1007/978-3-319-27033-3_8}, DOI={10.1007/978-3-319-27033-3_8}, booktitle={Software Quality. The Future of Systems- and Software Development}, publisher={Springer International Publishing}, author={Buchgeher, Georg and Weinreich, Rainer and Kriechbaum, Thomas}, year={2015}, month=dec, pages={109–121} }

 @inbook{Salgado_2015, title={Aligning Business Requirements with Services Quality Characteristics by Using Logical Architectures}, ISBN={9783319164861}, ISSN={2194-5365}, url={http://dx.doi.org/10.1007/978-3-319-16486-1_58}, DOI={10.1007/978-3-319-16486-1_58}, booktitle={Advances in Intelligent Systems and Computing}, publisher={Springer International Publishing}, author={Salgado, Carlos E. and Machado, Ricardo J. and Maciel, Rita S. P.}, year={2015}, pages={593–602} }

 @inbook{Sabou_2018, title={Exploring Enterprise Knowledge Graphs: A Use Case in Software Engineering}, ISBN={9783319934174}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-93417-4_36}, DOI={10.1007/978-3-319-93417-4_36}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Sabou, Marta and Ekaputra, Fajar J. and Ionescu, Tudor and Musil, Juergen and Schall, Daniel and Haller, Kevin and Friedl, Armin and Biffl, Stefan}, year={2018}, pages={560–575} }

 @inbook{Muccini_2018, title={IoT Architectural Styles: A Systematic Mapping Study}, ISBN={9783030007614}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-00761-4_5}, DOI={10.1007/978-3-030-00761-4_5}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Muccini, Henry and Moghaddam, Mahyar Tourchi}, year={2018}, pages={68–85} }

 @inbook{Schermann_2020, title={Topology-Aware Continuous Experimentation in Microservice-Based Applications}, ISBN={9783030653101}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-65310-1_2}, DOI={10.1007/978-3-030-65310-1_2}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Schermann, Gerald and Oliveira, Fábio and Wittern, Erik and Leitner, Philipp}, year={2020}, pages={19–35} }

 @inbook{Kopp_2021, title={Towards the Method and Information Technology for Evaluation of Business Process Model Quality}, ISBN={9783030775926}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-77592-6_5}, DOI={10.1007/978-3-030-77592-6_5}, booktitle={Communications in Computer and Information Science}, publisher={Springer International Publishing}, author={Kopp, Andrii and Orlovskyi, Dmytro}, year={2021}, pages={93–118} }

 @inbook{Adjepon_Yamoah_2016, title={cloud-ATAM: Method for Analysing Resilient Attributes of Cloud-Based Architectures}, ISBN={9783319458922}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-45892-2_8}, DOI={10.1007/978-3-319-45892-2_8}, booktitle={Software Engineering for Resilient Systems}, publisher={Springer International Publishing}, author={Adjepon-Yamoah, David Ebo}, year={2016}, pages={105–114} }

 @inbook{Ardagna_2014, title={A Multi-model Optimization Framework for the Model Driven Design of Cloud Applications}, ISBN={9783319099408}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-09940-8_5}, DOI={10.1007/978-3-319-09940-8_5}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Ardagna, Danilo and Gibilisco, Giovanni Paolo and Ciavotta, Michele and Lavrentev, Alexander}, year={2014}, pages={61–76} }

 @inbook{Ye_2021, title={EFMLP: A Novel Model for Web Service QoS Prediction}, ISBN={9783030675400}, ISSN={1867-822X}, url={http://dx.doi.org/10.1007/978-3-030-67540-0_22}, DOI={10.1007/978-3-030-67540-0_22}, booktitle={Collaborative Computing: Networking, Applications and Worksharing}, publisher={Springer International Publishing}, author={Ye, Kailing and Yu, Huiqun and Fan, Guisheng and Chen, Liqiong}, year={2021}, pages={375–385} }

 @inbook{Wen_2017, title={Runtime Exceptions Handling for Collaborative SOA Applications}, ISBN={9783319592886}, ISSN={1867-822X}, url={http://dx.doi.org/10.1007/978-3-319-59288-6_23}, DOI={10.1007/978-3-319-59288-6_23}, booktitle={Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering}, publisher={Springer International Publishing}, author={Wen, Bin and Luo, Ziqiang and Lin, Song}, year={2017}, pages={252–261} }

 @inbook{Shahin_2015, title={Improving the Quality of Architecture Design Through Peer-Reviews and Recombination}, ISBN={9783319237275}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-23727-5_6}, DOI={10.1007/978-3-319-23727-5_6}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Shahin, Mojtaba and Babar, Muhammad Ali}, year={2015}, pages={70–86} }

 @inbook{Rabelo_2020, title={For a Dynamic Web Services Discovery Model for Open Ecosystems of Software Providers}, ISBN={9783030624125}, ISSN={1868-422X}, url={http://dx.doi.org/10.1007/978-3-030-62412-5_7}, DOI={10.1007/978-3-030-62412-5_7}, booktitle={IFIP Advances in Information and Communication Technology}, publisher={Springer International Publishing}, author={Rabelo, Ricardo J. and Ruiz, Hernesto A. and Cancian, Maiara H.}, year={2020}, pages={83–97} }

 @inbook{Colomo_Palacios_2016, title={Towards Supporting International Standard-Based Software Engineering Approaches Using Semantic Web Technologies: A Systematic Literature Review}, ISBN={9783319480244}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-319-48024-4_14}, DOI={10.1007/978-3-319-48024-4_14}, booktitle={Technologies and Innovation}, publisher={Springer International Publishing}, author={Colomo-Palacios, Ricardo and Colombo-Mendoza, Luis Omar and Valencia-García, Rafael}, year={2016}, pages={169–183} }

 @inbook{Abrahao_2016, title={Human Factors in Software Development Processes: Measuring System Quality}, ISBN={9783319490946}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-49094-6_57}, DOI={10.1007/978-3-319-49094-6_57}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Abrahao, Silvia and Baldassarre, Maria Teresa and Caivano, Danilo and Dittrich, Yvonne and Lanzilotti, Rosa and Piccinno, Antonio}, year={2016}, pages={691–696} }

 @inbook{Brandenburger_2021, title={Quality 4.0 - Transparent Product Quality Supervision in the Age of Industry 4.0}, ISBN={9783030693671}, ISSN={2194-5365}, url={http://dx.doi.org/10.1007/978-3-030-69367-1_5}, DOI={10.1007/978-3-030-69367-1_5}, booktitle={Impact and Opportunities of Artificial Intelligence Techniques in the Steel Industry}, publisher={Springer International Publishing}, author={Brandenburger, Jens and Schirm, Christoph and Melcher, Josef and Hancke, Edgar and Vannucci, Marco and Colla, Valentina and Cateni, Silvia and Sellami, Rami and Dupont, Sébastien and Majchrowski, Annick and Arteaga, Asier}, year={2021}, pages={54–66} }

 @inbook{Elberzhager_2017, title={High Quality at Short Time-to-Market: Challenges Towards This Goal and Guidelines for the Realization}, ISBN={9783319714400}, ISSN={1865-1356}, url={http://dx.doi.org/10.1007/978-3-319-71440-0_7}, DOI={10.1007/978-3-319-71440-0_7}, booktitle={Software Quality: Methods and Tools for Better Software and Systems}, publisher={Springer International Publishing}, author={Elberzhager, Frank and Naab, Matthias}, year={2017}, month=nov, pages={121–132} }

 @inbook{De_Sanctis_2020, title={A Flexible Architecture for Key Performance Indicators Assessment in Smart Cities}, ISBN={9783030589233}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-58923-3_8}, DOI={10.1007/978-3-030-58923-3_8}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={De Sanctis, Martina and Iovino, Ludovico and Rossi, Maria Teresa and Wimmer, Manuel}, year={2020}, pages={118–135} }

 @inbook{Huynh_2015, title={A Lightweight Formal Approach for Component Reuse}, ISBN={9783319116808}, ISSN={2194-5365}, url={http://dx.doi.org/10.1007/978-3-319-11680-8_41}, DOI={10.1007/978-3-319-11680-8_41}, booktitle={Knowledge and Systems Engineering}, publisher={Springer International Publishing}, author={Huynh, Khai T. and Bui, Thang H. and Quan, Tho T.}, year={2015}, pages={513–524} }

 @inbook{Komarek_2015, title={Network Visualization Survey}, ISBN={9783319243061}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-319-24306-1_27}, DOI={10.1007/978-3-319-24306-1_27}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Komarek, Ales and Pavlik, Jakub and Sobeslav, Vladimir}, year={2015}, pages={275–284} }

 @inbook{Marins_2014, title={Improving High Availability and Reliability of Health Interoperability Systems}, ISBN={9783319059488}, ISSN={2194-5365}, url={http://dx.doi.org/10.1007/978-3-319-05948-8_20}, DOI={10.1007/978-3-319-05948-8_20}, booktitle={New Perspectives in Information Systems and Technologies, Volume 2}, publisher={Springer International Publishing}, author={Marins, Fernando and Cardoso, Luciana and Portela, Filipe and Santos, Manuel F. and Abelha, António and Machado, José}, year={2014}, pages={207–216} }

 @inbook{Alvarado_Valiente_2023, title={Quantum Services Generation and Deployment Process: A Quality-Oriented Approach}, ISBN={9783031437038}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-031-43703-8_15}, DOI={10.1007/978-3-031-43703-8_15}, booktitle={Quality of Information and Communications Technology}, publisher={Springer Nature Switzerland}, author={Alvarado-Valiente, Jaime and Romero-Álvarez, Javier and Díaz, Ana and Rodríguez, Moisés and García-Rodríguez, Ignacio and Moguel, Enrique and Garcia-Alonso, Jose and Murillo, Juan M.}, year={2023}, pages={200–214} }

 @inbook{Wentzel_2023, title={An Extensive Methodology and Framework for Quality Assessment of DCAT-AP Datasets}, ISBN={9783031411380}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-41138-0_17}, DOI={10.1007/978-3-031-41138-0_17}, booktitle={Electronic Government}, publisher={Springer Nature Switzerland}, author={Wentzel, Bianca and Kirstein, Fabian and Jastrow, Torben and Sturm, Raphael and Peters, Michael and Schimmler, Sonja}, year={2023}, pages={262–278} }

 @inbook{Graciano_Neto_2022, title={Foundations and Research Agenda for Simulation of Smart Ecosystems Architectures}, ISBN={9783031151163}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-15116-3_15}, DOI={10.1007/978-3-031-15116-3_15}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Graciano Neto, Valdemar Vicente and Manzano, Wallace and Antonino, Pablo Oliveira and Nakagawa, Elisa Yumi}, year={2022}, pages={333–352} }

 @inbook{Quattrocchi_2022, title={Blockchain-Oriented Services Computing in Action: Insights from a User Study}, ISBN={9783031209840}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-20984-0_27}, DOI={10.1007/978-3-031-20984-0_27}, booktitle={Lecture Notes in Computer Science}, publisher={Springer Nature Switzerland}, author={Quattrocchi, Giovanni and Tamburri, Damian Andrew and Heuvel, Willem-Jan Van Den}, year={2022}, pages={384–391} }

 @inbook{Gudenkauf_2023, title={A Concept and a Multitenant Web Application for Interactive Software Architecture Analysis}, ISBN={9783031268861}, ISSN={1865-1356}, url={http://dx.doi.org/10.1007/978-3-031-26886-1_16}, DOI={10.1007/978-3-031-26886-1_16}, booktitle={Enterprise Design, Operations, and Computing. EDOC 2022 Workshops}, publisher={Springer International Publishing}, author={Gudenkauf, Stefan and Bachmann, Uwe and Hartmann, Niklas}, year={2023}, pages={268–283} }

 @inbook{Qui_a_Mera_2022, title={GraphQL or REST for Mobile Applications?}, ISBN={9783031203190}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-031-20319-0_2}, DOI={10.1007/978-3-031-20319-0_2}, booktitle={Advanced Research in Technologies, Information, Innovation and Sustainability}, publisher={Springer Nature Switzerland}, author={Quiña-Mera, Antonio and García, José María and Fernández, Pablo and Vega-Molina, Paúl and Ruiz-Cortés, Antonio}, year={2022}, pages={16–30} }

 @inbook{Li_2023, title={General Architecture Framework and General Modelling Framework: Interoperability of Enterprise Architecture}, ISBN={9783031372285}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-031-37228-5_9}, DOI={10.1007/978-3-031-37228-5_9}, booktitle={Communications in Computer and Information Science}, publisher={Springer Nature Switzerland}, author={Li, Qing and Fang, Zhixiong and Liang, Bohang}, year={2023}, pages={135–158} }

 @inbook{Christy_Eunaicy_2022, title={Personalization and Prediction System Based on Learner Assessment Attributes Using CNN in E-learning Environment}, ISBN={9789811948312}, ISSN={1876-1119}, url={http://dx.doi.org/10.1007/978-981-19-4831-2_28}, DOI={10.1007/978-981-19-4831-2_28}, booktitle={Applications of Artificial Intelligence and Machine Learning}, publisher={Springer Nature Singapore}, author={Christy Eunaicy, J. I. and Sundaravadivelu, V. and Suguna, S.}, year={2022}, pages={343–356} }

 @inbook{Zouari_2022, title={A Service-Based Framework for Adaptive Data Curation in Data Lakehouses}, ISBN={9783031208911}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-20891-1_17}, DOI={10.1007/978-3-031-20891-1_17}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Zouari, Firas and Ghedira-Guegan, Chirine and Boukadi, Khouloud and Kabachi, Nadia}, year={2022}, pages={225–240} }

 @article{Zeng_2022, title={Performance optimization for cloud computing systems in the microservice era: state-of-the-art and research opportunities}, volume={16}, ISSN={2095-2236}, url={http://dx.doi.org/10.1007/s11704-020-0072-3}, DOI={10.1007/s11704-020-0072-3}, number={6}, journal={Frontiers of Computer Science}, publisher={Springer Science and Business Media LLC}, author={Zeng, Rong and Hou, Xiaofeng and Zhang, Lu and Li, Chao and Zheng, Wenli and Guo, Minyi}, year={2022}, month=jan }

 @article{Reyes_Delgado_2021, title={SOCAM: a service-oriented computing architecture modeling method}, volume={21}, ISSN={1619-1374}, url={http://dx.doi.org/10.1007/s10270-021-00946-2}, DOI={10.1007/s10270-021-00946-2}, number={4}, journal={Software and Systems Modeling}, publisher={Springer Science and Business Media LLC}, author={Reyes-Delgado, Paola Y. and Duran-Limon, Hector A. and Mora, Manuel and Rodriguez-Martinez, Laura C.}, year={2021}, month=nov, pages={1551–1581} }

 @article{Soares_2023, title={Trends in continuous evaluation of software architectures}, volume={105}, ISSN={1436-5057}, url={http://dx.doi.org/10.1007/s00607-023-01161-1}, DOI={10.1007/s00607-023-01161-1}, number={9}, journal={Computing}, publisher={Springer Science and Business Media LLC}, author={Soares, Rodrigo C. and Capilla, Rafael and dos Santos, Vinicius and Nakagawa, Elisa Yumi}, year={2023}, month=feb, pages={1957–1980} }

 @article{Krishnamoorthy_2021, title={Role of emerging technologies in future IoT-driven Healthcare 4.0 technologies: a survey, current challenges and future directions}, volume={14}, ISSN={1868-5145}, url={http://dx.doi.org/10.1007/s12652-021-03302-w}, DOI={10.1007/s12652-021-03302-w}, number={1}, journal={Journal of Ambient Intelligence and Humanized Computing}, publisher={Springer Science and Business Media LLC}, author={Krishnamoorthy, Sreelakshmi and Dua, Amit and Gupta, Shashank}, year={2021}, month=may, pages={361–407} }

 @article{Rwemalika_2022, title={Smells in system user interactive tests}, volume={28}, ISSN={1573-7616}, url={http://dx.doi.org/10.1007/s10664-022-10251-1}, DOI={10.1007/s10664-022-10251-1}, number={1}, journal={Empirical Software Engineering}, publisher={Springer Science and Business Media LLC}, author={Rwemalika, Renaud and Habchi, Sarra and Papadakis, Mike and Le Traon, Yves and Brasseur, Marie-Claude}, year={2022}, month=dec }

 @article{Zouari_2023, title={A semantic and service-based approach for adaptive mutli-structured data curation in data lakehouses}, volume={26}, ISSN={1573-1413}, url={http://dx.doi.org/10.1007/s11280-023-01218-3}, DOI={10.1007/s11280-023-01218-3}, number={6}, journal={World Wide Web}, publisher={Springer Science and Business Media LLC}, author={Zouari, Firas and Ghedira-Guegan, Chirine and Boukadi, Khouloud and Kabachi, Nadia}, year={2023}, month=nov, pages={4001–4023} }

 @article{Valderas_2022, title={Towards an Interdisciplinary Development of IoT-Enhanced Business Processes}, volume={65}, ISSN={1867-0202}, url={http://dx.doi.org/10.1007/s12599-022-00770-y}, DOI={10.1007/s12599-022-00770-y}, number={1}, journal={Business &amp; Information Systems Engineering}, publisher={Springer Science and Business Media LLC}, author={Valderas, Pedro and Torres, Victoria and Serral, Estefanía}, year={2022}, month=aug, pages={25–48} }

 @article{Farshidi_2023, title={Business process modeling language selection for research modelers}, ISSN={1619-1374}, url={http://dx.doi.org/10.1007/s10270-023-01110-8}, DOI={10.1007/s10270-023-01110-8}, journal={Software and Systems Modeling}, publisher={Springer Science and Business Media LLC}, author={Farshidi, Siamak and Kwantes, Izaak Beer and Jansen, Slinger}, year={2023}, month=may }

 @article{Davami_2021, title={Fog-based architecture for scheduling multiple workflows with high availability requirement}, volume={104}, ISSN={1436-5057}, url={http://dx.doi.org/10.1007/s00607-021-00905-1}, DOI={10.1007/s00607-021-00905-1}, number={1}, journal={Computing}, publisher={Springer Science and Business Media LLC}, author={Davami, Fatemeh and Adabi, Sahar and Rezaee, Ali and Rahmani, Amir Masoud}, year={2021}, month=feb, pages={169–208} }

 @article{Wang_2021, title={QoS-Aware Service Discovery and Selection Management for Cloud-Edge Computing Using a Hybrid Meta-Heuristic Algorithm in IoT}, volume={126}, ISSN={1572-834X}, url={http://dx.doi.org/10.1007/s11277-021-09052-4}, DOI={10.1007/s11277-021-09052-4}, number={3}, journal={Wireless Personal Communications}, publisher={Springer Science and Business Media LLC}, author={Wang, Ronghan and Lu, Junwei}, year={2021}, month=aug, pages={2269–2282} }

 @article{Ben_tez_Guijarro_2019, title={Architecting dietary intake monitoring as a service combining NLP and IoT}, volume={13}, ISSN={1868-5145}, url={http://dx.doi.org/10.1007/s12652-019-01553-2}, DOI={10.1007/s12652-019-01553-2}, number={11}, journal={Journal of Ambient Intelligence and Humanized Computing}, publisher={Springer Science and Business Media LLC}, author={Benítez-Guijarro, Antonio and Callejas, Zoraida and Noguera, Manuel and Benghazi, Kawtar}, year={2019}, month=nov, pages={5377–5389} }

 @article{Shamsa_2023, title={A decentralized prediction-based workflow load balancing architecture for cloud/fog/IoT environments}, ISSN={1436-5057}, url={http://dx.doi.org/10.1007/s00607-023-01216-3}, DOI={10.1007/s00607-023-01216-3}, journal={Computing}, publisher={Springer Science and Business Media LLC}, author={Shamsa, Zari and Rezaee, Ali and Adabi, Sahar and Rahmani, Amir Masoud}, year={2023}, month=aug }

 @article{Lagan__2023, title={From molecular beam technologies to virtual experiments and communities}, volume={34}, ISSN={1720-0776}, url={http://dx.doi.org/10.1007/s12210-023-01203-y}, DOI={10.1007/s12210-023-01203-y}, number={4}, journal={Rendiconti Lincei. Scienze Fisiche e Naturali}, publisher={Springer Science and Business Media LLC}, author={Laganà, Antonio}, year={2023}, month=nov, pages={1013–1020} }

 @article{Zhao_2023, title={A comprehensive and systematic review of the banking systems based on pay-as-you-go payment fashion and cloud computing in the pandemic era}, ISSN={1617-9854}, url={http://dx.doi.org/10.1007/s10257-022-00617-9}, DOI={10.1007/s10257-022-00617-9}, journal={Information Systems and e-Business Management}, publisher={Springer Science and Business Media LLC}, author={Zhao, Shajunyi and Miao, Jianchun and Zhao, Jingfeng and Naghshbandi, Nader}, year={2023}, month=jan }

 @article{Mehmood_2022, title={Distributed real-time ETL architecture for unstructured big data}, volume={64}, ISSN={0219-3116}, url={http://dx.doi.org/10.1007/s10115-022-01757-7}, DOI={10.1007/s10115-022-01757-7}, number={12}, journal={Knowledge and Information Systems}, publisher={Springer Science and Business Media LLC}, author={Mehmood, Erum and Anees, Tayyaba}, year={2022}, month=sep, pages={3419–3445} }

 @article{Ren_2022, title={Review on R&amp;D task integrated management of intelligent manufacturing equipment}, volume={34}, ISSN={1433-3058}, url={http://dx.doi.org/10.1007/s00521-022-07023-9}, DOI={10.1007/s00521-022-07023-9}, number={8}, journal={Neural Computing and Applications}, publisher={Springer Science and Business Media LLC}, author={Ren, Teng and Luo, Tianyu and Li, Shuxuan and Xing, Lining and Xiang, Shang}, year={2022}, month=feb, pages={5813–5837} }

 @article{Iqbal_2023, title={Advancing database security: a comprehensive systematic mapping study of potential challenges}, ISSN={1572-8196}, url={http://dx.doi.org/10.1007/s11276-023-03436-z}, DOI={10.1007/s11276-023-03436-z}, journal={Wireless Networks}, publisher={Springer Science and Business Media LLC}, author={Iqbal, Asif and Khan, Siffat Ullah and Niazi, Mahmood and Humayun, Mamoona and Sama, Najm Us and Khan, Arif Ali and Ahmad, Aakash}, year={2023}, month=jul }

 @inbook{Tsoumas_2020, title={Modelling 5G Cloud-Native Applications by Exploiting the Service Mesh Paradigm}, ISBN={9783030443221}, ISSN={1865-1356}, url={http://dx.doi.org/10.1007/978-3-030-44322-1_12}, DOI={10.1007/978-3-030-44322-1_12}, booktitle={Lecture Notes in Business Information Processing}, publisher={Springer International Publishing}, author={Tsoumas, Ilias and Symvoulidis, Chrysostomos and Kyriazis, Dimosthenis and Gouvas, Panagiotis and Zafeiropoulos, Anastasios and Melian, Javier and Sterle, Janez}, year={2020}, pages={151–162} }

 @inbook{Spillner_2018, title={Cloud-Native Databases: An Application Perspective}, ISBN={9783319790909}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-319-79090-9_7}, DOI={10.1007/978-3-319-79090-9_7}, booktitle={Advances in Service-Oriented and Cloud Computing}, publisher={Springer International Publishing}, author={Spillner, Josef and Toffetti, Giovanni and López, Manuel Ramírez}, year={2018}, pages={102–116} }

 @inbook{Bryzgalov_2021, title={A Cloud-Native Serverless Approach for Implementation of Batch Extract-Load Processes in Data Lakes}, ISBN={9783030812003}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-81200-3_3}, DOI={10.1007/978-3-030-81200-3_3}, booktitle={Data Analytics and Management in Data Intensive Domains}, publisher={Springer International Publishing}, author={Bryzgalov, Anton and Stupnikov, Sergey}, year={2021}, pages={27–42} }

 @inbook{Kratzke_2017, title={Investigation of Impacts on Network Performance in the Advance of a Microservice Design}, ISBN={9783319625942}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-319-62594-2_10}, DOI={10.1007/978-3-319-62594-2_10}, booktitle={Cloud Computing and Services Science}, publisher={Springer International Publishing}, author={Kratzke, Nane and Quint, Peter-Christian}, year={2017}, pages={187–208} }

 @inbook{Mercl_2019, title={Public Cloud Kubernetes Storage Performance Analysis}, ISBN={9783030283742}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-28374-2_56}, DOI={10.1007/978-3-030-28374-2_56}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Mercl, Lubos and Pavlik, Jakub}, year={2019}, pages={649–660} }

 @inbook{Mandal_2021, title={Improved Topology Extraction Using Discriminative Parameter Mining of Logs}, ISBN={9783030757625}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-75762-5_27}, DOI={10.1007/978-3-030-75762-5_27}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Mandal, Atri and Gupta, Saranya and Agarwal, Shivali and Mohapatra, Prateeti}, year={2021}, pages={333–345} }

 @inbook{Ribera_Laszkowski_2020, title={ElasTest: An Elastic Platform for E2E Testing Complex Distributed Large Software Systems}, ISBN={9783030631611}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-63161-1_20}, DOI={10.1007/978-3-030-63161-1_20}, booktitle={Advances in Service-Oriented and Cloud Computing}, publisher={Springer International Publishing}, author={Ribera Laszkowski, Juan Francisco and Edmonds, Andy and Harsh, Piyush and Gortazar, Francisco and Bohnert, Thomas Michael}, year={2020}, pages={210–218} }

 @inbook{Poth_2018, title={How to Deliver Faster with CI/CD Integrated Testing Services?}, ISBN={9783319979250}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-319-97925-0_33}, DOI={10.1007/978-3-319-97925-0_33}, booktitle={Systems, Software and Services Process Improvement}, publisher={Springer International Publishing}, author={Poth, Alexander and Werner, Mark and Lei, Xinyan}, year={2018}, pages={401–409} }

 @inbook{Lin_2018, title={Microscope: Pinpoint Performance Issues with Causal Graphs in Micro-service Environments}, ISBN={9783030035969}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-03596-9_1}, DOI={10.1007/978-3-030-03596-9_1}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Lin, Jinjin and Chen, Pengfei and Zheng, Zibin}, year={2018}, pages={3–20} }

 @inbook{Jindal_2021, title={From DevOps to NoOps: Is It Worth It?}, ISBN={9783030723699}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-72369-9_8}, DOI={10.1007/978-3-030-72369-9_8}, booktitle={Cloud Computing and Services Science}, publisher={Springer International Publishing}, author={Jindal, Anshul and Gerndt, Michael}, year={2021}, pages={178–202} }

 @inbook{Rosati_2019, title={Right Scaling for Right Pricing: A Case Study on Total Cost of Ownership Measurement for Cloud Migration}, ISBN={9783030291938}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-29193-8_10}, DOI={10.1007/978-3-030-29193-8_10}, booktitle={Cloud Computing and Services Science}, publisher={Springer International Publishing}, author={Rosati, Pierangelo and Fowley, Frank and Pahl, Claus and Taibi, Davide and Lynn, Theo}, year={2019}, pages={190–214} }

 @inbook{Fowley_2018, title={Cloud Migration Architecture and Pricing – Mapping a Licensing Business Model for Software Vendors to a SaaS Business Model}, ISBN={9783319721255}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-319-72125-5_7}, DOI={10.1007/978-3-319-72125-5_7}, booktitle={Advances in Service-Oriented and Cloud Computing}, publisher={Springer International Publishing}, author={Fowley, Frank and Pahl, Claus}, year={2018}, pages={91–103} }

 @inbook{Banijamali_2020, title={Kuksa$$^{*}$$: Self-adaptive Microservices in Automotive Systems}, ISBN={9783030641481}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-64148-1_23}, DOI={10.1007/978-3-030-64148-1_23}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Banijamali, Ahmad and Kuvaja, Pasi and Oivo, Markku and Jamshidi, Pooyan}, year={2020}, pages={367–384} }

 @inbook{Gkikopoulos_2021, title={Analysis and Improvement of Heterogeneous Hardware Support in Docker Images}, ISBN={9783030781989}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-78198-9_9}, DOI={10.1007/978-3-030-78198-9_9}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Gkikopoulos, Panagiotis and Schiavoni, Valerio and Spillner, Josef}, year={2021}, pages={125–142} }

 @inbook{Floerecke_2019, title={Business Model Characteristics for Local IaaS Providers for Counteracting the Dominance of the Hyperscalers}, ISBN={9783030133429}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-13342-9_12}, DOI={10.1007/978-3-030-13342-9_12}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Floerecke, Sebastian and Lehner, Franz}, year={2019}, pages={137–150} }

 @inbook{Bagnato_2019, title={European Project Space Papers for the PROFES 2019 - Summary}, ISBN={9783030353339}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-35333-9_40}, DOI={10.1007/978-3-030-35333-9_40}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Bagnato, Alessandra and Fucci, Davide}, year={2019}, pages={573–576} }

 @inbook{Dalla_Palma_2021, title={DevOps and Quality Management in Serverless Computing: The RADON Approach}, ISBN={9783030719067}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-030-71906-7_13}, DOI={10.1007/978-3-030-71906-7_13}, booktitle={Advances in Service-Oriented and Cloud Computing}, publisher={Springer International Publishing}, author={Dalla Palma, Stefano and Garriga, Martin and Di Nucci, Dario and Tamburri, Damian Andrew and Van Den Heuvel, Willem-Jan}, year={2021}, pages={155–160} }

 @inbook{Mubarkoot_2021, title={Towards Software Compliance Specification and Enforcement Using TOSCA}, ISBN={9783030929169}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-92916-9_14}, DOI={10.1007/978-3-030-92916-9_14}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Mubarkoot, Mohammed and Altmann, Jörn}, year={2021}, pages={168–177} }

 @inbook{Floerecke_2018, title={Success Factors of SaaS Providers’ Business Models – An Exploratory Multiple-Case Study}, ISBN={9783030007133}, ISSN={1865-1356}, url={http://dx.doi.org/10.1007/978-3-030-00713-3_15}, DOI={10.1007/978-3-030-00713-3_15}, booktitle={Lecture Notes in Business Information Processing}, publisher={Springer International Publishing}, author={Floerecke, Sebastian}, year={2018}, pages={193–207} }

 @inbook{Shabelnyk_2021, title={Updating Service-Based Software Systems in Air-Gapped Environments}, ISBN={9783030860448}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-86044-8_10}, DOI={10.1007/978-3-030-86044-8_10}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Shabelnyk, Oleksandr and Frangoudis, Pantelis A. and Dustdar, Schahram and Tsigkanos, Christos}, year={2021}, pages={147–163} }

 @inbook{Andrikopoulos_2018, title={Engineering Cloud-Based Applications: Towards an Application Lifecycle}, ISBN={9783319790909}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-319-79090-9_4}, DOI={10.1007/978-3-319-79090-9_4}, booktitle={Advances in Service-Oriented and Cloud Computing}, publisher={Springer International Publishing}, author={Andrikopoulos, Vasilios}, year={2018}, pages={57–72} }

 @article{Kosi_ska_2020, title={Autonomic Management Framework for Cloud-Native Applications}, volume={18}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-020-09532-0}, DOI={10.1007/s10723-020-09532-0}, number={4}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={Kosińska, Joanna and Zieliński, Krzysztof}, year={2020}, month=sep, pages={779–796} }

 @article{Kehrer_2019, title={Migrating parallel applications to the cloud: assessing cloud readiness based on parallel design decisions}, volume={34}, ISSN={2524-8529}, url={http://dx.doi.org/10.1007/s00450-019-00396-8}, DOI={10.1007/s00450-019-00396-8}, number={2–3}, journal={SICS Software-Intensive Cyber-Physical Systems}, publisher={Springer Science and Business Media LLC}, author={Kehrer, Stefan and Blochinger, Wolfgang}, year={2019}, month=feb, pages={73–84} }

 @article{Pecorelli_2021, title={The Relation of Test-Related Factors to Software Quality: A Case Study on Apache Systems}, volume={26}, ISSN={1573-7616}, url={http://dx.doi.org/10.1007/s10664-020-09891-y}, DOI={10.1007/s10664-020-09891-y}, number={2}, journal={Empirical Software Engineering}, publisher={Springer Science and Business Media LLC}, author={Pecorelli, Fabiano and Palomba, Fabio and De Lucia, Andrea}, year={2021}, month=feb }

 @article{Toka_2021, title={Ultra-Reliable and Low-Latency Computing in the Edge with Kubernetes}, volume={19}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-021-09573-z}, DOI={10.1007/s10723-021-09573-z}, number={3}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={Toka, László}, year={2021}, month=jul }

 @article{Wei_2021, title={Deployment Management and Topology Discovery of Microservice Applications in the Multicloud Environment}, volume={19}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-021-09539-1}, DOI={10.1007/s10723-021-09539-1}, number={1}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={Wei, Hao and Rodriguez, Joaquin Salvachua and Garcia, Octavio Nieto-Taladriz}, year={2021}, month=jan }

 @article{Li_2021, title={Enjoy your observability: an industrial survey of microservice tracing and analysis}, volume={27}, ISSN={1573-7616}, url={http://dx.doi.org/10.1007/s10664-021-10063-9}, DOI={10.1007/s10664-021-10063-9}, number={1}, journal={Empirical Software Engineering}, publisher={Springer Science and Business Media LLC}, author={Li, Bowen and Peng, Xin and Xiang, Qilin and Wang, Hanzhang and Xie, Tao and Sun, Jun and Liu, Xuanzhe}, year={2021}, month=nov }

 @article{Sfaxi_2021, title={Designing and implementing a Big Data benchmark in a financial context: application to a cash management use case}, volume={103}, ISSN={1436-5057}, url={http://dx.doi.org/10.1007/s00607-021-00933-x}, DOI={10.1007/s00607-021-00933-x}, number={9}, journal={Computing}, publisher={Springer Science and Business Media LLC}, author={Sfaxi, Lilia and Ben Aissa, Mohamed Mehdi}, year={2021}, month=apr, pages={1983–2005} }

 @article{Ariza_Porras_2021, title={The CMS monitoring infrastructure and applications}, volume={5}, ISSN={2510-2044}, url={http://dx.doi.org/10.1007/s41781-020-00051-x}, DOI={10.1007/s41781-020-00051-x}, number={1}, journal={Computing and Software for Big Science}, publisher={Springer Science and Business Media LLC}, author={Ariza-Porras, Christian and Kuznetsov, Valentin and Legger, Federica}, year={2021}, month=jan }

 @article{Wang_2021, title={Promises and challenges of microservices: an exploratory study}, volume={26}, ISSN={1573-7616}, url={http://dx.doi.org/10.1007/s10664-020-09910-y}, DOI={10.1007/s10664-020-09910-y}, number={4}, journal={Empirical Software Engineering}, publisher={Springer Science and Business Media LLC}, author={Wang, Yingying and Kadiyala, Harshavardhan and Rubin, Julia}, year={2021}, month=may }

 @article{Oztemel_2018, title={Literature review of Industry 4.0 and related technologies}, volume={31}, ISSN={1572-8145}, url={http://dx.doi.org/10.1007/s10845-018-1433-8}, DOI={10.1007/s10845-018-1433-8}, number={1}, journal={Journal of Intelligent Manufacturing}, publisher={Springer Science and Business Media LLC}, author={Oztemel, Ercan and Gursev, Samet}, year={2018}, month=jul, pages={127–182} }

 @article{Casale_2019, title={RADON: rational decomposition and orchestration for serverless computing}, volume={35}, ISSN={2524-8529}, url={http://dx.doi.org/10.1007/s00450-019-00413-w}, DOI={10.1007/s00450-019-00413-w}, number={1–2}, journal={SICS Software-Intensive Cyber-Physical Systems}, publisher={Springer Science and Business Media LLC}, author={Casale, G. and Artač, M. and van den Heuvel, W.-J. and van Hoorn, A. and Jakovits, P. and Leymann, F. and Long, M. and Papanikolaou, V. and Presenza, D. and Russo, A. and Srirama, S. N. and Tamburri, D. A. and Wurster, M. and Zhu, L.}, year={2019}, month=aug, pages={77–87} }

 @article{Cormier_2019, title={Ten Years of the Postdigital in the 52group: Reflections and Developments 2009–2019}, volume={1}, ISSN={2524-4868}, url={http://dx.doi.org/10.1007/s42438-019-00049-8}, DOI={10.1007/s42438-019-00049-8}, number={2}, journal={Postdigital Science and Education}, publisher={Springer Science and Business Media LLC}, author={Cormier, Dave and Jandrić, Petar and Childs, Mark and Hall, Richard and White, David and Phipps, Lawrie and Truelove, Ian and Hayes, Sarah and Fawns, Tim}, year={2019}, month=jun, pages={475–506} }

 @article{Zimmermann_2016, title={Architectural refactoring for the cloud: a decision-centric view on cloud migration}, volume={99}, ISSN={1436-5057}, url={http://dx.doi.org/10.1007/s00607-016-0520-y}, DOI={10.1007/s00607-016-0520-y}, number={2}, journal={Computing}, publisher={Springer Science and Business Media LLC}, author={Zimmermann, Olaf}, year={2016}, month=oct, pages={129–145} }

 @article{Opara_Martins_2016, title={Critical analysis of vendor lock-in and its impact on cloud computing migration: a business perspective}, volume={5}, ISSN={2192-113X}, url={http://dx.doi.org/10.1186/s13677-016-0054-z}, DOI={10.1186/s13677-016-0054-z}, number={1}, journal={Journal of Cloud Computing}, publisher={Springer Science and Business Media LLC}, author={Opara-Martins, Justice and Sahandi, Reza and Tian, Feng}, year={2016}, month=apr }

 @article{Wang_2020, title={A Case for Adaptive Resource Management in Alibaba Datacenter Using Neural Networks}, volume={35}, ISSN={1860-4749}, url={http://dx.doi.org/10.1007/s11390-020-9732-x}, DOI={10.1007/s11390-020-9732-x}, number={1}, journal={Journal of Computer Science and Technology}, publisher={Springer Science and Business Media LLC}, author={Wang, Sa and Zhu, Yan-Hai and Chen, Shan-Pei and Wu, Tian-Ze and Li, Wen-Jie and Zhan, Xu-Sheng and Ding, Hai-Yang and Shi, Wei-Song and Bao, Yun-Gang}, year={2020}, month=jan, pages={209–220} }

 @article{Falkenthal_2019, title={On the algebraic properties of concrete solution aggregation}, volume={34}, ISSN={2524-8529}, url={http://dx.doi.org/10.1007/s00450-019-00400-1}, DOI={10.1007/s00450-019-00400-1}, number={2–3}, journal={SICS Software-Intensive Cyber-Physical Systems}, publisher={Springer Science and Business Media LLC}, author={Falkenthal, Michael and Breitenbücher, Uwe and Barzen, Johanna and Leymann, Frank}, year={2019}, month=feb, pages={117–128} }

 @article{Rahman_2020, title={Virtualized controller placement for multi-domain optical transport networks using machine learning}, volume={40}, ISSN={1572-8188}, url={http://dx.doi.org/10.1007/s11107-020-00895-8}, DOI={10.1007/s11107-020-00895-8}, number={3}, journal={Photonic Network Communications}, publisher={Springer Science and Business Media LLC}, author={Rahman, Sabidur and Ahmed, Tanjila and Ferdousi, Sifat and Bhaumik, Partha and Chowdhury, Pulak and Tornatore, Massimo and Das, Goutam and Mukherjee, Biswanath}, year={2020}, month=jul, pages={126–136} }

 @article{Henning_2022, title={A configurable method for benchmarking scalability of cloud-native applications}, volume={27}, ISSN={1573-7616}, url={http://dx.doi.org/10.1007/s10664-022-10162-1}, DOI={10.1007/s10664-022-10162-1}, number={6}, journal={Empirical Software Engineering}, publisher={Springer Science and Business Media LLC}, author={Henning, Sören and Hasselbring, Wilhelm}, year={2022}, month=aug }

 @article{Kosi_ska_2023, title={Enhancement of Cloud-native applications with Autonomic Features}, volume={21}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-023-09675-w}, DOI={10.1007/s10723-023-09675-w}, number={3}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={Kosińska, Joanna and Zieliński, Krzysztof}, year={2023}, month=jul }

 @article{Bombini_2023, title={A cloud-native application for digital restoration of Cultural Heritage using nuclear imaging: THESPIAN-XRF}, volume={34}, ISSN={1720-0776}, url={http://dx.doi.org/10.1007/s12210-023-01174-0}, DOI={10.1007/s12210-023-01174-0}, number={3}, journal={Rendiconti Lincei. Scienze Fisiche e Naturali}, publisher={Springer Science and Business Media LLC}, author={Bombini, Alessandro and Bofías, Fernando García-Avello and Ruberto, Chiara and Taccetti, Francesco}, year={2023}, month=jul, pages={867–887} }

 @article{Erdei_2023, title={Minimizing Resource Allocation for Cloud-Native Microservices}, volume={31}, ISSN={1573-7705}, url={http://dx.doi.org/10.1007/s10922-023-09726-3}, DOI={10.1007/s10922-023-09726-3}, number={2}, journal={Journal of Network and Systems Management}, publisher={Springer Science and Business Media LLC}, author={Erdei, Roland and Toka, Laszlo}, year={2023}, month=feb }

 @article{Metsch_2023, title={Intent-Driven Orchestration: Enforcing Service Level Objectives for Cloud Native Deployments}, volume={4}, ISSN={2661-8907}, url={http://dx.doi.org/10.1007/s42979-023-01698-0}, DOI={10.1007/s42979-023-01698-0}, number={3}, journal={SN Computer Science}, publisher={Springer Science and Business Media LLC}, author={Metsch, Thijs and Viktorsson, Magdalena and Hoban, Adrian and Vitali, Monica and Iyer, Ravi and Elmroth, Erik}, year={2023}, month=mar }

 @article{Arulappan_2023, title={ZTMP: Zero Touch Management Provisioning Algorithm for the On-boarding of Cloud-native Virtual Network Functions}, ISSN={1572-8153}, url={http://dx.doi.org/10.1007/s11036-023-02260-1}, DOI={10.1007/s11036-023-02260-1}, journal={Mobile Networks and Applications}, publisher={Springer Science and Business Media LLC}, author={Arulappan, Arunkumar and Raja, Gunasekaran and Bashir, Ali Kashif and Mahanti, Aniket and Omar, Marwan}, year={2023}, month=nov }

 @article{Maller_2023, title={Edge computing in the loop simulation framework for automotive use cases evaluation}, volume={29}, ISSN={1572-8196}, url={http://dx.doi.org/10.1007/s11276-023-03432-3}, DOI={10.1007/s11276-023-03432-3}, number={8}, journal={Wireless Networks}, publisher={Springer Science and Business Media LLC}, author={Maller, Levente Márk and Suskovics, Péter and Bokor, László}, year={2023}, month=jul, pages={3717–3735} }

 @article{ZargarAzad_2023, title={An Auto-Scaling Approach for Microservices in Cloud Computing Environments}, volume={21}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-023-09713-7}, DOI={10.1007/s10723-023-09713-7}, number={4}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={ZargarAzad, Matineh and Ashtiani, Mehrdad}, year={2023}, month=nov }

 @article{Carri_n_2022, title={Kubernetes as a Standard Container Orchestrator - A Bibliometric Analysis}, volume={20}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-022-09629-8}, DOI={10.1007/s10723-022-09629-8}, number={4}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={Carrión, Carmen}, year={2022}, month=dec }

 @article{Bento_2023, title={Cost-Availability Aware Scaling: Towards Optimal Scaling of Cloud Services}, volume={21}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-023-09718-2}, DOI={10.1007/s10723-023-09718-2}, number={4}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={Bento, Andre and Araujo, Filipe and Barbosa, Raul}, year={2023}, month=dec }

 @article{Ullah_2023, title={Orchestration in the Cloud-to-Things compute continuum: taxonomy, survey and future directions}, volume={12}, ISSN={2192-113X}, url={http://dx.doi.org/10.1186/s13677-023-00516-5}, DOI={10.1186/s13677-023-00516-5}, number={1}, journal={Journal of Cloud Computing}, publisher={Springer Science and Business Media LLC}, author={Ullah, Amjad and Kiss, Tamas and Kovács, József and Tusa, Francesco and Deslauriers, James and Dagdeviren, Huseyin and Arjun, Resmi and Hamzeh, Hamed}, year={2023}, month=sep }

 @article{Mitropoulou_2023, title={Anomaly Detection in Cloud Computing using Knowledge Graph Embedding and Machine Learning Mechanisms}, volume={22}, ISSN={1572-9184}, url={http://dx.doi.org/10.1007/s10723-023-09727-1}, DOI={10.1007/s10723-023-09727-1}, number={1}, journal={Journal of Grid Computing}, publisher={Springer Science and Business Media LLC}, author={Mitropoulou, Katerina and Kokkinos, Panagiotis and Soumplis, Polyzois and Varvarigos, Emmanouel}, year={2023}, month=dec }

 @article{Adewojo_2023, title={A Novel Weight-Assignment Load Balancing Algorithm for Cloud Applications}, volume={4}, ISSN={2661-8907}, url={http://dx.doi.org/10.1007/s42979-023-01702-7}, DOI={10.1007/s42979-023-01702-7}, number={3}, journal={SN Computer Science}, publisher={Springer Science and Business Media LLC}, author={Adewojo, Adekunbi A. and Bass, Julian M.}, year={2023}, month=mar }

 @article{Moghadam_2021, title={An autonomous performance testing framework using self-adaptive fuzzy reinforcement learning}, volume={30}, ISSN={1573-1367}, url={http://dx.doi.org/10.1007/s11219-020-09532-z}, DOI={10.1007/s11219-020-09532-z}, number={1}, journal={Software Quality Journal}, publisher={Springer Science and Business Media LLC}, author={Moghadam, Mahshid Helali and Saadatmand, Mehrdad and Borg, Markus and Bohlin, Markus and Lisper, Björn}, year={2021}, month=mar, pages={127–159} }

 @article{Mondal_2021, title={Kubernetes in IT administration and serverless computing: An empirical study and research challenges}, volume={78}, ISSN={1573-0484}, url={http://dx.doi.org/10.1007/s11227-021-03982-3}, DOI={10.1007/s11227-021-03982-3}, number={2}, journal={The Journal of Supercomputing}, publisher={Springer Science and Business Media LLC}, author={Mondal, Subrota Kumar and Pan, Rui and Kabir, H M Dipu and Tian, Tan and Dai, Hong-Ning}, year={2021}, month=jul, pages={2937–2987} }

 @article{Georgara_2023, title={The AI4Citizen pilot: Pipelining AI-based technologies to support school-work alternation programmes}, volume={53}, ISSN={1573-7497}, url={http://dx.doi.org/10.1007/s10489-023-04758-3}, DOI={10.1007/s10489-023-04758-3}, number={20}, journal={Applied Intelligence}, publisher={Springer Science and Business Media LLC}, author={Georgara, Athina and Kazhamiakin, Raman and Mich, Ornella and Palmero Aprosio, Alessio and Pazzaglia, Jean-Christoph and Rodríguez Aguilar, Juan Antonio and Sierra, Carles}, year={2023}, month=jul, pages={24157–24186} }

 @article{Mechouche_2022, title={Conformance checking for autonomous multi-cloud SLA management and adaptation}, volume={78}, ISSN={1573-0484}, url={http://dx.doi.org/10.1007/s11227-022-04363-0}, DOI={10.1007/s11227-022-04363-0}, number={11}, journal={The Journal of Supercomputing}, publisher={Springer Science and Business Media LLC}, author={Mechouche, Jeremy and Touihri, Roua and Sellami, Mohamed and Gaaloul, Walid}, year={2022}, month=mar, pages={13004–13039} }

 @article{Bhimji_2023, title={Snowmass 2021 Computational Frontier CompF4 Topical Group Report Storage and Processing Resource Access}, volume={7}, ISSN={2510-2044}, url={http://dx.doi.org/10.1007/s41781-023-00097-7}, DOI={10.1007/s41781-023-00097-7}, number={1}, journal={Computing and Software for Big Science}, publisher={Springer Science and Business Media LLC}, author={Bhimji, W. and Carder, D. and Dart, E. and Duarte, J. and Fisk, I. and Gardner, R. and Guok, C. and Jayatilaka, B. and Lehman, T. and Lin, M. and Maltzahn, C. and McKee, S. and Neubauer, M. S. and Rind, O. and Shadura, O. and Tran, N. V. and van Gemmeren, P. and Watts, G. and Weaver, B. A. and Würthwein, F.}, year={2023}, month=apr }

 @article{Muslim_2022, title={S-RAP: relevance-aware QoS prediction in web-services and user contexts}, volume={64}, ISSN={0219-3116}, url={http://dx.doi.org/10.1007/s10115-022-01699-0}, DOI={10.1007/s10115-022-01699-0}, number={7}, journal={Knowledge and Information Systems}, publisher={Springer Science and Business Media LLC}, author={Muslim, Hafiz Syed Muhammad and Rubab, Saddaf and Khan, Malik M. and Iltaf, Naima and Bashir, Ali Kashif and Javed, Kashif}, year={2022}, month=jun, pages={1997–2022} }

 @article{Soldani_2023, title={Offline Mining of Microservice-Based Architectures (Extended Version)}, volume={4}, ISSN={2661-8907}, url={http://dx.doi.org/10.1007/s42979-023-01721-4}, DOI={10.1007/s42979-023-01721-4}, number={3}, journal={SN Computer Science}, publisher={Springer Science and Business Media LLC}, author={Soldani, Jacopo and Khalili, Javad and Brogi, Antonio}, year={2023}, month=apr }

 @inbook{Lichtenth_ler_2022, title={Towards a Quality Model for Cloud-native Applications}, ISBN={9783031047183}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-04718-3_7}, DOI={10.1007/978-3-031-04718-3_7}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Lichtenthäler, Robin and Wirtz, Guido}, year={2022}, pages={109–117} }

 @inbook{Vitali_2022, title={Towards Greener Applications: Enabling Sustainable-aware Cloud Native Applications Design}, ISBN={9783031074721}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-07472-1_6}, DOI={10.1007/978-3-031-07472-1_6}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Vitali, Monica}, year={2022}, pages={93–108} }

 @inbook{Gr_newald_2022, title={Cloud Native Privacy Engineering through DevPrivOps}, ISBN={9783030991005}, ISSN={1868-422X}, url={http://dx.doi.org/10.1007/978-3-030-99100-5_10}, DOI={10.1007/978-3-030-99100-5_10}, booktitle={IFIP Advances in Information and Communication Technology}, publisher={Springer International Publishing}, author={Grünewald, Elias}, year={2022}, pages={122–141} }

 @inbook{Forn_s_Leal_2022, title={Evolution of MANO Towards the Cloud-Native Paradigm for the Edge Computing}, ISBN={9789811929809}, ISSN={1876-1119}, url={http://dx.doi.org/10.1007/978-981-19-2980-9_1}, DOI={10.1007/978-981-19-2980-9_1}, booktitle={Advanced Computing and Intelligent Technologies}, publisher={Springer Nature Singapore}, author={Fornés-Leal, Alejandro and Lacalle, Ignacio and Vaño, Rafael and Palau, Carlos E. and Boronat, Fernando and Ganzha, Maria and Paprzycki, Marcin}, year={2022}, pages={1–16} }

 @inbook{Jiang_2023, title={Performance Curve Profiling and Gated Recurrent Unit Based State Detection for Cloud Native Microservices}, ISBN={9789819944026}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-981-99-4402-6_19}, DOI={10.1007/978-981-99-4402-6_19}, booktitle={Service Science}, publisher={Springer Nature Singapore}, author={Jiang, Xu and Cai, Zhicheng}, year={2023}, pages={263–275} }

 @inbook{Zhang_2022, title={Optimization Design of Privacy Protection System Based on Cloud Native}, ISBN={9783031067617}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-3-031-06761-7_48}, DOI={10.1007/978-3-031-06761-7_48}, booktitle={Communications in Computer and Information Science}, publisher={Springer International Publishing}, author={Zhang, Yifan and Zhang, Shuli and Guo, Chengyun and Zhang, Luogang and Sun, Yinggang and Huang, Hai}, year={2022}, pages={599–615} }

 @inbook{Horn_2022, title={Multi-objective Hybrid Autoscaling of Microservices in Kubernetes Clusters}, ISBN={9783031125973}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-12597-3_15}, DOI={10.1007/978-3-031-12597-3_15}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Horn, Angelina and Fard, Hamid Mohammadi and Wolf, Felix}, year={2022}, pages={233–250} }

 @inbook{Abdelfattah_2023, title={End-to-End Test Coverage Metrics in Microservice Systems: An Automated Approach}, ISBN={9783031462351}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-46235-1_3}, DOI={10.1007/978-3-031-46235-1_3}, booktitle={Lecture Notes in Computer Science}, publisher={Springer Nature Switzerland}, author={Abdelfattah, Amr S. and Cerny, Tomas and Salazar, Jorge Yero and Lehman, Austin and Hunter, Joshua and Bickham, Ashley and Taibi, Davide}, year={2023}, pages={35–51} }

 @inbook{Huang_2023, title={Construction Practice of Cloud Billing Message Based on Stream Native}, ISBN={9783031281242}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-28124-2_40}, DOI={10.1007/978-3-031-28124-2_40}, booktitle={Smart Computing and Communication}, publisher={Springer Nature Switzerland}, author={Huang, Xiaoli and Liu, Andi and Liu, Yizhong and Li, Li and Lv, Zhenglin and Wang, Fan}, year={2023}, pages={414–427} }

 @inbook{Xi_2022, title={Decentralized Access Control for Secure Microservices Cooperation with Blockchain}, ISBN={9783031230202}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-23020-2_34}, DOI={10.1007/978-3-031-23020-2_34}, booktitle={Lecture Notes in Computer Science}, publisher={Springer Nature Switzerland}, author={Xi, Ning and Li, Yajie and Liu, Jin}, year={2022}, pages={598–614} }

 @inbook{Liu_2022, title={MicroCBR: Case-Based Reasoning on Spatio-temporal Fault Knowledge Graph for Microservices Troubleshooting}, ISBN={9783031149238}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-14923-8_15}, DOI={10.1007/978-3-031-14923-8_15}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Liu, Fengrui and Wang, Yang and Li, Zhenyu and Ren, Rui and Guan, Hongtao and Yu, Xian and Chen, Xiaofan and Xie, Gaogang}, year={2022}, pages={224–239} }

 @inbook{Riccio_2023, title={Engineering Self-adaptive Microservice Applications: An Experience Report}, ISBN={9783031484216}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-48421-6_16}, DOI={10.1007/978-3-031-48421-6_16}, booktitle={Lecture Notes in Computer Science}, publisher={Springer Nature Switzerland}, author={Riccio, Vincenzo and Sorrentino, Giancarlo and Camilli, Matteo and Mirandola, Raffaela and Scandurra, Patrizia}, year={2023}, pages={227–242} }

 @inbook{Al_Bouhairi_2023, title={Encryption Proxies in a Confidential Computing Environment}, ISBN={9789819902729}, ISSN={1865-0937}, url={http://dx.doi.org/10.1007/978-981-99-0272-9_25}, DOI={10.1007/978-981-99-0272-9_25}, booktitle={Ubiquitous Security}, publisher={Springer Nature Singapore}, author={Al Bouhairi, Mohamad Jamil and Mullick, Mostakim and Wolf, Marvin and Gudymenko, Ivan and Clauss, Sebastian}, year={2023}, pages={366–379} }

 @inbook{El_Mariouli_2022, title={Migration Strategies and Refactoring Methodology When Moving a Legacy ERP System to Cloud Platform}, ISBN={9783030642587}, ISSN={2662-3714}, url={http://dx.doi.org/10.1007/978-3-030-64258-7_19}, DOI={10.1007/978-3-030-64258-7_19}, booktitle={Studies in Distributed Intelligence}, publisher={Springer International Publishing}, author={El Mariouli, Majda and Laassiri, Jalal}, year={2022}, pages={207–217} }

 @inbook{Hajlaoui_2022, title={Model Based Migration of Cloud Systems: Review and Roadmap}, ISBN={9783031105227}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-10522-7_18}, DOI={10.1007/978-3-031-10522-7_18}, booktitle={Lecture Notes in Computer Science}, publisher={Springer International Publishing}, author={Hajlaoui, Jaleleddine and Trifa, Zied and Brahmi, Zaki}, year={2022}, pages={249–264} }

 @inbook{Maamouri_2022, title={Phi: A Generic Microservices-Based Big Data Architecture}, ISBN={9783030959470}, ISSN={1865-1356}, url={http://dx.doi.org/10.1007/978-3-030-95947-0_1}, DOI={10.1007/978-3-030-95947-0_1}, booktitle={Lecture Notes in Business Information Processing}, publisher={Springer International Publishing}, author={Maamouri, Amine and Sfaxi, Lilia and Robbana, Riadh}, year={2022}, pages={3–16} }

@InProceedings{10.1145/3329379,
  author    = {Cruz-Filipe, Lu\'{\i}s and Di Nitto, Elisabetta and Mauro, Jacopo},
  booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
  title     = {Session Details: Theme: Distributed Systems: MiDOS - Microservices, DevOps, and Service-Oriented Architecture Track},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SAC '19},
  abstract  = {Service-oriented architectures have changed our vision of the Web, bringing a paradigmatic shift in the methodologies when designing and implementing distributed systems. Originally, the Web was mainly seen as a means of presenting information to a wide spectrum of people, but service-oriented programming triggered a radical transformation of the Web towards a computational fabric where loosely coupled services interact, can be discovered and then invoked. More recently, the microservices architectural style has been proposed, where applications are developed as a collection of fine-grained services running as independent processes. Distributed applications can then be constructed from independently deployable services taking advantage of the properties of the microservice architecture (e.g., flexibility, maintainability, reusability, compositionality, and scalability) as well as the elasticity of cloud infrastructure. From the practical point of view, the deployment and maintenance of (micro)services architectures are performed using DevOps, i.e., a collection of practices linking software development (Dev) with software operations (Ops). DevOps strongly advocates for automation and monitoring at all steps of software construction, from integration, testing, releasing to deployment and infrastructure management. By using the DevOps methodology, it is possible to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality.},
  doi       = {10.1145/3329379},
  isbn      = {9781450359337},
  location  = {Limassol, Cyprus},
  url       = {https://doi.org/10.1145/3329379},
}

@inproceedings{10.5555/3021955.3022024,
author = {Oliveira, Joyce Aline and Junior, Jose J.L.D.},
title = {A Three-Dimensional View of Reuse in Service Oriented Architecture},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {The reuse in Service Oriented Architecture (SOA) has been used strategically in organizations to reduce development costs and increase the quality of applications. This article reports a qualitative research realized with experts in order to identify goals, barriers, facilitators, strategies, metrics and benefits associated with reuse in SOA. The results were summarized in three dimensions (management, architecture, operation) and represented by a conceptual model that can serve as a preliminary roadmap to manage the reuse in SOA.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {409–416},
numpages = {8},
keywords = {qualitative research, Services Oriented Architecture, SOA reuse},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI '16}
}

@inproceedings{10.1109/CCGrid.2015.148,
author = {Beier, Maximilian and Jansen, Christoph and Mayer, Geert and Penzel, Thomas and Rodenbeck, Andrea and Siewert, Ren\'{e} and Wu, Jie and Krefting, Dagmar},
title = {Multicenter Data Sharing for Collaboration in Sleep Medicine},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.148},
doi = {10.1109/CCGrid.2015.148},
abstract = {Clinical Sleep Research is an inherent multidisciplinary field, as many health issues may affect a person's sleep conditions and sleep disorders may cause several health problems. Many patients with chronic sleep disorders suffer from different further medical conditions - called multimorbidity. Due to the high variety of the reasons and the courses of sleep disorders, individual cases are difficult to compare. Therefore there is a high demand for sleep researchers to collaborate with each other to reach necessary participant numbers and multidisciplinary expertise. To date, inter-institutional sleep research is poorly supported by IT systems. In particular the heterogeneity and the quality variations within the acquired biosignal data - caused by different biosignal recorders or different measurement procedures - are impeding common biosignal data processing. In this manuscript we introduce a virtual research platform supporting inter-institutional data sharing and processing. The infrastructure is based on XNAT - a free and open-source neuroimaging research platform - a loosely coupled service oriented architecture and scalable virtualization in the backend. The system is capable of local pseudonymization of biosignal data, mapping to a standardized set of parameters and automatic quality assessment. Terms and quality measures are derived from the "Manual for the Scoring of Sleep and Associated Events" of the American Academy of Sleep Medicine, the de-facto standard for diagnostic biosignal analysis in sleep medicine.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {880–889},
numpages = {10},
keywords = {cloud, sleep, REST, XNAT, OpenStack, biosignal, polysomnography},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3366423.3380111,
author = {Ma, Meng and Xu, Jingmin and Wang, Yuan and Chen, Pengfei and Zhang, Zonghua and Wang, Ping},
title = {AutoMAP: Diagnose Your Microservice-Based Web Applications Automatically},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380111},
doi = {10.1145/3366423.3380111},
abstract = {The high complexity and dynamics of the microservice architecture make its application diagnosis extremely challenging. Static troubleshooting approaches may fail to obtain reliable model applies for frequently changing situations. Even if we know the calling dependency of services, we lack a more dynamic diagnosis mechanism due to the existence of indirect fault propagation. Besides, algorithm based on single metric usually fail to identify the root cause of anomaly, as single type of metric is not enough to characterize the anomalies occur in diverse services. In view of this, we design a novel tool, named AutoMAP, which enables dynamic generation of service correlations and automated diagnosis leveraging multiple types of metrics. In AutoMAP, we propose the concept of anomaly behavior graph to describe the correlations between services associated with different types of metrics. Two binary operations, as well as a similarity function on behavior graph are defined to help AutoMAP choose appropriate diagnosis metric in any particular scenario. Following the behavior graph, we design a heuristic investigation algorithm by using forward, self, and backward random walk, with an objective to identify the root cause services. To demonstrate the strengths of AutoMAP, we develop a prototype and evaluate it in both simulated environment and real-work enterprise cloud system. Experimental results clearly indicate that AutoMAP achieves over 90\% precision, which significantly outperforms other selected baseline methods. AutoMAP can be quickly deployed in a variety of microservice-based systems without any system knowledge. It also supports introduction of various expert knowledge to improve accuracy.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {246–258},
numpages = {13},
keywords = {web application, anomaly diagnosis, root cause, Microservice architecture, cloud computing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3468264.3473915,
author = {Kalia, Anup K. and Xiao, Jin and Krishna, Rahul and Sinha, Saurabh and Vukovic, Maja and Banerjee, Debasish},
title = {Mono2Micro: A Practical and Effective Tool for Decomposing Monolithic Java Applications to Microservices},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473915},
doi = {10.1145/3468264.3473915},
abstract = {In migrating production workloads to cloud, enterprises often face the daunting task of evolving monolithic applications toward a microservice architecture. At IBM, we developed a tool called Mono2Micro to assist with this challenging task. Mono2Micro performs spatio-temporal decomposition, leveraging well-defined business use cases and runtime call relations to create functionally cohesive partitioning of application classes. Our preliminary evaluation of Mono2Micro showed promising results.  How well does Mono2Micro perform against other decomposition techniques, and how do practitioners perceive the tool? This paper describes the technical foundations of Mono2Micro and presents results to answer these two questions. To answer the first question, we evaluated Mono2Micro against four existing techniques on a set of open-source and proprietary Java applications and using different metrics to assess the quality of decomposition and tool’s efficiency. Our results show that Mono2Micro significantly outperforms state-of-the-art baselines in specific metrics well-defined for the problem domain. To answer the second question, we conducted a survey of twenty-one practitioners in various industry roles who have used Mono2Micro. This study highlights several benefits of the tool, interesting practitioner perceptions, and scope for further improvements. Overall, these results show that Mono2Micro can provide a valuable aid to practitioners in creating functionally cohesive and explainable microservice decompositions.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1214–1224},
numpages = {11},
keywords = {dynamic analysis, microservices, clustering},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3452383.3452385,
author = {Dasgupta, Gargi B.},
title = {AI and Its Applications in the Cloud Strategy},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452385},
doi = {10.1145/3452383.3452385},
abstract = {The fourth industrial revolution identifies cloud computing, data, and artificial intelligence (AI) as opportunity clusters with double digit growth in the next couple of years. As part of the cloud and digital transformation, the role of AI is crucial in enabling that transformation as well as creating the new breed of applications on top. AI mechanisms can help accelerate the modernization of applications, their management, and the testing on cloud architectures. I will focus on two sub-problems: 1) Refactoring of massive monolith applications using AI techniques. This problem statement is particularly relevant in understanding legacy un-optimized code and transforming them to be more cloud-ready. Microservices are indeed becoming the de-facto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently [2]. It also provides natural benefits when deployed on the cloud since resources can be allocated dynamically to necessary components based on demand. We are exploring how AI can help accelerate the transformation of existing applications to microservices. 2) Detecting faults in application behavior at runtime from operational data. This problem statement is particularly relevant in understanding how to manage this new architecture of multiple microservices across the cloud stack [1], [3]. Operational data artifacts span across logs, metrics, tickets, and traces. Looking at signals across the artifacts and across the stack presents a challenging data correlation problem. AI mechanisms can help accelerate problem determination in these complex environments. I will also share my thoughts on how fundamental breakthroughs in AI Research will be needed as we address some of the core problems of cloud computing.},
booktitle = {14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {2},
numpages = {1},
keywords = {modernization, AI Ops, code refactoring, log anomalies, hybrid cloud},
location = {Bhubaneswar, Odisha, India},
series = {ISEC 2021}
}

@inproceedings{10.1145/2797022.2797039,
author = {Anwar, Ali and Sailer, Anca and Kochut, Andrzej and Butt, Ali R.},
title = {Anatomy of Cloud Monitoring and Metering: A Case Study and Open Problems},
year = {2015},
isbn = {9781450335546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797022.2797039},
doi = {10.1145/2797022.2797039},
abstract = {Microservices based architecture has recently gained traction among the cloud service providers in quest for a more scalable and reliable modular architecture. In parallel with this architectural choice, cloud providers are also facing the market demand for fine grained usage based prices. Both the management of the microservices complex dependencies, as well as the fine grained metering require the providers to track and log detailed monitoring data from their deployed cloud setups. Hence, on one hand, the providers need to record all such performance changes and events, while on the other hand, they are concerned with the additional cost associated with the resources required to store and process this ever increasing amount of collected data.In this paper, we analyze the design of the monitoring subsystem provided by open source cloud solutions, such as OpenStack. Specifically, we analyze how the monitoring data is collected by OpenStack and assess the characteristics of the data it collects, aiming to pinpoint the limitations of the current approach and suggest alternate solutions. Our preliminary evaluation of the proposed solutions reveals that it is possible to reduce the monitored data size by up to 80\% and missed anomaly detection rate from 3\% to as low as 0.05\% to 0.1\%.},
booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
articleno = {6},
numpages = {7},
location = {Tokyo, Japan},
series = {APSys '15}
}

@inproceedings{10.1109/CCGrid.2015.152,
author = {Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
title = {Modeling Cross-Architecture Co-Tenancy Performance Interference},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.152},
doi = {10.1109/CCGrid.2015.152},
abstract = {Cloud computing has become a dominant computing paradigm to provide elastic, affordable computing resources to end users. Due to the increased computing power of modern machines powered by multi/many-core computing, data centers often co-locate multiple virtual machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing and competition. Applications or VMs co-locating in one physical machine can interfere with each other despite of the promise of performance isolation through virtualization. Modeling and predicting co-run interference therefore becomes critical for data center job scheduling and QoS (Quality of Service) assurance. Co-run interference can be categorized into two metrics, sensitivity and pressure, where the former denotes how an application's performance is affected by its co-run applications, and the latter measures how it impacts the performance of its co-run applications. This paper shows that sensitivity and pressure are both application- and architecture-dependent. Further, we propose a regression model that predicts an application's sensitivity and pressure across architectures with high accuracy. This regression model enables a data center scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate with another VMs/applications.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {231–240},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3412841.3441899,
author = {Torquato, Matheus and Maciel, Paulo and Vieira, Marco},
title = {Analysis of VM Migration Scheduling as Moving Target Defense against Insider Attacks},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441899},
doi = {10.1145/3412841.3441899},
abstract = {As cybersecurity threats evolve, cloud computing defenses must adapt to face new challenges. Unfortunately, due to resource sharing, cloud computing platforms open the door for insider attacks, which consist of malicious actions from cloud authorized users (e.g., clients of an Infrastructure-as-a-Service (IaaS) cloud) targeting the co-hosted users or the underlying provider environment. Virtual machine (VM) migration is a Moving Target Defense (MTD) technique to mitigate insider attacks effects, as it provides VMs positioning manageability. However, there is a clear demand for studies quantifying the security benefits of VM migration-based MTD considering different system architecture configurations. This paper tries to fill such a gap by presenting a Stochastic Reward Net model for the security evaluation of a VM migration-based MTD. The security metric of interest is the probability of attack success. We consider multiple architectures, ranging from one physical machine pool (without MTD) up to four physical machine pools. The evaluation also considers the unavailability due to VM migration. The key contributions are i) a set of results highlighting the probability of insider attacks success over time in different architectures and VM migration schedules, and ii) suggestions for selecting VMs as candidates for MTD deployment based on the tolerance levels of the attack success probability. The results are validated against simulation results to confirm the accuracy of the model.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {194–202},
numpages = {9},
keywords = {VM migration, stochastic petri nets, migration-based dynamic platform, moving target defense, availability},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1109/CCGrid.2014.103,
author = {Wu, Jie and Jansen, Christoph and Beier, Maximilian and Witt, Michael and Krefting, Dagmar},
title = {Extending XNAT towards a Cloud-Based Quality Assessment Platform for Retinal Optical Coherence Tomographies},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.103},
doi = {10.1109/CCGrid.2014.103},
abstract = {Neurosciencific research is increasingly based on image analysis methods. Large sets of imaging data are processed using complex image analysis tools. While today magnetic resonance imaging (MRI) is widely used for both functional and anatomical analysis of the human brain, new imaging modalities are beginning to prove their capabilities for neurological research. Among them, optical coherence tomography (OCT) allows for noninvasive visualization of anatomical structures on a micrometer scale. Becoming a standard diagnostic tool in ophthalmology, it is of rising interest for neurological research. Crucial to all data analysis methods is the quality of the input data. The platform presented in this paper is designed for automatic quality assessment of retinal OCTs. It extends the image management platform XNAT by services to calculate and store quality measures. It is also extensible regarding new quality measure algorithms, allowing the developer to upload Matlab code, compile it for the infrastructure's hardware architecture and test it in the system. The image processing tools to calculate the quality measures are provided as a cloud-based service employing OpenStack as underlying IT infrastructure. The prototype implementation encompassing security and performance aspects are presented.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {764–773},
numpages = {10},
keywords = {SaaS, XNAT, cloud, medical imaging, neuroimaging, IaaS, OCT},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3204949.3204976,
author = {Mekuria, Rufael and McGrath, Michael J. and Riccobene, Vincenzo and Bayon-Molino, Victor and Tselios, Christos and Thomson, John and Dobrodub, Artem},
title = {Automated Profiling of Virtualized Media Processing Functions Using Telemetry and Machine Learning},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204976},
doi = {10.1145/3204949.3204976},
abstract = {Most media streaming services are composed by different virtualized processing functions such as encoding, packaging, encryption, content stitching etc. Deployment of these functions in the cloud is attractive as it enables flexibility in deployment options and resource allocation for the different functions. Yet, most of the time overprovisioning of cloud resources is necessary in order to meet demand variability. This can be costly, especially for large scale deployments. Prior art proposes resource allocation based on analytical models that minimize the costs of cloud deployments under a quality of service (QoS) constraint. However, these models do not sufficiently capture the underlying complexity of services composed of multiple processing functions. Instead, we introduce a novel methodology based on full-stack telemetry and machine learning to profile virtualized or cloud native media processing functions individually. The basis of the approach consists of investigating 4 categories of performance metrics: throughput, anomaly, latency and entropy (TALE) in offline (stress tests) and online setups using cloud telemetry. Machine learning is then used to profile the media processing function in the targeted cloud/NFV environment and to extract the most relevant cloud level Key Performance Indicators (KPIs) that relate to the final perceived quality and known client side performance indicators. The results enable more efficient monitoring, as only KPI related metrics need to be collected, stored and analyzed, reducing the storage and communication footprints by over 85\%. In addition a detailed overview of the functions behavior was obtained, enabling optimized initial configuration and deployment, and more fine-grained dynamic online resource allocation reducing overprovisioning and avoiding function collapse. We further highlight the next steps towards cloud native carrier grade virtualized processing functions relevant for future network architectures such as in emerging 5G architectures.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {150–161},
numpages = {12},
keywords = {performance, characterization, video streaming, experimentation, telemetry, cloud computing},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/3291064.3291070,
author = {Thirunavukkarasu, Gokul Sidarth and Champion, Benjamin and Horan, Ben and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
title = {IoT-Based System Health Management Infrastructure as a Service},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291070},
doi = {10.1145/3291064.3291070},
abstract = {Customization, enhanced quality of streamlined maintenance services and uplifted productivity are some of the key highlights from the rapidly evolving concept of Industry 4.0. IoT (Internet of things) based service infrastructure models designed for delivering enterprise services with capabilities of pro-actively sensing malfunctions and responding with preventive measures to streamline the automated service offered is one of the prime application of this concept. Continuous maintenance services increase the optimum through-life cost and in-service life cycle of the product providing the customer with the feel of full ownership. In-service feedbacks also help the manufactures to identify issues with respect to the designs and improve it in the future versions. In this paper, as a proof of concept a cloud-based IoT service infrastructure for providing real-time prognostic and supervised vehicle maintenance system is proposed. This proposed system aims at providing an enterprise service infrastructure to the registered vehicle service centers to keep track of the real-time vehicle diagnostic information of their client's vehicle over cloud and use prognostic algorithms to identify any malfunctions or abnormal behavior of the vehicles for automatically scheduling a service appointment and automating the maintenance cycle of the vehicle. In addition to this, the system provides features like remote supervision and diagnostics maintenance enabling technicians to fix issues remotely, ensuring streamlined and reliable service. Initially, before building the proposed prototype system, a few experimental trails where conducted for analyzing the use of different IoT models used in the development to identify the best-suited approach. The results indicated that the publisher-subscriber (NodeJS) based model outperforms the request-response (PHP) based model in terms of the hits per second and mean request time for an increased number of active users. The results of the initial tests justify the reason for the using the publisher-subscriber based IOT architecture. The conceptualized enterprise infrastructure illustrated in the manuscript aims at providing a streamlined maintenance service.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {55–61},
numpages = {7},
keywords = {prognostic maintenance, vehicle diagnosis, internet of things, System health management infrastructure as a service, streamlined remote supervision},
location = {Singapore, Singapore},
series = {CCIOT '18}
}

@inproceedings{10.1109/UCC.2014.49,
author = {Keller, Matthias and Robbert, Christoph and Karl, Holger},
title = {Template Embedding: Using Application Architecture to Allocate Resources in Distributed Clouds},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.49},
doi = {10.1109/UCC.2014.49},
abstract = {In distributed cloud computing, application deployment across multiple sites can improve quality of service. Recent research developed algorithms to find optimal locations for virtual machines. However, those algorithms assume to have either single-tier applications or a fixed number of virtual machines--a strong simplification of reality. This paper investigates the placement and scaling of complex application architectures. An application is dynamically scaled to fit both the current demand situation and the currently available infrastructure resources. We compare two approaches: The first one is based on virtual network embedding. The second approach is a novel method called Template Embedding. It is based on a hierarchical 1-allocation hub flow problem and combines application scaling and embedding in one step. Extensive experiments on 43200 network configurations showed that Template Embedding outperforms virtual network embedding in all cases in three metrics: success rate, solution quality, and runtime. This positive result shows that template embedding is a promising approach for distributed cloud resource allocation.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {387–395},
numpages = {9},
keywords = {Distributed Cloud Computing, Cloud Resource Allocation, Application Architecture, Hub Problem, Flow Problem},
series = {UCC '14}
}

@inproceedings{10.5555/2602339.2602341,
author = {Buevich, Maxim and Schnitzer, Dan and Escalada, Tristan and Jacquiau-Chamski, Arthur and Rowe, Anthony},
title = {Fine-Grained Remote Monitoring, Control and Pre-Paid Electrical Service in Rural Microgrids},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {In this paper, we present the architecture, design and experiences from a wirelessly managed microgrid deployment in rural Les Anglais, Haiti. The system consists of a three-tiered architecture with a cloud-based monitoring and control service, a local embedded gateway infrastructure and a mesh network of wireless smart meters deployed at 52 buildings. Each smart meter device has an 802.15.4 radio that enables remote monitoring and control of electrical service. The meters communicate over a scalable multi-hop TDMA network back to a central gateway that manages load within the system. The gateway also provides an 802.11 interface for an on-site operator and a cellular modem connection to a cloud-backend that manages and stores billing and usage data. The cloud backend allows occupants in each home to pre-pay for electricity at a particular peak power limit using a text messaging service. The system activates each meter within seconds and locally enforces power limits with provisioning for theft detection. We believe that this fine-grained micro-payment model can enable sustainable power in otherwise unfeasible areas.This paper provides a chronology of our deployment and installation strategy that involved GPS-based site mapping along with various network conditioning actions required as the network evolved. Finally, we summarize key lessons learned and hypothesis about additional hardware that could be used to ease the tracing of faults like short circuits and downed lines within microgrids.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {1–12},
numpages = {12},
keywords = {sensor networks, wireless local area networks, microgrid},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3176258.3176328,
author = {Alshehri, Asma and Benson, James and Patwa, Farhan and Sandhu, Ravi},
title = {Access Control Model for Virtual Objects (Shadows) Communication for AWS Internet of Things},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176328},
doi = {10.1145/3176258.3176328},
abstract = {The concept of Internet of Things (IoT) has received considerable attention and development in recent years. There have been significant studies on access control models for IoT in academia, while companies have already deployed several cloud-enabled IoT platforms. However, there is no consensus on a formal access control model for cloud-enabled IoT. The access-control oriented (ACO) architecture was recently proposed for cloud-enabled IoT, with virtual objects (VOs) and cloud services in the middle layers. Building upon ACO, operational and administrative access control models have been published for virtual object communication in cloud-enabled IoT illustrated by a use case of sensing speeding cars as a running example.In this paper, we study AWS IoT as a major commercial cloud-IoT platform and investigate its suitability for implementing the afore-mentioned academic models of ACO and VO communication control. While AWS IoT has a notion of digital shadows closely analogous to VOs, it lacks explicit capability for VO communication and thereby for VO communication control. Thus there is a significant mismatch between AWS IoT and these academic models. The principal contribution of this paper is to reconcile this mismatch by showing how to use the mechanisms of AWS IoT to effectively implement VO communication models. To this end, we develop an access control model for virtual objects (shadows) communication in AWS IoT called AWS-IoT-ACMVO. We develop a proof-of-concept implementation of the speeding cars use case in AWS IoT under guidance of this model, and provide selected performance measurements. We conclude with a discussion of possible alternate implementations of this use case in AWS IoT.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {175–185},
numpages = {11},
keywords = {internet of things (iot), acl, virtual objects, aws iot, security, abac, iot architecture, devices, access control, rbac},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1145/3316615.3316622,
author = {Ming, Fan Xiu and Habeeb, Riyaz Ahamed Ariyaluran and Md Nasaruddin, Fariza Hanum Binti and Gani, Abdullah Bin},
title = {Real-Time Carbon Dioxide Monitoring Based on IoT \&amp; Cloud Technologies},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316622},
doi = {10.1145/3316615.3316622},
abstract = {In recent years, environment monitoring are of greater importance towards the area of climate monitoring, analysis, agricultural productivity management, quality assurance of water, air, alongside with other potential factors that are closely connected to industrial development and convenience of living. This research is motivated by creating awareness of smart home residents on indoor air quality, as well as providing insight of carbon dioxide emissions for industries and environmental organizations.This paper proposes an efficient solution towards environment monitoring of carbon dioxide integrated with Internet of Things capability and cloud computing technology. Aforementioned techniques will deliver highly accessible and real-time data visualization which would be greatly beneficial for Smart Homes efficiency of analysis actualization and counter-measures deployment. A monitoring architecture was developed to generate, accumulate, store and visualize carbon dioxide concentration using MQ135 carbon dioxide sensor, ESP8266 Wi-Fi module, Firebase Cloud Storage Service and Android mobile application Carbon Insight for data visualization. 2880 data points in the time frame of 10 days with a 30-second interval was collected, stored and visualized with the application of this system.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {517–521},
numpages = {5},
keywords = {cloud, Internet of things, environment monitoring},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@inproceedings{10.1145/3240508.3240642,
author = {Pang, Haitian and Zhang, Cong and Wang, Fangxin and Hu, Han and Wang, Zhi and Liu, Jiangchuan and Sun, Lifeng},
title = {Optimizing Personalized Interaction Experience in Crowd-Interactive Livecast: A Cloud-Edge Approach},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240642},
doi = {10.1145/3240508.3240642},
abstract = {Enabling users to interact with broadcasters and audience, the crowd-interactive livecast greatly improves viewer's quality of experience (QoE) and attracts millions of daily active users recently. In addition to striking the balance between resource utilization and viewers' QoE met in the traditional video streaming service, this novel service needs to take supererogatory efforts to improve the interaction QoE, which reflects the viewer interaction experience. To tackle this issue, we conduct measurement studies over a large-scale dataset crawled from a representative livecast service provider. We observe that the individual's interaction pattern is quite heterogeneous: only 10\% viewers proactively participate in the interaction, and the rest viewers usually watch passively. Incorporating the insight into the emerging cloud-edge architecture, we propose a framework PIECE, which optimizes the Personalized Interaction Experience with Cloud-Edge architecture (PIECE) for intelligent user access control and livecast distribution. In particular, we first devise a novel deep neural network based algorithm to predict users' interaction intensity using the historical viewer pattern. We then design an algorithm to maximize the individual's QoE, by strategically matching viewer sessions and transcoding-delivery paths over cloud-edge infrastructure. Finally, we use trace-driven experiments to verify the effectiveness of PIECE. Our results show that our prediction algorithm outperforms the state-of-the-art algorithms with a much smaller mean absolute error (40\% reduction). Furthermore, in comparison with the cloud-based video delivery strategy, the proposed framework can simultaneously improve the average viewers QoE (26\% improvement) and interaction QoE (21\% improvement), while maintaining a high streaming bitrate.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1217–1225},
numpages = {9},
keywords = {viewer interaction, cloud-edge, interactive live streaming},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/2809826.2809836,
author = {Khan, Yasir Imtiaz and Al-shaer, Ehab and Rauf, Usman},
title = {Cyber Resilience-by-Construction: Modeling, Measuring \&amp; Verifying},
year = {2015},
isbn = {9781450338219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809826.2809836},
doi = {10.1145/2809826.2809836},
abstract = {The need of cyber security is increasing as cyber attacks are escalating day by day. Cyber attacks are now so many and sophisticated that many will unavoidably get through. Therefore, there is an immense need to employ resilient architectures to defend known or unknown threats. Engineer- ing resilient system/infrastructure is a challenging task, that implies how to measure the resilience and how to obtain sufficient resilience necessary to maintain its service delivery under diverse situations. This paper has two fold objective, the first is to propose a formal approach to measure cyber resilience from different aspects (i.e., attacks, failures) and at different levels (i.e., pro-active, resistive and reactive). To achieve the first objective, we propose a formal frame- work named as: Cyber Resilience Engineering Framework (CREF). The second objective is to build a resilient system by construction. The idea is to build a formal model of a cyber system, which is initially not resilient with respect to attacks. Then by systematic refinements of the formal model and by its model checking, we attain resiliency. We exemplify our technique through the case study of simple cyber security device (i.e., network firewall).},
booktitle = {Proceedings of the 2015 Workshop on Automated Decision Making for Active Cyber Defense},
pages = {9–14},
numpages = {6},
keywords = {cyber resilience, algebraic petri nets, model checking, firewall},
location = {Denver, Colorado, USA},
series = {SafeConfig '15}
}

@inproceedings{10.1145/2693561.2693563,
author = {Klein, John and Gorton, Ian},
title = {Runtime Performance Challenges in Big Data Systems},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693563},
doi = {10.1145/2693561.2693563},
abstract = {Big data systems are becoming pervasive. They are distributed systems that include redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient to assure runtime performance in production. This is due to the scale of the deployed system, the continually evolving workloads, and the unpredictable quality of service of the shared infrastructure. Consequently, a solution for addressing performance requirements needs sophisticated runtime observability and measurement. Observability gives real-time insights into a system's health and status, both at the system and application level, and provides historical data repositories for forensic analysis, capacity planning, and predictive analytics. Due to the scale and heterogeneity of big data systems, significant challenges exist in the design, customization and operations of observability capabilities. These challenges include economical creation and insertion of monitors into hundreds or thousands of computation and data nodes, efficient, low overhead collection and storage of measurements (which is itself a big data problem), and application-aware aggregation and visualization. In this paper we propose a reference architecture to address these challenges, which uses a model-driven engineering toolkit to generate architecture-aware monitors and application-specific visualizations.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {17–22},
numpages = {6},
keywords = {model-driven engineering, observability, big data},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@article{10.1145/3442187,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet},
title = {VidCloud: Joint Stall and Quality Optimization for Video Streaming over Cloud},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3442187},
doi = {10.1145/3442187},
abstract = {As video-streaming services have expanded and improved, cloud-based video has evolved into a necessary feature of any successful business for reaching internal and external audiences. In this article, video streaming over distributed storage is considered where the video segments are encoded using an erasure code for better reliability. We consider a representative system architecture for a realistic (typical) content delivery network (CDN). Given multiple parallel streams/link between each server and the edge router, we need to determine, for each client request, the subset of servers to stream the video, as well as one of the parallel streams from each chosen server. To have this scheduling, this article proposes a two-stage probabilistic scheduling. The selection of video quality is also chosen with a certain probability distribution that is optimized in our algorithm. With these parameters, the playback time of video segments is determined by characterizing the download time of each coded chunk for each video segment. Using the playback times, a bound on the moment generating function of the stall duration is used to bound the mean stall duration. Based on this, we formulate an optimization problem to jointly optimize the convex combination of mean stall duration and average video quality for all requests, where the two-stage probabilistic scheduling, video quality selection, bandwidth split among parallel streams, and auxiliary bound parameters can be chosen. This non-convex problem is solved using an efficient iterative algorithm. Based on the offline version of our proposed algorithm, an online policy is developed where servers selection, quality, bandwidth split, and parallel streams are selected in an online manner. Experimental results show significant improvement in QoE metrics for cloud-based video as compared to the considered baselines.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {jan},
articleno = {17},
numpages = {32},
keywords = {two-stage probabilistic scheduling, erasure codes, video quality, mean stall duration, Video streaming over cloud}
}

@inproceedings{10.1145/3302541.3310294,
author = {Scheuner, Joel and Leitner, Philipp},
title = {Performance Benchmarking of Infrastructure-as-a-Service (IaaS) Clouds with Cloud WorkBench},
year = {2019},
isbn = {9781450362863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302541.3310294},
doi = {10.1145/3302541.3310294},
abstract = {The continuing growth of the cloud computing market has led to an unprecedented diversity of cloud services with different performance characteristics. To support service selection, researchers and practitioners conduct cloud performance benchmarking by measuring and objectively comparing the performance of different providers and configurations (e.g., instance types in different data center regions). In this tutorial, we demonstrate how to write performance tests for IaaS clouds using the Web-based benchmarking tool Cloud WorkBench (CWB). We will motivate and introduce benchmarking of IaaS cloud in general, demonstrate the execution of a simple benchmark in a public cloud environment, summarize the CWB tool architecture, and interactively develop and deploy a more advanced benchmark together with the participants.},
booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {53–56},
numpages = {4},
keywords = {cloud computing, performance, benchmarking},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1109/CCGrid.2014.50,
author = {Tolosana-Calasanz, Rafael and Ba\~{n}ares, Jos\'{e} \'{A}ngel and Rana, Omer and Pham, Congduc and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
title = {Enforcing Quality of Service on OpenNebula-Based Shared Clouds},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.50},
doi = {10.1109/CCGrid.2014.50},
abstract = {With an increase in the number of monitoring sensors deployed on physical infrastructures, there is a corresponding increase in data volumes that need to be processed. Data measured or collected by sensors is typically processed at destination or "in-transit" (i.e. from data capture to delivery to a user). When such data are processed in-transit over a shared distributed computing infrastructure, it is useful to provide elastic computational capability which can be adapted based on processing requirements and demand. Where Service Level Agreements (SLAs) have been pre-agreed, such available computational capacity needs to be shared in such a way that any Quality of Service related constraints in such SLAs are not violated. This is particularly challenging for time critical applications and with highly variable and unpredictable rates of data generation (e.g. in Smart Grid applications where energy usage patterns may change unpredictably). Previously, we proposed a Reference net based architectural model for supporting QoS for multiple concurrent data streams being processed (prior to delivery to a user) over a shared infrastructure. In this paper, we describe a practical realisation of this architecture using the OpenNebula Cloud platform. We consider our infrastructure to be composed of a number of nodes, each of which has multiple processing units and data buffers. We utilize the "token bucket" model for regulating, on a per stream basis, the data injection rate into each node. We subsequently demonstrate how a streaming pipeline can be supported and managed using a dynamic control strategy at each node.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {651–659},
numpages = {9},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@article{10.1109/TNET.2014.2354262,
author = {Adhikari, Vijay K. and Guo, Yang and Hao, Fang and Hilt, Volker and Zhang, Zhi-Li and Varvello, Matteo and Steiner, Moritz},
title = {Measurement Study of Netflix, Hulu, and a Tale of Three CDNs},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2014.2354262},
doi = {10.1109/TNET.2014.2354262},
abstract = {Netflix and Hulu are leading Over-the-Top (OTT) content service providers in the US and Canada. Netflix alone accounts for 29.7\% of the peak downstream traffic in the US in 2011. Understanding the system architectures and performance of Netflix and Hulu can shed light on the design of such large-scale video streaming platforms, and help improving the design of future systems. In this paper, we perform extensive measurement study to uncover their architectures and service strategies. Netflix and Hulu bear many similarities. Both Netflix and Hulu video streaming platforms rely heavily on the third-party infrastructures, with Netflix migrating that majority of its functions to the Amazon cloud, while Hulu hosts its services out of Akamai. Both service providers employ the same set of three content distribution networks (CDNs) in delivering the video contents. Using active measurement study, we dissect several key aspects of OTT streaming platforms of Netflix and Hulu, e.g., employed streaming protocols, CDN selection strategy, user experience reporting, etc. We discover that both platforms assign the CDN to a video request without considering the network conditions and optimizing the user-perceived video quality. We further conduct the performance measurement studies of the three CDNs employed by Netflix and Hulu. We show that the available bandwidths on all three CDNs vary significantly over the time and over the geographic locations. We propose a measurement-based adaptive CDN selection strategy and a multiple-CDN-based video delivery strategy that can significantly increase users' average available bandwidth.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {1984–1997},
numpages = {14},
keywords = {over-the-top (OTT) content service, Netflix, CDN selection strategy, video streaming, content distribution networks (CDN), Hulu}
}

@inproceedings{10.1145/2851613.2851727,
author = {Megyesi, P\'{e}ter and Botta, Alessio and Aceto, Giuseppe and Pescap\`{e}, Antonio and Moln\'{a}r, S\'{a}ndor},
title = {Available Bandwidth Measurement in Software Defined Networks},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851727},
doi = {10.1145/2851613.2851727},
abstract = {Software Defined Networking (SDN) is an emerging paradigm that is expected to revolutionize computer networks. With the decoupling of data and control plane and the introduction of open communication interfaces between layers, SDN enables programmability over the entire network, promising rapid innovation in this area. The SDN concept was already proven to work successfully in cloud and data center environments thus the proper monitoring of such networks is already in the focus of the research community. Methods for measuring Quality of Service (QoS) parameters such as bandwidth utilization, packet loss, and delay have been recently introduced in literature, but they lack a solution for tackling down the question of available bandwidth. In this paper, we attempt to fill this gap and introduce a novel mechanism for measuring available bandwidth in SDN networks. We take advantage of the SDN architecture and build an application over the Network Operating System (NOS). Our application can track the topology of the network and the bandwidth utilization over the network links, and thus it is able to calculate the available bandwidth between any two points in the network. We validate our method using the popular Mininet network emulation environment and the widely used NOS called Floodlight. We present results providing insights into the measurement accuracy and showing its relationship with the delay in the control network and the polling frequency.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {651–657},
numpages = {7},
keywords = {software defined networks, floodlight, network operating system, OpenFlow, available bandwidth, mininet},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3005745.3005762,
author = {Tilmans, Olivier and B\"{u}hler, Tobias and Vissicchio, Stefano and Vanbever, Laurent},
title = {Mille-Feuille: Putting ISP Traffic under the Scalpel},
year = {2016},
isbn = {9781450346610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3005745.3005762},
doi = {10.1145/3005745.3005762},
abstract = {For Internet Service Provider (ISP) operators, getting an accurate picture of how their network behaves is challenging. Given the traffic volumes that their networks carry and the impossibility to control end-hosts, ISP operators are typically forced to randomly sample traffic, and rely on aggregated statistics. This provides coarse-grained visibility, at a time resolution that is far from ideal (seconds or minutes). In this paper, we present Mille-Feuille, a novel monitoring architecture that provides fine-grained visibility over ISP traffic. Mille-Feuille schedules activation and deactivation of traffic-mirroring rules, that are then provisioned network-wide from a central location, within milliseconds. By doing so, Mille-Feuille combines the scalability of sampling with the visibility and controllability of traffic mirroring. As a result, it supports a set of monitoring primitives, ranging from checking key performance indicators (e.g., one-way delay) for single destinations to estimating traffic matrices in sub-seconds. Our preliminary measurements on existing routers confirm that Mille-Feuille is viable in practice.},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
pages = {113–119},
numpages = {7},
location = {Atlanta, GA, USA},
series = {HotNets '16}
}

@inproceedings{10.1145/3329391,
author = {Esposito, Christian and Pop, Florin and Choi, Chang},
title = {Session Details: Theme: Information Systems: SFECS - Sustainability of Fog/Edge Computing Systems Track},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329391},
doi = {10.1145/3329391},
abstract = {Fog/Edge Computing paradigms are widely used in enterprises to address the emerging challenges of big data analysis, because of their underlying scalable, flexible and distributed data management schemes. The data centers in the Clouds are facing great challenges on the burden of the consequent increasing the amount of data to be man- aged and the additional requirements of location awareness and low latency at the edge of network necessary by smart cites and factories. These are the reasons why a centralized model cannot be an efficient solution for generated or required data by the IoT devices in those applications and there is the progressive shift towards fog nodes and smarted edge nodes mediating between the cloud and the IoT devices. The Fog/Edge computing paradigm is a decentralized model that transfers a part of low computing data analysis from the cloud to the intermediate (fog) nodes or the edges, performing only high computing tasks in the cloud. This new approach tries to minimize the three factors that negatively compromise the effective and efficient application of the Cloud computing to smart cities and factories, or similar application domains: the network bandwidth usage, decentralization of the data processing tasks and reduced response latency for clients (IoT devices). Fog/Edge computing is a hierarchical approach where the overall infrastructure is structured in multiple layers, each responsible of offering a good coordination and data management to the nodes at the lower layer. The lowest layer is usually composed of sensors and/or actuators that measure and/or control the environment or a given business process, implemented as mobile devices that are running a sensing/controlling application. In this case, combining Sustainable computing with Fog and Edge computing represents a new approach for increasing quality-of- service and efficiency of the system, creating the capability to present temporal and geo-coded information, and increasing innovation, and co-designing sustainable future large scale distributed systems. This new paradigm appears to offer a good approach in handling the scale factor of the data size, reducing the network bandwidth usage and the response latency of the system. In order to support specifically the Fog/Edge architectures, there is a need, for instance, of location-awareness and computation placement, replication and recovery. In many cases Edge resources would be required for both computation and data storage to address the time and locality constraints. There are multiple kinds of orchestration management solutions for virtualization in this type of architecture with different characteristics and drawbacks. This results in different restrictions for application definition, scalability, availability, load balancing and so on. Also, virtualization may be needed at multiple levels in a Fog/Edge architecture as it consists of the following levels of abstraction: at the sensing level we have the IoT devices/smart things, at the Edge level there are the gateways to a first collection and the data from the IoT devices and their preliminary processing, at the Fog level we have an additional data management layer, and at the Cloud level there is the compute/storage infrastructure with applications on top. Last, but not least, the energy efficiency is particularly important at the IoT and edge level since the devices may be equipped with a limited battery, possible difficult or impossible to be charged. So, optimizing the energy consumption is a must. To address several open research is- sues regarding sustainability of future Fog/Edge systems, this track aims at solicit contributions highlighting challenges, state-of-the-art, and solutions to a set of currently unresolved key questions including - but not limited to - performance, modeling, optimization, energy-efficiency, reliability, security, privacy and techno-economic aspects of Fog/Edge systems. Through addressing these concerns while understanding their impacts and limitations, technological advancements will be channeled toward more sustainable/efficient platforms for tomorrow's ever-connected systems.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/2775292.2775312,
author = {Scully, Timothy and Dobo\v{s}, Jozef and Sturm, Timo and Jung, Yvonne},
title = {3drepo.Io: Building the next Generation Web3D Repository with AngularJS and X3DOM},
year = {2015},
isbn = {9781450336475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2775292.2775312},
doi = {10.1145/2775292.2775312},
abstract = {This paper presents a novel open source web-based 3D version control system positioned directly within the context of the recent strategic plan for digitising the construction sector in the United Kingdom. The aim is to achieve reduction of cost and carbon emissions in the built environment by up to 20\% simply by properly managing digital information and 3D models. Even though previous works in the field concentrated mainly on defining novel WebGL frameworks and later on the efficiency of 3D data delivery over the Internet, there is still the emerging need for a practical solution that would provide ubiquitous access to 3D assets, whether it is for large international enterprises or individual members of the general public. We have, therefore, developed a novel platform leveraging the latest open web-based technologies such as AngularJS and X3DOM in order to define an industrial-strength collaborative cloud hosting service 3drepo.io. Firstly, we introduce the work and outline the high-level system architecture as well as improvements in relation to previous work. Next, we describe database and front-end considerations with emphasis on scalability and enhanced security. Finally, we present several performance measurement experiments and a selection of real-life industrial use cases. We conclude that jQuery provides performance benefits over AngularJS when manipulating large scene graphs in web browsers.},
booktitle = {Proceedings of the 20th International Conference on 3D Web Technology},
pages = {235–243},
numpages = {9},
keywords = {BIM, 3D repo, version control, X3DOM, AngularJS},
location = {Heraklion, Crete, Greece},
series = {Web3D '15}
}

@inproceedings{10.1145/3375555.3384938,
author = {Zibitsker, Boris and Lupersolsky, Alex},
title = {How to Apply Modeling to Compare Options and Select the Appropriate Cloud Platform},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3384938},
doi = {10.1145/3375555.3384938},
abstract = {Organizations want to take advantage of the flexibility and scalability of Cloud platforms. By migrating to the Cloud, they hope to develop and implement new applications faster with lower cost. Amazon AWS, Microsoft Azure, Google, IBM, Oracle and others Cloud providers support different DBMS like Snowflake, Redshift, Teradata Vantage, and others. These platforms have different architectures, mechanisms of allocation and management of resources, and levels of sophistication of DBMS optimizers which affect performance, scalability and cost. As a result, the response time, CPU Service Time and the number of I/Os for the same query, accessing the similar table in the Cloud could be significantly different than On Prem. In order to select the appropriate Cloud platform as a first step we perform a Workload Characterization for On Prem Data Warehouse. Each Data Warehouse workload represents a specific line of business and includes activity of many users generating concurrently simple and complex queries accessing data from different tables. Each workload has different demands for resources and different Response Time and Throughput Service Level Goals. In this presentation we will review results of the workload characterization for an On Prem Data Warehouse environment. During the second step we collected measurement data for standard TPC-DS benchmark tests performed in AWS Vantage, Redshift and Snowflake Cloud platform for different sizes of the data sets and different number of concurrent users. During the third step we used the results of the workload characterization and measurement data collected during the benchmark to modify BEZNext On Prem Closed Queueing model to model individual Clouds. And finally, during the fourth step we used our Model to take into consideration differences in concurrency, priorities and resource allocation to different workloads. BEZNext optimization algorithms incorporating Graduate search mechanism are used to find the AWS instance type and minimum number of instances which will be required to meet SLGs for each of the workloads. Publicly available information about the cost of the different AWS instances is used to predict the cost of supporting workloads in the Cloud month by month during next 12 months.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {16},
numpages = {1},
keywords = {optimization., service level goals, workload characterization, benchmarking, workload forecasting, cloud platform, modeling, seasonality determination},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1109/UCC.2014.40,
author = {Chauvel, Franck and Song, Hui and Ferry, Nicolas and Fleurey, Franck},
title = {Robustness Indicators for Cloud-Based Systems Topologies},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.40},
doi = {10.1109/UCC.2014.40},
abstract = {Various services are now available in the Cloud, ranging from turnkey databases and application servers to high-level services such as continuous integration or source version control. To stand out of this diversity, robustness of service compositions is an important selling argument, but which remains difficult to understand and estimate as it does not only depend on services but also on the underlying platform and infrastructure. Yet, choosing a specific service composition may fail to deliver the expected robustness, but reverting early choices may jeopardise the success of any Cloud project. Inspired by existing models used in Biology to quantify the robustness of ecosystems, we show how to tailor them to obtain early indicators of robustness for cloud-based deployments. This technique helps identify weakest services in the overall architecture and in turn mitigates the risk of having to revert key architectural choices. We illustrate our approach by comparing the robustness of four alternative deployments of the Sens App application, which includes a Mongo DB database, four REST services and a graphical web-front end.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {307–316},
numpages = {10},
keywords = {robustness indicators, failures sequences, extinction sequences, cloud topologies, deployment, bio-inspired},
series = {UCC '14}
}

@inproceedings{10.1145/2590651.2590675,
author = {Coutinho, Emanuel F. and Moreira, Leonardo O. and Paillard, Gabriel A. L. and Maia, Jos\'{e} G. R.},
title = {How to Deploy a Virtual Learning Environment in the Cloud?},
year = {2014},
isbn = {9781450324359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590651.2590675},
doi = {10.1145/2590651.2590675},
abstract = {Cloud computing is a trend of technology aimed at providing on-demand services with payment based on usage. Virtual Learning Environments (VLEs) are applications that require a highly scalable architecture that provides for its users an acceptable level of Quality of Service (QoS). This work aims to show the steps needed to install a VLE in a cloud computing infrastructure. The VLE's migration to this new type of execution environment allows the increase of its use but also brings some performance issues that must be considered. The case study will consider the Moodle VLE which was chosen for its widespread use.},
booktitle = {Proceedings of the 7th Euro American Conference on Telematics and Information Systems},
articleno = {25},
numpages = {4},
keywords = {cloud computing, moodle, virtual learning environment},
location = {Valparaiso, Chile},
series = {EATIS '14}
}

@article{10.1145/3242901,
author = {Rolin, Raphael and Antaluca, Eduard and Batoz, Jean-Louis and Lamarque, Fabien and Lejeune, Mathieu},
title = {From Point Cloud Data to Structural Analysis Through a Geometrical HBIM-Oriented Model},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3242901},
doi = {10.1145/3242901},
abstract = {The assessment of the structural behavior of historic masonry structures like Gothic cathedrals is an important engineering and architectural issue, because of the economic and cultural relevance of such buildings. In this article, we present a complete numerical methodology for point clouds processing, geometrical and parametric 3D modeling, and finite element structural analysis of the spire of the Cathedral of Senlis, France. Our work highlights the particular difficulties linked with digitization and geometrical modeling of highly complex Gothic structures, as well as the need to find compromises between quality and accuracy of extracted data used for geometrical modeling and structural analysis.The methodology enables the semi-automatic transformation of a three-dimensional points cloud, surveyed through terrestrial laser scanner, into a three-dimensional geometrical historic building information modeling (hBIM)-oriented model, and its use to propose a consistent 3D finite element mesh suitable for advanced structural analysis. A full software chain is integrated in the proposed numerical process, so as to use the most important data contained in the real geometry and accurately transposed in the point clouds. After a successful data processing step with 3DReshaper software that proved to be necessary for enhancement of point clouds, a semi-automated geometrical hBIM-oriented modeling step with Rhinoceros5 software and VisualARQ plugin has allowed the construction of a hybrid model by reverse engineering from the point clouds. This 3D model, containing both geometrical and parametric data of the structure, has been exported to the Hyperworks suite for finite element structural analysis under self-weight. Our computations focused on the estimation of the structure deformation and on the distribution of compression and traction stresses in all components of the complex structure. It is found that the spire is safe. Based on reliable and properly detailed results, our study provides significant information for understanding the behavior of the structure and potential damage monitoring.},
journal = {J. Comput. Cult. Herit.},
month = {may},
articleno = {9},
numpages = {26},
keywords = {cultural heritage, finite element structural analysis, point clouds, building information modeling, geometrical modeling, Historical buildings, terrestrial laser scanning}
}

@inproceedings{10.1109/MICRO.2014.53,
author = {Zhang, Yunqi and Laurenzano, Michael A. and Mars, Jason and Tang, Lingjia},
title = {SMiTe: Precise QoS Prediction on Real-System SMT Processors to Improve Utilization in Warehouse Scale Computers},
year = {2014},
isbn = {9781479969982},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2014.53},
doi = {10.1109/MICRO.2014.53},
abstract = {One of the key challenges for improving efficiency in warehouse scale computers (WSCs) is to improve server utilization while guaranteeing the quality of service (QoS) of latency-sensitive applications. To this end, prior work has proposed techniques to precisely predict performance and QoS interference to identify 'safe' application co-locations. However, such techniques are only applicable to resources shared across cores. Achieving such precise interference prediction on real-system simultaneous multithreading (SMT) architectures has been a significantly challenging open problem due to the complexity introduced by sharing resources within a core.In this paper, we demonstrate through a real-system investigation that the fundamental difference between resource sharing behaviors on CMP and SMT architectures calls for a redesign of the way we model interference. For SMT servers, the interference on different shared resources, including private caches, memory ports, as well as integer and floating-point functional units, do not correlate with each other. This insight suggests the necessity of decoupling interference into multiple resource sharing dimensions. In this work, we propose SMiTe, a methodology that enables precise performance prediction for SMT co-location on real-system commodity processors. With a set of Rulers, which are carefully designed software stressors that apply pressure to a multidimensional space of shared resources, we quantify application sensitivity and contentiousness in a decoupled manner. We then establish a regression model to combine the sensitivity and contentiousness in different dimensions to predict performance interference. Using this methodology, we are able to precisely predict the performance interference in SMT co-location with an average error of 2.80\% on SPEC CPU2006 and 1.79\% on Cloud Suite. Our evaluation shows that SMiTe allows us to improve the utilization of WSCs by up to 42.57\% while enforcing an application's QoS requirements.},
booktitle = {Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {406–418},
numpages = {13},
keywords = {quality of service, simultaneous multithreading, warehouse scale computer, datacenter},
location = {Cambridge, United Kingdom},
series = {MICRO-47}
}

@inproceedings{10.1145/2602576.2602580,
author = {Chavarriaga, Jaime and Noguera, Carlos A. and Casallas, Rubby and Jonckers, Viviane},
title = {Architectural Tactics Support in Cloud Computing Providers: The Jelastic Case},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602580},
doi = {10.1145/2602576.2602580},
abstract = {When developing and deploying applications in the cloud, architects face the challenge of conciliating architectural decisions with the options and restrictions imposed by the chosen cloud provider. An architectural decision can be seen as a two-step process: selecting architectural tactics to promote quality attributes and choosing design alternatives to implement those tactics. Available design alternatives are limited by the offer of the cloud provider. When configuring the cloud platform and its services as directed by the chosen tactics, the architect must be mindful of conflicts among the available alternatives. These trade-offs amongst the desired quality attributes can be difficult to detect, understand and ultimately solve. In this paper, we consider the case of Jelastic, a particular cloud platform provider, to illustrate: 1) the modeling of architectural tactics and their corresponding design alternatives using cloud configuration options, and 2) a process that exploits these models to determine which options to use in order to implement a combination of tactics. Furthermore, we present an analysis for this cloud provider that explains which combinations of tactics and configurations lead to trade-offs.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {13–22},
numpages = {10},
keywords = {feature model, quality attributes, cloud computing, architectural tactics},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/3522664.3528601,
author = {Paleyes, Andrei and Cabrera, Christian and Lawrence, Neil D.},
title = {An Empirical Evaluation of Flow Based Programming in the Machine Learning Deployment Context},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528601},
doi = {10.1145/3522664.3528601},
abstract = {As use of data driven technologies spreads, software engineers are more often faced with the task of solving a business problem using data-driven methods such as machine learning (ML) algorithms. Deployment of ML within large software systems brings new challenges that are not addressed by standard engineering practices and as a result businesses observe high rate of ML deployment project failures. Data Oriented Architecture (DOA) is an emerging approach that can support data scientists and software developers when addressing such challenges. However, there is a lack of clarity about how DOA systems should be implemented in practice. This paper proposes to consider Flow-Based Programming (FBP) as a paradigm for creating DOA applications. We empirically evaluate FBP in the context of ML deployment on four applications that represent typical data science projects. We use Service Oriented Architecture (SOA) as a baseline for comparison. Evaluation is done with respect to different application domains, ML deployment stages, and code quality metrics. Results reveal that FBP is a suitable paradigm for data collection and data science tasks, and is able to simplify data collection and discovery when compared with SOA. We discuss the advantages of FBP as well as the gaps that need to be addressed to increase FBP adoption as a standard design paradigm for DOA.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {54–64},
numpages = {11},
keywords = {machine learning, service-oriented architecture, flow-based programming, software engineering},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3631204.3631859,
author = {Sapin, Etienne and Menon, Suraj and Ge, Jingquan and Habib, Sheikh Mahbub and Heymann, Maurice and Li, Yuekang and Palige, Rene and Byman, Gabriel and Liu, Yang},
title = {Monitoring Automotive Software Security Health through Trustworthiness Score},
year = {2023},
isbn = {9798400704543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631204.3631859},
doi = {10.1145/3631204.3631859},
abstract = {The automotive industry is drastically moving towards autonomous. This trend constitutes in a fundamental change of going from mechanical and electrical engineering towards software-driven approaches. Modern vehicles can embed more than hundred electronic control units (ECUs). As autonomous vehicles require more intelligence as well as more computing power, high-performance computers (HPCs) bring the data management capabilities for cloud and IoT services to support the transition to a service-oriented vehicle system architecture. With this growing reliance on software in vehicles, software reliability and trustworthiness are increasingly critical to vehicle security. Measuring security trustworthiness in automotive software is even more valuable as cybersecurity is shifting to the left, i.e. in the early phase of development and design process. In this article, we propose a novel method for evaluating security trustworthiness of automotive software by leveraging a computational trust model. The method consists of selecting different domains contributing to software security, calculating their respective expectation value (trustworthiness score) and combining it using operators from the computational trust model. We evaluate the method using an automotive use case, i.e. over-the-air (OTA) update software. We describe a possible integration of the proposed method into a solution which would be valuable for cybersecurity stakeholders, e.g. cybersecurity managers, cybersecurity architects and software quality managers, aiming to monitor security health of automotive software throughout its development life cycle.},
booktitle = {Proceedings of the 7th ACM Computer Science in Cars Symposium},
articleno = {1},
numpages = {9},
keywords = {Software health, Trustworthiness, Data visualization},
location = {<conf-loc>, <city>Darmstadt</city>, <country>Germany</country>, </conf-loc>},
series = {CSCS '23}
}

@inproceedings{10.1109/ASE51524.2021.9678708,
author = {Wang, Hanzhang and Wu, Zhengkai and Jiang, Huai and Huang, Yichao and Wang, Jiamu and Kopru, Selcuk and Xie, Tao},
title = {Groot: An Event-Graph-Based Approach for Root Cause Analysis in Industrial Settings},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678708},
doi = {10.1109/ASE51524.2021.9678708},
abstract = {For large-scale distributed systems, it is crucial to efficiently diagnose the root causes of incidents to maintain high system availability. The recent development of microservice architecture brings three major challenges (i.e., complexities of operation, system scale, and monitoring) to root cause analysis (RCA) in industrial settings. To tackle these challenges, in this paper, we present Groot, an event-graph-based approach for RCA. Groot constructs a real-time causality graph based on events that summarize various types of metrics, logs, and activities in the system under analysis. Moreover, to incorporate domain knowledge from site reliability engineering (SRE) engineers, Groot can be customized with user-defined events and domain-specific rules. Currently, Groot supports RCA among 5,000 real production services and is actively used by the SRE teams in eBay, a global e-commerce system serving more than 159 million active buyers per year. Over 15 months, we collect a data set containing labeled root causes of 952 real production incidents for evaluation. The evaluation results show that Groot is able to achieve 95\% top-3 accuracy and 78\% top-1 accuracy. To share our experience in deploying and adopting RCA in industrial settings, we conduct a survey to show that users of Groot find it helpful and easy to use. We also share the lessons learned from deploying and adopting Groot to solve RCA problems in production environments.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {419–429},
numpages = {11},
keywords = {microservices, root cause analysis, AIOps, observability},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3555962.3555968,
author = {Alzboon, Ghufran and Al-Said Ahmad, Amro},
title = {A Performance Evaluation Approach for N-Tier Cloud-Based Software Services},
year = {2022},
isbn = {9781450396578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555962.3555968},
doi = {10.1145/3555962.3555968},
abstract = {Cloud computing and cloud testing are vast fields that have attracted significant attention recently. In addition, the need to find an approach for measuring cloud-based applications' effectiveness has also increased. In this work, we introduced an approach to testing the performance of the cloud software services on the Amazon cloud. We used two cloud-based applications hosted in the Amazon cloud to demonstrate the approach depending on five technical performance metrics. We applied the testing methodology using a JMeter test script. The two selected applications represent two different taxonomies: 2-tier and 3-tier architectures. Following the testing process, we found that the WordPress application (i.e., 3-tier architecture) performs better than Ghost and is more stable in terms of the selected performance metrics. Practitioners would benefit from this study by a better understanding of the assessment and testing of n-tier Cloud-Based Software Services using technical arguments.},
booktitle = {Proceedings of the 2022 6th International Conference on Cloud and Big Data Computing},
pages = {31–36},
numpages = {6},
keywords = {evaluation method, Cloud computing, n-tier, Software services, performance},
location = {Birmingham, United Kingdom},
series = {ICCBDC '22}
}

@inproceedings{10.1145/3578244.3583726,
author = {Straesser, Martin and Mathiasch, Jonas and Bauer, Andr\'{e} and Kounev, Samuel},
title = {A Systematic Approach for Benchmarking of Container Orchestration Frameworks},
year = {2023},
isbn = {9798400700682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578244.3583726},
doi = {10.1145/3578244.3583726},
abstract = {Container orchestration frameworks play a critical role in modern cloud computing paradigms such as cloud-native or serverless computing. They significantly impact the quality and cost of service deployment as they manage many performance-critical tasks such as container provisioning, scheduling, scaling, and networking. Consequently, a comprehensive performance assessment of container orchestration frameworks is essential. However, until now, there is no benchmarking approach that covers the many different tasks implemented in such platforms and supports evaluating different technology stacks. In this paper, we present a systematic approach that enables benchmarking of container orchestrators. Based on a definition of container orchestration, we define the core requirements and benchmarking scope for such platforms. Each requirement is then linked to metrics and measurement methods, and a benchmark architecture is proposed. With COFFEE, we introduce a benchmarking tool supporting the definition of complex test campaigns for container orchestration frameworks. We demonstrate the potential of our approach with case studies of the frameworks Kubernetes and Nomad in a self-hosted environment and on the Google Cloud Platform. The presented case studies focus on container startup times, crash recovery, rolling updates, and more.},
booktitle = {Proceedings of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {187–198},
numpages = {12},
keywords = {container orchestration, kubernetes, benchmarking, performance, nomad},
location = {Coimbra, Portugal},
series = {ICPE '23}
}

@inproceedings{10.5555/3581644.3581646,
author = {Spatharakis, Dimitrios and Dimolitsas, Ioannis and Vlahakis, Eleftherios and Dechouniotis, Dimitrios and Athanasopoulos, Nikolaos and Papavassiliou, Symeon},
title = {Distributed Resource Autoscaling in Kubernetes Edge Clusters},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Maximizing the performance of modern applications requires timely resource management of the virtualized resources. However, proactively deploying resources for meeting specific application requirements subject to a dynamic workload profile of incoming requests is extremely challenging. To this end, the fundamental problems of task scheduling and resource autoscaling must be jointly addressed. This paper presents a scalable architecture compatible with the decentralized nature of Kubernetes [1], to solve both. Exploiting the stability guarantees of a novel AIMD-like task scheduling solution, we dynamically redirect the incoming requests towards the containerized application. To cope with dynamic workloads, a prediction mechanism allows us to estimate the number of incoming requests. Additionally, a Machine Learning-based (ML) Application Profiling Modeling is introduced to address the scaling, by co-designing the theoretically-computed service rates obtained from the AIMD algorithm with the current performance metrics. The proposed solution is compared with the state-of-the-art autoscaling techniques under a realistic dataset in a small edge infrastructure and the trade-off between resource utilization and QoS violations are analyzed. Our solution provides better resource utilization by reducing CPU cores by 8\% with only an acceptable increase in QoS violations.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {1},
numpages = {7},
keywords = {machine learning, resource autoscaling, edge computing, resource management, kubernetes},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@article{10.1145/3586181,
author = {Bachiega, Joao and Costa, Breno and Carvalho, Leonardo R. and Rosa, Michel J. F. and Araujo, Aleteia},
title = {Computational Resource Allocation in Fog Computing: A Comprehensive Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3586181},
doi = {10.1145/3586181},
abstract = {Fog computing is a paradigm that allows the provisioning of computational resources and services at the edge of the network, closer to the end devices and users, complementing cloud computing. The heterogeneity and large number of devices are challenges to obtaining optimized resource allocation in this environment. Over time, some surveys have been presented on resource management in fog computing. However, they now lack a broader and deeper view about this subject, considering the recent publications. This article presents a systematic literature review with a focus on resource allocation for fog computing, and in a more comprehensive way than the existing works. The survey is based on 108 selected publications from 2012 to 2022. The analysis has exposed their main techniques, metrics used, evaluation tools, virtualization methods, architecture, and domains where the proposed solutions were applied. The results show an updated and comprehensive view about resource allocation in fog computing. The main challenges and open research questions are discussed, and a new fog computing resource management cycle is proposed.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {336},
numpages = {31},
keywords = {resource allocation, Fog computing, resource management, resource provisioning}
}

@inproceedings{10.1145/3630202.3630233,
author = {Cornacchia, Alessandro and Benson, Theophilus A. and Bilal, Muhammad and Canini, Marco},
title = {MicroView: Cloud-Native Observability with Temporal Precision},
year = {2023},
isbn = {9798400704529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630202.3630233},
doi = {10.1145/3630202.3630233},
abstract = {We present MicroView, a system designed to improve the accuracy and timeliness of observability in cloud-native applications, while minimizing overhead. MicroView stands out from conventional observability tools by incorporating metrics processing stages at every node within a local lightweight data-plane. We preliminary demonstrate its benefits for distributed tracing and outline a set of architectural choices focused on offloading the MicroView data-plane to IPU accelerators, such as a BlueField-3 SmartNIC, thus limiting the interference with running services.},
booktitle = {Proceedings of the on CoNEXT Student Workshop 2023},
pages = {7–8},
numpages = {2},
keywords = {SmartNIC, programmable networks, microservices observability, cloud-native},
location = {<conf-loc>, <city>Paris</city>, <country>France</country>, </conf-loc>},
series = {CoNEXT-SW '23}
}

@inproceedings{10.1145/3570361.3592529,
author = {Wen, Hao and Li, Yuanchun and Zhang, Zunshuai and Jiang, Shiqi and Ye, Xiaozhou and Ouyang, Ye and Zhang, Yaqin and Liu, Yunxin},
title = {AdaptiveNet: Post-Deployment Neural Architecture Adaptation for Diverse Edge Environments},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570361.3592529},
doi = {10.1145/3570361.3592529},
abstract = {Deep learning models are increasingly deployed to edge devices for real-time applications. To ensure stable service quality across diverse edge environments, it is highly desirable to generate tailored model architectures for different conditions. However, conventional pre-deployment model generation approaches are not satisfactory due to the difficulty of handling the diversity of edge environments and the demand for edge information. In this paper, we propose to adapt the model architecture after deployment in the target environment, where the model quality can be precisely measured and private edge data can be retained. To achieve efficient and effective edge model generation, we introduce a pretraining-assisted on-cloud model elastification method and an edge-friendly on-device architecture search method. Model elastification generates a high-quality search space of model architectures with the guidance of a developer-specified oracle model. Each subnet in the space is a valid model with different environment affinity, and each device efficiently finds and maintains the most suitable subnet based on a series of edge-tailored optimizations. Extensive experiments on various edge devices demonstrate that our approach is able to achieve significantly better accuracy-latency tradeoffs (e.g. 46.74\% higher on average accuracy with a 60\% latency budget) than strong baselines with minimal overhead (13 GPU hours in the cloud and 2 minutes on the edge server).},
booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
articleno = {28},
numpages = {17},
keywords = {edge environments, post-deployment adaptation, neural networks, model elastification},
location = {Madrid, Spain},
series = {ACM MobiCom '23}
}

@inproceedings{10.1145/3584376.3584534,
author = {Liu, Xin and Wang, Jibin and Qin, Shuwei},
title = {An Anomaly Detection Framework Based on Data Center Operation and Maintenance Data},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584534},
doi = {10.1145/3584376.3584534},
abstract = {Data centers need to monitor various metrics of their different application platforms and applications in real-time. As the system architectures and application services of different application platforms within them become more complex, the requirements for their anomaly detection capabilities are higher. Therefore, this paper proposes an anomaly detection framework based on data center operation and maintenance data. The framework in this paper consists of three parts, including operation and maintenance data cleaning, data feature extraction, and model routing. It is used to select the appropriate model through model routing based on the indicators such as stability and periodicity obtained from data feature extraction of each application platform. At the same time, in order to enhance the expansion capability of the detection algorithm, a cloud-ground hybrid framework is used and a module for algorithm model management is designed to facilitate interaction with the cloud. After testing on SWAT and WADI datasets, the anomaly detection algorithm with the addition of model routing in the framework has good accuracy and recall performance compared to a single anomaly algorithm model, showing advantages in the task of identifying anomalies.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {883–889},
numpages = {7},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.5555/3539845.3539923,
author = {Li, Zhuoran and Zhao, Dan},
title = {ThingNet: A Lightweight Real-Time Mirai IoT Variants Hunter through CPU Power Fingerprinting},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Internet of Things (IoT) devices have become attractive targets of cyber criminals, whereas attackers have been leveraging these vulnerable devices most notably via the infamous Mirai-based botnets, accounting for nearly 90\% of IoT malware attacks in 2020. In this work, we propose a robust, universal and non-invasive Mirai-based malware detection engine employing a compact deep neural network architecture. Our design allows programmatic collection of CPU power footprints with integrated current sensors under various device states, such as idle, service and attack. A lightweight online inference model is deployed in the CPU for on-the-fly classification. Our model is robust against noisy environment with a lucid design of noise reduction function. This work appears to be the first step towards a viable CPU malware detection engine based on power fingerprinting. The extensive simulation study under ARM architecture that is widely used in IoT devices, demonstrates a high detection accuracy of 99.1\% at a speed less than 1ms. By analyzing Mirai-based infection under distinguishable phases for power feature extraction, our model has further demonstrated an accuracy of 96.3\% on model-unknown variants detection.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {310–315},
numpages = {6},
keywords = {lightweight deep learning, noise reduction, mirai IoT variants detection, power side-channel auditing},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@article{10.14778/3529337.3529344,
author = {Burckhardt, Sebastian and Chandramouli, Badrish and Gillum, Chris and Justo, David and Kallas, Konstantinos and McMahon, Connor and Meiklejohn, Christopher S. and Zhu, Xiangfeng},
title = {Netherite: Efficient Execution of Serverless Workflows},
year = {2022},
issue_date = {April 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3529337.3529344},
doi = {10.14778/3529337.3529344},
abstract = {Serverless is a popular choice for cloud service architects because it can provide scalability and load-based billing with minimal developer effort. Functions-as-a-service (FaaS) are originally stateless, but emerging frameworks add stateful abstractions. For instance, the widely used Durable Functions (DF) allow developers to write advanced serverless applications, including reliable workflows and actors, in a programming language of choice. DF implicitly and continuosly persists the state and progress of applications, which greatly simplifies development, but can create an IOps bottleneck.To improve efficiency, we introduce Netherite, a novel architecture for executing serverless workflows on an elastic cluster. Netherite groups the numerous application objects into a smaller number of partitions, and pipelines the state persistence of each partition. This improves latency and throughput, as it enables workflow steps to group commit, even if causally dependent. Moreover, Netherite leverages FASTER's hybrid log approach to support larger-than-memory application state, and to enable efficient partition movement between compute hosts.Our evaluation shows that (a) Netherite achieves lower latency and higher throughput than the original DF engine, by more than an order of magnitude in some cases, and (b) that Netherite has lower latency than some commonly used alternatives, like AWS Step Functions or cloud storage triggers.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {1591–1604},
numpages = {14}
}

@inproceedings{10.1145/3588444.3591024,
author = {Sekar, Santhoshini and Mishra, Ashok Kumar and Giladi, Alex and Grois, Dan},
title = {Novel Motion-Compensated Spatio-Temporal Filtering Scheme for X265 Open-Source Video Encoder},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588444.3591024},
doi = {10.1145/3588444.3591024},
abstract = {There is a strong demand to decrease the video transmission bitrate without reducing visual quality [1]. The x265 encoder [2]-[4] is a popular open-source encoder, which generates bitstreams compliant with the H.265/MPEG-HEVC video coding standard [5]. Built on top of x264[6], the x265 encoder is integrated into several popular open-source frameworks, such as ffmpeg [7], GStreamer [8], and Handbrake [9]. In addition, the x265 is used by a variety of broadcast and streaming service providers who leverage the benefits of HEVC for streaming live and over-the-top (OTT) content. In addition to implementing nearly all the tools defined in HEVC, it implements many algorithmic optimizations that enable trading off encoder performance for quality [2]-[4]. The performance-critical kernels are implemented with hand-coded assembly kernels that use AVX2 and AVX-512 single instruction, multiple data instructions to improve performance on x86 CPUs. This flexible architecture of x265 makes it a popular choice for HEVC encoding for both on-premises and cloud services.Recent x265 development efforts have been focused on further improving the coding gains. Specifically, the motion compensated spatio-temporal filtering (MCSTF) employed within the coding loop is especially useful for pictures that contain a high level of noise. It utilizes previously generated motion vectors across different video content resolutions to find the best temporal correspondence for low-pass filtering, while the temporal filtering is applied to the I- and P-frames. Figure 1 schematically illustrates the motion estimation process for temporal filtering in a temporal window, which consists of 5 adjacent pictures: two past, two future and one central picture used for producing a single filtered picture. Motion estimation is applied between the central picture and each future or past picture, thereby generating multiple motion-compensated predictions, which are then combined by using adaptive filtering to produce a final noise-reduced picture. Thus, a hierarchical motion estimation scheme is employed (layers L0, L1 and L2, are illustrated in Figure 2). Subsampled pictures are generated for all reference picturesand the original picture as well: i.e., L1, while L2 is derived from L1 by using the same subsampling method. First, the motion estimation is done for each 16x16 block in L2. Then, the selected motion vector is used as an initial value for estimating the motion in L1. After that, the same is performed for estimating the motion in L0.As a final step, the subpixel motion is estimated for each 8x8 block by using an interpolation filter on L0. Particularly, the motion of reference pictures before and after, relative to the original picture, is estimated per the 8x8 picture block. In turn, the motion compensation is applied on the pictures before and after the original picture according to the best matching motion for each block. i.e., such that pixel coordinates of the original picture in each block have the best matching coordinates within the referenced pictures. The filter is then applied to the current pixels, and after that, the filtered picture is encoded. Note that the pixels are processed one by one for the luma and chroma channels. The new sample value, is calculated by using the following equation:[EQUATION]where Io is the original pixel, Ir(i) is the intensity of the corresponding pixel within the motion compensated picture i, and wr(i, a) is the weight of the motion compensated picture where a is the number of available motion compensated pictures. The conducted extensive experimental results show significant bit-rate savings in terms of BD-BR [10].},
booktitle = {Proceedings of the 2nd Mile-High Video Conference},
pages = {126–127},
numpages = {2},
keywords = {open-source, computational complexity, MCSTF, H.265, HEVC, coding gain, x265, HM},
location = {Denver, CO, USA},
series = {MHV '23}
}

@inproceedings{10.1145/2737182.2737185,
author = {Lehrig, Sebastian and Eikerling, Hendrik and Becker, Steffen},
title = {Scalability, Elasticity, and Efficiency in Cloud Computing: A Systematic Literature Review of Definitions and Metrics},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737185},
doi = {10.1145/2737182.2737185},
abstract = {Context: In cloud computing, there is a multitude of definitions and metrics for scalability, elasticity, and efficiency. However, stakeholders have little guidance for choosing fitting definitions and metrics for these quality properties, thus leading to potential misunderstandings. For example, cloud consumers and providers cannot negotiate reliable and quantitative service level objectives directly understood by each stakeholder. Objectives: Therefore, we examine existing definitions and metrics for these quality properties from the viewpoint of cloud consumers, cloud providers, and software architects with regard to commonly used concepts. Methods: We execute a systematic literature review (SLR), reproducibly collecting common concepts in definitions and metrics for scalability, elasticity, and efficiency. As quality selection criteria, we assess whether existing literature differentiates the three properties, exemplifies metrics, and considers typical cloud characteristics and cloud roles. Results: Our SLR yields 418 initial results from which we select 20 for in-depth evaluation based on our quality selection criteria. In our evaluation, we recommend concepts, definitions, and metrics for each property. Conclusions: Software architects can use our recommendations to analyze the quality of cloud computing applications. Cloud providers and cloud consumers can specify service level objectives based on our metric suggestions.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {83–92},
numpages = {10},
keywords = {cloud, systematic literature review, scalability, metrics, elasticity, definitions, efficiency, cloud computing},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@inproceedings{10.5555/3233397.3233523,
author = {Kirsal, Yonal and Ever, Yoney Kirsal and Mostarda, Leonardo and Gemikonakli, Orhan},
title = {Analytical Modelling and Performability Analysis for Cloud Computing Using Queuing System},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {In recent years, cloud computing becomes a new computing model emerged from the rapid development of the internet. Users can reach their resources with high flexibility using the cloud computing systems all over the world. However, such systems are prone to failures. In order to obtain realistic quality of service (QoS) measurements, failure and recovery behaviours of the system should be considered. System's failures and repairs are associated with availability context in QoS measurements. In this paper, performance issues are considered with the availability of the system. Markov Reward Model (MRM) method is used to get QoS measurements. The mean queue length (MQL) results are calculated using the MRM. The results explicitly show that failures and repairs affect the system performance significantly.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {643–647},
numpages = {5},
keywords = {quality of service, queuing system, analytical modelling, performability analysis, cloud computing},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1109/CCGrid.2016.83,
author = {Farias, Victor A. E. and Sousa, Fl\'{a}vio R. C. and Maia, Jos\'{e} G. R. and Gomes, Jo\~{a}o P. P. and Machado, Javam C.},
title = {Machine Learning Approach for Cloud NoSQL Databases Performance Modeling},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.83},
doi = {10.1109/CCGrid.2016.83},
abstract = {Cloud computing is a successful, emerging paradigm that supports on-demand services with pay-as-you-go model. With the exponential growth of data, NoSQL databases have been used to manage data in the cloud. In these newly emerging settings, mechanisms to guarantee Quality of Service heavily relies on performance predictability, i.e., the ability to estimate the impact of concurrent query execution on the performance of individual queries in a continuously evolving workload. This paper presents a performance modeling approach for NoSQL databases in terms of performance metrics which is capable of capturing the non-linear effects caused by concurrency and distribution aspects. Experimental results confirm that our performance modeling can accurately predict mean response time measurements under a wide range of workload configurations.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {617–620},
numpages = {4},
keywords = {performance modeling, cloud computing, NoSQL},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.5555/3400397.3400612,
author = {Verghelet, Paula and Mocskos, Esteban},
title = {First Steps in Creating a Methodology to Develop and Test Scheduling Policies for Internet of Things},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Internet of Things (IoT) refers to a paradigm in which all objects can send information and collaborate with their computing resources through the Internet. The combination of Fog and Cloud Computing defines a distributed system composed of heterogeneous resources interconnected by different communication technologies. Despite its theoretical capacity, using these computational resources poses a challenge to distributed applications and scheduling policies. In this work, we show the initial steps in developing a tool to support the creation of scheduling policies combining simulation and validation. We show the details to be considered when selecting and configuring the different layers of software. To evaluate the proposal, we use a segmentation method in both platforms and a theoretical model to predict the total compute time. Our results show that both simulation and validation platforms agree in the obtained results which also can be explained in terms of a theoretical model.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2629–2640},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3349614.3356028,
author = {Tomei, Matthew and Schwing, Alexander and Narayanasamy, Satish and Kumar, Rakesh},
title = {Sensor Training Data Reduction for Autonomous Vehicles},
year = {2019},
isbn = {9781450369282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349614.3356028},
doi = {10.1145/3349614.3356028},
abstract = {Ensuring safety and reliability of autonomous vehicles requires good learning models which, in turn, require a large amount of real-world training data. Data produced by in-vehicle sensors (e.g., cameras, LIDARs, IMUs, etc.) can be used for training; however, both local storage and transmission of this sensor data to the cloud for subsequent use in training can be prohibitively expensive due to the staggering volume of data produced by these sensors, especially the cameras. In this paper, we perform the first exploration of techniques for reducing video frames in a way that the quality of training for autonomous vehicles is minimally affected. We particularly focus on utility aware data reduction schemes where the potential contribution of a video frame to enhancing the quality of learning (or utility) is explicitly considered during data reduction. Since actual utility of a video frame cannot be computed online, we use surrogate utility metrics to decide what video frames to keep for training and which ones to discard. Our results show that utility-aware data reduction schemes can reduce the amount of camera data required for training by as much as $16times$ compared to random sampling for the same quality of learning (in terms of IoU).},
booktitle = {Proceedings of the 2019 Workshop on Hot Topics in Video Analytics and Intelligent Edges},
pages = {45–50},
numpages = {6},
keywords = {data reduction, semantic segmentation, self driving car, sensor, compression, autonomous vehicle, active learning, machine learning},
location = {Los Cabos, Mexico},
series = {HotEdgeVideo'19}
}

@inproceedings{10.1145/3011077.3011126,
author = {Van Sinh, Nguyen and Ha, Tran Manh and Thanh, Nguyen Tien},
title = {Filling Holes on the Surface of 3D Point Clouds Based on Tangent Plane of Hole Boundary Points},
year = {2016},
isbn = {9781450348157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011077.3011126},
doi = {10.1145/3011077.3011126},
abstract = {Filling the holes of a triangular mesh has been studied for many years in the field of geometric modeling. This research is one of the reconstructing steps of a triangular mesh (or called refinement of a mesh) in order to improve the quality of a 3D triangular surface. With the same idea of hole filling in a mesh, filling in the holes of a 3D point cloud is still a challenge to the researchers. This paper describes a method for filling holes in an elevation surface of 3D point clouds structured in a 3D grid. The novelty of the method is processed directly on the 3D point clouds consisting of two steps. In the first step, we determine the boundary of hole. In the second step, we fill the holes based on the computation of tangent plane for each boundary point. Following clock-wise direction on the hole boundary, we compute and insert missing points on each tangent plane. This process is repeated and refined ring by ring from the hole boundary to the inside of the hole. The obtained results show that the processing time of algorithm is very fast, the output surfaces preserve their initial shapes and local curvatures.},
booktitle = {Proceedings of the 7th Symposium on Information and Communication Technology},
pages = {331–338},
numpages = {8},
keywords = {hole filling, elevation surface, boundary point, tangent plane, 3D point cloud},
location = {Ho Chi Minh City, Vietnam},
series = {SoICT '16}
}

@inproceedings{10.5555/3581644.3581660,
author = {Passos, Edenilson J\^{o}natas dos and Fiorese, Adriano},
title = {Monitoring Metrics for Load Balancing over Video Content Distribution Servers},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Cloud computing and video streaming services have been in constant expansion in recent years. Along with it, the demand for computing resources has also increased significantly. In this context, monitoring the use of these resources is crucial to maintain a satisfactory level of Quality of Service and, consequently, Quality of Experience, especially in video transmission services. This work discusses a new method of monitoring resources and quality of service metrics on content servers involving CPU utilization and server throughput, which is obtained in a distributed way. For that, a distributed collector system that is based on a modified version of the ring election algorithm is developed to retrieve the Quality of Service metrics in each server. Evaluation experiment results show that there are no performance gains on the system such as the content loading faster for the user, there are however, improvements in terms of the whole system scalability. The greater the number of servers for monitoring, the better the approach is compared to the traditional method of monitoring resources through request and response.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {13},
numpages = {7},
keywords = {load balancing, monitoring, SDN},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@INPROCEEDINGS{9568289,
  author={Weerasinghe, L. D. S. B. and Perera, Indika},
  booktitle={2021 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
  title={An exploratory evaluation of replacing ESB with microservices in service-oriented architecture}, 
  year={2021},
  volume={4},
  number={},
  pages={137-144},
  abstract={With the continuous progress in technology during the past few decades, cloud computing has become a fast-growing technology in the world, making computerized systems widespread. The emergence of Cloud Computing has evolved towards microservice concepts, which are highly demanded by corporates for enterprise application level. Most enterprise applications have moved away from traditional unified models of software programs like monolithic architecture and traditional SOA architecture to microservice architecture to ensure better scalability, lesser investment in hardware, and high performance. The monolithic architecture is designed in a manner that all the components and the modules are packed together and deployed on a single binary. However, in the microservice architecture, components are developed as small services so that horizontally and vertically scaling is made easier in comparison to monolith or SOA architecture. SOA and monolithic architecture are at a disadvantage compared to Microservice architecture, as they require colossal hardware specifications to scale the software. In general terms, the system performance of these architectures can be measured considering different aspects such as system capacity, throughput, and latency. This research focuses on how scalability and performance software quality attributes behave when converting the SOA system to microservice architecture. Experimental results have shown that microservice architecture can bring more scalability with a minimum cost generation. Nevertheless, specific gaps in performance are identified in the perspective of the final user experiences due to the interservice communication in the microservice architecture in a distributed environment.},
  keywords={},
  doi={10.1109/SCSE53661.2021.9568289},
  ISSN={2613-8662},
  month={Sep.},}

@INPROCEEDINGS{8029803,
  author={Mazlami, Genc and Cito, Jürgen and Leitner, Philipp},
  booktitle={2017 IEEE International Conference on Web Services (ICWS)}, 
  title={Extraction of Microservices from Monolithic Software Architectures}, 
  year={2017},
  volume={},
  number={},
  pages={524-531},
  abstract={Driven by developments such as mobile computing, cloud computing infrastructure, DevOps and elastic computing, the microservice architectural style has emerged as a new alternative to the monolithic style for designing large software systems. Monolithic legacy applications in industry undergo a migration to microservice-oriented architectures. A key challenge in this context is the extraction of microservices from existing monolithic code bases. While informal migration patterns and techniques exist, there is a lack of formal models and automated support tools in that area. This paper tackles that challenge by presenting a formal microservice extraction model to allow algorithmic recommendation of microservice candidates in a refactoring and migration scenario. The formal model is implemented in a web-based prototype. A performance evaluation demonstrates that the presented approach provides adequate performance. The recommendation quality is evaluated quantitatively by custom microservice-specific metrics. The results show that the produced microservice candidates lower the average development team size down to half of the original size or lower. Furthermore, the size of recommended microservice conforms with microservice sizing reported by empirical surveys and the domain-specific redundancy among different microservices is kept at a low rate.},
  keywords={},
  doi={10.1109/ICWS.2017.61},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9165482,
  author={Khan, Michel Gokan and Taheri, Javid and Khoshkholghi, Mohammad Ali and Kassler, Andreas and Cartwright, Carolyn and Darula, Marian and Deng, Shuiguang},
  booktitle={2020 6th IEEE Conference on Network Softwarization (NetSoft)}, 
  title={A Performance Modelling Approach for SLA-Aware Resource Recommendation in Cloud Native Network Functions}, 
  year={2020},
  volume={},
  number={},
  pages={292-300},
  abstract={Network Function Virtualization (NFV) becomes the primary driver for the evolution of 5G networks, and in recent years, Network Function Cloudification (NFC) proved to be an inevitable part of this evolution. Microservice architecture also becomes the de facto choice for designing a modern Cloud Native Network Function (CNF) due to its ability to decouple components of each CNF into multiple independently manageable microservices. Even though taking advantage of microservice architecture in designing CNFs solves specific problems, this additional granularity makes estimating resource requirements for a Production Environment (PE) a complex task and sometimes leads to an over-provisioned PE. Traditionally, performance engineers dimension each CNF within a Service Function Chain (SFC) in a smaller Performance Testing Environment (PTE) through a series of performance benchmarks. Then, considering the Quality of Service (QoS) constraints of a Service Provider (SP) that are guaranteed in the Service Level Agreement (SLA), they estimate the required resources to set up the PE. In this paper, we used a machine learning approach to model the impact of each microservice's resource configuration (i.e., CPU and memory) on the QoS metrics (i.e. serving throughput and latency) of each SFC in a PTE. Then, considering an SP's Service Level Objectives (SLO), we proposed an algorithm to predict each microservice's resource capacities in a PE. We evaluated the accuracy of our prediction on a prototype of a cloud native 5G Home Subscriber Server (HSS). Our model showed 95%-78% accuracy in a PE that has 2-5 times more computing resources than the PTE.},
  keywords={},
  doi={10.1109/NetSoft48620.2020.9165482},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9369609,
  author={Heideker, Alexandre and Kamienski, Carlos},
  booktitle={2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC)}, 
  title={Towards a Network Queuing Assessment for Elasticity Management of Virtualized Services}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={With the increasing adoption of cloud computing, microservice architecture, and network function virtualization (NFV), addressing scalability and elasticity management becomes essential. The high demand for these services challenges the research community to create new automated management techniques, from which an essential part is the detection of bottlenecks in infrastructures and application boxes. The traditional approach based on hardware resource metrics (CPU and RAM) is the most straightforward strategy, providing independence from particular applications but may not capture the application's behavior in terms of workload variations. On the other hand, using an application-oriented approach provides a significant correlation with the end-user quality of experience but needs to be tailored for each case. We propose the Network Queuing Assessment (NQA) that breaks away with this tradeoff, capturing the application's workload variations and providing a significant correlation with the end-user quality of experience. Also, similarly to CPU and RAM, it is independent of particular applications. Our performance analysis results for CPU, RAM, and NQA metrics using virtualized applications and network functions in a cloud environment confirm this approach's usefulness.},
  keywords={},
  doi={10.1109/CCNC49032.2021.9369609},
  ISSN={2331-9860},
  month={Jan},}

@INPROCEEDINGS{8818401,
  author={Yu, Guangba and Chen, Pengfei and Zheng, Zibin},
  booktitle={2019 IEEE International Conference on Web Services (ICWS)}, 
  title={Microscaler: Automatic Scaling for Microservices with an Online Learning Approach}, 
  year={2019},
  volume={},
  number={},
  pages={68-75},
  abstract={Recently, the microservice becomes a popular architecture to construct cloud native systems due to its agility. In cloud native systems, autoscaling is a core enabling technique to adapt to workload changes by scaling out/in. However, it becomes a challenging problem in a microservice system, since such a system usually comprises a large number of different micro services with complex interactions. When bursty and unpredictable workloads arrive, it is difficult to pinpoint the scaling-needed services which need to scale and evaluate how much resource they need. In this paper, we present a novel system named Microscaler to automatically identify the scaling-needed services and scale them to meet the service level agreement (SLA) with an optimal cost for micro-service systems. Microscaler collects the quality of service metrics (QoS) with the help of the service mesh enabled infrastructure. Then, it determines the under-provisioning or over-provisioning services with a novel criterion named service power. By combining an online learning approach and a step-by-step heuristic approach, Microscaler could achieve the optimal service scale satisfying the SLA requirements. The experimental evaluations in a micro-service benchmark show that Microscaler converges to the optimal service scale faster than several state-of-the-art methods.},
  keywords={},
  doi={10.1109/ICWS.2019.00023},
  ISSN={},
  month={July},}

@INPROCEEDINGS{8441460,
  author={Keserwani, Pankaj Kumar and Samaddar, Shefalika Ghosh},
  booktitle={2017 Ninth International Conference on Advanced Computing (ICoAC)}, 
  title={An SLA Design with Digital Forensic Capabilities}, 
  year={2017},
  volume={},
  number={},
  pages={109-113},
  abstract={Cloud computing is getting rapid momentum as an alternative to traditional and professional Infrastructure of Information Technology due to its attractive features of getting everything in a service mode rather than in a product mode. Service mode using cloud makes the products and services cost effective. As consumers willing to pass on their tasks as services provider to cloud providers, trust factor is required especially when consumers have critical data. The Service Level Agreements (SLA) between cloud service consumers (CSCs) and cloud service providers (CSPs) play important role for building up trust between involved parties. SLA between parties is established in a satisfactory way upon agreements. Cloud computing is very dynamic in nature, hence continuous monitoring on Quality of Service (QoS) attributes as mentioned in SLA is required to be implemented dynamically. Managing SLAs is complicated due to complex nature of the cloud due to multi-tenancy and distributed resource sharing. The paper proposes a methodology for SLAs to be signed digitally and its further management in a single or multi cloud computing environment. The framework had been used in Web Service Level Agreement (WSLA) for monitoring and enforcement of SLA using Service Oriented Architecture (SOA) environment. Cloud broker agents have the capability of automatic extraction of metrics from SLAs. The use of the third party support feature to manage the digital forensics in case of requirement of any violation of SLAs suggested in the present paper and it is also solving the trust issues as demonstrated in digital forensics usage from the initiation of SLA; making the SLA naturally forensic enabled.},
  keywords={},
  doi={10.1109/ICoAC.2017.8441460},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{6930517,
  author={Yaqub, Edwin and Yahyapour, Ramin and Wieder, Philipp and Kotsokalis, Constantinos and Lu, Kuan and Jehangiri, Ali Imran},
  booktitle={2014 IEEE International Conference on Services Computing}, 
  title={Optimal Negotiation of Service Level Agreements for Cloud-Based Services through Autonomous Agents}, 
  year={2014},
  volume={},
  number={},
  pages={59-66},
  abstract={Cloud-based services have become a cornerstone of today's IT. The self-service feature inherent in Cloud systems allows customers to play a greater role in service procurement. However, this restricts the value propositions and Service Level Agreements (SLAs) that Cloud providers offer because Quality of Service (QoS) and Non Functional Property (NFP) requirements vary from customer to customer. In feature-rich SLA templates, the contract space gets large, objectives are confidential and preferences over QoS and NFP often conflict between providers and customers. Hence, an SLA-gap exists between the two and contemporary providers bind their offerings to the inflexible take-it-or-leave-it SLAs. In this work, we address this problem by presenting a robust and computationally inexpensive negotiation strategy, using which agents can efficiently create near-optimal SLAs under time constraints. Experimental evaluations validate that our strategy performs at par with state of the art learning and non-learning strategies against a variety of metrics including utility, social welfare, social utility and the Pareto-optimal bids. This enables a dynamic SLA negotiation mechanism on top of our OpenShift (PaaS) based Cloud system designed using Service Oriented Cloud Computing Infrastructure (SOCCI) architecture. Negotiated procurement of services is shown to improve satisfaction of participants and reducing the SLA-gap.},
  keywords={},
  doi={10.1109/SCC.2014.17},
  ISSN={},
  month={June},}

@INPROCEEDINGS{10350465,
  author={Zhang, Yang and Li, Yang and Yang, Yilong and Chen, Shuang and Gao, Juntao and Wang, Weiru and Yin, Yongfeng},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={RapidMS: A Tool for Supporting Rapid Microservices Generation and Refinement from Requirements Model}, 
  year={2023},
  volume={},
  number={},
  pages={45-49},
  abstract={Microservices is a crucial architecture design pat-tern for developing cloud-native applications, which focuses on decomposing a large and complex software system into autonomous components that can be independently developed and deployed. However, microservices design is not a trivial task, which highly depends on the profound knowledge and experience of system design and target domain. This is a challenge for novice software architects. In this paper, we propose a microservices design tool named RapidMS, which only requires architects to specify potential context boundaries on the requirements model. The microservices architecture design model with component structure and interaction views can be automatically generated without extra human effort. Moreover, the proposed tool can automatically calculate the characteristic metrics of the microservices, which indicate the quality of the different aspects of models to support rapid architecture refinements. We demonstrate the tool's effectiveness through five case studies. The experimental result shows that architects can get better decomposition of requirement model within four iterations and over 90% of microservice architecture diagrams can be correctly generated within 10 seconds. RapidMS can be further extended and applied in the software industry to reduce the cost and difficulty of microservices decomposition and design. The tool can be downloaded at https://rm2pt.com/advs/ rapidms, and a demo video casting its features is at https://youtu.be/AoIM41FTnFO},
  keywords={},
  doi={10.1109/MODELS-C59198.2023.00017},
  ISSN={},
  month={Oct},}

@ARTICLE{9784409,
  author={Sebrechts, Merlijn and Volckaert, Bruno and De Turck, Filip and Yang, Kun and Al-Naday, Mays},
  journal={IEEE Communications Magazine}, 
  title={Fog Native Architecture: Intent-Based Workflows to Take Cloud Native toward the Edge}, 
  year={2022},
  volume={60},
  number={8},
  pages={44-50},
  abstract={The cloud native approach is rapidly transforming how applications are developed and operated, turning monolithic applications into microservice applications, allowing teams to release faster, increase reliability, and expedite operations by taking full advantage of cloud resources and their elasticity. At the same time, “fog computing” is emerging, bringing the cloud toward the edge, near the end user, in order to increase privacy, improve resource efficiency, and reduce latency. Combining these two trends, however, proves difficult because of four fundamental disconnects between the cloud native paradigm and fog computing. This article identifies these disconnects and proposes a fog native architecture along with a set of design patterns to take full advantage of the fog. Central to this approach is turning microservice applications into microservice workflows, constructed dynamically by the system using an intent-based approach taking into account a number of factors such as user requirements, request location, and available infrastructure and microservices. The architecture introduces a novel softwarized fog mesh facilitating both inter-microservice connectivity, external communication, and end-user aggregation. Our evaluation analyzes the impact of distributing microservice-based applications over a fog ecosystem, illustrating the impact of CPU and network latency and application metrics on perceived quality of service of fog native workflows compared to the cloud. The results show the fog can offer superior application performance given the right conditions.},
  keywords={},
  doi={10.1109/MCOM.003.2101075},
  ISSN={1558-1896},
  month={August},}

@ARTICLE{9057418,
  author={Yu, Guangba and Chen, Pengfei and Zheng, Zibin},
  journal={IEEE Transactions on Cloud Computing}, 
  title={Microscaler: Cost-Effective Scaling for Microservice Applications in the Cloud With an Online Learning Approach}, 
  year={2022},
  volume={10},
  number={2},
  pages={1100-1116},
  abstract={Recently, the microservice becomes a popular architecture to construct cloud native systems due to its agility. In cloud native systems, autoscaling is a key enabling technique to adapt to workload changes by acquiring or releasing the right amount of computing resources. However, it becomes a challenging problem in microservice applications, since such an application usually comprises a large number of different microservices with complex interactions. When the performance decreases due to an unpredictable workload peak, it is difficult to pinpoint the scaling-needed services which need to scale out and evaluate how many resources they need. In this article, we present a novel system named Microscaler to automatically identify the scaling-needed services and scale them to meet the Service Level Agreement (SLA) with an optimal cost for microservice applications. Microscaler first collects the quality of service (QoS) metrics in the service mesh enabled microservice infrastructure. Then, it determines under-provisioning or over-provisioning service instances along the service dependency graph with a novel scaling-needed service criterion named service power. The service dependency graph could be obtained by correlating each request flow in the service mesh. By combining an online learning approach and a step-by-step heuristic approach, Microscaler can precisely reach the optimal service scale meeting the SLA requirements. The experimental evaluations in a microservice benchmark show that Microscaler achieves an average 93 percent precision in scaling-needed service determination and converges to the optimal service scale faster than several state-of-the-art methods. Moreover, Microscaler is lightweight and flexible enough to work in a large-scale microservice system.},
  keywords={},
  doi={10.1109/TCC.2020.2985352},
  ISSN={2168-7161},
  month={April},}

@INPROCEEDINGS{10083635,
  author={S, Savitha and C, Sangana and K, Devendran and L, Pravin and M, Rajkumar and C, Nirmal},
  booktitle={2023 7th International Conference on Computing Methodologies and Communication (ICCMC)}, 
  title={Auto Scaling Infrastructure with Monitoring Tools using Linux Server on Cloud}, 
  year={2023},
  volume={},
  number={},
  pages={45-52},
  abstract={Cloud computing is the term that has gained widespread usage over these last few years. Due to the rapid increase in the use of information in the digital age of the 21st century, it is increasingly becoming a more attractive option for individuals and organizations to manage all their essential data, projects, and collaborations, rather than relying solely on in-house computers. The user's requirement for hardware and software is reduced via cloud computing. The interface software of cloud computing systems, typically as simple as a web browser, is the only thing the user must operate, and the Cloud network handles the rest. To decrease operational costs, both business and government organizations are adopting cloud computing, seeking a flexible and adaptable solution for the supply and delivery of their product services. Microservices and decoupled apps are becoming more popular. These container-based architectures make it easier to build sophisticated SaaS apps quickly, but managing and creating microservices can be a daunting task. Managing and creating microservices that involve a wide range of diverse functions, including handling and storing information, and performing predictive and prescriptive analysis, can be a challenging undertaking. Establishing auto scaling infrastructure on doud can be challenging due to several reasons, some of which are: understanding the application architecture, setting up monitoring, scaling policies, cost optimization and implementation complexity. Server farms include the tremendous and heterogeneous virtualized frameworks, which are continually extending and broadening after sometime are the essential starting point for registering specialized organizations. These solutions also need to be integrated into existing systems while adhering to Quality of Service (QoS) requirements. The principal objective of this work is to propose an on-premise design to leverage Kubernetes and Docker containers to improve the quality of service based on resource usage and Service Level Objectives (SLOs). The Prometheus Administrator set up is used to perform namespace checking. Normally, doud providers enable their own monitoring tools (like CloudWatch) for monitoring CPU, storage and network usage, service component, however these tools cannot monitor the service component. Additionally, the advancements have restricted the capacity to follow QoS highlights at the application level (like security and execution) since the main focus will be dedicated towards the equipment assets. These types of node-level monitoring make it difficult to scale requests and deploy pods to match the demand. Infrastructure monitoring should enable runtime changes to monitor the requirements or metric operationalization should be done on those criteria without modifying the underlying infrastructure.},
  keywords={},
  doi={10.1109/ICCMC56507.2023.10083635},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{9779689,
  author={Vale, Guilherme and Correia, Filipe Figueiredo and Guerra, Eduardo Martins and de Oliveira Rosa, Thatiane and Fritzsch, Jonas and Bogner, Justus},
  booktitle={2022 IEEE 19th International Conference on Software Architecture (ICSA)}, 
  title={Designing Microservice Systems Using Patterns: An Empirical Study on Quality Trade-Offs}, 
  year={2022},
  volume={},
  number={},
  pages={69-79},
  abstract={The promise of increased agility, autonomy, scalability, and reusability has made the microservices architecture a de facto standard for the development of large-scale and cloud-native commercial applications. Software patterns are an important design tool, and often they are selected and combined with the goal of obtaining a set of desired quality attributes. However, from a research standpoint, many patterns have not been widely validated against industry practice, making them not much more than interesting theories. To address this, we investigated how practitioners perceive the impact of 14 patterns on 7 quality attributes. Hence, we conducted 9 semi-structured interviews to collect industry expertise regarding (1) knowledge and adoption of software patterns, (2) the perceived architectural trade-offs of patterns, and (3) metrics professionals use to measure quality attributes. We found that many of the trade-offs reported in our study matched the documentation of each respective pattern, and identified several gains and pains which have not yet been reported, leading to novel insight about microservice patterns.},
  keywords={},
  doi={10.1109/ICSA53651.2022.00015},
  ISSN={},
  month={March},}

 @article{Monteiro_2020, title={Building orchestrated microservice systems using declarative business processes}, volume={14}, ISSN={1863-2394}, url={http://dx.doi.org/10.1007/s11761-020-00300-2}, DOI={10.1007/s11761-020-00300-2}, number={4}, journal={Service Oriented Computing and Applications}, publisher={Springer Science and Business Media LLC}, author={Monteiro, Davi and Maia, Paulo Henrique M. and Rocha, Lincoln S. and Mendonça, Nabor C.}, year={2020}, month=aug, pages={243–268} }

 @article{Alonso_2023, title={Understanding the challenges and novel architectural models of multi-cloud native applications – a systematic literature review}, volume={12}, ISSN={2192-113X}, url={http://dx.doi.org/10.1186/s13677-022-00367-6}, DOI={10.1186/s13677-022-00367-6}, number={1}, journal={Journal of Cloud Computing}, publisher={Springer Science and Business Media LLC}, author={Alonso, Juncal and Orue-Echevarria, Leire and Casola, Valentina and Torre, Ana Isabel and Huarte, Maider and Osaba, Eneko and Lobo, Jesus L.}, year={2023}, month=jan }

@Comment{jabref-meta: databaseType:bibtex;}
