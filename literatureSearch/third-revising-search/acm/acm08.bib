@inproceedings{10.1145/3026937.3026938,
author = {Alonso, Pedro and Catalan, Sandra and Herrero, Jos\'{e} R. and Quintana-Ort\'{\i}, Enrique S. and Rodr\'{\i}guez-S\'{a}nchez, Rafael},
title = {Reduction to Tridiagonal Form for Symmetric Eigenproblems on Asymmetric Multicore Processors},
year = {2017},
isbn = {9781450348836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3026937.3026938},
doi = {10.1145/3026937.3026938},
abstract = {Asymmetric multicore processors (AMPs), as those present in ARM big.LITTLE technology, have been proposed as a means to address the end of Dennard power scaling law. The idea of these architectures is to activate only the type (and number) of cores that satisfy the quality of service requested by the application(s) in execution while delivering high energy efficiency.For dense linear algebra problems though, performance is of paramount importance, asking for an efficient use of all computational resources in the AMP. In response to this, we investigate how to exploit the asymmetric cores of an ARMv7 big.LITTLE AMP in order to attain high performance for the reduction to tridiagonal form, an essential step towards the solution of dense symmetric eigenvalue problems. The routine for this purpose in LAPACK is especially challenging, since half of its floating-point arithmetic operations (flops) are cast in terms of compute-bound kernels while the remaining half correspond to memory-bound kernels. To deal with this scenario: 1) we leverage a tuned implementation of the compute-bound kernels for AMPs; 2) we develop and parallelize new architecture-aware micro-kernels for the memory-bound kernels; 3) and we carefully adjust the type and number of cores to use at each step of the reduction procedure.},
booktitle = {Proceedings of the 8th International Workshop on Programming Models and Applications for Multicores and Manycores},
pages = {39–47},
numpages = {9},
keywords = {symmetric eigenvalue problem, workload balancing, Condensed forms, asymmetric multicore processors, basic linear algebra subprograms (BLAS), multi-threading},
location = {Austin, TX, USA},
series = {PMAM'17}
}

@article{10.1145/3191737,
author = {Classen, Jiska and Wegemer, Daniel and Patras, Paul and Spink, Tom and Hollick, Matthias},
title = {Anatomy of a Vulnerable Fitness Tracking System: Dissecting the Fitbit Cloud, App, and Firmware},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191737},
doi = {10.1145/3191737},
abstract = {Fitbit fitness trackers record sensitive personal information, including daily step counts, heart rate profiles, and locations visited. By design, these devices gather and upload activity data to a cloud service, which provides aggregate statistics to mobile app users. The same principles govern numerous other Internet-of-Things (IoT) services that target different applications. As a market leader, Fitbit has developed perhaps the most secure wearables architecture that guards communication with end-to-end encryption. In this article, we analyze the complete Fitbit ecosystem and, despite the brand's continuous efforts to harden its products, we demonstrate a series of vulnerabilities with potentially severe implications to user privacy and device security. We employ a range of techniques, such as protocol analysis, software decompiling, and both static and dynamic embedded code analysis, to reverse engineer previously undocumented communication semantics, the official smartphone app, and the tracker firmware. Through this interplay and in-depth analysis, we reveal how attackers can exploit the Fitbit protocol to extract private information from victims without leaving a trace, and wirelessly flash malware without user consent. We demonstrate that users can tamper with both the app and firmware to selfishly manipulate records or circumvent Fitbit's walled garden business model, making the case for an independent, user-controlled, and more secure ecosystem. Finally, based on the insights gained, we make specific design recommendations that can not only mitigate the identified vulnerabilities, but are also broadly applicable to securing future wearable system architectures.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {5},
numpages = {24},
keywords = {Wearables, firmware reverse engineering, Nexmon, health}
}

@inproceedings{10.1145/3094405.3094411,
author = {Avino, G. and Malinverno, M. and Malandrino, F. and Casetti, C. and Chiasserini, C. F.},
title = {Characterizing Docker Overhead in Mobile Edge Computing Scenarios},
year = {2017},
isbn = {9781450350587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094405.3094411},
doi = {10.1145/3094405.3094411},
abstract = {Mobile Edge Computing (MEC) is an emerging network paradigm that provides cloud and IT services at the point of access of the network. Such proximity to the end user translates into ultra-low latency and high bandwidth, while, at the same time, it alleviates traffic congestion in the network core. Due to the need to run servers on edge nodes (e.g., an LTE-A macro eNodeB), a key element of MEC architectures is to ensure server portability and low overhead. A possible tool that can be used for this purpose is Docker, a framework that allows easy, fast deployment of Linux containers. This paper addresses the suitability of Docker in MEC scenarios by quantifying the CPU consumed by Docker when running two different containerized services: multiplayer gaming and video streaming. Our tests, run with varying numbers of clients and servers, yield different results for the two case studies: for the gaming service, the overhead logged by Docker increases only with the number of servers; conversely, for the video streaming case, the overhead is not affected by the number of either clients or servers.},
booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
pages = {30–35},
numpages = {6},
keywords = {Mobile Edge Computing, 5G networks, Containers, Docker},
location = {Los Angeles, CA, USA},
series = {HotConNet '17}
}

@inproceedings{10.5555/3192424.3192553,
author = {Giovanetti, Romain and Lancieri, Luigi},
title = {Model of Computer Architecture for Online Social Networks Flexible Data Analysis: The Case of Twitter Data},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {Since several years, there is an increasing interest for new services based on the analysis of data coming from online social networks. Such services can, for example, provide the e-reputation of a product or a company, detect new trends in a commercial, social or political context, etc. The huge quantity of data is an opportunity in term of representativeness but is also difficult to manage. Within Twitter, for example, it appears that the huge stream of data is, most of the time, incompatible with a flexible analysis unless to have high computer resources. The only practical solution is often to observe in a static way a limited portion of a phenomenon in a limited time slot. This paper is devoted to the study of necessary conditions to provide an equilibrium between the computer architecture complexity and the analysis flexibility.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {677–684},
numpages = {8},
keywords = {computer architecture, platform, distributed database, flexible data analysis, online social network, Twitter},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1145/3387514.3405881,
author = {Schomp, Kyle and Bhardwaj, Onkar and Kurdoglu, Eymen and Muhaimen, Mashooq and Sitaraman, Ramesh K.},
title = {Akamai DNS: Providing Authoritative Answers to the World's Queries},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405881},
doi = {10.1145/3387514.3405881},
abstract = {We present Akamai DNS, one of the largest authoritative DNS infrastructures in the world, that supports the Akamai content delivery network (CDN) as well as authoritative DNS hosting and DNS-based load balancing services for many enterprises. As the starting point for a significant fraction of the world's Internet interactions, Akamai DNS serves millions of queries each second and must be resilient to avoid disrupting myriad online services, scalable to meet the ever increasing volume of DNS queries, performant to prevent user-perceivable performance degradation, and reconfigurable to react quickly to shifts in network conditions and attacks. We outline the design principles and architecture used to achieve Akamai DNS's goals, relating the design choices to the system workload and quantifying the effectiveness of those designs. Further, we convey insights from operating the production system that are of value to the broader research community.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {465–478},
numpages = {14},
keywords = {DNS, Distributed Systems},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3313294.3313384,
author = {Di Stefano, Alessandro and Scat\`{a}, Marialisa and La Corte, Aurelio and Das, Sajal K. and Li\`{o}, Pietro},
title = {Improving QoE in Multi-Layer Social Sensing: A Cognitive Architecture and Game Theoretic Model},
year = {2019},
isbn = {9781450367066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313294.3313384},
doi = {10.1145/3313294.3313384},
abstract = {This paper proposes a novel cognitive architecture and game-theoretic model for resource sharing among netizens, thus improving their quality of experience (QoE) in multi-layer social sensing environments. The underlying approach is to quantify micro-rewards and inequalities derived from social multi-layer interactions. Specifically, we model our society as a social multi-layer network of individuals or groups of individuals (nodes), where the layers represent multiple channels of interactions (on various services). The weighted edges correspond to the multiple social relationships between nodes participating in different services, reflecting the importance assigned to each of these edges and are defined based on the concepts of awareness and homophily. Heterogeneity, both interactions-wise on the multiple layers and related to homophily between individuals, on each node and layer of a weighted multiplex network produces a complex multi-scale interplay between nodes in the multi-layer structure. Applying game theory, we quantify the impact of heterogeneity on the evolutionary dynamics of social sensing through a data driven approach based on the propagation of individual-level micro-affirmations and micro-inequalities. The micro-packets of energy continuously exchanged between nodes may impact positively or negatively on their social behaviors, producing peaks of extreme dissatisfaction and in some cases a form of distress. Quantifying the evolutionary dynamics of human behaviors enables the detection of such peaks in the population and enable us design a targeted control mechanism, where social rewards and self-healing help improve the QoE of the netizens.},
booktitle = {Proceedings of the Fourth International Workshop on Social Sensing},
pages = {18–23},
numpages = {6},
keywords = {game theory, social sensing, Cognitive architecture, IoP, QoE, multi-layer networks},
location = {Montreal, QC, Canada},
series = {SocialSense'19}
}

@article{10.14778/3476249.3476287,
author = {Chiosa, Monica and Preu\ss{}er, Thomas B. and Alonso, Gustavo},
title = {SKT: A One-Pass Multi-Sketch Data Analytics Accelerator},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476287},
doi = {10.14778/3476249.3476287},
abstract = {Data analysts often need to characterize a data stream as a first step to its further processing. Some of the initial insights to be gained include, e.g., the cardinality of the data set and its frequency distribution. Such information is typically extracted by using sketch algorithms, now widely employed to process very large data sets in manageable space and in a single pass over the data. Often, analysts need more than one parameter to characterize the stream. However, computing multiple sketches becomes expensive even when using high-end CPUs. Exploiting the increasing adoption of hardware accelerators, this paper proposes SKT, an FPGA-based accelerator that can compute several sketches along with basic statistics (average, max, min, etc.) in a single pass over the data. SKT has been designed to characterize a data set by calculating its cardinality, its second frequency moment, and its frequency distribution. The design processes data streams coming either from PCIe or TCP/IP, and it is built to fit emerging cloud service architectures, such as Microsoft's Catapult or Amazon's AQUA. The paper explores the trade-offs of designing sketch algorithms on a spatial architecture and how to combine several sketch algorithms into a single design. The empirical evaluation shows how SKT on an FPGA offers a significant performance gain over high-end, server-class CPUs.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2369–2382},
numpages = {14}
}

@inproceedings{10.1145/2896387.2896447,
author = {Mansour, Ibrahim and Sahandi, Reza and Cooper, Kendra and Warman, Adrian},
title = {Interoperability in the Heterogeneous Cloud Environment: A Survey of Recent User-Centric Approaches},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2896447},
doi = {10.1145/2896387.2896447},
abstract = {Cloud computing provides users the ability to access shared, online computing resources. However, providers often offer their own proprietary applications, interfaces, APIs and infrastructures, resulting in a heterogeneous cloud environment. This heterogeneous environment makes it difficult for users to change cloud service providers; exploring capabilities to support the automated migration from one provider to another is an active, open research area. Many standards bodies (IEEE, NIST, DMTF and SNIA), industry (middleware) and academia have been pursuing approaches to reduce the impact of vendor lock-in by investigating the cloud migration problem at the level of the VM. However, the migration downtime, decoupling VM from underlying systems and security of live channels remain open issues. This paper focuses on analysing recently proposed live, cloud migration approaches for VMs at the infrastructure level in the cloud architecture. The analysis reveals issues with flexibility, performance, and security of the approaches, including additional loads to the CPU and disk I/O drivers of the physical machine where the VM initially resides. The next steps of this research are to develop and evaluate a new approach LibZam (Libya Zamzem) that will work towards addressing the identified limitations.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {62},
numpages = {7},
keywords = {Cloud Architecture, Software Defined Network, Cloud Computing, Cloud Migration, Network Function Virtualization, Cloud Interoperability, VM Live Migration, Cloud Infrastructure},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@article{10.5555/2910557.2910562,
author = {Formiga, Llu\'{\i}s and Barr\'{o}n-Cede\~{n}o, Alberto and M\`{a}rquez, Llu\'{\i}s and Henr\'{\i}quez, Carlos A. and Mari\~{n}o, Jos\'{e} B.},
title = {Leveraging Online User Feedback to Improve Statistical Machine Translation},
year = {2015},
issue_date = {September 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {54},
number = {1},
issn = {1076-9757},
abstract = {In this article we present a three-step methodology for dynamically improving a statistical machine translation (SMT) system by incorporating human feedback in the form of free edits on the system translations. We target at feedback provided by casual users, which is typically error-prone. Thus, we first propose a filtering step to automatically identify the better user-edited translations and discard the useless ones. A second step produces a pivot-based alignment between source and user-edited sentences, focusing on the errors made by the system. Finally, a third step produces a new translation model and combines it linearly with the one from the original system. We perform a thorough evaluation on a real-world dataset collected from the Reverso.net translation service and show that every step in our methodology contributes significantly to improve a general purpose SMT system. Interestingly, the quality improvement is not only due to the increase of lexical coverage, but to a better lexical selection, reordering, and morphology. Finally, we show the robustness of the methodology by applying it to a different scenario, in which the new examples come from an automatically Web-crawled parallel corpus. Using exactly the same architecture and models provides again a significant improvement of the translation quality of a general purpose baseline SMT system.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {159–192},
numpages = {34}
}

@inproceedings{10.1145/3338103.3338106,
author = {Landwehr, Marvin and Borning, Alan and Wulf, Volker},
title = {The High Cost of Free Services: Problems with Surveillance Capitalism and Possible Alternatives for IT Infrastructure},
year = {2019},
isbn = {9781450372817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338103.3338106},
doi = {10.1145/3338103.3338106},
abstract = {A large portion of the software side of our information technology infrastructure, including web search, email, social media, transportation information, and much more, is provided "free" to the end users, although the corporations that provide this are often enormously profitable. The business model involves customized advertising and behavior manipulation, powered by intensive gathering and cross-correlation of personal information. Significant other parts of our IT infrastructure use fees-for-service but still involve intensive information gathering and behavior manipulation. There are significant indirect costs of these business models, including loss of privacy, supporting surveillance by both corporations and the state, automated manipulations of behavior, undermining the democratic process, and consumerism with its attendant environmental costs. In a recent book, Shoshana Zuboff terms this "surveillance capitalism." Our primary focus in this essay is how we could develop new models for providing these services. We describe some intermediate steps toward those models: education, regulation, and resistance. Following that, we discuss a partial solution, involving for-profit corporations that provide these services without tracking personal information. Finally, we describe desired characteristics for more comprehensive solutions, and outline a range of such solutions for different portions of the IT infrastructure that more truly return control to the end users. A common feature of several is the use of highly decentralized storage of information (either on the end user's own personal devices or on small servers), a modular architecture and interface to allow for customization of what information is to be shared, and a distributed ledger mechanism for authentication.},
booktitle = {Proceedings of the Fifth Workshop on Computing within Limits},
articleno = {3},
numpages = {10},
keywords = {economics, advertising, surveillance capitalism, political manipulation, manipulation of behavior, digital infrastructure, IT business models},
location = {Lappeenranta, Finland},
series = {LIMITS '19}
}

@inproceedings{10.1145/3079856.3080210,
author = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
title = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080210},
doi = {10.1145/3079856.3080210},
abstract = {With increasing deployment of virtual machines for cloud services and server applications, memory address translation overheads in virtualized environments have received great attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss necessitates up to 24 memory references for one guest to host translation. While dedicated page walk caches and such recent enhancements eliminate many of these memory references, our measurements on the Intel Skylake processors indicate that many programs in virtualized mode of execution still spend hundreds of cycles for translations that do not hit in the TLBs.This paper presents an innovative scheme to reduce the cost of address translations by using a very large Translation Lookaside Buffer that is part of memory, the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses required in commonly used 2D walks with radix-4 type of page tables. Even if many of the 24 accesses may hit in the page walk caches, the aggregated cost of the many hits plus the overhead of occasional misses from page walk caches still exceeds the cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space, TLB entries (as opposed to multiple page table entries) can be cached in large L2 and L3 data caches, yielding significant benefits. Through detailed evaluation running SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves performance by approximately 10\% on average. The improvement is more than 16\% for 5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate nearly all TLB misses in 8-core systems.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {469–480},
numpages = {12},
keywords = {Very Large TLB, Address Translation, Die-Stacked DRAM, Virtualization},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1145/3140659.3080210,
author = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
title = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080210},
doi = {10.1145/3140659.3080210},
abstract = {With increasing deployment of virtual machines for cloud services and server applications, memory address translation overheads in virtualized environments have received great attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss necessitates up to 24 memory references for one guest to host translation. While dedicated page walk caches and such recent enhancements eliminate many of these memory references, our measurements on the Intel Skylake processors indicate that many programs in virtualized mode of execution still spend hundreds of cycles for translations that do not hit in the TLBs.This paper presents an innovative scheme to reduce the cost of address translations by using a very large Translation Lookaside Buffer that is part of memory, the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses required in commonly used 2D walks with radix-4 type of page tables. Even if many of the 24 accesses may hit in the page walk caches, the aggregated cost of the many hits plus the overhead of occasional misses from page walk caches still exceeds the cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space, TLB entries (as opposed to multiple page table entries) can be cached in large L2 and L3 data caches, yielding significant benefits. Through detailed evaluation running SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves performance by approximately 10\% on average. The improvement is more than 16\% for 5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate nearly all TLB misses in 8-core systems.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {469–480},
numpages = {12},
keywords = {Die-Stacked DRAM, Very Large TLB, Address Translation, Virtualization}
}

@inproceedings{10.1145/3472727.3472798,
author = {Zhang, Zhi-Li and Dayalan, Udhaya Kumar and Ramadan, Eman and Salo, Timothy J.},
title = {Towards a Software-Defined, Fine-Grained QoS Framework for 5G and Beyond Networks},
year = {2021},
isbn = {9781450386333},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472727.3472798},
doi = {10.1145/3472727.3472798},
abstract = {5G offers a slew of new features and capabilities to support a whole gamut of new applications. On the other hand, 5G new radio (NR), especially, high-band mmWave radio, also poses new challenges, as shown by recent measurement studies of commercial 5G services. In order to effectively support new classes of application such as extra low-latency and/or high-bandwidth applications, we argue that truly cross-layer network-application integration that exposes application semantics to enable 5G and beyond 5G (B5G) networks to make intelligent decisions, e.g., for dynamic radio resource allocation, is needed. Unfortunately the existing 5G flow-based framework is inadequate to support such cross-layer integration. We therefore advocate a software-defined, fine-grained QoS framework. We use ultra-high resolution (UHR) volumetric video streaming as a use case and conduct very preliminary experiments to demonstrate the potential benefits of the proposed framework. This position paper serves as a strawman to call for new intelligent architectural designs for B5G networks and next-generation wireless systems.},
booktitle = {Proceedings of the ACM SIGCOMM 2021 Workshop on Network-Application Integration},
pages = {7–13},
numpages = {7},
keywords = {software -defined, application semantics, fine-grained, 5G and beyond, QoS framework},
location = {Virtual Event, USA},
series = {NAI'21}
}

@inproceedings{10.1145/2876019.2876023,
author = {Pan, Xiang and Yegneswaran, Vinod and Chen, Yan and Porras, Phillip and Shin, Seungwon},
title = {HogMap: Using SDNs to Incentivize Collaborative Security Monitoring},
year = {2016},
isbn = {9781450340786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876019.2876023},
doi = {10.1145/2876019.2876023},
abstract = {Cyber Threat Intelligence (CTI) sharing facilitates a comprehensive understanding of adversary activity and enables enterprise networks to prioritize their cyber defense technologies. To that end, we introduce HogMap, a novel software-defined infrastructure that simplifies and incentivizes collaborative measurement and monitoring of cyber-threat activity. HogMap proposes to transform the cyber-threat monitoring landscape by integrating several novel SDN-enabled capabilities: (i) intelligent in-place filtering of malicious traffic, (ii) dynamic migration of interesting and extraordinary traffic and (iii) a software-defined marketplace where various parties can opportunistically subscribe to and publish cyber-threat intelligence services in a flexible manner.We present the architectural vision and summarize our preliminary experience in developing and operating an SDN-based HoneyGrid, which spans three enterprises and implements several of the enabling capabilities (e.g., traffic filtering, traffic forwarding and connection migration). We find that SDN technologies greatly simplify the design and deployment of such globally distributed and elastic HoneyGrids.},
booktitle = {Proceedings of the 2016 ACM International Workshop on Security in Software Defined Networks \&amp; Network Function Virtualization},
pages = {7–12},
numpages = {6},
keywords = {cyber threat intelligence, honeynet, honeygrid, sdn, marketplace},
location = {New Orleans, Louisiana, USA},
series = {SDN-NFV Security '16}
}

@inproceedings{10.1145/3109859.3109861,
author = {Jaradat, Shatha},
title = {Deep Cross-Domain Fashion Recommendation},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109861},
doi = {10.1145/3109859.3109861},
abstract = {With the increasing number of online shopping services, the number of users and the quantity of visual and textual information on the Internet, there is a pressing need for intelligent recommendation systems that analyze the user's behavior amongst multiple domains and help them to find the desirable information without the burden of search. However, there is little research that has been done on complex recommendation scenarios that involve knowledge transfer across multiple domains. This problem is especially challenging when the involved data sources are complex in terms of the limitations on the quantity and quality of data that can be crawled. The goal of this paper is studying the connection between visual and textual inputs for better analysis of a certain domain, and to examine the possibility of knowledge transfer from complex domains for the purpose of efficient recommendations. The methods employed to achieve this study include both design of architecture and algorithms using deep learning technologies to analyze the effect of deep pixel-wise semantic segmentation and text integration on the quality of recommendations. We plan to develop a practical testing environment in a fashion domain.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {407–410},
numpages = {4},
keywords = {cnn, transfer learning, deep learning, fashion recommendation, domain adaptation, cross-domain knowledge transfer},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/3404868.3406662,
author = {Edeline, Korian and Donnet, Benoit},
title = {Evaluating the Impact of Path Brokenness on TCP Options},
year = {2020},
isbn = {9781450380393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404868.3406662},
doi = {10.1145/3404868.3406662},
abstract = {In-path network functions enforcing policies like firewalls, IDSes, NATs, and TCP enhancing proxies are ubiquitous. They are deployed in various types of networks and bring obvious value to the Internet.Unfortunately, they also break important architectural principles and, consequently, make the Internet less flexible by preventing the use of advanced protocols, features, or options. In some scenarios, feature-disabling middlebox policies can lead to a performance shortfall. Moreover, middleboxes are also prone to enforce policies that disrupt transport control mechanisms, which can also have direct consequences in term of Quality-of-Service (QoS).In this paper, we investigate the impact of the most prevalent in-path impairments on the TCP protocol and its features. Using network experiments in a controlled environment, we quantify the QoS decreases and shortfall induced by feature-breaking middleboxes, and show that even in the presence of a fallback mechanism, TCP QoS remains affected.},
booktitle = {Proceedings of the Applied Networking Research Workshop},
pages = {38–44},
numpages = {7},
location = {Virtual Event, Spain},
series = {ANRW '20}
}

@inproceedings{10.1145/3038450.3038452,
author = {Michalakis, Konstantinos and Aliprantis, John and Caridakis, George},
title = {Intelligent Visual Interface with the Internet of Things},
year = {2017},
isbn = {9781450349024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3038450.3038452},
doi = {10.1145/3038450.3038452},
abstract = {Communication between users and physical objects and sensors through the web within the Internet of Things framework, requires by definition the capability to perceive the sensors and the underlying information and services. Visualization of the Things in IoT is thus a requirement for natural interaction between users and IoT instances in the upcoming but steadily established computing paradigm. The immense quantity of sensors and variety of usable information introduces the need to intelligently filter and adapt the respective information sources and layers. Current work proposes an architecture that supports intelligent interaction between users and the IoT addressing the intelligent perception requirement described earlier. On the one hand, sensory visualization is tackled via Augmented Reality layers of sensors and information and on the other hand context and location awareness enhance the system by providing usable in the respective senses information.},
booktitle = {Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects},
pages = {27–30},
numpages = {4},
keywords = {context awareness, markerless tracking, natural interaction, augmented reality, internet of things},
location = {Limassol, Cyprus},
series = {SmartObject '17}
}

@article{10.1145/2699697,
author = {Yue, Tao and Briand, Lionel C. and Labiche, Yvan},
title = {AToucan: An Automated Framework to Derive UML Analysis Models from Use Case Models},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2699697},
doi = {10.1145/2699697},
abstract = {The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model comprising class, sequence and activity diagrams from a use case model and to automatically establish traceability links between model elements of the use case model and the generated analysis model. Note that our goal is to save effort through automated support, not to replace human abstraction and decision making.Seven (six) case studies were performed to compare class (sequence) diagrams generated by aToucan to the ones created by experts, Masters students, and trained, fourth-year undergraduate students. Results show that aToucan performs well regarding consistency (e.g., 88\% class diagram consistency) and completeness (e.g., 80\% class completeness) when comparing generated class diagrams with reference class diagrams created by experts and Masters students. Similarly, sequence diagrams automatically generated by aToucan are highly consistent with the ones devised by experts and are also rather complete, for instance, 91\% and 97\% message consistency and completeness, respectively. Further, statistical tests show that aToucan significantly outperforms fourth-year engineering students in this respect, thus demonstrating the value of automation. We also conducted two industrial case studies demonstrating the applicability of aToucan in two different industrial domains. Results showed that the vast majority of model elements generated by aToucan are correct and that therefore, in practice, such models would be good initial models to refine and augment so as to converge towards to correct and complete analysis models. A performance analysis shows that the execution time of aToucan (when generating class and sequence diagrams) is dependent on the number of simple sentences contained in the use case model and remains within a range of a few minutes. Five different software system descriptions (18 use cases altogether) were performed to evaluate the generation of activity diagrams. Results show that aToucan can generate 100\% complete and correct control flow information of activity diagrams and on average 85\% data flAow information completeness. Moreover, we show that aToucan outperforms three commercial tools in terms of activity diagram generation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {13},
numpages = {52},
keywords = {sequence diagram, traceability, analysis model, automation, Use case modeling, class diagram, activity diagram, transformation, UML}
}

@inproceedings{10.1145/3465481.3470030,
author = {Espinha Gasiba, Tiago and Andrei-Cristian, Iosif and Lechner, Ulrike and Pinto-Albuquerque, Maria},
title = {Raising Security Awareness of Cloud Deployments Using Infrastructure as Code through CyberSecurity Challenges},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470030},
doi = {10.1145/3465481.3470030},
abstract = {Improper deployment of software can have serious consequences, ranging from simple downtime to permanent data loss and data breaches. Infrastructure as Code tools serve to streamline delivery by promising consistency and speed, by abstracting away from the underlying actions. However, this simplicity may distract from architectural or configuration faults, potentially compromising the secure development lifecycle. One way to address this issue involves awareness training. Sifu is a platform that provides education on security through serious games, developed in the industry, for the industry. The presented work extends the Sifu platform with challenges addressing Terraform-aided cloud deployment on Amazon Web Services. This paper proposes an evaluation pipeline behind the challenges, and provides details of the vulnerability detection and feedback mechanisms, as well as a novel technique for detecting undesired differences between a given architecture and a target result. Furthermore, this paper quantifies the challenges’ perceived usefulness and impact, by evaluating the challenges among a total of twelve participants. Our preliminary results show that the challenges are suitable for education and the industry, with potential usage in internal training. A key finding is that, although the participants understand the importance of secure coding, their answers indicate that universities leave them unprepared in this area. Finally, our results are compared with related industry works, to extract and provide good practices and advice for practitioners.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {63},
numpages = {8},
keywords = {Serious Games, Secure Coding, Infrastructure as Code, Training, Awareness, DevSecOps, CyberSecurity Challenges},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1109/MICRO.2014.53,
author = {Zhang, Yunqi and Laurenzano, Michael A. and Mars, Jason and Tang, Lingjia},
title = {SMiTe: Precise QoS Prediction on Real-System SMT Processors to Improve Utilization in Warehouse Scale Computers},
year = {2014},
isbn = {9781479969982},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2014.53},
doi = {10.1109/MICRO.2014.53},
abstract = {One of the key challenges for improving efficiency in warehouse scale computers (WSCs) is to improve server utilization while guaranteeing the quality of service (QoS) of latency-sensitive applications. To this end, prior work has proposed techniques to precisely predict performance and QoS interference to identify 'safe' application co-locations. However, such techniques are only applicable to resources shared across cores. Achieving such precise interference prediction on real-system simultaneous multithreading (SMT) architectures has been a significantly challenging open problem due to the complexity introduced by sharing resources within a core.In this paper, we demonstrate through a real-system investigation that the fundamental difference between resource sharing behaviors on CMP and SMT architectures calls for a redesign of the way we model interference. For SMT servers, the interference on different shared resources, including private caches, memory ports, as well as integer and floating-point functional units, do not correlate with each other. This insight suggests the necessity of decoupling interference into multiple resource sharing dimensions. In this work, we propose SMiTe, a methodology that enables precise performance prediction for SMT co-location on real-system commodity processors. With a set of Rulers, which are carefully designed software stressors that apply pressure to a multidimensional space of shared resources, we quantify application sensitivity and contentiousness in a decoupled manner. We then establish a regression model to combine the sensitivity and contentiousness in different dimensions to predict performance interference. Using this methodology, we are able to precisely predict the performance interference in SMT co-location with an average error of 2.80\% on SPEC CPU2006 and 1.79\% on Cloud Suite. Our evaluation shows that SMiTe allows us to improve the utilization of WSCs by up to 42.57\% while enforcing an application's QoS requirements.},
booktitle = {Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {406–418},
numpages = {13},
keywords = {quality of service, simultaneous multithreading, warehouse scale computer, datacenter},
location = {Cambridge, United Kingdom},
series = {MICRO-47}
}

@inproceedings{10.1145/3075564.3076259,
author = {Llewellynn, Tim and Fern\'{a}ndez-Carrobles, M. Milagro and Deniz, Oscar and Fricker, Samuel and Storkey, Amos and Pazos, Nuria and Velikic, Gordana and Leufgen, Kirsten and Dahyot, Rozenn and Koller, Sebastian and Goumas, Georgios and Leitner, Peter and Dasika, Ganesh and Wang, Lei and Tutschku, Kurt},
title = {BONSEYES: Platform for Open Development of Systems of Artificial Intelligence: Invited Paper},
year = {2017},
isbn = {9781450344876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3075564.3076259},
doi = {10.1145/3075564.3076259},
abstract = {The Bonseyes EU H2020 collaborative project aims to develop a platform consisting of a Data Marketplace, a Deep Learning Toolbox, and Developer Reference Platforms for organizations wanting to adopt Artificial Intelligence. The project will be focused on using artificial intelligence in low power Internet of Things (IoT) devices ("edge computing"), embedded computing systems, and data center servers ("cloud computing"). It will bring about orders of magnitude improvements in efficiency, performance, reliability, security, and productivity in the design and programming of systems of artificial intelligence that incorporate Smart Cyber-Physical Systems (CPS). In addition, it will solve a causality problem for organizations who lack access to Data and Models. Its open software architecture will facilitate adoption of the whole concept on a wider scale. To evaluate the effectiveness, technical feasibility, and to quantify the real-world improvements in efficiency, security, performance, effort and cost of adding AI to products and services using the Bonseyes platform, four complementary demonstrators will be built. Bonseyes platform capabilities are aimed at being aligned with the European FI-PPP activities and take advantage of its flagship project FIWARE. This paper provides a description of the project motivation, goals and preliminary work.},
booktitle = {Proceedings of the Computing Frontiers Conference},
pages = {299–304},
numpages = {6},
keywords = {Internet of things, Deep Learning, Smart Cyber-Physical Systems, Data marketplace},
location = {Siena, Italy},
series = {CF'17}
}

@inproceedings{10.1145/2744769.2744912,
author = {Bokhari, Haseeb and Javaid, Haris and Shafique, Muhammad and Henkel, J\"{o}rg and Parameswaran, Sri},
title = {SuperNet: Multimode Interconnect Architecture for Manycore Chips},
year = {2015},
isbn = {9781450335201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2744769.2744912},
doi = {10.1145/2744769.2744912},
abstract = {Designers of the on-chip interconnect for manycore chips are faced with the dilemma of meeting performance, power and reliability requirements for different operational scenarios. In this paper, we propose a multimode on-chip interconnect called SuperNet. This interconnect can be configured to run in three different modes: energy efficient mode; performance mode; and, reliability mode. Our proposed interconnect is based on two parallel multi-vt optimized packet switched network-on-chip (NoC) meshes. We describe the circuit design techniques and architectural modifications required to realize such a multimode interconnect. Our evaluation with diverse set of applications show that the energy efficient mode can save on average 40\% NoC power, whereas the performance mode can improve the core IPC by up to 13\% on selected high MPKI applications. The reliability mode provides protection against soft errors in the router's data path through byte oriented SECDED codes that can correct up to 8 bit errors and detect up to 16 bit errors in a 64 bit flit, whereas the router's control path is protected through DMR lock step execution.},
booktitle = {Proceedings of the 52nd Annual Design Automation Conference},
articleno = {85},
numpages = {6},
keywords = {power optimization, fault tolerance, performance, network-on-chip, multimode},
location = {San Francisco, California},
series = {DAC '15}
}

@inproceedings{10.1145/3123939.3123983,
author = {Fang, Yuanwei and Zou, Chen and Elmore, Aaron J. and Chien, Andrew A.},
title = {UDP: A Programmable Accelerator for Extract-Transform-Load Workloads and More},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3123983},
doi = {10.1145/3123939.3123983},
abstract = {Big data analytic applications give rise to large-scale extract-transform-load (ETL) as a fundamental step to transform new data into a native representation. ETL workloads pose significant performance challenges on conventional architectures, so we propose the design of the unstructured data processor (UDP), a software programmable accelerator that includes multi-way dispatch, variable-size symbol support, Flexible-source dispatch (stream buffer and scalar registers), and memory addressing to accelerate ETL kernels both for current and novel future encoding and compression. Specifically, UDP excels at branch-intensive and symbol and pattern-oriented workloads, and can offload them from CPUs.To evaluate UDP, we use a broad set of data processing workloads inspired by ETL, but broad enough to also apply to query execution, stream processing, and intrusion detection/monitoring. A single UDP accelerates these data processing tasks 20-fold (geometric mean, largest increase from 0.4 GB/s to 40 GB/s) and performance per watt by a geomean of 1,900-fold. UDP ASIC implementation in 28nm CMOS shows UDP logic area of 3.82mm2 (8.69mm2 with 1MB local memory), and logic power of 0.149W (0.864W with 1MB local memory); both much smaller than a single core.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {55–68},
numpages = {14},
keywords = {control-flow accelerator, data analytics, parsing, data encoding and transformation, compression},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings{10.1145/3023956.3023958,
author = {Lachmann, Remo and Beddig, Simon and Lity, Sascha and Schulze, Sandro and Schaefer, Ina},
title = {Risk-Based Integration Testing of Software Product Lines},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023958},
doi = {10.1145/3023956.3023958},
abstract = {Software product lines (SPL) capture commonalities and variabilities of product families and, thus, enable mass customization of product variants according to customers desired configurations. However, they introduce new challenges to software testing due to a potentially large number of variants. While each variant should be tested, testing resources are limited and, thus, a retest of all, partially redundant, test cases for each variant is not feasible in SPL testing. Coping with these issues has been a major research focus in recent years, leading to different testing approaches. However, risk-based testing has not gained much attention in the SPL domain while being a successful approach for single-software systems. In this paper, we propose a novel risk-based testing approach for SPL integration testing. We incrementally test SPLs by stepping from one variant to the next. For each variant, we automatically compute failure probabilities and failure impacts for its architectural components. To avoid a computational overhead of generating and analyzing each variant, we exploit the variability between variants defined as deltas to focus on important changes. We evaluate our approach using an automotive case study, showing that the risk-based technique leads to positive results compared to random and delta-oriented testing.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {52–59},
numpages = {8},
keywords = {model-based testing, risk-based testing, software product lines, test case prioritization},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1145/3358696,
author = {Tang, Yibin and Wang, Ying and Li, Huawei and Li, Xiaowei},
title = {MV-Net: Toward Real-Time Deep Learning on Mobile GPGPU Systems},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3358696},
doi = {10.1145/3358696},
abstract = {Recently the development of deep learning has been propelling the sheer growth of vision and speech applications on lightweight embedded and mobile systems. However, the limitation of computation resource and power delivery capability in embedded platforms is recognized as a significant bottleneck that prevents the systems from providing real-time deep learning ability, since the inference of deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs) involves large quantities of weights and operations. Particularly, how to provide quality-of-services (QoS)-guaranteed neural network inference ability in the multitask execution environment of multicore SoCs is even more complicated due to the existence of resource contention. In this article, we present a novel deep neural network architecture, MV-Net, which provides performance elasticity and contention-aware self-scheduling ability for QoS enhancement in mobile computing systems. When the constraints of QoS, output accuracy, and resource contention status of the system change, MV-Net can dynamically reconfigure the corresponding neural network propagation paths and thus achieves an effective tradeoff between neural network computational complexity and prediction accuracy via approximate computing. The experimental results show that (1) MV-Net significantly improves the performance flexibility of current CNN models and makes it possible to provide always-guaranteed QoS in a multitask environment, and (2) it satisfies the quality-of-results (QoR) requirement, outperforming the baseline implementation significantly, and improves the system energy efficiency at the same time.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = {oct},
articleno = {35},
numpages = {25},
keywords = {deep learning, online scheduling, energy efficiency, approximate computing, Edge computing}
}

@article{10.14778/3484224.3484229,
author = {Koutsoukos, Dimitrios and M\"{u}ller, Ingo and Marroqu\'{\i}n, Renato and Klimovic, Ana and Alonso, Gustavo},
title = {Modularis: Modular Relational Analytics over Heterogeneous Distributed Platforms},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484229},
doi = {10.14778/3484224.3484229},
abstract = {The enormous quantity of data produced every day together with advances in data analytics has led to a proliferation of data management and analysis systems. Typically, these systems are built around highly specialized monolithic operators optimized for the underlying hardware. While effective in the short term, such an approach makes the operators cumbersome to port and adapt, which is increasingly required due to the speed at which algorithms and hardware evolve. To address this limitation, we present Modularis, an execution layer for data analytics based on sub-operators, i.e., composable building blocks resembling traditional database operators but at a finer granularity. To demonstrate the feasibility and advantages of our approach, we use Modularis to build a distributed query processing system supporting relational queries running on an RDMA cluster, a serverless cloud platform, and a smart storage engine. Modularis requires minimal code changes to execute queries across these three diverse hardware platforms, showing that the sub-operator approach reduces the amount and complexity of the code to maintain. In fact, changes in the platform affect only those sub-operators that depend on the underlying hardware (in our use cases, mainly the sub-operators related to network communication). We show the end-to-end performance of Modularis by comparing it with a framework for SQL processing (Presto), a commercial cluster database (SingleStore), as well as Query-as-a-Service systems (Athena, BigQuery). Modularis outperforms all these systems, proving that the design and architectural advantages of a modular design can be achieved without degrading performance. We also compare Modularis with a hand-optimized implementation of a join for RDMA clusters. We show that Modularis has the advantage of being easily extensible to a wider range of join variants and group by queries, all of which are not supported in the hand-tuned join.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3308–3321},
numpages = {14}
}

@inproceedings{10.1145/2811163.2811174,
author = {Park, Junseok and Choo, Sungji and Lee, Doheon},
title = {Citizen Organization System for Advanced MEDical Research (COSAMED)},
year = {2015},
isbn = {9781450337878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811163.2811174},
doi = {10.1145/2811163.2811174},
abstract = {Analyzing true effect of medicine or functional food is major issue in associated research area. In order to achieve that, gathering of massive clinical trial data is required. There are three major concepts for clinical trial data collection: Citizen Science, Health 2.0 and Crowdsourcing. Citizen Science, which uses web 2.0 technologies, is web-based service for health care. Health 2.0 uses non-professionally trained individuals to conduct science-related activities. Lastly, Crowdsourcing is an online distributed problem-solving and production model. Following systems have tried to process data based on above concepts. PatientsLikeme attempted to find potential benefits from clinical outcomes within longitudinal evaluation of online data-sharing platforms. ResearchKit is about to create apps that could revolutionize medical studies. However, these systems do not have reliable protocols to obtain credible results. In addition, they mainly focus on diseases with a medicine, not on effect with a functional food.Hereby, we are developing a novel system to solve the issues: Citizen Organization System for Advanced MEDical research(COSAMED). We are looking forward to find true effect information of a functional food with our new system. COSAMED is made of five steps to design a reliable protocol. (1) Target Item Selection, to select a target effect and effect related items. (2) Preparation of Research, to select designed clinical trial protocol on user demand with automated scientific criteria. (3) Recruiting Participants, to recruit participants from linked SNS friends or from other systems. (4) Data Collection, to collect effect information from various sources. (5) Analysis, to analyze the results by web-bases statistical tools, transfer results to a data warehouse and calculate credibility rate. Finally, the protocol is developed with product DB and clinical trial protocol snapshot DB on COSAMED. In future, we will integrate it with Openmhealth architecture to connect related systems easily and build it with user friendly interfaces to collect big data. COSAMED will be available at www.cosamed.org, and it will be a cornerstone of first citizen based clinical trial system.},
booktitle = {Proceedings of the ACM Ninth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {23},
numpages = {1},
keywords = {web based system, functional food, clinical trial protocol, citizen science, open source, health 2.0, crowdsourcing},
location = {Melbourne, Australia},
series = {DTMBIO '15}
}

@article{10.1109/TCBB.2014.2361348,
author = {Wang, Jianxin and Zhong, Jiancheng and Chen, Gang and Li, Min and Wu, Fang-xiang and Pan, Yi},
title = {ClusterViz: A Cytoscape APP for Cluster Analysis of Biological Network},
year = {2015},
issue_date = {July/August 2015},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {12},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2014.2361348},
doi = {10.1109/TCBB.2014.2361348},
abstract = {Cluster analysis of biological networks is one of the most important approaches for identifying functional modules and predicting protein functions. Furthermore, visualization of clustering results is crucial to uncover the structure of biological networks. In this paper, ClusterViz, an APP of Cytoscape 3 for cluster analysis and visualization, has been developed. In order to reduce complexity and enable extendibility for ClusterViz, we designed the architecture of ClusterViz based on the framework of Open Services Gateway Initiative. According to the architecture, the implementation of ClusterViz is partitioned into three modules including interface of ClusterViz, clustering algorithms and visualization and export. ClusterViz fascinates the comparison of the results of different algorithms to do further related analysis. Three commonly used clustering algorithms, FAG-EC, EAGLE and MCODE, are included in the current version. Due to adopting the abstract interface of algorithms in module of the clustering algorithms, more clustering algorithms can be included for the future use. To illustrate usability of ClusterViz, we provided three examples with detailed steps from the important scientific articles, which show that our tool has helped several research teams do their research work on the mechanism of the biological networks.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {815–822},
numpages = {8},
keywords = {cytoscape, biological networks, cluster, FAG-EC, EAGLE, visualization, MCODE}
}

@inproceedings{10.1145/2723372.2723719,
author = {Petraki, Eleni and Idreos, Stratos and Manegold, Stefan},
title = {Holistic Indexing in Main-Memory Column-Stores},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2723719},
doi = {10.1145/2723372.2723719},
abstract = {Great database systems performance relies heavily on index tuning, i.e., creating and utilizing the best indices depending on the workload. However, the complexity of the index tuning process has dramatically increased in recent years due to ad-hoc workloads and shortage of time and system resources to invest in tuning.This paper introduces holistic indexing, a new approach to automated index tuning in dynamic environments. Holistic indexing requires zero set-up and tuning effort, relying on adaptive index creation as a side-effect of query processing. Indices are created incrementally and partially;they are continuously refined as we process more and more queries. Holistic indexing takes the state-of-the-art adaptive indexing ideas a big step further by introducing the notion of a system which never stops refining the index space, taking educated decisions about which index we should incrementally refine next based on continuous knowledge acquisition about the running workload and resource utilization. When the system detects idle CPU cycles, it utilizes those extra cycles by refining the adaptive indices which are most likely to bring a benefit for future queries. Such idle CPU cycles occur when the system cannot exploit all available cores up to 100\%, i.e., either because the workload is not enough to saturate the CPUs or because the current tasks performed for query processing are not easy to parallelize to the point where all available CPU power is exploited.In this paper, we present the design of holistic indexing for column-oriented database architectures and we discuss a detailed analysis against parallel versions of state-of-the-art indexing and adaptive indexing approaches. Holistic indexing is implemented in an open-source column-store DBMS. Our detailed experiments on both synthetic and standard benchmarks (TPC-H) and workloads (SkyServer) demonstrate that holistic indexing brings significant performance gains by being able to continuously refine the physical design in parallel to query processing, exploiting any idle CPU resources.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1153–1166},
numpages = {14},
keywords = {self-organization, holistic indexing},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@article{10.1145/3464994.3464998,
author = {Balakrishnan, Hari and Banerjee, Sujata and Cidon, Israel and Culler, David and Estrin, Deborah and Katz-Bassett, Ethan and Krishnamurthy, Arvind and McCauley, Murphy and McKeown, Nick and Panda, Aurojit and Ratnasamy, Sylvia and Rexford, Jennifer and Schapira, Michael and Shenker, Scott and Stoica, Ion and Tennenhouse, David and Vahdat, Amin and Zegura, Ellen},
title = {Revitalizing the Public Internet by Making It Extensible},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/3464994.3464998},
doi = {10.1145/3464994.3464998},
abstract = {There is now a significant and growing functional gap between the public Internet, whose basic architecture has remained unchanged for several decades, and a new generation of more sophisticated private networks. To address this increasing divergence of functionality and overcome the Internet's architectural stagnation, we argue for the creation of an Extensible Internet (EI) that supports in-network services that go beyond best-effort packet delivery. To gain experience with this approach, we hope to soon deploy both an experimental version (for researchers) and a prototype version (for early adopters) of EI. In the longer term, making the Internet extensible will require a community to initiate and oversee the effort; this paper is the first step in creating such a community.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {18–24},
numpages = {7},
keywords = {internet architecture}
}

@inproceedings{10.1145/2702613.2732810,
author = {Park, Joongsin and Jeong, Beomtaek and Jeon, Seungjai and Han, Sehyung and Cho, Jun-Dong and Ko, JeongGil},
title = {Understanding Interactive Interface Design Requirements for the Visually Impaired},
year = {2015},
isbn = {9781450331463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702613.2732810},
doi = {10.1145/2702613.2732810},
abstract = {While taxies are widely considered as an easily accessible form of transportation to many, for the visually impaired, utilizing taxi services can be a significant challenge. In this paper we envision a system architecture where visually impaired people use GPS-enabled mobile computing devices to easily reserve and access taxi services. Specifically, as the first step in designing such a system, we try to understand the preferred interaction interface requirements of the visually impaired population using a set of interviews conducted over 28 visually impaired participants. Our results show that the smartphone usage rate of our interview participants is ~60\%; thus, smartphone-based applications should not be considered as the "everyone-will-use-platform" when interacting with the visually impaired. Results from an extensive set of questions reveal that interaction interfaces in the form of key chains and wrist watches can also be effective for various interactive applications.},
booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {881–886},
numpages = {6},
keywords = {visually impaired interfaces, wireless interaction systems},
location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {CHI EA '15}
}

@inproceedings{10.1145/2700171.2804454,
author = {Torre, Ilaria and Celik, Ilknur},
title = {User-Adapted Web of Things for Accessibility},
year = {2015},
isbn = {9781450333955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2700171.2804454},
doi = {10.1145/2700171.2804454},
abstract = {This paper describes a new wave of the Web that is the useradapted Web of Things. This is a new step in the evolution of the Web of Things and of adaptive web-based systems. The current proposals for the Web of Things focus on the augmentation of the physical objects in order to provide enhanced services. However, in our view, the Web of Things can also be a means to make physical objects accessible or more usable for people with special needs by exploiting adaptive and semantic techniques. The architecture presented in the paper describes the specific modules and components at the basis of this approach.},
booktitle = {Proceedings of the 26th ACM Conference on Hypertext \&amp; Social Media},
pages = {341–344},
numpages = {4},
keywords = {accessibility, user-adapted interaction, linked data, web of things, adaptation techniques, semantic web, special needs., adaptive web},
location = {Guzelyurt, Northern Cyprus},
series = {HT '15}
}

@inproceedings{10.1145/2602576.2602578,
author = {Johnson, Kenneth and Calinescu, Radu},
title = {Efficient Re-Resolution of SMT Specifications for Evolving Software Architectures},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602578},
doi = {10.1145/2602576.2602578},
abstract = {We present a generic method for the efficient constraint re-resolution of a component-based software architecture after changes such as addition, removal and modification of components. Given a formal description of an evolving system as a constraint-specification problem, our method identifies and executes the re-resolution steps required to verify the system's compliance with constraints after each change. At each step, satisfiability modulo theory (SMT) techniques determine the satisfiability of component constraints expressed as logical formulae over suitably chosen theories of arithmetic, reusing results obtained in previous steps. We illustrate the application of the approach on a constraint-satisfaction problem arising from cloud-deployed software services. The incremental method is shown to re-resolve system constraints in a fraction of the time taken by standard SMT resolution.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {93–102},
numpages = {10},
keywords = {incremental re-resolution, domain-specific languages, satisfiability modulo theory},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/3094405.3094412,
author = {Cozzolino, Vittorio and Ding, Aaron Yi and Ott, J\"{o}rg},
title = {FADES: Fine-Grained Edge Offloading with Unikernels},
year = {2017},
isbn = {9781450350587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094405.3094412},
doi = {10.1145/3094405.3094412},
abstract = {FADES is an edge offloading architecture that empowers us to run compact, single purpose tasks at the edge of the network to support a variety of IoT and cloud services. The design principle behind FADES is to efficiently exploit the resources of constrained edge devices through fine-grained computation offloading. FADES takes advantage of MirageOS unikernels to isolate and embed application logic in concise Xen-bootable images. We have implemented FADES and evaluated the system performance under various hardware and network conditions. Our results show that FADES can effectively strike a balance between running complex applications in the cloud and simple operations at the edge. As a solid step to enable fine-grained edge offloading, our experiments also reveal the limitation of existing IoT hardware and virtualization platforms, which shed light on future research to bring unikernel into IoT domain.},
booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
pages = {36–41},
numpages = {6},
keywords = {IoT, Edge Computing, Virtualization},
location = {Los Angeles, CA, USA},
series = {HotConNet '17}
}

@inproceedings{10.1145/3230833.3233248,
author = {Cabaj, Krzysztof and Gregorczyk, Marcin and Mazurczyk, Wojciech and Nowakowski, Piotr and \.{Z}\'{o}rawski, Piotr},
title = {SDN-Based Mitigation of Scanning Attacks for the 5G Internet of Radio Light System},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233248},
doi = {10.1145/3230833.3233248},
abstract = {Currently 5G communication networks are gaining on importance among industry, academia, and governments worldwide as they are envisioned to offer wide range of high-quality services and unfaltering user experiences. However, certain security, privacy and trust challenges need to be addressed in order for the 5G networks to be widely welcomed and accepted. That is why in this paper, we take a step towards these requirements and we introduce a dedicated SDN-based integrated security framework for the Internet of Radio Light (IoRL) system that is following 5G architecture design. In particular, we present how TCP SYN-based scanning activities which typically comprise the first phase of the attack chain can be detected and mitigated using such an approach. Enclosed experimental results prove that the proposed security framework has potential to become an effective defensive solution.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {49},
numpages = {10},
keywords = {5G System Architecture, Network Function Virtualization, Visible Light Communications, mm Wave Communications, Integrated Security Framework, Software Defined Networks},
location = {Hamburg, Germany},
series = {ARES '18}
}

@inproceedings{10.1145/3073763.3073770,
author = {de Dinechin, Beno\^{\i}t Dupont and Graillat, Amaury},
title = {Network-on-Chip Service Guarantees on the Kalray MPPA-256 Bostan Processor},
year = {2017},
isbn = {9781450352260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3073763.3073770},
doi = {10.1145/3073763.3073770},
abstract = {The Kalray MPPA-256 Bostan manycore processor implements a clustered architecture, where clusters of cores share a local memory, and a DMA-capable network-on-chip (NoC) connects the clusters. The NoC implements wormhole switching without virtual channels, with source routing, and can be configured for maximum flow rate and burstiness at ingress. We describe and illustrate the techniques used to configure the MPPA NoC for guaranteed services. Our approach is based on three steps: global selection of routes between end-points and computation of flow rates, by solving the max-min fairness with unsplittable path problem; configuration of the flow burstiness parameters at ingress, by solving an acyclic set of linear inequalities; and end-to-end latency upper bound computation, based on the principles of separated flow analysis (SFA). In this paper, we develop the two last steps, taking advantage of the effects of NoC link shaping on the leaky-bucket arrival curves of flows.},
booktitle = {Proceedings of the 2nd International Workshop on Advanced Interconnect Solutions and Technologies for Emerging Computing Systems},
pages = {35–40},
numpages = {6},
keywords = {network-on-chip, deterministic network calculus, separated flow analysis, link traffic shaping},
location = {Stockholm, Sweden},
series = {AISTECS '17}
}

@inproceedings{10.5555/3242181.3242227,
author = {Falcone, Alberto and Garro, Alfredo and Anagnostou, Anastasia and Taylor, Simon J. E.},
title = {An Introduction to Developing Federations with the High Level Architecture (HLA)},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {The IEEE 1516-2010 - High Level Architecture (HLA) for distributed simulation is growing in a variety of application domains due to its capabilities to enable the interoperability and reusability of distributed simulation components. However, the development of simulation models based on the HLA standard remains a challenging task that requires a considerable effort in terms of both time and cost. This paper provides an introduction tutorial on developing HLA-based simulations using the HLA Development Kit (DKF) framework. The tutorial guides developers through the necessary steps for defining and creating an HLA-based simulation, and explains how the HLA elements can be easily managed by using the DKF's services. The effectiveness of the DKF is proven by its concrete exploitation in the context of the Simulation Exploration Experience (SEE), a project led by NASA and which involves as partners several U.S. and European Institutions.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {43},
numpages = {15},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@article{10.14778/2733004.2733046,
author = {To, Quoc-Cuong and Nguyen, Benjamin and Pucheral, Philippe},
title = {SQL/AA: Executing SQL on an Asymmetric Architecture},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733046},
doi = {10.14778/2733004.2733046},
abstract = {Current applications, from complex sensor systems (e.g. quantified self) to online e-markets acquire vast quantities of personal information which usually end-up on central servers. This information represents an unprecedented potential for user customized applications and business (e.g., car insurance billing, carbon tax, traffic decongestion, resource optimization in smart grids, healthcare surveillance, participatory sensing). However, the PRISM affair has shown that public opinion is starting to wonder whether these new services are not bringing us closer to science fiction dystopias. It has become clear that centralizing and processing all one's data on a single server is a major problem with regards to privacy concerns. Conversely, decentralized architectures, devised to help individuals keep full control of their data, complexify global treatments and queries, often impeding the development of innovative services and applications.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1625–1628},
numpages = {4}
}

@inproceedings{10.1145/3318216.3363331,
author = {Romero, Eduardo and Stewart, Christopher and Morris, Nathaniel},
title = {Fast Inference Services for Alternative Deep Learning Structures},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363331},
doi = {10.1145/3318216.3363331},
abstract = {AI inference services receive requests, classify data and respond quickly. These services underlie AI-driven Internet of Things, recommendation engines and video analytics. Neural networks are widely used because they provide accurate results and fast inference, but it is hard to explain their classifications. Tree-based deep learning models can provide accuracy and are innately explainable. However, it is hard to achieve high inference rates because branch misprediction and cache misses produce inefficient executions. My research seeks to produce low latency inference services based on tree-based models. I will exploit the emergence of large L3 caches to convert tree-based model inference from sequential branching toward fast, in-cache lookups. Our approach begins with fully trained, accurate tree-based models, compiles them for inference on target processors and executes inference efficiently. If successful, our approach will enable qualitative advances in AI services. Tree-based models can report the most significant features in a classification in a single pass. In contrast, neural networks require iterative approaches to explain their results. Consider interactive AI recommendation services where users seek to explicitly order their instantaneous preferences to attract preferred content. Tree-based models can provide user feedback much more quickly than neural networks. Tree-based models also have less prediction variance than neural networks. Given the same training data, neural networks require many inferences to quantify variances of borderline classifications. Fast tree-based inference can explain variance in seconds (versus minutes). Our approach shows that competing machine learning approaches can provide comparable accuracy but desire wholly different architectural and platform support.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {329–331},
numpages = {3},
location = {Arlington, Virginia},
series = {SEC '19}
}

@inproceedings{10.1145/3278161.3278166,
author = {Ding, Aaron Yi and Janssen, Marijn},
title = {Opportunities for Applications Using 5G Networks: Requirements, Challenges, and Outlook},
year = {2018},
isbn = {9781450365802},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278161.3278166},
doi = {10.1145/3278161.3278166},
abstract = {The increasing demand for mobile network capacity driven by Internet of Things (IoT) applications results in the need for understanding better the potential and limitations of 5G networks. Vertical application areas like smart mobility, energy networks, industrial IoT applications, and AR/VR enhanced services all pose different requirements on the use of 5G networks. Some applications need low latency, whereas others need high bandwidth or security support. The goal of this paper is to identify the requirements and to understand the limitations for 5G driven applications. We review application areas and list the typical challenges and requirements posed on 5G networks. A main challenge will be to develop a network architecture being able to dynamically adapt to fluctuating traffic patterns and accommodating various technologies such as edge computing, blockchain based distributed ledger, software defined networking, and virtualization. To inspire future research, we reveal open problems and highlight the need for piloting with 5G applications, with tangible steps, to understand the configuration of 5G networks and the use of applications across multiple vertical industries.},
booktitle = {Proceedings of the Seventh International Conference on Telecommunications and Remote Sensing},
pages = {27–34},
numpages = {8},
keywords = {edge computing, smart city, 5G systems, pilot, IoT},
location = {Barcelona, Spain},
series = {ICTRS '18}
}

@inproceedings{10.1145/3095770.3095771,
author = {Maccabe, Arthur B.},
title = {Operating and Runtime Systems Challenges for HPC Systems},
year = {2017},
isbn = {9781450350860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3095770.3095771},
doi = {10.1145/3095770.3095771},
abstract = {Future HPC systems will be characterized by extreme heterogeneity. We will see increasing heterogeneity in virtually every aspect of node architecture from computational engines to memory systems. We will see increasing heterogeneity in applications, including heterogeneity within applications (as previously independent applications are composed to build new applications). We will see increasing heterogeneity in system usage models; in some cases, the HPC system is not the most precious resource being managed. We will also see increasing heterogeneity in the shared services (e.g., storage and visualization systems) that are connected to HPC systems.All of this increasing heterogeneity is certain to create new challenges in the design and implementation of operating and runtime systems. There will be new kinds of resources to manage and many resource management tactics will be invented (and some re-discovered and adapted) to address the new heterogeneity. In essence, we will tacitly agree that the operating and runtime systems need to adapt to enable the inevitable integration of new technologies, applications, usage models, and shared services. While this agreement is critical for our ability to make incremental progress, we, as a community, must step back and ask the relevant question: Does the OS or runtime system bear the brunt of the adaptation, or will we be able to insist on changes in the technologies, applications, and environment? In the past decade, we have seen a similar tradeoff play out between the application teams and the architects of computational engines: how much floating point precision is required and how is this precision implemented? How can we define similar tradeoffs that are important in the design and implementation of operating and runtime systems?},
booktitle = {Proceedings of the 7th International Workshop on Runtime and Operating Systems for Supercomputers ROSS 2017},
articleno = {1},
numpages = {1},
location = {Washingon, DC, USA},
series = {ROSS '17}
}

@inproceedings{10.5555/2772722.2772757,
author = {Bencha\"{\i}b, Yacine and Secci, Stefano and Phung, Chi-Dung},
title = {Transparent Cloud Access Performance Augmentation via an MPTCP-LISP Connection Proxy},
year = {2015},
isbn = {9781467366328},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The use by a growing number of users of Cloud-based services requires an adaptation of the network technologies used to access them. We propose to combine two novel protocols at the state of the art at Cloud access middle-boxes to better profit from spare unused network path diversity. The first protocol, Multipath TCP, allows creating multiple TCP/IP subflows, as much as needed. The second, the Locator/Identifier Separation Protocol (LISP), can be used to route the subflows on different wide-area network paths, possibly disjoint, and also allows native support for seamless virtual machine migrations. In this paper we specify how we can combine these two protocols to increase the bandwidth available to access applications run in multi-homed data-centers. We describe how these protocols can be integrated into a Cloud access middle-box. By means of a combined MPTCP-LISP access proxy, the acceleration is transparent to the user terminal that does not necessitate any upgrade. We provide the detailed system-level architecture based on open source code, and we document results from preliminary experimentations on one of two targeted use-cases. The evaluations conducted show that the overhead generated by our solution remains moderate despite the various system-level steps required to translate incoming TCP packets into MPTCP-LISP packets then routed over different IP paths.},
booktitle = {Proceedings of the Eleventh ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
pages = {201–202},
numpages = {2},
keywords = {data-center networking, cloud access protocol},
location = {Oakland, California, USA},
series = {ANCS '15}
}

@inproceedings{10.1145/3338840.3355655,
author = {Lu, Yung-Feng and Chen, Hung-Ming and Kuo, Chin-Fu and Tseng, Bo-Kai and Chou, Shih-Chun},
title = {Container-Based Load Balancing for WebRTC Applications},
year = {2019},
isbn = {9781450368438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338840.3355655},
doi = {10.1145/3338840.3355655},
abstract = {Nowadays, the progress of the communication technology is fast. With the popularity of smart phones, tablets and computers, social networking sites or social software have also developed rapidly, changing the user's habit of using network communication software. The demand for streaming audio and video communication has increased dramatically, resulting in the maturity of the Internet today. At present, we can know that there are a variety of applications that can be talked on the market, such as LINE, Skype, Hangouts, etc., which can make instant calls. In the era of the Internet, the communication software has shortened the dispersion in the world. The distance between people everywhere.This research implements a web-based instant messaging architecture of WebRTC (Web Real-Time Communication, WebRTC) built on a container. We solved the concatenation problem caused by constructing WebRTC services on the container and sought to improve the performance. WebRTC can directly provide instant video and audio communication technology, and cooperate with ICE mechanism to communicate on different domains. No additional Plug-in is needed, only web browser can realize instant messaging function through web browser. It saves a lot of complicated steps, such as: install the user user, and so on. Our system also implements a load balancing mechanism that distributes traffic across the TURN Server, improving overall system performance.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {20–26},
numpages = {7},
keywords = {web application, container, cloud, load balance},
location = {Chongqing, China},
series = {RACS '19}
}

@inproceedings{10.1145/3437359.3465576,
author = {Stubbs, Joe and Marru, Suresh and Mejia, Daniel and Navarro, John-Paul and Franz, Eric and Black, Steve and Wannipurage, Dimuthu and Pamidighantam, Sudhakar and Stirm, Claire and Dahan, Maytal and Pierce, Marlon and Zentner, Michael},
title = {Common Resource Descriptions for Interoperable Gateway Cyberinfrastructure},
year = {2021},
isbn = {9781450382922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437359.3465576},
doi = {10.1145/3437359.3465576},
abstract = {Science gateway projects face challenges utilizing the vast and heterogeneous landscape of powerful cyberinfrastructure available today, and interoperability across technologies remains poor. This interoperability issue leads to myriad problems: inability to bring multiple heterogeneous specialized resources together to solve problems where different resources are optimized for different facets of the problem; inability to choose from multiple resources on-the-fly as needed based on characteristics and available capacity; and ultimately a less than optimal application of nationally-funded resources toward advancing science. This paper presents version 1.0 of the Science Gateways Community Institute (SGCI) Resource Description Specification – a schema providing a common language for describing storage and computing resources utilized by science gateway technologies – as well as an Inventory API and software development kits for incorporating resource definitions into gateway projects. We discuss multiple gateway integration design options, with trade offs regarding robustness and availability. We detail the adoption to date of the SGCI Resource Specification by several prominent projects, including Apache Airavata, HUBzero®, Open OnDemand, Tapis, and XSEDE. The XSEDE adoption is worth highlighting explicitly as it has led to a new API within the XSEDE Information Services architecture which provides SGCI resource descriptions of all active XSEDE resources. Additionally, we show how the use of the SGCI Resource Specification provides interoperability across resource providers and projects that adopt it. Finally, as a proof of concept, we present a multi-step analysis that runs Quantum ESPRESSO and visualizes the energy band structures of a Gallium Arsenide (GaAs) crystal across multiple resource providers including the Halstead cluster at Purdue University and the Stampede2 supercomputer at TACC.},
booktitle = {Practice and Experience in Advanced Research Computing},
articleno = {20},
numpages = {9},
keywords = {science gateways community institute, Cyberinfrastructure, resource description, interoperability},
location = {Boston, MA, USA},
series = {PEARC '21}
}

@inproceedings{10.5555/3330299.3330308,
author = {Magnani, Antonio and D'Angelo, Gabriele and Ferretti, Stefano and Marzolla, Moreno},
title = {Anonymity and Confidentiality in Secure Distributed Simulation},
year = {2018},
isbn = {9781538650486},
publisher = {IEEE Press},
abstract = {Research on data confidentiality, integrity and availability is gaining momentum in the ICT community, due to the intrinsically insecure nature of the Internet. While many distributed systems and services are now based on secure communication protocols to avoid eavesdropping and protect confidentiality, the techniques usually employed in distributed simulations do not consider these issues at all. This is probably due to the fact that many real-world simulators rely on monolithic, offline approaches and therefore the issues above do not apply. However, the complexity of the systems to be simulated, and the rise of distributed and cloud based simulation, now impose the adoption of secure simulation architectures. This paper presents a solution to ensure both anonymity and confidentiality in distributed simulations. A performance evaluation based on an anonymized distributed simulator is used for quantifying the performance penalty for being anonymous. The obtained results show that this is a viable solution.},
booktitle = {Proceedings of the 22nd International Symposium on Distributed Simulation and Real Time Applications},
pages = {71–78},
numpages = {8},
keywords = {secure simulation, distributed simulation, anonymity, confidentiality},
location = {Madrid, Spain},
series = {DS-RT '18}
}

@proceedings{10.1145/2930238,
title = {UMAP '16: Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization},
year = {2016},
isbn = {9781450343688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 24th ACM International Conference on User Modeling, Adaptation, and Personalization (UMAP 2016) in Halifax, Canada, July 13-16, 2016. UMAP is the premier international conference for researchers and practitioners working on systems that adapt to individual users or to groups of users. UMAP is the successor of the biennial User Modeling (UM) and Adaptive Hypermedia and Adaptive Web-based Systems (AH) conferences that were merged in 2009. It has traditionally been organized under the auspices of User Modeling Inc. This year (2016) UMAP became an ACM conference, sponsored by ACM SIG CHI and SIG WEB.The conference spans a wide scope of topics related to user modeling, adaptation, and personalization. UMAP 2016 is focused on bringing together cutting-edge research from user interaction and modeling, adaptive technologies, and delivery platforms. It includes high-quality peer-reviewed papers featuring substantive new research in one of five research areas, each chaired by leaders in the field: User Modeling for Recommender Systems (chairs: Alexander Felfernig \&amp; Pasquale Lops)Adaptive \&amp; Personalized Educational Systems (chairs: Antonija Mitrovic \&amp; Kalina Yacef)Personalization in the Social Web \&amp; Crowdsourcing Era (chairs: Alessandro Bozzon \&amp; Harith Alani)Adaptive, Intelligent, \&amp; Multimodal User Interfaces (chairs: Julien Epps \&amp; Hatice Gunes)Architectures, Techniques, \&amp; Methodologies for UMAP (chairs: Stephan Weibelzahl \&amp; Mihaela Cocea)This year we received 123 submissions. In keeping with UMAPs rigorous standards, each paper was carefully reviewed by members of the Program Committee (PC) while the Area Chairs (ACs) coordinated the reviews and provided recommendations to the Program Chairs. The international Program Committee (PC) consisted of 132 members who were assisted by 49 subreviewers. These were leading researchers as well as highly promising young researchers.Papers were assigned to at least 4 members of the PC and to 1 AC member based on their expertise, interests, and other factors. Each paper received at least 3 reviews, and 95\% received 4 reviews. After the initial reviews were submitted, the designated AC facilitated discussion amongst reviewers in order to resolve differences and correct misunderstandings. The AC then provided a summative meta-review and a recommendation to the Program Chairs. The final decisions were based on these recommendations, the meta-reviews, and reviewer scores.We accepted 21 long papers (23.9\% acceptance rate) and 13 short papers (27.6\% acceptance rate) for oral presentation and an additional 17 extended abstracts for poster presentation and inclusion in the proceedings. The program also features posters, demos, and late breaking results, which collectively showcase the wide spectrum of novel ideas and latest results in user modeling, adaptation and personalization.We also invited three distinguished keynote speakers, each illustrating significant issues and prospective directions for the field. Hossein Derakhshan (shared keynote speaker with Hypertext 2016) is an Iranian-Canadian blogger who was imprisoned in Tehran from November 2008 to November 2014. He has been called the "father of Persian blogging" and has helped promote podcasting in Iran. His talk, "Killing the Hyperlink, Killing the Web: the Shift from Library-Internet to Television-Internet," reflects his views on the Internet today.Lada Adamic leads the Product Science group within Facebook's Data Science Team. She is also an adjunct associate professor at the University of Michigan's School of Information and Center for the Study of Complex Systems. Her talk "The Life and Times of Information in Networks" focuses on cascades of information-sharing and resharing within social media.Sandra Carberry was one of the founders of the User Modeling research area at the first workshop in Maria Laach in 1986. As appropriate for the 30th anniversary, her talk "User Modeling: The Past, The Present and The Future" discusses how the field evolved, insights into where the field is headed, and the hottest topics for exploration.The conference includes a doctoral consortium that provides an opportunity for doctoral students to explore and develop their research interests under the guidance of distinguished scholars. This track received 18 submissions of which nine were accepted as full papers and eight as posters.A set of seven workshops and two tutorials round out the program. (Workshop) IFUP: Workshop on Multi-dimension Information Fusion for Modeling and Personalisation (half-day) organized by Robin Burke (DePaul University, USA), Feida Zhu (Singapore Management University, Singapore), Neil Yorke-Smith (American University of Beirut, Lebanon), and Guibing Guo (Northeastern University, China)(Workshop) INRA: News Recommendation and Analytics (half-day) organized by Jon Atle Gulla (Norwegian University of Science and Technology Trondheim, Norway), Luc Martens (Minds-UGent-WiCa Ghent, Belgium), \"{O}zlem \"{O}zg\"{o}bek (Norwegian University of Science and Technology Trondheim, Norway), and Nafiseh Shabib (TNS Gallup, Oslo, Norway)(Workshop) SOAP: Workshop on Surprise, Opposition, and Obstruction in Adaptive and Personalized Systems (half-day) organized by Peter Knees (Johannes Kepler University Linz, Austria), Kristina Andersen (Studio for Electro Instrumental Music, Amsterdam, the Netherlands), Alan Said (Recorded Future, Gothenburg, Sweden), and Marko Tkalcic (Free University of Bozen-Bolzano, Italy)(Workshop) HAAPIE: Human Aspects in Adaptive and Personalised Interactive Environments (half-day) organized by Panagiotis Germanakos (SAP SE, Germany), Marios Belk (Department of Computer Science, University of Cyprus), George Samaras (Department of Computer Science, University of Cyprus), and Vania Dimitrova (University of Leeds, UK)(Workshop) EvalUMAP: Towards comparative evaluation in the user modelling, adaptation and personalization space (full-day) organized by Owen Conlan (Trinity College Dublin, Ireland), Liadh Kelly (Trinity College Dublin, Ireland), Kevin Koidl (Trinity College Dublin, Ireland), S\'{e}amus Lawless (Trinity College Dublin, Ireland), Killian Levacher (Trinity College Dublin, Ireland), and Athanasios Staikopoulos (Trinity College Dublin, Ireland)(Workshop) FuturePD: The future of personal data: envisioning new personalized services enabled by Quantified Self technologies (half-day) organized by Amon Rapp (University of Torino, Italy), Federica Cena (University of Torino, Italy), Judy Kay (University of Sidney, Australia), Bob Kummerfeld (University of Sydney, Australia), Frank Hopfgartner (University Gardens Glasgow, UK), Jakob Eg Larsen (Technical University of Denmark, Denmark), and Elise van den Hoven (University of Technology Sydney, Australia). (Workshop) PALE: Personalization Approaches in Learning Environments (full-day) organized by Milos Kravcik (RWTH Aachen University, Germany), Olga C. Santos (UNED, Spain), Jesus G. Boticario (UNED, Spain), and Maria Bielikova (FIIT STUBA, Slovakia)(Tutorial) Semantics-Aware Techniques for Social Media Analysis, User Modeling, and Recommender Systems (half-day) by Pasquale Lops and Cataldo Musto (University of Bari Aldo Moro, Italy)(Tutorial) Games, Gamification and Personalization (half-day) by Amon Rapp (University of Torino, Italy).},
location = {Halifax, Nova Scotia, Canada}
}

@inproceedings{10.1145/3180155.3182528,
author = {Segura, Sergio and Parejo, Jos\'{e} A. and Troya, Javier and Ruiz-Cort\'{e}s, Antonio},
title = {Metamorphic Testing of RESTful Web APIs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3182528},
doi = {10.1145/3180155.3182528},
abstract = {Web Application Programming Interfaces (APIs) specify how to access services and data over the network, typically using Web services. Web APIs are rapidly proliferating as a key element to foster reusability, integration, and innovation, enabling new consumption models such as mobile or smart TV apps. Companies such as Facebook, Twitter, Google, eBay or Netflix receive billions of API calls every day from thousands of different third-party applications and devices, which constitutes more than half of their total traffic.As Web APIs are progressively becoming the cornerstone of software integration, their validation is getting more critical. In this context, the fast detection of bugs is of utmost importance to increase the quality of internal products and third-party applications. However, testing Web APIs is challenging mainly due to the difficulty to assess whether the output of an API call is correct, i.e., the oracle problem. For instance, consider the Web API of the popular music streaming service Spotify. Suppose a search for albums with the query "redhouse" returning 21 total matches: Is this output correct? Do all the albums in the result set contain the keyword? Are there any albums containing the keyword not included in the result set? Answering these questions is difficult, even with small result sets, and often infeasible when the results are counted by thousands or millions.Metamorphic testing alleviates the oracle problem by providing an alternative when the expected output of a test execution is complex or unknown. Rather than checking the output of an individual program execution, metamorphic testing checks whether multiple executions of the program under test fulfil certain necessary properties called metamorphic relations. For instance, consider the following metamorphic relation in Spotify: two searches for albums with the same query should return the same number of total results regardless of the size of pagination. Suppose that a new Spotify search is performed using the exact same query as before and increasing the maximum number of results per page from 20 (default value) to 50: This search returns 27 total albums (6 more matches than in the previous search), which reveals a bug. This is an example of a real and reproducible fault detected using the approach presented in this paper and reported to Spotify. According to Spotify developers, it was a regression fault caused by a fix with undesired side effects.In this paper [1], we present a metamorphic testing approach for the automated detection of faults in RESTful Web APIs (henceforth also referred to as simply Web APIs). We introduce the concept of metamorphic relation output patterns. A Metamorphic Relation Output Pattern (MROP) defines an abstract output relation typically identified in Web APIs, regardless of their application domain. Each MROP is defined in terms of set operations among test outputs such as equality, union, subset, or intersection. MROPs provide a helpful guide for the identification of metamorphic relations, broadening the scope of our work beyond a particular Web API. Based on the notion of MROP, a methodology is proposed for the application of the approach to any Web API following the REST architectural pattern.The approach was evaluated in several steps. First, we used the proposed methodology to identify 33 metamorphic relations in four Web APIs developed by undergraduate students. All the relations are instances of the proposed MROPs. Then, we assessed the effectiveness of the identified relations at revealing 317 automatically seeded faults (i.e., mutants) in the APIs under test. As a result, 302 seeded faults were detected, achieving a mutation score of 95.3\%. Second, we evaluated the approach using real Web APIs and faults. In particular, we identified 20 metamorphic relations in the Web API of Spotify and 40 metamorphic relations in the Web API of YouTube. Each metamorphic relation was implemented and automatically executed using both random and manual test data. In total, 469K metamorphic tests were generated. As a result, 21 metamorphic relations were violated, and 11 issues revealed and reported (3 issues in Spotify and 8 issues in YouTube). To date, 10 of the reported issues have been either confirmed by the API developers or reproduced by other users supporting the effectiveness of our approach.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {882},
numpages = {1},
keywords = {metamorphic testing, RESTful web services, web API, REST},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2602576.2602580,
author = {Chavarriaga, Jaime and Noguera, Carlos A. and Casallas, Rubby and Jonckers, Viviane},
title = {Architectural Tactics Support in Cloud Computing Providers: The Jelastic Case},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602580},
doi = {10.1145/2602576.2602580},
abstract = {When developing and deploying applications in the cloud, architects face the challenge of conciliating architectural decisions with the options and restrictions imposed by the chosen cloud provider. An architectural decision can be seen as a two-step process: selecting architectural tactics to promote quality attributes and choosing design alternatives to implement those tactics. Available design alternatives are limited by the offer of the cloud provider. When configuring the cloud platform and its services as directed by the chosen tactics, the architect must be mindful of conflicts among the available alternatives. These trade-offs amongst the desired quality attributes can be difficult to detect, understand and ultimately solve. In this paper, we consider the case of Jelastic, a particular cloud platform provider, to illustrate: 1) the modeling of architectural tactics and their corresponding design alternatives using cloud configuration options, and 2) a process that exploits these models to determine which options to use in order to implement a combination of tactics. Furthermore, we present an analysis for this cloud provider that explains which combinations of tactics and configurations lead to trade-offs.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {13–22},
numpages = {10},
keywords = {feature model, quality attributes, cloud computing, architectural tactics},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/2652524.2652585,
author = {Giacalone, Matteo and Paci, Federica and Mammoliti, Rocco and Perugino, Rodolfo and Massacci, Fabio and Selli, Claudio},
title = {Security Triage: An Industrial Case Study on the Effectiveness of a Lean Methodology to Identify Security Requirements},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652585},
doi = {10.1145/2652524.2652585},
abstract = {Context: Poste Italiane is a large corporation offering integrated services in banking and savings, postal services, and mobile communication. Every year, it receives thousands of change requests for its ICT services. Applying to each and every request a security assessment "by the book" is simply not possible. Goal: We report the experience by Poste Italiane of a lean methodology to identify security requirements that can be inserted in the production cycle of a normal company. Method: The process is based on surveying the overall IT architectures (Security Survey) and then a lean dynamic process (Security Triage) to evaluate individual change requests, so that important changes get the attention they need, minor changes can be quickly implemented, and compliance and security obligations are met. Results: The empirical evaluation conducted for over an year at Poste Italiane shows that the process significantly reduces the time to identify security requirements at the pace of change. Conclusions: The Security Survey and Triage process should thus be embedded in a company's production cycle as mandatory step to manage change requests so that security initiatives are prioritized based on the relevance of the assets and of the business objectives of the company.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {24},
numpages = {8},
keywords = {change requests, security requirements elicitation},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.1145/2820783.2820791,
author = {Magalh\~{a}es, Regis Pires and Coutinho, Gustavo and Mac\^{e}do, Jos\'{e} and Ferreira, Camila and Cruz, L\'{\i}via and Nascimento, Mario},
title = {Graphast: An Extensible Framework for Building Applications on Time-Dependent Networks},
year = {2015},
isbn = {9781450339674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2820783.2820791},
doi = {10.1145/2820783.2820791},
abstract = {Graphast is a framework tool that allows developers to compose a number of network models, data importing/exporting services as well as query services, in order to quickly build applications on time-dependent networks. The main goal is to allow developers to implement solutions to different types of problems on time-dependent networks using spatial queries, such as nearest neighbor queries, optimal sequenced routes, etc. Graphast allows the combination of facilities provided by the framework via a public API and/or the building of new facilities, e.g., a new query processing algorithm, and incorporate those into Graphast for others to use them as well. In this paper, we discuss Graphast's architectural components and how one can create/store instances of those components in order to build an application. The steps necessary for building a real world application are also presented.},
booktitle = {Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {93},
numpages = {4},
keywords = {query services, time-dependent networks, framework},
location = {Seattle, Washington},
series = {SIGSPATIAL '15}
}

@article{10.1145/3447868,
author = {Michel, Oliver and Bifulco, Roberto and R\'{e}tv\'{a}ri, G\'{a}bor and Schmid, Stefan},
title = {The Programmable Data Plane: Abstractions, Architectures, Algorithms, and Applications},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3447868},
doi = {10.1145/3447868},
abstract = {Programmable data plane technologies enable the systematic reconfiguration of the low-level processing steps applied to network packets and are key drivers toward realizing the next generation of network services and applications. This survey presents recent trends and issues in the design and implementation of programmable network devices, focusing on prominent abstractions, architectures, algorithms, and applications proposed, debated, and realized over the past years. We elaborate on the trends that led to the emergence of this technology and highlight the most important pointers from the literature, casting different taxonomies for the field, and identifying avenues for future research.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {82},
numpages = {36},
keywords = {Programmable data planes, programmable switches, packet processing, in-network computation, network programmability}
}

