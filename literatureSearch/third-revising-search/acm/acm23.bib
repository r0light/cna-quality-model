@article{10.1145/2746232,
author = {Yoginath, Srikanth B. and Perumalla, Kalyan S.},
title = {Efficient Parallel Discrete Event Simulation on Cloud/Virtual Machine Platforms},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1049-3301},
url = {https://doi.org/10.1145/2746232},
doi = {10.1145/2746232},
abstract = {Cloud and Virtual Machine (VM) technologies present new challenges with respect to performance and monetary cost in executing parallel discrete event simulation (PDES) applications. Due to the introduction of overall cost as a metric, the traditional use of the highest-end computing configuration is no longer the most obvious choice. Moreover, the unique runtime dynamics and configuration choices of Cloud and VM platforms introduce new design considerations and runtime characteristics specific to PDES over Cloud/VMs. Here, an empirical study is presented to help understand the dynamics, trends, and trade-offs in executing PDES on Cloud/VM platforms. Performance and cost measures obtained from multiple PDES applications executed on the Amazon EC2 Cloud and on a high-end VM host machine reveal new, counterintuitive VM--PDES dynamics and guidelines. One of the critical aspects uncovered is the fundamental mismatch in hypervisor scheduler policies designed for general Cloud workloads versus the virtual time ordering needed for PDES workloads. This insight is supported by experimental data revealing the gross deterioration in PDES performance traceable to VM scheduling policy. To overcome this fundamental problem, the design and implementation of a new deadlock-free scheduler algorithm are presented, optimized specifically for PDES applications on VMs. The scalability of our scheduler has been tested in up to 128 VMs multiplexed on 32 cores, showing significant improvement in the runtime relative to the default Cloud/VM scheduler. The observations, algorithmic design, and results are timely for emerging Cloud/VM-based installations, highlighting the need for PDES-specific support in high-performance discrete event simulations on Cloud/VM platforms.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {jul},
articleno = {5},
numpages = {26},
keywords = {scheduler, virtual machines, Parallel discrete event simulation, time warp, global virtual time}
}

@inproceedings{10.1145/2649387.2660839,
author = {Lindsey, Aaron and Yeh, Hsin-Yi (Cindy) and Wu, Chih-Peng and Thomas, Shawna and Amato, Nancy M.},
title = {Improving Decoy Databases for Protein Folding Algorithms},
year = {2014},
isbn = {9781450328944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2649387.2660839},
doi = {10.1145/2649387.2660839},
abstract = {Predicting protein structures and simulating protein folding are two of the most important problems in computational biology today. Simulation methods rely on a scoring function to distinguish the native structure (the most energetically stable) from non-native structures. Decoy databases are collections of non-native structures used to test and verify these functions.We present a method to evaluate and improve the quality of decoy databases by adding novel structures and removing redundant structures. We test our approach on 17 different decoy databases of varying size and type and show significant improvement across a variety of metrics. We also test our improved databases on a popular modern scoring function and show that they contain a greater number of native-like structures than the original databases, thereby producing a more rigorous database for testing scoring functions.},
booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {717–724},
numpages = {8},
keywords = {decoy databases, sampling methods, protein folding},
location = {Newport Beach, California},
series = {BCB '14}
}

@article{10.1109/TNET.2019.2900434,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet and Ra, Moo-Ryong},
title = {Multi-Tier Caching Analysis in CDN-Based Over-the-Top Video Streaming Systems},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2900434},
doi = {10.1109/TNET.2019.2900434},
abstract = {Internet video traffic has been rapidly increasing and is further expected to increase with the emerging 5G applications, such as higher definition videos, the IoT, and augmented/virtual reality applications. As end users consume video in massive amounts and in an increasing number of ways, the content distribution network CDN should be efficiently managed to improve the system efficiency. The streaming service can include multiple caching tiers, at the distributed servers and the edge routers, and efficient content management at these locations affects the quality of experience QoE of the end users. In this paper, we propose a model for video streaming systems, typically composed of a centralized origin server, several CDN sites, and edge-caches located closer to the end user. We comprehensively consider different systems design factors, including the limited caching space at the CDN sites, allocation of CDN for a video request, choice of different ports or paths from the CDN and the central storage, bandwidth allocation, the edge-cache capacity, and the caching policy. We focus on minimizing a performance metric, stall duration tail probability SDTP, and present a novel and efficient algorithm accounting for the multiple design flexibilities. The theoretical bounds with respect to the SDTP metric are also analyzed and presented. The implementation of a virtualized cloud system managed by Openstack demonstrates that the proposed algorithms can significantly improve the SDTP metric compared with the baseline strategies.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {835–847},
numpages = {13}
}

@inproceedings{10.1145/2884781.2884814,
author = {Su, Guoxin and Rosenblum, David S. and Tamburrelli, Giordano},
title = {Reliability of Run-Time Quality-of-Service Evaluation Using Parametric Model Checking},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884814},
doi = {10.1145/2884781.2884814},
abstract = {Run-time Quality-of-Service (QoS) assurance is crucial for business-critical systems. Complex behavioral performance metrics (PMs) are useful but often difficult to monitor or measure. Probabilistic model checking, especially parametric model checking, can support the computation of aggregate functions for a broad range of those PMs. In practice, those PMs may be defined with parameters determined by run-time data. In this paper, we address the reliability of QoS evaluation using parametric model checking. Due to the imprecision with the instantiation of parameters, an evaluation outcome may mislead the judgment about requirement violations. Based on a general assumption of run-time data distribution, we present a novel framework that contains light-weight statistical inference methods to analyze the reliability of a parametric model checking output with respect to an intuitive criterion. We also present case studies in which we test the stability and accuracy of our inference methods and describe an application of our framework to a cloud server management problem.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {73–84},
numpages = {12},
keywords = {run-time evaluation, data distribution, reliability, Quality-of-Service, probabilistic model checking},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/2821357.2821366,
author = {Herbst, Nikolas Roman and Kounev, Samuel and Weber, Andreas and Groenda, Henning},
title = {BUNGEE: An Elasticity Benchmark for Self-Adaptive IaaS Cloud Environments},
year = {2015},
publisher = {IEEE Press},
abstract = {Today's infrastructure clouds provide resource elasticity (i.e. auto-scaling) mechanisms enabling self-adaptive resource provisioning to reflect variations in the load intensity over time. These mechanisms impact on the application performance, however, their effect in specific situations is hard to quantify and compare. To evaluate the quality of elasticity mechanisms provided by different platforms and configurations, respective metrics and benchmarks are required. Existing metrics for elasticity only consider the time required to provision and deprovision resources or the costs impact of adaptations. Existing benchmarks lack the capability to handle open workloads with realistic load intensity profiles and do not explicitly distinguish between the performance exhibited by the provisioned underlying resources, on the one hand, and the quality of the elasticity mechanisms themselves, on the other hand.In this paper, we propose reliable metrics for quantifying the timing aspects and accuracy of elasticity. Based on these metrics, we propose a novel approach for benchmarking the elasticity of Infrastructure-as-a-Service (IaaS) cloud platforms independent of the performance exhibited by the provisioned underlying resources. We show that the proposed metrics provide consistent ranking of elastic platforms on an ordinal scale. Finally, we present an extensive case study of real-world complexity demonstrating that the proposed approach is applicable in realistic scenarios and can cope with different levels of resource efficiency.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {46–56},
numpages = {11},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/3036290.3036321,
author = {L\'{o}pez, Cindy and Heinsen, Rene and Huh, Eui-Nam},
title = {Improving Availability Applying Intelligent Replication in Federated Cloud Storage Based on Log Analysis},
year = {2017},
isbn = {9781450348287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3036290.3036321},
doi = {10.1145/3036290.3036321},
abstract = {This study is focusing on improving the availability of federated storage services in order to provide better quality-of-service (QoS) to the customer with the minimum use of resources. One of the most efficient solutions to get the best experience in the cloud is to combine the services offered. In order for this to happen, there exist different approaches for selecting the best subset of services to reach the optimal performance. However, those works focus on one time selection processes, despite of customer's requirements are continuously changing and demanding adaptable storage service. In this research, I propose a method to improve storage availability through log sentiment analysis and intelligent replication. This methodology is based on the merging of two types of log analysis and the measurement of availability and performance metrics in order to select the best subset of services in cloud storage service federation.},
booktitle = {Proceedings of the 2017 International Conference on Machine Learning and Soft Computing},
pages = {148–153},
numpages = {6},
keywords = {cloud computing, replication, sentiment analysis, log analysis, Federated Cloud Storage, performance, availability, subset selection},
location = {Ho Chi Minh City, Vietnam},
series = {ICMLSC '17}
}

@inproceedings{10.1145/2792745.2792782,
author = {Soltani, Kiumars and Parameswaran, Aditya and Wang, Shaowen},
title = {GeoHashViz: Interactive Analytics for Mapping Spatiotemporal Diffusion of Twitter Hashtags},
year = {2015},
isbn = {9781450337205},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2792745.2792782},
doi = {10.1145/2792745.2792782},
abstract = {Since its birth in 2006, Twitter has evolved to a multi-purpose social media that attracts hundreds of millions of users to share their activities and ideas on a daily basis. The potential of capturing fine-grained activity log of users, combined with ever increasing geographical information derived from GPS-enabled devices, has made Twitter data a valuable source for spatiotemporal analysis of human activities. One of the early innovations of Twitter is the use of hashtag as a unique tagging mechanism to provide additional information about a user post. From its emergence in late 2007, hashtags have been used extensively to express ideas, group tweets and report events among Twitter users. The increasing popularity of hashtags, in addition to their simple and concise structure, has inspired multiple recent studies to propose hashtag as a medium to assess diffusion of ideas in a virtual world. Studying collective effort of users in making a hashtag go viral can shed light on the complex process of idea diffusion that involves psychological, sociological and geographical elements.Although most of the previous research on idea diffusion in virtual world purely focuses on the users social graph, recent studies have confirmed that the spatial relationship among users and regions also play a crucial role in its adoption patterns [1]. This comes back to First Law of Geography that was formulated by Waldo Tobler more than 40 years ago, as "everything is related to everything else, but near things are more related than distant things". However, previous work on designing an interactive visual analytical framework for hashtag diffusion (http://keyhole.co/, http://hashtracking.com/, https://tagboard.com/), lack in-depth spatial analysis capabilities, hence not well-suited to be used for studying diffusion patterns. This research aims to fill this gap by providing an interactive framework to offer visual analytics on geographical diffusion of hashtags over time. Our framework, called GeoHashViz, can provide both textual and visual analytics on the role of location in adoption of hashtags and offer insights on diffusion patterns among different hashtags. GeoHashViz processes large stream of incoming tweets using a Hadoop-based approach and calculates multiple measures that will be used to generate visual analytics for the user. Furthermore, it integrates online maps with a live animation tool to visualize both spatial and temporal diffusion of hashtags at the same time.Data Collection: we gather our data using the Twitter Streaming API (details in [3]).Since we are only interested in common hashtags, which have a certain level of popularity, we only keep the hashtags with more than 1000 appearances. Our unit of spatial resolution is set to cities in United States with a population larger than 60000 people that give us 645 unique locations. These locations will form our reference grid and every geographical point will be assigned to its nearest neighbor in the reference grid.Analytics: To formulate the problem of spatiotemporal analysis of hashtag diffusion, we recognized two main categories of hashtag-based and location-based analytics. In hashtag-based analytics we focus on specific hashtags and their associated diffusion patterns. On the other hand, location-based analytics study the similarity and closeness of locations in terms of their hashtag adoption. To evaluate the usability of the framework, we identify five core analytical features that cover wide ranges of research questions. However, our framework can be easily extended to include more analytical features. The five visual analytical capabilities are listed in Table 1. Spread and focus points (locations with highest occurrence of the hashtag [1]) provide users with a visual estimate of how the hashtag is diffused over time. However, we also provided four metrics that gives a user a more concrete sense of the diffusion patterns: a) Entropy: Measures the randomness of hashtag distribution [1] ;b) KL-divergence: Compare the geographical distribution of hashtag in consecutive time windows using KL-divergence method ;c) Spatial Dispersion: Measures how scattered is the hashtag from its geographical midpoint ;d) Count:. Plot the cumulative count of the hashtag over time.For location-based analytics we included two functions. Top-k hashtags calculate the most popular hashtags in a region and visualize that using a word cloud. However by simply looking at the counts, we may miss some locally significant due to their relative low count. To reduce the dominance of globally popular hashtags, we introduce another analytic that will visualize top-k locally significant hashtags. This analytic uses a Tf-idf like metric [5] to measure the local popularity of a hashtag in a specific region, hence assigning lower rank to the hashtags which are popular in other places as well. In addition, we provide two metrics for comparing two different regions in terms of hashtag adoption: a) Jaccard Similarity Compare the set of hashtag used in two different regions, with higher number assigned to more similar regions ;b) Adoption Lag This measure depicts how long it takes for a hashtag to travel between two region, by averaging the time difference between the first appearance of hashtags in two regions.Architecture: GeoHashViz framework follows a two-layer architecture: an offline-processing module and an interactive module. The offline-processing module, implemented entirely in Apache Hadoop and called periodically, processes the raw data and pre-computes measures related to spatiotemporal diffusion of hashtags. The interactive module on the other hand is called on demand and based on user requests. The two modules connect with each other through a distributed MongoDB database. The two-layer architecture enables a fast interactive final framework by reducing the data processing that interactive module is required to do.In the offline-processing module, significant hashtags are extracted and the points are laid on the geographical mesh that we defined above. Then two MapReduce jobs are executed: one for pre-computing measures related to hashtag-based analytics and one for location-based analytics. All the Hadoop experiments were conducted using XSEDE Gordon Hadoop cluster. The data-intensive nature of our problem, requiring aggregation of large number of tweets based on both hashtags and locations, make Hadoop an ideal choice for the offline-processing module. Using Hadoop, we distribute the tweets into multiple nodes, and then take advantage of MapReduce model to aggregate them based on their associated location on the mesh and their included hashtags. In the reduce step, having access to all the tweets for a certain location/hashtag, we can generate the analytics for different timestamps. In addition, since the nodes on Gordon Hadoop cluster have relatively high memory, we are able to store the geographical mesh in memory and quickly map the location of users to their closest point on the mesh (using kd-tree). The same technique is employed in the interactive module to find the set of mesh points which lies into the user-defined bounding box.The interactive module includes a web application and a Java Servlet. The web application is integrated into Cyber-GIS Gateway [2] to increase usability of the application and easier integration with other CyberGIS applications. Figure 1 shows a view of the application visualizing top 20 hashtags in the southern California region in September 2014.},
booktitle = {Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure},
articleno = {37},
numpages = {2},
keywords = {CyberGIS, interactive visualization, GeoHashViz, social media, Hadoop},
location = {St. Louis, Missouri},
series = {XSEDE '15}
}

@inproceedings{10.1145/2663165.2663333,
author = {Raghavan, Ajaykrishna and Chandra, Abhishek and Weissman, Jon B.},
title = {Tiera: Towards Flexible Multi-Tiered Cloud Storage Instances},
year = {2014},
isbn = {9781450327855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663165.2663333},
doi = {10.1145/2663165.2663333},
abstract = {Cloud providers offer an array of storage services that represent different points along the performance, cost, and durability spectrum. If an application desires the composite benefits of multiple storage tiers, then it must manage the complexity of different interfaces to these storage services and their diverse policies. We believe that it is possible to provide the benefits of customized tiered cloud storage to applications without compromising simplicity using a lightweight middleware. In this paper, we introduce Tiera, a middleware that enables the provision of multi-tiered cloud storage instances that are easy to specify, flexible, and enable a rich array of storage policies and desired metrics to be realized. Tiera's novelty lies in the first-class support for encapsulated tiered cloud storage, ease of programmability of data management policies, and support for runtime replacement and addition of policies and tiers. Tiera enables an application to realize a desired metric (e.g., low latency or low cost) by selecting different storage services that constitute a Tiera instance, and easily specifying a policy, using event and response pairs, to manage the life cycle of data stored in the instance. We illustrate the benefits of Tiera through a prototype implemented on the Amazon cloud. By deploying unmodified MySQL database engine and a TPC-W Web bookstore application on Tiera, we are able to improve their respective throughputs by 47\% -- 125\% and 46\% -- 69\%, over standard deployments. We further show the flexibility of Tiera in achieving different desired application metrics with minimal overhead.},
booktitle = {Proceedings of the 15th International Middleware Conference},
pages = {1–12},
numpages = {12},
location = {Bordeaux, France},
series = {Middleware '14}
}

@inproceedings{10.1145/3366423.3380111,
author = {Ma, Meng and Xu, Jingmin and Wang, Yuan and Chen, Pengfei and Zhang, Zonghua and Wang, Ping},
title = {AutoMAP: Diagnose Your Microservice-Based Web Applications Automatically},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380111},
doi = {10.1145/3366423.3380111},
abstract = {The high complexity and dynamics of the microservice architecture make its application diagnosis extremely challenging. Static troubleshooting approaches may fail to obtain reliable model applies for frequently changing situations. Even if we know the calling dependency of services, we lack a more dynamic diagnosis mechanism due to the existence of indirect fault propagation. Besides, algorithm based on single metric usually fail to identify the root cause of anomaly, as single type of metric is not enough to characterize the anomalies occur in diverse services. In view of this, we design a novel tool, named AutoMAP, which enables dynamic generation of service correlations and automated diagnosis leveraging multiple types of metrics. In AutoMAP, we propose the concept of anomaly behavior graph to describe the correlations between services associated with different types of metrics. Two binary operations, as well as a similarity function on behavior graph are defined to help AutoMAP choose appropriate diagnosis metric in any particular scenario. Following the behavior graph, we design a heuristic investigation algorithm by using forward, self, and backward random walk, with an objective to identify the root cause services. To demonstrate the strengths of AutoMAP, we develop a prototype and evaluate it in both simulated environment and real-work enterprise cloud system. Experimental results clearly indicate that AutoMAP achieves over 90\% precision, which significantly outperforms other selected baseline methods. AutoMAP can be quickly deployed in a variety of microservice-based systems without any system knowledge. It also supports introduction of various expert knowledge to improve accuracy.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {246–258},
numpages = {13},
keywords = {web application, cloud computing, Microservice architecture, root cause, anomaly diagnosis},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1109/UCC.2014.87,
author = {Ali-Eldin, Ahmed and Seleznjev, Oleg and Sj\"{o}stedt-de Luna, Sara and Tordsson, Johan and Elmroth, Erik},
title = {Measuring Cloud Workload Burstiness},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.87},
doi = {10.1109/UCC.2014.87},
abstract = {Workload burstiness and spikes are among the main reasons for service disruptions and decrease in the Quality-of-Service (QoS) of online services. They are hurdles that complicate autonomic resource management of data enters. In this paper, we review the state-of-the-art in online identification of workload spikes and quantifying burstiness. The applicability of some of the proposed techniques is examined for Cloud systems where various workloads are co-hosted on the same platform. We discuss Sample Entropy (Samp En), a measure used in biomedical signal analysis, as a potential measure for burstiness. A modification to the original measure is introduced to make it more suitable for Cloud workloads.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {566–572},
numpages = {7},
series = {UCC '14}
}

@article{10.1145/2853073.2853085,
author = {Baliyan, Niyati and Kumar, Sandeep},
title = {A Hierarchical Fuzzy System for Quality Assessment of Semantic Web Application as a Service},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853085},
doi = {10.1145/2853073.2853085},
abstract = {Semantic Web enabled applications are becoming popular due to the presence of their machine comprehensible description, which makes them easily sharable across machines. If such applications are deployed as services to the user through the Cloud, they can facilitate transparency and reusability. There exist no attributes, metrics, or models for monitoring the quality of such applications. In the current work, a hierarchical fuzzy system for quality assessment of Semantic Web based applications delivered as services on the Cloud, is proposed. The quality attributes proposed herein have been validated through the standard IEEE-1061 validation framework. Experimental results reveal that the proposed hierarchical fuzzy system handles the multiplicity of quality attributes, and can be used for the relative ranking of Semantic Web applications available as services},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {1–7},
numpages = {7},
keywords = {Cloud, Fuzzy Logic, Quality Metrics, Semantic Web}
}

@inproceedings{10.1145/3328886.3328892,
author = {Alcivar, Nayeth I. Solorzano and Gallego, Diego Carrera and Quijije, Lissenia Sornoza and Quelal, Marco Mendoza},
title = {Developing a Dashboard for Monitoring Usability of Educational Games Apps for Children},
year = {2019},
isbn = {9781450361682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328886.3328892},
doi = {10.1145/3328886.3328892},
abstract = {Nowadays digital game applications or interactive children's educational games implemented in mobile devices (to be identified as Apps), are beginning to be widely used to complement children's education, particularly during early childhood education. However, digital game Apps do not generate a timely collection of data that could be obtained, so that with a proper interpretation they can serve as a guide in making decisions about the content, types, and level of games that should be created as digital tools to support children's education. In this article, is indicated how through the development of a dashboard, linked to a database in the cloud, it is possible to obtain and present information that allows measuring the use and playability and usability factors for these types of Apps, in an orderly and precise manner. For the development of the dashboard and its link in real time with the Apps to monitor, JavaScript was used through the framework Sails.js and the database implemented in PostgreSQL. In parallel, for the data transmission tests, two mobile applications were implemented in Android, one programmed in Unity and the second using Adobe Animate. Both Apps were designed by recording internal data in JSON file format. To analyze and obtain results, we used PQM metrics 2014 (Playability Quality Model), and we applied an adapted theory which helps to facilitate the identification of factors affecting the use and adoption of information systems and technologies in Latin American local contexts. The Pilot tests were carried out with children from 4 to 8 years attending schools of marginal areas in the city of Guayaquil, Ecuador. These children with little knowledge of technology use, facilitate better evaluation of different scenarios to measure the behavioral use of the Apps and their contents without significant influence of previous knowledge about digital educational games. This article presents the first results of an extensive and longitudinal multidisciplinary research, relevant to organizations and people involved in early childhood education.},
booktitle = {Proceedings of the 2019 2nd International Conference on Computers in Management and Business},
pages = {70–75},
numpages = {6},
keywords = {Apps, Latin America, MIDI, Ecuador, Digital Games, Dashboard, Usability, Technology Adoption and Education, Children},
location = {Cambridge, United Kingdom},
series = {ICCMB '19}
}

@inproceedings{10.1145/2790060.2790071,
author = {Legrand, H\'{e}l\`{e}ne and Boubekeur, Tamy},
title = {Morton Integrals for High Speed Geometry Simplification},
year = {2015},
isbn = {9781450337076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790060.2790071},
doi = {10.1145/2790060.2790071},
abstract = {Real time geometry processing has progressively reached a performance level that makes a number of signal-inspired primitives practical for on-line applications scenarios. This often comes through the joint design of operators, data structure and even dedicated hardware. Among the major classes of geometric operators, filtering and super-sampling (via tessellation) have been successfully expressed under high-performance constraints. The subsampling operator i.e., adaptive simplification, remains however a challenging case for non-trivial input models. In this paper, we build a fast geometry simplification algorithm over a new concept: Morton Integrals. By summing up quadric error metric matrices along Morton-ordered surface samples, we can extract concurrently the nodes of an adaptive cut in the so-defined implicit hierarchy, and optimize all simplified vertices in parallel. This approach is inspired by integral images and exploits recent advances in high performance spatial hierarchy construction and traversal. As a result, our GPU implementation can downsample a mesh made of several millions of polygons at interactive rates, while providing better quality than uniform simplification and preserving important salient features. We present results for surface meshes, polygon soups and point clouds, and discuss variations of our approach to account for per-sample attributes and alternatives error metrics.},
booktitle = {Proceedings of the 7th Conference on High-Performance Graphics},
pages = {105–112},
numpages = {8},
keywords = {GPU algorithms, Morton code, mesh simplification, adaptive clustering},
location = {Los Angeles, California},
series = {HPG '15}
}

@inproceedings{10.1109/CCGrid.2014.103,
author = {Wu, Jie and Jansen, Christoph and Beier, Maximilian and Witt, Michael and Krefting, Dagmar},
title = {Extending XNAT towards a Cloud-Based Quality Assessment Platform for Retinal Optical Coherence Tomographies},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.103},
doi = {10.1109/CCGrid.2014.103},
abstract = {Neurosciencific research is increasingly based on image analysis methods. Large sets of imaging data are processed using complex image analysis tools. While today magnetic resonance imaging (MRI) is widely used for both functional and anatomical analysis of the human brain, new imaging modalities are beginning to prove their capabilities for neurological research. Among them, optical coherence tomography (OCT) allows for noninvasive visualization of anatomical structures on a micrometer scale. Becoming a standard diagnostic tool in ophthalmology, it is of rising interest for neurological research. Crucial to all data analysis methods is the quality of the input data. The platform presented in this paper is designed for automatic quality assessment of retinal OCTs. It extends the image management platform XNAT by services to calculate and store quality measures. It is also extensible regarding new quality measure algorithms, allowing the developer to upload Matlab code, compile it for the infrastructure's hardware architecture and test it in the system. The image processing tools to calculate the quality measures are provided as a cloud-based service employing OpenStack as underlying IT infrastructure. The prototype implementation encompassing security and performance aspects are presented.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {764–773},
numpages = {10},
keywords = {SaaS, XNAT, medical imaging, cloud, IaaS, neuroimaging, OCT},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/2739482.2764720,
author = {Oprescu, Ana-Maria and (Vintila) Filip, Alexandra and Kielmann, Thilo},
title = {Fast Pareto Front Approximation for Cloud Instance Pool Optimization},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764720},
doi = {10.1145/2739482.2764720},
abstract = {Computing the Pareto Set (PS) of optimal cloud schedules in terms of cost and makespan for a given application and set of cloud instance types is NP-complete. Moreover, cloud instances' volatility requires fast PS recomputations. While genetic algorithms (GA) are a promising approach, little knowledge of an approximated PS's quality leads to GAs running for overly many generations, contradicting the goal of quickly computing an approximate solution. We address this with MOO-GA, our GA enhanced with a domain-tailored termination criteria delivering fast, well-approximated Pareto sets. We compare to NSGAIII using PS convergence and diversity, and computational effort metrics. Results show MOO-GA consistently computing better quality Pareto sets within one second on average (df=98, p-value&lt;10-3).},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1443–1444},
numpages = {2},
keywords = {pareto frontier, infrastructure-as-a-service, genetic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3361525.3361543,
author = {Grohmann, Johannes and Nicholson, Patrick K. and Iglesias, Jesus Omana and Kounev, Samuel and Lugones, Diego},
title = {Monitorless: Predicting Performance Degradation in Cloud Applications with Machine Learning},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361543},
doi = {10.1145/3361525.3361543},
abstract = {Today, software operation engineers rely on application key performance indicators (KPIs) for sizing and orchestrating cloud resources dynamically. KPIs are monitored to assess the achievable performance and to configure various cloud-specific parameters such as flavors of instances and autoscaling rules, among others. Usually, keeping KPIs within acceptable levels requires application expertise which is expensive and can slow down the continuous delivery of software. Expertise is required because KPIs are normally based on application-specific quality-of-service metrics, like service response time and processing rate, instead of generic platform metrics, like those typical across various environments (e.g., CPU and memory utilization, I/O rate, etc.)In this paper, we investigate the feasibility of outsourcing the management of application performance from developers to cloud operators. In the same way that the serverless paradigm allows the execution environment to be fully managed by a third party, we discuss a monitorless model to streamline application deployment by delegating performance management. We show that training a machine learning model with platform-level data, collected from the execution of representative containerized services, allows inferring application KPI degradation. This is an opportunity to simplify operations as engineers can rely solely on platform metrics -- while still fulfilling application KPIs -- to configure portable and application agnostic rules and other cloud-specific parameters to automatically trigger actions such as autoscaling, instance migration, network slicing, etc.Results show that monitorless infers KPI degradation with an accuracy of 97\% and, notably, it performs similarly to typical autoscaling solutions, even when autoscaling rules are optimally tuned with knowledge of the expected workload.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {149–162},
numpages = {14},
keywords = {Machine learning, DevOps, Cloud computing, Monitoring},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/3194124.3194130,
author = {Shatnawi, Anas and Orr\`{u}, Matteo and Mobilio, Marco and Riganelli, Oliviero and Mariani, Leonardo},
title = {Cloudhealth: A Model-Driven Approach to Watch the Health of Cloud Services},
year = {2018},
isbn = {9781450357302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194124.3194130},
doi = {10.1145/3194124.3194130},
abstract = {Cloud systems are complex and large systems where services provided by different operators must coexist and eventually cooperate. In such a complex environment, controlling the health of both the whole environment and the individual services is extremely important to timely and effectively react to misbehaviours, unexpected events, and failures. Although there are solutions to monitor cloud systems at different granularity levels, how to relate the many KPIs that can be collected about the health of the system and how health information can be properly reported to operators are open questions.This paper reports the early results we achieved in the challenge of monitoring the health of cloud systems. In particular we present CloudHealth, a model-based health monitoring approach that can be used by operators to watch specific quality attributes. The Cloud-Health Monitoring Model describes how to operationalize high level monitoring goals by dividing them into subgoals, deriving metrics for the subgoals, and using probes to collect the metrics. We use the CloudHealth Monitoring Model to control the probes that must be deployed on the target system, the KPIs that are dynamically collected, and the visualization of the data in dashboards.},
booktitle = {Proceedings of the 1st International Workshop on Software Health},
pages = {40–47},
numpages = {8},
keywords = {monitoring model, cloud service, KPI, software health, monitoring, metrics, quality model},
location = {Gothenburg, Sweden},
series = {SoHeal '18}
}

@inproceedings{10.1145/2964284.2973806,
author = {Mekuria, Rufael and Cesar, Pablo},
title = {MP3DG-PCC, Open Source Software Framework for Implementation and Evaluation of Point Cloud Compression},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973806},
doi = {10.1145/2964284.2973806},
abstract = {We present MP3DG-PCC, an open source framework for design, implementation and evaluation of point cloud compression algorithms. The framework includes objective quality metrics, lossy and lossless anchor codecs, and a test bench for consistent comparative evaluation. The framework and proposed methodology is in use for the development of an international point cloud compression standard in MPEG. In addition, the library is integrated with the popular point cloud library, making a large number of point cloud processing available and aligning the work with the broader open source community.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1222–1226},
numpages = {5},
keywords = {evaluation, point cloud compression, 3d virtual reality, compression},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.1145/3284360,
author = {Dey, Tamal K. and Shi, Dayu and Wang, Yusu},
title = {SimBa: An Efficient Tool for Approximating Rips-Filtration Persistence via Simplicial Batch Collapse},
year = {2019},
issue_date = {2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
issn = {1084-6654},
url = {https://doi.org/10.1145/3284360},
doi = {10.1145/3284360},
abstract = {In topological data analysis, a point cloud data P extracted from a metric space is often analyzed by computing the persistence diagram or barcodes of a sequence of Rips complexes built on P indexed by a scale parameter. Unfortunately, even for input of moderate size, the size of the Rips complex may become prohibitively large as the scale parameter increases. Starting with the Sparse Rips filtration introduced by Sheehy, some existing methods aim to reduce the size of the complex to improve time efficiency as well. However, as we demonstrate, existing approaches still fall short of scaling well, especially for high-dimensional data. In this article, we investigate the advantages and limitations of existing approaches. Based on insights gained from the experiments, we propose an efficient new algorithm, called SimBa, for approximating the persistent homology of Rips filtrations with quality guarantees. Our new algorithm leverages a batch-collapse strategy as well as a new Sparse Rips-like filtration. We experiment on a variety of low- and high-dimensional datasets. We show that our strategy presents a significant size reduction and that our algorithm for approximating Rips filtration persistence is an order of magnitude faster than existing methods in practice.},
journal = {ACM J. Exp. Algorithmics},
month = {jan},
articleno = {1.5},
numpages = {16},
keywords = {simplicial maps, rips filtration, persistent homology, approximation, Topological data analysis}
}

@inproceedings{10.1145/2996890.2996906,
author = {Chhetri, Mohan Baruwal and Vo, Quoc Bao and Kowalczyk, Ryszard},
title = {CL-SLAM: Cross-Layer SLA Monitoring Framework for Cloud Service-Based Applications},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.2996906},
doi = {10.1145/2996890.2996906},
abstract = {Modern applications are increasingly being composed from multiple components that require and consume services at different layers of the cloud stack. The diverse, dynamic and unpredictable nature of both cloud services and application workloads makes quality-assured provision of such cloud service-based applications (CSBAs) a major challenge. While elasticity and autoscaling gives CSBA providers the ability to scale cloud resources on-demand, they require a comprehensive, system-wide view of the application performance in order to make timely, cost-effective and performance-efficient scaling decisions. In this paper, we propose, develop and validate CL-SLAM - a Cross-Layer SLA Monitoring Framework for CSBAs. Its main features include (a) realtime, fine-grained visibility into CSBA performance, (b) visual descriptive analytics to identify correlations and inter-dependencies between cross-layer performance metrics, (c) temporal profiling of CSBA performance, (d) proactive monitoring, detection and root-cause analysis of SLA violation, and (e) support for both reactive and proactive adaptation in support of quality-assured CSBA provision. We validate our approach through a prototype implementation.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {30–36},
numpages = {7},
keywords = {cross-layer SLA monitoring, cloud service-based application},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/3423328.3423497,
author = {Li, Yen-Chun and Hsu, Chia-Hsin and Lin, Yu-Chun and Hsu, Cheng-Hsin},
title = {Performance Measurements on a Cloud VR Gaming Platform},
year = {2020},
isbn = {9781450381581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423328.3423497},
doi = {10.1145/3423328.3423497},
abstract = {As cloud gaming and Virtual Reality (VR) games become popular in the game industry, game developers engage in these fields to boost their sales. Because cloud gaming possesses the merit of lifting computation loads from client devices to servers, it solves the high resource consumption issue of VR games on regular clients. However, it is important to know where is the bottleneck of the cloud VR gaming platform and how can it be improved in the future. In this paper, we conduct extensive experiments on the state-of-the-art cloud VR gaming platform--Air Light VR (ALVR). In particular, we analyze the performance of ALVR using both Quality-of-Service and Quality-of-Experience metrics. Our experiments reveal that latency (up to 90 ms RTT) has less influence on user experience compared to bandwidth limitation (as small as 35 Mbps) and packet loss rate (as high as 8\%) . Moreover, we find that VR gamers can hardly notice the difference between the gaming experience with different latency values (between 0 and 90 ms RTT). Such findings shed some lights on how to further improve the cloud VR gaming platform, e.g., a budget of up to 90 ms RTT may be used to absorb network dynamics when bandwidth is insufficient.},
booktitle = {Proceedings of the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications},
pages = {37–45},
numpages = {9},
keywords = {cloud computing, measurement, computer games, virtual reality, prototype},
location = {Seattle, WA, USA},
series = {QoEVMA'20}
}

@inproceedings{10.1145/2774993.2775063,
author = {Sun, Peng and Vanbever, Laurent and Rexford, Jennifer},
title = {Scalable Programmable Inbound Traffic Engineering},
year = {2015},
isbn = {9781450334518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2774993.2775063},
doi = {10.1145/2774993.2775063},
abstract = {With the rise of video streaming and cloud services, enterprise and access networks receive much more traffic than they send, and must rely on the Internet to offer good end-to-end performance. These edge networks often connect to multiple ISPs for better performance and reliability, but have only limited ways to influence which of their ISPs carries the traffic for each service. In this paper, we present Sprite, a software-defined solution for flexible inbound traffic engineering (TE). Sprite offers direct, fine-grained control over inbound traffic, by announcing different public IP prefixes to each ISP, and performing source network address translation (SNAT) on outbound request traffic. Our design achieves scalability in both the data plane (by performing SNAT on edge switches close to the clients) and the control plane (by having local agents install the SNAT rules). The controller translates high-level TE objectives, based on client and server names, as well as performance metrics, to a dynamic network policy based on real-time traffic and performance measurements. We evaluate Sprite with live data from "in the wild" experiments on an EC2-based testbed, and demonstrate how Sprite dynamically adapts the network policy to achieve high-level TE objectives, such as balancing YouTube traffic among ISPs to improve video quality.},
booktitle = {Proceedings of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research},
articleno = {12},
numpages = {7},
keywords = {software-defined networking, scalability, traffic engineering},
location = {Santa Clara, California},
series = {SOSR '15}
}

@inproceedings{10.1145/3308560.3317075,
author = {Debattista, Jeremy and Attard, Judie and Brennan, Rob and O'Sullivan, Declan},
title = {Is the LOD Cloud at Risk of Becoming a Museum for Datasets? Looking Ahead towards a Fully Collaborative and Sustainable LOD Cloud},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317075},
doi = {10.1145/3308560.3317075},
abstract = {The Linked Open Data (LOD) cloud has been around since 2007. Throughout the years, this prominent depiction served as the epitome for Linked Data and acted as a starting point for many. In this article we perform a number of experiments on the dataset metadata provided by the LOD cloud, in order to understand better whether the current visualised datasets are accessible and with an open license. Furthermore, we perform quality assessment of 17 metrics over accessible datasets that are part of the LOD cloud. These experiments were compared with previous experiments performed on older versions of the LOD cloud. The results showed that there was no improvement on previously identified problems. Based on our findings, we therefore propose a strategy and architecture for a potential collaborative and sustainable LOD cloud.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {850–858},
numpages = {9},
keywords = {Linked Data, LOD cloud, metadata quality, data quality, sustainable services},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3344341.3368796,
author = {Kuhlenkamp, J\"{o}rn and Werner, Sebastian and Borges, Maria C. and El Tal, Karim and Tai, Stefan},
title = {An Evaluation of FaaS Platforms as a Foundation for Serverless Big Data Processing},
year = {2019},
isbn = {9781450368940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344341.3368796},
doi = {10.1145/3344341.3368796},
abstract = {Function-as-a-Service (FaaS), offers a new alternative to operate cloud-based applications. FaaS platforms enable developers to define their application only through a set of service functions, relieving them of infrastructure management tasks, which are executed automatically by the platform. Since its introduction, FaaS has grown to support workloads beyond the lightweight use-cases it was originally intended for, and now serves as a viable paradigm for big data processing. However, several questions regarding FaaS platform quality are still unanswered. Specifically, the impact of automatic infrastructure management on serverless big data applications remains unexplored.In this paper, we propose a novel evaluation method (SIEM) to understand the impact of these tasks. For this purpose, we introduce new metrics to quantify quality in different big data application scenarios. We show an application of SIEM by evaluating the four major FaaS providers, and contribute results and new insights for FaaS-based big data processing.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
pages = {1–9},
numpages = {9},
keywords = {serverless, benchmarking, cloud computing, big data processing},
location = {Auckland, New Zealand},
series = {UCC'19}
}

@inproceedings{10.1145/3307630.3342385,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {HADAS: Analysing Quality Attributes of Software Configurations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342385},
doi = {10.1145/3307630.3342385},
abstract = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses of SPLs rely on solvers to navigate complex dependencies among features and find legal solutions. Variability analysis tools are complex due to the diversity of products and domain-specific knowledge. On that, while there are experimental studies that analyse quality attributes, the knowledge is not easily accessible for developers, and its appliance is not trivial. Aiming to allow the industry to quality-explore SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects quality attributes metrics in a cloud repository, and (2) reasons about it helping developers with quality attributes requirements.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {model, variability, attribute, software product line, NFQA, numerical},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3038912.3052560,
author = {Haq, Osama and Raja, Mamoon and Dogar, Fahad R.},
title = {Measuring and Improving the Reliability of Wide-Area Cloud Paths},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052560},
doi = {10.1145/3038912.3052560},
abstract = {Many popular cloud applications use inter-data center paths; yet, little is known about the characteristics of these ``cloud paths''. Over an eighteen month period, we measure the inter-continental cloud paths of three providers (Amazon, Google, and Microsoft) using client side (VM-to-VM) measurements. We find that cloud paths are more predictable compared to public Internet paths, with an order of magnitude lower loss rate and jitter at the tail (95th percentile and beyond) compared to public Internet paths. We also investigate the nature of packet losses on these paths (e.g., random vs. bursty) and potential reasons why these paths may be better in quality. Based on our insights, we consider how we can further improve the quality of these paths with the help of existing loss mitigation techniques. We demonstrate that using the cloud path in conjunction with a detour path can mask most of the cloud losses, resulting in up to five 9's of network availability for applications.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {253–262},
numpages = {10},
keywords = {latency, cloud paths reliability, bandwidth, detour routing, inter-data center networks, cloud availability, loss rate},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.1145/2816795.2818061,
author = {Liao, Jing and Finch, Mark and Hoppe, Hugues},
title = {Fast Computation of Seamless Video Loops},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2816795.2818061},
doi = {10.1145/2816795.2818061},
abstract = {Short looping videos concisely capture the dynamism of natural scenes. Creating seamless loops usually involves maximizing spatiotemporal consistency and applying Poisson blending. We take an end-to-end view of the problem and present new techniques that jointly improve loop quality while also significantly reducing processing time. A key idea is to relax the consistency constraints to anticipate the subsequent blending, thereby enabling looping of low-frequency content like moving clouds and changing illumination. We also analyze the input video to remove an undesired bias toward short loops. The quality gains are demonstrated visually and confirmed quantitatively using a new gradient-domain consistency metric. We improve system performance by classifying potentially loopable pixels, masking the 2D graph cut, pruning graph-cut labels based on dominant periods, and optimizing on a coarse grid while retaining finer detail. Together these techniques reduce computation times from tens of minutes to nearly real-time.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {197},
numpages = {10},
keywords = {blend-aware consistency, cinemagraphs, video textures}
}

@inproceedings{10.1145/2822332.2822339,
author = {Evans, Kieran and Jones, Andrew and Preece, Alun and Quevedo, Francisco and Rogers, David and Spasi\'{c}, Irena and Taylor, Ian and Stankovski, Vlado and Taherizadeh, Salman and Trnkoczy, Jernej and Suciu, George and Suciu, Victor and Martin, Paul and Wang, Junchao and Zhao, Zhiming},
title = {Dynamically Reconfigurable Workflows for Time-Critical Applications},
year = {2015},
isbn = {9781450339896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2822332.2822339},
doi = {10.1145/2822332.2822339},
abstract = {Cloud-based applications that depend on time-critical data processing or network throughput require the capability of reconfiguring their infrastructure on demand as and when conditions change. Although the ability to apply quality of service constraints on the current Cloud offering is limited, there are ongoing efforts to change this. One such effort is the European funded SWITCH project that aims to provide a programming model and toolkit to help programmers specify quality of service and quality of experience metrics of their distributed application and to provide the means to specify the reconfiguration actions which can be taken to maintain these requirements. In this paper, we present an approach to application reconfiguration by applying a workflow methodology to implement a prototype involving multiple reconfiguration scenarios of a distributed real-time social media analysis application, called Sentinel. We show that by using a lightweight RPC-based workflow approach, we can monitor a live application in real time and spawn dependency-based workflows to reconfigure the underlying Docker containers that implement the distributed components of the application. We propose to use this prototype as the basis for part of the SWITCH workbench, which will support more advanced programmable infrastructures.},
booktitle = {Proceedings of the 10th Workshop on Workflows in Support of Large-Scale Science},
articleno = {7},
numpages = {10},
keywords = {workflows, quality of service, time-critical applications, quality of experience, dynamic data driven systems},
location = {Austin, Texas},
series = {WORKS '15}
}

@inproceedings{10.1145/3173162.3173207,
author = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A. and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark},
title = {Vbench: Benchmarking Video Transcoding in the Cloud},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173207},
doi = {10.1145/3173162.3173207},
abstract = {This paper presents vbench, a publicly available benchmark for cloud video services. We are the first study, to the best of our knowledge, to characterize the emerging video-as-a-service workload. Unlike prior video processing benchmarks, vbench's videos are algorithmically selected to represent a large commercial corpus of millions of videos. Reflecting the complex infrastructure that processes and hosts these videos, vbench includes carefully constructed metrics and baselines. The combination of validated corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and compression. We demonstrate the importance of video selection with a microarchitectural study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial corpus that are not visible in other video corpuses. Our experiments with GPUs under vbench's scoring scenarios reveal that context is critical: GPUs are well suited for live-streaming, while for video-on-demand shift costs from compute to storage and network. Counterintuitively, they are not viable for popular videos, for which highly compressed, high quality copies are required. We instead find that popular videos are currently well-served by the current trajectory of software encoders.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {797–809},
numpages = {13},
keywords = {benchmark, accelerator, video transcoding},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}

@article{10.1145/3296957.3173207,
author = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A. and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark},
title = {Vbench: Benchmarking Video Transcoding in the Cloud},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3173207},
doi = {10.1145/3296957.3173207},
abstract = {This paper presents vbench, a publicly available benchmark for cloud video services. We are the first study, to the best of our knowledge, to characterize the emerging video-as-a-service workload. Unlike prior video processing benchmarks, vbench's videos are algorithmically selected to represent a large commercial corpus of millions of videos. Reflecting the complex infrastructure that processes and hosts these videos, vbench includes carefully constructed metrics and baselines. The combination of validated corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and compression. We demonstrate the importance of video selection with a microarchitectural study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial corpus that are not visible in other video corpuses. Our experiments with GPUs under vbench's scoring scenarios reveal that context is critical: GPUs are well suited for live-streaming, while for video-on-demand shift costs from compute to storage and network. Counterintuitively, they are not viable for popular videos, for which highly compressed, high quality copies are required. We instead find that popular videos are currently well-served by the current trajectory of software encoders.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {797–809},
numpages = {13},
keywords = {accelerator, benchmark, video transcoding}
}

@inproceedings{10.1145/2666310.2666376,
author = {Qamar, Ahmad M. and Afyouni, Imad and Rahman, Md. Abdur and Rehman, Faizan Ur and Hussain, Delwar and Basalamah, Saleh and Lbath, Ahmed},
title = {A GIS-Based Serious Game Interface for Therapy Monitoring},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666376},
doi = {10.1145/2666310.2666376},
abstract = {In this paper, we present a novel idea of a map-based therapy environment for people with Hemiplegia. The therapy environment is designed according to the suggestions of therapists, which consists of a spatial map browsing serious game augmented with our novel multi-sensory natural user interface (NUI). The NUI is based on 3D motion sensors that can recognize different hand and body gestures used for browsing a 3D or 2D map. The 3D motion sensors work in a non-invasive way; hence, they do not require any wearable body attachments and can be used at home without assistance from the therapists. The map-browsing environment provides an immersive experience to the disabled users, which helps in performing therapy in an interesting and entertaining manner. We have developed analytics for measuring certain quality of health improvement metrics from each type of spatial map browsing movements. The 3D motion sensors have been tested with Nokia, Google, ESRI, and a number of other maps that allow a subject to visualize and browse the 3D and 2D maps of the world. The map browsing session data shows the nature of big data; hence, the session data is stored in a cloud environment. Our developed serious game environment is web-based; thus anyone having the appropriate low cost sensor hardware can plug it in and start experiencing a natural way of hands free map browsing. We have deployed our framework in a hospital that treats Hemiplegic patients. Based on the feedback obtained, the developed platform shows a huge potential for use in hospitals that provide physiotherapy services as well as at patients' home as an assistive therapeutic service.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {589–592},
numpages = {4},
keywords = {kinect, serious games, GIS, e-health, therapy, leap},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/3380851.3416742,
author = {Berger, Arthur},
title = {Designing an Analytics Approach for Technical Content},
year = {2020},
isbn = {9781450375252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380851.3416742},
doi = {10.1145/3380851.3416742},
abstract = {Working on an enterprise cloud product, my documentation team rethought our approach to content analytics. Despite a variety of tools and awareness of industry best practices, my team felt stuck using analytics only in annual or on-demand reports to management, instead of to produce value for our end users. We employed Design Thinking practices to guide a multifaceted user research project that led to changes in the way that we created documentation and automated quality content checks. Key takeaways include to involve the technical documentation team in identifying not only what metrics to collect, but also how to collect, report, and use the metrics in order to increase buy-in and the likelihood that data analytics about content leads to meaningful change within the content itself.},
booktitle = {Proceedings of the 38th ACM International Conference on Design of Communication},
articleno = {7},
numpages = {5},
keywords = {content strategy, design thinking,, Data analytics},
location = {Denton, TX, USA},
series = {SIGDOC '20}
}

@inproceedings{10.1109/CCGRID.2018.00021,
author = {Imai, Shigeru and Patterson, Stacy and Varela, Carlos A.},
title = {Uncertainty-Aware Elastic Virtual Machine Scheduling for Stream Processing Systems},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00021},
doi = {10.1109/CCGRID.2018.00021},
abstract = {Stream processing systems deployed on the cloud need to be elastic to effectively accommodate workload variations over time. Performance models can predict maximum sustainable throughput (MST) as a function of the number of VMs allocated. We present a scheduling framework that incorporates three statistical techniques to improve Quality of Service (QoS) of cloud stream processing systems: (i) uncertainty quantification to consider variance in the MST model; (ii) online learning to update MST model as new performance metrics are gathered; and (iii) workload models to predict input data stream rates assuming regular patterns occur over time. Our framework can be parameterized by a QoS satisfaction target that statistically finds the best performance/cost tradeoff. Our results illustrate that each of the three techniques alone significantly improves QoS, from 52\% to 73-81\% QoS satisfaction rates on average for eight benchmark applications. Furthermore, applying all three techniques allows us to reach 98.62\% QoS satisfaction rate with a cost less than twice the cost of the optimal (in hindsight) VM allocations, and half of the cost of allocating VMs for the peak demand in the workload.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {62–71},
numpages = {10},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/3412841.3441886,
author = {Chikhaoui, Amina and Lemarchand, Laurent and Boukhalfa, Kamel and Boukhobza, Jalil},
title = {StorNIR, a Multi-Objective Replica Placement Strategy for Cloud Federations},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441886},
doi = {10.1145/3412841.3441886},
abstract = {Federation of clouds makes it possible to transparently extend the resources of Cloud Service Providers (CSPs). For storage services several metrics need to be considered to satisfy customers QoS, that is storage performance, network latency and data availability. Data replication is a key strategy to optimize such metrics. For a CSP, member of a Federation, an effective placement of customers data object replicas is crucial to satisfy QoS demands. In this paper, we modeled the replica placement problem as a multi-objective optimization problem (MOOP) taking into account the local storage classes, other federation CSPs (external) storage services, and customers requirements. To solve this problem, we propose StorNIR a cost-efficient data object Storing scheme based on NSGAII upgraded with Injection and Reparation operators. StorNIR is a matheuristic that consists in hybridizing an exact method with NSGAII meta-heuristic. A repair operator was designed to make the solutions feasible with regards to the system constraints (storage volume, IOPs, etc). StorNIR performed better than both NSGAII meta-heuristic and the exact method in terms of quality of solutions and scalability. The repair function improves the NSGAII meta-heuristic up to 7 times with 7.4\% more extra time execution. On average, StorNIR enhances by 17 times the quality of the initial solutions calculated by CPLEX in terms of Hypervolume. In addition, the designed matheuristic approach can be generalized to other meta-heuristics than NSGAII such as MOPSO meta-heuristic.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {50–59},
numpages = {10},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/3291291.3291304,
author = {Silva, Gabriel Costa and R\'{e}, Reginaldo and Silva, Marco Aur\'{e}lio Graciotto},
title = {Evaluating Efficiency, Effectiveness and Satisfaction of AWS and Azure from the Perspective of Cloud Beginners},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Quality has long been regarded as an important driver of cloud adoption. In particular, quality in use (QiU) of cloud platforms may drive cloud beginners to the cloud platform that offers the best cloud experience. Cloud beginners are critical to the cloud market because they currently represent nearly a third of cloud users. We carried out three experiments to measure the QiU (dependent variable) of public cloud platforms (independent variable) regarding efficiency, effectiveness and satisfaction. AWS EC2 and Azure Virtual Machines are the two cloud services used as representative proxies to evaluate cloud platforms (treatments). Eleven undergraduate students with limited cloud knowledge (participants) manually created 152 VMs (task) using the web interface of cloud platforms (instrument) following seven different configurations (trials) for each cloud platform. Whereas AWS performed significantly better than Azure for efficiency (p-value not exceeding 0.001, A-statistic = 0.68), we could not find a significant difference between platforms for effectiveness (p-value exceeding 0.05) - although the effect size was found relevant (odds ratio = 0.41). Regarding satisfaction, most of our participants perceived the AWS as (i) having the best GUI to benefiting user interaction, (ii) the easiest platform to use, and (iii) the preferred cloud platform for creating VMs. Once confirmed by independent replications, our results suggest that AWS outperforms Azure regarding QiU. Therefore, cloud beginners might have a better cloud experience starting off their cloud projects by using AWS rather than Azure. In addition, our results may help to explain the AWS's cloud leadership.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {114–125},
numpages = {12},
keywords = {quality in use, experimentation, cloud platforms},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3452940.3452990,
author = {Yu, Minqi and Xie, Linjin and Huang, Rui and He, Xing and Yang, Maotao and Yang, Libin},
title = {Impedance Measurement of 0.4kV Power Supply Line in the Station Area Based on the Smart Energy Meter},
year = {2021},
isbn = {9781450388665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452940.3452990},
doi = {10.1145/3452940.3452990},
abstract = {Develop a monitoring and management system based on a cloud platform, and finally implement related applications such as abnormal monitoring of the electric energy meter wiring process, wiring fault diagnosis, line aging assessment, and power outage warning analysis on the cloud platform, which can promptly warn the occurrence of power outages and comprehensively improve high-quality power supply services Level. Therefore, this paper proposes a method for phase line and neutral line impedance estimation of 0.4kV low voltage distribution network based on intelligent electricity meters. In this paper, the voltage of each sequence, current of each sequence, and information of complex power of each sequence of each node are extracted by intelligent electricity meter, and then the measurement of line impedance is completed piecewise. Finally, the impedance measurement model of the 0.4kV low-voltage distribution network was built. The simulation verification of the line impedance measurement was completed through the effective cooperation with the high-precision and high-synchronous sampling smart electricity meter supporting the impedance measurement and the acquisition terminal supporting the of the impedance measurement. The simulation results show that the line impedance measurement error is small, and the prediction of line loss and impedance trend can be completed effectively.},
booktitle = {Proceedings of the 3rd International Conference on Information Technologies and Electrical Engineering},
pages = {255–260},
numpages = {6},
keywords = {Intelligent electric energy meter, Edge calculation, Line impedance measurement, Intelligent diagnosis},
location = {Changde City, Hunan, China},
series = {ICITEE '20}
}

@article{10.1145/3442187,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet},
title = {VidCloud: Joint Stall and Quality Optimization for Video Streaming over Cloud},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3442187},
doi = {10.1145/3442187},
abstract = {As video-streaming services have expanded and improved, cloud-based video has evolved into a necessary feature of any successful business for reaching internal and external audiences. In this article, video streaming over distributed storage is considered where the video segments are encoded using an erasure code for better reliability. We consider a representative system architecture for a realistic (typical) content delivery network (CDN). Given multiple parallel streams/link between each server and the edge router, we need to determine, for each client request, the subset of servers to stream the video, as well as one of the parallel streams from each chosen server. To have this scheduling, this article proposes a two-stage probabilistic scheduling. The selection of video quality is also chosen with a certain probability distribution that is optimized in our algorithm. With these parameters, the playback time of video segments is determined by characterizing the download time of each coded chunk for each video segment. Using the playback times, a bound on the moment generating function of the stall duration is used to bound the mean stall duration. Based on this, we formulate an optimization problem to jointly optimize the convex combination of mean stall duration and average video quality for all requests, where the two-stage probabilistic scheduling, video quality selection, bandwidth split among parallel streams, and auxiliary bound parameters can be chosen. This non-convex problem is solved using an efficient iterative algorithm. Based on the offline version of our proposed algorithm, an online policy is developed where servers selection, quality, bandwidth split, and parallel streams are selected in an online manner. Experimental results show significant improvement in QoE metrics for cloud-based video as compared to the considered baselines.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {jan},
articleno = {17},
numpages = {32},
keywords = {Video streaming over cloud, erasure codes, video quality, two-stage probabilistic scheduling, mean stall duration}
}

@inproceedings{10.1145/3290688.3290705,
author = {Tesfamicael, Aklilu Daniel and Liu, Vicky and Foo, Ernest and Caelli, Bill},
title = {QoE Estimation Model for a Secure Real-Time Voice Communication System in the Cloud},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290705},
doi = {10.1145/3290688.3290705},
abstract = {As moving towards cloud-based real-time services, we are witnessing the shift from a technology-driven services to service provisioning paradigms, that is, from Quality of Service (QoS) to Quality of Experience (QoE). User experience and satisfaction are placed at the epicenter of the system design. QoE is a measurement of user experience on the provided service by a system. Often QoE is measured by subjective mechanisms, such as user experience surveys and mean opinion scores (MOS) methods, which can be a costly and time-consuming process. Using an adequate QoE model to measure user experience of perceived quality is cost-effective, compared to using time-consuming subjective surveys. Applying an adequate QoE model to assess user experience is advantageous for cloud-based real-time services such as voice and video. This study uses a formula-based QoE estimation model to estimate and predict QoE prior to the deployment or during the planning stage of the system service. This study investigates a real-world scenario of a company that recently moved to its premises-based real-time trading communication system (TCS) to a public cloud. A simulation system using OPNET is also implemented to illustrate the usefulness of the model. Our result shows that the effect of delay on the users experience of the service provided by the cloud-based TCS is minimum comparing to packet loss rate (PLR) and Jitter. However, it has been observed that the overhead of the different security settings of the TCS system had no major negative impact to the user experience. The proposed model can be used as a QoE control mechanism and network optimization for cloud-based TCS services.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {10},
numpages = {9},
keywords = {VoIP, QoS, E-Model, QoE, TCS, Real-time},
location = {Sydney, NSW, Australia},
series = {ACSW '19}
}

@inproceedings{10.1145/3078633.3081037,
author = {Wade, April W. and Kulkarni, Prasad A. and Jantz, Michael R.},
title = {AOT vs. JIT: Impact of Profile Data on Code Quality},
year = {2017},
isbn = {9781450350303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078633.3081037},
doi = {10.1145/3078633.3081037},
abstract = {Just-in-time (JIT) compilation during program execution and ahead-of-time (AOT) compilation during software installation are alternate techniques used by managed language virtual machines (VM) to generate optimized native code while simultaneously achieving binary code portability and high execution performance. Profile data collected by JIT compilers at run-time can enable profile-guided optimizations (PGO) to customize the generated native code to different program inputs. AOT compilation removes the speed and energy overhead of online profile collection and dynamic compilation, but may not be able to achieve the quality and performance of customized native code. The goal of this work is to investigate and quantify the implications of the AOT compilation model on the quality of the generated native code for current VMs.  First, we quantify the quality of native code generated by the two compilation models for a state-of-the-art (HotSpot) Java VM. Second, we determine how the amount of profile data collected affects the quality of generated code. Third, we develop a mechanism to determine the accuracy or similarity for different profile data for a given program run, and investigate how the accuracy of profile data affects its ability to effectively guide PGOs. Finally, we categorize the profile data types in our VM and explore the contribution of each such category to performance.},
booktitle = {Proceedings of the 18th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {1–10},
numpages = {10},
keywords = {Profile-guided optimizations, Program profiling},
location = {Barcelona, Spain},
series = {LCTES 2017}
}

@article{10.1145/3140582.3081037,
author = {Wade, April W. and Kulkarni, Prasad A. and Jantz, Michael R.},
title = {AOT vs. JIT: Impact of Profile Data on Code Quality},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/3140582.3081037},
doi = {10.1145/3140582.3081037},
abstract = {Just-in-time (JIT) compilation during program execution and ahead-of-time (AOT) compilation during software installation are alternate techniques used by managed language virtual machines (VM) to generate optimized native code while simultaneously achieving binary code portability and high execution performance. Profile data collected by JIT compilers at run-time can enable profile-guided optimizations (PGO) to customize the generated native code to different program inputs. AOT compilation removes the speed and energy overhead of online profile collection and dynamic compilation, but may not be able to achieve the quality and performance of customized native code. The goal of this work is to investigate and quantify the implications of the AOT compilation model on the quality of the generated native code for current VMs.  First, we quantify the quality of native code generated by the two compilation models for a state-of-the-art (HotSpot) Java VM. Second, we determine how the amount of profile data collected affects the quality of generated code. Third, we develop a mechanism to determine the accuracy or similarity for different profile data for a given program run, and investigate how the accuracy of profile data affects its ability to effectively guide PGOs. Finally, we categorize the profile data types in our VM and explore the contribution of each such category to performance.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {1–10},
numpages = {10},
keywords = {Program profiling, Profile-guided optimizations}
}

@article{10.1145/3263878,
author = {Carlsson, Niklas and Liu, Zhenhua and Nguyen, Thu and Rosenberg, Catherine and Wierman, Adam},
title = {Session Details: Special Issue on the 2016 Greenmetrics Workshop},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3263878},
doi = {10.1145/3263878},
abstract = {The seventh annual GreenMetrics Workshop was held on June 14, 2016 in Antibes Juan-les-Pins, France, in conjunction with the ACM SIGMETRICS/IFIP Performance 2016 conference. For the past five years the workshop has been expanded from topics on the energy and ecological impact of Information and Communication Technology (ICT) systems, to include emerging work on the Smart Grid. Topics of interest fall broadly into three main areas: designing sustainable ICT, ICT for sustainability, and building a smarter, more sustainable electricity grid. The workshop brought together researchers from the traditional SIGMETRICS and Performance communities with researchers and practitioners in the three areas above, to exchange technical ideas and experiences on issues related to sustainability and ICT.The workshop program included three 45-min keynote talks, and nine 20-min presentations of technical papers. All papers are included in this special issue and we briefly summarize the keynote talks here.In the first keynote "The New Sharing Economy for the Grid2050", Kameshwar Poolla from UC Berkeley discussed three sharing economy opportunities in the electricity sector- sharing storage, sharing PV generation, and sharing recruited demand flexibility. He also discussed regulatory and technical challenges to these opportunities. In addition, he presented a micro-economic analysis of decisions by firms, and quantify the benefits of sharing to various participants. Xue (Steve) Liu from McGill University presented the second keynote talk, titled "When Bits Meet Joules: A View from Data Center Operations' Perspective". He used data centers as an example to illustrate the importance of the codesign of information technologies and new energy technologies. Specifically, he focused on how to design cost-saving power management strategies for Internet data center operations.Our third keynote talk was by Florian D\"{o}rfler from ETH Z\"{u}rich, titled "Virtual Inertia Emulation and Placement in Power Grids". He presented a comprehensive analysis to address the optimal inertia placement problem, in particular, by providing a set of closed-form global optimality results for particular problem instances as well as a computational approach resulting in locally optimal solutions. He illustrated the results with a three-region power grid case study. The best student paper award was given to "Opportunities for Price Manipulation by Aggregators in Electricity Markets" by Ruhi et al. The award was determined by a committee of the invited speakers, chaired by Catherine Rosenberg, after considering both the papers and the presentations of the candidates. The authors quantified the profit an aggregator can obtain through strategic curtailment of generation in an electricity market. Efficient algorithms were shown to exist when the topology of the network is radial (acyclic). Further, significant increases in profit can be obtained through strategic curtailment in practical settings.Demand response is discussed in the following two papers. In "Optimizing the Level of Commitment in Demand Response", Comden et al. proposed a generalized demand response framework called Flexible Commitment Demand Response (FCDR) to allow for explicit choices of the level of commitment. Numerical simulations were conducted to demonstrate that FCDR brings in significant (around 50\%) social cost reductions and benefits both the LSE and customers simultaneously. In "An Emergency Demand Response Mechanism for Cloud Computing", Zhou et al. proposed an online auction for dynamic cloud resource provisioning under the emergency demand response program, which runs in polynomial time, achieves truthfulness and close-to-optimal social welfare for the cloud ecosystem.Geographical load balancing was examined by Neglia et al. in "Geographical Load Balancing Across Green Datacenters: a Mean Field Analysis". They modeled via a Markov Chain the problem of scheduling jobs by prioritizing datacenters where renewable energy is currently available. Mean field techniques were employed to derive an asymptotic approximate model and to investigate relationships and tradeoffs among the various system parameters. In "Emergence of Shared Behaviour in Distributed Scheduling Systems for Domestic Appliances", Facchini et al. showed social interaction can increase the flexibility of users and lower the peak power, resulting in a more smooth usage of energy throughout the day. Rossi et al. examined public lighting in "AURORA: an Energy Efficient Public Lighting IoT System for Smart Cities" by proposing Aurora: a low-budget, easy-to-deploy IoT control system. Aurora was deployed in a mid-size Italian municipality and its performance over 4 months was evaluated to quantify both the power and the economic saving.Wireless and wired network power consumption was studied in the following three papers. In "Radio Resource Management for Improving Energy Self-Sufficiency of Green Mobile Networks", Dalmasso et al. designed Resource on Demand strategies to reduce the base station cluster energy consumption and to adapt it to energy availability. Fan etal. also examined base stations in "Boosting Service Availability for Base Stations of Cellular Networks by Event-Driven Battery Profiling" by conducting a systematical analysis on a real world dataset and proposing an event-driven battery profiling approach to precisely extract the features that cause the working condition degradation of the battery group. Last but not least, in "Toward Power-Efficient Backbone Routers", Lu et al. studied how InTerFaces can distribute traffic flows to the Processing Engines (PEs) so that the offered loads on all active PEs are near-perfectly balanced over time, and kept close to a target load, so that the number of active PEs can be minimized.The papers presented at the workshop reflected a current concern of energy consumption associated with proliferating data centers, and other fundamental issues in green computing. The workshop incited interesting discussions and exchange among participants from North America, Europe, and Asia.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {sep},
numpages = {1}
}

@inproceedings{10.1145/2740908.2742827,
author = {Assaf, Ahmad and Senart, Aline and Troncy, Rapha\"{e}l},
title = {Roomba: Automatic Validation, Correction and Generation of Dataset Metadata},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742827},
doi = {10.1145/2740908.2742827},
abstract = {Data is being published by both the public and private sectors and covers a diverse set of domains ranging from life sciences to media or government data. An example is the Linked Open Data (LOD) cloud which is potentially a gold mine for organizations and individuals who are trying to leverage external data sources in order to produce more informed business decisions. Considering the significant variation in size, the languages used and the freshness of the data, one realizes that spotting spam datasets or simply finding useful datasets without prior knowledge is increasingly complicated. In this paper, we propose Roomba, a scalable automatic approach for extracting, validating, correcting and generating descriptive linked dataset profiles. While Roomba is generic, we target CKAN-based data portals and we validate our approach against a set of open data portals including the Linked Open Data (LOD) cloud as viewed on the DataHub. The results demonstrate that the general state of various datasets and groups, including the LOD cloud group, needs more attention as most of the datasets suffer from bad quality metadata and lack some informative metrics that are required to facilitate dataset search.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {159–162},
numpages = {4},
keywords = {linked data, dataset profile, data quality, metadata},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3373724.3373726,
author = {Chen, Wenyu and Xiong, Wei and Cheng, Jierong and Li, Yusha},
title = {Automatic Dimensional Measurement Using Datums Generated from Point Clouds},
year = {2020},
isbn = {9781450372350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373724.3373726},
doi = {10.1145/3373724.3373726},
abstract = {Dimensional measurement is critical for quality control. Manual dimensional measurement using standard gauges can only be applied on a few datums. To measure a huge number of datums, a component needs to be scanned into a point cloud and measured digitally. For precision components, datum generation on the scanned point cloud is labor-intensive. Given a raw point cloud from scanner, this paper proposes an automatic dimensional measurement solution with an adaptive local registration algorithm and an adaptive datum generation algorithm. Using datums on the CAD model as reference, the adaptive local registration algorithm selects local regions on the scanned model to compensate the local deviation between the CAD model and the scanned model. After that, with outliers and noises in the raw data, the adaptive datum generation algorithm creates the correct datums on the scanned model adaptive to the actual geometry. Dimensional measurement based on the generated datums can be achieved automatically. Moreover, the solution does not require users to manually preprocess the point cloud, such as outlier and noise removal. As such, it improves the productivity in dimensional inspection.},
booktitle = {Proceedings of the 5th International Conference on Robotics and Artificial Intelligence},
pages = {59–63},
numpages = {5},
keywords = {Inspection, Dimensional measurement, Datum generation},
location = {Singapore, Singapore},
series = {ICRAI '19}
}

@inproceedings{10.1145/3196398.3196422,
author = {Widder, David Gray and Hilton, Michael and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {I'm Leaving You, Travis: A Continuous Integration Breakup Story},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196422},
doi = {10.1145/3196398.3196422},
abstract = {Continuous Integration (CI) services, which can automatically build, test, and deploy software projects, are an invaluable asset in distributed teams, increasing productivity and helping to maintain code quality. Prior work has shown that CI pipelines can be sophisticated, and choosing and configuring a CI system involves tradeoffs. As CI technology matures, new CI tool offerings arise to meet the distinct wants and needs of software teams, as they negotiate a path through these tradeoffs, depending on their context. In this paper, we begin to uncover these nuances, and tell the story of open-source projects falling out of love with Travis, the earliest and most popular cloud-based CI system. Using logistic regression, we quantify the effects that open-source community factors and project technical factors have on the rate of Travis abandonment. We find that increased build complexity reduces the chances of abandonment, that larger projects abandon at higher rates, and that a project's dominant language has significant but varying effects. Finally, we find the surprising result that metrics of configuration attempts and knowledge dispersion in the project do not affect the rate of abandonment.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {165–169},
numpages = {5},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3447526.3472057,
author = {Chaudhary, Akash and Belani, Manshul and Maheshwari, Naman and Parnami, Aman},
title = {Verbose : Designing a Context-Based Educational System for Improving Communicative Expressions},
year = {2021},
isbn = {9781450383288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447526.3472057},
doi = {10.1145/3447526.3472057},
abstract = {ESL (English as a second language) speakers tend to follow the tone structure of their first language, making their speech difficult to understand for native speakers, thereby limiting their opportunities for education and employment. To address this problem, we build an interactive smartphone-based educational mobile application using the user-centered design process. This application teaches English intonations based on globally consistent pitch patterns through conversations with a trained chat assistant, which inculcates expert linguists’ teaching principles. After co-designing the application’s parameters with primary stakeholders and expert visual designers, we assess its effectiveness by measuring the pre and post-performance of the users after the system usage, using various quantitative measures, like intonation scores, SEQ, and SUS. Feedback from users suggests that ESL speakers find significant improvement in the perception of their vocal expressions, thereby highlighting the necessity of such a system in improving the quality of conversations that people have in general.},
booktitle = {Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction},
articleno = {41},
numpages = {13},
keywords = {Intonations, Learning application, Stress-timed language, Context-based learning, Communicative expressions},
location = {Toulouse \&amp; Virtual, France},
series = {MobileHCI '21}
}

@inproceedings{10.5555/2602339.2602375,
author = {Zheng, Yixin and Li, Linglong and Zhang, Lin},
title = {Poster Abstract: PiMi Air Community: Getting Fresher Indoor Air by Sharing Data and Know-Hows},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {PiMiair.org is a participatory indoor air quality data sharing project we launched in January 2014. Over 200 PiMi air boxes, a low-cost indoor air quality monitor, were given out to volunteer users across China. The PiMi air boxes measure the approximate indoor particulate matter concentration, and the ambient temperate and humidity. When a user accesses the PiMi air box for his personal air quality data on his smartphone, the data is relayed to the backend PiMi cloud server for analysis. Accumulating large amount of indoor air quality data under different circumstances, the PiMi cloud server is able to use statistical learning methodologies to detect point of interests (POIs) in the data series, and asks users to label their activities or events at the POIs. Together with the user-reported physicality information on the indoor environments, PiMiair.org is able to quantitatively evaluate the impacts of the environment physicality and human behaviors on the indoor air quality, and mine the knowledges on how to alleviate indoor air pollution. We believe that by sharing these knowledge among the community, healthier breathing environments could be nurtured for the well-being of the public.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {283–284},
numpages = {2},
keywords = {participatory sensing, human factors, indoor air quality},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3098603.3098608,
author = {Tasiopoulos, Argyrios G. and Atarashi, Ray and Psaras, Ioannis and Pavlou, George},
title = {On the Bitrate Adaptation of Shared Media Experience Services},
year = {2017},
isbn = {9781450350563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098603.3098608},
doi = {10.1145/3098603.3098608},
abstract = {In Shared Media Experience Services (SMESs), a group of people is interested in streaming consumption in a synchronised way, like in the case of cloud gaming, live streaming, and interactive social applications. However, group synchronisation comes at the expense of other Quality of Experience (QoE) factors due to both the dynamic and diverse network conditions that each group member experiences. Someone might wonder if there is a way to keep a group synchronised while maintaining the highest possible QoE for each one of its members. In this work, at first we create a Quality Assessment Framework capable of evaluating different SMESs improvement approaches with respect to traditional metrics like media bitrate quality, playback disruption, and end user desynchronisation. Secondly, we focus on the bitrate adaptation for improving the QoE of SMESs, as an incrementally deployable end user triggered approach, and we formulate the problem in the context of Adaptive Real Time Dynamic Programming (ARTDP). Finally, we develop and apply a simple QoE aware bitrate adaptation mechanism that we compare against youtube live-streaming traces to find that it improves the youtube performance by more than 30\%.},
booktitle = {Proceedings of the Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {25–30},
numpages = {6},
keywords = {QoE Assessment Framework, Bitrate Adaptation, Shared Media Experience Services (SMESs)},
location = {Los Angeles, CA, USA},
series = {Internet QoE '17}
}

@inproceedings{10.5555/2755535.2755538,
author = {Claypool, Mark and Finkel, David},
title = {The Effects of Latency on Player Performance in Cloud-Based Games},
year = {2014},
publisher = {IEEE Press},
abstract = {Cloud-based games are an increasingly popular method to distribute and play computer games on the Internet. While there has been some work studying network aspects of cloud-based games and examining the effects of latency on traditional games, there has not been sufficient research on the impact of latency on cloud-based games nor a comparison of the impact of latency on cloud-based games versus traditional games. This paper presents the results of two user studies that measure the objective and subjective effects of latency on cloud-based games, one study using the commercial cloud game system OnLive and the other study using the academic cloud game system GamingAnywhere. Analysis of the results shows both quality of experience and user performance degrade linearly with an increase in latency. More significantly, latency affects cloud-based games in a manner most similar to that of traditional first-person avatar games, the most sensitive class of games, despite the fact that the cloud-based games may have a different user perspective. These results have implications for cloud-based game designers and cloud system developers.},
booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
articleno = {2},
numpages = {6},
location = {Nagoya, Japan},
series = {NetGames '14}
}

@inproceedings{10.1145/3097895.3097900,
author = {Zhang, Wenxiao and Han, Bo and Hui, Pan},
title = {On the Networking Challenges of Mobile Augmented Reality},
year = {2017},
isbn = {9781450350556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097895.3097900},
doi = {10.1145/3097895.3097900},
abstract = {In this paper, we conduct a reality check for Augmented Reality (AR) on mobile devices. We dissect and measure the cloud-offloading feature for computation-intensive visual tasks of two popular commercial AR systems. Our key finding is that their cloud-based recognition is still not mature and not optimized for latency, data usage and energy consumption. In order to identify the opportunities for further improving the Quality of Experience (QoE) for mobile AR, we break down the end-to-end latency of the pipeline for typical cloud-based mobile AR and pinpoint the dominating components in the critical path.},
booktitle = {Proceedings of the Workshop on Virtual Reality and Augmented Reality Network},
pages = {24–29},
numpages = {6},
keywords = {cloud offloading, networking challenges, reality check, Augmented reality, end-to-end latency},
location = {Los Angeles, CA, USA},
series = {VR/AR Network '17}
}

@article{10.1145/3337956,
author = {Moghaddam, Sara Kardani and Buyya, Rajkumar and Ramamohanarao, Kotagiri},
title = {Performance-Aware Management of Cloud Resources: A Taxonomy and Future Directions},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3337956},
doi = {10.1145/3337956},
abstract = {The dynamic nature of the cloud environment has made the distributed resource management process a challenge for cloud service providers. The importance of maintaining quality of service in accordance with customer expectations and the highly dynamic nature of cloud-hosted applications add new levels of complexity to the process. Advances in big-data learning approaches have shifted conventional static capacity planning solutions to complex performance-aware resource management methods. It is shown that the process of decision-making for resource adjustment is closely related to the behavior of the system, including the utilization of resources and application components. Therefore, a continuous monitoring of system attributes and performance metrics provides the raw data for the analysis of problems affecting the performance of the application. Data analytic methods, such as statistical and machine-learning approaches, offer the required concepts, models, and tools to dig into the data and find general rules, patterns, and characteristics that define the functionality of the system. Obtained knowledge from the data analysis process helps to determine the changes in the workloads, faulty components, or problems that can cause system performance to degrade. A timely reaction to performance degradation can avoid violations of service level agreements, including performing proper corrective actions such as auto-scaling or other resource adjustment solutions. In this article, we investigate the main requirements and limitations of cloud resource management, including a study of the approaches to workload and anomaly analysis in the context of performance management in the cloud. A taxonomy of the works on this problem is presented that identifies main approaches in existing research from the data analysis side to resource adjustment techniques. Finally, considering the observed gaps in the general direction of the reviewed works, a list of these gaps is proposed for future researchers to pursue.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {84},
numpages = {37},
keywords = {Anomaly detection, performance management, big-data analytics, resource management}
}

@inproceedings{10.1145/3349614.3356028,
author = {Tomei, Matthew and Schwing, Alexander and Narayanasamy, Satish and Kumar, Rakesh},
title = {Sensor Training Data Reduction for Autonomous Vehicles},
year = {2019},
isbn = {9781450369282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349614.3356028},
doi = {10.1145/3349614.3356028},
abstract = {Ensuring safety and reliability of autonomous vehicles requires good learning models which, in turn, require a large amount of real-world training data. Data produced by in-vehicle sensors (e.g., cameras, LIDARs, IMUs, etc.) can be used for training; however, both local storage and transmission of this sensor data to the cloud for subsequent use in training can be prohibitively expensive due to the staggering volume of data produced by these sensors, especially the cameras. In this paper, we perform the first exploration of techniques for reducing video frames in a way that the quality of training for autonomous vehicles is minimally affected. We particularly focus on utility aware data reduction schemes where the potential contribution of a video frame to enhancing the quality of learning (or utility) is explicitly considered during data reduction. Since actual utility of a video frame cannot be computed online, we use surrogate utility metrics to decide what video frames to keep for training and which ones to discard. Our results show that utility-aware data reduction schemes can reduce the amount of camera data required for training by as much as $16times$ compared to random sampling for the same quality of learning (in terms of IoU).},
booktitle = {Proceedings of the 2019 Workshop on Hot Topics in Video Analytics and Intelligent Edges},
pages = {45–50},
numpages = {6},
keywords = {data reduction, semantic segmentation, self driving car, sensor, compression, autonomous vehicle, active learning, machine learning},
location = {Los Cabos, Mexico},
series = {HotEdgeVideo'19}
}

@article{10.1145/3383464,
author = {Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv},
title = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3383464},
doi = {10.1145/3383464},
abstract = {Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {46},
numpages = {40},
keywords = {big data analytics application, Big data, SLA, service layer, service level agreement, SLA metrics}
}

