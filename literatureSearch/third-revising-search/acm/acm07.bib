@article{10.1109/TNET.2020.2976129,
author = {Xie, Kun and Chen, Yuxiang and Wang, Xin and Xie, Gaogang and Cao, Jiannong and Wen, Jigang},
title = {Accurate and Fast Recovery of Network Monitoring Data: A GPU Accelerated Matrix Completion},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2976129},
doi = {10.1109/TNET.2020.2976129},
abstract = {Gaining a full knowledge of end-to-end network performance is important for some advanced network management and services. Although it becomes increasingly critical, end-to-end network monitoring usually needs active probing of the path and the overhead will increase quadratically with the number of network nodes. To reduce the measurement overhead, matrix completion is proposed recently to predict the end-to-end network performance among all node pairs by only measuring a small set of paths. Despite its potential, applying matrix completion to recover the missing data suffers from low recovery accuracy and long recovery time. To address the issues, we propose MC-GPU to exploit Graphics Processing Units (GPUs) to enable parallel matrix factorization for high-speed and highly accurate Matrix Completion. To well exploit the special architecture features of GPUs for both task independent and data-independent parallel task execution, we propose several novel techniques: similar OD (origin and destination) pairs reordering taking advantage of the locality-sensitive hash (LSH) functions, balanced matrix partition, and parallel matrix completion. We implement the proposed MC-GPU on the GPU platform and evaluate the performance using real trace data. We compare the proposed MC-GPU with the state of the art matrix completion algorithms, and our results demonstrate that MC-GPU can achieve significantly faster speed with high data recovery accuracy.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {958–971},
numpages = {14}
}

@inproceedings{10.1145/2980258.2980430,
author = {Muthuraman, Sangeetha and Venkatesan, V. Prasanna},
title = {Design of QOS Based Web Service Selection/Composition Hyper-Heuristic Model},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2980430},
doi = {10.1145/2980258.2980430},
abstract = {A web service selection/composition problem is a NP-complete problem that cannot be solved in polynomial time. An efficient solution is essential to solve this problem. This solution may be attained by following hyper-heuristic strategies. As a first step in addressing the problem, this paper presents a new web services selection/composition model which enables such a hyper-heuristic notion. Various parts of this proposed model can be implemented by using different algorithms thus enabling many hybrid implementations. In this paper the proposed model has been implemented by using a reference score and trust based service selection algorithm and a strategic tree based service composition algorithm. To realize this implementation agent based architecture has been proposed. A well defined QOS model has been used to accurately receive customer's request and update service specific quality values. The algorithms implemented are efficient as the computational complexities of these algorithms have been greatly reduced and also a fault tolerant approach has been adopted. The experimental results illustrate that the proposed model and algorithms have effectively solved the web services selection/composition problem.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {80},
numpages = {10},
keywords = {service composition, QOS based web service selection and composition, web services selection/composition model, Service selection, hyper-heuristic model},
location = {Pondicherry, India},
series = {ICIA-16}
}

@inproceedings{10.1145/3473714.3473742,
author = {Xin, An},
title = {Research on Multi-Sensor Fusion Perception Method of Vehicle-Infrastructure Collaboration for Smart Automobiles},
year = {2021},
isbn = {9781450390231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473714.3473742},
doi = {10.1145/3473714.3473742},
abstract = {Traffic congestion should be solved, it reduce traffic safety potential risks, and improve people's travel efficiency. The paper based on intelligent car's own operation characteristics (Smart car perception is generally only 300 ~ 500 meters, there is a perceived problem of super-visual distance and vision blind zone), In order to effectively improve the safety operation level of smart cars, and combined with intelligent non-smart cars on the road may encounter cars a smart car trip perception highway outside the scope of frequently asked questions, such as over-site sensations, the whole scene and area's perception, the blind area, the emergency corner, tunnel, bridge, other highways travel common scenes and so on. This paper is based on new infrastructure transformations or newly build's research and practical results such as road traffic intelligence infrastructure, it deployed the current road traffic to intelligently infrastructure, especially the technical difficulties existing in the process of common perceptual equipment (smart cameras, radar) are synonymous with multiple sensor information fusion perceptions, it proposed a multi-sensor fusion algorithm based on error variance, and designed a multi-object multi-sensor data processing system architecture. This paper also proposes traffic operation scheduling architecture based on the game theory of car road synergies on the basis of multi-sensor data fusion. Finally, these architectures were analyzed using computer simulation techniques. The results show that the traffic operation schedule for multi-sensor fusion algorithm based on error variance and game theory based on the study proposed this study can be more obvious. The safety and efficiency of the road traffic environment of smart vehicles. Optimization, smart vehicles equipped with smart vehicles are also more than 25\% higher than traditional common vehicles in terms of vehicle safety. All in all, this study proposed to synergistic multi-sensor convergence method for smart cars, that based on smart car a smart car perception, compared to non-smart roads after intelligent infrastructure construction and transformation of road traffic intelligent transportation systems, Higher efficiency and more intelligent can better solve the common problems in road traffic environment, providing people with safer, efficient and high-quality traffic travel services.},
booktitle = {Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics},
pages = {164–175},
numpages = {12},
keywords = {smart vehicle, perceptual method research, multi-sensor fusion, vehicle-infrastructure collaboration},
location = {Guangzhou, China},
series = {ICCIR '21}
}

@article{10.1109/TNET.2013.2253797,
author = {De Cicco, Luca and Mascolo, Saverio},
title = {An Adaptive Video Streaming Control System: Modeling, Validation, and Performance Evaluation},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2013.2253797},
doi = {10.1109/TNET.2013.2253797},
abstract = {Adaptive video streaming is a relevant advancement with respect to classic progressive download streaming a la YouTube. Among the different approaches, the video stream-switching technique is getting wide acceptance, being adopted by Microsoft, Apple, and popular video streaming services such as Akamai, Netflix, Hulu, Vudu, and Livestream. In this paper, we present a model of the automatic video stream-switching employed by one of these leading video streaming services along with a description of the client-side communication and control protocol. From the control architecture point of view, the automatic adaptation is achieved by means of two interacting control loops having the controllers at the client and the actuators at the server: One loop is the buffer controller, which aims at steering the client playout buffer to a target length by regulating the server sending rate; the other one implements the stream-switching controller and aims at selecting the video level. A detailed validation of the proposed model has been carried out through experimental measurements in an emulated scenario.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {526–539},
numpages = {14},
keywords = {performance evaluation, modeling, stream-switching, adaptive video streaming}
}

@inproceedings{10.1145/2664591.2664611,
author = {Poulo, Lebeko and Phiri, Lighton and Suleman, Hussein},
title = {Fine-Grained Scalability of Digital Library Services in the Cloud},
year = {2014},
isbn = {9781450332460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2664591.2664611},
doi = {10.1145/2664591.2664611},
abstract = {Modern digital library systems are increasingly handling massive data volumes; this content needs to be stored, indexed and made easily accessible to end users. Cloud computing promises to address some of these needs through a set of services that arguably support scalability of service provision. This paper discusses a set of experiments to assess the scalability of typical digital library services that use cloud computing facilities for core processing and storage. Horizontal scalability experiments were performed to benchmark the overall performance of the architecture with increasing load. The results of the experiments indicate that stable response times and some degree of variability are attainable due to multiple middleware servers when browsing and/or searching a collection of a fixed size. There is minimal variation in response times when varying collection sizes and equally after the caching phases. Most importantly, request sequencing proved that the quantity and age of requests have no impact on response times. The experimental results thus provide evidence to support the feasibility of building and deploying cloud-based Digital Libraries.},
booktitle = {Proceedings of the Southern African Institute for Computer Scientist and Information Technologists Annual Conference 2014 on SAICSIT 2014 Empowered by Technology},
pages = {157–165},
numpages = {9},
keywords = {Scalability, Amazon AWS, Cloud Computing},
location = {Centurion, South Africa},
series = {SAICSIT '14}
}

@inproceedings{10.5555/3042094.3042231,
author = {Valadares, Arthur and Lopes, Cristina V. and Achar, Rohan and Bowman, Mic},
title = {CADIS: Aspect-Oriented Architecture for Collaborative Modeling and Simulation},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {The development of large and complex simulated models often requires teams to collaborate. One approach is to break a large model into independently developed partial models that, when combined, capture the overall behavior. However, maintaining consistent world state across independently developed simulations is a challenge.In this paper, we introduce the Collaborative Aspect-Oriented Distributed Interactive Simulation (CADIS) architecture and development platform. CADIS embodies a new paradigm for integrating independently developed time-discrete partial models and simulations, focusing on transparently maintaining synchronized shared state. Data is pulled and instantiated in the beginning of each time step, and pushed at the end of each time step. An urban simulation is used to demonstrate CADIS capabilities and performance. We show how simple optimizations can bring the performance of the framework to acceptable levels, making CADIS a viable modeling and simulation methodology supporting separation of concerns.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {1024–1035},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

@inproceedings{10.1145/3277868.3277880,
author = {Klugman, Noah and Dutta, Prabal},
title = {Set and Forget Sensing with Applets on IFTTT},
year = {2018},
isbn = {9781450360494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277868.3277880},
doi = {10.1145/3277868.3277880},
abstract = {Rich data sets can be collected trivially by bootstrapping off mobile phones and cloud services. We describe an end-to-end system built with IFTTT that requires no code to collect arrival and departure times from a geographic area on the campus of the University of California, Berkeley. This system was configured and deployed in less than one half hour, cost nothing to deploy or run, and functioned without interruption for seven months, taking 463 measurements of a single participant. Along with providing the data set, which provides some insight into the working life of a graduate student, we describe each part of the system architecture and discuss how a model of sensing-as-an-applet enables data streams with de-facto standardized, high reliability, and close-to-no-barrier of entry.},
booktitle = {Proceedings of the First Workshop on Data Acquisition To Analysis},
pages = {23–24},
numpages = {2},
keywords = {trigger-action programming, sensing at scale, IFTTT},
location = {Shenzhen, China},
series = {DATA '18}
}

@inproceedings{10.1145/2785956.2787495,
author = {Hartert, Renaud and Vissicchio, Stefano and Schaus, Pierre and Bonaventure, Olivier and Filsfils, Clarence and Telkamp, Thomas and Francois, Pierre},
title = {A Declarative and Expressive Approach to Control Forwarding Paths in Carrier-Grade Networks},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787495},
doi = {10.1145/2785956.2787495},
abstract = {SDN simplifies network management by relying on declarativity (high-level interface) and expressiveness (network flexibility). We propose a solution to support those features while preserving high robustness and scalability as needed in carrier-grade networks. Our solution is based on (i) a two-layer architecture separating connectivity and optimization tasks; and (ii) a centralized optimizer called framework, which translates high-level goals expressed almost in natural language into compliant network configurations. Our evaluation on real and synthetic topologies shows that framework improves the state of the art by (i) achieving better trade-offs for classic goals covered by previous works, (ii) supporting a larger set of goals (refined traffic engineering and service chaining), and (iii) optimizing large ISP networks in few seconds. We also quantify the gains of our implementation, running Segment Routing on top of IS-IS, over possible alternatives (RSVP-TE and OpenFlow).},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {15–28},
numpages = {14},
keywords = {optimization, service chaining, mpls, isp, segment routing (sr), traffic engineering, software defined networking (sdn)},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@article{10.1145/2829988.2787495,
author = {Hartert, Renaud and Vissicchio, Stefano and Schaus, Pierre and Bonaventure, Olivier and Filsfils, Clarence and Telkamp, Thomas and Francois, Pierre},
title = {A Declarative and Expressive Approach to Control Forwarding Paths in Carrier-Grade Networks},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2829988.2787495},
doi = {10.1145/2829988.2787495},
abstract = {SDN simplifies network management by relying on declarativity (high-level interface) and expressiveness (network flexibility). We propose a solution to support those features while preserving high robustness and scalability as needed in carrier-grade networks. Our solution is based on (i) a two-layer architecture separating connectivity and optimization tasks; and (ii) a centralized optimizer called framework, which translates high-level goals expressed almost in natural language into compliant network configurations. Our evaluation on real and synthetic topologies shows that framework improves the state of the art by (i) achieving better trade-offs for classic goals covered by previous works, (ii) supporting a larger set of goals (refined traffic engineering and service chaining), and (iii) optimizing large ISP networks in few seconds. We also quantify the gains of our implementation, running Segment Routing on top of IS-IS, over possible alternatives (RSVP-TE and OpenFlow).},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {15–28},
numpages = {14},
keywords = {optimization, mpls, service chaining, isp, segment routing (sr), traffic engineering, software defined networking (sdn)}
}

@inproceedings{10.1145/3368235.3368838,
author = {Harsh, Piyush and Ribera Laszkowski, Juan Francisco and Edmonds, Andy and Quang Thanh, Tran and Pauls, Michael and Vlaskovski, Radoslav and Avila-Garc\'{\i}a, Orlando and Pages, Enric and Gort\'{a}zar Bellas, Francisco and Gallego Carrillo, Micael},
title = {Cloud Enablers For Testing Large-Scale Distributed Applications},
year = {2019},
isbn = {9781450370448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368235.3368838},
doi = {10.1145/3368235.3368838},
abstract = {Testing large-scale distributed systems (also known as testing in the large) is a challenge that spreads across different technical domains and areas of expertise. Current methods and tools provide some minimal guarantees in relation to the correctness of their functional properties and have serious limitations when evaluating their extra-functional properties in realistic conditions, such as scalability, availability and performance efficiency. Cloud Testing and more specifically "testing in the cloud'' has arisen to tackle those challenges. In this new paradigm, cloud-based environment and infrastructure are used to run realistic end-to-end and/or system-level tests, collect test data and analyse them. In this paper we present a set of cloud-native services to take from the tester the responsibility of managing the resources and complementary services required to simulate realistic operational conditions and production environments. Specifically, they provide cloud testing capabilities such as logs and measurements collection from both testing jobs and system under test; test data analytics and visualization; provisioning and operation of additional services and processes to replicate realistic production ecosystems; support to scalability and diversity of underlying testing infrastructure; and replication of the operational conditions of the software under test through its instrumentation. We present the architecture of the cloud testing solution and the detailed design of each of the services; we also evaluate their relative contribution to satisfy different needs in the context of test execution.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
pages = {35–42},
numpages = {8},
keywords = {cloud testing, testing, reliability, scalability, continuous testing, large-scale distributed systems, continuous integration},
location = {Auckland, New Zealand},
series = {UCC '19 Companion}
}

@inproceedings{10.1145/3229556.3229563,
author = {Kastanakis, Savvas and Sermpezis, Pavlos and Kotronis, Vasileios and Dimitropoulos, Xenofontas},
title = {CABaRet: Leveraging Recommendation Systems for Mobile Edge Caching},
year = {2018},
isbn = {9781450359061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229556.3229563},
doi = {10.1145/3229556.3229563},
abstract = {Joint caching and recommendation has been recently proposed for increasing the efficiency of mobile edge caching. While previous works assume collaboration between mobile network operators and content providers (who control the recommendation systems), this might be challenging in today's economic ecosystem, with existing protocols and architectures. In this paper, we propose an approach that enables cache-aware recommendations without requiring a network and content provider collaboration. We leverage information provided publicly by the recommendation system, and build a system that provides cache-friendly and high-quality recommendations. We apply our approach to the YouTube service, and conduct measurements on YouTube video recommendations and experiments with video requests, to evaluate the potential gains in the cache hit ratio. Finally, we analytically study the problem of caching optimization under our approach. Our results show that significant caching gains can be achieved in practice; 8 to 10 times increase in the cache hit ratio from cache-aware recommendations, and an extra 2 times increase from caching optimization.},
booktitle = {Proceedings of the 2018 Workshop on Mobile Edge Communications},
pages = {19–24},
numpages = {6},
keywords = {Mobile Edge Networks, Recommendation Systems, Joint Caching and Recommendation},
location = {Budapest, Hungary},
series = {MECOMM'18}
}

@inproceedings{10.1145/3314058.3318167,
author = {Aghaei, Ehsan and Al-shaer, Ehab},
title = {ThreatZoom: Neural Network for Automated Vulnerability Mitigation},
year = {2019},
isbn = {9781450371476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314058.3318167},
doi = {10.1145/3314058.3318167},
abstract = {Increasing the variety and quantity of cyber threats becoming the evident that traditional human-in-loop approaches are no longer sufficient to keep systems safe. To address this momentous moot point, forward-thinking pioneers propose new cyber security strategy using automation to build a more efficient and cheaper defense. Associating large number of unpatchable CVEs (vulnerability descriptions) generated everyday to appropriate CWE (weakness) and CAPEC (attack pattern) can be used to automatically infer the expected impact and corresponding mitigation course of actions for that new CVE. Routinely, adversary exploits a vulnerability to trigger a cyber attack where this vulnerability results from a product or system weakness. Hence, finding a common system weakness associated with a vulnerability within a particular product can help to identifying the software, system, or architecture flaw and the potential attack impacts. This identification leads to prevent, detect, and mitigate those flaws. On the other hand, after recognizing the cause and the effect of a vulnerability, discovering the procedural-oriented description of the attack to create behavioral observables for detection and mitigation is necessary that can be derived from CAPEC and ATTCK. Mapping the CWE to CAPEC and ATTCK which provides pre-TTP and post-TTP respectively where TTP stands for Tactics, Techniques, and Procedures. Having all CWE, CAPEC, and ATTCK in one hand enables us to find corresponding mitigation for each one. On the other hand, extracting threat actions provided by each of these concepts leads to find another type of mitigation coming from Critical Security Controls (CSC).In this proposal, the target is to do mapping all the way from CVE to CAPEC and ATTCk automatically using machine learning, deep learning, and natural language processing and find the appropriate mitigation for each one and then find a proper patch as course of action defense. So far, we have introduced a neural network model which successfully classifies CVE to CWE automatically and as working on a deep learning model to classify CWEs to CAPEC.},
booktitle = {Proceedings of the 6th Annual Symposium on Hot Topics in the Science of Security},
articleno = {24},
numpages = {3},
keywords = {CVE, CWE, cyber security, CAPEC},
location = {Nashville, Tennessee, USA},
series = {HotSoS '19}
}

@inproceedings{10.1145/2645884.2645890,
author = {Irish, Andrew T. and Iland, Daniel and Isaacs, Jason T. and Hespanha, Jo\~{a}o P. and Belding, Elizabeth M. and Madhow, Upamanyu},
title = {Using Crowdsourced Satellite SNR Measurements for 3D Mapping and Real-Time GNSS Positioning Improvement},
year = {2014},
isbn = {9781450330732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645884.2645890},
doi = {10.1145/2645884.2645890},
abstract = {Geopositioning using Global Navigation Satellite Systems (GNSS), such as the Global Positioning System (GPS), is inaccurate in urban environments due to frequent non-line-of-sight (NLOS) signal reception. This poses a major problem for mobile services that benefit from accurate urban localization, such as navigation, hyperlocal advertising, and geofencing applications. However, urban NLOS signal reception can be exploited in two ways. First, one can use satellite signal-to-noise ratio (SNR) measurements crowdsourced from mobile devices to create 3D environment maps. This is possible because, for example, the SNR of signals obstructed by buildings is lower on average than that of line-of-sight (LOS) signals. Second, in a sort of reverse process called Shadow Matching, SNR readings from a particular device at an instant in time can be compared to 3D maps to provide real-time localization improvement. In this paper we give a brief overview of how such a system works and describe a scalable, low-cost, software-only architecture that implements it.},
booktitle = {Proceedings of the 6th Annual Workshop on Wireless of the Students, by the Students, for the Students},
pages = {5–8},
numpages = {4},
keywords = {localization improvement, gps, gnss, crowdsourcing, 3d mapping, shadow matching},
location = {Maui, Hawaii, USA},
series = {S3 '14}
}

@inproceedings{10.1145/2968219.2968315,
author = {Alissandrakis, Aris and Nake, Isabella},
title = {A New Approach for Visualizing Quantified Self Data Using Avatars},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968315},
doi = {10.1145/2968219.2968315},
abstract = {In recent years, it is becoming more common for people to use applications or devices that keep track of their life and activities, such as physical fitness, places they visited, the music they listen to, or pictures they took. This generates data that are used by the service providers for a variety of (usually analytics) purposes, but commonly there are limitations on how the users themselves can also explore or interact with these data. Our position paper describes a new approach of visualizing such Quantified Self data, in a meaningful and enjoyable way that can give the users personal insights into their own data. The visualization of the information is proposed as an avatar that maps the different activities the user is engaged with, along with each such activity level, as graphical features. An initial prototype (both in terms of graphical design and software architecture) as well as possible future extensions are discussed.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {522–527},
numpages = {6},
keywords = {quantified self, avatars, data visualization},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@proceedings{10.1145/2619239,
title = {SIGCOMM '14: Proceedings of the 2014 ACM Conference on SIGCOMM},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SIGCOMM 2014!This year's conference continues the SIGCOMM tradition of being the premier forum for the presentation of research on networking and communications. The technical program this year features a set of outstanding papers that cover a wide variety of areas including network architecture, software defined networks, data center networks, wireless networks, network services, congestion management, security, privacy, measurement and analysis.This year's call for papers attracted 242 submissions from all over the world. The 54 member Technical Program Committee along with a selected group of external experts carefully considered all of the submissions over two rounds of reviewing including an author feedback period - with a total of 968 detailed reviews completed. The TPC meeting to select the final program was held at ICSI, Berkeley, in late April 2014. At the conclusion of the meeting, the committee had assembled a wonderful program composed of 45 papers, to be presented over three days at the conference. The quality of submissions was extremely high as reflected in the final technical program.},
location = {Chicago, Illinois, USA}
}

@inproceedings{10.1145/3090354.3090463,
author = {Rafii, Fadoua and Hassani, Badr Dine Rossi and Kbir, M'hamed A\"{\i}t},
title = {New Approach for Microarray Data Decision Making with Respect to Multiple Sources},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090463},
doi = {10.1145/3090354.3090463},
abstract = {Microarray technology is an innovative technology, which has brought changes to the biological fields. It is considered as an interesting advent for worthwhile researches. It has permitted simultaneous measurements of the hundreds of activities of genes. However, most of users and specifically researchers and biologists find difficulties while extracting and interpreting this kind of data, also the results of Microarray experiments are stored in multiple and different databases. The present paper focuses on providing a global architecture for making decisions on Microarray data, by taking advantages from the semantic web technologies and the data mining techniques. The major goal consists on getting decisions about a given disease from many experiment data distributed on many sources over the net. The input dataset, real elements array form, is retrieved from the integrated experiments designed for cancer studies. This work is interested to two huge Microarray databases: GEO and ArrayExpress. The integration was based on semantic web technologies used to integrate data from several Web sites and Microarray data sources. This can be done by a user to combine several experiments that treat the same disease or phenomenon in order to have more significant results. Also a user can upload a specific dataset, via Web services provided by a laboratory, that can be combined with other data, containing the same genes and treating the same disease, and receive results of data mining techniques proposed by this laboratory. We suppose that each laboratory has its own Web services that can receive data which respects a predefined format.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {106},
numpages = {5},
keywords = {Microarray, Data mining, Semantic web},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/2744769.2744897,
author = {Wang, Ying and Han, Yinhe and Wang, Cheng and Li, Huawei and Li, Xiaowei},
title = {RADAR: A Case for Retention-Aware DRAM Assembly and Repair in Future FGR DRAM Memory},
year = {2015},
isbn = {9781450335201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2744769.2744897},
doi = {10.1145/2744769.2744897},
abstract = {Refresh operations consume substantial energy and bandwidth in high-density DRAM memory. To cope with this issue, Fine-Grained Refresh (FGR) is recently proposed to eliminate unnecessary refresh operations caused by minor weak cells. Even JEDEC's DDR4 DRAM specification announces the support of FGR to make DRAM refresh more scalable. Unfortunately, we observe that the effectiveness of FGR is greatly confined by the procedure of refresh-oblivious device integration because all memory devices within a module have to be controlled and refreshed in a lockstep way after the step of assembly. In this work, we firstly propose to intelligently integrate the "compatible" devices through a pre-assembly testing and retention-aware matching method. Second, we reuse the reconfiguration structure from yield-oriented remapping mechanism in memory chips and propose Microfix to create a balanced distribution of retention time in memory banks through fine-grained row-address tuning. With this optimization architecture, RADAR, we can eliminate the refresh overhead of produced memory modules by 28\% on average.},
booktitle = {Proceedings of the 52nd Annual Design Automation Conference},
articleno = {19},
numpages = {6},
location = {San Francisco, California},
series = {DAC '15}
}

@inproceedings{10.1145/3027063.3053237,
author = {Savic, Selena and B\"{u}hlmann, Vera},
title = {Digital Literacy in Architecture: How Space is Organized by Computation},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3053237},
doi = {10.1145/3027063.3053237},
abstract = {The integration of architecture and digital technologies happens on an instrumental level, where digital is associated with making the design process more efficient. Architects commonly report on interaction with computers describing the service software has provided. Computational procedures remain obscured by design outputs. In this project, we propose to critically study the relationship of architecture and technology from a perspective of interaction with digital tools. We propose the use of text-mining on a corpus of architectural discourse in social media. With concepts extracted from this initial step, we will conduct a series of experiments on collaborative qualification using a mobile application. We will show how the challenge of organizing a discourse on computational process in architectural design could involve computation in productive new ways. Finally, we will discuss how these insights could enrich the future development of computer-based tools for design.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2909–2914},
numpages = {6},
keywords = {collaborative qualification, digital literacy, computer-aided design, meaning-making, text-mining},
location = {<conf-loc>, <city>Denver</city>, <state>Colorado</state>, <country>USA</country>, </conf-loc>},
series = {CHI EA '17}
}

@inproceedings{10.1145/3366623.3368137,
author = {Barcelona-Pons, Daniel and Garc\'{\i}a-L\'{o}pez, Pedro and Ruiz, \'{A}lvaro and G\'{o}mez-G\'{o}mez, Amanda and Par\'{\i}s, Gerard and S\'{a}nchez-Artigas, Marc},
title = {FaaS Orchestration of Parallel Workloads},
year = {2019},
isbn = {9781450370387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366623.3368137},
doi = {10.1145/3366623.3368137},
abstract = {Function as a Service (FaaS) is based on a reactive programming model where functions are activated by triggers in response to cloud events (e.g., objects added to an object store). The inherent elasticity and the pay-per-use model of serverless functions make them very appropriate for embarrassingly parallel tasks like data preprocessing, or even the execution of MapReduce jobs in the cloud.But current Serverless orchestration systems are not designed for managing parallel fork-join workflows in a scalable and efficient way. We demonstrate in this paper that existing services like AWS Step Functions or Azure Durable Functions incur in considerable overheads, and only Composer at IBM Cloud provides suitable performance.Successively, we analyze the architecture of OpenWhisk as an open-source FaaS systems and its orchestration features (Composer). We outline its architecture problems and propose guidelines for orchestrating massively parallel workloads using serverless functions.},
booktitle = {Proceedings of the 5th International Workshop on Serverless Computing},
pages = {25–30},
numpages = {6},
keywords = {orchestration, FaaS, Serverless, event-based},
location = {Davis, CA, USA},
series = {WOSC '19}
}

@article{10.1145/3057857,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Visual Simple Transformations: Empowering End-Users to Wire Internet of Things Objects},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3057857},
doi = {10.1145/3057857},
abstract = {Empowering end-users to wire Internet of Things (IoT) objects (things and services) together would allow them to more easily conceive and realize interesting IoT solutions. A challenge lies in devising a simple end-user development approach to support the specification of transformations, which can bridge the mismatch in the data being exchanged among IoT objects. To tackle this challenge, we present Visual Simple Transformations (ViSiT) as an approach that allows end-users to use a jigsaw puzzle metaphor for specifying transformations that are automatically converted into underlying executable workflows. ViSiT is explained by presenting meta-models and an architecture for implementing a system of connected IoT objects. A tool is provided for supporting end-users in visually developing and testing transformations. Another tool is also provided for allowing software developers to modify, if they wish, a transformation's underlying implementation. This work was evaluated from a technical perspective by developing transformations and measuring ViSiT's efficiency and scalability and by constructing an example application to show ViSiT's practicality. A study was conducted to evaluate this work from an end-user perspective, and its results showed positive indications of perceived usability, learnability, and the ability to conceive real-life scenarios for ViSiT.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {apr},
articleno = {10},
numpages = {43},
keywords = {End-user development, internet of things, transformations}
}

@inproceedings{10.1145/3199902.3199904,
author = {\v{S}ljivo, Amina and Kerkhove, Dwight and Moerman, Ingrid and De Poorter, Eli and Hoebeke, Jeroen},
title = {Interactive Web Visualizer for IEEE 802.11ah Ns-3 Module},
year = {2018},
isbn = {9781450364133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199902.3199904},
doi = {10.1145/3199902.3199904},
abstract = {The main purpose of running ns-3 simulations is to generate relevant data sets for further study. There are two strategies to generate output from ns-3, either using generic predefined bulk output mechanisms or using the ns-3's Tracing system. Both require parsing the raw output data to extract and process the data of interest to obtain meaningful information. However, parsing such output is in most cases time consuming and prone to mistakes. Post-processing is even harder when a large number of simulations needs to be analyzed and even the tracing system cannot simplify this task. Moreover, results obtained this way are only available once the simulation is finished.Therefore, we developed a user-friendly interactive visualization and post-processing tool for IEEE 802.11ah called ahVisualizer. Beside the topology and MAC configuration, ahVisualizer also plots our traces for each node over time during the simulation, as well as averages and standard deviations for each traced parameter. It can compare all the measured values across different simulations. Users can easily download figures and data in various formats. Moreover, it includes a post-processing tool which plots desired series, with desired fixed parameters, from a large set of simulations. This paper presents the ahVisualizer, its services and its architecture and shows how this tool enables much faster and easier data analysis and monitoring of ns-3 simulations with 802.11ah.},
booktitle = {Proceedings of the 2018 Workshop on Ns-3},
pages = {23–29},
numpages = {7},
keywords = {distributed simulations, IEEE 802.11ah, visualization, post-processing, ns-3, Wi-Fi HaLow, analysis},
location = {Surathkal, India},
series = {WNS3 '18}
}

@inproceedings{10.1145/3195970.3196056,
author = {Yang, Haoyu and Li, Shuhe and Ma, Yuzhe and Yu, Bei and Young, Evangeline F. Y.},
title = {GAN-OPC: Mask Optimization with Lithography-Guided Generative Adversarial Nets},
year = {2018},
isbn = {9781450357005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195970.3196056},
doi = {10.1145/3195970.3196056},
abstract = {Mask optimization has been a critical problem in the VLSI design flow due to the mismatch between the lithography system and the continuously shrinking feature sizes. Optical proximity correction (OPC) is one of the prevailing resolution enhancement techniques (RETs) that can significantly improve mask printability. However, in advanced technology nodes, the mask optimization process consumes more and more computational resources. In this paper, we develop a generative adversarial network (GAN) model to achieve better mask optimization performance. We first develop an OPC-oriented GAN flow that can learn target-mask mapping from the improved architecture and objectives, which leads to satisfactory mask optimization results. To facilitate the training process and ensure better convergence, we also propose a pre-training procedure that jointly trains the neural network with inverse lithography technique (ILT). At convergence, the generative network is able to create quasi-optimal masks for given target circuit patterns and fewer normal OPC steps are required to generate high quality masks. Experimental results show that our flow can facilitate the mask optimization process as well as ensure a better printability.},
booktitle = {Proceedings of the 55th Annual Design Automation Conference},
articleno = {131},
numpages = {6},
location = {San Francisco, California},
series = {DAC '18}
}

@article{10.1109/TNET.2016.2518712,
author = {Afek, Yehuda and Bremler-Barr, Anat and Harchol, Yotam and Hay, David and Koral, Yaron},
title = {Making DPI Engines Resilient to Algorithmic Complexity Attacks},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2518712},
doi = {10.1109/TNET.2016.2518712},
abstract = {This paper starts by demonstrating the vulnerability of Deep Packet Inspection DPI mechanisms, which are at the core of security devices, to algorithmic complexity denial of service attacks, thus exposing a weakness in the first line of defense of enterprise networks and clouds. A system and a multi-core architecture to defend from these algorithmic complexity attacks is presented in the second part of the paper. The integration of this system with two different DPI engines is demonstrated and discussed. The vulnerability is exposed by showing how a simple low bandwidth cache-miss attack takes down the Aho-Corasick AC pattern matching algorithm that lies at the heart of most DPI engines. As a first step in the mitigation of the attack, we have developed a compressed variant of the AC algorithm that improves the worst case performance under an attack. Still, under normal traffic its running-time is worse than classical AC implementations. To overcome this problem, we introduce  ${rm MCA}^{2}$—Multi-Core Architecture to Mitigate Complexity Attacks, which dynamically combines the classical AC algorithm with our compressed implementation, to provide a robust solution to mitigate this cache-miss attack. We demonstrate the effectiveness of our architecture by examining cache-miss algorithmic complexity attacks against DPI engines and show a goodput boost of up to 73\%. Finally, we show that our architecture may be generalized to provide a principal solution to a wide variety of algorithmic complexity attacks.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {3262–3275},
numpages = {14}
}

@article{10.1145/3059149,
author = {Ying, Xuhang and Zhang, Jincheng and Yan, Lichao and Chen, Yu and Zhang, Guanglin and Chen, Minghua and Chandra, Ranveer},
title = {Exploring Indoor White Spaces in Metropolises},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3059149},
doi = {10.1145/3059149},
abstract = {It is a promising vision to exploit white spaces, that is, vacant VHF and UHF TV channels, to meet the rapidly growing demand for wireless data services in both outdoor and indoor scenarios. While most prior works have focused on outdoor white space, the indoor story is largely open for investigation. Motivated by this observation and discovering that 70\% of the spectrum demand comes from indoor environment, we carry out a comprehensive study to explore indoor white spaces. We first conduct a large-scale measurement study and compare outdoor and indoor TV spectrum occupancy at 30+ diverse locations in a typical metropolis—Hong Kong. Our results show that abundant white spaces are available in different areas in Hong Kong, which account for more than 50\% and 70\% of the entire TV spectrum in outdoor and indoor scenarios, respectively. Although there are substantially more white spaces indoors than outdoors, there have been very few solutions for identifying indoor white space. To fill in this gap, we develop the first data-driven, low-cost indoor white space identification system for White-space Indoor Spectrum EnhanceR (WISER), to allow secondary users to identify white spaces for communication without sensing the spectrum themselves. We design the architecture and algorithms to address the inherent challenges. We build a WISER prototype and carry out real-world experiments to evaluate its performance. Our results show that WISER can identify 30\%--40\% more indoor white spaces with negligible false alarms, as compared to alternative baseline approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {9},
numpages = {25},
keywords = {sensor placement, clustering algorithms, TV white spaces}
}

@article{10.1145/3448738,
author = {Shi, Cong and Liu, Jian and Liu, Hongbo and Chen, Yingying},
title = {WiFi-Enabled User Authentication through Deep Learning in Daily Activities},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3448738},
doi = {10.1145/3448738},
abstract = {User authentication is a critical process in both corporate and home environments due to the ever-growing security and privacy concerns. With the advancement of smart cities and home environments, the concept of user authentication is evolved with a broader implication by not only preventing unauthorized users from accessing confidential information but also providing the opportunities for customized services corresponding to a specific user. Traditional approaches of user authentication either require specialized device installation or inconvenient wearable sensor attachment. This article supports the extended concept of user authentication with a device-free approach by leveraging the prevalent WiFi signals made available by IoT devices, such as smart refrigerator, smart TV, and smart thermostat, and so on. The proposed system utilizes the WiFi signals to capture unique human physiological and behavioral characteristics inherited from their daily activities, including both walking and stationary ones. Particularly, we extract representative features from channel state information (CSI) measurements of WiFi signals, and develop a deep-learning-based user authentication scheme to accurately identify each individual user. To mitigate the signal distortion caused by surrounding people’s movements, our deep learning model exploits a CNN-based architecture that constructively combines features from multiple receiving antennas and derives more reliable feature abstractions. Furthermore, a transfer-learning-based mechanism is developed to reduce the training cost for new users and environments. Extensive experiments in various indoor environments are conducted to demonstrate the effectiveness of the proposed authentication system. In particular, our system can achieve over 94\% authentication accuracy with 11 subjects through different activities.},
journal = {ACM Trans. Internet Things},
month = {may},
articleno = {13},
numpages = {25},
keywords = {WiFi signals, User authentication, IoT}
}

@inproceedings{10.1145/3301551.3301582,
author = {Ruangvanich, Supparang and Nilsook, Prachyanun},
title = {Personality Learning Analytics System in Intelligent Virtual Learning Environment},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301582},
doi = {10.1145/3301551.3301582},
abstract = {In this paper, the researchers propose a conceptual for system architecture of learning analytics process in the intelligent learning environment. Within this concept, today's competitive business environment need for businesses in order to implement the monitor and analyze the user-generated data on their own and their competitors. The achievement of competitive advantage is often necessary to listen to and understand what customers are saying about competitors' products and services. Not only personality analytics but also the conceptual description can capture an intelligent learning environment, and it is the analytic tools that are used to improve learning and education. The researchers also discuss how learning analytics is developed in different fields. It closely tied to, a series of other fields of study including business intelligence, web analytics, academic analytics, educational data mining, and action analytics. The researchers believe that conceptual of personality analytics in the intelligent learning environment can play an essential role in managing and analyzing personality and contribute to the concept of personality analytics in the intelligent learning environment. The results of this research could be summarized as follows: learning analytics process should be used as measuring and collecting data about learners and learning with the aim of improving teaching and learning practice through analysis of the data. By achieving this process, it should collect data to report or analyze the happening about the learner. Then, instructors monitor learning what is happening now, while as learning analytics should get what is going to happen in the future for learners. Finally, instructors take action to feedback learners.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {245–250},
numpages = {6},
keywords = {Intelligent Environment, Personal Analytics, Virtual Learning Environment, Learning Analytics, System Architecture},
location = {Hong Kong, Hong Kong},
series = {ICIT '18}
}

@inproceedings{10.1145/2701973.2702098,
author = {de Silva, Lavindra and Yan, Rongjie and Ingrand, Felix and Alami, Rachid and Bensalem, Saddek},
title = {A Verifiable and Correct-by-Construction Controller for Robots in Human Environments},
year = {2015},
isbn = {9781450333184},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701973.2702098},
doi = {10.1145/2701973.2702098},
abstract = {With the increasing use of domestic and service robots alongside humans, it is now becoming crucial to be able to verify whether robot-software is safe, dependable, and correct. Indeed, in the near future it may well be necessary for robot-software developers to provide safety certifications guaranteeing, e.g. that a hospital nursebot will not move too fast while a person is leaning on it, that the arm of a service robot will not unexpectedly open its gripper while holding a glass, or that there will never be a software deadlock while a robot is navigating in an office. To this end, we have provided a framework and software engineering methodology for developing safe and dependable real-world robotic architectures, with a focus on the functional level--the lowest level of a typical layered robotic architecture--which has all the basic action and perception capabilities such as image processing, obstacle avoidance, and motion control. Unlike past work we address the formal verification of the functional level, which allows providing guarantees that it will not do steps leading to undesirable/disastrous outcomes.},
booktitle = {Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts},
pages = {281},
numpages = {1},
keywords = {verification, reliability, human factors},
location = {Portland, Oregon, USA},
series = {HRI'15 Extended Abstracts}
}

@inproceedings{10.1145/2723372.2735363,
author = {Chevalier, Jules and Subercaze, Julien and Gravier, Christophe and Laforest, Fr\'{e}d\'{e}rique},
title = {Slider: An Efficient Incremental Reasoner},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2735363},
doi = {10.1145/2723372.2735363},
abstract = {The Semantic Web has gained substantial momentum over the last decade. It contributes to the manifestation of knowledge from data, and leverages implicit knowledge through reasoning algorithms. The main drawbacks of current reasoning methods over ontologies are two-fold: first they struggle to provide scalability for large datasets, and second, the batch processing reasoners who provide the best scalability so far are unable to infer knowledge from evolving data. We contribute to solving these problems by introducing Slider, an efficient incremental reasoner. Slider goes a significant step beyond existing system, including i) performance, by more than a 70\% improvement in average compared to the fastest reasoner available to the best of our knowledge, and ii) inferences on streams of semantic data, by using intrinsic features that are themselves streams-oriented. Slider is fragment agnostic and conceived to handle expanding data with a growing background knowledge base. It natively supports pdf and RDFS, and its architecture allows to extend it to more complex fragments with a minimal effort. In this demo a web-based interface allows the users to visualize the internal behaviour of Slider during the inference, to better understand its design and principles.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1081–1086},
numpages = {6},
keywords = {web of data, streamed reasoning, incremental reasoning},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3167020.3167061,
author = {Pacheco, Fannia and Exposito, Ernesto and Gineste, Mathieu and Budoin, Cedric},
title = {An Autonomic Traffic Analysis Proposal Using Machine Learning Techniques},
year = {2017},
isbn = {9781450348959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167020.3167061},
doi = {10.1145/3167020.3167061},
abstract = {Network analysis has recently become in one of the most challenging tasks to handle due to the rapid growth of communication technologies. For network management, accurate identification and classification of network traffic is a key task. For example, identifying traffic from different applications is critical to manage bandwidth resources and to ensure Quality of Service objectives. Machine learning emerges as a suitable tool for traffic classification; however, it requires several steps that must be followed adequately in order to achieve the goals. In this paper, we proposed an architecture to perform traffic analysis based on Machine Learning techniques and autonomic computing. We analyze the procedures to perform Machine Learning over traffic network classification, and at the same time we give guidelines to introduce all these procedures into the architecture proposed. The main contribution of our proposal is the reconfiguration of the traffic classifier that will change according to the knowledge acquired from the traffic analysis process.},
booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
pages = {273–280},
numpages = {8},
keywords = {traffic analysis, quality of service, autonomic computing, Machine Learning},
location = {Bangkok, Thailand},
series = {MEDES '17}
}

@inproceedings{10.1145/2590651.2590675,
author = {Coutinho, Emanuel F. and Moreira, Leonardo O. and Paillard, Gabriel A. L. and Maia, Jos\'{e} G. R.},
title = {How to Deploy a Virtual Learning Environment in the Cloud?},
year = {2014},
isbn = {9781450324359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590651.2590675},
doi = {10.1145/2590651.2590675},
abstract = {Cloud computing is a trend of technology aimed at providing on-demand services with payment based on usage. Virtual Learning Environments (VLEs) are applications that require a highly scalable architecture that provides for its users an acceptable level of Quality of Service (QoS). This work aims to show the steps needed to install a VLE in a cloud computing infrastructure. The VLE's migration to this new type of execution environment allows the increase of its use but also brings some performance issues that must be considered. The case study will consider the Moodle VLE which was chosen for its widespread use.},
booktitle = {Proceedings of the 7th Euro American Conference on Telematics and Information Systems},
articleno = {25},
numpages = {4},
keywords = {cloud computing, moodle, virtual learning environment},
location = {Valparaiso, Chile},
series = {EATIS '14}
}

@inproceedings{10.1145/3391614.3399389,
author = {Taguchi, Shuhei and Yamamura, Chigusa and Ohmata, Hisayuki and Sekine, Daisuke and Kajita, Kaisei and Fujii, Arisa},
title = {Sharing Same Elements in User Viewing History Data Securely Through Private Set Intersection Under User-Centric Data Control},
year = {2020},
isbn = {9781450379762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391614.3399389},
doi = {10.1145/3391614.3399389},
abstract = {Recently, various over-the-top (OTT) streaming services as well as traditional broadcasts distribute numerous content every day, allowing users to watch their favorite content at any time. While users can choose from a quantity of content, they often view the same content mainly because most OTT streaming services implement recommendation systems. However, it is often difficult for users to realize the same content they viewed and enjoy sharing their opinions or feelings because they do not know each other when and which content they viewed. For the purpose of encouraging such enjoyable experiences, we propose a system architecture that allows users to share the same content they viewed by using the user's viewing history data. Such viewing history data is currently collected and stored by hundreds of different services and companies. Therefore, our system architecture adopts a user-centric data control model that allows users to collect and store their data on their own online storages, and use it for their purposes. If users are asked to disclose all of the raw data of their viewing history to each other or to third party when sharing, most of them will feel anxiety because the data often contains sensitive personal information. Therefore, we introduce a method using private set intersection (PSI), a cryptographic technique that allows users to share the same elements in the users’ viewing history data without revealing anything to each other except the elements at the intersection. We also demonstrate the feasibility of the architecture through use cases.},
booktitle = {Proceedings of the 2020 ACM International Conference on Interactive Media Experiences},
pages = {138–142},
numpages = {5},
keywords = {Viewing history, Private set intersection, Personal data, Decentralization, User-centric, GDPR;},
location = {Cornella, Barcelona, Spain},
series = {IMX '20}
}

@inproceedings{10.1145/3426462.3426467,
author = {Ibrahim, Seif and Harrison, Cyrus and Larsen, Matthew},
title = {JIT’s Complicated: A Comprehensive System For Derived Field Generation},
year = {2020},
isbn = {9781450388122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426462.3426467},
doi = {10.1145/3426462.3426467},
abstract = {Derived field calculations are a vital part of the visualization and analysis workflow. These calculations allow simulation users to create important quantities of interest that are not generated by the simulation, and systems that calculate derived quantities must be flexible enough to accommodate a wide variety of user requests. In situ analysis imposes additional constraints on the system, and derived field calculations must be able to leverage the same resources as the simulation to minimize the runtime and memory usage. Just-in-time (JIT) compilation defers code creation until runtime, and a JIT based system is capable of fusing a complex expression into a single kernel invocation (i.e., kernel fusion). Without kernel fusion, the system would be forced to evaluate each piece of the expression (e.g., an operator or function call) as separate kernel invocations, which increases both runtime and memory pressure on the host simulation. In this paper, we present a production-oriented in situ derived field system that leverages JIT compilation to target heterogeneous HPC architectures. Additionally, we explore the runtime costs of using this system to calculate three expressions in three simulation codes.},
booktitle = {ISAV'20 In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization},
pages = {27–31},
numpages = {5},
location = {Atlanta, GA, USA},
series = {ISAV'20}
}

@inproceedings{10.1145/2976749.2978361,
author = {Holzinger, Philipp and Triller, Stefan and Bartel, Alexandre and Bodden, Eric},
title = {An In-Depth Study of More Than Ten Years of Java Exploitation},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978361},
doi = {10.1145/2976749.2978361},
abstract = {When created, the Java platform was among the first runtimes designed with security in mind. Yet, numerous Java versions were shown to contain far-reaching vulnerabilities, permitting denial-of-service attacks or even worse allowing intruders to bypass the runtime's sandbox mechanisms, opening the host system up to many kinds of further attacks.This paper presents a systematic in-depth study of 87 publicly available Java exploits found in the wild. By collecting, minimizing and categorizing those exploits, we identify their commonalities and root causes, with the goal of determining the weak spots in the Java security architecture and possible countermeasures.Our findings reveal that the exploits heavily rely on a set of nine weaknesses, including unauthorized use of restricted classes and confused deputies in combination with caller-sensitive methods. We further show that all attack vectors implemented by the exploits belong to one of three categories: single-step attacks, restricted-class attacks, and information hiding attacks.The analysis allows us to propose ideas for improving the security architecture to spawn further research in this area.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {779–790},
numpages = {12},
keywords = {java security, exploits, access control, security analysis},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/2590651.2590682,
author = {Urra, Enrique and Cabrera-Paniagua, Daniel and Cubillos, Claudio},
title = {Towards a Distributed Hyperheuristic Deploy Architecture},
year = {2014},
isbn = {9781450324359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590651.2590682},
doi = {10.1145/2590651.2590682},
abstract = {The hyperheuristic term is known in the optimization field as an automated methodology for selecting or generating heuristics to solve hard computational search problems. From the design perspective, it is based on decoupling the solving intelligence from the domain expertise, allowing to reuse the same solver for multiple, usually related problem domains. There are few works in which hyperheuristics have been designed and evaluated in distributed environments. In this paper, we propose a conceptual design of a distributed hyperheuristic architecture, from the problem domain deploying perspective, which allows to communicate different optimization environments (such as solver and domain) and to offering a "solving service". Different problems domains could be addressed using an encapsulated hyperheuristic solver, and through well defined interfaces, users can provide different heuristic components to perform the optimization process. The proposed architecture is only an initial step for which different modeling, design and implementation issues must be addressed. Such research should be focused on defining how conceptual design contributions must be leveraged to implement well defined interfaces, capable of connecting hyperheuristic solvers and problem domains within distributed environments.},
booktitle = {Proceedings of the 7th Euro American Conference on Telematics and Information Systems},
articleno = {31},
numpages = {4},
keywords = {hyperheuristics, optimization, distributed architecture},
location = {Valparaiso, Chile},
series = {EATIS '14}
}

@inproceedings{10.5555/3437539.3437714,
author = {Dev, Sundar and Lo, David and Cheng, Liqun and Ranganathan, Parthasarathy},
title = {Autonomous Warehouse-Scale Computers},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Modern Warehouse-Scale Computers (WSCs), composed of many generations of servers and a myriad of domain specific accelerators, are becoming increasingly heterogeneous. Meanwhile, WSC workloads are also becoming incredibly diverse with different communication patterns, latency requirements, and service level objectives (SLOs). Insufficient understanding of the interactions between workload characteristics and the underlying machine architecture leads to resource over-provisioning, thereby significantly impacting the utilization of WSCs.We present Autonomous Warehouse-Scale Computers, a new WSC design that leverages machine learning techniques and automation to improve job scheduling, resource management, and hardware-software co-optimization to address the increasing heterogeneity in WSC hardware and workloads. Our new design introduces two new layers in the WSC stack, namely: (a) a Software-Defined Server (SDS) Abstraction Layer which redefines the hardware-software boundary and provides greater control of the hardware to higher layers of the software stack through stable abstractions; and (b) a WSC Efficiency Layer which regularly monitors the resource usage of workloads on different hardware types, autonomously quantifies the performance sensitivity of workloads to key system configurations, and continuously improves scheduling decisions and hardware resource QoS policies to maximize cluster level performance. Our new WSC design has been successfully deployed across all WSCs at Google for several years now. The new WSC design improves throughput of workloads (by 7--10\%, on average), increases utilization of hardware resources (up to 2x), and reduces performance variance for critical workloads (up to 25\%).},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {175},
numpages = {6},
keywords = {machine learning, heterogeneity, WSC, automation},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.1109/TCBB.2017.2665542,
author = {Chai, Guoshi and Yu, Min and Jiang, Lixu and Duan, Yaocong and Huang, Jian},
title = {HMMCAS: A Web Tool for the Identification and Domain Annotations of CAS Proteins},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2017.2665542},
doi = {10.1109/TCBB.2017.2665542},
abstract = {The CRISPR-Cas clustered regularly interspaced short palindromic repeats-CRISPR-associated proteins adaptive immune systems are discovered in many bacteria and most archaea. These systems are encoded by cas CRISPR-associated operons that have an extremely diverse architecture. The most crucial step in the depiction of cas operons composition is the identification of cas genes or Cas proteins. With the continuous increase of the newly sequenced archaeal and bacterial genomes, the recognition of new Cas proteins is becoming possible, which not only provides candidates for novel genome editing tools but also helps to understand the prokaryotic immune system better. Here, we describe HMMCAS, a web service for the detection of CRISPR-associated structural and functional domains in protein sequences. HMMCAS uses hmmscan similarity search algorithm in HMMER3.1 to provide a fast, interactive service based on a comprehensive collection of hidden Markov models of Cas protein family. It can accurately identify the Cas proteins including those fusion proteins, for example the Cas1-Cas4 fusion protein in Candidatus Chloracidobacterium thermophilum B Cab. thermophilum B. HMMCAS can also find putative cas operon and determine which type it belongs to. HMMCAS is freely available at http://i.uestc.edu.cn/hmmcas.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {1313–1315},
numpages = {3}
}

@inproceedings{10.1145/3109761.3109783,
author = {Bacciu, Davide and Chessa, Stefano and Gallicchio, Claudio and Micheli, Alessio},
title = {On the Need of Machine Learning as a Service for the Internet of Things},
year = {2017},
isbn = {9781450352437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109761.3109783},
doi = {10.1145/3109761.3109783},
abstract = {In recent years we are witnessing a rapid increase in the diffusion of the Internet of Things (IoT) technology, with a large scale adoption of interconnected heterogeneous devices that are pervasively collecting information through the interaction with humans in their environment. The adoption of Machine Learning (ML) methodologies can play a fundamental role, allowing smarter IoT applications to continuously adapt to evolving environmental conditions and user's needs. In this context, the time is now ripe for a decisive step forward in the direction of a systematic integration of ML functionalities within the IoT platform.In this paper, we outline the principles that should guide the realization of a ML service for the IoT, proposing a conceptual architecture of such a learning service, integrated within the IoT reference model. Our proposal leverages on the experience of recent successful European initiatives that led to the realization of intelligent sensor networks built on the synergy between resource efficient ML models for temporal data processing and wireless sensor networks. The relevant impact of ML in applicative domains of interest for the IoT is also enucleated through a brief summary of recent results.},
booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
articleno = {22},
numpages = {8},
keywords = {machine learning service, adaptive IoT applications, intelligent sensor networks, distributed learning service, internet of things},
location = {Liverpool, United Kingdom},
series = {IML '17}
}

@inproceedings{10.1145/2611765.2611773,
author = {Dhawan, Udit and Vasilakis, Nikos and Rubin, Raphael and Chiricescu, Silviu and Smith, Jonathan M. and Knight, Thomas F. and Pierce, Benjamin C. and DeHon, Andr\'{e}},
title = {PUMP: A Programmable Unit for Metadata Processing},
year = {2014},
isbn = {9781450327770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611765.2611773},
doi = {10.1145/2611765.2611773},
abstract = {We introduce the Programmable Unit for Metadata Processing (PUMP), a novel software-hardware element that allows flexible computation with uninterpreted metadata alongside the main computation with modest impact on runtime performance (typically 10--40\% for single policies, compared to metadata-free computation on 28 SPEC CPU2006 C, C++, and Fortran programs). While a host of prior work has illustrated the value of ad hoc metadata processing for specific policies, we introduce an architectural model for extensible, programmable metadata processing that can handle arbitrary metadata and arbitrary sets of software-defined rules in the spirit of the time-honored 0-1-∞ rule. Our results show that we can match or exceed the performance of dedicated hardware solutions that use metadata to enforce a single policy, while adding the ability to enforce multiple policies simultaneously and achieving flexibility comparable to software solutions for metadata processing. We demonstrate the PUMP by using it to support four diverse safety and security policies---spatial and temporal memory safety, code and data taint tracking, control-flow integrity including return-oriented-programming protection, and instruction/data separation---and quantify the performance they achieve, both singly and in combination.},
booktitle = {Proceedings of the Third Workshop on Hardware and Architectural Support for Security and Privacy},
articleno = {8},
numpages = {8},
keywords = {memory safety, taint tracking, tagged architecture, metadata, control-flow integrity, security},
location = {Minneapolis, Minnesota, USA},
series = {HASP '14}
}

@inproceedings{10.1145/3394885.3431616,
author = {Chakaravarthy, Ravikumar V. and Kwon, Hyun and Jiang, Hua},
title = {Vision Control Unit in Fully Self Driving Vehicles Using Xilinx MPSoC and Opensource Stack},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431616},
doi = {10.1145/3394885.3431616},
abstract = {Fully self-driving (FSD) vehicles are becoming increasing popular over the last few years and companies are investing significantly into its research and development. In the recent years, FSD technology innovators like Tesla, Google etc. have been working on proprietary autonomous driving stacks and have been able to successfully bring the vehicle to the roads. On the other end, organizations like Autoware Foundation and Baidu are fueling the growth of self-driving mobility using open source stacks. These organizations firmly believe in enabling autonomous driving technology for everyone and support developing software stacks through the open source community that is SoC vendor agnostic. In this proposed solution we describe a vision control unit for a fully self-driving vehicle developed on Xilinx MPSoC platform using open source software components.The vision control unit of an FSD vehicle is responsible for camera video capture, image processing and rendering, AI algorithm processing, data and meta-data transfer to next stage of the FSD pipeline. In this proposed solution we have used many open source stacks and frameworks for video and AI processing. The processing of the video pipeline and algorithms take full advantage of the pipelining and parallelism using all the heterogenous cores of the Xilinx MPSoC. In addition, we have developed an extensible, scalable, adaptable and configurable AI backend framework, XTA, for acceleration purposes that is derived from a popular, open source AI backend framework, TVM-VTA. XTA uses all the MPSoC cores for its computation in a parallel and pipelined fashion. XTA also adapts to the compute and memory parameters of the system and can scale to achieve optimal performance for any given AI problem. The FSD system design is based on a distributed system architecture and uses open source components like Autoware for autonomous driving algorithms, ROS and Distributed Data Services as a messaging middleware between the functional nodes and a real-time kernel to coordinate the actions. The details of image capture, rendering and AI processing of the vision perception pipeline will be presented along with the performance measurements of the vision pipeline.In this proposed solution we will demonstrate some of the key use cases of vision perception unit like surround vision and object detection. In addition, we will also show the capability of Xilinx MPSoC technology to handle multiple channels of real time camera and the integration with the Lidar/Radar point cloud data to feed into the decision-making unit of the overall system. The system is also designed with the capability to update the vision control unit through Over the Air Update (OTA). It is also envisioned that the core AI engine will require regular updates with the latest training values; hence a built-in platform level mechanism supporting such capability is essential for real world deployment.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {311–317},
numpages = {7},
keywords = {MPSoC, Vision Pipeline, AI, ROS, FSD, Autoware, XTA, Heterogenous Processing},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@article{10.1145/2894750,
author = {To, Quoc-Cuong and Nguyen, Benjamin and Pucheral, Philippe},
title = {Private and Scalable Execution of SQL Aggregates on a Secure Decentralized Architecture},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/2894750},
doi = {10.1145/2894750},
abstract = {Current applications, from complex sensor systems (e.g., quantified self) to online e-markets, acquire vast quantities of personal information that usually end up on central servers where they are exposed to prying eyes. Conversely, decentralized architectures that help individuals keep full control of their data complexify global treatments and queries, impeding the development of innovative services. This article aims precisely at reconciling individual's privacy on one side and global benefits for the community and business perspectives on the other. It promotes the idea of pushing the security to secure hardware devices controlling the data at the place of their acquisition. Thanks to these tangible physical elements of trust, secure distributed querying protocols can reestablish the capacity to perform global computations, such as Structured Query Language (SQL) aggregates, without revealing any sensitive information to central servers. This article studies how to secure the execution of such queries in the presence of honest-but-curious and malicious attackers. It also discusses how the resulting querying protocols can be integrated in a concrete decentralized architecture. Cost models and experiments on SQL/Asymmetric Architecture (AA), our distributed prototype running on real tamper-resistant hardware, demonstrate that this approach can scale to nationwide applications.},
journal = {ACM Trans. Database Syst.},
month = {aug},
articleno = {16},
numpages = {43},
keywords = {parallel computing, decentralized architecture, Trusted hardware}
}

@article{10.1109/TNET.2016.2627005,
author = {Habibi Gharakheili, Hassan and Sivaraman, Vijay and Moors, Tim and Vishwanath, Arun and Matthews, John and Russell, Craig},
title = {Enabling Fast and Slow Lanes for Content Providers Using Software Defined Networking},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2627005},
doi = {10.1109/TNET.2016.2627005},
abstract = {Residential broadband consumption is growing rapidly, increasing the gap between Internet service provider ISP costs and revenues. Meanwhile, proliferation of Internet-enabled devices is congesting access networks, degrading end-user experience, and affecting content provider monetization. In this paper, we propose a new model whereby the content provider explicitly signals fast- and slow-lane requirements to the ISP on a per-flow basis, using open APIs supported through software defined networking SDN. Our first contribution is to develop an architecture that supports this model, presenting arguments on why this benefits consumers better user experience, ISPs two-sided revenue, and content providers fine-grained control over peering arrangement. Our second contribution is to evaluate our proposal using a real trace of over 10 million flows to show that video flow quality degradation can be nearly eliminated by the use of dynamic fast-lanes, and web-page load times can be hugely improved by the use of slow-lanes for bulk transfers. Our third contribution is to develop a fully functional prototype of our system using open-source SDN components Openflow switches and POX controller modules and instrumented video/file-transfer servers to demonstrate the feasibility and performance benefits of our approach. Our proposal is a first step towards the long-term goal of realizing open and agile access network service quality management that is acceptable to users, ISPs, and content providers alike.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1373–1385},
numpages = {13}
}

@inproceedings{10.1145/3127041.3131363,
author = {Grosch, Franz-Josef},
title = {Elevate Embedded Real-Time Programming with a Synchronous Language},
year = {2017},
isbn = {9781450350938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127041.3131363},
doi = {10.1145/3127041.3131363},
abstract = {Product development at companies such as Bosch requires systems engineering for digital hardware and mechatronic components as well as software engineering for resource-constrained real-time applications cooperating with distributed server applications. While many of the involved engineering disciplines greatly benefit from model-based approaches and from advances in software infrastructures, deeply embedded software still is written in C since the seventies and runs on platforms designed in the nineties (e.g. OSEK). Simulation tools like Simulink or Modelica are used to test discrete code against continuous plant models or to generate code for certain aspects, but they do not really provide modern implementation technologies to address software architecture and qualities or to make embedded programming "attractive" for software professionals.We regard synchronous languages as suitable to solve many of the issues in the integration (causality) and synchronisation (clocks) of time-triggered and event-triggered embedded functions that exhibit their behaviour over time steps and are coordinated according to their mode-switching in a structured synchronous control flow. Searching for an imperative synchronous language (with deterministic concurrent composition, and synchronous control flow), equipped with features for encapsulation and composition (objects, packages, separate compilation) and supporting programming parallel tasks deployed to separate cores (clock refinement and deterministic inter-task communication), we ended up in designing our own language, suitable for resource-constrained, real-time applications running on multi-core controllers.We will explain the main requirements and features of this language, how they integrate with the principles of a synchronous language, how they can be applied to typical everyday problems in embedded development, and how such locally synchronous services may integrate in a globally asynchronous service architecture.},
booktitle = {Proceedings of the 15th ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {156},
numpages = {1},
location = {Vienna, Austria},
series = {MEMOCODE '17}
}

@inproceedings{10.1145/3456415.3456416,
author = {Chen, Ken and He, Jiabei},
title = {Big-Data-Based Research on the Architecture Design of University Hydropower Intelligent Decision Service Platform},
year = {2021},
isbn = {9781450389174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456415.3456416},
doi = {10.1145/3456415.3456416},
abstract = {With the continuous development and wide application of big data and artificial intelligence technology, how to efficiently use and mine the whole process data of university hydropower models, perception, business and flows, and realize the transformation of informationization of hydropower management to intelligentialize and wisdom, it has become one of the main tasks in the construction of universitiy informatization under the strategy of advocating energy conservation, lowcarbon sustainable development. Combining with the actual demand of university hydropower management, managing and serving the whole process of hydropower data collection, storage, analysis, monitoring and decision-making assistance, this paper proposes the architecture of an intelligent decision-making service platform for university hydropower on big data, and sorts out the core and key technologies in the platform development process and the current mainstream development frameworks and tools to provide technical references for the realization of intelligent hydropower management and application services in universities, and promote the overall planning and step-by-step implementation of smart campuses.},
booktitle = {Proceedings of the 2021 9th International Conference on Communications and Broadband Networking},
pages = {1–5},
numpages = {5},
keywords = {hydropower information, big data, intelligentization},
location = {Shanghai, China},
series = {ICCBN '21}
}

@article{10.1145/2724942.2724952,
author = {Souza, Jeferson L. R. and Rufino, Jos\'{e}},
title = {The Wi-STARK Architecture for Resilient Real-Time Wireless Communications},
year = {2015},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
url = {https://doi.org/10.1145/2724942.2724952},
doi = {10.1145/2724942.2724952},
abstract = {Networking communications play an important role to secure a dependable and timely operation of distributed and real-time embedded system applications; however, an effective real-time support is not yet properly addressed in the wireless realm. This paper presents Wi-STARK, a novel architecture for resilient and real-time wireless communications within an one-hop communication domain. Low level reliable (frame) communications, node failure detection, membership management, and networking partition control are provided; since these low level services extend and build upon the exposed interface offered by networking technologies, Wi-STARK is in strict compliance with wireless communication standards, such as IEEE 802.15.4 and IEEE 802.11p. The Wi-STARK service interface is then offered as operating system primitives, helpful for building distributed control applications. The one-hop dependability and timeliness guarantees offered by Wi-STARK are a fundamental step towards an effective design of real-time wireless networks with multiple hops, including end-to-end schedulability analysis of networking operations.},
journal = {SIGBED Rev.},
month = {jan},
pages = {61–66},
numpages = {6},
keywords = {timeliness, Wi-STARK, fault tolerance, resilience, wireless communications, dependability, real-time}
}

@inproceedings{10.5555/3374138.3374174,
author = {Kaya, M. Cagri and Karamanlioglu, Alper and \c{C}etinta\c{s}, undefined. \c{C}a\u{g}lar and \c{C}ilden, Erkin and Canberi, Haluk and O\u{g}uzt\"{u}z\"{u}n, Halit},
title = {A Configurable Gateway for DDS-HLA Interoperability},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Interoperability is a challenge for constructing Live-Virtual-Constructive (LVC) systems. This study is a step toward LVC interoperability adhering to a gateway-based approach with a particular focus on two standard middleware, namely, Data Distribution Service for Real-Time Systems (DDS) and High-Level Architecture (HLA) for distributed simulation. A gateway is designed and implemented to achieve DDS-HLA interoperability. This gateway has the ability to realize two-way data transfer between DDS and HLA systems. The gateway design is based on the idea of a configurable connector that is equipped with variability capabilities. It can perform data-type conversions between these two systems according to the mappings specified by the user. In a case study, the gateway is integrated into a distributed tactical environment simulation system.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {36},
numpages = {11},
keywords = {high-level architecture, data distribution service, gateway, interoperability},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@inproceedings{10.1109/ICCPS.2014.6843740,
author = {Zhang, Jiaxing and Qiu, Hanjiao and Shamsabadi, Salar Shahini and Birken, Ralf and Schirner, Gunar},
title = {WiP Abstract: System-Level Integration of Mobile Multi-Modal Multi-Sensor Systems},
year = {2014},
isbn = {9781479949304},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCPS.2014.6843740},
doi = {10.1109/ICCPS.2014.6843740},
abstract = {Heterogeneous roaming sensor systems have gained significant importance in many domains of civil infrastructure performance inspection as they accelerate data collection and analysis. However, designing such systems is challenging due to the immense complexity in the heterogeneity and processing demands of the involved sensors. Unifying frameworks are needed to simplify development, deployment and operation of roaming sensors and computing units.To address the sensing needs, we propose SIROM3, a Scalable Intelligent Roaming Multi-Modal Multi-Sensor framework. SIROM3 incorporates a CPS approach for infrastructure performance monitoring to address the following challenges: 1. Scalability and expandability. It offers a scalable and expandable solution enabling diversity in sensing and the growth in processing platforms from sensors to control centers. 2. Fusion foundations. SIROM3 enables fusion of data collected by logically and geo-spatially distributed sensors. 3. Big data handling. Automatic collection, categorization, storage and manipulation of heterogeneous large volume of data streams. 4. Automation. SIROM3 minimizes human interaction through full automation from data acquisition to visualization of the fused results.Illustrated in Fig. 1, SIROM3 realizes scalability and expandability in a system-level design approach encapsulating common functionality across hierarchical components in a Run-Time Environment (RTE). The RTE deploys a layered design paradigm defining services in both software and hardware architectures. Equipped with multiple RTE-enabled Multi-Sensor Aggregators (MSA), an array of Roaming Sensor Systems (RSS) operate as mobile agents attached to vehicles to provide distributed computing services regulated by Fleet Control and Management (FCM) center via communication network. A series of foundational services including the Precise Timing Protocol (PTP), GPS timing systems, Distance Measurement Instruments (DMI) through middleware services (CORBA) embedded in the RTE build the fusion foundations for data correlation and analysis. A Heterogeneous Stream File-system Overlay (HSFO) alleviates the big data challenge. It facilitates storing, processing, categorizing and fusing large heterogeneous data stream collected by versatile sensors. A GIS visualization module is integrated for visual analysis and monitoring.SIROM3 enables coordination and collaboration across sensors, MSAs and RSSes, which produce high volume of heterogeneous data stored in HSFO. To fuse the data efficiently, SIROM3 contains an expandable plugin system (part of RTE) for rapid algorithm prototyping using data streams in the architectural hierarchy (i.e. from MSAs to FCM) via the HSFO API. This makes an ideal test-bed to develop new algorithms and methodologies expanding CPS principles to civil infrastructure performance monitoring. In result, SIROM3 simplifies the development, construction and operation of roaming multi-modal multi-sensor systems.We demonstrate the efficiency of SIROM3 by automating the assessment of road surface conditions at the city scale. We realized an RSS with 6 MSAs and 30 heterogeneous sensors, including radars, microphones, GPS and cameras, all deployed onto a van sponsored by the VOTERS (Versatile Onboard Traffic Embedded Roaming Sensors) project. Over 20 terabytes of data have been collected, aggregated, fused, analyzed and geo-spatially visualized using SIROM3 for studying the pavement conditions of the city of Brockton, MA covering 300 miles. The expandability of SIROM3 is shown by adding a millimeter-wave radar needing less than 50 lines of C++ code for system integration. SIROM3 offers a unified solution for comprehensive roadway assessment and evaluation. The integrated management of big data (from collection to automated processing) is an ideal research platform for automated assessment of civil infrastructure performance.},
booktitle = {ICCPS '14: ACM/IEEE 5th International Conference on Cyber-Physical Systems (with CPS Week 2014)},
pages = {227},
numpages = {1},
location = {Berlin, Germany},
series = {ICCPS '14}
}

@article{10.1145/3242901,
author = {Rolin, Raphael and Antaluca, Eduard and Batoz, Jean-Louis and Lamarque, Fabien and Lejeune, Mathieu},
title = {From Point Cloud Data to Structural Analysis Through a Geometrical HBIM-Oriented Model},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3242901},
doi = {10.1145/3242901},
abstract = {The assessment of the structural behavior of historic masonry structures like Gothic cathedrals is an important engineering and architectural issue, because of the economic and cultural relevance of such buildings. In this article, we present a complete numerical methodology for point clouds processing, geometrical and parametric 3D modeling, and finite element structural analysis of the spire of the Cathedral of Senlis, France. Our work highlights the particular difficulties linked with digitization and geometrical modeling of highly complex Gothic structures, as well as the need to find compromises between quality and accuracy of extracted data used for geometrical modeling and structural analysis.The methodology enables the semi-automatic transformation of a three-dimensional points cloud, surveyed through terrestrial laser scanner, into a three-dimensional geometrical historic building information modeling (hBIM)-oriented model, and its use to propose a consistent 3D finite element mesh suitable for advanced structural analysis. A full software chain is integrated in the proposed numerical process, so as to use the most important data contained in the real geometry and accurately transposed in the point clouds. After a successful data processing step with 3DReshaper software that proved to be necessary for enhancement of point clouds, a semi-automated geometrical hBIM-oriented modeling step with Rhinoceros5 software and VisualARQ plugin has allowed the construction of a hybrid model by reverse engineering from the point clouds. This 3D model, containing both geometrical and parametric data of the structure, has been exported to the Hyperworks suite for finite element structural analysis under self-weight. Our computations focused on the estimation of the structure deformation and on the distribution of compression and traction stresses in all components of the complex structure. It is found that the spire is safe. Based on reliable and properly detailed results, our study provides significant information for understanding the behavior of the structure and potential damage monitoring.},
journal = {J. Comput. Cult. Herit.},
month = {may},
articleno = {9},
numpages = {26},
keywords = {cultural heritage, finite element structural analysis, point clouds, building information modeling, geometrical modeling, Historical buildings, terrestrial laser scanning}
}

@inproceedings{10.1145/3447545.3451195,
author = {Calzarossa, Maria Carla and Massari, Luisa and Tessera, Daniele},
title = {Performance Monitoring Guidelines},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451195},
doi = {10.1145/3447545.3451195},
abstract = {Monitoring, that is, the process of collecting measurements on infrastructures and services, is an important subject of performance engineering. Although monitoring is not a new education topic, nowadays its relevance is rapidly increasing and its application is particularly demanding due to the complex distributed architectures of new and emerging technologies. As a consequence, monitoring has become a "must have" skill for students majoring in computer science and in computing-related fields. In this paper, we present a set of guidelines and recommendations to plan, design and setup sound monitoring projects. Moreover, we investigate and discuss the main challenges to be faced to build confidence in the entire monitoring process and ensure measurement quality. Finally, we describe practical applications of these concepts in teaching activities.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {109–114},
numpages = {6},
keywords = {measurement platforms, performance engineering, performance monitoring, measurement quality, passive measurements, active measurements},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1145/3390605,
author = {Ismail, Leila and Materwala, Huned},
title = {Computing Server Power Modeling in a Data Center: Survey, Taxonomy, and Performance Evaluation},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3390605},
doi = {10.1145/3390605},
abstract = {Data centers are large-scale, energy-hungry infrastructure serving the increasing computational demands as the world is becoming more connected in smart cities. The emergence of advanced technologies such as cloud-based services, internet of things (IoT), and big data analytics has augmented the growth of global data centers, leading to high energy consumption. This upsurge in energy consumption of the data centers not only incurs the issue of surging high cost (operational and maintenance) but also has an adverse effect on the environment. Dynamic power management in a data center environment requires the cognizance of the correlation between the system and hardware-level performance counters and the power consumption. Power consumption modeling exhibits this correlation and is crucial in designing energy-efficient optimization strategies based on resource utilization. Several works in power modeling are proposed and used in the literature. However, these power models have been evaluated using different benchmarking applications, power-measurement techniques, and error-calculation formulas on different machines. In this work, we present a taxonomy and evaluation of 24 software-based power models using a unified environment, benchmarking applications, power-measurement techniques, and error formulas, with the aim of achieving an objective comparison. We use different server architectures to assess the impact of heterogeneity on the models’ comparison. The performance analysis of these models is elaborated in the article.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {58},
numpages = {34},
keywords = {machine learning, Data center, green computing, server power consumption modeling, resource utilization, energy-efficiency}
}

@inproceedings{10.1145/3404835.3462878,
author = {Li, Houyi and Chen, Zhihong and Li, Chenliang and Xiao, Rong and Deng, Hongbo and Zhang, Peng and Liu, Yongchao and Tang, Haihong},
title = {Path-Based Deep Network for Candidate Item Matching in Recommenders},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462878},
doi = {10.1145/3404835.3462878},
abstract = {The large-scale recommender system mainly consists of two stages: matching and ranking. The matching stage (also known as the retrieval step) identifies a small fraction of relevant items from billion-scale item corpus in low latency and computational cost. Item-to-item collaborative filtering (item-based CF) and embedding-based retrieval (EBR) have been long used in the industrial matching stage owing to its efficiency. However, item-based CF is hard to meet personalization, while EBR has difficulty in satisfying diversity. In this paper, we propose a novel matching architecture, Path-based Deep Network (named PDN), through incorporating both personalization and diversity to enhance matching performance. Specifically, PDN is comprised of two modules: Trigger Net and Similarity Net. PDN utilizes Trigger Net to capture the user's interest in each of his/her interacted item. Similarity Net is devised to evaluate the similarity between each interacted item and the target item based on these items' profile and CF information. The final relevance between the user and the target item is calculated by explicitly considering user's diverse interests, ie aggregating the relevance weights of the related two-hop paths (one hop of a path corresponds to user-item interaction and the other to item-item relevance). Furthermore, we describe the architecture design of the proposed PDN in a leading real-world E-Commerce service (Mobile Taobao App). Based on offline evaluations and online A/B test, we show that PDN outperforms the existing solutions for the same task. The online results also demonstrate that PDN can retrieve more personalized and more diverse items to significantly improve user engagement. Currently, PDN system has been successfully deployed at Mobile Taobao App and handling major online traffic.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1493–1502},
numpages = {10},
keywords = {recommendation systems, deep learning},
location = {<conf-loc>, <city>Virtual Event</city>, <country>Canada</country>, </conf-loc>},
series = {SIGIR '21}
}

@inproceedings{10.1145/3267955.3267972,
author = {Sardara, Mauro and Muscariello, Luca and Compagno, Alberto},
title = {A Transport Layer and Socket API for (h)ICN: Design, Implementation and Performance Analysis},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3267972},
doi = {10.1145/3267955.3267972},
abstract = {In this paper we present the design of a transport layer and socket API that can be used in several ICN architectures such as NDN, CCN and hICN. The current design makes it possible to expose an API that is simple to insert in current applications and easy to use to develop novel ones. The proliferation of connected applications for very different use cases and services with wide spectrum of requirements suggests that several transport services will coexist in the Internet. This is just about to happen with QUIC, MPTCP, LEDBAT as the most notable ones but is expected to grow and diversify with the advent of applications for 5G, IoT, MEC with heterogeneous connectivity. The advantages of ICN have to be measurable from the application, end-services and in the network, with relevant key performance indicators. We have implemented an high speed transport stack with most of the designed features that we present in this paper with extensive experiments and benchmarks to show the scalability of the current systems in different use cases.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {137–147},
numpages = {11},
keywords = {socket API, transport services, ICN},
location = {Boston, Massachusetts},
series = {ICN '18}
}

