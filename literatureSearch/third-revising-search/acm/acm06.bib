@inproceedings{10.1145/3005745.3005762,
author = {Tilmans, Olivier and B\"{u}hler, Tobias and Vissicchio, Stefano and Vanbever, Laurent},
title = {Mille-Feuille: Putting ISP Traffic under the Scalpel},
year = {2016},
isbn = {9781450346610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3005745.3005762},
doi = {10.1145/3005745.3005762},
abstract = {For Internet Service Provider (ISP) operators, getting an accurate picture of how their network behaves is challenging. Given the traffic volumes that their networks carry and the impossibility to control end-hosts, ISP operators are typically forced to randomly sample traffic, and rely on aggregated statistics. This provides coarse-grained visibility, at a time resolution that is far from ideal (seconds or minutes). In this paper, we present Mille-Feuille, a novel monitoring architecture that provides fine-grained visibility over ISP traffic. Mille-Feuille schedules activation and deactivation of traffic-mirroring rules, that are then provisioned network-wide from a central location, within milliseconds. By doing so, Mille-Feuille combines the scalability of sampling with the visibility and controllability of traffic mirroring. As a result, it supports a set of monitoring primitives, ranging from checking key performance indicators (e.g., one-way delay) for single destinations to estimating traffic matrices in sub-seconds. Our preliminary measurements on existing routers confirm that Mille-Feuille is viable in practice.},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
pages = {113–119},
numpages = {7},
location = {Atlanta, GA, USA},
series = {HotNets '16}
}

@inproceedings{10.1145/3018981.3018986,
author = {Mell, Peter and Shook, James and Harang, Richard},
title = {Measuring and Improving the Effectiveness of Defense-in-Depth Postures},
year = {2016},
isbn = {9781450347884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018981.3018986},
doi = {10.1145/3018981.3018986},
abstract = {Defense-in-depth is an important security architecture principle that has significant application to industrial control systems (ICS), cloud services, storehouses of sensitive data, and many other areas. We claim that an ideal defense-in-depth posture is 'deep', containing many layers of security, and 'narrow', the number of node independent attack paths is minimized. Unfortunately, accurately calculating both depth and width is difficult using standard graph algorithms because of a lack of independence between multiple vulnerability instances (i.e., if an attacker can penetrate a particular vulnerability on one host then they can likely penetrate the same vulnerability on another host). To address this, we represent known weaknesses and vulnerabilities as a type of colored attack graph. We measure depth and width through solving the shortest color path and minimum color cut problems. We prove both of these to be NP-Hard and thus for our solution we provide a suite of greedy heuristics. We then empirically apply our approach to large randomly generated networks as well as to ICS networks generated from a published ICS attack template. Lastly, we discuss how to use these results to help guide improvements to defense-in-depth postures.},
booktitle = {Proceedings of the 2nd Annual Industrial Control System Security Workshop},
pages = {15–22},
numpages = {8},
keywords = {attack graph, measurement, defense in depth, security},
location = {Los Angeles, CA, USA},
series = {ICSS '16}
}

@article{10.1145/3428151,
author = {Bibi, Iram and Akhunzada, Adnan and Malik, Jahanzaib and Khan, Muhammad Khurram and Dawood, Muhammad},
title = {Secure Distributed Mobile Volunteer Computing with Android},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3428151},
doi = {10.1145/3428151},
abstract = {Volunteer Computing provision of seamless connectivity that enables convenient and rapid deployment of greener and cheaper computing infrastructure is extremely promising to complement next-generation distributed computing systems. Undoubtedly, without tactile Internet and secure VC ecosystems, harnessing its full potentials and making it an alternative viable and reliable computing infrastructure is next to impossible. Android-enabled smart devices, applications, and services are inevitable for Volunteer computing. Contrarily, the progressive developments of sophisticated Android malware may reduce its exponential growth. Besides, Android malwares are considered the most potential and persistent cyber threat to mobile VC systems. To secure Android-based mobile volunteer computing, the authors proposed MulDroid, an efficient and self-learning autonomous hybrid (Long-Short-Term Memory, Convolutional Neural Network, Deep Neural Network) multi-vector Android malware threat detection framework. The proposed mechanism is highly scalable with well-coordinated infrastructure and self-optimizing capabilities to proficiently tackle fast-growing dynamic variants of sophisticated malware threats and attacks with 99.01\% detection accuracy. For a comprehensive evaluation, the authors employed current state-of-the-art malware datasets (Android Malware Dataset, Androzoo) with standard performance evaluation metrics. Moreover, MulDroid is compared with our constructed contemporary hybrid DL-driven architectures and benchmark algorithms. Our proposed mechanism outperforms in terms of detection accuracy with a trivial tradeoff speed efficiency. Additionally, a 10-fold cross-validation is performed to explicitly show unbiased results.},
journal = {ACM Trans. Internet Technol.},
month = {sep},
articleno = {2},
numpages = {21},
keywords = {android malware, Volunteer computing (VC), tactile internet, deep learning (DL)}
}

@inproceedings{10.1145/2815675.2815677,
author = {Gracia-Tinedo, Ra\'{u}l and Tian, Yongchao and Samp\'{e}, Josep and Harkous, Hamza and Lenton, John and Garc\'{\i}a-L\'{o}pez, Pedro and S\'{a}nchez-Artigas, Marc and Vukolic, Marko},
title = {Dissecting UbuntuOne: Autopsy of a Global-Scale Personal Cloud Back-End},
year = {2015},
isbn = {9781450338486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815675.2815677},
doi = {10.1145/2815675.2815677},
abstract = {Personal Cloud services, such as Dropbox or Box, have been widely adopted by users. Unfortunately, very little is known about the internal operation and general characteristics of Personal Clouds since they are proprietary services.In this paper, we focus on understanding the nature of Personal Clouds by presenting the internal structure and a measurement study of UbuntuOne (U1). We first detail the U$1$ architecture, core components involved in the U1 metadata service hosted in the datacenter of Canonical, as well as the interactions of U$1$ with Amazon S3 to outsource data storage. To our knowledge, this is the first research work to describe the internals of a large-scale Personal Cloud.Second, by means of tracing the U$1$ servers, we provide an extensive analysis of its back-end activity for one month. Our analysis includes the study of the storage workload, the user behavior and the performance of the U1 metadata store. Moreover, based on our analysis, we suggest improvements to U1 that can also benefit similar Personal Cloud systems.Finally, we contribute our dataset to the community, which is the first to contain the back-end activity of a large-scale Personal Cloud. We believe that our dataset provides unique opportunities for extending research in the field.},
booktitle = {Proceedings of the 2015 Internet Measurement Conference},
pages = {155–168},
numpages = {14},
keywords = {performance analysis, measurement, personal cloud},
location = {Tokyo, Japan},
series = {IMC '15}
}

@inproceedings{10.1145/3372224.3419195,
author = {Zhang, Chaoyun and Fiore, Marco and Ziemlicki, Cezary and Patras, Paul},
title = {Microscope: Mobile Service Traffic Decomposition for Network Slicing as a Service},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3419195},
doi = {10.1145/3372224.3419195},
abstract = {The growing diversification of mobile services imposes requirements on network performance that are ever more stringent and heterogeneous. Network slicing aligns mobile network operation to this context, by enabling operators to isolate and customize network resources on a per-service basis. A key input for provisioning resources to slices is real-time information about the traffic demands generated by individual services. Acquiring such knowledge is however challenging, as legacy approaches based on in-depth inspection of traffic streams have high computational costs, which inflate with the widening adoption of encryption over data and control traffic. In this paper, we present a new approach to service-level demand estimation for slicing, which hinges on decomposition, i.e., the inference of per-service demands from traffic aggregates. By operating on total traffic volumes only, our approach overcomes the complexity and limitations of legacy traffic classification techniques, and provides a suitable input to recent 'Network Slice as a Service' (NSaaS) models. We implement decomposition through Microscope, a novel framework that uses deep learning to infer individual service demands from complex spatiotemporal features hidden in traffic aggregates. Microscope (i) transforms traffic data collected in irregular radio access deployments in a format suitable for convolutional learning, and (ii) can accommodate a variety of neural network architectures, including original 3D Deformable Convolutional Neural Networks (3D-DefCNNs) that we explicitly design for decomposition. Experiments with measurement data collected in an operational network demonstrate that Microscope accurately estimates per-service traffic demands with relative errors below 1.2\%. Further, tests in practical NSaaS management use cases show that resource allocations informed by decomposition yield affordable costs for the mobile network operator.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {38},
numpages = {14},
keywords = {traffic decomposition, service demand estimation, network slicing, neural networks, mobile network data traffic, deep learning},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@inproceedings{10.1145/3329391,
author = {Esposito, Christian and Pop, Florin and Choi, Chang},
title = {Session Details: Theme: Information Systems: SFECS - Sustainability of Fog/Edge Computing Systems Track},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329391},
doi = {10.1145/3329391},
abstract = {Fog/Edge Computing paradigms are widely used in enterprises to address the emerging challenges of big data analysis, because of their underlying scalable, flexible and distributed data management schemes. The data centers in the Clouds are facing great challenges on the burden of the consequent increasing the amount of data to be man- aged and the additional requirements of location awareness and low latency at the edge of network necessary by smart cites and factories. These are the reasons why a centralized model cannot be an efficient solution for generated or required data by the IoT devices in those applications and there is the progressive shift towards fog nodes and smarted edge nodes mediating between the cloud and the IoT devices. The Fog/Edge computing paradigm is a decentralized model that transfers a part of low computing data analysis from the cloud to the intermediate (fog) nodes or the edges, performing only high computing tasks in the cloud. This new approach tries to minimize the three factors that negatively compromise the effective and efficient application of the Cloud computing to smart cities and factories, or similar application domains: the network bandwidth usage, decentralization of the data processing tasks and reduced response latency for clients (IoT devices). Fog/Edge computing is a hierarchical approach where the overall infrastructure is structured in multiple layers, each responsible of offering a good coordination and data management to the nodes at the lower layer. The lowest layer is usually composed of sensors and/or actuators that measure and/or control the environment or a given business process, implemented as mobile devices that are running a sensing/controlling application. In this case, combining Sustainable computing with Fog and Edge computing represents a new approach for increasing quality-of- service and efficiency of the system, creating the capability to present temporal and geo-coded information, and increasing innovation, and co-designing sustainable future large scale distributed systems. This new paradigm appears to offer a good approach in handling the scale factor of the data size, reducing the network bandwidth usage and the response latency of the system. In order to support specifically the Fog/Edge architectures, there is a need, for instance, of location-awareness and computation placement, replication and recovery. In many cases Edge resources would be required for both computation and data storage to address the time and locality constraints. There are multiple kinds of orchestration management solutions for virtualization in this type of architecture with different characteristics and drawbacks. This results in different restrictions for application definition, scalability, availability, load balancing and so on. Also, virtualization may be needed at multiple levels in a Fog/Edge architecture as it consists of the following levels of abstraction: at the sensing level we have the IoT devices/smart things, at the Edge level there are the gateways to a first collection and the data from the IoT devices and their preliminary processing, at the Fog level we have an additional data management layer, and at the Cloud level there is the compute/storage infrastructure with applications on top. Last, but not least, the energy efficiency is particularly important at the IoT and edge level since the devices may be equipped with a limited battery, possible difficult or impossible to be charged. So, optimizing the energy consumption is a must. To address several open research is- sues regarding sustainability of future Fog/Edge systems, this track aims at solicit contributions highlighting challenges, state-of-the-art, and solutions to a set of currently unresolved key questions including - but not limited to - performance, modeling, optimization, energy-efficiency, reliability, security, privacy and techno-economic aspects of Fog/Edge systems. Through addressing these concerns while understanding their impacts and limitations, technological advancements will be channeled toward more sustainable/efficient platforms for tomorrow's ever-connected systems.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3412821.3412823,
author = {Cerina, L. and Santambrogio, M. D.},
title = {SAGe: A Configurable Code Generator for Efficient Symbolic Analysis of Time-Series},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
url = {https://doi.org/10.1145/3412821.3412823},
doi = {10.1145/3412821.3412823},
abstract = {Some of the most recent applications and services revolve around the analysis of time-series, which generally exhibits chaotic characteristics. This behavior brought back the necessity to simplify their representation to discover meaningful patterns and extract information efficiently. Furthermore, recent trends show how computation is moving back from the Cloud to the Edge of network, meaning that algorithms should be compatible with low-power embedded devices. A family of methods called Symbolic Analysis (SA) tries to solve this issue, reducing the dimensionality of the original data in a set of symbolic words and providing distance metrics for the obtained symbols. However, SA is usually implemented using application-specific tools, which are not easily adaptable, or mathematical environments (e.g. R, Julia) that do not ensure portability, or that require additional work to maximize computing performance. We propose here SAGe: a code generation tool that helps the user to prototype efficient and portable code, starting from a high-level representation of SA requirements. Other than exploiting similarities between SA pipelines, SAGe employs general code templates to build and deploy the code on different architectures, such as embedded devices, microcontrollers, and FPGAs. Preliminary results show a speedup up to 223x against Python implementations running on an x86 desktop machine and a notable increase in computational efficiency on a reconfigurable device.},
journal = {SIGBED Rev.},
month = {jul},
pages = {12–17},
numpages = {6}
}

@inproceedings{10.1145/3459104.3459136,
author = {A. Panayiotou, Nikolaos and P. Stavrou, Vasileios and E. Stergiou, Konstantinos},
title = {Applying the Industry 4.0 in a Smart Gas Grid: The Greek Gas Distribution Network Case},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459136},
doi = {10.1145/3459104.3459136},
abstract = {The aim of this paper is to design and implement a series of actions regarding the operation of DEDA S.A. (Natural Gas Distribution Networks), based on principles of Industry 4.0. The Natural Gas Distribution sector is one of the most critical and innovative areas where Industry 4.0 can be applied, being part of critical infrastructure management. At first, company's business process architecture was developed, with the aim to export DEDA's business process and functional specifications related to the required information systems. Subsequently, company's communication network is implemented alongside the company's gas network, in coordination with the company's control room.In addition, modernization of metering system is taking place in order to exchange information between smart meters and the control room. A number of Information Systems, such as the pipeline surveillance system and the Business Intelligence system will also be installed in order to ensure communication at different levels using Cloud technologies. The implementation is expected to improve DEDA's organization, increasing customers' service level. As a result, there will be an expected increase in the operational efficiency of DEDA's network through the use of advanced technologies, in cooperation with business process modelling techniques. The effort should be continued in this direction in order to achieve even greater improvement in business processes, information systems and pipeline automation.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {180–184},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3449301.3449322,
author = {Wen, Yana and Wei, Tingyue and Cui, Kewei and Ling, Bai and Zhang, Yahao and Huang, Meng},
title = {Research on Belt and Road Big Data Visualization Based on Text Clustering Algorithm},
year = {2021},
isbn = {9781450388597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449301.3449322},
doi = {10.1145/3449301.3449322},
abstract = {In the era of big data, people's visual needs for data expression are increasing. In order to achieve better big data display effects, this article introduced the use of text clustering algorithms to achieve data crawling and Echarts technology to realize big data visualization. This system used mvvm's architecture and vue framework development platform, ThinkPHP was used as the background framework, and ES6 related technologies and specifications were used for application development. This system used Echarts, IView, GIS technology and JavaScript development methods to demonstrate economic big data module functions on the web side; Applied CSS3, HTML5, GIS technology to implement project achievement module and university alliance module; Applied Echarts, HTML5, JS function library technology to achieve national information module. This system used stored procedure, database index optimization technology to achieve rapid screening of massive data, and dynamically update and displayed related data through two-way data binding. This system combined real-time location technology with GIS technology to measure the distance between the user and the destination, and automatically plan the tour route to provide related services. This system can provide feasibility suggestions for strategic researchers or experts in related areas of the “Belt and Road”, and provide theoretical basis and technical support.},
booktitle = {Proceedings of the 6th International Conference on Robotics and Artificial Intelligence},
pages = {121–125},
numpages = {5},
keywords = {One Belt One Road, big data visualization, Keywords-component, Text clustering algorithm},
location = {Singapore, Singapore},
series = {ICRAI '20}
}

@inproceedings{10.1145/2809730.2809749,
author = {Walter, Nadine and Altm\"{u}ller, Tobias and Bengler, Klaus},
title = {Concept of a Reference Architecture for an Extendable In-Vehicle Adaptive Recommendation Service},
year = {2015},
isbn = {9781450338585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809730.2809749},
doi = {10.1145/2809730.2809749},
abstract = {An adaptive recommendation service can reduce driver distraction through reducing the amount of operation steps needed to call a function. It learns the routine user behavior of the driver related to a situation and supports the driver with this knowledge by giving proactive recommendations for preconfigured functions. An adaptive recommendation service is a complex system and the development includes several challenges. One is the development of an architecture which needs to be modular, extendible in regard to the support of different functions and integrated in an overall in-vehicle HMI architecture. This architecture describes the components and interfaces of an adaptive recommendation service which need to be researched and developed. It is a starting point for the implementation of realistic prototypes in a real vehicle or driving simulator which enables an extensive evaluation of the whole adaptive recommendation service.},
booktitle = {Adjunct Proceedings of the 7th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {88–93},
numpages = {6},
keywords = {software architecture, prototyping, driver distraction, recommendation system, adaptive system},
location = {Nottingham, United Kingdom},
series = {AutomotiveUI '15}
}

@inproceedings{10.1145/2883851.2883876,
author = {Renz, Jan and Hoffmann, Daniel and Staubitz, Thomas and Meinel, Christoph},
title = {Using A/B Testing in MOOC Environments},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883876},
doi = {10.1145/2883851.2883876},
abstract = {In recent years, Massive Open Online Courses (MOOCs) have become a phenomenon offering the possibility to teach thousands of participants simultaneously. In the same time the platforms used to deliver these courses are still in their fledgling stages. While course content and didactics of those massive courses are the primary key factors for the success of courses, still a smart platform may increase or decrease the learners experience and his learning outcome. The paper at hand proposes the usage of an A/B testing framework that is able to be used within an micro-service architecture to validate hypotheses about how learners use the platform and to enable data-driven decisions about new features and settings. To evaluate this framework three new features (Onboarding Tour, Reminder Mails and a Pinboard Digest) have been identified based on a user survey. They have been implemented and introduced on two large MOOC platforms and their influence on the learners behavior have been measured. Finally this paper proposes a data driven decision workflow for the introduction of new features and settings on e-learning platforms.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics \&amp; Knowledge},
pages = {304–313},
numpages = {10},
keywords = {A/B testing, MOOC, controlled online tests, e-learning, microservice},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2642687.2642690,
author = {Migault, Daniel and Palomares, Daniel and Hendrik, Hendrik and Laurent, Maryline},
title = {Secure IPsec Based Offload Architectures for Mobile Data},
year = {2014},
isbn = {9781450330275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642687.2642690},
doi = {10.1145/2642687.2642690},
abstract = {Radio Access Network (RAN) are likely to be overloaded, and some places will not be able to provide the necessary requested bandwidth. In order to respond to the demand of bandwidth, overloaded RAN are currently offloading their traffic on WLAN. WLAN Access Points like (ISP provided xDSL boxes) are untrusted, unreliable and do not handle mobility. As a result, mobility, multihoming, and security cannot be handled by the network anymore, and must be handled by the terminal. This paper positions offload architectures based on IPsec and shows that IPsec can provide end-to-end security, as well as seamless connectivity across IP networks. Then, the remaining of the paper evaluates how mobility on these IPsec based architectures impacts the Quality of Service (QoS) for real time applications such as an audio streaming service. QoS is measured using network interruption time and POLQA. Measurements compare TCP/HLS and UDP/RTSP over various IPsec configurations.},
booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {95–104},
numpages = {10},
keywords = {IPsec multiple interfaces, IPsec mobility, wlan offload architecture, terminal mobility, quality of service},
location = {Montreal, QC, Canada},
series = {Q2SWinet '14}
}

@inproceedings{10.1145/2568088.2576760,
author = {Ghaith, Shadi and Wang, Miao and Perry, Philip and Murphy, Liam},
title = {Software Contention Aware Queueing Network Model of Three-Tier Web Systems},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2576760},
doi = {10.1145/2568088.2576760},
abstract = {Using modelling to predict the performance characteristics of software applications typically uses Queueing Network Models representing the various system hardware resources. Leaving out the software resources, such as the limited number of threads, in such models leads to a reduced prediction accuracy. Accounting for Software Contention is a challenging task as existing techniques to model software components are complex and require deep knowledge of the software architecture. Furthermore, they also require complex measurement processes to obtain the model's service demands. In addition, solving the resultant model usually require simulation solvers which are often time consuming.In this work, we aim to provide a simpler model for three-tier web software systems which accounts for Software Contention that can be solved by time efficient analytical solvers. We achieve this by expanding the existing "Two-Level Iterative Queuing Modelling of Software Contention" method to handle the number of threads at the Application Server tier and the number of Data Sources at the Database Server tier. This is done in a generic manner to allow for extending the solution to other software components like memory and critical sections. Initial results show that our technique clearly outperforms existing techniques.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {273–276},
numpages = {4},
keywords = {performance prediction, software contention, performance models, web applications},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1145/3152434.3152450,
author = {Singhvi, Arjun and Banerjee, Sujata and Harchol, Yotam and Akella, Aditya and Peek, Mark and Rydin, Pontus},
title = {Granular Computing and Network Intensive Applications: Friends or Foes?},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152450},
doi = {10.1145/3152434.3152450},
abstract = {Computing/infrastructure as a service continues to evolve with bare metal, virtual machines, containers and now serverless granular computing service offerings. Granular computing enables developers to decompose their applications into smaller logical units or functions, and run them on small, low cost and short lived computation containers without having to worry about setting up servers - hence the term serverless computing. While serverless environments can be used very cost effectively for large scale parallel processing data analytics applications, it is less clear if network intensive packet processing applications can also benefit from these new computing services as they do not share the same characteristics. This paper examines the architectural constraints as well as current serverless implementations to develop a position on this topic and influence the next generation of computing services. We support our position through measurement and experimentation on Amazon's AWS Lambda service with a few popular network functions.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {157–163},
numpages = {7},
location = {<conf-loc>, <city>Palo Alto</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {HotNets '17}
}

@inproceedings{10.1145/2976749.2978414,
author = {Jia, Yaoqi and Chua, Zheng Leong and Hu, Hong and Chen, Shuo and Saxena, Prateek and Liang, Zhenkai},
title = {"The Web/Local" Boundary Is Fuzzy: A Security Study of Chrome's Process-Based Sandboxing},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978414},
doi = {10.1145/2976749.2978414},
abstract = {Process-based isolation, suggested by several research prototypes, is a cornerstone of modern browser security architectures. Google Chrome is the first commercial browser that adopts this architecture. Unlike several research prototypes, Chrome's process-based design does not isolate different web origins, but primarily promises to protect "the local system" from "the web". However, as billions of users now use web-based cloud services (e.g., Dropbox and Google Drive), which are integrated into the local system, the premise that browsers can effectively isolate the web from the local system has become questionable. In this paper, we argue that, if the process-based isolation disregards the same-origin policy as one of its goals, then its promise of maintaining the "web/local system (local)" separation is doubtful. Specifically, we show that existing memory vulnerabilities in Chrome's renderer can be used as a stepping-stone to drop executables/scripts in the local file system, install unwanted applications and misuse system sensors. These attacks are purely data-oriented and do not alter any control flow or import foreign code. Thus, such attacks bypass binary-level protection mechanisms, including ASLR and in-memory partitioning. Finally, we discuss various full defenses and present a possible way to mitigate the attacks presented.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {791–804},
numpages = {14},
keywords = {data-oriented attacks, browser design, browser security},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/2973750.2973766,
author = {Yu, Der-Yeuan and Ranganathan, Aanjhan and Masti, Ramya Jayaram and Soriente, Claudio and Capkun, Srdjan},
title = {SALVE: Server Authentication with Location Verification},
year = {2016},
isbn = {9781450342261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973750.2973766},
doi = {10.1145/2973750.2973766},
abstract = {The Location Service (LCS) proposed by the telecommunication industry is an architecture that allows the location of mobile devices to be accessed in various applications. We explore the use of LCS in location-enhanced server authentication, which traditionally relies on certificates. Given recent incidents involving certificate authorities, various techniques to strengthen server authentication were proposed. They focus on improving the certificate validation process, such as pinning, revocation, or multi-path probing. In this paper, we propose using the server's geographic location as a second factor of its authenticity. Our solution, SALVE, achieves location-based server authentication by using secure DNS resolution and by leveraging LCS for location measurements. We develop a TLS extension that enables the client to verify the server's location in addition to its certificate. Successful server authentication therefore requires a valid certificate and the server's presence at a legitimate geographic location, e.g., on the premises of a data center. SALVE prevents server impersonation by remote adversaries with mis-issued certificates or stolen private keys of the legitimate server. We develop a prototype implementation and our evaluation in real-world settings shows that it incurs minimal impact to the average server throughput. Our solution is backward compatible and can be integrated with existing approaches for improving server authentication in TLS.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking},
pages = {401–414},
numpages = {14},
keywords = {location service, TLS, server authentication, location-based authentication},
location = {New York City, New York},
series = {MobiCom '16}
}

@inproceedings{10.1145/3131365.3131373,
author = {Chung, Taejoong and van Rijswijk-Deij, Roland and Choffnes, David and Levin, Dave and Maggs, Bruce M. and Mislove, Alan and Wilson, Christo},
title = {Understanding the Role of Registrars in DNSSEC Deployment},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131373},
doi = {10.1145/3131365.3131373},
abstract = {The Domain Name System (DNS) provides a scalable, flexible name resolution service. Unfortunately, its unauthenticated architecture has become the basis for many security attacks. To address this, DNS Security Extensions (DNSSEC) were introduced in 1997. DNSSEC's deployment requires support from the top-level domain (TLD) registries and registrars, as well as participation by the organization that serves as the DNS operator. Unfortunately, DNSSEC has seen poor deployment thus far: despite being proposed nearly two decades ago, only 1\% of .com, .net, and .org domains are properly signed.In this paper, we investigate the underlying reasons why DNSSEC adoption has been remarkably slow. We focus on registrars, as most TLD registries already support DNSSEC and registrars often serve as DNS operators for their customers. Our study uses large-scale, longitudinal DNS measurements to study DNSSEC adoption, coupled with experiences collected by trying to deploy DNSSEC on domains we purchased from leading domain name registrars and resellers. Overall, we find that a select few registrars are responsible for the (small) DNSSEC deployment today, and that many leading registrars do not support DNSSEC at all, or require customers to take cumbersome steps to deploy DNSSEC. Further frustrating deployment, many of the mechanisms for conveying DNSSEC information to registrars are error-prone or present security vulnerabilities. Finally, we find that using DNSSEC with third-party DNS operators such as Cloudflare requires the domain owner to take a number of steps that 40\% of domain owners do not complete. Having identified several operational challenges for full DNSSEC deployment, we make recommendations to improve adoption.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {369–383},
numpages = {15},
keywords = {DNS, public key infrastructure, DNS security extension, PKI, DNS operator, registrar, DNSSEC},
location = {London, United Kingdom},
series = {IMC '17}
}

@inproceedings{10.1145/3446434.3446523,
author = {Daroshka, Vitali and Evgrafov, Arkady and Nikiforova, Jeanne and Chargazia, Grigory and Parshukov, Aleksey},
title = {The Specific of Business Reputation Value Measurement in Transformational Economy (on Example of Economy of the Republic of Belarus)},
year = {2021},
isbn = {9781450388900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446434.3446523},
doi = {10.1145/3446434.3446523},
abstract = {Globalization of economy and overcoming of the world economic crisis consequences increases the value of evidence-based suggestions to improve management in domestic industrial organizations. The increase of the competition from foreign business defines the need for justification of the adequate, complete concept of ensuring the stability of industrial enterprises' functioning that combines effective management of tangible and intangible assets. Looking at current trends of the world economic development, it's objectively seen that intangible competitive production components play an increasingly important role to preserve market leadership both for one organization and the entire national economy.Global trends in globalization dictate fundamentally new rules for the formation of the architecture of the business model of an industrial organization, which determine the transition from the production of products and services for the impersonal mass of consumers to the global scale of personalized service for each client. In other words, the current business environment requires the simultaneous realization of two trends that are opposite in nature: the global scale of activity (globalization) and the personal approach to each consumer (customization and personalization of products and services), which makes it necessary to create a system of criteria for assessing the technological transition.The main attention is given to an element of intangible assets that is one of the most difficult for value measurement and management, the business reputation of an organization. According to literature, there is no common opinion among the scientists as to what business reputation means and how it is linked to another subjective intangible asset, goodwill.},
booktitle = {Proceedings of the International Scientific Conference - Digital Transformation on Manufacturing, Infrastructure and Service},
articleno = {89},
numpages = {8},
keywords = {transformational economy, risks, Business reputation, value-based management},
location = {Saint Petersburg, Russian Federation},
series = {DTMIS '20}
}

@inproceedings{10.1145/2775292.2775312,
author = {Scully, Timothy and Dobo\v{s}, Jozef and Sturm, Timo and Jung, Yvonne},
title = {3drepo.Io: Building the next Generation Web3D Repository with AngularJS and X3DOM},
year = {2015},
isbn = {9781450336475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2775292.2775312},
doi = {10.1145/2775292.2775312},
abstract = {This paper presents a novel open source web-based 3D version control system positioned directly within the context of the recent strategic plan for digitising the construction sector in the United Kingdom. The aim is to achieve reduction of cost and carbon emissions in the built environment by up to 20\% simply by properly managing digital information and 3D models. Even though previous works in the field concentrated mainly on defining novel WebGL frameworks and later on the efficiency of 3D data delivery over the Internet, there is still the emerging need for a practical solution that would provide ubiquitous access to 3D assets, whether it is for large international enterprises or individual members of the general public. We have, therefore, developed a novel platform leveraging the latest open web-based technologies such as AngularJS and X3DOM in order to define an industrial-strength collaborative cloud hosting service 3drepo.io. Firstly, we introduce the work and outline the high-level system architecture as well as improvements in relation to previous work. Next, we describe database and front-end considerations with emphasis on scalability and enhanced security. Finally, we present several performance measurement experiments and a selection of real-life industrial use cases. We conclude that jQuery provides performance benefits over AngularJS when manipulating large scene graphs in web browsers.},
booktitle = {Proceedings of the 20th International Conference on 3D Web Technology},
pages = {235–243},
numpages = {9},
keywords = {BIM, 3D repo, version control, X3DOM, AngularJS},
location = {Heraklion, Crete, Greece},
series = {Web3D '15}
}

@inproceedings{10.1145/2736084.2736091,
author = {Zhang, Cong and Liu, Jiangchuan},
title = {On Crowdsourced Interactive Live Streaming: A Twitch.Tv-Based Measurement Study},
year = {2015},
isbn = {9781450333528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2736084.2736091},
doi = {10.1145/2736084.2736091},
abstract = {Empowered by today's rich tools for media generation and collaborative production, the multimedia service paradigm is shifting from the conventional single source, to multi-source, to many sources, and now toward crowdsource. Such crowdsourced live streaming platforms as Twitch.tv allow general users to broadcast their content to massive viewers, thereby greatly expanding the content and user bases. The resources available for these non-professional broadcasters however are limited and unstable, which potentially impair the streaming quality and viewers' experience. The diverse live interactions among the broadcasters and viewers can further aggravate the problem.In this paper, we present an initial investigation on the modern crowdsourced live streaming systems. Taking Twitch as a representative, we outline their inside architecture using both crawled data and captured traffic of local broadcasters/viewers. Closely examining the access data collected in a two-month period, we reveal that the view patterns are determined by both events and broadcasters' sources. Our measurements explore the unique source- and event-driven views, showing that the current delay strategy on the viewer's side substantially impacts the viewers' interactive experience, and there is significant disparity between the long broadcast latency and the short live messaging latency. On the broadcaster's side, the dynamic uploading capacity is a critical challenge, which noticeably affects the smoothness of live streaming for viewers.},
booktitle = {Proceedings of the 25th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {55–60},
numpages = {6},
keywords = {view statistics, crowdsourced live streaming, Twitch.tv, interactive latency},
location = {Portland, Oregon},
series = {NOSSDAV '15}
}

@inproceedings{10.1145/2602458.2602476,
author = {Spacek, Petr and Dony, Christophe and Tibermacine, Chouki},
title = {A Component-Based Meta-Level Architecture and Prototypical Implementation of a Reflective Component-Based Programming and Modeling Language},
year = {2014},
isbn = {9781450325776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602458.2602476},
doi = {10.1145/2602458.2602476},
abstract = {Component-based Software Engineering studies the design, development and maintenance of software constructed upon sets of connected components. Using existing standard solutions, component-based models are frequently transformed into non-component-based programs, most of the time object-oriented, for run-time execution. As a consequence many component-level descriptions (part of code), e.g. explicit architectures or ports declarations, vanish at the implementation stage, making debugging, transformations or reverse-engineering difficult. It has been shown that component-based programming languages contribute to bridge this gap between design and implementation and to provide a conceptual and practical continuum to fully develop applications with components. In this paper we go one step further in this direction by making a component-oriented programming and modeling language truly reflective, thus making verification, evolution or transformation stages of software development part of this new continuum. The gained reflection capabilities indeed make it possible to perform architecture checking, code refactoring, model transformations or even to implement new languages constructs with and for components. The paper presents an original executable meta-level architecture achieving the vision that "everything is a component}' and an operational implementation demonstrating its feasibility and effectiveness. Our system revisits some standard solutions for reification in the component's context and also handles new cases, such as ports reification, to allow for runtime introspection and intercession on components and on their descriptors. We validate these ideas in the context of an executable prototypical and minimal component-based language, named Compo, whose first goal is to help imagining the future.},
booktitle = {Proceedings of the 17th International ACM Sigsoft Symposium on Component-Based Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {component, reflection, architecture, programming, transformations, reflexive meta-model, modeling, constraints},
location = {Marcq-en-Bareul, France},
series = {CBSE '14}
}

@inproceedings{10.1145/2666620.2666630,
author = {Vidas, Timothy and Tan, Jiaqi and Nahata, Jay and Tan, Chaur Lih and Christin, Nicolas and Tague, Patrick},
title = {A5: Automated Analysis of Adversarial Android Applications},
year = {2014},
isbn = {9781450331555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666620.2666630},
doi = {10.1145/2666620.2666630},
abstract = {Mobile malware is growing - both in overall volume and in number of existing variants - at a pace rapid enough that systematic manual, human analysis is becoming increasingly difficult. As a result, there is a pressing need for techniques and tools that provide automated analysis of mobile malware samples. We present A5, an open source automated system to process Android malware. A5 is a hybrid system combining static and dynamic malware analysis techniques. Android's architecture permits many different paths for malware to react to system events, any of which may result in malicious behavior. Key innovations in A5 consist of novel methods of interacting with mobile malware to better coerce malicious behavior, and in combining both virtual and physical pools of Android platforms to capture behavior that could otherwise be missed. The primary output of A5 is a set of network threat indicators and intrusion detection system signatures that can be used to detect and prevent malicious network activity. We detail A5's distributed design and demonstrate applicability of our interaction techniques using examples from real malware. Additionally, we compare A5 with other automated systems and provide performance measurements of an implementation, using a published dataset of 1,260 unique malware samples, showing that A5 can quickly process large amounts of malware. We provide a public web interface to our implementation of A5 that allows third parties to use A5 as a web service.},
booktitle = {Proceedings of the 4th ACM Workshop on Security and Privacy in Smartphones \&amp; Mobile Devices},
pages = {39–50},
numpages = {12},
keywords = {virtualization, static analysis, sandbox, dynamic analysis, mobile malware, malicious behavior},
location = {Scottsdale, Arizona, USA},
series = {SPSM '14}
}

@inproceedings{10.1145/3030207.3044531,
author = {Jun, Tae Joon and Yoo, Myong Hwan and Kim, Daeyoung and Cho, Kyu Tae and Lee, Seung Young and Yeun, Kyuoke},
title = {HPC Supported Mission-Critical Cloud Architecture},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3044531},
doi = {10.1145/3030207.3044531},
abstract = {Tactical Operations Center (TOC) system in military field is an advanced computer system composed of multiple servers and desktops to interlock internal/external weapon systems processing mission-critical applications in combat situation. However, the current TOC system has several limitations such as difficulty of integrating tactical weapon systems including missile launch system and radar system into the single TOC system due to the heterogeneity of HW and SW between systems, and an inefficient computing resource management for the weapon systems.In this paper, we proposed a novel HPC supported mission-critical Cloud architecture as TOC for Surface-to-Air-Missile (SAM) system with OpenStack Cloud OS, Data Distribution Service (DDS), and GPU virtualization techniques. With this approach, our system provides elastic resource management over the weapon systems with virtual machines, integration of heterogeneous systems with different kinds of guest OS, real-time, reliable, and high-speed communication between the virtual machines and virtualized GPU resource over the virtual machines. Evaluation of our TOC system includes DDS performance measurement over 10Gbps Ethernet and QDR InfiniBand networks on the virtualized environment with OpenStack Cloud OS, and GPU virtualization performance evaluation with two different methods, PCI pass-through and remote-API. With the evaluation results, we conclude that our system provides reasonable performance in the combat situation compared to the previous TOC system while additionally supports scalable and elastic use of computing resource through the virtual machines.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {223–232},
numpages = {10},
keywords = {tactical operations center, cloud computing, data distribution service, gpgpu},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/3375555.3384938,
author = {Zibitsker, Boris and Lupersolsky, Alex},
title = {How to Apply Modeling to Compare Options and Select the Appropriate Cloud Platform},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3384938},
doi = {10.1145/3375555.3384938},
abstract = {Organizations want to take advantage of the flexibility and scalability of Cloud platforms. By migrating to the Cloud, they hope to develop and implement new applications faster with lower cost. Amazon AWS, Microsoft Azure, Google, IBM, Oracle and others Cloud providers support different DBMS like Snowflake, Redshift, Teradata Vantage, and others. These platforms have different architectures, mechanisms of allocation and management of resources, and levels of sophistication of DBMS optimizers which affect performance, scalability and cost. As a result, the response time, CPU Service Time and the number of I/Os for the same query, accessing the similar table in the Cloud could be significantly different than On Prem. In order to select the appropriate Cloud platform as a first step we perform a Workload Characterization for On Prem Data Warehouse. Each Data Warehouse workload represents a specific line of business and includes activity of many users generating concurrently simple and complex queries accessing data from different tables. Each workload has different demands for resources and different Response Time and Throughput Service Level Goals. In this presentation we will review results of the workload characterization for an On Prem Data Warehouse environment. During the second step we collected measurement data for standard TPC-DS benchmark tests performed in AWS Vantage, Redshift and Snowflake Cloud platform for different sizes of the data sets and different number of concurrent users. During the third step we used the results of the workload characterization and measurement data collected during the benchmark to modify BEZNext On Prem Closed Queueing model to model individual Clouds. And finally, during the fourth step we used our Model to take into consideration differences in concurrency, priorities and resource allocation to different workloads. BEZNext optimization algorithms incorporating Graduate search mechanism are used to find the AWS instance type and minimum number of instances which will be required to meet SLGs for each of the workloads. Publicly available information about the cost of the different AWS instances is used to predict the cost of supporting workloads in the Cloud month by month during next 12 months.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {16},
numpages = {1},
keywords = {optimization., service level goals, workload characterization, benchmarking, workload forecasting, cloud platform, modeling, seasonality determination},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1109/UCC.2014.40,
author = {Chauvel, Franck and Song, Hui and Ferry, Nicolas and Fleurey, Franck},
title = {Robustness Indicators for Cloud-Based Systems Topologies},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.40},
doi = {10.1109/UCC.2014.40},
abstract = {Various services are now available in the Cloud, ranging from turnkey databases and application servers to high-level services such as continuous integration or source version control. To stand out of this diversity, robustness of service compositions is an important selling argument, but which remains difficult to understand and estimate as it does not only depend on services but also on the underlying platform and infrastructure. Yet, choosing a specific service composition may fail to deliver the expected robustness, but reverting early choices may jeopardise the success of any Cloud project. Inspired by existing models used in Biology to quantify the robustness of ecosystems, we show how to tailor them to obtain early indicators of robustness for cloud-based deployments. This technique helps identify weakest services in the overall architecture and in turn mitigates the risk of having to revert key architectural choices. We illustrate our approach by comparing the robustness of four alternative deployments of the Sens App application, which includes a Mongo DB database, four REST services and a graphical web-front end.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {307–316},
numpages = {10},
keywords = {robustness indicators, failures sequences, extinction sequences, cloud topologies, deployment, bio-inspired},
series = {UCC '14}
}

@inproceedings{10.1145/3120895.3120916,
author = {Arndt, Oliver Jakob and Spindeldreier, Christian and Wohnrade, Kevin and Pfefferkorn, Daniel and Neuenhahn, Martin and Blume, Holger},
title = {FPGA Accelerated NoC-Simulation: A Case Study on the Intel Xeon Phi Ringbus Topology},
year = {2017},
isbn = {9781450353168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3120895.3120916},
doi = {10.1145/3120895.3120916},
abstract = {Complex signal processing algorithms targeted on architectures with increasingly high numbers of parallel processing units require high performance core-interconnections (i.e., low latencies, high throughput, no pinch-offs or bottlenecks). Therefore, assisting techniques, exploring characteristics of diverse topologies of common as well as innovative Network-on-Chips (NoCs), are necessary for the development of chips with massive parallel processing cores. In contrast to analytic NoC models, event driven NoC simulations can handle even complex task graphs, but however feature long simulation times. Enabling the simulation of even complex task graphs, in this work, we propose to use FPGA accelerated simulation. While we extend such a simulator in order to imitate cache coherence communication-behavior, we also present a translation of real measured profiles to task graphs for in-depth simulation of the communication behavior of an existing NoC-based manycore. Therefore, this approach is able to not only deal with synthetic scenarios, but analyse the communication behavior of real world applications. Additionally, a simulation of the Histograms of Oriented Gradients algorithm, running on the Intel Xeon Phi manycore, exhibiting a 70-stop ring-bus, exemplifies this approach.},
booktitle = {Proceedings of the 8th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
articleno = {21},
numpages = {6},
location = {Bochum, Germany},
series = {HEART '17}
}

@inproceedings{10.1145/2684746.2689095,
author = {Ben Fakih, Hichem and Elhossini, Ahmed and Juurlink, Ben},
title = {An Efficient and Flexible FPGA Implementation of a Face Detection System (Abstract Only)},
year = {2015},
isbn = {9781450333153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684746.2689095},
doi = {10.1145/2684746.2689095},
abstract = {Robust and rapid face detection systems are constantly gaining more interest, since they represent the first stone for many challenging tasks in the field of computer vision. In this paper a software-hardware co-design approach is presented, that enables the detection of frontal faces in real time. A complete hardware implementation of all components taking part of the face detection is introduced. This work is based on the object detection framework of Viola and Jones, which makes use of a cascade of classifiers to reduce the computation time. The proposed architecture is flexible, as it allows the use of multiple instances of the face detector. This makes developers free to choose the speed range and reserved resources for this task. The current implementation runs on the Zynq SoC and receives images over IP network, which allows exposing the face detection task as a remote service that can be consumed from any device connected to the network. We performed several measurements for the final detector and the software equivalent. Using three Evaluator cores, the ZedBoard system achieves a maximal average frame rate of 13.4 FPS when analysing an image containing 640x480 pixels. This stands for an improvement of 5.25 times compared to the software solution and represents acceptable results for most real-time systems. On the ZC706 system, a higher frame rate of 16.58 FPS is achieved. The proposed hardware solution achieved 92\% accuracy, which is low compared to the software solution (97\%) due to different scaling algorithm. The proposed solution achieved higher frame rate compared to other solutions found in the literature.},
booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {261},
numpages = {1},
keywords = {copmuter visioin, fpga, viola and jones, zynq, face detection},
location = {Monterey, California, USA},
series = {FPGA '15}
}

@inproceedings{10.1145/3437120.3437284,
author = {Psilias, Dimitrios and Milidonis, Athanasios and Voyiatzis, Ioannis},
title = {Architecture for Secure UAV Systems},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437284},
doi = {10.1145/3437120.3437284},
abstract = {UAV applications are providing an extended range of services in society's needs. These applications require high execution speed and security to all transmitted data. In this paper an architecture is proposed for secure UAV applications. The architecture consists of a microcontroller to execute the flight controller tasks and a FPGA for implementing the security related tasks. The microcontroller is an Arduino which is widely used in UAVs. Arduino communicates with all sensors and generates outputs needed for controlling the UAV's motors. The circuit inside the FPGA encrypts/decrypts data related to transmission. Measurements taken concerning the execution time and power consumption, reveal the benefits of the extra hardware added for encryption/decryption in comparison with those of a single microcontroller.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {99–102},
numpages = {4},
location = {Athens, Greece},
series = {PCI '20}
}

@proceedings{10.1145/2656434,
title = {RIIT '14: Proceedings of the 3rd Annual Conference on Research in Information Technology},
year = {2014},
isbn = {9781450327114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great pleasure that we welcome you to the 15th Annual Conference on Information Technology Education (SIGITE 2014) and the 3rd Annual Conference on Research in Information Technology (RIIT 2014). The theme this year is "Riding the Wave of Change in Information Technology" and the many quality submissions we received allowed us to assemble one of the strongest programs in the history of the conferences. As in past years, the synergies between research and education in information technology are prevalent, and several themes emerged from the accepted submissions. Networking, security, and development remain popular with researchers, and interest in mobile computing, resource measurement and management, capstone courses, and personalization has grown.The call for participation attracted 111 submissions, 72 of which were submitted to SIGITE and 39 to RIIT. Both numbers represent a larger pool than in recent years, demonstrating that the conferences are of great interest in the community. Ninety-five of the submissions were papers, with 59 papers submitted to SIGITE and 36 papers submitted to RIIT. SIGITE has 27 papers in its program for an acceptance rate of 46\% and RIIT has 14 papers for an acceptance rate of 39\%. All of the authors presenting should be congratulated on their excellent work.A conference cannot happen without the help of its reviewers, and this year was no exception. Fiftyfive reviewers worked diligently to ensure that every paper had at least three independent reviews. It was a significant effort to produce the 317 reviews that ended up in the system, and we thank the reviewers from the bottom of our heart. New to the conferences this year was a meta review process, in which 13 diligent meta reviewers together examined all reviews for each submission and reconciled those reviews into a coherent message for each author. We hope the meta review process enabled authors to have more substantive feedback on their work, whether it appears in the final program or not.The conference runs from Thursday to Saturday and each day offers something of interest to attendees. On Thursday our keynote speaker is Dr. Flavio Villanustre, Vice-President of Technology Architecture \&amp; Product for LexisNexis and HPCC Systems. The day continues with a workshop on end-user development activities and paper sessions for both SIGITE and RIIT. Thursday concludes with a reception, which we know will be useful for networking with colleagues old and new. Friday introduces a new presentation format, lightning talks on research in progress, at the conferences. There are also paper sessions for SIGITE and RIIT, a poster session in the afternoon and, of course, more opportunities for networking during lunch and the breaks. Saturday offers a three-hour workshop on process-oriented guided inquiry learning (POGIL) as well as a panel on mobile computing courses and some excellent SIGITE papers. We also hope that you stay for the closing session where we will share our plans for SIGITE/RIIT 2015 in Chicago.We hope you find the conference presentations interesting and thought-provoking, you reconnect with colleagues you know, you find new collaborators, and you submit the work that results to SIGITE or RIIT next year. The excellence you see at SIGITE/RIIT 2014 depends on your energy and effort, and we thank you for letting us be a part of it.},
location = {Atlanta, Georgia, USA}
}

@proceedings{10.1145/2656450,
title = {SIGITE '14: Proceedings of the 15th Annual Conference on Information Technology Education},
year = {2014},
isbn = {9781450326865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great pleasure that we welcome you to the 15th Annual Conference on Information Technology Education (SIGITE 2014) and the 3rd Annual Conference on Research in Information Technology (RIIT 2014). The theme this year is "Riding the Wave of Change in Information Technology" and the many quality submissions we received allowed us to assemble one of the strongest programs in the history of the conferences. As in past years, the synergies between research and education in information technology are prevalent, and several themes emerged from the accepted submissions. Networking, security, and development remain popular with researchers, and interest in mobile computing, resource measurement and management, capstone courses, and personalization has grown.The call for participation attracted 111 submissions, 72 of which were submitted to SIGITE and 39 to RIIT. Both numbers represent a larger pool than in recent years, demonstrating that the conferences are of great interest in the community. Ninety-five of the submissions were papers, with 59 papers submitted to SIGITE and 36 papers submitted to RIIT. SIGITE has 27 papers in its program for an acceptance rate of 46\% and RIIT has 14 papers for an acceptance rate of 39\%. All of the authors presenting should be congratulated on their excellent work.A conference cannot happen without the help of its reviewers, and this year was no exception. Fiftyfive reviewers worked diligently to ensure that every paper had at least three independent reviews. It was a significant effort to produce the 317 reviews that ended up in the system, and we thank the reviewers from the bottom of our heart. New to the conferences this year was a meta review process, in which 13 diligent meta reviewers together examined all reviews for each submission and reconciled those reviews into a coherent message for each author. We hope the meta review process enabled authors to have more substantive feedback on their work, whether it appears in the final program or not.The conference runs from Thursday to Saturday and each day offers something of interest to attendees. On Thursday our keynote speaker is Dr. Flavio Villanustre, Vice-President of Technology Architecture \&amp; Product for LexisNexis and HPCC Systems. The day continues with a workshop on end-user development activities and paper sessions for both SIGITE and RIIT. Thursday concludes with a reception, which we know will be useful for networking with colleagues old and new. Friday introduces a new presentation format, lightning talks on research in progress, at the conferences. There are also paper sessions for SIGITE and RIIT, a poster session in the afternoon and, of course, more opportunities for networking during lunch and the breaks. Saturday offers a three-hour workshop on process-oriented guided inquiry learning (POGIL) as well as a panel on mobile computing courses and some excellent SIGITE papers. We also hope that you stay for the closing session where we will share our plans for SIGITE/RIIT 2015 in Chicago.We hope you find the conference presentations interesting and thought-provoking, you reconnect with colleagues you know, you find new collaborators, and you submit the work that results to SIGITE or RIIT next year. The excellence you see at SIGITE/RIIT 2014 depends on your energy and effort, and we thank you for letting us be a part of it.},
location = {Atlanta, Georgia, USA}
}

@inproceedings{10.1145/3230833.3233251,
author = {Blanc, Gregory and Kheir, Nizar and Ayed, Dhouha and Lefebvre, Vincent and de Oca, Edgardo Montes and Bisson, Pascal},
title = {Towards a 5G Security Architecture: Articulating Software-Defined Security and Security as a Service},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233251},
doi = {10.1145/3230833.3233251},
abstract = {5G is envisioned as a transformation of the communications architecture towards multi-tenant, scalable and flexible infrastructure, which heavily relies on virtualised network functions and programmable networks. In particular, orchestration will advance one step further in blending both compute and data resources, usually dedicated to virtualisation technologies, and network resources into so-called slices. Although 5G security is being developed in current working groups, slice security is seldom addressed.In this work, we propose to integrate security in the slice life cycle, impacting its management and orchestration that relies on the virtualization/softwarisation infrastructure. The proposed security architecture connects the demands specified by the tenants through as-a-service mechanisms with built-in security functions relying on the ability to combine enforcement and monitoring functions within the software-defined network infrastructure. The architecture exhibits desirable properties such as isolating slices down to the hardware resources or monitoring service-level performance.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {47},
numpages = {8},
keywords = {Security as a Service, Network Slicing, Software-Defined Security},
location = {Hamburg, Germany},
series = {ARES '18}
}

@inproceedings{10.4108/icst.pervasivehealth.2014.255331,
author = {Weiss, Patrick and Heldmann, Marcus and Gabrecht, Alexander and Schweikard, Achim and M\"{u}nte, Thomas M. and Maehle, Erik},
title = {A Low Cost Tele-Rehabilitation Device for Training of Wrist and Finger Functions after Stroke},
year = {2014},
isbn = {9781631900112},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.pervasivehealth.2014.255331},
doi = {10.4108/icst.pervasivehealth.2014.255331},
abstract = {There is a need for robotic rehabilitation devices that improve the outcome while reducing the cost of therapy. This paper presents a device for training of supination/pronation, dorsal wrist extension, and finger manipulation after stroke. The system exhibits modularity in terms of the communication architecture and different optional components. User interfaces (UI) can be implemented on different kinds of devices including a Rasperry Pi single-board computer on which a Qt-based graphical UI was run in this instance. Tele-rehabilitation functionality is included using SSL-encrypted RESTful web services on a three-tier architecture. Expensive sensors were omitted in order to have a cost-effective system which is a requirement for home-based rehabilitation. The current-based torque sensing is evaluated by comparing current measurements to force-torque sensor values. After canceling out the static friction, the low error justified the omission of an additional sensor.},
booktitle = {Proceedings of the 8th International Conference on Pervasive Computing Technologies for Healthcare},
pages = {422–425},
numpages = {4},
keywords = {robotic rehabilitation, stroke, wrist and finger functions, home health care, tele-rehabilitation},
location = {Oldenburg, Germany},
series = {PervasiveHealth '14}
}

@article{10.1145/3399742,
author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
title = {Spectre Attacks: Exploiting Speculative Execution},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3399742},
doi = {10.1145/3399742},
abstract = {Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try to guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access the victim's memory and registers, and can perform operations with measurable side effects.Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side-channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, such as operating system process separation, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing and side-channel attacks. These attacks represent a serious threat to actual systems because vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices.Although makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.},
journal = {Commun. ACM},
month = {jun},
pages = {93–101},
numpages = {9}
}

@article{10.1145/2935634.2935640,
author = {Orwat, Carsten and Bless, Roland},
title = {Values and Networks: Steps Toward Exploring Their Relationships},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2935634.2935640},
doi = {10.1145/2935634.2935640},
abstract = {Many technical systems of the Information and Communication Technology (ICT) sector enable, structure and/or constrain social interactions. Thereby, they influence or implement certain values, including human rights, and affect or raise conflicts among values. The ongoing developments toward an "Internet of everything'' is likely to lead to further value conflicts. This trend illustrates that a better understanding of the relationships between social values and networks is urgently needed because it is largely unknown what values lie behind protocols, design principles, or technical and organizational options of the Internet. This paper focuses on the complex steps of realizing human rights in Internet architectures and protocols as well as in Internet-based products and services. Besides direct implementation of values in Internet protocols, there are several other options that can indirectly contribute to realizing human rights via political processes and market choices. Eventually, a better understanding of what values can be realized by networks in general, what technical measures may affect certain values, and where complementary institutional developments are needed may lead toward a methodology for considering technical and institutional systems together.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {25–31},
numpages = {7},
keywords = {governance, rules, human rights, values, institutions, communication protocols, network design}
}

@inproceedings{10.1145/2785592.2785611,
author = {Adjepon-Yamoah, David and Romanovsky, Alexander and Iliasov, Alexei},
title = {A Reactive Architecture for Cloud-Based System Engineering},
year = {2015},
isbn = {9781450333467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785592.2785611},
doi = {10.1145/2785592.2785611},
abstract = {The paper introduces an architecture to support system engineering on the cloud. It employs the main benefits of the cloud: scalability, parallelism, cost-effectiveness, multi-user access and flexibility. The architecture includes an open toolbox which provides tools as a service to support various phases of system engineering. The architecture uses the Open Services for Life-cycle Collaboration (OSLC) technology to create a reactive middleware that informs all stakeholders about any changes in the development artefacts. It facilitates the interoperability of tools and enables the workflow of tools to support complex engineering steps. Another component of the architecture is a shared repository of artefacts. All the artefacts generated during a system engineering process are stored in the repository, and can be accessed by relevant stakeholders. The shared repository also serves as a platform to support a protocol for formal model decomposition and group work on the decomposed models. Finally, the architecture includes components for ensuring the dependability of the system engineering process.},
booktitle = {Proceedings of the 2015 International Conference on Software and System Process},
pages = {77–81},
numpages = {5},
keywords = {dependability, artefact repository, reactive architecture, cloud computing, system engineering},
location = {Tallinn, Estonia},
series = {ICSSP 2015}
}

@inproceedings{10.1145/2789168.2790089,
author = {Kurose, James F.},
title = {Research Challenges and Opportunities in a Mobility-Centric World},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2790089},
doi = {10.1145/2789168.2790089},
abstract = {The Internet recently passed an historic inflection point, with the number of broadband mobile devices surpassing the number of wired PCs and servers connected to the Internet. Mobility now profoundly affects the architecture, services and applications in both the wireless and wired domains. In this "bottom up" talk, we begin by discussing several specific mobility-related challenges and recent results in areas including mobility measurement (including privacy considerations) and modeling, and context-sensitive services. We then take a broader look at current and future challenges, and conclude by discussing several NSF investments in programs and projects in area of mobile networking.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {290},
numpages = {1},
keywords = {measurement modeling, architecture, mobility, computer networks},
location = {Paris, France},
series = {MobiCom '15}
}

@inproceedings{10.1145/2987443.2987482,
author = {Orsini, Chiara and King, Alistair and Giordano, Danilo and Giotsas, Vasileios and Dainotti, Alberto},
title = {BGPStream: A Software Framework for Live and Historical BGP Data Analysis},
year = {2016},
isbn = {9781450345262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987443.2987482},
doi = {10.1145/2987443.2987482},
abstract = {We present BGPStream, an open-source software framework for the analysis of both historical and real-time Border Gateway Protocol (BGP) measurement data. Although BGP is a crucial operational component of the Internet infrastructure, and is the subject of research in the areas of Internet performance, security, topology, protocols, economics, etc., there is no efficient way of processing large amounts of distributed and/or live BGP measurement data. BGPStream fills this gap, enabling efficient investigation of events, rapid prototyping, and building complex tools and large-scale monitoring applications (e.g., detection of connectivity disruptions or BGP hijacking attacks). We discuss the goals and architecture of BGPStream. We apply the components of the framework to different scenarios, and we describe the development and deployment of complex services for global Internet monitoring that we built on top of it.},
booktitle = {Proceedings of the 2016 Internet Measurement Conference},
pages = {429–444},
numpages = {16},
keywords = {bgp monitoring, internet routing, network measurement, network monitoring, bgp measurement, real-time monitoring, internet measurement},
location = {Santa Monica, California, USA},
series = {IMC '16}
}

@inproceedings{10.1145/2789168.2790094,
author = {Cui, Yong and Lai, Zeqi and Wang, Xin and Dai, Ningwei and Miao, Congcong},
title = {QuickSync: Improving Synchronization Efficiency for Mobile Cloud Storage Services},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2790094},
doi = {10.1145/2789168.2790094},
abstract = {Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstance even though the incremental sync is implemented. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 52.9\% sync time in our experiment settings.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {592–603},
numpages = {12},
keywords = {mobile cloud storage, measurement, performance},
location = {Paris, France},
series = {MobiCom '15}
}

@article{10.1145/2998573,
author = {Fernandes, Fernando and Weigel, Lucas and Jung, Claudio and Navaux, Philippe and Carro, Luigi and Rech, Paolo},
title = {Evaluation of Histogram of Oriented Gradients Soft Errors Criticality for Automotive Applications},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2998573},
doi = {10.1145/2998573},
abstract = {Pedestrian detection reliability is a key problem for autonomous or aided driving, and methods that use Histogram of Oriented Gradients (HOG) are very popular. Embedded Graphics Processing Units (GPUs) are exploited to run HOG in a very efficient manner. Unfortunately, GPUs architecture has been shown to be particularly vulnerable to radiation-induced failures. This article presents an experimental evaluation and analytical study of HOG reliability. We aim at quantifying and qualifying the radiation-induced errors on pedestrian detection applications executed in embedded GPUs.We analyze experimental results obtained executing HOG on embedded GPUs from two different vendors, exposed for about 100 hours to a controlled neutron beam at Los Alamos National Laboratory. We consider the number and position of detected objects as well as precision and recall to discriminate critical erroneous computations. The reported analysis shows that, while being intrinsically resilient (65\% to 85\% of output errors only slightly impact detection), HOG experienced some particularly critical errors that could result in undetected pedestrians or unnecessary vehicle stops.Additionally, we perform a fault-injection campaign to identify HOG critical procedures. We observe that Resize and Normalize are the most sensitive and critical phases, as about 20\% of injections generate an output error that significantly impacts HOG detection. With our insights, we are able to find those limited portions of HOG that, if hardened, are more likely to increase reliability without introducing unnecessary overhead.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {38},
numpages = {25},
keywords = {pedestrian detection, HOG}
}

@inproceedings{10.1145/2656075.2656086,
author = {Hsieh, Chih-Ming and Samie, Farzad and Srouji, M. Sammer and Wang, Manyi and Wang, Zhonglei and Henkel, J\"{o}rg},
title = {Hardware/Software Co-Design for a Wireless Sensor Network Platform},
year = {2014},
isbn = {9781450330510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656075.2656086},
doi = {10.1145/2656075.2656086},
abstract = {Wireless sensor networks have become shared resources providing sensing services to monitor ambient environment. The tasks performed by the sensor nodes and the network structure are becoming more and more complex so that they cannot be handled efficiently by traditional sensor nodes any more. The traditional sensor node architecture, which has software implementation running on a fixed hardware design, is no longer fit to the changing requirements when new applications with complex computation are added to this shared infrastructure due to several reasons. First, the operation behavior changes because of the application requirements and the environmental conditions which makes a fixed architecture not efficient all the time. Second, to collaborate with other already deployed sensor networks and to maintain an efficient network structure, the sensor nodes require flexible communication capabilities. Furthermore, the information required to determine an efficient hardware/software co-design under the system constraints cannot be known a priori. Therefore a platform which can adapt to run-time situations will play an important role in wireless sensor networks. In this paper, we present a hardware/software co-design framework for a wireless sensor platform, which can adaptively change its hardware/software configuration to accelerate complex operations and provides a flexible communication mechanism to deal with complex network structures. We perform real-world measurements on our prototype to analyze its capabilities. In addition, our case studies with prototype implementation and network simulations show the energy savings of the sensor network application by using the proposed design with run-time adaptivity.},
booktitle = {Proceedings of the 2014 International Conference on Hardware/Software Codesign and System Synthesis},
articleno = {1},
numpages = {10},
keywords = {sensor networks, FPGA, multi-radio, reconfiguration, hardware accelerator, low power},
location = {New Delhi, India},
series = {CODES '14}
}

@inproceedings{10.1145/3132340.3132358,
author = {Olariu, Stephan and Florin, Ryan},
title = {Vehicular Clouds Research: What is Missing?},
year = {2017},
isbn = {9781450351645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132340.3132358},
doi = {10.1145/3132340.3132358},
abstract = {Vehicular Clouds (VCs) have become an active research topic. However, even a cursory look reveals that the VC literature of recent years is full of papers discussing fanciful VC architectures and services that often seem too good to be true. And many of them are. It seems to us that promoting VC models without any regard to their practical feasibility is apt to discredit the VC concept altogether. Part of the problem stems from the fact that some authors do not seem to be concerned with the obvious fact that moving vehicles' residency times in the VC may, indeed, be very short and, therefore, so is their contribution to the amount of useful work performed. Should a vehicle running a user job leave the VC prematurely, the amount of work performed by that vehicle may be lost, unless special precautions are taken. Such precautionary measures involve either some flavor of checkpointing or some form of redundant job assignment. Both approaches have consequences in terms of overhead and impact job completion time. The success of conventional cloud computing (CC) is attributable to the ability to provide quantifiable functional characteristics such as scalability, reliability and availability. By the same token, if the VCs are to see a widespread adoption, the same quantitative aspects have to be addressed here, too. Feasibility issues in terms of sufficient compute power, communication bandwidth, reliability, availability, and job duration time are all fundamental quantitative aspects of VCs that need to be studied and understood before one can claim with any degree of certainty that they can support the workload for which they are intended. The first contribution of this paper is to make a case for the stringent need to address quantitatively the performance characteristics of VC architectures and proposed services. Our second contribution is to point out directions and challenges facing the VC community.},
booktitle = {Proceedings of the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {77–84},
numpages = {8},
keywords = {vehicular clouds, reliability, redundancy, availability, acm proceedings, cloud computing},
location = {Miami, Florida, USA},
series = {DIVANet '17}
}

@article{10.1109/TNET.2020.2981514,
author = {Li, Yang and Zheng, Jianwei and Li, Zhenhua and Liu, Yunhao and Qian, Feng and Bai, Sen and Liu, Yao and Xin, Xianlong},
title = {Understanding the Ecosystem and Addressing the Fundamental Concerns of Commercial MVNO},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2981514},
doi = {10.1109/TNET.2020.2981514},
abstract = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs), which operate on top of existing cellular infrastructures of base carriers, while offering cheaper or more flexible data plans compared to those of the base carriers. In this paper, we present a two-year measurement study towards understanding various fundamental aspects of today's MVNO ecosystem, including its architecture, customers, performance, economics, and the complex interplay with the base carrier. Our study focuses on a large commercial MVNO with one million customers, operating atop a nation-wide base carrier. Our measurements clarify several key concerns raised by MVNO customers, such as inaccurate billing and potential performance discrimination with the base carrier. We also leverage big data analytics, statistical modeling, and machine learning to address the MVNO's key concerns with regard to data usage prediction, data plan reselling, customer churn mitigation, and billing delay reduction. Our proposed techniques can help achieve higher revenues and improved services for commercial MVNOs.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1364–1377},
numpages = {14}
}

@inproceedings{10.1145/3307334.3326070,
author = {Xiao, Ao and Liu, Yunhao and Li, Yang and Qian, Feng and Li, Zhenhua and Bai, Sen and Liu, Yao and Xu, Tianyin and Xin, Xianlong},
title = {An In-Depth Study of Commercial MVNO: Measurement and Optimization},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326070},
doi = {10.1145/3307334.3326070},
abstract = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs), which operate on top of the existing cellular infrastructures of base carriers while offering cheaper or more flexible data plans compared to those of the base carriers. In this paper, we present a nearly two-year measurement study towards understanding various key aspects of today's MVNO ecosystem, including its architecture, performance, economics, customers, and the complex interplay with the base carrier. Our study focuses on a large commercial MVNO with reviseabout 1 million customers, operating atop a nation-wide base carrier. Our measurements clarify several key concerns raised by MVNO customers, such as inaccurate billing and potential performance discrimination with the base carrier. We also leverage big data analytics and machine learning to optimize an MVNO's key businesses such as data plan reselling and customer churn mitigation. Our proposed techniques can help achieve \%will lead to higher revenues and improved services for commercial MVNOs.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {457–468},
numpages = {12},
keywords = {network performance, mvno, machine learning, churn mitigation, data prediction},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@inproceedings{10.1109/CCGRID.2017.27,
author = {Colmant, Maxime and Felber, Pascal and Rouvoy, Romain and Seinturier, Lionel},
title = {WattsKit: Software-Defined Power Monitoring of Distributed Systems},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.27},
doi = {10.1109/CCGRID.2017.27},
abstract = {The design and the deployment of energy-efficient distributed systems is a challenging task, which requires software engineers to consider all the layers of a system, from hardware to software. In particular, monitoring and analyzing the power consumption of a distributed system spanning several---potentially heterogeneous---nodes becomes particularly tedious when aiming at a finer granularity than observing the power consumption of hosting nodes. While the state-of-the-art in software-defined power meters fails to deliver adaptive solutions to offer such service-level perspective and to cope with the diversity of hardware CPU architectures, this paper proposes to automatically learn the power models of the nodes supporting a distributed system, and then to use these inferred power models to better understand how the power consumption of the system's processes is distributed across nodes at runtime.Our solution, named WattsKit, offers a modular toolkit to build software-defined power meters "\`{a} la carte", thus dealing with the diversity of user and hardware requirements. Beyond the demonstrated capability of covering a wide diversity of CPU architectures with high accuracy, we illustrate the benefits of adopting software-defined power meters to analyze the power consumption of complex layered and distributed systems. In particular, we illustrate the capability of our approach to monitor the power consumption of a system composed of Docker Swarm, Weave,Elasticsearch, and Apache ZooKeeper. Thanks to WattsKit, developers and administrators are now able to identify potential power leaks in their software infrastructure.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {514–523},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3404555.3404632,
author = {Lin, Huang and Diangang, Wang and Xiao, Liu and Yongning, Zhuo and Yong, Zeng},
title = {A Predictor Based on Parallel LSTM for Burst Network Traffic Flow},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404632},
doi = {10.1145/3404555.3404632},
abstract = {The network traffic prediction is a key step for service quality control in computer network. Aimed at the problem that the performance of the traditional prediction method significantly degrades for the burst short-term flow, this paper proposed a double LSTM architecture, one of which acts as the main flow predictor, another as the detector for the moment the burst flow starts. The two LSTM unit can exchange their internal state's information, and the predictor uses the detector's information to improve the accuracy of the prediction. To train the offline double LSTM architecture, a Depth-Backstep algorithm is put forward. To use the architecture to perform the online prediction, a pulse series is used as a simulant of the burst event. A simulation experiment is designed to test performance of the predictor. The results of the experiment show that the prediction accuracy of the double LSTM architecture is significantly improved, compared with the traditional single LSTM architecture.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {476–480},
numpages = {5},
keywords = {computer network, Traffic prediction, machine learning, LSTM},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1145/3053600.3053634,
author = {Walter, J\"{u}rgen and Stier, Christian and Koziolek, Heiko and Kounev, Samuel},
title = {An Expandable Extraction Framework for Architectural Performance Models},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053634},
doi = {10.1145/3053600.3053634},
abstract = {Providing users with Quality of Service (QoS) guarantees and the prevention of performance problems are challenging tasks for software systems. Architectural performance models can be applied to explore performance properties of a software system at design time and run time. At design time, architectural performance models support reasoning on effects of design decisions. At run time, they enable automatic reconfigurations by reasoning on the effects of changing user behavior. In this paper, we present a framework for the extraction of architectural performance models based on monitoring log files generalizing over the targeted architectural modeling language. Using the presented framework, the creation of a performance model extraction tool for a specific modeling formalism requires only the implementation of a key set of object creation routines specific to the formalism. Our framework integrates them with extraction techniques that apply to many architectural performance models, e.g., resource demand estimation techniques. This lowers the effort to implement performance model extraction tools tremendously through a high level of reuse. We evaluate our framework presenting builders for the Descartes Modeling Language (DML) and the Palladio Component Model(PCM). For the extracted models we compare simulation results with measurements receiving accurate results.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {165–170},
numpages = {6},
keywords = {builder pattern, descartes modeling language, palladio component model, automated performance model extraction},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1109/ASE.2015.93,
author = {Salama, Maria},
title = {Stability of Self-Adaptive Software Architectures},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.93},
doi = {10.1109/ASE.2015.93},
abstract = {Stakeholders and organisations are increasingly looking for long-lived software. As architectures have a profound effect on the operational life-time of the software and the quality of the service provision, architectural stability could be considered a primary criterion towards achieving the long-livety of the software. Architectural stability is envisioned as the next step in quality attributes, combining many inter-related qualities. This research suggests the notion of behavioural stability as a primary criterion for evaluating whether the architecture maintains achieving the expected quality attributes, maintaining architecture robustness, and evaluating how well the architecture accommodates run-time evolutionary changes. The research investigates the notion of architecture stability at run-time in the context of self-adaptive software architectures. We expect to define, characterise and analyse this intuitive concept, as well as identify the consequent trade-offs to be dynamically managed and enhance the self-adaptation process for a long-lived software.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {886–889},
numpages = {4},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.5555/2685048.2685073,
author = {Zhai, Ennan and Chen, Ruichuan and Wolinsky, David Isaac and Ford, Bryan},
title = {Heading off Correlated Failures through Independence-as-a-Service},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {Today's systems pervasively rely on redundancy to ensure reliability. In complex multi-layered hardware/software stacks, however - especially in the clouds where many independent businesses deploy interacting services on common infrastructure - seemingly independent systems may share deep, hidden dependencies, undermining redundancy efforts and introducing unanticipated correlated failures. Complementing existing post-failure forensics, we propose Independence-as-a-Service (or INDaaS), an architecture to audit the independence of redundant systems proactively, thus avoiding correlated failures. INDaaS first utilizes pluggable dependency acquisition modules to collect the structural dependency information (including network, hardware, and software dependencies) from a variety of sources. With this information, INDaaS then quantifies the independence of systems of interest using pluggable auditing modules, offering various performance, precision, and data secrecy tradeoffs. While the most general and efficient auditing modules assume the auditor is able to obtain all required information, INDaaS can employ private set intersection cardinality protocols to quantify the independence even across businesses unwilling to share their full structural information with anyone. We evaluate the practicality of INDaaS with three case studies via auditing realistic network, hardware, and software dependency structures.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {317–334},
numpages = {18},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{10.1145/2627566.2627575,
author = {Antonescu, Alexandru-Florian and Braun, Torsten},
title = {Modeling and Simulation of Concurrent Workload Processing in Cloud-Distributed Enterprise Information Systems},
year = {2014},
isbn = {9781450329927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627566.2627575},
doi = {10.1145/2627566.2627575},
abstract = {Cloud Computing enables provisioning and distribution of highly scalable services in a reliable, on-demand and sustainable manner. Distributed Enterprise Information Systems (dEIS) are a class of applications with important economic value and with strong requirements in terms of performance and reliability. In order to validate dEIS architectures, stability, scaling and SLA compliance, large testing deployments are necessary, adding complexity to the design and testing of such systems. To fill this gap, we present and validate a methodology for modeling and simulating such complex distributed systems using the CloudSim cloud computing simulator, based on measurement data from an actual distributed system. We present an approach for creating a performance-based model of a distributed cloud application using recorded service performance traces. We then show how to integrate the created model into CloudSim. We validate the CloudSim simulation model by comparing performance traces gathered during distributed concurrent experiments with simulation results using different VM configurations. We demonstrate the usefulness of using a cloud simulator for modeling properties of real cloud-distributed applications.},
booktitle = {Proceedings of the 2014 ACM SIGCOMM Workshop on Distributed Cloud Computing},
pages = {11–16},
numpages = {6},
keywords = {cloud computing, distributed applications, modelling and simulation, performance profiling},
location = {Chicago, Illinois, USA},
series = {DCC '14}
}

@inproceedings{10.5555/3395101.3395117,
author = {Patan\'{e}, Giancarlo M. M. and Valastro, Gianluca C. and Sambo, Yusuf A. and Ozturk, Metin and Hussain, Sajjad and Imran, Muhammad A. and Panno, Daniela},
title = {Flexible SDN/NFV-Based SON Testbed for 5G Mobile Networks},
year = {2020},
isbn = {9781728129235},
publisher = {IEEE Press},
abstract = {In the next few years, a considerable innovation concerning the design of the future 5G mobile networks will be a concrete step towards enabling effective high throughput and low latency services. Software Defined Networking (SDN), Network Function Virtualization (NFV) and Self Organizing Network (SON) are considered the enabling technologies to achieve these goals. In this paper, assuming a Control-Data Separation Architecture (CDSA), we propose a flexible SDN/NFV-based SON testbed, for future 5G mobile networks. The main contribution of our work is to cover the need for a CDSA based testbed, enabling the investigation of the NG-SON capabilities for practical implementations. We implement two different testbed setups, a real one and a virtualized one, both based on the FlexRAN and OpenAirInterface software tools. First, we implement a specific case study, i.e., the RAN entities activation/deactivation procedures. Next, we carry out time measurements, concerning the aforementioned procedures, in order to prove proper testbed functioning. Finally, we validate the C-SON and D-SON capabilities of our testbed, considering the features of the results.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Symposium on Distributed Simulation and Real Time Applications},
pages = {79–86},
numpages = {8},
keywords = {5G, FlexRAN, NFV, Cloud-RAN, SDN, OpenAirInterface},
location = {Cosenza, Italy},
series = {DS-RT '19}
}

