@inproceedings{10.1145/3306306.3328001,
author = {Blanchard, Sam and Huang, Jia-Bin and Williams, Christopher B. and Meenakshisundaram, Viswanath and Kubalak, Joseph and Lokegaonkar, Sanket},
title = {Source Form an Automated Crowdsourced Object Generator},
year = {2019},
isbn = {9781450363167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306306.3328001},
doi = {10.1145/3306306.3328001},
abstract = {Source Form is a stand-alone device capable of collecting crowdsourced images of a user-defined object, stitching together available visual data (e.g., photos tagged with search term) through photogrammetry, creating watertight models from the resulting point cloud and 3D printing a physical form. This device works completely independent of subjective user input resulting in two possible outcomes:1. Produce iterative versions of a specific object (e.g., the Statue of Liberty) increasing in detail and accuracy over time as the collective dataset (e.g., uploaded images of the statue) grows.2. Produce democratized versions of common objects (e.g., an apple) by aggregating a spectrum of tagged image results.This project demonstrates that an increase in readily available image data closes the gap between physical and digital perceptions of form through time. For example, when Source Form is asked to print the Statue of Liberty today and then print again 6 months from now, the later result will be more accurate and detailed than the previous version. As people continue to take pictures of the monument and upload them to social media, blogs and photo sharing sites, the database of images grows in quantity and quality. Because Source Form gathers a new dataset with each print, the resulting forms will always be evolving. The collection of prints the machine produces over time are cataloged and displayed in linear groupings, providing viewers an opportunity to see growth and change in physical space.In addition to rendering change over time, a snapshot of a more common object's web perception could be created. For example, when an image search for "apple" is performed, the results are a spectrum of condition and species from rotting crab apples to gleaming Granny-Smith's. Source Form aggregates all of these images into one model and outputs the collective web presence of an "apple". Characteristics of the model are guided by the frequency and order in response to the image web search. The resulting democratized forms are emblematic of the web's collective and popular perceptions.},
booktitle = {ACM SIGGRAPH 2019 Studio},
articleno = {13},
numpages = {2},
keywords = {3D printing, photogrammetry, hardware, sculpture, fine art, crowdsourcing, automation},
location = {Los Angeles, California},
series = {SIGGRAPH '19}
}

@inproceedings{10.1145/3011077.3011126,
author = {Van Sinh, Nguyen and Ha, Tran Manh and Thanh, Nguyen Tien},
title = {Filling Holes on the Surface of 3D Point Clouds Based on Tangent Plane of Hole Boundary Points},
year = {2016},
isbn = {9781450348157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011077.3011126},
doi = {10.1145/3011077.3011126},
abstract = {Filling the holes of a triangular mesh has been studied for many years in the field of geometric modeling. This research is one of the reconstructing steps of a triangular mesh (or called refinement of a mesh) in order to improve the quality of a 3D triangular surface. With the same idea of hole filling in a mesh, filling in the holes of a 3D point cloud is still a challenge to the researchers. This paper describes a method for filling holes in an elevation surface of 3D point clouds structured in a 3D grid. The novelty of the method is processed directly on the 3D point clouds consisting of two steps. In the first step, we determine the boundary of hole. In the second step, we fill the holes based on the computation of tangent plane for each boundary point. Following clock-wise direction on the hole boundary, we compute and insert missing points on each tangent plane. This process is repeated and refined ring by ring from the hole boundary to the inside of the hole. The obtained results show that the processing time of algorithm is very fast, the output surfaces preserve their initial shapes and local curvatures.},
booktitle = {Proceedings of the 7th Symposium on Information and Communication Technology},
pages = {331–338},
numpages = {8},
keywords = {hole filling, elevation surface, boundary point, tangent plane, 3D point cloud},
location = {Ho Chi Minh City, Vietnam},
series = {SoICT '16}
}

@article{10.1145/3072959.3073635,
author = {Schertler, Nico and Tarini, Marco and Jakob, Wenzel and Kazhdan, Misha and Gumhold, Stefan and Panozzo, Daniele},
title = {Field-Aligned Online Surface Reconstruction},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073635},
doi = {10.1145/3072959.3073635},
abstract = {Today's 3D scanning pipelines can be classified into two overarching categories: offline, high accuracy methods that rely on global optimization to reconstruct complex scenes with hundreds of millions of samples, and online methods that produce real-time but low-quality output, usually from structure-from-motion or depth sensors. The method proposed in this paper is the first to combine the benefits of both approaches, supporting online reconstruction of scenes with hundreds of millions of samples from high-resolution sensing modalities such as structured light or laser scanners. The key property of our algorithm is that it sidesteps the signed-distance computation of classical reconstruction techniques in favor of direct filtering, parametrization, and mesh and texture extraction. All of these steps can be realized using only weak notions of spatial neighborhoods, which allows for an implementation that scales approximately linearly with the size of each dataset that is integrated into a partial reconstruction. Combined, these algorithmic differences enable a drastically more efficient output-driven interactive scanning and reconstruction workflow, where the user is able to see the final quality field-aligned textured mesh during the entirety of the scanning procedure. Holes or parts with registration problems are displayed in real-time to the user and can be easily resolved by adding further localized scans, or by adjusting the input point cloud using our interactive editing tools with immediate visual feedback on the output mesh. We demonstrate the effectiveness of our algorithm in conjunction with a state-of-the-art structured light scanner and optical tracking system and test it on a large variety of challenging models.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {77},
numpages = {13},
keywords = {surface reconstruction, parameterization}
}

@inproceedings{10.1145/2797022.2797039,
author = {Anwar, Ali and Sailer, Anca and Kochut, Andrzej and Butt, Ali R.},
title = {Anatomy of Cloud Monitoring and Metering: A Case Study and Open Problems},
year = {2015},
isbn = {9781450335546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797022.2797039},
doi = {10.1145/2797022.2797039},
abstract = {Microservices based architecture has recently gained traction among the cloud service providers in quest for a more scalable and reliable modular architecture. In parallel with this architectural choice, cloud providers are also facing the market demand for fine grained usage based prices. Both the management of the microservices complex dependencies, as well as the fine grained metering require the providers to track and log detailed monitoring data from their deployed cloud setups. Hence, on one hand, the providers need to record all such performance changes and events, while on the other hand, they are concerned with the additional cost associated with the resources required to store and process this ever increasing amount of collected data.In this paper, we analyze the design of the monitoring subsystem provided by open source cloud solutions, such as OpenStack. Specifically, we analyze how the monitoring data is collected by OpenStack and assess the characteristics of the data it collects, aiming to pinpoint the limitations of the current approach and suggest alternate solutions. Our preliminary evaluation of the proposed solutions reveals that it is possible to reduce the monitored data size by up to 80\% and missed anomaly detection rate from 3\% to as low as 0.05\% to 0.1\%.},
booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
articleno = {6},
numpages = {7},
location = {Tokyo, Japan},
series = {APSys '15}
}

@inproceedings{10.1109/UCC.2014.157,
author = {Wehrle, Dennis and Liebetraut, Thomas and Valizada, Isgandar and Rechert, Klaus},
title = {Emulation-as-a-Service - Workflows and Infrastructure to Support Recomputable Science},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.157},
doi = {10.1109/UCC.2014.157},
abstract = {The computational age and its fast technological progress boosted research output of almost all disciplines. However, these early benefits come along with a burden: while the exchange of research data and ideas is easier and more effective than ever, assuring both short- and long-term access to fundamental scientific methods is more difficult than anticipated. In particular, functional access to data processing methods, tool-chains and scientific workflows is indispensable in order to verify and replicate research findings. This article is proposing emulation as a technique for generalization of data processing environments, serving as first step towards long-term accessibility of research data and associated methods. We present a Cloud-based emulation-as-a-service framework for publication and citation of scientific workflows and research data, since having re-usable processing environments, emulation can be used for technical verification of research data, i.e. Ensure minimal quality assurance like completeness and an explicit list of potential external dependencies, fundamental for future risk-assessment.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {962–967},
numpages = {6},
keywords = {long-term preservation, citing research data, workflows, re-use, verification, research data, replicability},
series = {UCC '14}
}

@inproceedings{10.1145/3338498.3358650,
author = {Matyunin, Nikolay and Wang, Yujue and Arul, Tolga and Kullmann, Kristian and Szefer, Jakub and Katzenbeisser, Stefan},
title = {MagneticSpy: Exploiting Magnetometer in Mobile Devices for Website and Application Fingerprinting},
year = {2019},
isbn = {9781450368308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338498.3358650},
doi = {10.1145/3338498.3358650},
abstract = {Recent studies have shown that aggregate CPU usage and power consumption traces on smartphones can leak information about applications running on the system or websites visited. In response, access to such data has been blocked for mobile applications starting from Android 8. In this work, we explore a new source of side-channel leakage for this class of attacks. Our method is based on the fact that electromagnetic activity caused by mobile processors leads to noticeable disturbances in magnetic sensor measurements on mobile devices, with the amplitude being proportional to the CPU workload. Therefore, recorded sensor data can be analyzed to reveal information about ongoing activities. The attack works on a number of devices: we evaluated 80 models of modern smartphones and tablets and observed the reaction of the magnetometer to the CPU activity on 56 of them. On selected devices we were able to successfully identify which application has been opened (with up to 90\% accuracy) or which web page has been loaded (up to 91\% accuracy). The presented side channel poses a significant risk to end users' privacy, as the sensor data can be recorded from native apps or even from web pages without user permissions. Finally, we discuss possible countermeasures to prevent the presented information leakage.},
booktitle = {Proceedings of the 18th ACM Workshop on Privacy in the Electronic Society},
pages = {135–149},
numpages = {15},
keywords = {magnetometer, hardware side channels, mobile security, smartphone sensors, information leakage, website fingerprinting, application fingerprinting},
location = {London, United Kingdom},
series = {WPES'19}
}

@inproceedings{10.1145/3297067.3297079,
author = {Al-Omair, Osamah M. and Huang, Shihong},
title = {A Comparative Study on Detection Accuracy of Cloud-Based Emotion Recognition Services},
year = {2018},
isbn = {9781450366052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297067.3297079},
doi = {10.1145/3297067.3297079},
abstract = {The ability of software systems adapting to human's input is a key element in the symbiosis of human-system co-adaptation, where human and software-based systems work together in a close partnership to achieve synergetic goals. This seamless integration will eliminate the barriers between human and machine. A critical requirement for co-adaptive systems is software system's ability to recognize human emotion, in which the system can detect and interpret users' emotions and adapt accordingly. There are numerous solutions that provide the technologies for emotion recognition. However, selecting an appropriate solution for a given task within a specific application domain can be challenging. The vast variation between these solutions makes the selecting task even more difficult. This paper compares cloud-based emotion recognition services offered by Amazon, Google, and Microsoft. These services detect human emotion through facial expression recognition with the utilization of computer vision. The focus of this paper is to measure the detection accuracy of these services. Accuracy is calculated based on the highest confidence rating returned by each service. All three services have been tested with the same dataset. This paper concludes with findings and recommendations based on our comparative analysis among these services.},
booktitle = {Proceedings of the 2018 International Conference on Signal Processing and Machine Learning},
pages = {142–148},
numpages = {7},
keywords = {Co-adaptive systems, Affective computing, Machine emotional intelligence, Machine learning, Facial expression recognition, Human-computer interaction, Emotion recognition},
location = {Shanghai, China},
series = {SPML '18}
}

@inproceedings{10.1145/3236024.3236043,
author = {Basios, Michail and Li, Lingbo and Wu, Fan and Kanthan, Leslie and Barr, Earl T.},
title = {Darwinian Data Structure Selection},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236043},
doi = {10.1145/3236024.3236043},
abstract = {Data structure selection and tuning is laborious but can vastly improve an application’s performance and memory footprint. Some data structures share a common interface and enjoy multiple implementations. We call them Darwinian Data Structures (DDS), since we can subject their implementations to survival of the fittest. We introduce ARTEMIS a multi-objective, cloud-based search-based optimisation framework that automatically finds optimal, tuned DDS modulo a test suite, then changes an application to use that DDS. ARTEMIS achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects and 30 uniformly sampled projects from GitHub. For execution time, CPU usage, and memory consumption, ARTEMIS finds at least one solution that improves all measures for 86\% (37/43) of the projects. The median improvement across the best solutions is 4.8\%, 10.1\%, 5.1\% for runtime, memory and CPU usage. These aggregate results understate ARTEMIS’s potential impact. Some of the benchmarks it improves are libraries or utility functions. Two examples are gson, a ubiquitous Java serialization framework, and xalan, Apache’s XML transformation tool. ARTEMIS improves gson by 16.5\%, 1\% and 2.2\% for memory, runtime, and CPU; ARTEMIS improves xalan’s memory consumption by 23.5\%. Every client of these projects will benefit from these performance improvements.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {118–128},
numpages = {11},
keywords = {Search-based Software Engineering, Software Analysis and Optimisation, Data Structure Optimisation, Genetic Improvement},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3328778.3372567,
author = {Bachrach, Mayra and Morreale, Patricia and Verdi, Gail},
title = {Improving the Outcomes of Hispanics in AP Computer Science},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3372567},
doi = {10.1145/3328778.3372567},
abstract = {This lightning talk describes a proof of concept research project funded by a Google CS Education (CS-ER) Research grant. The project focuses on pedagogical interventions aimed at improving the outcomes of English Language Learners (ELLs) in Advanced Placement Computer Science. The research underway examines the use of Sheltered Instruction (SI), a model from English as a Second Language (ESL) and bilingual education, used in mainstream classrooms across other content areas, in the AP CSA and AP CSP classroom. Strategies and pedagogy from the Sheltered Instruction model are being infused into AP Computer Science curriculum and used in classrooms in participating districts. The districts have been selected to include a range of ELLs and native English speakers. The impact of this approach will be measured by comparing the AP CS exam scores of students in the participating districts with the national and state AP CS exam scores. This lightning talk will focus on the pedagogy development which has taken place and preliminary findings from two cohorts of AP CS and AP CSA teachers, in particular the impact and changes to the teacher's development of CS education lessons and in-class lesson delivery. The project is an interdisciplinary collaboration between faculty from the School of Computer Science and the School of Curriculum and Teaching.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1411},
numpages = {1},
keywords = {academic-discourse, scaffolding, academic-language, equity, hispanics, pedagogy},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/3323503.3349543,
author = {Costa, Arthur F. da and D'Addio, Rafael M. and Fressato, Eduardo P. and Manzato, Marcelo G.},
title = {A Personalized Clustering-Based Approach Using Open Linked Data for Search Space Reduction in Recommender Systems},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3349543},
doi = {10.1145/3323503.3349543},
abstract = {Recommender systems use information about the users' preferences to define relatedness scores towards items. Regardless of the method, a noticeable problem is that the system is required to compute scores for a large amount of unknown items in the database, even though these items may not be related to a determined user. In this manuscript, we propose a technique called search space reduction for recommender systems (SSR4Rec) that reduces the number of unknown pairs the recommender must process. As a pre-processing step, we cluster related items and assign only the closest group to each user, producing a reduced set of unknown pairs. The distance between items, and between clusters and users, is computed by comparing item representations and user profiles built based on attributes extracted from the Linked Open Data cloud. We assess the quality of SSR4Rec by applying it into two well-known RS and comparing the results against the same recommenders without our pre-processing step, as well as against other related baselines. Results show a significant improvement in both ranking accuracy and computational time.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {409–416},
numpages = {8},
keywords = {recommender systems, search space reduction, clustering, linked open data},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/2835022.2835024,
author = {Zang, Andi and Chen, Xin and Trajcevski, Goce},
title = {Digital Terrain Model Generation Using LiDAR Ground Points},
year = {2015},
isbn = {9781450339735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835022.2835024},
doi = {10.1145/2835022.2835024},
abstract = {As the trend of autonomous self-driving cars is becoming more of a reality, High-quality navigation methods and tools become a paramount. This, in turn, is crucially dependent on High-definition maps, for which one of the enabling tools is high resolution Digital Terrain Model (DTM) -- the role and values of which have already been demonstrated even in the settings of manned cars. Traditional DTM generation methods have insurmountable barriers in creating centimeter-level resolution. In this paper, we propose a novel method for fully-automated, high precision DTM generation using the database generated and maintained in our existed dataset, and with no additional overheads in terms of extract labor and equipment cost. The input data is a point cloud captured by the vehicle-mount LiDAR devices which, naturally, has extremely large volume. We show how with Ground Points Processing and DTM Generation steps, we can generate a centimeter-resolution DTM and, as our experiments demonstrate, when compared to DTM form U.S. Geological Survey (USGS) and altitude data from a third party surveying dataset, our proposed DTM indeed provides a higher precision.},
booktitle = {Proceedings of the 1st International ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
pages = {9–15},
numpages = {7},
keywords = {LiDAR, GIS, Digital Terrain Model, Point cloud processing},
location = {Bellevue, WA, USA},
series = {UrbanGIS'15}
}

@inproceedings{10.17210/hcik.2016.01.298,
author = {Kim, Jungwoo and Kim, Hyesook and Choi, Jaeboong},
title = {Development of Smart Product, DUET Using SQFD and Storytelling},
year = {2016},
isbn = {9788968487910},
publisher = {Hanbit Media, Inc.},
address = {Seoul, KOR},
url = {https://doi.org/10.17210/hcik.2016.01.298},
doi = {10.17210/hcik.2016.01.298},
abstract = {This paper presents a smart product design process for a wearable device to provide empathy and fun to users. As the first step, keywords were extracted using open-coding methods from text WebData of online sites for wearable devices, Smardi, Sblog, and Wsite. The Smart Quality Function Deployment (SQFD) was then applied to prioritize the keywords and corresponding user requirements. The key user requirements such as 'separable band from core module' and 'function for media control' were then materialized into a wearable band, DUET, using rapid prototyping, and refined through three stages of user evaluation. DUET connectable to iOS and Android smartphones was introduced by a storytelling transmedia videoclip by experts with a theme of empathy and fun. It was also advertised on a cloud funding site, Indiegogo, and through a PPL in S entertainment program, and received positive responses. Further detailed analysis of user responses was performed for 72 days through the operation of facebook-DUET site and for 10 days through Google keyword marketing which derived various levels of user activities.},
booktitle = {Proceedings of HCI Korea},
pages = {298–306},
numpages = {9},
keywords = {RP, SQFD},
location = {Jeongseon, Republic of Korea},
series = {HCIK '16}
}

@inproceedings{10.1145/3183713.3190656,
author = {Jindal, Alekh and Qiao, Shi and Patel, Hiren and Yin, Zhicheng and Di, Jieming and Bag, Malay and Friedman, Marc and Lin, Yifung and Karanasos, Konstantinos and Rao, Sriram},
title = {Computation Reuse in Analytics Job Service at Microsoft},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3190656},
doi = {10.1145/3183713.3190656},
abstract = {Analytics-as-a-service, or analytics job service, is emerging as a new paradigm for data analytics, be it in a cloud environment or within enterprises. In this setting, users are not required to manage or tune their hardware and software infrastructure, and they pay only for the processing resources consumed per job. However, the shared nature of these job services across several users and teams leads to significant overlaps in partial computations, i.e., parts of the processing are duplicated across multiple jobs, thus generating redundant costs. In this paper, we describe a computation reuse framework, coined CLOUDVIEWS, which we built to address the computation overlap problem in Microsoft's SCOPE job service. We present a detailed analysis from our production workloads to motivate the computation overlap problem and the possible gains from computation reuse. The key aspects of our system are the following: (i) we reuse computations by creating materialized views over recurring workloads, i.e., periodically executing jobs that have the same script templates but process new data each time, (ii) we select the views to materialize using a feedback loop that reconciles the compile-time and run-time statistics and gathers precise measures of the utility and cost of each overlapping computation, and (iii) we create materialized views in an online fashion, without requiring an offline phase to materialize the overlapping computations.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {191–203},
numpages = {13},
keywords = {shared clouds, computation reuse, materialized views},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3394810.3394818,
author = {Ibarz, Jean and Lauer, Micha\"{e}l and Roy, Matthieu and Fabre, Jean-Charles and Fl\'{e}bus, Olivier},
title = {Optimizing Vehicle-to-Cloud Data Transfers Using Soft Real-Time Scheduling Concepts},
year = {2020},
isbn = {9781450375931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394810.3394818},
doi = {10.1145/3394810.3394818},
abstract = {The main promise of intelligent transportation systems (ITS) is that leveraging the information sensed by millions of vehicles will increase the quality of the user's experience. However, the unpredictable nature of road events, combined with a projected network overload, calls for a careful optimization of the vehicles' data transfers, taking into account spatio-temporal, safety and value constraints. In this article, we provide a methodical solution to optimize vehicle-to-cloud (V2C) data transfers, based on a series of steps. First, we show that this optimization problem can be modeled as a soft real-time scheduling problem. Second, we provide an extension of a classical algorithm for the generation of workloads, by increasing its coverage with regards to our use-case representation. Third, we estimate the bounds of an optimal clairvoyant algorithm in order to have a baseline for a fair comparison of existing scheduling algorithms. The results show that, within all these algorithms, one clearly outperforms the others regardless of the load rate. Interestingly, its performance gain increases when overload grows, and it can be implemented very efficiently, which makes it highly suitable for embedded systems.},
booktitle = {Proceedings of the 28th International Conference on Real-Time Networks and Systems},
pages = {161–171},
numpages = {11},
keywords = {data collection, event-based, reliability, embedded system, V2C, real-time, distributed sensing, IoT, mobile system},
location = {Paris, France},
series = {RTNS '20}
}

@article{10.1145/3242901,
author = {Rolin, Raphael and Antaluca, Eduard and Batoz, Jean-Louis and Lamarque, Fabien and Lejeune, Mathieu},
title = {From Point Cloud Data to Structural Analysis Through a Geometrical HBIM-Oriented Model},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3242901},
doi = {10.1145/3242901},
abstract = {The assessment of the structural behavior of historic masonry structures like Gothic cathedrals is an important engineering and architectural issue, because of the economic and cultural relevance of such buildings. In this article, we present a complete numerical methodology for point clouds processing, geometrical and parametric 3D modeling, and finite element structural analysis of the spire of the Cathedral of Senlis, France. Our work highlights the particular difficulties linked with digitization and geometrical modeling of highly complex Gothic structures, as well as the need to find compromises between quality and accuracy of extracted data used for geometrical modeling and structural analysis.The methodology enables the semi-automatic transformation of a three-dimensional points cloud, surveyed through terrestrial laser scanner, into a three-dimensional geometrical historic building information modeling (hBIM)-oriented model, and its use to propose a consistent 3D finite element mesh suitable for advanced structural analysis. A full software chain is integrated in the proposed numerical process, so as to use the most important data contained in the real geometry and accurately transposed in the point clouds. After a successful data processing step with 3DReshaper software that proved to be necessary for enhancement of point clouds, a semi-automated geometrical hBIM-oriented modeling step with Rhinoceros5 software and VisualARQ plugin has allowed the construction of a hybrid model by reverse engineering from the point clouds. This 3D model, containing both geometrical and parametric data of the structure, has been exported to the Hyperworks suite for finite element structural analysis under self-weight. Our computations focused on the estimation of the structure deformation and on the distribution of compression and traction stresses in all components of the complex structure. It is found that the spire is safe. Based on reliable and properly detailed results, our study provides significant information for understanding the behavior of the structure and potential damage monitoring.},
journal = {J. Comput. Cult. Herit.},
month = {may},
articleno = {9},
numpages = {26},
keywords = {finite element structural analysis, terrestrial laser scanning, point clouds, cultural heritage, Historical buildings, building information modeling, geometrical modeling}
}

@inproceedings{10.1145/3178461.3178464,
author = {Tahat, Ashraf and Aburub, Ruba and Al-Zyoude, Aseel and Talhi, Chamseddine},
title = {A Smart City Environmental Monitoring Network and Analysis Relying on Big Data Techniques},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178464},
doi = {10.1145/3178461.3178464},
abstract = {A new integrated environmental monitoring system to carry-out real-time measurements on board a moving vehicle is presented. It is composed of an arbitrary number of Electronic Measurements Units (EMU), a smart phone application to relay collected data, and a cloud Central Processing Platform (CPP) to perform analysis utilizing big data techniques and algorithms. Each EMU consists of an electric circuit that incorporates an ultra violet (UV) sensor, an air particles concentration sensor, a temperature sensor and a humidity sensor that all interface to a microcontroller. Bluetooth is employed for communication between the EMU and the smart phone application, while a 3G/4G cellular communications network furnishes the wireless connectivity to the remote CPP. When the collected data reaches the designated cloud server (CPP), it is immediately stored for subsequent analysis. Finally, big data statistical analysis (clustering and classification), mapping and plotting are performed to deduce correlations and to facilitate inferencing. Moreover, the scalability and low-cost of selected components of this realistic system makes it very feasible for large scale deployments in the context of smart cities initiatives, ad-hoc designs, or educational projects.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {82–86},
numpages = {5},
keywords = {environment, air particles, temperature sensor, smart phone, telemetry, Big data, UV index},
location = {Casablanca, Morocco},
series = {ICSIM '18}
}

@inproceedings{10.1145/3447548.3470819,
author = {Barajas, Joel and Bhamidipati, Narayan and Shanahan, James G.},
title = {Online Advertising Incrementality Testing And Experimentation: Industry Practical Lessons},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470819},
doi = {10.1145/3447548.3470819},
abstract = {Online advertising has historically been approached as user targeting and ad-to-user matching problems within sophisticated optimization algorithms. As the research area and ad tech industry have progressed over the last couple of decades, advertisers have increasingly emphasized the causal effect estimation of their ads (aka incrementality) using controlled experiments (or A/B testing). Even though observational approaches have been derived in marketing science since the 80s including media mix models, the availability of online advertising personalization has enabled the deployment of more rigorous randomized controlled experiments with millions of individuals. These evolutions in marketing science, online advertising, and the ad tech industry have posed incredible challenges for engineers, data scientists, and marketers alike. With low effect percentage differences (or lift) and often sparse conversion rates, the development of incrementality testing platforms at scale suggests tremendous engineering challenges in the measurement precision and detailed implementation. Similarly, the correct interpretation of results addressing a business goal within the marketing science domain requires significant data science and experimentation research expertise. All these challenges on the ongoing evolution of the online advertising industry and the heterogeneity of its sources (social, paid search, native, programmatic, etc). In the current tutorial, we propose a practical, grounded view in the incrementality testing landscape, including: The business need Solutions in the literature Design and choices in the development of incrementality testing platform The testing cycle, case studies, and recommendations to effective results delivery Incrementality testing evolution in the industry We will provide first-hand lessons on developing and operationalizing such a platform in a major combined DSP and ad network; these are based on running tens of experiments for up to two months each over the last couple of years.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {4027–4028},
numpages = {2},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3329379,
author = {Cruz-Filipe, Lu\'{\i}s and Di Nitto, Elisabetta and Mauro, Jacopo},
title = {Session Details: Theme: Distributed Systems: MiDOS - Microservices, DevOps, and Service-Oriented Architecture Track},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329379},
doi = {10.1145/3329379},
abstract = {Service-oriented architectures have changed our vision of the Web, bringing a paradigmatic shift in the methodologies when designing and implementing distributed systems. Originally, the Web was mainly seen as a means of presenting information to a wide spectrum of people, but service-oriented programming triggered a radical transformation of the Web towards a computational fabric where loosely coupled services interact, can be discovered and then invoked. More recently, the microservices architectural style has been proposed, where applications are developed as a collection of fine-grained services running as independent processes. Distributed applications can then be constructed from independently deployable services taking advantage of the properties of the microservice architecture (e.g., flexibility, maintainability, reusability, compositionality, and scalability) as well as the elasticity of cloud infrastructure. From the practical point of view, the deployment and maintenance of (micro)services architectures are performed using DevOps, i.e., a collection of practices linking software development (Dev) with software operations (Ops). DevOps strongly advocates for automation and monitoring at all steps of software construction, from integration, testing, releasing to deployment and infrastructure management. By using the DevOps methodology, it is possible to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@proceedings{10.1145/3229762,
title = {ReQuEST '18: Proceedings of the 1st on Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning},
year = {2018},
isbn = {9781450359238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Artificial Intelligence (AI), Machine Learning (ML) and other emerging workloads demand efficient computer systems from the cloud to the edge. Systems designers, however, face numerous challenges from tackling the ever-growing space of design and optimization choices (including algorithms, models, software frameworks, libraries, hardware platforms, optimization techniques) to balancing off multiple objectives (including accuracy, speed, throughput, power, size, price). Furthermore, the lack of a common experimental framework and methodology makes it even more challenging to keep up with and build upon the latest research advances.The Reproducibly Quality-Efficient Systems Tournaments () initiative is a community effort to develop a rigorous methodology, open platform and  for co-designing the efficient and reliable software/hardware stack for emerging workloads. ReQuEST invites a multidisciplinary community to collaborate on benchmarking and optimizing workloads across diverse platforms, models, data sets, libraries and tools, while gradually adopting best practice. The community effectively creates a "marketplace" for trading Pareto-efficient implementations (code and data) as portable, customizable and reusable   and . We envision that such a community-driven and decentralized marketplace will help accelerate adoption and technology transfer of novel AI/ML techniques similar to the open-source movement.Please see the front matter for the 1st ReQuEST tournament on Co-designing Pareto-efficient Deep Learning Inference at ASPLOS'18 to learn more about the shared workflows and validated results, as well as about our next steps for the ReQuEST initiative.},
location = {Williamsburg, VA, USA}
}

@inproceedings{10.1145/3427771.3427852,
author = {Ahmed, Shamim and Bons, Marc},
title = {Edge Computed NILM: A Phone-Based Implementation Using MobileNet Compressed by Tensorflow Lite},
year = {2020},
isbn = {9781450381918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427771.3427852},
doi = {10.1145/3427771.3427852},
abstract = {In the context of residential Non-intrusive load monitoring (NILM), the usual service deployment process consists of collecting data from a metering device to the cloud, run algorithms on the cloud and then display results in a Web front or in an App. This approach comes with two major problems: on the one hand, important resources are allocated to the cloud process (including maintenance) while selling the solution on a substantial subscription basis is still a challenge. On the other hand, end-users are more and more reluctant to see their personal data being uploaded. In order to propose an alternative, this research has focused on edge computed NILM, namely the possibility to run NILM algorithms on existing devices on the end-user side, such as a smart phone. A two-stage model development has been carried out to obtain good disaggregation accuracy with lower model size. In the first stage, an efficient deep learning algorithm (MobileNet) has been adopted to obtain an accurate and light weight model. In the second stage, TensorFlow Lite has been used to compress further, in order to reduce edge device memory usage and computing time. To deal with real-life diversity, we have built large and diverse training and testing sets based on a combination of HES, UKDALE and REFIT datasets. Disaggregation performance has been assessed for both models: before and after TensorFlow Lite compression. Comparative analysis has been performed to facilitate implementation choices.},
booktitle = {Proceedings of the 5th International Workshop on Non-Intrusive Load Monitoring},
pages = {44–48},
numpages = {5},
keywords = {TensorFlow Lite, Energy Disaggregation, Deep Learning, MobileNet, NILM, Edge Computing},
location = {Virtual Event, Japan},
series = {NILM'20}
}

@inproceedings{10.1145/3307650.3322273,
author = {Murali, Prakash and Linke, Norbert Matthias and Martonosi, Margaret and Abhari, Ali Javadi and Nguyen, Nhung Hong and Alderete, Cinthia Huerta},
title = {Full-Stack, Real-System Quantum Computer Studies: Architectural Comparisons and Design Insights},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322273},
doi = {10.1145/3307650.3322273},
abstract = {In recent years, Quantum Computing (QC) has progressed to the point where small working prototypes are available for use. Termed Noisy Intermediate-Scale Quantum (NISQ) computers, these prototypes are too small for large benchmarks or even for Quantum Error Correction (QEC), but they do have sufficient resources to run small benchmarks, particularly if compiled with optimizations to make use of scarce qubits and limited operation counts and coherence times. QC has not yet, however, settled on a particular preferred device implementation technology, and indeed different NISQ prototypes implement qubits with very different physical approaches and therefore widely-varying device and machine characteristics.Our work performs a full-stack, benchmark-driven hardware-software analysis of QC systems. We evaluate QC architectural possibilities, software-visible gates, and software optimizations to tackle fundamental design questions about gate set choices, communication topology, the factors affecting benchmark performance and compiler optimizations. In order to answer key cross-technology and cross-platform design questions, our work has built the first top-to-bottom toolflow to target different qubit device technologies, including superconducting and trapped ion qubits which are the current QC front-runners. We use our toolflow, TriQ, to conduct real-system measurements on seven running QC prototypes from three different groups, IBM, Rigetti, and University of Maryland. Overall, we demonstrate that leveraging microarchitecture details in the compiler improves program success rate up to 28x on IBM (geomean 3x), 2.3x on Rigetti (geomean 1.45x), and 1.47x on UMDTI (geomean 1.17x), compared to vendor toolflows. In addition, from these real-system experiences at QC's hardware-software interface, we make observations and recommendations about native and software-visible gates for different QC technologies, as well as communication topologies, and the value of noise-aware compilation even on lower-noise platforms. This is the largest cross-platform real-system QC study performed thus far; its results have the potential to inform both QC device and compiler design going forward.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {527–540},
numpages = {14},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3365265.3365270,
author = {Ellwein, Carsten and Schmidt, Alexander and Lechler, Armin and Riedel, Oliver},
title = {Distributed Manufacturing: A Vision about Shareconomy in the Manufacturing Industry},
year = {2019},
isbn = {9781450372886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365265.3365270},
doi = {10.1145/3365265.3365270},
abstract = {Four major trends in recent manufacturing technology have been identified and are introduced. Those trends are mass customization, shareconomy, digitalization and cloud manufacturing. The impact of those trends on manufacturing paradigms has been evaluated and three possible paradigms have been identified. Those manufacturing paradigms are separation of design and manufacturing, collaboration across company borders and on-site production. The separation of design and manufacturing does empower customers with regard to the product and does allow true mass customization where customers are included in the product description process. The collaboration across company borders does empower customers in regard of the process and lets them choose their contractual partner for every production step. The following on-site-production focuses on the throughput and thus on the delivery time by re-location of the production into the end-customers' daily field of action. Each paradigm is explained, the vision of possible future implementations is drawn and the possible benefits are outlined. However, the technical realization of those paradigms is yet not fully feasible due to still unsolved problems and challenges. Therefore, a research agenda has been composed to list and address those deficits. The main deficits that have been identified are the lack of standardized data models, an integrated and automated toolchain, the protection of intellectual property and the compliance with quality demands.},
booktitle = {Proceedings of the 2019 3rd International Conference on Automation, Control and Robots},
pages = {90–95},
numpages = {6},
keywords = {Distributed Manufacturing, Prosumption, Shareconomy, Manufacturing Access Point, Cloud Manufacturing, Mass Customization},
location = {Prague, Czech Republic},
series = {ICACR 2019}
}

@article{10.1145/3386361,
author = {Seeger, Jan and Br\"{o}ring, Arne and Carle, Georg},
title = {Optimally Self-Healing IoT Choreographies},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3386361},
doi = {10.1145/3386361},
abstract = {In the industrial Internet of Things domain, applications are moving from the Cloud into the Edge, closer to the devices producing and consuming data. This means that applications move from the scalable and homogeneous Cloud environment into a potentially constrained heterogeneous Edge network. Making Edge applications reliable enough to fulfill Industry 4.0 use cases remains an open research challenge. Maintaining operation of an Edge system requires advanced management techniques to mitigate the failure of devices. This article tackles this challenge with a twofold approach: (1) a policy-enabled failure detector that enables adaptable failure detection and (2) an allocation component for the efficient selection of failure mitigation actions. The parameters and performance of the failure detection approach are evaluated, and the performance of an energy-efficient allocation technique is measured. Finally, a vision for a complete system and an example use case are presented.},
journal = {ACM Trans. Internet Technol.},
month = {jul},
articleno = {27},
numpages = {20},
keywords = {optimization, IOT, failure detection}
}

@inproceedings{10.1145/3208806.3208816,
author = {Discher, S\"{o}ren and Richter, Rico and D\"{o}llner, J\"{u}rgen},
title = {A Scalable WebGL-Based Approach for Visualizing Massive 3D Point Clouds Using Semantics-Dependent Rendering Techniques},
year = {2018},
isbn = {9781450358002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208806.3208816},
doi = {10.1145/3208806.3208816},
abstract = {3D point cloud technology facilitates the automated and highly detailed digital acquisition of real-world environments such as assets, sites, cities, and countries; the acquired 3D point clouds represent an essential category of geodata used in a variety of geoinformation applications and systems. In this paper, we present a web-based system for the interactive and collaborative exploration and inspection of arbitrary large 3D point clouds. Our approach is based on standard WebGL on the client side and is able to render 3D point clouds with billions of points. It uses spatial data structures and level-of-detail representations to manage the 3D point cloud data and to deploy out-of-core and web-based rendering concepts. By providing functionality for both, thin-client and thick-client applications, the system scales for client devices that are vastly different in computing capabilities. Different 3D point-based rendering techniques and post-processing effects are provided to enable task-specific and data-specific filtering and highlighting, e.g., based on per-point surface categories or temporal information. A set of interaction techniques allows users to collaboratively work with the data, e.g., by measuring distances and areas, by annotating, or by selecting and extracting data subsets. Additional value is provided by the system's ability to display additional, context-providing geodata alongside 3D point clouds and to integrate task-specific processing and analysis operations. We have evaluated the presented techniques and the prototype system with different data sets from aerial, mobile, and terrestrial acquisition campaigns with up to 120 billion points to show their practicality and feasibility.},
booktitle = {Proceedings of the 23rd International ACM Conference on 3D Web Technology},
articleno = {19},
numpages = {9},
keywords = {point-based rendering, 3D point clouds, web-based rendering},
location = {Pozna\'{n}, Poland},
series = {Web3D '18}
}

@inproceedings{10.1145/3132847.3132931,
author = {Rong, Yu and Cheng, Hong},
title = {Minimizing Dependence between Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132931},
doi = {10.1145/3132847.3132931},
abstract = {In recent years, modeling the relation between two graphs has received unprecedented attention from researchers due to its wide applications in many areas, such as social analysis and bioinformatics. The nature of relations between two graphs can be divided into two categories: the vertex relation and the link relation. Many studies focus on modeling the vertex relation between graphs and try to find the vertex correspondence between two graphs. However, the link relation between graphs has not been fully studied. Specifically, we model the cross-graph link relation as cross-graph dependence, which reflects the dependence of a vertex in one graph on a vertex in the other graph. A generic problem, called Graph Dependence Minimization (GDM), is defined as: given two graphs with cross-graph dependence, how to select a subset of vertexes from one graph and copy them to the other, so as to minimize the cross-graph dependence. Many real applications can benefit from the solution to GDM. Examples include reducing the cross-language links in online encyclopedias, optimizing the cross-platform communication cost between different cloud services, and so on. This problem is trivial if we can select as many vertexes as we want to copy. But what if we can only choose a limited number of vertexes to copy so as to make the two graphs as independent as possible? We formulate GDM with a budget constraint into a combinatorial optimization problem, which is proven to be NP-hard. We propose two algorithms to solve GDM. Firstly, we prove the submodularity of the objective function of GDM and adopt the size-constrained submodular minimization (SSM) algorithm to solve it. Since the SSM-based algorithm cannot scale to large graphs, we design a heuristic algorithm with a provable approximation guarantee. We prove that the error achieved by the heuristic algorithm is bounded by an additive factor which is proportional to the square of the given budget. Extensive experiments on both synthetic and real-world graphs show that the proposed algorithms consistently outperform the well-studied graph centrality measure based solutions. Furthermore, we conduct a case study on the Wikipedia graphs with millions of vertexes and links to demonstrate the potential of GDM to solve real-world problems.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1827–1836},
numpages = {10},
keywords = {submodular minimization, graph analytics, graph dependence minimization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@proceedings{10.1145/2899415,
title = {ITiCSE '16: Proceedings of the 2016 ACM Conference on Innovation and Technology in Computer Science Education},
year = {2016},
isbn = {9781450342315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ITiCSE 2016 in Arequipa.In Peru, Arequipa is called the "The independent Republic of Arequipa." Our city is a UNESCO World Heritage Site. Its historic heritage, natural scenery and cultural sites make the city a major tourist destination. Its religious, colonial, and republican architectural styles blend European and native characteristics into a unique style. During the last years, Arequipa has been recognized as the best place to study computing programs in Peru. It is probably one of the reasons we have ITiCSE this year here!The conference continues to be a truly international conference with 147 submissions from 35 countries on six continents (Africa - 5, Asia - 13, Europe - 36, North America - 55, Oceania - 18, and South America - 20); and that is only considering the first author. These submissions consisted of 134 papers, 3 panels, and 10 working group proposals. Additionally, there were 43 posters and the tips \&amp; techniques submissions spanning 13 countries.All research papers were double blind reviewed by at least three reviewers, though most papers received between four and six reviews. This year 38\% were selected for presentation and inclusion in the proceedings. The first authors of the papers are distributed over 22 different countries on six continents.All posters and tips \&amp; techniques submissions were also reviewed by members of the program committee. Twenty six Poster papers and nine Tips and Techniques papers were accepted, representing 12 countries.There are seven working groups including gender equity, game development, academic integrity, teaching specific issues and internationalization. Participating in a working group is probably one of the most efficient ways to become part of the ITiCSE community. It provides participants a unique opportunity to work with people from different countries who are interested and knowledgeable in the area of the working group.This conference will have two keynote speakers. Mehran Sahami from Stanford University, under the title "Statistical Modeling to Better Understand CS Students" will present some statistical models trying to explain specific behavior for CS students. Mats Daniels will talk about "Professional Competencies for Real? A Question about Identity!" will talk about how to meet competences for real problems.The conference dinner will be held at the Santa Catalina Monastery. It was built in 1579 and was enlarged in the 17th century. The over 20,000-square-meter monastery was built predominantly in the Mud\'{e}jar style, and is characterized by its vividly painted walls. There are approximately 20 nuns currently living in the northern corner of the complex; the rest of the monastery is open to the public. Attendees will also have the opportunity to enjoy one of two local excursions and one or two post conference tours, the first to the famous Colca Canyon and also the world famous Inca ruins of Machu Picchu.},
location = {Arequipa, Peru}
}

@inproceedings{10.1145/3361821.3361825,
author = {Podhoranyi, Michal and Vojacek, Lukas},
title = {Social Media Data Processing Infrastructure by Using Apache Spark Big Data Platform: Twitter Data Analysis},
year = {2019},
isbn = {9781450372411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361821.3361825},
doi = {10.1145/3361821.3361825},
abstract = {Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.},
booktitle = {Proceedings of the 2019 4th International Conference on Cloud Computing and Internet of Things},
pages = {1–6},
numpages = {6},
keywords = {Apache Spark, social network data, data processing architecture, Twitter},
location = {Tokyo, Japan},
series = {CCIOT '19}
}

@inproceedings{10.1109/UCC.2014.40,
author = {Chauvel, Franck and Song, Hui and Ferry, Nicolas and Fleurey, Franck},
title = {Robustness Indicators for Cloud-Based Systems Topologies},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.40},
doi = {10.1109/UCC.2014.40},
abstract = {Various services are now available in the Cloud, ranging from turnkey databases and application servers to high-level services such as continuous integration or source version control. To stand out of this diversity, robustness of service compositions is an important selling argument, but which remains difficult to understand and estimate as it does not only depend on services but also on the underlying platform and infrastructure. Yet, choosing a specific service composition may fail to deliver the expected robustness, but reverting early choices may jeopardise the success of any Cloud project. Inspired by existing models used in Biology to quantify the robustness of ecosystems, we show how to tailor them to obtain early indicators of robustness for cloud-based deployments. This technique helps identify weakest services in the overall architecture and in turn mitigates the risk of having to revert key architectural choices. We illustrate our approach by comparing the robustness of four alternative deployments of the Sens App application, which includes a Mongo DB database, four REST services and a graphical web-front end.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {307–316},
numpages = {10},
keywords = {failures sequences, extinction sequences, robustness indicators, bio-inspired, cloud topologies, deployment},
series = {UCC '14}
}

@inproceedings{10.1145/3117811.3131250,
author = {Hogan, Mary and Esposito, Flavio},
title = {Poster: A Portfolio Theory Approach to Edge Traffic Engineering via Bayesian Networks},
year = {2017},
isbn = {9781450349161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3117811.3131250},
doi = {10.1145/3117811.3131250},
abstract = {One of the main goals of mobile edge computing is to support new generation latency-sensitive networked applications. To manage such demanding applications, a fine-grained control of end-to-end paths is imperative. End-to-end delay estimation and forecast techniques were essential traffic engineering tools even before the mobile edge computing paradigm pushed the cloud closer to the end user. In this paper, we model the path selection problem for edge traffic engineering using a risk minimization technique inspired by portfolio theory in economics, and we use machine learning to estimate the risk of a path. In particular, using real latency time series measurements, collected with and without the GENI testbed, we compare four short-horizon latency estimation techniques, commonly used by the finance community to estimate prices of volatile financial instruments. Our initial results suggest that a Bayesian Network approach may lead to good latency estimation performance and open a few research questions that we are currently exploring.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking},
pages = {555–557},
numpages = {3},
keywords = {Bayesian network, latency prediction, portfolio theory, machine learning, edge computing},
location = {Snowbird, Utah, USA},
series = {MobiCom '17}
}

@article{10.1145/3319618,
author = {Belson, Bruce and Holdsworth, Jason and Xiang, Wei and Philippa, Bronson},
title = {A Survey of Asynchronous Programming Using Coroutines in the Internet of Things and Embedded Systems},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3319618},
doi = {10.1145/3319618},
abstract = {Many Internet of Things and embedded projects are event driven, and therefore require asynchronous and concurrent programming. Current proposals for C++20 suggest that coroutines will have native language support. It is timely to survey the current use of coroutines in embedded systems development. This article investigates existing research which uses or describes coroutines on resource-constrained platforms. The existing research is analysed with regard to: software platform, hardware platform, and capacity; use cases and intended benefits; and the application programming interface design used for coroutines. A systematic mapping study was performed, to select studies published between 2007 and 2018 which contained original research into the application of coroutines on resource-constrained platforms. An initial set of 566 candidate papers, collated from on-line databases, were reduced to only 35 after filters were applied, revealing the following taxonomy. The C 8 C++ programming languages were used by 22 studies out of 35. As regards hardware, 16 studies used 8- or 16-bit processors while 13 used 32-bit processors. The four most common use cases were concurrency (17 papers), network communication (15), sensor readings (9), and data flow (7). The leading intended benefits were code style and simplicity (12 papers), scheduling (9), and efficiency (8). A wide variety of techniques have been used to implement coroutines, including native macros, additional tool chain steps, new language features, and non-portable assembly language. We conclude that there is widespread demand for coroutines on resource-constrained devices. Our findings suggest that there is significant demand for a formalised, stable, well-supported implementation of coroutines in C++, designed with consideration of the special needs of resource-constrained devices, and further that such an implementation would bring benefits specific to such devices.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jun},
articleno = {21},
numpages = {21},
keywords = {asynchronous, scheduling, Embedded, direct style, resource-constrained}
}

@inproceedings{10.5555/3374138.3374140,
author = {Weyl, Julius and Lenfers, Ulfia A. and Clemen, Thomas and Glake, Daniel and Panse, Fabian and Ritter, Norbert},
title = {Large-Scale Traffic Simulation for Smart City Planning with Mars},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Understanding individual mobility in larger cities is an important success factor for future smart cities. Related simulation scenarios incorporate enormous numbers of agents, with the disadvantage of long run times. In order to provide large-scale and multimodal traffic simulations, we developed MARS V3. Adapting the Modeling and Simulation as a Service (MSaaS) paradigm, a seamless workflow can be provided to the modeling community. An integrated domain-specific language allows model descriptions without a technical overhead. For this study, selected parts of an individual-based traffic model of the City of Hamburg, Germany, were taken as an example. The entire workflow from model development, open data integration, simulation, and result analysis will be described and evaluated. Performance was measured for local and cloud-based simulation execution for up to one million agents. First results show that this concept can be utilized for building decision support systems for smart cities in the near future.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {2},
numpages = {12},
keywords = {domain-specific-language, MSaaS, individual mobility, agent-based, large-scale traffic scenario},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@proceedings{10.1145/2745844,
title = {SIGMETRICS '15: Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
year = {2015},
isbn = {9781450334860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGMETRICS 2015. SIGMETRICS is the flagship conference of the ACM special interest group for the computer systems performance evaluation community. This year marks the forty-second anniversary since SIGMETRICS (under its prior name, SIGME) held the First National SIGME Symposium on Measurement and Evaluation in 1973. The past four decades have seen enormous changes in the field of computer science, but the importance of measurement, modeling, and performance evaluation remains as critical as ever.This year's conference includes papers on topics that have been a mainstay since the founding of our SIG, including queuing, scheduling, resource allocation, and performance measurement. Application areas that have emerged in recent years, such as data center and cloud, big data, solidstate storage, machine learning, crowdsourcing, energy optimization, continue to be represented in our program. We received 239 submissions to this year's conference, of which 32 appear in the program as full papers, which is a highly competitive acceptance ratio below 14\%. An additional 24 submissions appear in the abbreviated form of poster presentations and extended abstracts.As in some prior years, we performed reviews in three rounds. In the first round, each paper was assigned to three reviewers. In the second round, papers were up for discussion among the reviewers, especially those with highly divergent review opinions. In some cases, additional reviews were necessary to ensure that every paper would receive at least three reviews. Based on this and immediately prior to the TPC meeting, papers were bucketed into "accept", "reject", and "discuss at TPC meeting" buckets. Finally, in the TPC meeting at MIT (Cambridge, MA), we entertained comments and objections on papers in the "accept" and "reject" buckets and discuss all papers in the "discuss at TPC meeting" bucket.It was no mean feat to winnow a set of nearly two hundred and fifty submissions down to a set of appropriate size for a three-day conference. We offer tremendous thanks to the 57 members of the program committee who collectively performed this daunting task. We are grateful for the support of the SIGMETRICS board during the lengthy process of selecting this year's conference program.},
location = {Portland, Oregon, USA}
}

@article{10.1145/3403954,
author = {Sharma, Pratima and Jindal, Rajni and Borah, Malaya Dutta},
title = {Blockchain Technology for Cloud Storage: A Systematic Literature Review},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3403954},
doi = {10.1145/3403954},
abstract = {The demand for Blockchain innovation and the significance of its application has inspired ever-progressing exploration in various scientific and practical areas. Even though it is still in the initial testing stage, the blockchain is being viewed as a progressive solution to address present-day technology concerns, such as decentralization, identity, trust, character, ownership of data, and information-driven choices. Simultaneously, the world is facing an increase in the diversity and quantity of digital information produced by machines and users. While effectively looking for the ideal approach to storing and processing cloud data, the blockchain innovation provides significant inputs. This article reviews the application of blockchain technology for securing cloud storage.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {89},
numpages = {32},
keywords = {decentralization, cloud storage, Blockchain technology, cloud security, cloud computing}
}

@inproceedings{10.1145/2775292.2775312,
author = {Scully, Timothy and Dobo\v{s}, Jozef and Sturm, Timo and Jung, Yvonne},
title = {3drepo.Io: Building the next Generation Web3D Repository with AngularJS and X3DOM},
year = {2015},
isbn = {9781450336475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2775292.2775312},
doi = {10.1145/2775292.2775312},
abstract = {This paper presents a novel open source web-based 3D version control system positioned directly within the context of the recent strategic plan for digitising the construction sector in the United Kingdom. The aim is to achieve reduction of cost and carbon emissions in the built environment by up to 20\% simply by properly managing digital information and 3D models. Even though previous works in the field concentrated mainly on defining novel WebGL frameworks and later on the efficiency of 3D data delivery over the Internet, there is still the emerging need for a practical solution that would provide ubiquitous access to 3D assets, whether it is for large international enterprises or individual members of the general public. We have, therefore, developed a novel platform leveraging the latest open web-based technologies such as AngularJS and X3DOM in order to define an industrial-strength collaborative cloud hosting service 3drepo.io. Firstly, we introduce the work and outline the high-level system architecture as well as improvements in relation to previous work. Next, we describe database and front-end considerations with emphasis on scalability and enhanced security. Finally, we present several performance measurement experiments and a selection of real-life industrial use cases. We conclude that jQuery provides performance benefits over AngularJS when manipulating large scene graphs in web browsers.},
booktitle = {Proceedings of the 20th International Conference on 3D Web Technology},
pages = {235–243},
numpages = {9},
keywords = {X3DOM, AngularJS, BIM, version control, 3D repo},
location = {Heraklion, Crete, Greece},
series = {Web3D '15}
}

@inproceedings{10.1145/2749246.2749252,
author = {Cheng, Yue and Iqbal, M. Safdar and Gupta, Aayush and Butt, Ali R.},
title = {CAST: Tiering Storage for Data Analytics in the Cloud},
year = {2015},
isbn = {9781450335508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749246.2749252},
doi = {10.1145/2749246.2749252},
abstract = {Enterprises are increasingly moving their big data analytics to the cloud with the goal of reducing costs without sacrificing application performance. Cloud service providers offer their tenants a myriad of storage options, which while flexible, makes the choice of storage deployment non trivial. Crafting deployment scenarios to leverage these choices in a cost-effective manner - under the unique pricing models and multi-tenancy dynamics of the cloud environment - presents unique challenges in designing cloud-based data analytics frameworks.In this paper, we propose CAST, a Cloud Analytics Storage Tiering solution that cloud tenants can use to reduce monetary cost and improve performance of analytics workloads. The approach takes the first step towards providing storage tiering support for data analytics in the cloud. CAST performs offline workload profiling to construct job performance prediction models on different cloud storage services, and combines these models with workload specifications and high-level tenant goals to generate a cost-effective data placement and storage provisioning plan. Furthermore, we build CAST++ to enhance CAST's optimization model by incorporating data reuse patterns and across-jobs interdependencies common in realistic analytics workloads. Tests with production workload traces from Facebook and a 400-core Google Cloud based Hadoop cluster demonstrate that CAST++ achieves 1.21X performance and reduces deployment costs by 51.4\% compared to local storage configuration.},
booktitle = {Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {45–56},
numpages = {12},
keywords = {big data analytics, cloud computing, mapreduce, storage tiering},
location = {Portland, Oregon, USA},
series = {HPDC '15}
}

@inproceedings{10.1145/3318216.3363300,
author = {Chen, Qi and Ma, Xu and Tang, Sihai and Guo, Jingda and Yang, Qing and Fu, Song},
title = {F-Cooper: Feature Based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363300},
doi = {10.1145/3318216.3363300},
abstract = {Autonomous vehicles are heavily reliant upon their sensors to perfect the perception of surrounding environments, however, with the current state of technology, the data which a vehicle uses is confined to that from its own sensors. Data sharing between vehicles and/or edge servers is limited by the available network bandwidth and the stringent real-time constraints of autonomous driving applications. To address these issues, we propose a point cloud feature based cooperative perception framework (F-Cooper) for connected autonomous vehicles to achieve a better object detection precision. Not only will feature based data be sufficient for the training process, we also use the features' intrinsically small size to achieve real-time edge computing, without running the risk of congesting the network. Our experiment results show that by fusing features, we are able to achieve a better object detection result, around 10\% improvement for detection within 20 meters and 30\% for further distances, as well as achieve faster edge computing with a low communication delay, requiring 71 milliseconds in certain feature selections. To the best of our knowledge, we are the first to introduce feature-level data fusion to connected autonomous vehicles for the purpose of enhancing object detection and making real-time edge computing on inter-vehicle data feasible for autonomous vehicles.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {88–100},
numpages = {13},
keywords = {feature fusion, edge computing, connected autonomous vehicle},
location = {Arlington, Virginia},
series = {SEC '19}
}

@inproceedings{10.1145/3339825.3391862,
author = {Hansen, Henry Haugsten and Muchallil, Sayed and Griwodz, Carsten and Sillerud, Vetle and Johanssen, Fredrik},
title = {Dense LIDAR Point Clouds from Room-Scale Scans},
year = {2020},
isbn = {9781450368452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339825.3391862},
doi = {10.1145/3339825.3391862},
abstract = {LiDARs can capture distances with high accuracy and should be very useful to create point clouds that provide highly detailed representations of an environment. If these reconstructions are meant as baseline or ground truth for other algorithms, they must have a high density and accuracy.Currently available LiDARs do still face some limitations. Either they have a limited range, or they have a rather limited resolution in one or more dimensions. As a consequence, all of them have to undergo motion to capture a larger environment. While some systems follow extremely well-predictable motion paths such as satellite trajectories or robotic arms, others require more spontaneous and flexible motion. These systems use either visual simultaneous localization and mapping (vSLAM), GPS or IMU to achieve this, but they are generally designed in such a way that human intervention is required during the creation of high-quality point clouds.In this paper, we make use of a rotating LiDAR with an attached IMU to create dense point clouds of room-scale environments with the base accuracy of the LiDAR by compensating for the various inaccuracies that are introduced by the LiDAR's motion. The resulting dense scans are suitable as ground truths for other techniques because we retain the error distribution of the LiDAR itself through the densification.In contrast to other works, we do not aim at a visually pleasing or easily meshable result and we can therefore avoid potentially inaccurate assumptions about the flatness of surfaces. We take a two-step approach. First, we densify from a stationary position changing only the LiDAR's pitch. Second, we add free motion to expose obstructed views. We show that motion paths determined by repeated Iterative Closest Point (ICP) as well as image matching on height maps can be used to create feasible priors for densification using ICP.},
booktitle = {Proceedings of the 11th ACM Multimedia Systems Conference},
pages = {88–98},
numpages = {11},
keywords = {machine learning, point clouds, densification, height maps},
location = {Istanbul, Turkey},
series = {MMSys '20}
}

@inproceedings{10.1145/2726935.2726938,
author = {Rewari, Gaurav and Kapoor, Rahul},
title = {Analytics Applications on the Cloud: Business Potential, Solution Requirements, and Research Opportunities (Invited Talk)},
year = {2015},
isbn = {9781450334051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2726935.2726938},
doi = {10.1145/2726935.2726938},
abstract = {The rapid adoption of cloud applications like SalesForce, ServiceNow, NetSuite, Marketo etc. has opened up an interesting opportunity for vendors of analytical applications and BI middleware as these modern cloud data sources are not well served by existing BI Tools and applications. For customers, part or all of their data moving to the cloud also means any existing on-premise warehouses and analytical applications are partially or fully defunct, and with the increased acceptance of moving application level functionality to the cloud there is little interest in upgrading the defunct on-premise offerings. A new class of software vendors, including Numerify, step into this gap by providing a cloud based, end-to-end solution for data extraction, transformation and warehouse based analytical applications, directly to the business user in select domains (e.g. IT Service Management, Human Resources and Financials). This talk summarizes the overall business potential for analytics on the cloud, expands on “analytical applications” with some examples, discusses engineering challenges in implementing cloud based analytics solutions highlighting some advances in ETL techniques, and points to potential research opportunities in this space.},
booktitle = {Proceedings of the 2nd Workshop on Parallel Programming for Analytics Applications},
pages = {3},
numpages = {1},
keywords = {MDM, Warehousing, Data Integration, Analytical Applications, Data Quality, Business Intelligence, ETL, Cloud Software},
location = {San Francisco, CA, USA},
series = {PPAA 2015}
}

@article{10.1145/2903146,
author = {Mencagli, Gabriele},
title = {A Game-Theoretic Approach for Elastic Distributed Data Stream Processing},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/2903146},
doi = {10.1145/2903146},
abstract = {Distributed data stream processing applications are structured as graphs of interconnected modules able to ingest high-speed data and to transform them in order to generate results of interest. Elasticity is one of the most appealing features of stream processing applications. It makes it possible to scale up/down the allocated computing resources on demand in response to fluctuations of the workload. On clouds, this represents a necessary feature to keep the operating cost at affordable levels while accommodating user-defined QoS requirements. In this article, we study this problem from a game-theoretic perspective. The control logic driving elasticity is distributed among local control agents capable of choosing the right amount of resources to use by each module. In a first step, we model the problem as a noncooperative game in which agents pursue their self-interest. We identify the Nash equilibria and we design a distributed procedure to reach the best equilibrium in the Pareto sense. As a second step, we extend the noncooperative formulation with a decentralized incentive-based mechanism in order to promote cooperation by moving the agreement point closer to the system optimum. Simulations confirm the results of our theoretical analysis and the quality of our strategies.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {jun},
articleno = {13},
numpages = {34},
keywords = {game theory, data stream processing, elasticity, Autonomic computing}
}

@proceedings{10.1145/3307681,
title = {HPDC '19: Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
year = {2019},
isbn = {9781450366700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the International Symposium on High-Performance Parallel and Distributed Computing (ACM HPDC 2019)!, the premier conference at the intersection of high performance and distributed computing, now in our 27th year. HPDC is fortunate to be co-located and sponsorsed by ACM FCRC this year which is providing exceptional keynotes, the Turing Award Lecture, and a chance to mingle with our colleagues in other disciplines of Computer Science. ACM HPDC has a focus on high-performance parallel and distributed computing topics over the years including platforms spanning clouds, clusters, grids, big data, massively multicore, and extreme-scale computing systems. One of the unique features of HPDC is that it welcomes a blend of ideas ranging from applied research in the form of experience papers on operational deployments and applications, and more fundamental research in parallel and distributed techniques and systems. The conference has always appreciated the heroic work taken to deploy real systems and applications and the insights gained by live measurement and experimentation. The HPDC 2019 program is no exception with topics ranging from hybrid systems, scalable graph processing, GPU applications, and cloud systems, to name a few. In addition to a strong technical program, the HPDC 2019 achievement award was given to Professor Geoffrey Fox, Indiana University for his foundational contributions to parallel computing, high-performance software, the interface between applications and systems, contributions to education, and outreach to underrepresented communities. This year the conference will include five exciting workshops on cutting edge topics including perenial favorites ScienceCloud and ROSS. The program will also include a PhD Forum event for our future budding stars in topics relating to HPDC.Conferences like HPDC require a large committment by the community. First, I would like to acknowledge and thank our sponsors ACM SIGARCH and the University of Arizona, and supporters, IBM, DOE, and the NSF. Next, I would like to thank the various organizing committees that ensure that HPDC remains a top-tier conference at the intersection of high performance and distributed computing. My deepest thanks go to Evgenia Smirni and Ali Butt for their leadership as co-chairs of the Program Committee helping to bring a strong and exciting technical program which is a cornerstone of HPDC. I would also like to acknowledge the HPDC Awards Committee led by Douglas Thain in their selection of Geoffrey Fox for the 2019 HPDC Achievement Award.Additional thanks go to Alex Iosup for chairing the Workshop committee which has had a long tradition at HPDC bringing cutting-edge topics that complement the main program. Thanks also to Jay F Lofstead II for leading the effort as Poster Chair and the mentoring session as part of the Ph.D. Forum. I would also like to acknowledge Antonino Tumeo for his work as our Publications Chair, an often thankless job. Conferences cannot succeed without the tireless work of publicity and I would like to thank Ioan Raicu, Torsten Hoefler, Shuaiwen Leon Song, Kenjiro Taura, for spreading the word. Lastly, I express gratitude to Zachary Leidall, our Web chair, for keeping the HPDC 2019 Web site informative and up to date. Finally, let me thank the entire HPDC community including the steering committee, organizing committee participants, volunteers, session chairs, attendees, and co-authors, for making HPDC 2019 a great success.Jon Weissman, University of Minnesota—HPDC 2019 General Chair, Steering Committee Chair},
location = {Phoenix, AZ, USA}
}

@article{10.1145/3414685.3417812,
author = {Jones, R. Kenny and Barton, Theresa and Xu, Xianghao and Wang, Kai and Jiang, Ellen and Guerrero, Paul and Mitra, Niloy J. and Ritchie, Daniel},
title = {ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417812},
doi = {10.1145/3414685.3417812},
abstract = {Manually authoring 3D shapes is difficult and time consuming; generative models of 3D shapes offer compelling alternatives. Procedural representations are one such possibility: they offer high-quality and editable results but are difficult to author and often produce outputs with limited diversity. On the other extreme are deep generative models: given enough data, they can learn to generate any class of shape but their outputs have artifacts and the representation is not editable.In this paper, we take a step towards achieving the best of both worlds for novel 3D shape synthesis. First, we propose ShapeAssembly, a domain-specific "assembly-language" for 3D shape structures. ShapeAssembly programs construct shape structures by declaring cuboid part proxies and attaching them to one another, in a hierarchical and symmetrical fashion. ShapeAssembly functions are parameterized with continuous free variables, so that one program structure is able to capture a family of related shapes.We show how to extract ShapeAssembly programs from existing shape structures in the PartNet dataset. Then, we train a deep generative model, a hierarchical sequence VAE, that learns to write novel ShapeAssembly programs. Our approach leverages the strengths of each representation: the program captures the subset of shape variability that is interpretable and editable, and the deep generative model captures variability and correlations across shape collections that is hard to express procedurally.We evaluate our approach by comparing the shapes output by our generated programs to those from other recent shape structure synthesis models. We find that our generated shapes are more plausible and physically-valid than those of other methods. Additionally, we assess the latent spaces of these models, and find that ours is better structured and produces smoother interpolations. As an application, we use our generative model and differentiable program interpreter to infer and fit shape programs to unstructured geometry, such as point clouds.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {234},
numpages = {20},
keywords = {neurosymbolic models, procedural modeling, deep learning, shape synthesis, shape analysis, generative models}
}

@article{10.1145/3369818,
author = {Qin, Xin and Chen, Yiqiang and Wang, Jindong and Yu, Chaohui},
title = {Cross-Dataset Activity Recognition via Adaptive Spatial-Temporal Transfer Learning},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3369818},
doi = {10.1145/3369818},
abstract = {Human activity recognition (HAR) aims at recognizing activities by training models on the large quantity of sensor data. Since it is time-consuming and expensive to acquire abundant labeled data, transfer learning becomes necessary for HAR by transferring knowledge from existing domains. However, there are two challenges existing in cross-dataset activity recognition. The first challenge is source domain selection. Given a target task and several available source domains, it is difficult to determine how to select the most similar source domain to the target domain such that negative transfer can be avoided. The second one is accurately activity transfer. After source domain selection, how to achieve accurate knowledge transfer between the selected source and the target domain remains another challenge. In this paper, we propose an Adaptive Spatial-Temporal Transfer Learning (ASTTL) approach to tackle both of the above two challenges in cross-dataset HAR. ASTTL learns the spatial features in transfer learning by adaptively evaluating the relative importance between the marginal and conditional probability distributions. Besides, it captures the temporal features via incremental manifold learning. Therefore, ASTTL can learn the adaptive spatial-temporal features for cross-dataset HAR and can be used for both source domain selection and accurate activity transfer. We evaluate the performance of ASTTL through extensive experiments on 4 public HAR datasets, which demonstrates its effectiveness. Furthermore, based on ASTTL, we design and implement an adaptive cross-dataset HAR system called Client-Cloud Collaborative Adaptive Activity Recognition System (3C2ARS) to perform HAR in the real environment. By collecting activities in the smartphone and transferring knowledge in the cloud server, ASTTL can significantly improve the performance of source domain selection and accurate activity transfer.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {148},
numpages = {25},
keywords = {Domain Adaptation, Transfer Learning, Human Activity Recognition, Cross-Dataset Recognition}
}

@inproceedings{10.1145/2665970.2665977,
author = {Jung, Seunghwan and Lee, Sejoon and Kim, Sangwoo and Nam, Hojung},
title = {Identification of Genomic Features in the Classification of Loss- and Gain-of-Function Mutation: [Extended Abstract]},
year = {2014},
isbn = {9781450312752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2665970.2665977},
doi = {10.1145/2665970.2665977},
abstract = {In this work, we propose a comprehensive analysis of the genomic features of the human in mutations to classify loss-of-function (LoF) and gain-of-function (GoF) mutations. Through these genetic mutations, a protein can lose its native function, or it can confer a new function. However, when a mutation occurs, it is difficult to determine whether it will result in a LoF or a GoF. Therefore, we propose a study that analyzes the human genomic features of LoF and GoF instances to find features that can be used to classify LoF and GoF mutations. In order to collect experimentally verified LoF and GoF mutational information, we obtained 816 LoF mutations and 474 GoF mutations from a literature text-mining process. Next, with data-preprocessing steps, 258 LoF and 129 GoF mutations remained for a further analysis. We analyzed the properties of these LoF and GoF mutations. Among the properties, we selected features which show different tendencies between the two groups. Finally, we implemented classifications using support vector machine, random forest, and logistic regression methods to confirm whether or not these features can identify LoF and GoF mutations. By implementing classifications with the selected features, it is demonstrated that the selected features have good discriminative power.},
booktitle = {Proceedings of the ACM 8th International Workshop on Data and Text Mining in Bioinformatics},
pages = {23},
numpages = {1},
keywords = {gain-of-function, loss-of-function},
location = {Shanghai, China},
series = {DTMBIO '14}
}

@inproceedings{10.1145/3427921.3450256,
author = {Ogden, Samuel S. and Kong, Xiangnan and Guo, Tian},
title = {PieSlicer: Dynamically Improving Response Time for Cloud-Based CNN Inference},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450256},
doi = {10.1145/3427921.3450256},
abstract = {Executing deep-learning inference on cloud servers enables the usage of high complexity models for mobile devices with limited resources. However, pre-execution time-the time it takes to prepare and transfer data to the cloud-is variable and can take orders of magnitude longer to complete than inference execution itself. This pre-execution time can be reduced by dynamically deciding the order of two essential steps, preprocessing and data transfer, to better take advantage of on-device resources and network conditions. In this work, we present PieSlicer, a system for making dynamic preprocessing decisions to improve cloud inference performance using linear regression models. PieSlicer then leverages these models to select the appropriate preprocessing location. We show that for image classification applications PieSlicer reduces median and 99th percentile pre-execution time by up to 50.2ms and 217.2ms respectively when compared to static preprocessing methods.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {249–256},
numpages = {8},
keywords = {cloud inference, performance modeling, mobile deep learning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.14778/3484224.3484227,
author = {Mailis, Theofilos and Kotidis, Yannis and Christoforidis, Stamatis and Kharlamov, Evgeny and Ioannidis, Yannis},
title = {View Selection over Knowledge Graphs in Triple Stores},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484227},
doi = {10.14778/3484224.3484227},
abstract = {Knowledge Graphs (KGs) are collections of interconnected and annotated entities that have become powerful assets for data integration, search enhancement, and other industrial applications. Knowledge Graphs such as DBPEDIA may contain billion of triple relations and are intensively queried with millions of queries per day. A prominent approach to enhance query answering on Knowledge Graph databases is View Materialization, ie., the materialization of an appropriate set of computations that will improve query performance.We study the problem of view materialization and propose a view selection methodology for processing query workloads with more than a million queries. Our approach heavily relies on subgraph pattern mining techniques that allow to create efficient summarizations of massive query workloads while also identifying the candidate views for materialization. In the core of our work is the correspondence between the view selection problem to that of Maximizing a Nondecreasing Submodular Set Function Subject to a Knapsack Constraint. The latter leads to a tractable view-selection process for native triple stores that allows a (1 - e---1)-approximation of the optimal selection of views. Our experimental evaluation shows that all the steps of the view-selection process are completed in a few minutes, while the corresponding rewritings accelerate 67.68\% of the queries in the DBPEDIA query workload. Those queries are executed in 2.19\% of their initial time on average.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3281–3294},
numpages = {14}
}

@article{10.1162/EVCO_a_00176,
author = {Garza-Fabre, Mario and Kandathil, Shaun M. and Handl, Julia and Knowles, Joshua and Lovell, Simon C.},
title = {Generating, Maintaining, and Exploiting Diversity in a Memetic Algorithm for Protein Structure Prediction},
year = {2016},
issue_date = {Winter 2016},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {24},
number = {4},
issn = {1063-6560},
url = {https://doi.org/10.1162/EVCO_a_00176},
doi = {10.1162/EVCO_a_00176},
abstract = {Computational approaches to de novo protein tertiary structure prediction, including those based on the preeminent "fragment-assembly" technique, have failed to scale up fully to larger proteins on the order of 100 residues and above. A number of limiting factors are thought to contribute to the scaling problem over and above the simple combinatorial explosion, but the key ones relate to the lack of exploration of properly diverse protein folds, and to an acute form of "deception" in the energy function, whereby low-energy conformations do not reliably equate with native structures. In this article, solutions to both of these problems are investigated through a multistage memetic algorithm incorporating the successful Rosetta method as a local search routine. We found that specialised genetic operators significantly add to structural diversity and that this translates well to reaching low energies. The use of a generalised stochastic ranking procedure for selection enables the memetic algorithm to handle and traverse deep energy wells that can be considered deceptive, which further adds to the ability of the algorithm to obtain a much-improved diversity of folds. The results should translate to a tangible improvement in the performance of protein structure prediction algorithms in blind experiments such as CASP, and potentially to a further step towards the more challenging problem of predicting the three-dimensional shape of large proteins.},
journal = {Evol. Comput.},
month = {dec},
pages = {577–607},
numpages = {31},
keywords = {Memetic algorithms., Protein structure prediction, Fragment assembly}
}

@inproceedings{10.1145/3311790.3400853,
author = {Choi, In Kwon and Abeysinghe, Eroma and Coulter, Eric and Marru, Suresh and Pierce, Marlon and Liu, Xiaowen},
title = {TopPIC Gateway: A Web Gateway for Top-Down Mass Spectrometry Data Interpretation},
year = {2020},
isbn = {9781450366892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311790.3400853},
doi = {10.1145/3311790.3400853},
abstract = {Top-down mass spectrometry-based proteomics has become the method of choice for identifying and quantifying intact proteoforms in biological samples. We present a web-based gateway for TopPIC suite, a widely used software suite consisting of four software tools for top-down mass spectrometry data interpretation: TopFD, TopPIC, TopMG, and TopDiff. The gateway enables the community to use heterogeneous collection of computing resources that includes high performance computing clusters at Indiana University and virtual clusters on XSEDE’s Jetstream Cloud resource for top-down mass spectral data analysis using TopPIC suite. The gateway will be a useful resource for proteomics researchers and students who have limited access to high-performance computing resources or who are not familiar with interacting with server-side supercomputers.},
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {461–464},
numpages = {4},
keywords = {Proteomics, Apache Airavata, SciGaP, XSEDE, Top-down mass spectrometry, Science Gateways},
location = {Portland, OR, USA},
series = {PEARC '20}
}

@inproceedings{10.1145/2950290.2983930,
author = {Gulzar, Muhammad Ali and Interlandi, Matteo and Condie, Tyson and Kim, Miryung},
title = {BigDebug: Interactive Debugger for Big Data Analytics in Apache Spark},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983930},
doi = {10.1145/2950290.2983930},
abstract = {To process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. In terms of debugging, DISC systems support post-mortem log analysis but do not provide interactive debugging features in realtime. This tool demonstration paper showcases a set of concrete usecases on how BigDebug can help debug Big Data Applications by providing interactive, realtime debug primitives. To emulate interactive step-wise debugging without reducing throughput, BigDebug provides simulated breakpoints to enable a user to inspect a program without actually pausing the entire computation. To minimize unnecessary communication and data transfer, BigDebug provides on-demand watchpoints that enable a user to retrieve intermediate data using a guard and transfer the selected data on demand. To support systematic and efficient trial-and-error debugging, BigDebug also enables users to change program logic in response to an error at runtime and replay the execution from that step. BigDebug is available for download at http://web.cs.ucla.edu/~miryung/software.html},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {1033–1037},
numpages = {5},
keywords = {interactive tools, fault localization and recovery, big data analytics, data-intensive scalable computing (DISC), Debugging},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3378679.3394534,
author = {Cooke, Ryan A. and Fahmy, Suhaib A.},
title = {Quantifying the Latency Benefits of Near-Edge and in-Network FPGA Acceleration},
year = {2020},
isbn = {9781450371322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378679.3394534},
doi = {10.1145/3378679.3394534},
abstract = {Transmitting data to cloud datacenters in distributed IoT applications introduces significant communication latency, but is often the only feasible solution when source nodes are computationally limited. To address latency concerns, Cloudlets, in-network computing, and more capable edge nodes are all being explored as a way of moving processing capability towards the edge of the network. Hardware acceleration using Field programmable gate arrays (FPGAs) is also seeing increased interest due to reduced computation time and improved efficiency. This paper evaluates the the implications of these offloading approaches using a case study neural network based image classification application, quantifying both the computation and communication latency resulting from different platform choices. We demonstrate that emerging in-network accelerator approaches offer much improved and predictable performance as well as better scaling to support multiple data sources.},
booktitle = {Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking},
pages = {7–12},
numpages = {6},
keywords = {edge computing, hardware acceleration},
location = {Heraklion, Greece},
series = {EdgeSys '20}
}

@article{10.14778/3476249.3476284,
author = {Li, Side and Kumar, Arun},
title = {Towards an Optimized GROUP by Abstraction for Large-Scale Machine Learning},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476284},
doi = {10.14778/3476249.3476284},
abstract = {Many applications that use large-scale machine learning (ML) increasingly prefer different models for subgroups (e.g., countries) to improve accuracy, fairness, or other desiderata. We call this emerging popular practice learning over groups, analogizing to GROUP BY in SQL, albeit for ML training instead of SQL aggregates. From the systems standpoint, this practice compounds the already data-intensive workload of ML model selection (e.g., hyperparameter tuning). Often, thousands of models may need to be trained, necessitating high-throughput parallel execution. Alas, most ML systems today focus on training one model at a time or at best, parallelizing hyperparameter tuning. This status quo leads to resource wastage, low throughput, and high runtimes. In this work, we take the first step towards enabling and optimizing learning over groups from the data systems standpoint for three popular classes of ML: linear models, neural networks, and gradient-boosted decision trees. Analytically and empirically, we compare standard approaches to execute this workload today: task-parallelism and data-parallelism. We find neither is universally dominant. We put forth a novel hybrid approach we call grouped learning that avoids redundancy in communications and I/O using a novel form of parallel gradient descent we call Gradient Accumulation Parallelism (GAP). We prototype our ideas into a system we call Kingpin built on top of existing ML tools and the flexible massively-parallel runtime Ray. An extensive empirical evaluation on large ML benchmark datasets shows that Kingpin matches or is 4x to 14x faster than state-of-the-art ML systems, including Ray's native execution and PyTorch DDP.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2327–2340},
numpages = {14}
}

