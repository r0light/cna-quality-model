@article{10.1145/3529162,
author = {Duboc, Leticia and Bahsoon, Rami and Alrebeish, Faisal and Mera-G\'{o}mez, Carlos and Nallur, Vivek and Kazman, Rick and Bianco, Philip and Babar, Ali and Buyya, Rajkumar},
title = {Systematic Scalability Modeling of QoS-Aware Dynamic Service Composition},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3–4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3529162},
doi = {10.1145/3529162},
abstract = {In Dynamic Service Composition (DSC), an application can be dynamically composed using web services to achieve its functional and Quality of Services (QoS) goals. DSC is a relatively mature area of research that crosscuts autonomous and services computing. Complex autonomous and self-adaptive computing paradigms (e.g., multi-tenant cloud services, mobile/smart services, services discovery and composition in intelligent environments such as smart cities) have been leveraging DSC to dynamically and adaptively maintain the desired QoS, cost and to stabilize long-lived software systems. While DSC is fundamentally known to be an NP-hard problem, systematic attempts to analyze its scalability have been limited, if not absent, though such analysis is of a paramount importance for their effective, efficient, and stable operations.This article reports on a new application of goal-modeling, providing a systematic technique that can support DSC designers and architects in identifying DSC-relevant characteristics and metrics that can potentially affect the scalability goals of a system. The article then applies the technique to two different approaches for QoS-aware dynamic services composition, where the article describes two detailed exemplars that exemplify its application. The exemplars hope to provide researchers and practitioners with guidance and transferable knowledge in situations where the scalability analysis may not be straightforward. The contributions provide architects and designers for QoS-aware dynamic service composition with the fundamentals for assessing the scalability of their own solutions, along with goal models and a list of application domain characteristics and metrics that might be relevant to other solutions. Our experience has shown that the technique was able to identify in both exemplars application domain characteristics and metrics that had been overlooked in previous scalability analyses of these DSC, some of which indeed limited their scalability. It has also shown that the experiences and knowledge can be transferable: The first exemplar was used as an example to inform and ease the work of applying the technique in the second one, reducing the time to create the model, even for a non-expert.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {nov},
articleno = {10},
numpages = {39},
keywords = {Scalability modelling, autonomous and adaptive systems, dynamic service composition}
}

@inproceedings{10.1145/3468737.3494094,
author = {Wallis, Kevin and Reich, Christoph and Varghese, Blesson and Schindelhauer, Christian},
title = {QUDOS: Quorum-Based Cloud-Edge Distributed DNNs for Security Enhanced Industry 4.0},
year = {2021},
isbn = {9781450385640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468737.3494094},
doi = {10.1145/3468737.3494094},
abstract = {Distributed machine learning algorithms that employ Deep Neural Networks (DNNs) are widely used in Industry 4.0 applications, such as smart manufacturing. The layers of a DNN can be mapped onto different nodes located in the cloud, edge and shop floor for preserving privacy. The quality of the data that is fed into and processed through the DNN is of utmost importance for critical tasks, such as inspection and quality control. Distributed Data Validation Networks (DDVNs) are used to validate the quality of the data. However, they are prone to single points of failure when an attack occurs. This paper proposes QUDOS, an approach that enhances the security of a distributed DNN that is supported by DDVNs using quorums. The proposed approach allows individual nodes that are corrupted due to an attack to be detected or excluded when the DNN produces an output. Metrics such as corruption factor and success probability of an attack are considered for evaluating the security aspects of DNNs. A simulation study demonstrates that if the number of corrupted nodes is less than a given threshold for decision-making in a quorum, the QUDOS approach always prevents attacks. Furthermore, the study shows that increasing the size of the quorum has a better impact on security than increasing the number of layers. One merit of QUDOS is that it enhances the security of DNNs without requiring any modifications to the algorithm and can therefore be applied to other classes of problems.},
booktitle = {Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing},
articleno = {5},
numpages = {10},
keywords = {industry 4.0, cloud-edge computing, distributed DNN, distributed data validation network, edge security},
location = {Leicester, United Kingdom},
series = {UCC '21}
}

@inproceedings{10.1145/3517745.3561464,
author = {Xu, Xiaokun and Claypool, Mark},
title = {Measurement of Cloud-Based Game Streaming System Response to Competing TCP Cubic or TCP BBR Flows},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561464},
doi = {10.1145/3517745.3561464},
abstract = {Cloud-based game streaming is emerging as a convenient way to play games when clients have a good network connection. However, high-quality game streams need high bitrates and low latencies, a challenge when competing for network capacity with other flows. While some network aspects of cloud-based game streaming have been studied, missing are comparative performance and congestion responses to competing TCP flows. This paper presents results from experiments that measure how three popular commercial cloud-based game streaming systems - Google Stadia, NVidia GeForce Now, and Amazon Luna - respond and then recover to TCP Cubic and TCP BBR flows on a congested network link. Analysis of bitrates, loss rates and round-trip times show the three systems have markedly different responses to the arrival and departure of competing network traffic.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {305–316},
numpages = {12},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3484274.3484307,
author = {Fang, Yuan and Fan, Lei},
title = {Comparisons of Eight Simplification Methods for Data Reduction of Terrain Point Cloud},
year = {2021},
isbn = {9781450390477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3484274.3484307},
doi = {10.1145/3484274.3484307},
abstract = {In recent years, the applications of 3D point cloud data representing terrain surfaces have been growing rapidly. Such data typically have a very fine spatial resolution, which can lead to computational and visualisation issues. To overcome these issues, it is a common practice to reduce the density of point cloud data during initial data processing. As such, various simplification methods had been developed and used in practice. The choice of those methods is crucial to preserve features and shapes of the terrain in the simplified point cloud data. Previous studies on this matter were focused mainly on the methods commonly used in geosciences, but did not consider those in computer graphics. In this study, a total of eight simplification methods that are used widely in both geosciences and computer graphics were compared and analyzed using four sets of terrain surface point cloud data. In addition, unlike previous studies where a global RMSE (root mean squared error) was used as the metric for comparing different methods, the standard deviation of local RMSEs (root mean squared errors) was also calculated in this study to check the uniformity of local RMSEs over the whole terrain areas considered. The results show that the adaptive sampling method yielded thinned point cloud data of higher overall accuracy and more consistent local RMSEs than those obtained using the other methods considered.},
booktitle = {Proceedings of the 4th International Conference on Control and Computer Vision},
pages = {135–141},
numpages = {7},
keywords = {data density, computer graphics, terrain, simplification, sampling, point clouds},
location = {Macau, China},
series = {ICCCV '21}
}

@article{10.1145/3508030,
author = {Bhuyan, Sandeepa and Zhao, Shulin and Ying, Ziyu and Kandemir, Mahmut T. and Das, Chita R.},
title = {End-to-End Characterization of Game Streaming Applications on Mobile Platforms},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3508030},
doi = {10.1145/3508030},
abstract = {With the advent of 5G, supporting high-quality game streaming applications on edge devices has become a reality. This is evidenced by a recent surge in cloud gaming applications on mobile devices. In contrast to video streaming applications, interactive games require much more compute power for supporting improved rendering (such as 4K streaming) with the stipulated frames-per second (FPS) constraints. This in turn consumes more battery power in a power-constrained mobile device. Thus, the state-of-the-art gaming applications suffer from lower video quality (QoS) and/or energy efficiency. While there has been a plethora of recent works on optimizing game streaming applications, to our knowledge, there is no study that systematically investigates the &lt;QoS, Energy&gt; design pairs on the end-to-end game streaming pipeline across the cloud, network, and edge devices to understand the individual contributions of the different stages of the pipeline for improving the overall QoS and energy efficiency. In this context, this paper presents a comprehensive performance and power analysis of the entire game streaming pipeline consisting of the server/cloud side, network, and edge. Through extensive measurements with a high-end workstation mimicking the cloud end, an open-source platform (Moonlight-GameStreaming) emulating the edge device/mobile platform, and two network settings (WiFi and 5G) we conduct a detailed measurement-based study with seven representative games with different characteristics. We characterize the performance in terms of frame latency, QoS, bitrate, and energy consumption for different stages of the gaming pipeline. Our study shows that the rendering stage and the encoding stage at the cloud end are the bottlenecks to support 4K streaming. While 5G is certainly more suitable for supporting enhanced video quality with 4K streaming, it is more expensive in terms of power consumption compared to WiFi. Further, fluctuations in 5G network quality can lead to huge frame drops thus affecting QoS, which needs to be addressed by a coordinated design between the edge device and the server. Finally, the network interface and the decoder units in a mobile platform need more energy-efficient design to support high quality games at a lower cost. These observations should help in designing more cost-effective future cloud gaming platforms.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {feb},
articleno = {10},
numpages = {25},
keywords = {energy efficiency, cloud gaming, smartphones, 5g, performance}
}

@inproceedings{10.1145/3580305.3599878,
author = {Xi, Yunjia and Liu, Weiwen and Wang, Yang and Tang, Ruiming and Zhang, Weinan and Zhu, Yue and Zhang, Rui and Yu, Yong},
title = {On-Device Integrated Re-Ranking with Heterogeneous Behavior Modeling},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599878},
doi = {10.1145/3580305.3599878},
abstract = {As an emerging field driven by industrial applications, integrated re-ranking combines lists from upstream sources into a single list, and presents it to the user. The quality of integrated re-ranking is especially sensitive to real-time user behaviors and preferences. However, existing methods are all built on the cloud-to-edge framework, where mixed lists are generated by the cloud model and then sent to the devices. Despite its effectiveness, such a framework fails to capture users' real-time preferences due to the network bandwidth and latency. Hence, we propose to place the integrated re-ranking model on devices, allowing for the full exploitation of real-time behaviors. To achieve this, we need to address two key issues: first, how to extract users' preferences for different sources from heterogeneous and imbalanced user behaviors; second, how to explore the correlation between the extracted personalized preferences and the candidate items. In this work, we present the first on-Device Integrated Re-ranking framework, DIR, to avoid delays in processing real-time user behaviors. DIR includes a multi-sequence behavior modeling module to extract the user's source-level preferences, and a preference-adaptive re-ranking module to incorporate personalized source-level preferences into the re-ranking of candidate items. Besides, we design exposure loss and utility loss to jointly optimize exposure fairness and overall utility. Extensive experiments on three datasets show that DIR significantly outperforms the state-of-the-art baselines in utility-based and fairness-based metrics.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5225–5236},
numpages = {12},
keywords = {integrated re-ranking, edge computing, recommender system},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@inproceedings{10.1145/3564858.3564860,
author = {Yang, Liu and Wang, Jian and Zhao, Zebin},
title = {Quality Evaluation of Government Epidemic Data Openness Based on Cloud Model and PSR Theory},
year = {2022},
isbn = {9781450396721},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564858.3564860},
doi = {10.1145/3564858.3564860},
abstract = {Government data opening has become one of the key measures of government emergency management in the big data era. Investigating deeply the quality of government data opening under public health emergencies can help grasp the current situation and provide experience for future work. This paper evaluates the 31 regional health commissions’ epidemic data openness using a framework based on PSR theory in China, and the cloud model was used to evaluate the quality of government data opening at the national and regional levels. The comprehensive evaluation level of government data openness under the epidemic situation in China was ordinary level and the level of data openness varies greatly among regions. Data state is the main factor restricting the level of data openness.},
booktitle = {Proceedings of the 5th International Conference on Information Management and Management Science},
pages = {7–14},
numpages = {8},
keywords = {public health emergencies, cloud model, open government data, PSR theory},
location = {Chengdu, China},
series = {IMMS '22}
}

@inproceedings{10.1145/3579990.3580010,
author = {Li, Bolun and Su, Pengfei and Chabbi, Milind and Jiao, Shuyin and Liu, Xu},
title = {DJXPerf: Identifying Memory Inefficiencies via Object-Centric Profiling for Java},
year = {2023},
isbn = {9798400701016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579990.3580010},
doi = {10.1145/3579990.3580010},
abstract = {Java is the “go-to” programming language choice for developing scalable enterprise cloud applications. In such systems, even a few percent CPU time savings can offer a significant competitive advantage and cost savings. Although performance tools abound for Java, those that focus on the data locality in the memory hierarchy are rare.  

In this paper, we first categorize data locality issues in Java programs. We then present DJXPerf, a lightweight, object-centric memory profiler for Java, which associates memory-hierarchy performance metrics (e.g., cache/TLB misses) with Java objects. DJXPerf uses statistical sampling of hardware performance monitoring counters to attribute metrics to not only source code locations but also Java objects. DJXPerf presents Java object allocation contexts combined with their usage contexts and presents them ordered by the poor locality behaviors. DJXPerf’s performance measurement, object attribution, and presentation techniques guide optimizing object allocation, layout, and access patterns. DJXPerf incurs only ~8.5\% runtime overhead and ∼6\% memory overhead on average, requiring no modifications to hardware, OS, Java virtual machine, or application source code, which makes it attractive to use in production. Guided by DJXPerf, we study and optimize a number of Java and Scala programs, including well-known benchmarks and real-world applications, and demonstrate significant speedups.},
booktitle = {Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {81–94},
numpages = {14},
keywords = {performance optimization, Java, profiling, PMU},
location = {Montr\'{e}al, QC, Canada},
series = {CGO 2023}
}

@inproceedings{10.1145/3489048.3522650,
author = {Bhuyan, Sandeepa and Zhao, Shulin and Ying, Ziyu and Kandemir, Mahmut T. and Das, Chita R.},
title = {End-to-End Characterization of Game Streaming Applications on Mobile Platforms},
year = {2022},
isbn = {9781450391412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489048.3522650},
doi = {10.1145/3489048.3522650},
abstract = {With the advent of 5G, hosting high-quality game streaming applications on mobile devices has become a reality. To our knowledge, no prior study systematically investigates the &lt; QoS, Energy &gt; tuple on the end-to-end game streaming pipeline across the cloud, network, and edge devices to understand the individual contributions of the different pipeline stages. In this paper, we present a comprehensive performance and power analysis of the entire game streaming pipeline through extensive measurements with a high-end workstation mimicking the cloud end, an open-source platform (Moonlight-GameStreaming) emulating the edge device/mobile platform, and two network settings (WiFi and 5G). Our study shows that the rendering stage and the encoding stage at the cloud end are the bottlenecks for 4K streaming. While 5G is certainly more suitable for supporting enhanced video quality with 4K streaming, it is more expensive in terms of power consumption compared to WiFi. Further, the network interface and the decoder units in mobile devices need more energy-efficient design to support high quality games at a lower cost. These observations should help in designing more cost-effective future cloud gaming platforms.},
booktitle = {Abstract Proceedings of the 2022 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {11–12},
numpages = {2},
keywords = {performance, energy efficiency, smartphones, 5g, cloud gaming},
location = {Mumbai, India},
series = {SIGMETRICS/PERFORMANCE '22}
}

@article{10.1145/3547353.3522650,
author = {Bhuyan, Sandeepa and Zhao, Shulin and Ying, Ziyu and Kandemir, Mahmut T. and Das, Chita R.},
title = {End-to-End Characterization of Game Streaming Applications on Mobile Platforms},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/3547353.3522650},
doi = {10.1145/3547353.3522650},
abstract = {With the advent of 5G, hosting high-quality game streaming applications on mobile devices has become a reality. To our knowledge, no prior study systematically investigates the &lt; QoS, Energy &gt; tuple on the end-to-end game streaming pipeline across the cloud, network, and edge devices to understand the individual contributions of the different pipeline stages. In this paper, we present a comprehensive performance and power analysis of the entire game streaming pipeline through extensive measurements with a high-end workstation mimicking the cloud end, an open-source platform (Moonlight-GameStreaming) emulating the edge device/mobile platform, and two network settings (WiFi and 5G). Our study shows that the rendering stage and the encoding stage at the cloud end are the bottlenecks for 4K streaming. While 5G is certainly more suitable for supporting enhanced video quality with 4K streaming, it is more expensive in terms of power consumption compared to WiFi. Further, the network interface and the decoder units in mobile devices need more energy-efficient design to support high quality games at a lower cost. These observations should help in designing more cost-effective future cloud gaming platforms.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jul},
pages = {11–12},
numpages = {2},
keywords = {cloud gaming, energy efficiency, 5g, performance, smartphones}
}

@article{10.1145/3470970,
author = {Min, Xiongkuo and Gu, Ke and Zhai, Guangtao and Yang, Xiaokang and Zhang, Wenjun and Le Callet, Patrick and Chen, Chang Wen},
title = {Screen Content Quality Assessment: Overview, Benchmark, and Beyond},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3470970},
doi = {10.1145/3470970},
abstract = {Screen content, which is often computer-generated, has many characteristics distinctly different from conventional camera-captured natural scene content. Such characteristic differences impose major challenges to the corresponding content quality assessment, which plays a critical role to ensure and improve the final user-perceived quality of experience (QoE) in various screen content communication and networking systems. Quality assessment of such screen content has attracted much attention recently, primarily because the screen content grows explosively due to the prevalence of cloud and remote computing applications in recent years, and due to the fact that conventional quality assessment methods can not handle such content effectively. As the most technology-oriented part of QoE modeling, image/video content/media quality assessment has drawn wide attention from researchers, and a large amount of work has been carried out to tackle the problem of screen content quality assessment. This article is intended to provide a systematic and timely review on this emerging research field, including (1) background of natural scene vs. screen content quality assessment; (2) characteristics of natural scene vs. screen content; (3) overview of screen content quality assessment methodologies and measures; (4) relevant benchmarks and comprehensive evaluation of the state-of-the-art; (5) discussions on generalizations from screen content quality assessment to QoE assessment, and other techniques beyond QoE assessment; and (6) unresolved challenges and promising future research directions. Throughout this article, we focus on the differences and similarities between screen content and conventional natural scene content. We expect that this review article shall provide readers with an overview of the background, history, recent progress, and future of the emerging screen content quality assessment research.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {187},
numpages = {36},
keywords = {natural scene, quality of experience, Screen content, quality assessment}
}

@inproceedings{10.1145/3617023.3617068,
author = {Cesar, Pablo},
title = {Towards Volumetric Video Conferencing},
year = {2023},
isbn = {9798400709081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617023.3617068},
doi = {10.1145/3617023.3617068},
abstract = {With Social Extended Reality (XR) emerging as a new medium, where users can remotely experience immersive content with others, the vision of a true feeling of ‘being there together’ has become a realistic goal. This keynote will provide an overview about the challenges to achieve such a goal, based on results from practical case studies like the TRANSMIXR and MediaScape XR projects. We will discuss about different technologies, like point clouds, that can be used as the format for representing highly-realistic digital humans, and about metrics and protocols for quantifying the quality of experience. The final intention of the talk is to shed some light on social XR, as a new group of virtual reality experiences based on social photorealistic immersive content. We will discuss about the challenges regarding production and user-centric processes, and discover the new opportunities open by this new medium},
booktitle = {Proceedings of the 29th Brazilian Symposium on Multimedia and the Web},
pages = {5},
numpages = {1},
keywords = {social extended reality, quality of experience, Volumetric video},
location = {<conf-loc>, <city>Ribeir\~{a}o Preto</city>, <country>Brazil</country>, </conf-loc>},
series = {WebMedia '23}
}

@inproceedings{10.1145/3485447.3512276,
author = {Alhilal, Ahmad and Braud, Tristan and Han, Bo and Hui, Pan},
title = {Nebula: Reliable Low-Latency Video Transmission for Mobile Cloud Gaming},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512276},
doi = {10.1145/3485447.3512276},
abstract = {Mobile cloud gaming enables high-end games on constrained devices by streaming the game content from powerful servers through mobile networks. Mobile networks suffer from highly variable bandwidth, latency, and losses that affect the gaming experience. This paper introduces , an end-to-end cloud gaming framework to minimize the impact of network conditions on the user experience. relies on an end-to-end distortion model adapting the video source rate and the amount of frame-level redundancy based on the measured network conditions. As a result, it minimizes the motion-to-photon (MTP) latency while protecting the frames from losses. We fully implement and evaluate its performance against the state-of-the-art techniques and latest research in real-time mobile cloud gaming transmission on a physical testbed over emulated and real wireless networks. consistently balances MTP latency (&lt;140&nbsp;ms) and visual quality (&gt;31dB) even in highly variable environments. A user experiment confirms that maximizes the user experience with high perceived video quality, playability, and low user load.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3407–3417},
numpages = {11},
keywords = {Adaptive Rate., Forward Error Correction, Mobile Cloud Gaming},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3578495.3578498,
author = {Wien, Mathias},
title = {MPEG Visual Quality Assessment Advisory Group: Overview and Perspectives},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
url = {https://doi.org/10.1145/3578495.3578498},
doi = {10.1145/3578495.3578498},
abstract = {The perceived visual quality is of utmost importance in the context of visual media compression, such as 2D, 3D, immersive video, and point clouds. The trade-off between compression efficiency and computational/implementation complexity has a crucial impact on the success of a compression scheme. This specifically holds for the development of visual media compression standards which typically aims at maximum compression efficiency using state-of-the-art coding technology. In MPEG, the subjective and objective assessment of visual quality has always been an integral part of the standards development process. Due to the significant effort of formal subjective evaluations, the standardization process typically relies on such formal tests in the starting phase and for verification while in the development phase objective metrics are used. In the new MPEG structure, established in 2020, a dedicated advisory group has been installed for the purpose of providing, maintaining, and developing visual quality assessment methods suitable for use in the standardization process.This column lays out the scope and tasks of this advisory group and reports on its first achievements and developments. After a brief overview of the organizational structure, current projects are presented, and initial results are presented.},
journal = {SIGMultimedia Rec.},
month = {dec},
articleno = {3},
numpages = {1}
}

@inproceedings{10.1145/3524273.3532898,
author = {Slivar, Ivan and Bacic, Kresimir and Orsolic, Irena and Skorin-Kapov, Lea and Suznjevic, Mirko},
title = {CGD: A Cloud Gaming Dataset with Gameplay Video and Network Recordings},
year = {2022},
isbn = {9781450392839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524273.3532898},
doi = {10.1145/3524273.3532898},
abstract = {With advances in network capabilities, the gaming industry is increasingly turning towards offering "gaming on demand" solutions, with cloud gaming services such as Sony PlayStation Now, Google Stadia, and NVIDIA GeForce NOW expanding their market offerings. Similar to adaptive video streaming services, cloud gaming services typically adapt the quality of game streams (e.g., bitrate, resolution, frame rate) in accordance with current network conditions. To select the most appropriate video encoding parameters given certain conditions, it is important to understand their impact on Quality of Experience (QoE). On the other hand, network operators are interested in understanding the relationships between parameters measurable in the network and cloud gaming QoE, to be able to invoke QoE-aware network management mechanisms. To encourage developments in these areas, comprehensive datasets are crucial, including both network and application layer data. This paper presents CGD, a dataset consisting of 600 game streaming sessions corresponding to 10 games of different genres being played and streamed using the following encoding parameters: bitrate (5, 10, 20 Mbps), resolution (720p, 1080p), and frame rate (30, 60 fps). For every combination repeated five times for each game, the dataset includes: 1) gameplay video recordings, 2) network traffic traces, 3) user input logs (mouse and keyboard), and 4) streaming performance logs.},
booktitle = {Proceedings of the 13th ACM Multimedia Systems Conference},
pages = {272–278},
numpages = {7},
keywords = {network traffic, dataset, cloud gaming, video metrics, gameplay, user input, raw video},
location = {Athlone, Ireland},
series = {MMSys '22}
}

@inproceedings{10.1145/3582935.3583055,
author = {Wu, Guofang and Dong, Guoliang and Xu, Shuquan},
title = {Application of Personnel Safety Management System in Network Security Guarantee},
year = {2023},
isbn = {9781450396806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582935.3583055},
doi = {10.1145/3582935.3583055},
abstract = {The rapid development of cloud protection technology provides high-quality security protection barriers for the security of websites deployed on the Internet from a technical level. After the website administrators patch the relevant vulnerabilities in time, the occurrence of network security accidents can be greatly reduced at the technical level. However, cyber attackers take advantage of the negligence of the website's personnel management to launch attacks on the website, resulting in frequent network security incidents such as account theft, web page tampering, website downtime, and core data theft. As a major part of network security management, the formulation and implementation of personnel management systems are crucial to the security of websites. This paper summarizes the experience of network security management and security over the years, puts forward the requirements and methods of personnel management in network security security, and strengthens the management requirements of human factors in the network security system. These measures have played an effective role in network security assurance.},
booktitle = {Proceedings of the 5th International Conference on Information Technologies and Electrical Engineering},
pages = {714–718},
numpages = {5},
keywords = {Network security, Permissions, Cloud computer room, Security awareness, Personnel management, Security administrator},
location = {Changsha, China},
series = {ICITEE '22}
}

@inproceedings{10.1145/3570361.3592529,
author = {Wen, Hao and Li, Yuanchun and Zhang, Zunshuai and Jiang, Shiqi and Ye, Xiaozhou and Ouyang, Ye and Zhang, Yaqin and Liu, Yunxin},
title = {AdaptiveNet: Post-Deployment Neural Architecture Adaptation for Diverse Edge Environments},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570361.3592529},
doi = {10.1145/3570361.3592529},
abstract = {Deep learning models are increasingly deployed to edge devices for real-time applications. To ensure stable service quality across diverse edge environments, it is highly desirable to generate tailored model architectures for different conditions. However, conventional pre-deployment model generation approaches are not satisfactory due to the difficulty of handling the diversity of edge environments and the demand for edge information. In this paper, we propose to adapt the model architecture after deployment in the target environment, where the model quality can be precisely measured and private edge data can be retained. To achieve efficient and effective edge model generation, we introduce a pretraining-assisted on-cloud model elastification method and an edge-friendly on-device architecture search method. Model elastification generates a high-quality search space of model architectures with the guidance of a developer-specified oracle model. Each subnet in the space is a valid model with different environment affinity, and each device efficiently finds and maintains the most suitable subnet based on a series of edge-tailored optimizations. Extensive experiments on various edge devices demonstrate that our approach is able to achieve significantly better accuracy-latency tradeoffs (e.g. 46.74\% higher on average accuracy with a 60\% latency budget) than strong baselines with minimal overhead (13 GPU hours in the cloud and 2 minutes on the edge server).},
booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
articleno = {28},
numpages = {17},
keywords = {post-deployment adaptation, edge environments, neural networks, model elastification},
location = {Madrid, Spain},
series = {ACM MobiCom '23}
}

@inproceedings{10.1145/3581783.3613827,
author = {Zhang, Junzhe and Chen, Tong and Ding, Dandan and Ma, Zhan},
title = {G-PCC++: Enhanced Geometry-Based Point Cloud Compression},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613827},
doi = {10.1145/3581783.3613827},
abstract = {MPEG Geometry-based Point Cloud Compression (G-PCC) standard is developed for lossy encoding of point clouds to enable immersive services over the Internet. However, lossy G-PCC introduces superimposed distortions from both geometry and attribute information, seriously deteriorating the Quality of Experience (QoE). This paper thus proposes the Enhanced G-PCC (GPCC++), to effectively address the compression distortion and restore the quality. G-PCC++ separates the enhancement into two stages: it first enhances the geometry and then maps the decoded attribute to the enhanced geometry for refinement. As for geometry restoration, a k Nearest Neighbors (kNN)-based Linear Interpolation is first used to generate a denser geometry representation, on top of which GeoNet further generates sufficient candidates to restore geometry through probability-sorted selection. For attribute enhancement, a kNN-based Gaussian Distance Weighted Mapping is devised to re-colorize all points in enhanced geometry tensor, which are then refined by AttNet for the final reconstruction. G-PCC++ is the first solution addressing the geometry and attribute artifacts together. Extensive experiments on several public datasets demonstrate the superiority of G-PCC++, e.g., on the solid point cloud dataset 8iVFB, G-PCC++ outperforms G-PCC by 88.24\% (80.54\%) BD-BR in D1 (D2) measurement of geometry and by 14.64\% (13.09\%) BD-BR in Y (YUV) attribute. Moreover, when considering both geometry and attribute, G-PCC++ also largely surpasses G-PCC by 25.58\% BD-BR using PCQM assessment.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {1352–1363},
numpages = {12},
keywords = {point cloud compression, quality enhancement, compression artifact},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@article{10.1145/3512937,
author = {Zhang, Yongle and Asamoah Owusu, Dennis and Carpuat, Marine and Gao, Ge},
title = {Facilitating Global Team Meetings Between Language-Based Subgroups: When and How Can Machine Translation Help?},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512937},
doi = {10.1145/3512937},
abstract = {Global teams frequently consist of language-based subgroups who put together complementary information to achieve common goals. Previous research outlines a two-step work communication flow in these teams. There are team meetings using a required common language (i.e., English); in preparation for those meetings, people have subgroup conversations in their native languages. Work communication at team meetings is often less effective than in subgroup conversations. In the current study, we investigate the idea of leveraging machine translation (MT) to facilitate global team meetings. We hypothesize that exchanging subgroup conversation logs before a team meeting offers contextual information that benefits teamwork at the meeting. MT can translate these logs, which enables comprehension at a low cost. To test our hypothesis, we conducted a between-subjects experiment where twenty quartets of participants performed a personnel selection task. Each quartet included two English native speakers (NS) and two non-native speakers (NNS) whose native language was Mandarin. All participants began the task with subgroup conversations in their native languages, then proceeded to team meetings in English. We manipulated the exchange of subgroup conversation logs prior to team meetings: with MT-mediated exchanges versus without. Analysis of participants' subjective experience, task performance, and depth of discussions as reflected through their conversational moves jointly indicates that team meeting quality improved when there were MT-mediated exchanges of subgroup conversation logs as opposed to no exchanges. We conclude with reflections on when and how MT could be applied to enhance global teamwork across a language barrier.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {90},
numpages = {26},
keywords = {global teams, shared context, language choice, machine translation}
}

@inproceedings{10.1145/3603269.3604826,
author = {Hamadanian, Pouya and Gallatin, Doug and Alizadeh, Mohammad and Chintalapudi, Krishna},
title = {Ekho: Synchronizing Cloud Gaming Media across Multiple Endpoints},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603269.3604826},
doi = {10.1145/3603269.3604826},
abstract = {Online cloud gaming platforms stream game media to multiple end-points (e.g., a television display and a controller-connected headset) via possibly different networks with considerably different latencies. This leads to the media being played out of sync with one another, and severely degrades user experience. Typical approaches that rely on network and software timing measurements fail to reach synchronization goals. In this work, we propose Ekho, a robust and efficient end-to-end approach for synchronizing streams transmitted to two devices. Ekho adds faint, human-inaudible pseudo-noise (PN) markers to the game audio, and listens for these markers in the chat audio captured by the player's microphone to measure inter-stream delay (ISD). The game server then compensates for the ISD to synchronize the streams. We evaluate Ekho in depth, with a corpus of audio samples from popular online games, and demonstrate that it calculates ISD with sub-millisecond accuracy, has low computational overhead, and is resilient to background chatter, compression and microphone quality. In end-to-end tests over WiFi and cellular links with frequent packet loss and playback disruption, Ekho maintains human-imperceptible ISD (&lt; 10 ms) 86.8\% of the time. Without Ekho, the ISD exceeds 50 ms at all times.},
booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
pages = {533–549},
numpages = {17},
keywords = {cloud gaming, inter-device synchronization, ITU-T P.808, media synchronization},
location = {New York, NY, USA},
series = {ACM SIGCOMM '23}
}

@inproceedings{10.1145/3503161.3547807,
author = {Zhang, Rui-Xiao and Yang, Changpeng and Wang, Xiaochan and Huang, Tianchi and Wu, Chenglei and Liu, Jiangchuan and Sun, Lifeng},
title = {AggCast: Practical Cost-Effective Scheduling for Large-Scale Cloud-Edge Crowdsourced Live Streaming},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547807},
doi = {10.1145/3503161.3547807},
abstract = {Conventional wisdom claims that in order to improve viewer engagement, the cloud-edge providers should serve the viewers with the nearest edge nodes, however, we show that doing this for crowdsourced live streaming (CLS) services can introduce significant costs inefficiency. We observe that the massive number of channels has greatly burdened the operating expenditure of the cloud-edge providers, and most importantly, unbalanced viewer distribution makes the edge nodes suffer significant costs inefficiency. To tackle the above concerns, we propose AggCast, a novel CLS scheduling framework to optimize the edge node utilization for the cloud-edge provider. The core idea of AggCast is to aggregate some viewers who are initially scattered on different regions, and assign them to fewer pre-selected nodes, thereby reducing bandwidth costs. In particular, by leveraging the insights obtained from our large-scale measurement, AggCast can not only ensure quality of experience (QoS), but also satisfy the systematic requirements of CLS services. AggCast has been A/B tested and fully deployed in a top cloud-edge provider in China for over eight months. The online and trace-driven experiments show that, compared to the common practice, AggCast can save over 15\% back-to-source (BTS) bandwidth costs while having no negative impacts on QoS.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3026–3034},
numpages = {9},
keywords = {content delivery, cloud edge computing, live streaming},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3474717.3483959,
author = {Bhore, Sujoy and Ganian, Robert and Li, Guangping and N\"{o}llenburg, Martin and Wulms, Jules},
title = {Worbel: Aggregating Point Labels into Word Clouds},
year = {2021},
isbn = {9781450386647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474717.3483959},
doi = {10.1145/3474717.3483959},
abstract = {Point feature labeling is a classical problem in cartography and GIS that has been extensively studied for geospatial point data. At the same time, word clouds are a popular visualization tool to show the most important words in text data which has also been extended to visualize geospatial data (Buchin et al. PacificVis 2016).In this paper, we study a hybrid visualization, which combines aspects of word clouds and point labeling. In the considered setting, the input data consists of a set of points grouped into categories and our aim is to place multiple disjoint and axis-aligned rectangles, each representing a category, such that they cover points of (mostly) the same category under some natural quality constraints.In our visualization, we then place category names inside the computed rectangles to produce a labeling of the covered points which summarizes the predominant categories globally (in a word-cloud-like fashion) while locally avoiding excessive misrepresentation of points (i.e., retaining the precision of point labeling).We show that computing a minimum set of such rectangles is NP-hard. Hence, we turn our attention to developing heuristics and exact SAT models to compute our visualizations. We evaluate our algorithms quantitatively, measuring running time and quality of the produced solutions, on several artificial and real-world data sets. Our experiments show that the heuristics produce solutions of comparable quality to the SAT models while running much faster.},
booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
pages = {256–267},
numpages = {12},
keywords = {labeling, categorical point data, word clouds},
location = {Beijing, China},
series = {SIGSPATIAL '21}
}

@article{10.1145/3603376,
author = {Bhore, Sujoy and Ganian, Robert and Li, Guangping and N\"{o}llenburg, Martin and Wulms, Jules},
title = {Worbel: Aggregating Point Labels into Word Clouds},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2374-0353},
url = {https://doi.org/10.1145/3603376},
doi = {10.1145/3603376},
abstract = {Point feature labeling is a classical problem in cartography and GIS that has been extensively studied for geospatial point data. At the same time, word clouds are a popular visualization tool to show the most important words in text data which has also been extended to visualize geospatial data (Buchin et&nbsp;al. PacificVis 2016). In this article, we study a hybrid visualization, which combines aspects of word clouds and point labeling. In the considered setting, the input data consist of a set of points grouped into categories and our aim is to place multiple disjoint and axis-aligned rectangles, each representing a category, such that they cover points of (mostly) the same category under some natural quality constraints. In our visualization, we then place category names inside the computed rectangles to produce a labeling of the covered points which summarizes the predominant categories globally (in a word-cloud-like fashion) while locally avoiding excessive misrepresentation of points (i.e., retaining the precision of point labeling). We show that computing a minimum set of such rectangles is NP-hard. Hence, we turn our attention to developing a heuristic with (optional) exact components using SAT models to compute our visualizations. We evaluate our algorithms quantitatively, measuring running time and quality of the produced solutions, on several synthetic and real-world data sets. Our experiments show that the fully heuristic approach produces solutions of comparable quality to heuristics combined with exact SAT models, while running much faster.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = {aug},
articleno = {19},
numpages = {32},
keywords = {word clouds, categorical point data, Labeling}
}

@inproceedings{10.1145/3546000.3546005,
author = {Lou, Ren and Zhang, Jiacheng and Zhang, Lei and Hong, Qiang and Zhou, Yueqi and Li, Xinghua},
title = {Research on Highway CPS-T of Wide Area Communication and Data Cloud},
year = {2022},
isbn = {9781450396295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546000.3546005},
doi = {10.1145/3546000.3546005},
abstract = {Cyber physical transportation system (CPS-T) is a traffic perception, control and service system based on algorithm, model, data and computing power. With the help of radar point cloud, video image, GNSS, sensor and other types of monitoring equipment, the highway data can be collected with high quality and transmitted with high reliability under the condition of relatively complete communication conditions. This paper studies the technical application of CPS-T for highways. Through the deployment of communication technology and data cloud platform, it can intelligently perceive and analyze dynamic and static operation data, accurately identify or predict key ramps, bottleneck sections and mainstream traffic channels, and dynamically implement active control strategies such as ramp control, shoulder control, lane control and rate adjustment, so as to realize the advance guidance and control of highway traffic flow. The relevant research has been measured in the highway sections of Shanghai and Zhejiang Province, and the research results have strong engineering reference value.},
booktitle = {Proceedings of the 6th International Conference on High Performance Compilation, Computing and Communications},
pages = {32–37},
numpages = {6},
location = {Jilin, China},
series = {HP3C '22}
}

@article{10.1145/3630614.3630616,
author = {Tannu, Swamit and Nair, Prashant J.},
title = {The Dirty Secret of SSDs: Embodied Carbon},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3630614.3630616},
doi = {10.1145/3630614.3630616},
abstract = {Scalable Solid-State Drives (SSDs) have ushered in a transformative era in data storage and accessibility, spanning both data centers and portable devices. However, the strides made in scaling this technology can bear significant environmental consequences. On a global scale, a notable portion of semiconductor manufacturing relies on electricity derived from coal and natural gas sources. A striking example of this is the manufacturing process for a single Gigabyte of Flash memory, which emits approximately 0.16 Kg of CO2 - a considerable fraction of the total carbon emissions attributed to the system. Remarkably, the manufacturing of storage devices alone contributed to an estimated 20 million metric tonnes of CO2 emissions in the year 2021.In light of these environmental concerns, this paper delves into an analysis of the sustainability trade-offs inherent in Solid-State Drives (SSDs) when compared to traditional Hard Disk Drives (HDDs). Moreover, this study proposes methodologies to gauge the embodied carbon costs associated with storage systems effectively. The research encompasses four key strategies to enhance the sustainability of storage systems.Firstly, the paper offers insightful guidance for selecting the most suitable storage medium, be it SSDs or HDDs, considering the broader ecological impact. Secondly, the paper advocates for implementing techniques that extend the lifespan of SSDs, thereby mitigating premature replacements and their attendant environmental toll. Thirdly, the paper emphasizes the need for efficient recycling and reuse of high-density multi-level cell-based SSDs, underscoring the significance of minimizing electronic waste.Lastly, for handheld devices, the paper underscores the potential of harnessing the elasticity offered by cloud storage solutions as a means to curtail the ecological repercussions of localized data storage. In summation, this study critically addresses the embodied carbon issues associated with SSDs, comparing them with HDDs, and proposes a comprehensive framework of strategies to enhance the sustainability of storage systems.},
journal = {SIGENERGY Energy Inform. Rev.},
month = {oct},
pages = {4–9},
numpages = {6},
keywords = {sustainability, solid state drives, embodied carbon, hard disk drive}
}

@inproceedings{10.1145/3604930.3605721,
author = {Arora, Rohan and Devi, Umamaheswari and Eilam, Tamar and Goyal, Aanchal and Narayanaswami, Chandra and Parida, Pritish},
title = {Towards Carbon Footprint Management in Hybrid Multicloud},
year = {2023},
isbn = {9798400702426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604930.3605721},
doi = {10.1145/3604930.3605721},
abstract = {Enterprises today aspire to optimize the operating costs and carbon footprint (CFP) of their IT operations jointly without compromising their business imperatives. This has given rise to a hybrid approach in which enterprises retain the dynamic choice to leverage private data centers and one or more public clouds in conjunction. While cloud service providers (CSPs) have long provided APIs for estimating, reconciling, and optimizing operating costs, they have only recently started exposing APIs related to CFP.Indeed, this is a step in the right direction. Nevertheless, our analyses of these APIs reveals many gaps that need to be addressed to facilitate sizing and placement decisions that can factor in carbon. First, there is a lack of standardized, transparent methodology for CFP quantification across different CSPs. Second, the coarse granularity of the CFP data provided today can help with post-facto reporting but is not suitable for proactive fine-grained optimization. Last, enterprises themselves are unable to independently compute the current CFP or estimate potential CFP savings since CSPs do not share the required power usage data.To address these gaps, enterprises have started developing their own carbon assessment methodologies and tools to estimate the CFP of workloads running on public clouds using the available user-facing APIs. These systems hold the promise for an independent and unbiased evaluation and estimation of relative savings between different deployment options by cloud users. We describe and analyze the details of CSP-native carbon-reporting tools and their quantification methodology, and the "outside-of-the-cloud" estimation approaches. Finally, we present opportunities for future research in the direction of trustworthy, fine-grained, public cloud workload CFP estimation, which is a prerequisite for meaningful realization of carbon optimization.},
booktitle = {Proceedings of the 2nd Workshop on Sustainable Computer Systems},
articleno = {9},
numpages = {7},
keywords = {GHG accounting, data centers, carbon-aware optimization, sustainable computing, GHG emissions, cloud, carbon footprint},
location = {Boston, MA, USA},
series = {HotCarbon '23}
}

@inproceedings{10.1145/3510858.3511355,
author = {Zhang, Tianze},
title = {An Optimal Grasping Point Identification Method Based on Deep Learning and Point Cloud Processing},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511355},
doi = {10.1145/3510858.3511355},
abstract = {With the development of the field of robotics and the increasingly convenient acquisition of point cloud data, point cloud is widely used in robots to complete many tasks more accurately because of its rich three-dimensional information. In this paper, we propose a method based on PointNet++ combining deep learning and point cloud processing to extract the grasping points of the objects. This method selects YCB dataset. The grasping point pairs in each perspective are sampled by point cloud feature point detection and scored by Force Closure together with Shape of the grasp polygon metrics. The input of PointNet++ is the single-perspective field point cloud after the searching algorithm based on grasping point pairs and the final output classification results are divided into two classes. Experimental results conducted based on Kinect2 and UR5 mechanical arm show that our method can achieve 85\% accuracy in the two-class simulation experiment and 80.32\% average success rate in the real grasping tasks, whose robustness is also verified in this paper.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {667–671},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00026,
author = {Yan, Xiaohan and Hsieh, Ken and Liyanage, Yasitha and Ma, Minghua and Chintalapati, Murali and Lin, Qingwei and Dang, Yingnong and Zhang, Dongmei},
title = {Aegis: Attribution of Control Plane Change Impact across Layers and Components for Cloud Systems},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00026},
doi = {10.1109/ICSE-SEIP58684.2023.00026},
abstract = {Modern cloud control plane infrastructure like Microsoft Azure has evolved into a complex one to serve customer needs for diverse types of services and adequate cloud-based resources. On such interconnected system, implementing changes at one component can have an impact on other components, even across different hierarchical computing layers. As a result of the complexity and interconnected nature of the cloud-based services, it poses a challenge to correctly attribute service quality degradation to a control plane change, to infer causality between the two and to mitigate any negative impact. In this paper, we present Aegis, an end-to-end analytical service for attributing control plane change impact across computing layers and service components in large-scale real-world cloud systems. Aegis processes and correlates service health signals and control plane changes across components to construct the most probable causal relationship. Aegis at its core leverages a domain knowledge-driven correlation algorithm to attribute platform signals to changes, and a counterfactual projection model to quantify control plane change impact to customers. Aegis can mitigate the impact of bad changes by alerting service team and recommending pausing the bad ones. Since Aegis' inception in Azure Control Plane 12 months ago, it has caught several bad changes across service components and layers, and promptly paused them to guard the quality of service. Aegis achieves precision and recall around 80\% on real-world control plane deployments.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {222–233},
numpages = {12},
keywords = {regression detection, counterfactual analysis, impact assessment, safe deployment, cloud computing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3543873.3584626,
author = {Zhao, Xinping and Zhang, Ying and Xiao, Qiang and Ren, Yuming and Yang, Yingchun},
title = {Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3584626},
doi = {10.1145/3543873.3584626},
abstract = {We study a particular matching task we call Music Cold-Start Matching. In short, given a cold-start song request, we expect to retrieve songs with similar audiences and then fastly push the cold-start song to the audiences of the retrieved songs to warm up it. However, there are hardly any studies done on this task. Therefore, in this paper, we will formalize the problem of Music Cold-Start Matching detailedly and give a scheme. During the offline training, we attempt to learn high-quality song representations based on song content features. But, we find supervision signals typically follow power-law distribution causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization. During the online serving, to locate the target audiences more accurately, we propose Clustering-based Audience Targeting (CAT) that clusters audience representations to acquire a few cluster centroids and then locate the target audiences by measuring the relevance between the audience representations and the cluster centroids. Extensive experiments on the offline dataset and online system demonstrate the effectiveness and efficiency of our method. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {351–355},
numpages = {5},
keywords = {Bootstrapping Contrastive Learning, Clustering-based Audience Targeting, Music Cold-Start Matching},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3589806.3600032,
author = {Rosendo, Daniel and Keahey, Kate and Costan, Alexandru and Simonin, Matthieu and Valduriez, Patrick and Antoniu, Gabriel},
title = {KheOps: Cost-Effective Repeatability, Reproducibility, and Replicability of Edge-to-Cloud Experiments},
year = {2023},
isbn = {9798400701764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589806.3600032},
doi = {10.1145/3589806.3600032},
abstract = {Distributed infrastructures for computation and analytics are now evolving towards an interconnected ecosystem allowing complex scientific workflows to be executed across hybrid systems spanning from IoT Edge devices to Clouds, and sometimes to supercomputers (the Computing Continuum). Understanding the performance trade-offs of large-scale workflows deployed on such complex Edge-to-Cloud Continuum is challenging. To achieve this, one needs to systematically perform experiments, to enable their reproducibility and allow other researchers to replicate the study and the obtained conclusions on different infrastructures. This breaks down to the tedious process of reconciling the numerous experimental requirements and constraints with low-level infrastructure design choices. To address the limitations of the main state-of-the-art approaches for distributed, collaborative experimentation, such as Google Colab, Kaggle, and Code Ocean, we propose KheOps, a collaborative environment specifically designed to enable cost-effective reproducibility and replicability of Edge-to-Cloud experiments. KheOps is composed of three core elements: (1) an experiment repository; (2) a notebook environment; and (3) a multi-platform experiment methodology. We illustrate KheOps with a real-life Edge-to-Cloud application. The evaluations explore the point of view of the authors of an experiment described in an article (who aim to make their experiments reproducible) and the perspective of their readers (who aim to replicate the experiment). The results show how KheOps helps authors to systematically perform repeatable and reproducible experiments on the Grid5000 + FIT IoT LAB testbeds. Furthermore, KheOps helps readers to cost-effectively replicate authors experiments in different infrastructures such as Chameleon Cloud + CHI@Edge testbeds, and obtain the same conclusions with high accuracies (&gt; 88\% for all performance metrics).},
booktitle = {Proceedings of the 2023 ACM Conference on Reproducibility and Replicability},
pages = {62–73},
numpages = {12},
keywords = {Computing Continuum, Reproducibility, Repeatability, Edge Computing, Cloud Computing, Workflows, Replicability},
location = {Santa Cruz, CA, USA},
series = {ACM REP '23}
}

@inproceedings{10.1145/3487552.3487847,
author = {Chang, Hyunseok and Varvello, Matteo and Hao, Fang and Mukherjee, Sarit},
title = {Can You See Me Now? A Measurement Study of Zoom, Webex, and Meet},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487847},
doi = {10.1145/3487552.3487847},
abstract = {Since the outbreak of the COVID-19 pandemic, videoconferencing has become the default mode of communication in our daily lives at homes, workplaces and schools, and it is likely to remain an important part of our lives in the post-pandemic world. Despite its significance, there has not been any systematic study characterizing the user-perceived performance of existing videoconferencing systems other than anecdotal reports. In this paper, we present a detailed measurement study that compares three major videoconferencing systems: Zoom, Webex and Google Meet. Our study is based on 48 hours' worth of more than 700 videoconferencing sessions, which were created with a mix of emulated videoconferencing clients deployed in the cloud, as well as real mobile devices running from a residential network. We find that the existing videoconferencing systems vary in terms of geographic scope, which in turns determines streaming lag experienced by users. We also observe that streaming rate can change under different conditions (e.g., number of users in a session, mobile device status, etc), which affects user-perceived streaming quality. Beyond these findings, our measurement methodology can enable reproducible benchmark analysis for any types of comparative or longitudinal study on available videoconferencing systems.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {216–228},
numpages = {13},
location = {Virtual Event},
series = {IMC '21}
}

@inproceedings{10.1145/3581784.3607075,
author = {Zhang, Wang and Shi, Zhan and Liao, Ziyi and Li, Yiling and Du, Yu and Wu, Yutong and Wang, Fang and Feng, Dan},
title = {Graph3PO: A Temporal Graph Data Processing Method for Latency QoS Guarantee in Object Cloud Storage System},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3607075},
doi = {10.1145/3581784.3607075},
abstract = {Object cloud storage systems are deployed with diverse applications that have varying latency service level objectives (SLOs), posting challenges for supporting quality of service with limited storage resources. Existing methods provide prediction-based recommendations for dispatching requests from applications to storage devices, but the prediction accuracy can be affected by complex system topology. To address this issue, Graph3PO is designed to combine storage device queue information with system topological information for forming a temporal graph, which can accurately predict device queue states. Additionally, Graph3PO contains the urgency degree model and cost model for measuring SLO violation risks and penalties of scheduling requests on storage device queues. When the urgency degree of a request exceeds a threshold, Graph3PO determines whether to schedule it in the queue or initiate a hedge request to another storage device. Experimental results show that Graph3PO outperforms its competitors, with SLO violation rates 2.8 to 201.1 times lower.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {23},
numpages = {16},
keywords = {latency QoS guarantee, object cloud storage system, temporal graph},
location = {<conf-loc>, <city>Denver</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {SC '23}
}

@article{10.1145/3520132,
author = {Herzog, Benedict and Reif, Stefan and Hemp, Judith and H\"{o}nig, Timo and Schr\"{o}der-Preikschat, Wolfgang},
title = {Resource-Demand Estimation for Edge Tensor Processing Units},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3520132},
doi = {10.1145/3520132},
abstract = {Machine learning has shown tremendous success in a large variety of applications. The evolution of machine-learning applications from cloud-based systems to mobile and embedded devices has shifted the focus from only quality-related aspects towards the resource demand of machine learning. For embedded systems, dedicated accelerator hardware promises the energy-efficient execution of neural network inferences. Their precise resource demand in terms of execution time and power demand, however, is undocumented. Developers, therefore, face the challenge to fine-tune their neural networks such that their resource demand matches the available budgets. This article presents Precious, a comprehensive approach to estimate the resource demand of an embedded neural network accelerator. We generate randomised neural networks, analyse them statically, execute them on an embedded accelerator while measuring their actual power draw and execution time, and train estimators that map the statically analysed neural network properties to the measured resource demand. In addition, this article provides an in-depth analysis of the neural networks’ resource demands and the responsible network properties. We demonstrate that the estimation error of Precious can be below 1.5\% for both power draw and execution time. Furthermore, we discuss what estimator accuracy is practically achievable and how much effort is required to achieve sufficient accuracy.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {58},
numpages = {24},
keywords = {resource awareness, Neural network accelerator}
}

@article{10.1109/TNET.2022.3171467,
author = {Chang, Hyunseok and Varvello, Matteo and Hao, Fang and Mukherjee, Sarit},
title = {A Tale of Three Videoconferencing Applications: Zoom, Webex, and Meet},
year = {2022},
issue_date = {Oct. 2022},
publisher = {IEEE Press},
volume = {30},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3171467},
doi = {10.1109/TNET.2022.3171467},
abstract = {Since the outbreak of the COVID-19 pandemic, videoconferencing has become the default mode of communication in our daily lives at homes, workplaces and schools, and it is likely to remain an important part of our lives in the post-pandemic world. Despite its significance, there has not been any systematic study characterizing the user-perceived performance of existing videoconferencing systems other than anecdotal reports. In this paper, we present a detailed measurement study that compares three major videoconferencing systems: Zoom, Webex and Google Meet. Our study is based on 62 hours’ worth of more than 1.1K videoconferencing sessions, which were created with a mix of emulated videoconferencing clients deployed in the cloud, as well as real mobile devices running from a residential network over two separate periods with nine months apart. We find that the existing videoconferencing systems vary in terms of geographic scope and resource provisioning strategies, which in turns determine streaming lag experienced by users. We also observe that streaming rate can change under different conditions (e.g., available bandwidth, number of users in a session, mobile device status), which affects user-perceived streaming quality. Beyond these findings, our measurement methodology enables reproducible benchmark analysis for any types of comparative or longitudinal study on available videoconferencing systems.},
journal = {IEEE/ACM Trans. Netw.},
month = {may},
pages = {2343–2358},
numpages = {16}
}

@article{10.1145/3629138,
author = {Martin, Noah and Dogar, Fahad},
title = {Divided at the Edge - Measuring Performance and the Digital Divide of Cloud Edge Data Centers},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CoNEXT3},
url = {https://doi.org/10.1145/3629138},
doi = {10.1145/3629138},
abstract = {Cloud providers are highly incentivized to reduce latency. One way they do this is by locating data centers as close to users as possible. These “cloud edge” data centers are placed in metropolitan areas and enable edge computing for residents of these cities. Therefore, which cities are selected to host edge data centers determines who has the fastest access to applications requiring edge compute — creating a digital divide between those closest and furthest from the edge. In this study we measure latency to the current and predicted cloud edge of three major cloud providers around the world. Our measurements use the RIPE Atlas platform targeting cloud regions, AWS Local Zones, and network optimization services that minimize the path to the cloud edge. An analysis of the digital divide shows rising inequality as the relative difference between users closest and farthest from cloud compute increases. We also find this inequality unfairly affects lower income census tracts in the US. This result is extended globally using remotely sensed night time lights as a proxy for wealth. Finally, we demonstrate that low earth orbit satellite internet can help to close this digital divide and provide more fair access to the cloud edge.},
journal = {Proc. ACM Netw.},
month = {nov},
articleno = {16},
numpages = {23},
keywords = {networks, edge, measurement, digital divide, datacenter}
}

@article{10.1145/3528412,
author = {Maalek, Reza and Maalek, Shahrokh},
title = {Automatic Recognition and Digital Documentation of Cultural Heritage Hemispherical Domes Using Images},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3528412},
doi = {10.1145/3528412},
abstract = {Recent advancements in optical metrology have enabled continuous documentation of dense 3-dimensional (3D) point clouds of construction projects, including cultural heritage preservation projects. These point clouds must then be further processed to generate semantic digital models, which is integral to the lifecycle management of heritage sites. For large-scale and continuous digital documentation, processing of dense 3D point clouds is computationally cumbersome, and consequentially requires additional hardware for data management and analysis, increasing the time, cost, and complexity of the project. Fast and reliable solutions for generating the geometric digital models is, hence, eminently desirable. This article presents an original approach to generate reliable semantic digital models of heritage hemispherical domes using only two images. New closed formulations were derived to establish the relationships between a sphere and its projected ellipse onto an image. These formulations were then utilised to create new methods for: (i) selecting the best pair of images from an image network; (ii) detecting ellipses corresponding to projection of spheres in images; (iii) matching of the detected ellipses between images; and (iv) generating the sphere's geometric digital models. The effectiveness of the proposed method was evaluated under both laboratory and real-world datasets. Laboratory experiments revealed that the proposed process using the best pair of images provided results as accurate as that achieved using eight randomly selected images, while improving computation time by a factor of 50. The results of the two real-world datasets showed that the digital model of a hemispherical dome was generated with 6.2 mm accuracy, while improving the total computation time of current best practice by a factor of 7. Real-world experimentation also showed that the proposed method can provide metric-scale definition for photogrammetric point clouds with 3 mm accuracy using spherical targets. The results suggest that the proposed method was successful in automatically generating fast and accurate geometric digital models of hemispherical domes.},
journal = {J. Comput. Cult. Herit.},
month = {dec},
articleno = {6},
numpages = {21},
keywords = {spherical targets, metric scale definition, hemispherical domes, Sphere detection, sphere projection in images, digital documentation of spheres}
}

@inproceedings{10.1145/3584376.3584534,
author = {Liu, Xin and Wang, Jibin and Qin, Shuwei},
title = {An Anomaly Detection Framework Based on Data Center Operation and Maintenance Data},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584534},
doi = {10.1145/3584376.3584534},
abstract = {Data centers need to monitor various metrics of their different application platforms and applications in real-time. As the system architectures and application services of different application platforms within them become more complex, the requirements for their anomaly detection capabilities are higher. Therefore, this paper proposes an anomaly detection framework based on data center operation and maintenance data. The framework in this paper consists of three parts, including operation and maintenance data cleaning, data feature extraction, and model routing. It is used to select the appropriate model through model routing based on the indicators such as stability and periodicity obtained from data feature extraction of each application platform. At the same time, in order to enhance the expansion capability of the detection algorithm, a cloud-ground hybrid framework is used and a module for algorithm model management is designed to facilitate interaction with the cloud. After testing on SWAT and WADI datasets, the anomaly detection algorithm with the addition of model routing in the framework has good accuracy and recall performance compared to a single anomaly algorithm model, showing advantages in the task of identifying anomalies.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {883–889},
numpages = {7},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3628797.3628985,
author = {Pham, Stefan and Midoglu, Cise and Seeliger, Robert and Arbanowski, Stefan and Steglich, Stephan},
title = {A Novel Approach to Streaming QoE Score Calculation by Integrating Error Impacts},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628985},
doi = {10.1145/3628797.3628985},
abstract = {Video streaming services have become prominent in the last decade. As any other cloud service, these services are error-prone, and the errors during startup and/or playback affect the viewing experience of end-users. Hence, the calculation of Quality-of-Experience (QoE) scores should also account for error impacts. In this paper, we introduce a player-based error classification scheme, which classifies errors based on origin and severity. We use this scheme to quantify the quality degradation due to errors, and propose to improve the QoE score by integrating these quality factors. We instrument the open-source media players dash.js and Exoplayer in our proposed system which follows the guidelines of various multimedia streaming standards. We define several scenarios focusing on different QoE influencing factors, and assess our proposed model’s performance. Comparisons with various state-of-the-art QoE models show that our model captures the effect on user experience better in scenarios induced with player-related errors.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {415–421},
numpages = {7},
keywords = {streaming analytics, CMCD, video players, error classification, QoE, SAND},
location = {<conf-loc>, <city>Ho Chi Minh</city>, <country>Vietnam</country>, </conf-loc>},
series = {SOICT '23}
}

@inproceedings{10.1145/3613424.3614307,
author = {Zhu, Ligeng and Hu, Lanxiang and Lin, Ji and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
title = {PockEngine: Sparse and Efficient Fine-Tuning in a Pocket},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614307},
doi = {10.1145/3613424.3614307},
abstract = {On-device learning and efficient fine-tuning enable continuous and privacy-preserving customization (e.g., locally fine-tuning large language models on personalized data). However, existing training frameworks are designed for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and lack the optimizations for learning on the edge, which faces challenges of resource limitations and edge hardware diversity. We introduce PockEngine: a tiny, sparse and efficient engine to enable fine-tuning on various edge devices. PockEngine supports sparse backpropagation: it prunes the backward graph and sparsely updates the model with measured memory saving and latency reduction while maintaining the model quality. Secondly, PockEngine is compilation first: the entire training graph (including forward, backward and optimization steps) is derived at compile-time, which reduces the runtime overhead and brings opportunities for graph transformations. PockEngine also integrates a rich set of training graph optimizations, thus can further accelerate the training cost, including operator reordering and backend switching. PockEngine supports diverse applications, frontends and hardware backends: it flexibly compiles and tunes models defined in PyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. We evaluated PockEngine on both vision models and large language models. PockEngine achieves up to 15 \texttimes{} speedup over off-the-shelf TensorFlow (Raspberry Pi), 5.6 \texttimes{} memory saving back-propagation (Jetson AGX Orin). Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orin at 550 tokens/s, 7.9 \texttimes{} faster than the PyTorch.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1381–1394},
numpages = {14},
keywords = {neural network, on-device training, sparse update, efficient finetuning},
location = {<conf-loc>, <city>Toronto</city>, <state>ON</state>, <country>Canada</country>, </conf-loc>},
series = {MICRO '23}
}

@article{10.1145/3569471,
author = {Sangar, Yaman and Biradavolu, Yoganand and Pederson, Kai and Ranganathan, Vaishnavi and Krishnaswamy, Bhuvana},
title = {PACT: Scalable, Long-Range Communication for Monitoring and Tracking Systems Using Battery-Less Tags},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3569471},
doi = {10.1145/3569471},
abstract = {The food and drug industry is facing the need to monitor the quality and safety of their products. This has made them turn to low-cost solutions that can enable smart sensing and tracking without adding much overhead. One such popular low-power solution is backscatter-based sensing and communication system. While it offers the promise of battery-less tags, it does so at the cost of a reduced communication range. In this work, we propose PACT - a scalable communication system that leverages the knowledge asymmetry in the network to improve the communication range of the tags. Borrowing from the backscatter principles, we design custom PACT Tags that are battery-less but use an active radio to extend the communication range beyond standard passive tags. They operate using the energy harvested from the PACT Source. A wide-band Reader is used to receive multiple Tag responses concurrently and upload them to a cloud server, enabling real-time monitoring and tracking at a longer range. We identify and address the challenges in the practical design of battery-less PACT Tags using an active radio and prototype them using off-the-shelf components. We show experimentally that our Tag consumes only 23μJ energy, which is harvested from an excitation Source that is up to 24 meters away from the Tag. We show that in outdoor deployments, the responses from an estimated 520 Tags can be received by a Reader concurrently while being 400 meters away from the Tags.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jan},
articleno = {180},
numpages = {27},
keywords = {passive tag, RF harvesting, backscatter, battery-less}
}

@inproceedings{10.1145/3631204.3631859,
author = {Sapin, Etienne and Menon, Suraj and Ge, Jingquan and Habib, Sheikh Mahbub and Heymann, Maurice and Li, Yuekang and Palige, Rene and Byman, Gabriel and Liu, Yang},
title = {Monitoring Automotive Software Security Health through Trustworthiness Score},
year = {2023},
isbn = {9798400704543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631204.3631859},
doi = {10.1145/3631204.3631859},
abstract = {The automotive industry is drastically moving towards autonomous. This trend constitutes in a fundamental change of going from mechanical and electrical engineering towards software-driven approaches. Modern vehicles can embed more than hundred electronic control units (ECUs). As autonomous vehicles require more intelligence as well as more computing power, high-performance computers (HPCs) bring the data management capabilities for cloud and IoT services to support the transition to a service-oriented vehicle system architecture. With this growing reliance on software in vehicles, software reliability and trustworthiness are increasingly critical to vehicle security. Measuring security trustworthiness in automotive software is even more valuable as cybersecurity is shifting to the left, i.e. in the early phase of development and design process. In this article, we propose a novel method for evaluating security trustworthiness of automotive software by leveraging a computational trust model. The method consists of selecting different domains contributing to software security, calculating their respective expectation value (trustworthiness score) and combining it using operators from the computational trust model. We evaluate the method using an automotive use case, i.e. over-the-air (OTA) update software. We describe a possible integration of the proposed method into a solution which would be valuable for cybersecurity stakeholders, e.g. cybersecurity managers, cybersecurity architects and software quality managers, aiming to monitor security health of automotive software throughout its development life cycle.},
booktitle = {Proceedings of the 7th ACM Computer Science in Cars Symposium},
articleno = {1},
numpages = {9},
keywords = {Software health, Data visualization, Trustworthiness},
location = {<conf-loc>, <city>Darmstadt</city>, <country>Germany</country>, </conf-loc>},
series = {CSCS '23}
}

@inproceedings{10.1145/3482632.3487548,
author = {Zhang, Xiaoxiao},
title = {Design and Implementation of College Physical Education Intelligent Management System Based on Big Data Cloud Platform},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487548},
doi = {10.1145/3482632.3487548},
abstract = {For the development of information education in Colleges and universities, the Ministry of education has proposed to adopt the teaching method of big data and cloud platform to improve the teaching level, promote the continuous renewal and development of the traditional education model and integrate the advantages and advantages of online teaching. Take the integrated "cloud platform education mode combining online and offline" as the current normal teaching mode. Based on modern high-tech microelectronics technology, intelligent IC card technology, database technology and network technology, this paper develops an intelligent management system of PE. Through practice, PE has been transformed from traditional decentralized management of teachers to systematic management of sports departments, and from simple qualitative management of objectives or processes to comprehensive management, thus avoiding the phenomenon of inconsistent scale and content of teacher management, getting rid of the defects of insufficient quantity and excessive quality, and strengthening the management level and improving the management efficiency in essence.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2958–2963},
numpages = {6},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3558819.3558836,
author = {Zhang, Yaqiong and Zeng, Wenming and Li, Guanghui and Wen, Yixiao and Yu, Manjiang and Lu, Zhen and Ruan, Hongli and Li, Yuling},
title = {Design and Development of Precise Mango Irrigation Decision-Making System Based on Lora},
year = {2022},
isbn = {9781450397414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558819.3558836},
doi = {10.1145/3558819.3558836},
abstract = {Aiming at the waste of irrigation water resources caused by inaccurate irrigation in mango orchard, a mango accurate irrigation decision-making system based on Lora is designed and developed to improve the yield and quality of mango. The system forms a wireless transmission and remote control network by wireless soil moisture sensor, Lora data transmission terminal, PLC control center, plc4G gateway, etc. Soil moisture data is wirelessly transmitted to the cloud platform in real time, and then is transmitted to the precision irrigation decision-making system through OPC for analysis, processing and storage. The intelligent remote control of the start-stop operation of valves in the irrigation area is realized through the PLC control center. The experiment on system timeliness and accuracy is carried out in mango orchard. According to experimental results, the absolute error between the temperature and humidity data collected by soil moisture sensor and the standard value was small; the measured temperature and humidity data was more accurate, and the accuracy of the data collected by soil moisture sensor was higher; the irrigation decision-making system started quickly as a whole; the instruction delay was small, and the data reporting was relatively rapid; the system had good overall timeliness and accuracy. It could realize the precise irrigation of mango and achieve the purpose of water saving.},
booktitle = {Proceedings of the 7th International Conference on Cyber Security and Information Engineering},
pages = {95–102},
numpages = {8},
keywords = {Lora, Precision irrigation, Cloud platform, Mango, Decision-making system},
location = {<conf-loc>, <city>Brisbane</city>, <state>QLD</state>, <country>Australia</country>, </conf-loc>},
series = {ICCSIE '22}
}

@article{10.1145/3533768,
author = {Wang, Pengfei and Wang, Zixiong and Xin, Shiqing and Gao, Xifeng and Wang, Wenping and Tu, Changhe},
title = {Restricted Delaunay Triangulation for Explicit Surface Reconstruction},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3533768},
doi = {10.1145/3533768},
abstract = {The task of explicit surface reconstruction is to generate a surface mesh by interpolating a given point cloud. Explicit surface reconstruction is necessary when the point cloud is required to appear exactly on the surface. However, for a non-perfect input, such as lack of normals, low density, irregular distribution, thin and tiny parts, and high genus, a robust explicit reconstruction method that can generate a high-quality manifold triangulation is missing.We propose a robust explicit surface reconstruction method that starts from an initial simple surface mesh, alternately performs a Filmsticking step and a Sculpting step of the initial mesh, and converges when the surface mesh interpolates all input points (except outliers) and remains stable. The Filmsticking is to minimize the geometric distance between the surface mesh and the point cloud through iteratively performing a restricted Voronoi diagram technique on the surface mesh, whereas the Sculpting is to bootstrap the Filmsticking iteration from local minima by applying appropriate geometric and topological changes of the surface mesh.Our algorithm is fully automatic and produces high-quality surface meshes for non-perfect inputs that are typically considered to be challenging for prior state of the art. We conducted extensive experiments on simulated scans and real scans to validate the effectiveness of our approach.},
journal = {ACM Trans. Graph.},
month = {oct},
articleno = {180},
numpages = {20},
keywords = {watertight manifold, Surface reconstruction, winding number, restricted Voronoi diagram}
}

@inproceedings{10.1145/3573942.3574093,
author = {Wang, Ziwei and Sun, Wei and Tian, Linyang},
title = {3D Point Cloud Denoising Based on Hybrid Attention Mechanism and Score Matching},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3574093},
doi = {10.1145/3573942.3574093},
abstract = {Due to the limitations of the acquisition equipment, sensors, and the illumination or reflection characteristics of the ground, the acquired point clouds will inevitably be noisy. Noise degrades the quality of point clouds and hinders the subsequent point cloud processing tasks, so the denoising technique becomes a crucial step in point cloud processing. This paper proposes a point cloud denoising algorithm based on a hybrid attention mechanism, which takes into account the complexity of the internal features of point clouds and the randomness of point cloud transformations. Generates channel and spatial attention by parallel maximum pooling and average pooling of point cloud data, trains adaptive attention weights using a multilayer perceptron with shared weights, and serially fuses them, multiplies them with the input features to obtain more robust point cloud features, and connect to the score estimation module using the residuals. By studying and analyzing the mechanism proposed in this paper, it is experimentally demonstrated that the performance of the proposed model under various noise models is vastly improved over the baseline network and outperforms the advanced denoising methods without significantly increasing the network operation cost.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {767–772},
numpages = {6},
keywords = {Point cloud, Hybrid attention module, Denoising, Filtering},
location = {Xiamen, China},
series = {AIPR '22}
}

@article{10.1145/3522742,
author = {Ulvi, Ali},
title = {Using UAV Photogrammetric Technique for Monitoring, Change Detection, and Analysis of Archeological Excavation Sites},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3522742},
doi = {10.1145/3522742},
abstract = {The shrinkage of the sensors installed in unmanned aerial vehicles and the increase in data quality have provided great advantages to UAV users, especially in analysis and interpretation works. Archaeologists, in particular, can take full advantage of new opportunities to research and identify objects and artifacts, using remote sensing methods, by studying the past at excavation sites using modern technologies such as UAVs. These methods enable researchers to discover objects on the ground with the help of sensors. This study includes the UAV monitoring, documentation, and analyses of the excavation works that took place in 2014 (phase 1), 2017 (phase 2- phase 3), and 2020 (phase 4) at the Ancient Theatre of Uzuncabur\c{c} built in the Roman Empire. For this purpose, photos were taken with the UAV for each phase, and measurements were made from the excavation site's points with precision gauges (total-station and GNSS). 3D point cloud, orthophoto map, Digital Elevation Model (DEM) map, and 3D models of each phase were produced with the taken pictures. Since UAV photogrammetry was used in this study, each excavation phase was recorded precisely. This, unlike classical documentation techniques, enabled the deformations in the excavation areas to be revealed. The 3D position accuracy calculated for the control point (ChP) used in the four excavation phases ranges from 5.8 mm to 33.5 mm. The most important feature of this study is the sensitive examination of the changes in the excavation area for many years with the UAV photogrammetry technique. At the end of the study, the excavation phases and the determination of the deformation points in the excavation area were recorded digitally.},
journal = {J. Comput. Cult. Herit.},
month = {sep},
articleno = {58},
numpages = {19},
keywords = {monitoring, excavation analyze, archeological excavation, UAV photogrammetry, Word}
}

@inproceedings{10.1145/3501409.3501571,
author = {Ning, YeYan and Wang, ChunLei and Zhang, ZhenYu and Li, YunJi},
title = {Precise Point Cloud Segmentation Method Based on Distance Judgment Function},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501571},
doi = {10.1145/3501409.3501571},
abstract = {Effective segmentation of point cloud data is an important step in point cloud processing, and it is also a popular research direction in 3D point cloud processing. Traditional region growing algorithms are simple and easy to implement, and are widely used in 3D point cloud segmentation. However, the disorder and complexity of point cloud data and the uncertainty of initial seed node selection lead to over-segmentation and under-segmentation. This paper proposes a region growing algorithm based on distance judgment function calculation. First, we use the octree method to establish the topological relationship of the point cloud data, and construct local k neighborhoods, and eliminate outliers based on its density information; second, we perform k neighborhood search on the data points to obtain the covariance matrix of the neighborhood points, and use principal component analysis to calculate the eigenvalues and eigenvectors of the matrix; we use the minimum spanning tree method to compare the vector dot product, adjust the direction of the normal vector, and ensure the global consistency of the point cloud data; through the average curvature and Gaussian curvature Combined with calculation, the minimum curvature point is selected as the initial seed node, which improves the stability of seed node selection and avoids repeated segmentation; introduces a distance judgment function to judge the attributes of the seed point, calculates the normal distance from the selected seed point to its tangent plane, and passes the distance Threshold divides the point cloud data into flat points and sharp points to improve the efficiency of point cloud adjustment; filter the neighboring points according to the angle between the normal of the seed point and the normal of the neighboring point; finally set the curvature threshold reasonably and determine Guidelines for regional growth. According to the experimental results of segmentation, the region growing algorithm based on the distance judgment function improves the accuracy and stability of part segmentation, and improves the quality of segmentation.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {895–900},
numpages = {6},
keywords = {Distance judgment function, Region growth, Principal component analysis, Three-dimensional point cloud},
location = {Xiamen, China},
series = {EITCE '21}
}

@article{10.1145/3488586,
author = {Wu, Chao and Horiuchi, Shingo and Murase, Kenji and Kikushima, Hiroaki and Tayama, Kenichi},
title = {An Intent-Driven DaaS Management Framework to Enhance User Quality of Experience},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3488586},
doi = {10.1145/3488586},
abstract = {Desktop as a Service (DaaS) has become widely used by enterprises. In 2020, the use of DaaS increased dramatically due to the demand to work remotely from home during the COVID-19 pandemic. The DaaS market is expected to continue growing rapidly [1]. The quality of experience (QoE) of a DaaS service has been one of the main factors to enhance DaaS user satisfaction. To ensure user QoE, the amount of cloud computation resources for a DaaS service must be appropriately designed. We propose an Intent-driven DaaS Management (IDM) framework to autonomously determine the cloud-resource-amount configurations for a given DaaS QoE requirement. IDM enables autonomous resource design by abstracting the knowledge about the dependency between DaaS workload, resource configuration, and performance from previous DaaS performance log data. To ensure the IDM framework's applicability to actual DaaS services, we analyzed five main challenges in applying the IDM framework to actual DaaS services: identifying the resource-design objective, quantifying DaaS QoE, addressing low log data availability, designing performance-inference models, and addressing low resource variations in the log data. We addressed these challenges through detailed designing of IDM modules. The effectiveness of the IDM framework was assessed from the aspects of DaaS performance-inference precision, DaaS resource design, and time and human-resource cost reduction.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {98},
numpages = {25},
keywords = {DaaS, intent-driven management, cloud resource design}
}

@inproceedings{10.1145/3526114.3558724,
author = {Feng, K. J. Kevin and Gao, Alice and Karras, Johanna Suvi},
title = {Towards Semantically Aware Word Cloud Shape Generation},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558724},
doi = {10.1145/3526114.3558724},
abstract = {Word clouds are a data visualization technique that showcases a subset of words from a body of text in a cluster form, where a word’s font size encodes some measure of its relative importance—typically frequency—in the text. This technique is primarily used to help viewers glean the most pertinent information from long text documents and to compare and contrast different pieces of text. Despite their popularity, previous research has shown that word cloud designs are often not optimally suited for analytical tasks such as summarization or topic understanding. We propose a solution for generating more effective visualization technique that shapes the word cloud to reflect the key topic(s) of the text. Our method automates the processes of manual image selection and masking required from current word cloud tools to generate shaped word clouds, better allowing for quick summarization. We showcase two approaches using classical and state-of-the-art methods. Upon successfully generating semantically shaped word clouds using both methods, we performed preliminary evaluations with 5 participants. We found that although most participants preferred shaped word clouds over regular ones, the shape can be distracting and detrimental to information extraction if it is not directly relevant to the text or contains graphical imperfections. Our work has implications on future semantically-aware word cloud generation tools as well as efforts to balance visual appeal of word clouds with their effectiveness in textual comprehension.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {35},
numpages = {5},
keywords = {Text visualization, word clouds, multimodal computer vision},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3482632.3482646,
author = {Cheng, Weiku},
title = {Research on the Value Core and Practice Path Selection of Curriculum Ideology Based on Education Cloud Platform},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482646},
doi = {10.1145/3482632.3482646},
abstract = {The progress of science and technology and the development of computer technology have led us into the information age, and information education has become an inevitable trend in the development of Chinese education. "Curriculum ideology" points to a new concept of ideology, which is an inevitable choice for higher education to realize the whole process and all-round education. The reform of higher vocational curriculum ideology is based on moral education, with the goal of realizing students' all-round growth, emphasizing the organic combination of moral education and intellectual education in the process of education, which is an important measure to solve the "isolated island" dilemma of higher vocational curriculum ideology, build a "great ideology" collaborative education pattern, and train socialist successors who are responsible for national rejuvenation. The emergence of educational cloud platform provides a direction for the reform of the teaching mode of "ideology", and the integration of educational resources is realized by using educational cloud platform to carry out online teaching of "ideology". This paper analyzes the core essence and internal mechanism of "Curriculum ideology" based on educational cloud platform, and puts forward the practical path of synergy effect of curriculum ideology.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {65–69},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3588444.3590997,
author = {Reznik, Yuriy and Barman, Nabajeet and Wagstrom, Patrick},
title = {Improving the Performance of Web-Streaming by Super-Resolution Upscaling},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588444.3590997},
doi = {10.1145/3588444.3590997},
abstract = {In recent years, we have seen significant progress in advanced image and video upscaling techniques, sometimes called super-resolution, or AI-based upscaling. Such algorithms are now broadly available in the forms of software SDKs, as well as functions natively supported by modern graphics cards. However, to take advantage of such technologies in video streaming applications, one needs to (a) add support for super-resolution upscaling in the video rendering chain, (b) develop means for quantifying the effects of using different upscaling techniques on perceived quality, and c) modify streaming clients to use such more advanced scaling techniques in a way that leads to improvements in quality, efficiency, or both.In this paper, we discuss several techniques addressing these challenges. We first present an overview of super-resolution technology. We review available SDKs and libraries for adding super-resolution functionality in streaming players. We next propose a parametric quality model suitable for modeling the effects of different upscaling techniques. We validate it by using an existing widely used dataset with subjective scores. And finally, we present an improved adaptation logic for streaming clients, allowing them to save bandwidth while maintaining quality at the level achievable by standard scaling techniques. Our experiments show that this logic can reduce streaming bitrates by up to 38.9\%.},
booktitle = {Proceedings of the 2nd Mile-High Video Conference},
pages = {8–13},
numpages = {6},
keywords = {quality enhancement, super-resolution, video streaming, machine learning, deep learning, upsampling, adaptive streaming},
location = {Denver, CO, USA},
series = {MHV '23}
}

