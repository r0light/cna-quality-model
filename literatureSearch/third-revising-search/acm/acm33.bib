@inproceedings{10.1145/3578356.3592588,
author = {Christofidi, Georgia and Papaioannou, Konstantinos and Doudali, Thaleia Dimitra},
title = {Toward Pattern-Based Model Selection for Cloud Resource Forecasting},
year = {2023},
isbn = {9798400700842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578356.3592588},
doi = {10.1145/3578356.3592588},
abstract = {Cloud resource management solutions, such as autoscaling and overcommitment policies, often leverage robust prediction models to forecast future resource utilization at the task-, job- and machine-level. Such solutions maintain a collection of different models and at decision time select to use the model that provides the best performance, typically minimizing a cost function. In this paper, we explore a more generalizable model selection approach, based on the patterns of resource usage that are common across the tasks of a job. To learn such patterns, we train a collection of Long Short Term Memory (LSTM) neural networks, at the granularity of a job. During inference, we select which model to use to predict the resource usage of a given task via distance-based time series comparisons. Our experimentation with various time series data representations and similarity metrics reveals cases where even sophisticated approaches, such as dynamic time warping, lead to suboptimal model selection and as a result significantly lower prediction accuracy. Our analysis establishes the importance and impact of pattern-based model selection, and discusses relevant challenges, opportunities and future directions based on our findings.},
booktitle = {Proceedings of the 3rd Workshop on Machine Learning and Systems},
pages = {115–122},
numpages = {8},
keywords = {deep neural network, cloud resource forecasting, long short term memory, cloud computing, timeseries comparison, pattern matching, machine learning},
location = {Rome, Italy},
series = {EuroMLSys '23}
}

@article{10.1145/3577949.3577967,
author = {Loureiro, J. and Cec\'{\i}lio, J.},
title = {Deep Learning for Reliable Communication Optimization on Autonomous Vehicles},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1094-3641},
url = {https://doi.org/10.1145/3577949.3577967},
doi = {10.1145/3577949.3577967},
abstract = {Recent breakthroughs in the autonomous vehicle industry have brought this technology closer to consumers. However, the cost of self-driving solutions still constitutes an entry barrier to many potential users due to its reliance on powerful onboard computers. As an alternative, autonomous driving algorithm processing may be offloaded to remote machines, which requires a reliable connection to the cloud servers. However, despite significant 5G coverage in many countries, mobile network reliability and latency are still inadequate for this purpose. This work explores deep learning concepts to forecast signal quality as a vehicle moves, predicting when periods of degraded network quality will occur. We develop a Long Short-Term Memory (LSTM)-based neural network, trained on multivariate time series containing historical data on several mobile network parameters, and evaluate the results of multi-step Reference Signal Received Power (RSRP) prediction. Results show that our model achieves a rapidly increasing Root-Mean-Square Error (RMSE), reaching over 8 dBm after 25-time steps. This error does not allow for the accurate prediction of future signal quality.},
journal = {Ada Lett.},
month = {dec},
pages = {90–94},
numpages = {5},
keywords = {deep learning, signal quality, autonomous vehicle, forecasting}
}

@inproceedings{10.1145/3511808.3557124,
author = {Pasupuleti, Krishna Kantikiran and Das, Dinesh and Valluri, Satyanarayana R and Zait, Mohamed},
title = {Observability of SQL Hints in Oracle},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557124},
doi = {10.1145/3511808.3557124},
abstract = {Observability is a critical requirement of increasingly complex and cloud-first data management systems. In most commercial databases, this relies on telemetry like logs, traces, and metrics, which helps to identify, mitigate, and resolve issues expeditiously. SQL monitoring tools, for example, can show how a query is performing. One area that has received comparatively less attention is the observability of the query optimizer whose inner workings are often shrouded in mystery. Optimizer traces can illuminate the plan selection process for a query, but they are comprehensible only to human experts and are not easily machine-parsable to remediate sub-optimal plans. Hints are directives that guide the optimizer toward specific directions. While hints can be used manually, they are often used by automatic SQL plan management tools that can quickly identify and resolve regressions by selecting alternate plans. It is important to know when input hints are inapplicable so that the tools can try other strategies. For example, a manual hint may have syntax errors, or an index in an automatic hint may have been accidentally dropped. In this paper, we describe the design and implementation of Oracle's hint observability framework which provides a comprehensive usage report of all hints, manual or otherwise, used to compile a query. The report, which is available directly in the execution plan in a human-understandable and machine-readable format, can be used to automate any necessary corrective actions. This feature is available in Oracle Autonomous Database 19c.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3441–3450},
numpages = {10},
keywords = {SQL plan management, SQL hints, autonomous database administration, observability, query optimization},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3550454.3555443,
author = {Xu, Rui and Wang, Zixiong and Dou, Zhiyang and Zong, Chen and Xin, Shiqing and Jiang, Mingyan and Ju, Tao and Tu, Changhe},
title = {RFEPS: Reconstructing Feature-Line Equipped Polygonal Surface},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555443},
doi = {10.1145/3550454.3555443},
abstract = {Feature lines are important geometric cues in characterizing the structure of a CAD model. Despite great progress in both explicit reconstruction and implicit reconstruction, it remains a challenging task to reconstruct a polygonal surface equipped with feature lines, especially when the input point cloud is noisy and lacks faithful normal vectors. In this paper, we develop a multistage algorithm, named RFEPS, to address this challenge. The key steps include (1) denoising the point cloud based on the assumption of local planarity, (2) identifying the feature-line zone by optimization of discrete optimal transport, (3) augmenting the point set so that sufficiently many additional points are generated on potential geometry edges, and (4) generating a polygonal surface that interpolates the augmented point set based on restricted power diagram. We demonstrate through extensive experiments that RFEPS, benefiting from the edge-point augmentation and the feature preserving explicit reconstruction, outperforms state of the art methods in terms of the reconstruction quality, especially in terms of the ability to reconstruct missing feature lines.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {228},
numpages = {15},
keywords = {feature line, point cloud, restricted power diagram, computer-aided design, surface reconstruction}
}

@inproceedings{10.1145/3581783.3612257,
author = {Tang, Sheng-Ming and Sun, Yuan-Chun and Hsu, Cheng-Hsin},
title = {A Blind Streaming System for Multi-Client Online 6-DoF View Touring},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612257},
doi = {10.1145/3581783.3612257},
abstract = {Online 6-DoF view touring has become increasingly popular due to hardware advances and the recent pandemic. One way for content creators to support many 6-DoF clients is by transmitting 3D content to them, which leads to content leakage. Another way for content creators is to render and stream novel views for 6-DoF clients, which incurs staggering computational and networking workloads. In this paper, we develop a blind streaming system that leverages cloud service providers between content creators and 6-DoF clients. Our system has two core design objectives: (i) to generate high-quality novel views for 6-DoF clients without retrieving 3D content from content creators, (ii) to support many 6-DoF clients without overloading the content creators. We achieve these two goals in the following steps. First, we design a source view request/response interface between cloud service providers and content creators for efficient communications. Second, we design novel view optimization algorithms for cloud service providers to intelligently select the minimal set of source views while considering the workload of content creators. Third, we employ scalable client side view synthesis for 6-DoF clients with heterogeneous device capabilities and personalized 6-DoF client poses and preferences. Our evaluation results demonstrate the merits of our solution, compared to the state-of-the-arts, our system: (i) improves synthesized novel views by 2.27 dB in PSNR and 12 in VMAF on average and (ii) reduces the bandwidth consumption by 94\% on average. In fact, our solution approaches the performance of an unrealistic optimal solution with unlimited source views, achieving performance gaps as small as 0.75 dB in PSNR and 3.8 in VMAF.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9124–9133},
numpages = {10},
keywords = {system design, discrete optimization, content privacy, computer graphics, view synthesis},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{10.1145/3510450.3517285,
author = {Hadar, Ravid and Schapira, Michael},
title = {Network Congestion Control and Its Impact on Video Streaming QoE},
year = {2022},
isbn = {9781450392228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510450.3517285},
doi = {10.1145/3510450.3517285},
abstract = {Congestion control plays a crucial role in Internet-based content delivery. Congestion control brings order to the Internet's crowded traffic system by sharing the scarce network bandwidth between competing services and users. Congestion control algorithms continuously modulate the rate at which data packets are injected into the network by traffic sources in response to network conditions.Congestion control immensely impacts quality of experience (QoE) for services like video streaming, video conferencing, and cloud gaming; sending packets too slowly prevents supporting high video quality (HD/UHD); sending too fast can overwhelm the network, resulting in data being lost or delayed, leading to phenomena such as video rebuffering.While congestion control has been a key focus for both academic and industrial research for decades, the exact correlation between the performance of the congestion control algorithms employed by video servers and the QoE experienced by video clients remains poorly understood. We will report on our experimental results along these lines.We evaluated and contrasted three dominant congestion control schemes: TCP Cubic [3], which is the default for many operating systems, and two recently proposed congestion control schemes, namely, Google's Bottleneck-Bandwidth-and-RTT (BBR) [1] protocol, and Performance-oriented Congestion Control (PCC) [2].Our experimental setup consisted of a video cache that sends http-based video traffic across an emulated network environment towards a video client. We took into consideration both MPEG-DASH and HLS-based video streaming and both wired and wireless networks. We ran multiple experiments for varying network conditions (e.g., the available bandwidth, non-congestion-related packet loss, network latency, and depth of in-network buffers, etc.).By monitoring the behavior of the congestion controller and examining the QoE data from the video player (e.g., video start-time, average bitrate, rebuffering ratio, etc.), we have been able to draw meaningful conclusions. Specifically, our results shed light on the features of network-level performance that most impact user-perceived QoE, quantify the benefits for performance of employing modern congestion control protocols, and provide insights into the interplay between congestion control, the network environment, and the video player.Below is a diagram describing the experiment setup:},
booktitle = {Proceedings of the 1st Mile-High Video Conference},
pages = {111},
numpages = {1},
keywords = {quality of experience, online learning, congestion control, transport protocols, QoE, video streaming},
location = {Denver, Colorado},
series = {MHV '22}
}

@inproceedings{10.1145/3487553.3524628,
author = {Gregoriadis, Marcel and Muth, Robert and Florian, Martin},
title = {Analysis of Arbitrary Content on Blockchain-Based Systems Using BigQuery},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524628},
doi = {10.1145/3487553.3524628},
abstract = {Blockchain-based systems have gained immense popularity as enablers of independent asset transfers and smart contract functionality. They have also, since as early as the first Bitcoin blocks, been used for storing arbitrary contents such as texts and images. On-chain data storage functionality is useful for a variety of legitimate use cases. It does, however, also pose a systematic risk. If abused, for example by posting illegal contents on a public blockchain, data storage functionality can lead to legal consequences for operators and users that need to store and distribute the blockchain, thereby threatening the operational availability of entire blockchain ecosystems. In this paper, we develop and apply a cloud-based approach for quickly discovering and classifying content on public blockchains. Our method can be adapted to different blockchain systems and offers insights into content-related usage patterns and potential cases of abuse. We apply our method on the two most prominent public blockchain systems—Bitcoin and Ethereum—and discuss our results. To the best of our knowledge, the presented study is the first to systematically analyze non-financial content stored on the Ethereum blockchain and the first to present a side-by-side comparison between different blockchains in terms of the quality and quantity of stored data.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {478–487},
numpages = {10},
keywords = {Cryptocurrency, BigQuery, Blockchain, Ethereum, Bitcoin},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3605573.3605643,
author = {Zhang, Songli and Zheng, Zhenzhe and Wu, Fan and Li, Bingshuai and Shao, Yunfeng and Chen, Guihai},
title = {Learning From Your Neighbours: Mobility-Driven Device-Edge-Cloud Federated Learning},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605573.3605643},
doi = {10.1145/3605573.3605643},
abstract = {Federated learning (FL) in large-scale wireless networks is implemented in a hierarchical way by introducing edge servers as relays between the cloud server and devices, where devices are dispersed within multiple clusters coordinated by edges. However, the devices are usually mobile users with unpredictable mobile trajectories, whose effects on the model training process are still less studied. In this work, we propose a new MobIlity-Driven feDerated LEarning framework, namely MIDDLE in wireless networks, which can relieve unbalanced and biased model updates by leveraging the new model aggregation opportunities on mobile devices due to their mobility across edges. Specifically, mobile devices can have different models while traversing across edges, and adequately aggregate these models on the device. By theoretical analysis, we can show that this on-device model aggregation can reduce the bias of model updating on edges and cloud, and then accelerate the convergence of model training in FL. Then, we define a model similarity utility to measure the difference in gradient updates among various models, which guides the adaptive on-device model aggregation and in-edge device selection to facilitate the comprehensive information sharing between edges. Extensive experiment results validate that MIDDLE can achieve 1.51 \texttimes{} −6.85 \texttimes{} speedup on the model training, compared with the state-of-the-art model training approaches in hierarchical FL.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing},
pages = {462–471},
numpages = {10},
keywords = {Device Mobility., Device-Edge-Cloud Cooperation, Federated Learning},
location = {<conf-loc>, <city>Salt Lake City</city>, <state>UT</state>, <country>USA</country>, </conf-loc>},
series = {ICPP '23}
}

@article{10.1145/3550454.3555497,
author = {Kopanas, Georgios and Leimk\"{u}hler, Thomas and Rainer, Gilles and Jambon, Cl\'{e}ment and Drettakis, George},
title = {Neural Point Catacaustics for Novel-View Synthesis of Reflections},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555497},
doi = {10.1145/3550454.3555497},
abstract = {View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {201},
numpages = {15},
keywords = {neural rendering, catacaustics, point-based rendering, differentiable rasterization, reflections}
}

@article{10.1109/TNET.2021.3103796,
author = {Ma, Richard T. B.},
title = {Internet Transport Economics: Model and Analysis},
year = {2021},
issue_date = {Dec. 2021},
publisher = {IEEE Press},
volume = {29},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3103796},
doi = {10.1109/TNET.2021.3103796},
abstract = {With the rise of video streaming and cloud services, the Internet has evolved into a content-centric service platform. Due to the best-effort service model of the Internet, the quality of service (QoS) of Internet services however cannot be guaranteed. Furthermore, characterizing QoS is challenging since it depends on the autonomous business decisions such as capacity planning, routing strategies and peering agreements of network providers. To quantify the QoS for Internet-based services, we regard the Internet infrastructure as a transport system for data packets and study the Internet ecosystem and the economics of transport services collectively provided by the autonomous network providers. In contrast to the traditional transport economics that studies the movement of people and goods over space and time, our focus in the &lt;italic&gt;Internet transport economics&lt;/italic&gt; is the movement of streams of data packets that create information services. In particular, we model the supply of network capacities and demands of throughput driven by network protocols and establish a macroscopic network equilibrium under which both the end-to-end delays and drop rates of Internet routes can be derived. We show that this equilibrium solution always exists and its uniqueness can be guaranteed under various realistic scenarios. We analyze the impacts of user demands and resource capacities on the network equilibrium and provide implications of Netflix-Comcast type of peering on the QoS of users. We demonstrate that our framework can be used as a building block to understand the routing strategies under a Wardrop equilibrium and to enable further studies such as Internet peering and in-network caching.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {2843–2854},
numpages = {12}
}

@inproceedings{10.1145/3542929.3563502,
author = {Wang, Haodong and Du, Kuntai and Jiang, Junchen},
title = {Minimizing Packet Retransmission for Real-Time Video Analytics},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3542929.3563502},
doi = {10.1145/3542929.3563502},
abstract = {In smart-city and video analytics (VA) applications, high-quality data streams (video frames) must be accurately analyzed with a low delay. Since maintaining high accuracy requires compute-intensive deep neural nets (DNNs), these applications often stream massive video data to remote, more powerful cloud servers, giving rise to a strong need for low streaming delay between video sensors and cloud servers while still delivering enough data for accurate DNN inference. In response, many recent efforts have proposed distributed VA systems that aggressively compress/prune video frames deemed less important to DNN inference, with the underlying assumptions being that (1) without increasing available bandwidth, reducing delays means sending fewer bits, and (2) the most important frames can be precisely determined before streaming. This short paper challenges both views. First, in high-bandwidth networks, the delay of real-time videos is primarily bounded by packet losses and delay jitters, so reducing bitrate is not always as effective as reducing packet retransmissions. Second, for many DNNs, the impact of missing a video frame depends not only on itself but also on which other frames have been received or lost. We argue that some changes must be made in the transport layer, to determine whether to resend a packet based on the packet's impact on DNN's inference dependent on which packets have been received. While much research is needed toward an optimal design of DNN-driven transport layer, we believe that we have taken the first step in reducing streaming delay while maintaining a high inference accuracy.},
booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
pages = {340–347},
numpages = {8},
keywords = {systems for machine learning, action recognition, transport layer protocol, video analytics},
location = {San Francisco, California},
series = {SoCC '22}
}

@inproceedings{10.1145/3512388.3512401,
author = {Guo, Shaogang and Xu, Yunfei and Li, Wang},
title = {Spatial Non-Cooperative Target Point Cloud Reconstruction},
year = {2022},
isbn = {9781450395465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512388.3512401},
doi = {10.1145/3512388.3512401},
abstract = {Due to the inherent defects of laser sensors, the original point cloud of non-cooperate targets are usually irregularly distributed, which brings great challenges to high-quality 3D surface reconstruction of non-cooperate targets. In this paper, we leverage a method based local hierarchical clustering (LHC) to improve the consistency of point distribution. Specifically, our method includes two main steps. The first one is the adaptive octree-based 3D spatial decomposition and the second one is hierarchical clustering. The main purpose of the former one is to reduce the complexity of the algorithm, and the later aims to convert the non-uniform point set to uniform one. We carried out experiments on three non-cooperative target models. The results of visualization and quantitative calculation verify the effectiveness of our method.},
booktitle = {Proceedings of the 2022 5th International Conference on Image and Graphics Processing},
pages = {84–88},
numpages = {5},
keywords = {Point cloud, Non-cooperative target, 3D reconstruction},
location = {Beijing, China},
series = {ICIGP '22}
}

@article{10.1145/3517805,
author = {Yinying, Cai and Li, Juan and Wang, Bo},
title = {Data Mining Techniques and Machine Learning Algorithms in the Multimedia System to Enhance Engineering Education},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3517805},
doi = {10.1145/3517805},
abstract = {In the current digital era, engineering education worldwide faces a massive challenge in education and career development. By authorizing educators and administrators to migrate to the actions, cloud services technology has transformed into the educational environment. A Multimedia assisted smart learning system (MSLS) has been suggested in this paper where universities/colleges will advocate future development and begin skill-set enhancement courses by e-learning. To classify their employment prospects at the early stage of graduation, this proposed system measures learners' academic/skill data. Machine learning and Data mining are advanced research fields whose accelerated advancement is attributable to developments in data processing research, database industry growth, and business requirements for methods capable of extracting useful information from massive data stores. In addition, for skill set evaluation, a practical algorithm is suggested to find different groups of students that lack the appropriate skill set. The anticipated student groups can be provided with opportunities by e-learning to enhance their required skill set. The findings suggest that more critical choices can boost employment prospects and overall educational development by implementing the new engineering education system.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {112},
numpages = {21},
keywords = {multimedia system, Machine learning, data mining, engineering education}
}

@inproceedings{10.1145/3517206.3526269,
author = {B\"{a}urle, Simon and Mohan, Nitinder},
title = {ComB: A Flexible, Application-Oriented Benchmark for Edge Computing},
year = {2022},
isbn = {9781450392532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517206.3526269},
doi = {10.1145/3517206.3526269},
abstract = {Edge computing is an attractive platform where applications, previously hosted in the cloud, shift parts of their workload on resources closer to the users. The field is still in its nascent stages with significant ongoing innovation in small form-factor hardware designed to operate at the edge. However, the increased hardware heterogeneity at the edge makes it difficult for application developers to determine if their workloads will operate as desired. Simultaneously, edge providers have to make expensive deployment choices for the "correct" hardware that will remain suitable for the near future. We present ComB, an application-oriented benchmarking suite for edge that assists early adopters in evaluating the suitability of an edge deployment. ComB is flexible, extensible, and incorporates a microservice-based video analytics pipeline as default workload to measure underlying hardware's compute and networking capabilities accurately. Our evaluation on a heterogeneous testbed shows that ComB enables both providers and developers to understand better the runtime capabilities of different hardware configurations for supporting operations of applications designed for the edge.},
booktitle = {Proceedings of the 5th International Workshop on Edge Systems, Analytics and Networking},
pages = {19–24},
numpages = {6},
keywords = {edge computing, benchmarking, next-generation applications},
location = {Rennes, France},
series = {EdgeSys '22}
}

@inproceedings{10.1145/3538969.3539012,
author = {Ardagna, Claudio A. and Bena, Nicola and de Pozuelo, Ramon Mart\'{\i}n},
title = {Bridging the Gap Between Certification and Software Development},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3539012},
doi = {10.1145/3538969.3539012},
abstract = {While certification is widely recognized as a means to increase system trustworthiness and reduce uncertainty in decision making, it faces severe challenges preventing a wider adoption thereof. Certification is not adequately planned and integrated within the development process, leading to suboptimal scenarios where certification introduces the need to further modify the developed system with high costs. We propose a methodology that bridges the gap between software development and certification processes. Our methodology automatically produces the certification requirements driving all steps of the development process, and maximizes the strength of certificates while taking costs under control. We formalize the above problem as a multi-objective mathematical program and solve it through a genetic algorithm. The proposed approach is tested in a real-world, cloud-based financial scenario at CaixaBank and its performance and quality is evaluated in a simulated scenario.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {19},
numpages = {10},
keywords = {Software Development, Security, Certification},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.1145/3604930.3605711,
author = {Maji, Diptyaroop and Pfaff, Ben and P R, Vipin and Sreenivasan, Rajagopal and Firoiu, Victor and Iyer, Sreeram and Josephson, Colleen and Pan, Zhelong and Sitaraman, Ramesh K},
title = {Bringing Carbon Awareness to Multi-Cloud Application Delivery},
year = {2023},
isbn = {9798400702426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604930.3605711},
doi = {10.1145/3604930.3605711},
abstract = {Data centers consume roughly 1--2\% of the world's electricity, with the majority of it attributed to compute, making the computing industry a substantial source of greenhouse gas emissions. Resources in data centers typically focus on providing high performance and availability, but the question of sustainability in managing these distributed resources often goes unnoticed over these other metrics. This problem will only exacerbate as the data center computing demand continues to increase.In this paper, we address the sustainability aspect of load balancing in VMware's Avi Global Server Load Balancer (GSLB). Our GSLB deployment spans data centers across geographies and clouds and relies on geographical proximity to shift client application requests to the closest data center. In this work, we enhance the GSLB service to additionally consider the real-time carbon intensity at each data center as a factor in making a load-balancing choice. Our carbon-aware prototype shows an average of 21\% and a maximum of 51\% reduction in carbon emissions while operating with an acceptable latency.},
booktitle = {Proceedings of the 2nd Workshop on Sustainable Computer Systems},
articleno = {6},
numpages = {6},
keywords = {marginal carbon intensity, data center computing, spatial load balancing, stateless workloads},
location = {Boston, MA, USA},
series = {HotCarbon '23}
}

@inproceedings{10.5555/3581644.3581689,
author = {Kuran, Mehmet \c{S}\"{u}kr\"{u} and K\"{o}ksal, O≈guz Kaan and Kili\c{c}, Melih and undefinedlter, Ahmet U≈gur and Nehas, G\"{o}k\c{c}e Ekin and \"{O}zt\"{u}rk, Sadik},
title = {Forecasting-Based Cloud-Assisted Dynamic Channel Assignment Mechanism for Mesh WiFi Networks},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {In this paper, we propose a cloud-assisted dynamic channel assignment system for WiFi mesh networks considering both the 2.4 GHz and 5 GHz interfaces to increase the overall performance and user experience in the WiFi network. Our solution utilizes periodic interference level measurements by the access points (AP) in all possible channels via conducting clear channel assessments. These measurements are sent to and processed by a cloud component with a forecasting module that predicts the state of each applicable channel in the near future. Finally, a channel change decision is sent to each AP if there is a better channel than its operating channel in the near future.We have conducted numerous field trials for a good selection of the various key parameters of the system with both the overall system's performance and impact over time-sensitive critical applications such as real-time applications in mind. We have also conducted a field trial of our proposed system over a large real-life population of fifty thousand APs and compared its performance against the widely deployed Least Congested Channel Search (LCCS) mechanism. Our results show that not only our mechanism outperforms LCCS in terms of operating channel interference level but achieves this goal with much less number of channel changes yielding a much less disruptive user experience.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {36},
numpages = {9},
keywords = {channel assignment, wifi, cloud, IEEE 802.11, forecasting},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@inproceedings{10.1145/3540250.3558958,
author = {Shetty, Manish and Bansal, Chetan and Upadhyayula, Sai Pramod and Radhakrishna, Arjun and Gupta, Anurag},
title = {AutoTSG: Learning and Synthesis for Incident Troubleshooting},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558958},
doi = {10.1145/3540250.3558958},
abstract = {Incident management is a key aspect of operating large-scale cloud services. To aid with faster and efficient resolution of incidents, engineering teams document frequent troubleshooting steps in the form of Troubleshooting Guides (TSGs), to be used by on-call engineers (OCEs). However, TSGs are siloed, unstructured, and often incomplete, requiring developers to manually understand and execute necessary steps. This results in a plethora of issues such as on-call fatigue, reduced productivity, and human errors. In this work, we conduct a large-scale empirical study of over 4K+ TSGs mapped to incidents and find that TSGs are widely used and help significantly reduce mitigation efforts. We then analyze feedback on TSGs provided by 400+ OCEs and propose a taxonomy of issues that highlights significant gaps in TSG quality. To alleviate these gaps, we investigate the automation of TSGs and propose AutoTSG -- a novel framework for automation of TSGs to executable workflows by combining machine learning and program synthesis. Our evaluation of AutoTSG on 50 TSGs shows the effectiveness in both identifying TSG statements (accuracy 0.89) and parsing them for execution (precision 0.94 and recall 0.91). Lastly, we survey ten Microsoft engineers and show the importance of TSG automation and the usefulness of AutoTSG.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1477–1488},
numpages = {12},
keywords = {Meta Learning, Incident Management, Troubleshooting, Program Synthesis, Cloud Reliability},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3524273.3535781,
author = {O'Sullivan, Samantha and Murray, Niall and Rodrigues, Thiago Braga},
title = {A Telehealth and Sensor-Based System for User-Centered Physical Therapy in Parkinson's Disease: Research Proposal},
year = {2022},
isbn = {9781450392839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524273.3535781},
doi = {10.1145/3524273.3535781},
abstract = {This paper contains the research proposal of Samantha O'Sullivan that was presented at the MMSys 2022 doctoral symposium. The use of wearable sensors for the understanding and quantification of movement within research communities working on Parkinson's Disease (PD) has increased significantly in recent years with a motivation to objectively diagnose, assess and then understand the progression of the disease. Most studies taking this approach for PD have stated that there is a need for a long-term solution, due to varying symptoms at different stages of the disease. COVID-19 has brought further limitations in the delivery of clinical care, reducing time with therapists and doctors whilst increasing the preference for at-home care. The necessity for a system for patients with PD is extremely significant. There is no clinically available long-term assessment for tremors, which is an issue highlighted in the literature. By using wireless sensors to track tremor severity continuously, and telehealth to create communication between patient and clinician, this proposed system will allow for better targeted therapy, accurate statistics, and constant accessible data. In this context, this work will design, build, and evaluate a novel system that would allow for constant monitoring of a patient with tremors. By using wireless sensors and telehealth, it will provide more detailed data that may enable directed and informed physical therapy. It will also improve communication creating a data flow constantly between clinician and patient to improve person-centered feedback, and aid towards the diagnosis and assessment of disease progression. The incorporation of a mobile/cloud-based application to assist this is due to the current heightened preference for home-based healthcare, long-term evaluation of tremors and personalized physical therapy. The primary focus of the PhD will be on capturing tremor activity and progression through a telehealth-based system. This proposed system will obtain real-time readings of tremors using wireless sensors and an application that will communicate consistently with healthcare professionals. The aim will be to provide better home-based care, person-centered physical therapy and improve quality of life.},
booktitle = {Proceedings of the 13th ACM Multimedia Systems Conference},
pages = {383–387},
numpages = {5},
keywords = {sensors, rehabilitation, quantitative motor assessment, telehealth, Parkinson's disease},
location = {Athlone, Ireland},
series = {MMSys '22}
}

@inproceedings{10.1145/3587828.3587866,
author = {Bimenyimana, Emmanuel and Nsengiyumva, Philibert and Ngoga, Said Rutabayiro},
title = {IoT Monitoring and Control System of Distribution Transformers in Rwanda},
year = {2023},
isbn = {9781450398589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587828.3587866},
doi = {10.1145/3587828.3587866},
abstract = {In developing countries, many customers do not get good quality of electricity power supply due to frequent and prolonged power fluctuations/cuts. Distribution transformer&nbsp;(DT) is a&nbsp;service transformer&nbsp;that provides the final&nbsp;voltage&nbsp;transformation in the&nbsp;electric power distribution&nbsp;system, stepping down the voltage to the level used by the customers. Monitoring and control of DT (as a crucial and expensive asset in the network) is a key enabler of power stability to consumers. A power utility is said to be a business oriented with good image representation if it delivers a reliable and affordable electricity. Modern technologies such as the Internet of Things (IoT) offer a wide range of applications in the energy sector to smoothly monitor, control and optimize processes. Currently, many energy companies in developing countries are not yet implementing the remote system to control and monitor the secondary side of DTs and timely get the notifications of fluctuations/abnormalities occurred on those DTs. That is why it is still challenging and time consuming to intervene urgently and do the necessary actions to prevent severe and prolonged power cuts/fluctuations and safeguard the damage of DTs themselves with customer's appliances connected on those DTs. The secondary side of DT is the one connected directly and supply power to the customers. For this reason, we developed an affordable IoT system that automatically detects the abnormalities/fluctuations of three core technical parameters of DT (which are voltage, current and temperature) using current sensors, voltage sensors, and temperature sensor with ATmega 328P Microcontroller to collect and process data from sensors connected to DT system. Once one or all of those technical parameters become abnormal, the system cut off automatically the secondary side of DT in 2 seconds to isolate and protect the customers’ load with safeguarding DT itself using a power relay. At the same time, GSM/GPRS module uploads the sensed abnormal data to the cloud storage, displays them on web-based application for visualization, and sends the corresponding short message service (sms) to notify the issue to the authorized person in 5 seconds for speeding up the interventions and power restoration. In case there is a movement related to the vandalism in the compound of DT, a PIR sensor detects the human motion then a camera takes the related picture and send it to the utility with the corresponding sms. A buzzer generates an audio signaling to warn the culprit/criminal until he left the site. If there is no abnormality detected, the system keeps sensing without sending the data to the cloud. We can open and close remotely the secondary side of DT and buzzer. This system is powered using a rechargeable battery.},
booktitle = {Proceedings of the 2023 12th International Conference on Software and Computer Applications},
pages = {253–259},
numpages = {7},
keywords = {voltage, Internet of Things (IoT), current, temperature, distribution transformer},
location = {<conf-loc>, <city>Kuantan</city>, <country>Malaysia</country>, </conf-loc>},
series = {ICSCA '23}
}

@inproceedings{10.1145/3588444.3591024,
author = {Sekar, Santhoshini and Mishra, Ashok Kumar and Giladi, Alex and Grois, Dan},
title = {Novel Motion-Compensated Spatio-Temporal Filtering Scheme for X265 Open-Source Video Encoder},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588444.3591024},
doi = {10.1145/3588444.3591024},
abstract = {There is a strong demand to decrease the video transmission bitrate without reducing visual quality [1]. The x265 encoder [2]-[4] is a popular open-source encoder, which generates bitstreams compliant with the H.265/MPEG-HEVC video coding standard [5]. Built on top of x264[6], the x265 encoder is integrated into several popular open-source frameworks, such as ffmpeg [7], GStreamer [8], and Handbrake [9]. In addition, the x265 is used by a variety of broadcast and streaming service providers who leverage the benefits of HEVC for streaming live and over-the-top (OTT) content. In addition to implementing nearly all the tools defined in HEVC, it implements many algorithmic optimizations that enable trading off encoder performance for quality [2]-[4]. The performance-critical kernels are implemented with hand-coded assembly kernels that use AVX2 and AVX-512 single instruction, multiple data instructions to improve performance on x86 CPUs. This flexible architecture of x265 makes it a popular choice for HEVC encoding for both on-premises and cloud services.Recent x265 development efforts have been focused on further improving the coding gains. Specifically, the motion compensated spatio-temporal filtering (MCSTF) employed within the coding loop is especially useful for pictures that contain a high level of noise. It utilizes previously generated motion vectors across different video content resolutions to find the best temporal correspondence for low-pass filtering, while the temporal filtering is applied to the I- and P-frames. Figure 1 schematically illustrates the motion estimation process for temporal filtering in a temporal window, which consists of 5 adjacent pictures: two past, two future and one central picture used for producing a single filtered picture. Motion estimation is applied between the central picture and each future or past picture, thereby generating multiple motion-compensated predictions, which are then combined by using adaptive filtering to produce a final noise-reduced picture. Thus, a hierarchical motion estimation scheme is employed (layers L0, L1 and L2, are illustrated in Figure 2). Subsampled pictures are generated for all reference picturesand the original picture as well: i.e., L1, while L2 is derived from L1 by using the same subsampling method. First, the motion estimation is done for each 16x16 block in L2. Then, the selected motion vector is used as an initial value for estimating the motion in L1. After that, the same is performed for estimating the motion in L0.As a final step, the subpixel motion is estimated for each 8x8 block by using an interpolation filter on L0. Particularly, the motion of reference pictures before and after, relative to the original picture, is estimated per the 8x8 picture block. In turn, the motion compensation is applied on the pictures before and after the original picture according to the best matching motion for each block. i.e., such that pixel coordinates of the original picture in each block have the best matching coordinates within the referenced pictures. The filter is then applied to the current pixels, and after that, the filtered picture is encoded. Note that the pixels are processed one by one for the luma and chroma channels. The new sample value, is calculated by using the following equation:[EQUATION]where Io is the original pixel, Ir(i) is the intensity of the corresponding pixel within the motion compensated picture i, and wr(i, a) is the weight of the motion compensated picture where a is the number of available motion compensated pictures. The conducted extensive experimental results show significant bit-rate savings in terms of BD-BR [10].},
booktitle = {Proceedings of the 2nd Mile-High Video Conference},
pages = {126–127},
numpages = {2},
keywords = {H.265, computational complexity, HEVC, coding gain, x265, HM, open-source, MCSTF},
location = {Denver, CO, USA},
series = {MHV '23}
}

@inproceedings{10.1145/3592813.3592898,
author = {Silva J\'{u}nior, Paulo Freitas and Fran\c{c}a, Tiago Cruz and Sampaio, Jonice Oliveira},
title = {CARAMEL: Ecosystem for Big Social Data},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592813.3592898},
doi = {10.1145/3592813.3592898},
abstract = {Context: A large volume of data produced in social media is analyzed through different perspectives. Much effort goes into retrieving and processing the data, maintaining the necessary infrastructure, and building and sharing the foundation between actors with different roles. These challenges are observed in data ecosystems. Problem: The central systems to support data analysis from social networks have some restrictions (data collection, sharing, reuse, etc.). Data collection and analysis require technical skills that some users need and do not have, impacting the quality of inferences, accounting, and conclusions. Solution: We propose an architecture for “Big Social Data” ecosystems considering the collaborative construction of data extraction and sharing mechanisms. IS Theory: This proposal is related to “knowledge-based theory,” as much knowledge can be inferred from social data. It also supports the Externalization and Combination steps of the Organizational knowledge creation model. Method: We observe aspects related to data analysis, considering the reuse of the mechanisms created and the sharing of bases that can run and be stored in a distributed way to meet even instantaneous analysis. Results: The architecture was implemented to work in a distributed way, contains a collector and a filter and allows data sharing. A data collection test was conducted during the 2022 presidential elections in Brazil. Contributions: The main contribution is the architecture of a Big Social Data Ecosystem, focused on the evolution of social data analysis that also observes the interoperability between distributed solutions. The technological contributions are an instance of this architecture for the cloud, social media data collectors, and datasets of the 2022 election in Brazil.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Information Systems},
pages = {136–142},
numpages = {7},
keywords = {Social networks., Social data analysis, Microservices, Data ecosystem, Cloud computing},
location = {<conf-loc>, <city>Macei\'{o}</city>, <country>Brazil</country>, </conf-loc>},
series = {SBSI '23}
}

@inproceedings{10.1145/3583133.3595841,
author = {Syu, Yang and Fanjiang, Yong-Yi},
title = {Multi-Step-Ahead Web Service QoS Time Series Forecasting: A Multi-Predictor-Based Genetic Programming Approach},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3595841},
doi = {10.1145/3583133.3595841},
abstract = {Previously, in a GECCO 2022 Hot-off-the-Press paper [1], we presented a comprehensive survey of the modeling and prediction of Web service (WS) quality of service (QoS) time series [2]. Based on the exhaustive investigation in [2], this research subject has already been deeply and widely studied for over a decade; for the one-step-ahead version of this problem, which can be considered its most primitive problem form, overall, our proposed and developed genetic programming (GP)-based solution outperforms competitors in terms of both modeling and forecasting accuracy, according to our ongoing study, which has been reported in [3] [4] [5]. Nevertheless, as argued in [6], for the long-term use and rental of cloud-based WSs, multi-step-ahead QoS time series prediction of these services is needed. Thus, the authors employed and revised the two most widely used single-predictor-based time series methods, namely, autoregressive integrated moving average (ARIMA) models and exponential smoothing (ES), to address this latest version of the problem.For this multi-step-ahead variant of the problem, in Y. Y. Fanjiang, Y. Syu and W. L. Huang, "Time Series QoS Forecasting for Web Services Using Multi-Predictor-based Genetic Programming", IEEE Transactions on Services Computing (TSC), Vol. 15, P.P. 1423--1435, 2022, we devise and employ a multipredictor-based approach to genetic programming. First, due to its superiority in our past work for the basic (i.e., one-step-ahead) version of the problem, we investigate the performance of GP on this newly emerged (multi-step-ahead) form of the problem. Second, instead of using a single model for predictions regarding multiple future time points, which is the method commonly adopted in prior research [2], we evolve and apply a dedicated GP-generated predictor for each targeted future time point and its projection. Furthermore, two different strategies for the consumed predictor inputs are tested to determine their differences and influence on accuracy so that a better strategy can be empirically determined. In addition, we propose in the reported paper [7] two disparate techniques to further enhance the resulting performance of our multi-predictor-based GP method.As in our previous GECCO Hot-off-the-Press paper [1], this abstract paper presents to the GECCO community a verified application of GP on a more difficult and challenging type of WS QoS time series forecasting. Our purpose is to enable the GECCO community to use this application of GP, to try to improve GP to obtain more accurate and better results, and to investigate other potential evolutionary paradigms and techniques for this issue.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {43–44},
numpages = {2},
keywords = {genetic programming, time series forecasting, service-oriented software engineering, machine learning, web services},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3566099.3569006,
author = {Lin, Hai and Chen, Xianfu},
title = {Transformer-Driven Multi-Agent Deep Reinforcement Learning Based Point Cloud Video Transmissions},
year = {2022},
isbn = {9781450397841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3566099.3569006},
doi = {10.1145/3566099.3569006},
abstract = {The point cloud videos, a medium for representing natural content in AR/VR with point clouds, have attracted a wide range of attention for its characteristics and have the potential to be the next generation of video technology. Given the high data volume, the point cloud video raises the challenge of intelligent transmission and resource scheduling in multi-user scenarios under time-varying system conditions. In this paper, we propose a multi-agent deep reinforcement learning (DRL) approach to optimize the expected long-term multi-user QoE and adopt a Field of View (FoV) prediction model with Transformer for high-accuracy FoV prediction. Over the time horizon, the proposed approach learns to select the tiles of the corresponding video in accordance with a proposed well-defined QoE model capable of quantifying users' satisfaction for transmissions in an iterative way. Under various settings, extensive numerical experiments based on real throughput data traces and different computation capabilities data demonstrate that the proposed approach is effective for long-term multi-agent point cloud video transmissions.},
booktitle = {Proceedings of the 1st Workshop on Digital Twin \&amp; Edge AI for Industrial IoT},
pages = {25–30},
numpages = {6},
keywords = {quality of experience, point cloud video, deep reinforcement learning, transformer},
location = {Sydney, NSW, Australia},
series = {AIIOT '22}
}

@article{10.1145/3478513.3480486,
author = {Liu, Yanchao and Guo, Jianwei and Benes, Bedrich and Deussen, Oliver and Zhang, Xiaopeng and Huang, Hui},
title = {TreePartNet: Neural Decomposition of Point Clouds for 3D Tree Reconstruction},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3478513.3480486},
doi = {10.1145/3478513.3480486},
abstract = {We present TreePartNet, a neural network aimed at reconstructing tree geometry from point clouds obtained by scanning real trees. Our key idea is to learn a natural neural decomposition exploiting the assumption that a tree comprises locally cylindrical shapes. In particular, reconstruction is a two-step process. First, two networks are used to detect priors from the point clouds. One detects semantic branching points, and the other network is trained to learn a cylindrical representation of the branches. In the second step, we apply a neural merging module to reduce the cylindrical representation to a final set of generalized cylinders combined by branches. We demonstrate results of reconstructing realistic tree geometry for a variety of input models and with varying input point quality, e.g., noise, outliers, and incompleteness. We evaluate our approach extensively by using data from both synthetic and real trees and comparing it with alternative methods.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {232},
numpages = {16},
keywords = {optimization, deep learning, procedural generation, geometric modeling, procedural modeling, 3D reconstruction}
}

@inproceedings{10.1145/3603165.3607394,
author = {Zhou, Ruiting and Yu, Jieling},
title = {A Reinforcement Learning Approach for Minimizing Job Completion Time in Clustered Federated Learning},
year = {2023},
isbn = {9798400702334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603165.3607394},
doi = {10.1145/3603165.3607394},
abstract = {Federated Learning (FL) enables potentially a large number of clients to collaboratively train a global model with the coordination of a central cloud server without exposing client raw data. However, the FL model convergence performance, often measured by the job completion time, is hindered by two critical factors: non independent and identically distributed (non-IID) data across clients and the straggler effect. In this work, we propose a clustered FL framework, MCFL, to minimize the job completion time by mitigating the influence of non-IID data and the straggler effect while guaranteeing the FL model convergence performance. MCFL builds upon a two-stage operation: i) a clustering algorithm constructs clusters, each containing clients with similar computing and communications capabilities to combat the straggler effect within a cluster; ii) a deep reinforcement learning (DRL) algorithm based on soft actor-critic with discrete actions intelligently selects a subset of clients from each cluster to mitigate the impact of non-IID data, and derives the number of intra-cluster aggregation iterations for each cluster to reduce the straggler effect among clusters. Extensive testbed experiments are conducted under various configurations to verify the efficacy of MCFL. The results show that MCFL can reduce the job completion time by up to compared with three state-of-the-art FL frameworks.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2023},
pages = {55–56},
numpages = {2},
location = {Wuhan, China},
series = {ACM TURC '23}
}

@article{10.1145/3470007,
author = {Bhimani, Janki and Yang, Zhengyu and Yang, Jingpei and Maruf, Adnan and Mi, Ningfang and Pandurangan, Rajinikanth and Choi, Changho and Balakrishnan, Vijay},
title = {Automatic Stream Identification to Improve Flash Endurance in Data Centers},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3470007},
doi = {10.1145/3470007},
abstract = {The demand for high performance I/O in Storage-as-a-Service (SaaS) is increasing day by day. To address this demand, NAND Flash-based Solid-state Drives (SSDs) are commonly used in data centers as cache- or top-tiers in the storage rack ascribe to their superior performance compared to traditional hard disk drives (HDDs). Meanwhile, with the capital expenditure of SSDs declining and the storage capacity of SSDs increasing, all-flash data centers are evolving to serve cloud services better than SSD-HDD hybrid data centers. During this transition, the biggest challenge is how to reduce the Write Amplification Factor (WAF) as well as to improve the endurance of SSD since this device has a limited program/erase cycles. A specified case is that storing data with different lifetimes (i.e., I/O streams with similar temporal fetching patterns such as reaccess frequency) in one single SSD can cause high WAF, reduce the endurance, and downgrade the performance of SSDs. Motivated by this, multi-stream SSDs have been developed to enable data with a different lifetime to be stored in different SSD regions. The logic behind this is to reduce the internal movement of data—when garbage collection is triggered, there are high chances of having data blocks with either all the pages being invalid or valid. However, the limitation of this technology is that the system needs to manually assign the same streamID to data with a similar lifetime. Unfortunately, when data arrives, it is not known how important this data is and how long this data will stay unmodified. Moreover, according to our observation, with different definitions of a lifetime (i.e., different calculation formulas based on selected features previously exhibited by data, such as sequentiality, and frequency), streamID identification may have varying impacts on the final WAF of multi-stream SSDs. Thus, in this article, we first develop a portable and adaptable framework to study the impacts of different workload features and their combinations on write amplification. We then propose a feature-based stream identification approach, which automatically co-relates the measurable workload attributes (such as I/O size, I/O rate, and so on.) with high-level workload features (such as frequency, sequentiality, and so on.) and determines a right combination of workload features for assigning streamIDs. Finally, we develop an adaptable stream assignment technique to assign streamID for changing workloads dynamically. Our evaluation results show that our automation approach of stream detection and separation can effectively reduce the WAF by using appropriate features for stream assignment with minimal implementation overhead.},
journal = {ACM Trans. Storage},
month = {apr},
articleno = {17},
numpages = {29},
keywords = {coherency, write amplification factor, NAND flash endurance, I/O workload characterization, multi-streaming, Solid state drives, I/O stream detection}
}

@inproceedings{10.1145/3576842.3582376,
author = {Nouma, Saif E. and Yavuz, Attila A.},
title = {Practical Cryptographic Forensic Tools for Lightweight Internet of Things and Cold Storage Systems},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3582376},
doi = {10.1145/3576842.3582376},
abstract = {Internet of Things (IoT) and Storage-as-a-Service (STaaS) continuum permit cost-effective maintenance of security-sensitive information collected by IoT devices over cloud systems. It is necessary to guarantee the security of sensitive data in IoT-STaaS applications. Especially, log entries trace critical events in computer systems and play a vital role in the trustworthiness of IoT-STaaS. An ideal log protection tool must be scalable and lightweight for vast quantities of resource-limited IoT devices while permitting efficient and public verification at STaaS. However, the existing cryptographic logging schemes either incur significant computation/signature overhead to the logger or extreme storage and verification costs to the cloud. There is a critical need for a cryptographic forensic log tool that respects the efficiency requirements of the IoT-STaaS continuum. In this paper, we created novel digital signatures for logs called Optimal Signatures for secure Logging (), which are the first (to the best of our knowledge) to offer both small-constant signature and public key sizes with near-optimal signing and batch verification via various granularities. We introduce new design features such as one-time randomness management, flexible aggregation along with various optimizations to attain these seemingly conflicting properties simultaneously. Our experiments show that &nbsp;offers 50 \texttimes{} faster verification (for 235 entries) than the most compact alternative with equal signature sizes, while also being several magnitudes of more compact than its most logger efficient counterparts. These properties make &nbsp;an ideal choice for the IoT-STaaS, wherein lightweight logging and efficient batch verification of massive-size logs are vital for the IoT edge and cold storage servers, respectively.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {340–353},
numpages = {14},
keywords = {secure logs, cold storage, digital signatures, Authentication},
location = {<conf-loc>, <city>San Antonio</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {IoTDI '23}
}

@inproceedings{10.1145/3546000.3546015,
author = {Qlu, Ye},
title = {Secure Mechanism of Intelligent Urban Railway Cloud Platform Based on Zero-Trust Security Architecture},
year = {2022},
isbn = {9781450396295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546000.3546015},
doi = {10.1145/3546000.3546015},
abstract = {Aiming to strengthen the stability of operation and maintenance of the urban rail transit network cloud platform at this stage, it is emerging to solve the security mechanism of the intelligent urban railway cloud platform. In this paper, we proposed a zero-trust network security solution for the rail transit system network construction. First, we built a zero-trust network construction for smart city rail transit at the architecture level, it can break the phenomenon of information security silo of rail transit line platform and minimize the system security risk based on a zero-trust network. Next, we focus on building a cloud security brain for urban rail transit networks and proposed the self-learning trust algorithm for a zero-trust network. Specifically, we illustrated the modified network model and constructed a dynamic updating user trust profile as the trustworthy access list. The parameters of the self-learning trust algorithm consist of the state, available chain road bandwidth, waiting for queue state of network traffic, linkage actions, and so on. We adopted a dynamic self-learning strategy for adjusting mitigation policy, the learning step predicted the state of the predetermined congestion and selected the rich links for execution. Finally, experiments show the efficiency of our secure mechanism of railway cloud platform based on zero-trust security architecture.},
booktitle = {Proceedings of the 6th International Conference on High Performance Compilation, Computing and Communications},
pages = {99–105},
numpages = {7},
keywords = {cloud platform, Zero-trust security mechanism, simulation analysis},
location = {Jilin, China},
series = {HP3C '22}
}

@inproceedings{10.1145/3607828.3617794,
author = {Abdur Rahman, Lubnaa and Papathanail, Ioannis and Brigato, Lorenzo and Mougiakakou, Stavroula},
title = {A Comparative Analysis of Sensor-, Geometry-, and Neural-Based Methods for Food Volume Estimation},
year = {2023},
isbn = {9798400702846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607828.3617794},
doi = {10.1145/3607828.3617794},
abstract = {With the rapid advancements in artificial intelligence and computer vision within health and nutrition fields, image-based automatic dietary assessment is gaining popularity. This automation involves food segmentation, recognition, volume estimation, and estimation of nutritional content. While considerable progress has been made in food segmentation and recognition, accurate volume estimation remains challenging. Measuring food volume is crucial in many fields, even thought this is difficult to automate precisely. This is hampering progress, and is leading to continued reliance on time-consuming traditional methods, such as manual computation of food volume through water displacement. The manuscript presents a comparative analysis of sensor-, geometry-, and neural-based methods for computing food volume. We have performed multiple experiments using 20 meal images captured under different settings, with reliable measurements of ground-truth volume obtained by capturing 360-degree views of the food items and computing their volumes in a 3D space. An extensive analysis of our results then serves to identify the strengths and limitations of each approach, and offers valuable insights for selecting the most suitable method in specific settings. Moreover, we have made the collected data (including RGB images, ground-truth point clouds, volumes, etc.) open-source. We intend this as a contribution to the research community and to address the scarcity of food datasets with depth-related information.},
booktitle = {Proceedings of the 8th International Workshop on Multimedia Assisted Dietary Management},
pages = {21–29},
numpages = {9},
keywords = {image-based food volume estimation, multimedia systems, computer vision, sensor-based volume estimation, stereo vision, depth prediction, artificial intelligence},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MADiMa '23}
}

@article{10.14778/3529337.3529344,
author = {Burckhardt, Sebastian and Chandramouli, Badrish and Gillum, Chris and Justo, David and Kallas, Konstantinos and McMahon, Connor and Meiklejohn, Christopher S. and Zhu, Xiangfeng},
title = {Netherite: Efficient Execution of Serverless Workflows},
year = {2022},
issue_date = {April 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3529337.3529344},
doi = {10.14778/3529337.3529344},
abstract = {Serverless is a popular choice for cloud service architects because it can provide scalability and load-based billing with minimal developer effort. Functions-as-a-service (FaaS) are originally stateless, but emerging frameworks add stateful abstractions. For instance, the widely used Durable Functions (DF) allow developers to write advanced serverless applications, including reliable workflows and actors, in a programming language of choice. DF implicitly and continuosly persists the state and progress of applications, which greatly simplifies development, but can create an IOps bottleneck.To improve efficiency, we introduce Netherite, a novel architecture for executing serverless workflows on an elastic cluster. Netherite groups the numerous application objects into a smaller number of partitions, and pipelines the state persistence of each partition. This improves latency and throughput, as it enables workflow steps to group commit, even if causally dependent. Moreover, Netherite leverages FASTER's hybrid log approach to support larger-than-memory application state, and to enable efficient partition movement between compute hosts.Our evaluation shows that (a) Netherite achieves lower latency and higher throughput than the original DF engine, by more than an order of magnitude in some cases, and (b) that Netherite has lower latency than some commonly used alternatives, like AWS Step Functions or cloud storage triggers.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {1591–1604},
numpages = {14}
}

@inproceedings{10.1145/3508398.3511503,
author = {Asvadishirehjini, Aref and Kantarcioglu, Murat and Malin, Bradley},
title = {GINN: Fast GPU-TEE Based Integrity for Neural Network Training},
year = {2022},
isbn = {9781450392204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508398.3511503},
doi = {10.1145/3508398.3511503},
abstract = {Machine learning models based on Deep Neural Networks (DNNs) are increasingly deployed in a wide variety of applications, ranging from self-driving cars to COVID-19 diagnosis. To support the computational power necessary to train a DNN, cloud environments with dedicated Graphical Processing Unit (GPU) hardware support have emerged as critical infrastructure. However, there are many integrity challenges associated with outsourcing the computation to use GPU power, due to its inherent lack of safeguards to ensure computational integrity. Various approaches have been developed to address these challenges, building on trusted execution environments (TEE). Yet, no existing approach scales up to support realistic integrity-preserving DNN model training for heavy workloads (e.g., deep architectures and millions of training examples) without sustaining a significant performance hit. To mitigate the running time difference between pure TEE (i.e., full integrity) and pure GPU (i.e., no integrity) , we combine random verification of selected computation steps with systematic adjustments of DNN hyperparameters (e.g., a narrow gradient clipping range), which limits the attacker's ability to shift the model parameters arbitrarily. Experimental analysis shows that the new approach can achieve a 2X to 20X performance improvement over a pure TEE-based solution while guaranteeing an extremely high probability of integrity (e.g., 0.999) with respect to state-of-the-art DNN backdoor attacks.},
booktitle = {Proceedings of the Twelfth ACM Conference on Data and Application Security and Privacy},
pages = {4–15},
numpages = {12},
keywords = {intel sgx, trusted exexution environments, deep learning, integrity preserving deep learning training},
location = {Baltimore, MD, USA},
series = {CODASPY '22}
}

