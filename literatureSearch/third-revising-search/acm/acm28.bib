@inproceedings{10.1145/2568088.2576761,
author = {Guo, Yong and Varbanescu, Ana Lucia and Iosup, Alexandru and Martella, Claudio and Willke, Theodore L.},
title = {Benchmarking Graph-Processing Platforms: A Vision},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2576761},
doi = {10.1145/2568088.2576761},
abstract = {Processing graphs, especially at large scale, is an increasingly useful activity in a variety of business, engineering, and scientific domains. Already, there are tens of graph-processing platforms, such as Hadoop, Giraph, GraphLab, etc., each with a different design and functionality. For graph-processing to continue to evolve, users have to find it easy to select a graph-processing platform, and developers and system integrators have to find it easy to quantify the performance and other non-functional aspects of interest. However, the state of performance analysis of graph-processing platforms is still immature: there are few studies and, for the few that exist, there are few similarities, and relatively little understanding of the impact of dataset and algorithm diversity on performance. Our vision is to develop, with the help of the performance-savvy community, a comprehensive benchmarking suite for graph-processing platforms. In this work, we take a step in this direction, by proposing a set of seven challenges, summarizing our previous work on performance evaluation of distributed graph-processing platforms, and introducing our on-going work within the SPEC Research Group's Cloud Working Group.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {289–292},
numpages = {4},
keywords = {benchmarking, experimentation, performance, graph processing},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1145/3341105.3373948,
author = {Kuhlenkamp, J\"{o}rn and Werner, Sebastian and Borges, Maria C. and Ernst, Dominik and Wenzel, Daniel},
title = {Benchmarking Elasticity of FaaS Platforms as a Foundation for Objective-Driven Design of Serverless Applications},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373948},
doi = {10.1145/3341105.3373948},
abstract = {Application providers have to solve the trade-off between performance and deployment costs by selecting the "right" amount of provisioned computing resources for their application. The high value of changing this trade-off decision at runtime fueled a decade of combined efforts by industry and research to develop elastic applications. Despite these efforts, the development of elastic applications still demands significant time and expertise from application providers.To address this demand, FaaS platforms shift responsibilities associated with elasticity from the application developer to the cloud provider. While this shift is highly promising, FaaS platforms do not quantify elasticity; thus, application developers are unaware of how elastic FaaS platforms are. This lack of knowledge significantly impairs effective objective-driven design of serverless applications.In this paper, we present an experiment design and corresponding toolkit for quantifying elasticity and its associated trade-offs with latency, reliability, and execution costs. We present results for the evaluation of four popular FaaS platforms by AWS, Google, IBM, Microsoft, and show significant differences between the service offers. Based on our results, we assess the applicability of the individual FaaS platforms in three scenarios under different objectives: web serving, online data analysis, and offline batch processing.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1576–1585},
numpages = {10},
keywords = {serverless, experimentation, elasticity, benchmarking},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/2683483.2683544,
author = {Karthik, M. Siva and Mittal, Sudhanshu and Krishna, K. Madhava},
title = {Guess from Far, Recognize When Near: Searching the Floor for Small Objects},
year = {2014},
isbn = {9781450330619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2683483.2683544},
doi = {10.1145/2683483.2683544},
abstract = {In indoor environments, there would be several small objects lying around on the floor. In this work, we develop an efficient strategy to search for a set of queried objects amongst a large number of small objects lying around. Small objects of the order of 1cm – 5cm, appear very small, making it difficult for the present algorithms to recognize them from far away. A human like strategy in such cases is to infer each object's similarity to the queried objects, from far away. Subsequently, the objects of interest are approached and analyzed from a closer proximity through an optimal plan. We develop an optimal plan for the robot, to strategically visit a selected few among all the objects. From far away, we assign Existential Probabilities to the objects, indicating their similarity to queried objects. A Bayes' Net is constructed over the probabilities, to overlay and orient a Viewpoint Object Potential(VOP) map over potential search objects. VOP quantifies the probability of accurately recognizing an object through its RGB-D Point Cloud at various viewpoints. The belief from the Bayes' Net and the discriminative viewpoints from the VOP are utilized to formulate a Decision Tree which helps in building an optimal control plan. Hence, the robot reaches strategic viewpoints around potential objects, to recognize them through their RGB-D point clouds. The framework is experimentally evaluated using Kinect mounted on a Turtlebot using ROS platform.},
booktitle = {Proceedings of the 2014 Indian Conference on Computer Vision Graphics and Image Processing},
articleno = {61},
numpages = {8},
keywords = {Mobile Robotics, Visual Object Search},
location = {Bangalore, India},
series = {ICVGIP '14}
}

@proceedings{10.1145/3267357,
title = {MPS '18: Proceedings of the 2nd International Workshop on Multimedia Privacy and Security},
year = {2018},
isbn = {9781450359887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 2nd International Workshop on Multimedia Privacy and Security - MPS 2018. This year's workshop builds upon our previous workshop, MPS 2017, which brought together researchers and practitioners in the fields of multimedia, data privacy, and cybersecurity. MPS 2018 once again highlights the important intersection of these fields that has been brought about by the emergence and widespread adoption of the Internet of Things (IoT) and Web 2.0/3.0. Indeed, with the rise of social media, mobile technologies, cloud services, and IoT, massive quantities of multimedia content are being created and disseminated as users post updates that include photos, video and audio data, personal information and other analytical information. Multimedia has arguably expanded well beyond the scope of its original definition.Our call for papers attracted submissions from North America, Europe, and Asia. We received 18 valid submissions, of which 6 were selected as Full Papers (33\%) and 7 were accepted as Short Papers after a double-blind review by our program committee. In addition to these presentations, we feature an invited keynote address entitled, "Family Reunion: Adversarial Machine Learning meets Digital Watermarking," given by Professor Konrad Rieck (Technische Universit\"{a}t Braunschweig, Germany).},
location = {Toronto, Canada}
}

@proceedings{10.1145/3137616,
title = {MPS '17: Proceedings of the 2017 on Multimedia Privacy and Security},
year = {2017},
isbn = {9781450352062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 1st International Workshop on Multimedia Privacy and Security - MPS 2017. We are pleased to present this newly-established workshop that brings together researchers and practitioners in the fields of multimedia, data privacy, and cybersecurity. MPS 2017 highlights the important intersection of these fields that has been brought about by the emergence and widespread adoption of social media, Internet of Things (IoT), and other ubiquitous computing platforms. Indeed, with the rise of social media, mobile technologies, cloud services, and IoT, massive quantities of multimedia content are created and disseminated as users post updates that include personal information, video and audio data, and other analytical information. Multimedia has arguably expanded well beyond the scope of its original definition.Our call for papers attracted submissions from North America, Central America, Asia, and Europe. We received seven submissions, of which three were selected as Full Papers (42\%) and two were accepted as Short Papers after a double-blind review by our program committee. In addition to these presentations, we will feature an invited keynote address and discussion panel: An NSF View of Multimedia Privacy and Security, Mr. Jeremy Epstein (Deputy Division Director, Computer and Network Systems, National Science Foundation)Multimedia Security and Privacy with IoT and Social Networks, moderated by Kurt Rohloff and Roger Hallman},
location = {Dallas, Texas, USA}
}

@inproceedings{10.1145/2910017.2910633,
author = {Bondarenko, Olga and De Schepper, Koen and Tsang, Ing-Jyh and Briscoe, Bob and Petlund, Andreas and Griwodz, Carsten},
title = {Ultra-Low Delay for All: Live Experience, Live Analysis},
year = {2016},
isbn = {9781450342971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910017.2910633},
doi = {10.1145/2910017.2910633},
abstract = {This demo dramatically illustrates how replacing 'Classic' TCP congestion control (Reno, Cubic, etc.) with a 'Scalable' alternative like Data Centre TCP (DCTCP) keeps queuing delay ultra-low; not just for a select few light applications like voice or gaming, but even when a variety of interactive applications all heavily load the same (emulated) Internet access. DCTCP has so far been confined to data centres because it is too aggressive---it starves Classic TCP flows. To allow DCTCP to be exploited on the public Internet, we developed DualQ Coupled Active Queue Management (AQM), which allows the two TCP types to safely co-exist. Visitors can test all these claims. As well as running Web-based apps, they can pan and zoom a panoramic video of a football stadium on a touch-screen, and experience how their personalized HD scene seems to stick to their finger, even though it is encoded on the fly on servers accessed via an emulated delay, representing 'the cloud'. A pair of VR goggles can be used at the same time, making a similar point. The demo provides a dashboard so that visitors can not only experience the interactivity of each application live, but they can also quantify it via a wide range of performance stats, updated live. It also includes controls so visitors can configure different TCP variants, AQMs, network parameters and background loads and immediately test the effect.},
booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
articleno = {33},
numpages = {4},
location = {Klagenfurt, Austria},
series = {MMSys '16}
}

@inproceedings{10.1145/3433210.3453095,
author = {Boutet, Antoine and Frindel, Carole and Gambs, S\'{e}bastien and Jourdan, Th\'{e}o and Ngueveu, Rosin Claude},
title = {DySan: Dynamically Sanitizing Motion Sensor Data Against Sensitive Inferences through Adversarial Networks},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3453095},
doi = {10.1145/3433210.3453095},
abstract = {With the widespread development of the quantified-self movement, an increasing number of users rely on mobile applications to monitor their physical activity through their smartphones. However, granting applications a direct access to sensor data exposes users to privacy risks. In particular, motion sensor data are usually transmitted to analytics applications hosted in the cloud, which leverages on machine learning models to provide feedback on their activity status to users. In this setting, nothing prevents the service provider to infer private and sensitive information about a user such as health or demographic attributes. To address this issue, we propose DySan, a privacy-preserving framework to sanitize motion sensor data against unwanted sensitive inferences (i.e., improving privacy) while limiting the loss of accuracy on the physical activity monitoring (i.e., maintaining data utility). Our approach is inspired from the framework of Generative Adversarial Networks to sanitize the sensor data for the purpose of ensuring a good trade-off between utility and privacy. More precisely, by learning in a competitive manner several networks, DySan is able to build models that sanitize motion data against inferences on a specified sensitive attribute (e.g., gender) while maintaining an accurate activity recognition. DySan builds various sanitizing models, characterized by different sets of hyperparameters in the global loss function, to propose a transfer learning scheme over time by dynamically selecting the model which provides the best utility and privacy trade-off according to the incoming data. Experiments conducted on real datasets demonstrate that DySan can drastically limit the gender inference up to 41\% (from 98\% with raw data to 57\% with sanitized data) while only reducing the accuracy of activity recognition by 3\% (from 95\% with raw data to 92\% with sanitized data).},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {672–686},
numpages = {15},
keywords = {gan, privacy, utility-privacy trade-off, activity detection},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@inproceedings{10.1145/3141128.3141146,
author = {Sianipar, Johannes and Willems, Christian and Meinel, Christoph},
title = {Team Placement in Crowd-Resourcing Virtual Laboratory for IT Security e-Learning},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141146},
doi = {10.1145/3141128.3141146},
abstract = {A crowd-resourcing virtual laboratory is a virtual laboratory in which some of the resources are obtained from the crowd. The virtual laboratory is for IT Security e-Learning, where a trainee needs an isolated laboratory environment to do the practical exercises. The isolated laboratory environment, which is called as a Team, consists of virtual machines (VMs) or containers and virtual network devices. The crowd contributes their resources such as virtual machines or physical machines, to the virtual laboratory. The virtual laboratory automatically occupies the contributed resources and uses them to create a Team. The team that consists of containers, will be run in a VM. Since there could be a lot of VMs available, the system needs to select the best VM to run a Team. We present CTPlace, an approach for Team Placement in crowd-resourcing virtual laboratory.CTPlace groups the VMs into tree hierarchical clusters based on the Geo-location of the VMs. CTPlace has two steps in the Team placement. First, it selects a nearest cluster to the trainee location to get the highest throughput. Second, it selects a VM inside the selected cluster. To select a VM inside a public cloud cluster, it uses Most-Full-First algorithm to reduce service cost by reducing the number of running VMs. To select a VM inside a private cloud or within contributed resources, it uses Least-Full-First and Tag-Pack to balance the load and try to place the same type of Teams on the same VM. We compare the CTPlace with three other placement algorithms in a simulated environment, to evaluate the performance of the CTPlace.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {60–66},
numpages = {7},
keywords = {Virtual Laboratory, Cloud Computing, Placement Algorithm, Crowd-resourcing},
location = {London, United Kingdom},
series = {ICCBDC '17}
}

@inproceedings{10.1145/3306131.3317023,
author = {Steinlechner, Harald and Rainer, Bernhard and Schw\"{a}rzler, Michael and Haaser, Georg and Szabo, Attila and Maierhofer, Stefan and Wimmer, Michael},
title = {Adaptive Pointcloud Segmentation for Assisted Interactions},
year = {2019},
isbn = {9781450363105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306131.3317023},
doi = {10.1145/3306131.3317023},
abstract = {In this work, we propose an interaction-driven approach streamlined to support and improve a wide range of real-time 2D interaction metaphors for arbitrarily large pointclouds based on detected primitive shapes. Rather than performing shape detection as a costly pre-processing step on the entire point cloud at once, a user-controlled interaction determines the region that is to be segmented next. By keeping the size of the region and the number of points small, the algorithm produces meaningful results and therefore feedback on the local geometry within a fraction of a second. We can apply these finding for improved picking and selection metaphors in large point clouds, and propose further novel shape-assisted interactions that utilize this local semantic information to improve the user's workflow.},
booktitle = {Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games},
articleno = {14},
numpages = {9},
keywords = {interactive editing, shape detection, pointcloud segmentation},
location = {Montreal, Quebec, Canada},
series = {I3D '19}
}

@article{10.1145/3451964.3451979,
author = {Butnaru, Andrei-M\u{a}d\u{a}lin},
title = {Machine Learning Applied in Natural Language Processing},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3451964.3451979},
doi = {10.1145/3451964.3451979},
abstract = {Machine Learning is present in our lives now more than ever. One of the most researched areas in machine learning is focused on creating systems that are able to understand natural language. Natural language processing is a broad domain, having a vast number of applications with a significant impact in society. In our current era, we rely on tools that can ease our lives. We can search through thousands of documents to find something that we need, but this can take a lot of time. Having a system that can understand a simple query and return only relevant documents is more efficient. Although current approaches are well capable of understanding natural language, there is still space for improvement.This thesis studies multiple natural language processing tasks, presenting approaches on applications such as information retrieval, polarity detection, dialect identification [Butnaru and Ionescu, 2018], automatic essay scoring [Cozma et al., 2018], and methods that can help other systems to understand documents better. Part of the described approaches from this thesis are employing kernel methods, especially string kernels. A method based on string kernels that can determine in what dialect a document is written is presented in this thesis. The approach is treating texts at the character level, extracting features in the form of p-grams of characters, and combining several kernels, including presence bits kernel and intersection kernel. Kernel methods are also presented as a solution for defining the complexity of a specific word. By combining multiple low-level features and high-level semantic features, the approach can find if a non-native speaker of a language can see a word as complicated or not. With one focus on string kernels, this thesis proposes two transductive methods that can improve the results obtained by employing string kernels. One approach suggests using the pairwise string kernel similarities between samples from the training and test sets as features. The other method defines a simple self-training algorithm composed of two iterations. As usual, a classifier is trained over the training data, then is it used to predict the labels of the test samples. In the second iteration, the algorithm adds a predefined number of test samples to the training set for another round of training. These two transductive methods work by adapting the learning method to the test set.A novel cross-dialectal corpus is shown in this thesis. The Moldavian versus Romanian Corpus (MOROCO) [Butnaru and Ionescu, 2019a] contains over 30.000 samples collected from the news domain, split across six categories. Several studies can be employed over this corpus such as binary classification between Romanian and Moldavian samples, intra-dialect multi-class categorization by topic, and cross-dialect multi-class classification by topic. Two baseline approaches are presented for this collection of texts. One method is based on a simple string kernel model. The second approach consists of a character-level deep neural network, which includes several Squeeze-and-Excitation Blocks (SE-blocks). As known at this moment, this is the first time when a SE-block is employed in a natural language processing context. This thesis also presents a method for German Dialect Identification composed on a voting scheme that combines a Character-level Convolutional Neural Network, a Long Short-Term Memory Network, and a model based on String Kernels.Word sense disambiguation is still one of the challenges of the NLP domain. In this context, this thesis tackles this challenge and presents a novel disambiguation algorithm, known as ShowtgunWSD [Butnaru and Ionescu, 2019b]. By treating the global disambiguation problem as multiple local disambiguation problems, ShotgunWSD is capable of determining the sense of the words in an unsupervised and deterministic way, using WordNet as a resource. For this method to work, three functions that can compute the similarity between two words senses are defined. The disambiguation algorithm works as follows. The document is split into multiple windows of words of a specific size for each window. After that, a brute-force algorithm that computes every combination of senses for each word within that window is employed. For every window combination, a score is calculated using one of the three similarity functions. The last step merges the windows using a prefix and suffix matching to form more significant and relevant windows. In the end, the formed windows are ranked by the length and score, and the top ones, based on a voting scheme, will determine the sense for each word.Documents can contain a variable number of words, therefore employing them in machine learning may be hard at times. This thesis presents two novel approaches [Ionescu and Butnaru, 2019] that can represent documents using a finite number of features. Both methods are inspired by computer vision, and they work by first transforming the words within documents to a word representation, such as word2vec. Having words represented in this way, a k-means clustering algorithm can be applied over the words. The centroids of the formed clusters are gathered into a vocabulary. Each word from a document is then represented by the closest centroid from the previously formed vocabulary. To this point, both methods share the same steps. One approach is designed to compute the final representation of a document by calculating the frequency of each centroid found inside it. This method is named Bag of Super Word Embeddings (BOSWE) because each centroid can be viewed as a super word. The second approach presented in this thesis, known as Vector of Locally-Aggregated Word Embeddings (VLAWE), computes the document representation by accumulating the differences between each centroid and each word vector associated with the respective centroid. This thesis also describes a new way to score essays automatically by combining a low-level string kernel model with a high-level semantic feature representation, namely the BOSWE representation.The methods described in this thesis exhibit state-of-the-art performance levels over multiple tasks. One fact to support this claim is that the string kernel method employed for Arabic Dialect Identification obtained the first place, two years in a row at the Fourth and Fifth Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial). The same string kernel model obtained the fifth place at the German Dialect Identification Closed Shared Task at VarDial Workshop of EACL 2017. Second of all, the Complex Word Identification model scored a third-place at the CWI Shared Task of the BEA-13 of NAACL 2018. Third of all, it is worth to mention that the ShotgunWSD algorithm surpassed the MCS baseline on several datasets. Lastly, the model that combines string kernel and bag of super word embeddings obtained state-of-the-art performance over the Automated Student Assessment Prize dataset.},
journal = {SIGIR Forum},
month = {feb},
articleno = {15},
numpages = {3}
}

@inproceedings{10.1145/3443467.3443846,
author = {Wang, Shaomin},
title = {Sentiment Analysis of the Song "Mojito"},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443846},
doi = {10.1145/3443467.3443846},
abstract = {With the development of the Internet, people share views and opinions on things anytime and anywhere. While receiving information, people also produce various information. Based on the evaluation of Jay Chou's new song mojito by different users on Douban, this paper uses Python's JSON tool to calculate the positive and negative probability value of each comment by setting the probability value of positive tendency greater than 0.5 as positive evaluation, otherwise as negative. In order to understand the reasons for user ratings directly, a word cloud map is drawn based on comment data.On the basis of determining the positive and negative emotional tags, the first step is data processing, such as data cleaning, Chinese word segmentation, removing stop words, text vectorization, etc. Then, three different models of naive Bayes, logistic regression and support vector machine are established for comparison. Finally, naive Bayes model is selected for prediction based on cross validation score. Through confusion matrix evaluation, it is found that the model is more accurate for negative evaluation classification results, but not accurate enough for positive evaluation prediction. This may be related to the expressions of irony and double negation in text reviews.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {739–744},
numpages = {6},
keywords = {support vector machine, Sentiment analysis, logistic regression, naive Bayes, sentiment tendency probability},
location = {Xiamen, China},
series = {EITCE '20}
}

@inproceedings{10.1145/3448016.3457274,
author = {Shah, Vraj and Lacanlale, Jonathan and Kumar, Premanand and Yang, Kevin and Kumar, Arun},
title = {Towards Benchmarking Feature Type Inference for AutoML Platforms},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457274},
doi = {10.1145/3448016.3457274},
abstract = {The paradigm of AutoML has created an opportunity to enable ML for the masses. Emerging industrial-scale cloud AutoML platforms aim to automate the end-to-end ML workflow. While many works have looked into automated feature engineering, model selection, or hyper-parameter search in AutoML, little work has studied a crucial step that serves as an entry point to this workflow: ML feature type inference. The semantic gap between attribute types (e.g., strings, numbers) in databases/files and ML feature types (e.g., Numeric, Categorical) necessitates type inference. In this work, we formalize and standardize this task by creating the first ever benchmark labeled dataset, which we use to objectively evaluate existing AutoML tools. Our dataset has 9921 examples and a 9-class label vocabulary. Our labeled data also offers an alternative approach to automate this task than existing rule-based or syntax-based approaches: use ML itself to predict feature types. We collate a benchmark suite of 30 classification and regression tasks to assess the importance of type inference for downstream models. Empirical comparison on our labeled data shows that an ML-based approach delivers a lift of an average 14\% and up to 38\% in accuracy for identifying feature types compared to prominent industrial tools. Our downstream benchmark suite reveals that the ML-based approach outperforms existing industrial-strength tools for 47 out of 60 downstream models. We release our labeled dataset, models, and downstream benchmarks in a public repository with a leaderboard.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1584–1596},
numpages = {13},
keywords = {benchmark data, data preparation, ML feature type inference, labeled data, autoML},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3239576.3239607,
author = {Du, Yuntao and Zhang, Lu and Shi, Jiahao and Tang, Jingjuan and Yin, Ying},
title = {Feature-Grouping-Based Two Steps Feature Selection Algorithm in Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239607},
doi = {10.1145/3239576.3239607},
abstract = {In order to improve the effect of software defect prediction, many algorithms including feature selection, have been proposed. Based on Wrapper and Filter hybrid framework, a feature-grouping-based feature selection algorithm is proposed in this paper. The algorithm is composed of two steps. In the first step, in order to remove the redundant features, we group the features according to the redundancy between the features. The symmetry uncertainty is used as the constant indicator of the correlation and the FCBF-based grouping algorithm is used to group the features. In the second step, a subset of the features are selected from each group to form the final subset of features. Many classical methods select the representative feature from each group. We consider that when the number of intra-group features is large, the representative features are not enough to reflect the information in this group. Therefore, we require that at least one feature be selected within each group, in this step, the PSO algorithm is used for Searching Randomly from each group. We tested on the open source NASA and PROMISE data sets. Using three kinds of classifier. Compared to the other methods tested in this article, our method resulted in 90\% improvement in the predictive performance of 30 sets of results on 10 data sets. Compared with the algorithms without feature selection, the AUC values of this method in the Logistic regression, Naive Bayesian, and K-neighbor classifiers are improved by 5.94\% and 4.69\% And 8.05\%. The FCBF algorithm can also be regarded as a kind of first performing feature grouping. Compared with the FCBF algorithm, the AUC values of this method are improved by 4.78\%, 6.41\% and 4.4\% on the basis of Logistic regression, Naive Bayes and K-neighbor. We can also see that for the FCBF-based grouping algorithm, it could be better to choose a characteristic cloud from each group than to choose a representative one.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {173–178},
numpages = {6},
keywords = {FCBF-based grouping algorithm, PSO, Feature grouping, Software defect prediction, Intra-group feature selection},
location = {Chengdu, China},
series = {ICAIP '18}
}

@inproceedings{10.1145/3093338.3104153,
author = {Chourasia, A. and Nadeau, D. and Norman, M.},
title = {SeedMe: Data Sharing Building Blocks},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3104153},
doi = {10.1145/3093338.3104153},
abstract = {Data sharing is essential and pervasive in scientific research. The requirements for data sharing vary as research projects mature and iterate through early designs and prototypes with a small number of collaborators, and develop into publishable results and larger collaborator teams. Along the way, preliminary and transient results often need to be shared, discussed, and visualized with a quick turn-around time in order to guide the next steps of the project. Data sharing throughout this process requires that the data itself be shared, along with essential context, such as descriptions, provenance, scripts, visualizations, and threaded discussions. However, current consumer-oriented data sharing solutions mainly rely on local or cloud file systems or web-based drop boxes. These mechanisms are rather basic and are largely focused on data storage for individual use, rather than data collaboration. Using them for scientific data sharing is cumbersome.SeedMe is a platform that enables easy sharing of transient and preliminary data for a broad research computing community by offering cyberinfrastructure as a service and a modular software stack that could be customized. SeedMe is based on Drupal content management system as a set of building blocks with additional PHP modules and web services clients.In this poster we present our progress on implementing a web based modular data sharing platform that collocates shared data, along with the data's context, including descriptions, discussion, light-weight visualizations, and support files. This project is an evolution of the earlier SeedMe[1, 2] project, which created prototype data sharing tools and garnered user feedback from realworld use. The new SeedMe platform is developing modular components for data sharing, light-weight visualization, collaboration, DOI registration, video encoding and playback, REST APIs, command-line data import/export tools, and more. These modules may be added to any web site based upon the widely-used open-source Drupal content management system.The new SeedMe modules allow extensive customization enabling the sites to select and enhance functionality to provide features specific to a research community or a project. The SeedMe modules are widely applicable to a broad research community. They will be released as a suite of open source extensible building blocks. With this poster we showcase current progress along with an interactive demonstration of the project and engage with the HPC community to get feedback.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {69},
numpages = {1},
keywords = {Data sharing, Cloud, CMS, Visualization, HPC, Collaboration},
location = {New Orleans, LA, USA},
series = {PEARC '17}
}

@inproceedings{10.1145/3427921.3450250,
author = {Carnevali, Laura and Reali, Riccardo and Vicario, Enrico},
title = {Compositional Evaluation of Stochastic Workflows for Response Time Analysis of Composite Web Services},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450250},
doi = {10.1145/3427921.3450250},
abstract = {Workflows are patterns of orchestrated activities designed to deliver some specific output, with application in various relevant contexts including software services, business processes, supply chain management. In most of these scenarios, durational properties of individual activities can be identified from logged data and cast in stochastic models, enabling quantitative evaluation of time behavior for diagnostic and predictive analytics. However, effective fitting of observed durations commonly requires that distributions break the limits of memoryless behavior and unbounded support of Exponential distributions, casting the problem in the class of non-Markovian models. This results in a major hurdle for numerical solution, largely exacerbated by the concurrency structure of workflows, which natively subtend concurrent activities with overlapping execution intervals and a limited number of regeneration points, i.e., time points at which the Markov property is satisfied and analysis can be decomposed according to a renewal argument. We propose a compositional method for quantitative evaluation of end-to-end response time of complex workflows. The workflow is modeled through Stochastic Time Petri Nets (STPNs), associating activity durations with Exponential distributions truncated over bilateral firmly bounded supports that fit mean and coefficient of variation of real logged histograms. Based on the model structure, the workflow is decomposed into a hierarchy of subworkflows, each amenable to efficient numerical solution through Markov regenerative transient analysis. In this step, the grain of decomposition is driven by non-deterministic analysis of the space of feasible behaviors in the underlying Time Petri Net (TPN) model, which permits efficient characterization of the factors that affect behavior complexity between regeneration points. Duration distributions of the subworkflows obtained through separate analyses are then repeatedly recomposed in numerical form to compute the response time distribution of the overall workflow.Applicability is demonstrated on a case from the literature of composite web services, here extended in complexity to demonstrate scalability of the approach towards finer grain composition schemes, and associated with a variety of durations randomly selected from a data set in the literature of service oriented computing, so as to assess variability of accuracy and complexity of the overall approach with respect to specific timings.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {177–188},
numpages = {12},
keywords = {Markov regenerative processes, regenerative transient analysis, composite web services, performance evaluation, stochastic workflows},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3319619.3321891,
author = {Coetzee, Leon and Nitschke, Geoff},
title = {Evolving Optimal Sun-Shading Building Fa\c{c}ades},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3321891},
doi = {10.1145/3319619.3321891},
abstract = {Evolutionary algorithms have been applied to numerous architectural design applications in what is popularly known as evolutionary design [3], [4], [6]. Such applications include architectural support [7] and structural design for buildings [5] and floor-plan layout design [8]. However, evolutionary design of optimally shaped building fa\c{c}ades is less explored in evolutionary architectural design applications [6], [12], [13].This research investigates the evolutionary design of building fa\c{c}ades, optimally shaped for a given climate. This study applies evolutionary methods to optimally design sun-shades (covering windows on building fa\c{c}ades). Ideally, sun-shades will maximally block direct sunlight but minimize window coverage, thus allowing unobstructed views out of the window and maximizing ambient natural lighting inside. Also, sun-shades help to passively control building climate and determine occupant comfort. Optimal sun-shade designs allow direct sunlight (solar penetration) to enter interior spaces in winter months, heating the building, and minimize solar penetration in summer months, cooling the building [11].This study applies an Evolutionary Strategy (ES) [1] to automate sun-shade design such that solar penetration is minimized for both east and west facing windows, given summer solstice daylight hours in various geographic locations. An ES was selected given the demonstrated effectiveness of such evolutionary optimization on a range of engineering design problems with various constraints [9]. We focus on sun-shade design for rectangular shaped windows (vertical Y axis is 1.5 times the length of the horizontal X axis), where we anticipate sun-shade design will be replicated for many identical windows comprising a building's fa\c{c}ade, as is the case for many modern tall buildings [14].The ES was initialized with 20000 uniform random [1] points in a continuous three-dimensional (1.0 x 1.0 x 1.0) space adjacent to the window (figure 1). These points were possible mesh vertices for sun-shade design and thus the design solution space. The fitness function computed sun-shade effectiveness via calculating how many sun-rays were blocked assuming an increasing or decreasing sun height above the horizontal plane (angle V in figure 1). Thus, we tested the portion of sun-rays blocked by an evolving sun-shade (mesh formed by 20000 vertices) over half of daylight hours (separate sun-shades were evolved for east and west facing fa\c{c}ades). In successive generations, sun-shade mesh vertices blocking sun-rays (at varying degrees of inclination and declination) aimed at the window were selected for as vertices in evolving designs.Evolving sun-shade effectiveness was computed as the intersection of sun-rays at 15 second intervals during simulated half-days. For east facing fa\c{c}ades, from the point where sun is on the horizontal plane (Y axis in figure 1) and incrementally increases until it is directly above the vertical axis of the building fa\c{c}ade (Y-Z plane in figure 1), and for west facing fa\c{c}ades where the sun starts at this midday point and incrementally declines. Sun-shades were evolved for east and west facing fa\c{c}ades given half of summer solstice daylight hours1 (for east versus west fa\c{c}ades) indicative of Cape Town, South Africa, and Amsterdam, the Netherlands (~ 14 hours, 25 minutes and 16 hours, 48 minutes, respectively).At these two geographic locations, 15 second intervals indicated incremental sun movements during day-light hours. For Cape Town, this was approximated as 0.052° increases and decreases and for Amsterdam, 0.045° increases and decreases (for east and west facing fa\c{c}ades, respectively). Half-day simulations thus tested, every 15 seconds, sun-ray intersection (vector: Xp, Yp, Zp at angle V from the horizontal or vertical plane) with any point in the sun-shade. This was a point-cloud in generation 1 and mesh-points in subsequent generations (figure 1). Points intersecting the sun-ray were given maximum (normalized) fitness of 1.0, and points within a given ray distance were assigned a logarithmically decreasing fitness that equalled 0.0 at the maximum ray distance. To account for random variation and diffusion of sun-ray light, each 15 seconds, a random angle (in the range: [-0.01°, +0.01°]) was added to the sun-ray's vector value V.Evolutionary design used a µ+λ ES [1], where (λ = 20000) off-spring were created per generation. This combined population was ranked by fitness and the least ft λ genotypes discarded. Each genotype encoded an (x, y, z) point in an N point-mesh (evolving sun-shade design), and corresponding σ mutation step-size for each coordinate. For simplicity, the X, Y, Z dimensions of the 3D solution space for evolving sun-shades (adjacent to the window) was normalized the range [0.0, 1.0] and the window dimensions normalized to the range [0.0, 1.5] for the X, Y window axes, respectively. Thus, sun-shades only evolved to cover the top two-thirds of a window, ensuring that sufficient ambient light still entered the building and that occupants have a view out of the window.One generation was the evaluation of all 20000 genotypes (in sun-ray simulations), where the fittest 10\% were selected, mutation operators: σxNx(0,1), σyNy(0, 1), σzNz(0,1) applied to permutate each genotype's coordinate and step-size values (p=1.0 and p=0.05, respectively), such that (λ=20000) offspring genotypes were created. All µ+λ genotypes were then evaluated and the fittest 20000 selected as survivors [1]. Sun-shade evolution for Cape Town and Amsterdam constituted experiment set 1 and 2, respectively. Each experiment set was 10 ES runs, for east and west facing fa\c{c}ades, and each run was 100 generations (ES run stopping condition).Sun-shade fitness was the portion of points (constituting a sunshade design) that blocked or partially blocked sun-rays during each half-day simulation. Points that intersected a sun-ray were assigned a maximum fitness of 1.0, and points close to a sun-ray (&lt; ray distance) were assigned a partial fitness in the range: (0.0, 1.0). In generation 1, all 20,000 possible points were considered for sun-shade design. In subsequent generations only points given a fitness value were considered part of the evolving sun-shade (point-mesh) design. For simplicity, sun-shade fitness was normalized to the range: [0.0, 1.0], where 0.0 indicated no sun-rays blocked and 1 indicated all sun-rays blocked (over all day-light hours tested).As a benchmark comparison for evolved sun-shade effectiveness, the fittest sun-shades evolved for east and west facing fa\c{c}ades (at both locations) were selected from each run and compared to ten heuristic design sun-shades (figure 1). The effectiveness of these sun-shades was similarly computed using sun-ray simulations of 15 second intervals during half-day periods for east and west facing fa\c{c}ades and a given number of day-light hours at both locations.Thus for each heuristic design sun-shade a fitness value was similarly calculated, normalized to the range: [0.0, 1.0], where 0 indicated no sun-rays were blocked and 1.0 indicated that all sun-rays were blocked during a sun-ray simulation.Results indicated that, on average, evolved sun-shades, for both shorter and longer day lengths and east versus west facing fa\c{c}ades, were significantly more effective (with statistical significance, two-tailed t-test, p &lt; 0.05, [2]) compared to the ten tested heuristic designed sun-shades. Results also indicated that evolutionary design is suitable for automating optimal sun-shade (and potentially building fa\c{c}ade) design and support current hypotheses on the efficacy of evolutionary design for improving current architectural designs and automating efficient and effective industrial design production [3], [4], [12]. Ongoing work is evaluating sun-shade evolution in comparison to other heuristic designs in various geographic locations, as well as evolving sun-shades that dynamically adapt their form to suit varying daylight lengths and sun intensity.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {393–394},
numpages = {2},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

