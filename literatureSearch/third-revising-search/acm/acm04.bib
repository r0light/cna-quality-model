@inproceedings{10.1145/3298689.3346961,
author = {Panteli, Maria},
title = {Recommendation Systems Compliant with Legal and Editorial Policies: The BBC+ App Journey},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346961},
doi = {10.1145/3298689.3346961},
abstract = {The BBC produces thousands of pieces of content every day and numerous BBC products deliver this content to millions of users. For many years the content has been manually curated (this is evident in the selection of stories on the front page of the BBC News website and app for example). To support content creation and curation, a set of editorial guidelines have been developed to build quality and trust in the BBC. As personalisation becomes more important for audience engagement, we have been exploring how algorithmically-driven recommendations could be integrated in our products. In this talk we describe how we developed recommendation systems for the BBC+ app that comply with legal and editorial policies and promote the values of the organisation. We also discuss the challenges we face moving forward, extending the use of recommendation systems for a public service media organisation like the BBC.The BBC+ app is the first product to host in-house recommendations in a fully algorithmically-driven application. The app surfaces short video clips and is targeted at younger audiences. The first challenge we dealt with was content metadata. Content metadata are created for different purposes and managed by different teams across the organisation making it difficult to have reliable and consistent information. Metadata enrichment strategies have been applied to identify content that is considered to be editorially sensitive, such as political content, current legal cases, archived news, commercial content, and content unsuitable for an under 16 audience. Metadata enrichment is also applied to identify content that due care has not been taken such as poor titles, and spelling and grammar mistakes. The first versions of recommendation algorithms exclude all editorially risky content from the recommendations, the most serious of which is avoiding contempt of court. In other cases we exclude content that could undermine our quality and trustworthiness.The General Data Protection Regulation (GDPR) that recently came into effect had strong implications on the design of our system architecture, the choice of the recommendation models, and the implementation of specific product features. For example, the user should be able to delete their data or switch off personalisation at any time. Our system architecture should allow us to trace down and delete all data from that user and switch to non-personalised content. The recommendations should also be explainable and this led us to sometimes choosing a simpler model so that it is possible to more easily explain why a user was recommended a particular type of content. Specific product features were also added to enhance transparency and explainability. For example, the user could view their history of watched items, delete any item, and get an explanation of why a piece of content was recommended to them.At the BBC we aim to not only entertain our audiences but also to inform and educate. These BBC values are also reflected in our evaluation strategies and metrics. While we aim to increase audience engagement we are also responsible for providing recent and diverse content that meets the needs of all our audiences. Accuracy metrics such as Hit Rate and Normalized Discounted Cumulative Gain (NDCG) can give a good estimate of the predictive performance of the model. However, recency and diversity metrics have sometimes more weight in our products, especially in applications delivering news content. What is more, qualitative evaluation is very important before releasing any new model into production. We work closely with editorial teams who provide feedback on the quality of the recommendations and flag content not adhering to the BBC's values or the legal and editorial policies.The development of the BBC+ app has been a great journey. We learned a lot about our content metadata, the implications of GDPR in our system, and our evaluation strategies. We created a minimum viable product that is compliant with legal and editorial policies. However, a lot needs to be done to ensure the recommendations meet the quality standards of the BBC. While excluding editorially sensitive content has limited the risk of contempt of court, algorithmic fairness and impartiality still need to be addressed. We encourage the community to look more into these topics and help us create the way forward towards applications with responsible machine learning.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {529},
numpages = {1},
keywords = {technology policy, recommendations, public service},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{10.1145/2816839.2816875,
author = {Ilyas, Bambrik and Fedoua, Didi},
title = {A Load Management Algorithm For Wireless Mesh Networks},
year = {2015},
isbn = {9781450334587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2816839.2816875},
doi = {10.1145/2816839.2816875},
abstract = {The WMN (Wireless Mesh Network) is a new emerging technology that can render the field of industrial network more efficient and profitable. Due to its versatility that allows a flexible configuration, this kind of network is commonly considered as a very suitable architecture for mobile clients. The difference between the WMNs and other dynamic networks, such as the MANET (Mobile Ad-hoc Network), is that the Mesh network contains static wireless nodes called MR (Mesh Routers). Consequently, the presence of this infrastructure makes the WMN more suitable to provide QoS (Quality of Service). However, the guarantee of QoS in a dynamic topology is a difficult task by comparison with static networks. These difficulties are caused by the random movement of the clients, the shared nature of the wireless channel, the complexity of multi-hop communications and most importantly the management of the traffic load forwarded through the MRs. In this paper, we propose a new algorithm for load balancing in WMN that can search for alternative paths in order to deviate from the loaded MRs. The proposed algorithm can operate with different metrics at the same time and applies the Genetic Algorithm in case there is a large population of possible solutions.},
booktitle = {Proceedings of the International Conference on Intelligent Information Processing, Security and Advanced Communication},
articleno = {46},
numpages = {6},
keywords = {traffic load, Genetic Algorithm, WMN, Mesh Routers, mobile clients, QoS},
location = {Batna, Algeria},
series = {IPAC '15}
}

@inproceedings{10.1145/3453688.3461512,
author = {Gao, Chengsi and Li, Bing and Wang, Ying and Chen, Weiwei and Zhang, Lei},
title = {Tenet: A Neural Network Model Extraction Attack in Multi-Core Architecture},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453688.3461512},
doi = {10.1145/3453688.3461512},
abstract = {As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4\%.},
booktitle = {Proceedings of the 2021 on Great Lakes Symposium on VLSI},
pages = {21–26},
numpages = {6},
keywords = {memory timing channel, machine learning, deep learning security, multi-core},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@inproceedings{10.1145/3383313.3412248,
author = {Hansen, Casper and Hansen, Christian and Maystre, Lucas and Mehrotra, Rishabh and Brost, Brian and Tomasi, Federico and Lalmas, Mounia},
title = {Contextual and Sequential User Embeddings for Large-Scale Music Recommendation},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412248},
doi = {10.1145/3383313.3412248},
abstract = {Recommender systems play an important role in providing an engaging experience on online music streaming services. However, the musical domain presents distinctive challenges to recommender systems: tracks are short, listened to multiple times, typically consumed in sessions with other tracks, and relevance is highly context-dependent. In this paper, we argue that modeling users’ preferences at the beginning of a session is a practical and effective way to address these challenges. Using a dataset from Spotify, a popular music streaming service, we observe that a) consumption from the recent past and b) session-level contextual variables (such as the time of the day or the type of device used) are indeed predictive of the tracks a user will stream—much more so than static, average preferences. Driven by these findings, we propose CoSeRNN, a neural network architecture that models users’ preferences as a sequence of embeddings, one for each session. CoSeRNN predicts, at the beginning of a session, a preference vector, based on past consumption history and current context. This preference vector can then be used in downstream tasks to generate contextually relevant just-in-time recommendations efficiently, by using approximate nearest-neighbour search algorithms. We evaluate CoSeRNN on session and track ranking tasks, and find that it outperforms the current state of the art by upwards of 10\% on different ranking metrics. Dissecting the performance of our approach, we find that sequential and contextual information are both crucial.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {53–62},
numpages = {10},
keywords = {Sequence, Music Recommendation, User Embeddings, Context},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3405656.3418718,
author = {G\"{u}ndo\u{g}an, Cenk and Ams\"{u}ss, Christian and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
title = {Toward a RESTful Information-Centric Web of Things: A Deeper Look at Data Orientation in CoAP},
year = {2020},
isbn = {9781450380409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405656.3418718},
doi = {10.1145/3405656.3418718},
abstract = {The information-centric networking (ICN) paradigm offers replication of autonomously verifiable content throughout a network, in which content is bound to names instead of hosts. This has proven beneficial in particular for the constrained IoT. Several approaches, the most prominent of which being Named Data Networking, propose access to named content directly on the network layer. Independently, the IETF CoAP protocol group started to develop mechanisms that support autonomous content processing and in-network storage.In this paper, we explore the emerging CoAP protocol building blocks and how they contribute to an information-centric network architecture for a data-oriented RESTful Web of Things. We discuss design options and measure characteristic performances of different network configurations, which deploy CoAP proxies and OSCORE content object security, and compare with NDN. Our findings indicate an almost continuous design space ranging from plain CoAP at the one end to NDN on the other. On both ends---ICN and CoAP---we identify protocol features and aspects whose mutual transfer potentially improves design and operation of the other.},
booktitle = {Proceedings of the 7th ACM Conference on Information-Centric Networking},
pages = {77–88},
numpages = {12},
keywords = {CoAP Proxy, Internet of Things, ICN, OSCORE, content object security, protocol evaluation},
location = {Virtual Event, Canada},
series = {ICN '20}
}

@inproceedings{10.1145/3291064.3291070,
author = {Thirunavukkarasu, Gokul Sidarth and Champion, Benjamin and Horan, Ben and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
title = {IoT-Based System Health Management Infrastructure as a Service},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291070},
doi = {10.1145/3291064.3291070},
abstract = {Customization, enhanced quality of streamlined maintenance services and uplifted productivity are some of the key highlights from the rapidly evolving concept of Industry 4.0. IoT (Internet of things) based service infrastructure models designed for delivering enterprise services with capabilities of pro-actively sensing malfunctions and responding with preventive measures to streamline the automated service offered is one of the prime application of this concept. Continuous maintenance services increase the optimum through-life cost and in-service life cycle of the product providing the customer with the feel of full ownership. In-service feedbacks also help the manufactures to identify issues with respect to the designs and improve it in the future versions. In this paper, as a proof of concept a cloud-based IoT service infrastructure for providing real-time prognostic and supervised vehicle maintenance system is proposed. This proposed system aims at providing an enterprise service infrastructure to the registered vehicle service centers to keep track of the real-time vehicle diagnostic information of their client's vehicle over cloud and use prognostic algorithms to identify any malfunctions or abnormal behavior of the vehicles for automatically scheduling a service appointment and automating the maintenance cycle of the vehicle. In addition to this, the system provides features like remote supervision and diagnostics maintenance enabling technicians to fix issues remotely, ensuring streamlined and reliable service. Initially, before building the proposed prototype system, a few experimental trails where conducted for analyzing the use of different IoT models used in the development to identify the best-suited approach. The results indicated that the publisher-subscriber (NodeJS) based model outperforms the request-response (PHP) based model in terms of the hits per second and mean request time for an increased number of active users. The results of the initial tests justify the reason for the using the publisher-subscriber based IOT architecture. The conceptualized enterprise infrastructure illustrated in the manuscript aims at providing a streamlined maintenance service.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {55–61},
numpages = {7},
keywords = {prognostic maintenance, vehicle diagnosis, internet of things, System health management infrastructure as a service, streamlined remote supervision},
location = {Singapore, Singapore},
series = {CCIOT '18}
}

@article{10.1145/3340290,
author = {Wang, Ji and Bao, Weidong and Zheng, Lei and Zhu, Xiaomin and Yu, Philip S.},
title = {An Attention-Augmented Deep Architecture for Hard Drive Status Monitoring in Large-Scale Storage Systems},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/3340290},
doi = {10.1145/3340290},
abstract = {Data centers equipped with large-scale storage systems are critical infrastructures in the era of big data. The enormous amount of hard drives in storage systems magnify the failure probability, which may cause tremendous loss for both data service users and providers. Despite a set of reactive fault-tolerant measures such as RAID, it is still a tough issue to enhance the reliability of large-scale storage systems. Proactive prediction is an effective method to avoid possible hard-drive failures in advance. A series of models based on the SMART statistics have been proposed to predict impending hard-drive failures. Nonetheless, there remain some serious yet unsolved challenges like the lack of explainability of prediction results. To address these issues, we carefully analyze a dataset collected from a real-world large-scale storage system and then design an attention-augmented deep architecture for hard-drive health status assessment and failure prediction. The deep architecture, composed of a feature integration layer, a temporal dependency extraction layer, an attention layer, and a classification layer, cannot only monitor the status of hard drives but also assist in failure cause diagnoses. The experiments based on real-world datasets show that the proposed deep architecture is able to assess the hard-drive status and predict the impending failures accurately. In addition, the experimental results demonstrate that the attention-augmented deep architecture can reveal the degradation progression of hard drives automatically and assist administrators in tracing the cause of hard drive failures.},
journal = {ACM Trans. Storage},
month = {aug},
articleno = {21},
numpages = {26},
keywords = {recurrent neural network, SMART, attention mechanism, deep neural network, Hard drive failure}
}

@inproceedings{10.1145/3446999.3447004,
author = {Jiang, Dongming and Jiang, Yuan and Li, Jinzi},
title = {A Session-Based Interaction Model for Cloud Service},
year = {2021},
isbn = {9781450388559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446999.3447004},
doi = {10.1145/3446999.3447004},
abstract = {Being executed in the Internet space, cloud services provide elastic computing resource to satisfy customers’ demand. Since single service usually can't suit their complex need, customers need to assemble multiple services into one service workflow which coordinates individual service to fulfill the specific task by means of service interaction. Obviously, the interaction mechanism of cloud service is the glue of workflow and plays an important role in service workflow. However, owing to the dynamic characteristic of the Internet, cloud service interaction is complicated and volatile. Hence, the question how to describe and formalize the complex interaction of cloud service workflow is a non-trivial work, which has directly influence on the overall design of cloud service architecture engineering and its performance.Consider the question above, this article put forward a session-based interaction model of cloud service. Regarding the complexity of service workflow, this paper applies the divide-and-conquer strategy to decompose the interaction of cloud services into session, then constructs the interaction model of cloud service. In the first step, we introduce and formalize the notion of session, so the complex interaction of cloud service workflow can be decomposed into hierarchical session pattern, which is easy to formalize by the guard automata. Next, using the session as the abstract data type, the session-based interaction model is proposed in order to facilitate the formalization of cloud service workflow. In addition, the model incorporates the notion of role and business protocol to strength the model flexibility. Like the role of object in object-oriented programming language, this model provides a new perspective for modelling the cloud service workflow and lay the solid foundation for its formal verification.},
booktitle = {Proceedings of the 2020 8th International Conference on Information Technology: IoT and Smart City},
pages = {25–28},
numpages = {4},
keywords = {role, interface, session, service workflow, cloud service},
location = {Xi'an, China},
series = {ICIT '20}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-Grained Test Case Prioritization for Integration Testing of Delta-Oriented Software Product Lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3054977.3057290,
author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
title = {Simulating Conflict Detection in Heterogeneous Services of a Smart City: Demo Abstract},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3057290},
doi = {10.1145/3054977.3057290},
abstract = {Despite the increasing intelligence of smart services and sophistication of IoT platforms, the safety issues in smart cities are not addressed adequately, especially the safety issues arising from the integration of smart services. Therefore, in this demo abstract, we present CityGuard, a safety-aware watchdog architecture to detect conflicts among actions of heterogeneous services considering both safety and performance requirements. This demo simulates parts of New York City to depict how CityGuard identifies unsafe actions and thus helps to prevent the city from safety hazards, detects two major types of conflicts, i.e., device and environmental conflicts, and improves the overall city performance in terms of multiple performance metrics. This demo complements the full paper on CityGuard that appears in this conference [2].},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {275–276},
numpages = {2},
keywords = {Smart City, City Safety, Conflict Detection, City Simulation},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@article{10.1109/TNET.2017.2746011,
author = {Sapountzis, Nikolaos and Spyropoulos, Thrasyvoulos and Nikaein, Navid and Salim, Umer},
title = {User Association in HetNets: Impact of Traffic Differentiation and Backhaul Limitations},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2017.2746011},
doi = {10.1109/TNET.2017.2746011},
abstract = {Operators, struggling to continuously add capacity and upgrade their architecture to keep up with data traffic increase, are turning their attention to denser deployments that improve spectral efficiency. Denser deployments make the problem of user association challenging, and much work has been devoted to finding algorithms that strike a tradeoff between user quality of service, and network-wide performance load-balancing. Nevertheless, the majority of these algorithms typically consider simple setups with a single type of traffic, usually elastic non-guaranteed bit rate GBR. They also focus on the radio access part, ignoring the backhaul topology and potential capacity limitations. Backhaul constraints are emerging as a key performance bottleneck in future networks, partly due to the continuous improvement of the radio interface, and partly due to the need for inexpensive backhaul links to reduce capital and operational expenditures. To this end, we propose an analytical framework for user association that jointly considers radio access and backhaul network performance. Specifically, we derive an algorithm that takes into account spectral efficiency, base station load, backhaul link capacities and topology, and two traffic classes GBR and non-GBR in both the uplink and downlink directions. We prove analytically an optimal user association rule that ends up maximizing either an arithmetic or a weighted harmonic mean of the achieved performance along different dimensions e.g., uplink and downlink performances or GBR and non-GBR performances. We then use extensive simulations to study the impact of: 1 traffic differentiation; and 2 backhaul capacity limitations and topology on key performance metrics.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {3396–3410},
numpages = {15}
}

@inproceedings{10.1145/2684822.2697043,
author = {Lattanzi, Silvio and Mirrokni, Vahab},
title = {Distributed Graph Algorithmics: Theory and Practice},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2697043},
doi = {10.1145/2684822.2697043},
abstract = {As a fundamental tool in modeling and analyzing social, and information networks, large-scale graph mining is an important component of any tool set for big data analysis. Processing graphs with hundreds of billions of edges is only possible via developing distributed algorithms under distributed graph mining frameworks such as MapReduce, Pregel, Gigraph, and alike. For these distributed algorithms to work well in practice, we need to take into account several metrics such as the number of rounds of computation and the communication complexity of each round. For example, given the popularity and ease-of-use of MapReduce framework, developing practical algorithms with good theoretical guarantees for basic graph algorithms is a problem of great importance.In this tutorial, we first discuss how to design and implement algorithms based on traditional MapReduce architecture. In this regard, we discuss various basic graph theoretic problems such as computing connected components, maximum matching, MST, counting triangle and overlapping or balanced clustering. We discuss a computation model for MapReduce and describe the sampling, filtering, local random walk, and core-set techniques to develop efficient algorithms in this framework. At the end, we explore the possibility of employing other distributed graph processing frameworks. In particular, we study the effect of augmenting MapReduce with a distributed hash table (DHT) service and also discuss the use of a new graph processing framework called ASYMP based on asynchronous message-passing method. In particular, we will show that using ASyMP, one can improve the CPU usage, and achieve significantly improved running time.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {419–420},
numpages = {2},
keywords = {mapreduce algorithms, large scale data-mining, parallel computing},
location = {Shanghai, China},
series = {WSDM '15}
}

@article{10.1145/3183517,
author = {Floris, Alessandro and Ahmad, Arslan and Atzori, Luigi},
title = {QoE-Aware OTT-ISP Collaboration in Service Management: Architecture and Approaches},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3183517},
doi = {10.1145/3183517},
abstract = {It is a matter of fact that quality of experience (QoE) has become one of the key factors determining whether a new multimedia service will be successfully accepted by the final users. Accordingly, several QoE models have been developed with the aim of capturing the perception of the user by considering as many influencing factors as possible. However, when it comes to adopting these models in the management of the services and networks, it frequently happens that no single provider has access to all of the tools to either measure all influencing factors parameters or control over the delivered quality. In particular, it often happens to the over-the-top (OTT) and Internet service providers (ISPs), which act with complementary roles in the service delivery over the Internet. On the basis of this consideration, in this article we first highlight the importance of a possible OTT-ISP collaboration for a joint service management in terms of technical and economic aspects. Then we propose a general reference architecture for a possible collaboration and information exchange among them. Finally, we define three different approaches, namely joint venture, customer lifetime value based, and QoE fairness based. The first aims to maximize the revenue by providing better QoE to customers paying more. The second aims to maximize the profit by providing better QoE to the most profitable customers (MPCs). The third aims to maximize QoE fairness among all customers. Finally, we conduct simulations to compare the three approaches in terms of QoE provided to the users, profit generated for the providers, and QoE fairness.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {apr},
articleno = {36},
numpages = {24},
keywords = {ISP, QoE, Over The Top service providers, Internet service providers, QoE management, quality of experience, OTT, OTT-ISP collaboration}
}

@inproceedings{10.1145/3357141.3357149,
author = {Seabra, Matheus and Naz\'{a}rio, Marcos Felipe and Pinto, Gustavo},
title = {REST or GraphQL? A Performance Comparative Study},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357149},
doi = {10.1145/3357141.3357149},
abstract = {Given the variety of architectural models that can be used, a frequent questioning among software development practitioners is: which architectural model to use? To respond this question regarding performance issues, three target applications have been studied, each written using two models web services architectures: REST and GraphQL. Through research of performance metrics of response time and the average transfer rate between the requests, it was possible to deduce the particularities of each architectural model in terms of performance metrics. It was observed that migrating to GraphQL. resulted in an increase in performance in two-thirds of the tested applications, with respect to average number of requests per second and transfer rate of data. However, it was noticed that services after migration for GraphQL performed below its REST counterpart for workloads above 3000 requests, ranging from 98 to 2159 Kbytes per second after the migration study. On the other hand, for more trivial workloads, services on both REST and GraphQL architectures presented similar performances, where values between REST and GraphQL services ranged from 6.34 to 7.68 requests per second for workloads of 100 requests.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {123–132},
numpages = {10},
keywords = {GraphQL, Teste de desempenho, REST, Modelo arquitetural},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/3167132.3167178,
author = {Fernandes, Rodrigo and Sim\~{a}o, Jos\'{e} and Veiga, Lu\'{\i}s},
title = {EcoVMbroker: Energy-Aware Scheduling for Multi-Layer Datacenters},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167178},
doi = {10.1145/3167132.3167178},
abstract = {The cloud relies on efficient algorithms to find resources for jobs by fulfilling the job's requirements and at the same time optimise an objective function. Utility is a measure of the client satisfaction that can be seen as an objective function maximised by schedulers based on the agreed service level agreement (SLA). We propose EcoVM-Broker which can reduce energy consumption by using dynamic voltage frequency scaling (DVFS) and applying reductions of utility, different for classes of users and across ranges of resource allocations. Using efficient data structures and a hierarchical architecture, we created a scalable solution for the fast growing heterogeneous cloud. EcoVMBroker proved that we can delegate work in a hierarchical datacenter, make decisions based on summaries of resource usage collected from several nodes and still be efficient.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {403–410},
numpages = {8},
keywords = {energy efficiency, virtual machice scheduling, DVFS, partial utility},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1145/2641361.2641374,
author = {Ohkawa, Takeshi and Uetake, Daichi and Yokota, Takashi and Ootsu, Kanemitsu and Baba, Takanobu},
title = {Reconfigurable and Hardwired ORB Engine on FPGA by Java-to-HDL Synthesizer for Realtime Application},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {5},
issn = {0163-5964},
url = {https://doi.org/10.1145/2641361.2641374},
doi = {10.1145/2641361.2641374},
abstract = {A platform for networked FPGA system design, which is named "ORB Engine", is proposed to add more controllability and design productivity on FPGA-based systems composed of software and hardwired IPs. A developer can define an object-oriented interface for the circuit IP in FPGA, and implement the control sequence part using Java. The circuit IP in FPGA can be handled through object-oriented interface from variety of programing languages like C++, Java, Python, Ruby and so on. Application specific and high-efficiency circuit for ORB (Object Request Broker) protocol processing is synthesized from easy-handling Java code using JavaRock Java-to-HDL synthesizer within the de-facto standard CORBA (Common Object Request Broker Architecture). The measurement result shows a very low latency as low as 200us of UDP/IP packet in/out and exhibits a fluctuation free delay performance, which is desirable for real-time applications.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {77–82},
numpages = {6}
}

@inproceedings{10.5555/2821481.2821486,
author = {Van Landuyt, Dimitri and Joosen, Wouter},
title = {On the Role of Early Architectural Assumptions in Quality Attribute Scenarios: A Qualitative and Quantitative Study},
year = {2015},
publisher = {IEEE Press},
abstract = {Architectural assumptions are fundamentally different from architectural decisions because they can not be traced directly to requirements, nor to domain, technical or environmental constraints; they represent conditions under which the designed solution is expected to be valid. Early architectural assumptions are similar in nature, with the key difference that they are not made during architectural design but during requirement elicitation, not by the software architect (a solution-oriented stakeholder), but by the requirements engineer (a problem-oriented stakeholder). They represent initial assumptions about the system's architecture, and allow the requirements engineer to be more precise in documenting the requirements of the system.The role of early architectural assumptions in the current practice of quality attribute scenario elicitation and related development activities in the transition to architecture is unknown and under-investigated. In this paper, we present the results of an exploratory study that focuses on the role and nature of these assumptions in the early development stages. We studied a reasonably large set of quality attribute scenarios for a realistic industrial case, a smart metering system. Our study (i) confirms that quality attribute scenario elicitation in practice does rely heavily on early architectural assumptions, and (ii) shows that they do influence the perceived quality of the requirements body as a whole, in some cases positively, in other cases negatively.These findings provide empirical arguments in favor of making such assumptions explicit already during the requirements elicitation activities. Especially in the context of iterative software development methodologies such as the Twin Peaks model, a well-defined and -documented set of assumptions could smoothen the transition between successive development iterations.},
booktitle = {Proceedings of the Fifth International Workshop on Twin Peaks of Requirements and Architecture},
pages = {9–15},
numpages = {7},
location = {Florence, Italy},
series = {TwinPeaks '15}
}

@inproceedings{10.1145/3220267.3220280,
author = {Odema, Mohanad and Adly, Ihab and El-Baz, Ahmed and Amin, Hani},
title = {A RESTful Architecture for Portable Remote Online Experimentation Services},
year = {2018},
isbn = {9781450364690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220267.3220280},
doi = {10.1145/3220267.3220280},
abstract = {In this paper, an architecture is proposed to deliver portable remote online experimentation services. This can benefit the educational and academic sectors in terms of providing remote online accessibility to real experiment setups. Thus, the users can be relieved from geographical and time dependence for the experiment to be conducted. Nowadays, almost all web services leverage the efficiency and prevalence of the REST (Representational State Transfer) architecture. Hence, this proposed remote online service has been implemented in compliance with the RESTful architectural style.Web-based experiments require compatibility with any of the users' portable devices and accessibility at any time. A RESTful architecture can fulfill these requirements. In addition, different experiments can be made available online based on this architecture while sharing the same infrastructure. A case study has been selected to obtain measurements of different force components existing inside wind tunnels. The complete implementation of this system is provided starting from the embedded controller retrieving sensor measurements to the web server development and user interface design.},
booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
pages = {102–105},
numpages = {4},
keywords = {Online testing and experimentation, RESTful architecture, Remote testing facilities},
location = {Cairo, Egypt},
series = {ICSIE '18}
}

@inproceedings{10.1109/UCC.2014.49,
author = {Keller, Matthias and Robbert, Christoph and Karl, Holger},
title = {Template Embedding: Using Application Architecture to Allocate Resources in Distributed Clouds},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.49},
doi = {10.1109/UCC.2014.49},
abstract = {In distributed cloud computing, application deployment across multiple sites can improve quality of service. Recent research developed algorithms to find optimal locations for virtual machines. However, those algorithms assume to have either single-tier applications or a fixed number of virtual machines--a strong simplification of reality. This paper investigates the placement and scaling of complex application architectures. An application is dynamically scaled to fit both the current demand situation and the currently available infrastructure resources. We compare two approaches: The first one is based on virtual network embedding. The second approach is a novel method called Template Embedding. It is based on a hierarchical 1-allocation hub flow problem and combines application scaling and embedding in one step. Extensive experiments on 43200 network configurations showed that Template Embedding outperforms virtual network embedding in all cases in three metrics: success rate, solution quality, and runtime. This positive result shows that template embedding is a promising approach for distributed cloud resource allocation.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {387–395},
numpages = {9},
keywords = {Distributed Cloud Computing, Cloud Resource Allocation, Application Architecture, Hub Problem, Flow Problem},
series = {UCC '14}
}

@inproceedings{10.1145/3465481.3470018,
author = {Eckel, Michael and Riemann, Tim},
title = {Userspace Software Integrity Measurement},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470018},
doi = {10.1145/3465481.3470018},
abstract = {Todays computing systems are more interconnected and sophisticated than ever before. Especially in healthcare 4.0, services and infrastructures rely on cyber-physical systemss (CPSess) and Internet of Things (IoT) devices. This adds to the complexity of these highly connected systems and their manageability. Even worse, the variety of emerging cyber attacks is becoming more severe and sophisticated, making healthcare one of the most important sectors with major security risks. The development of appropriate countermeasures constitutes one of the most complex and difficult challenges in cyber security research. Research areas include, among others, anomaly detection, network security, multi-layer event detection, cyber resiliency, and integrity protection. Securing the integrity of software running on a device is a desirable protection goal in the context of systems security. With a Trusted Platform Module (TPM), measured boot, and remote attestation there exist technologies to ensure that a system has booted up correctly and runs only authentic software. The Linux Integrity Measurement Architecture (IMA) extends these principles into the operating systems (OSes), measuring native binaries before they are loaded. However, interpreted language files, such as Java classes and Python scripts, are not considered executables and are not measured as such. Contemporary OSess ship with many of these and it is vital to consider them as security-critical as native binaries. In this paper, we introduce Userspace Software Integrity Measurement (USIM) for the Linux OSes. Userspace Software Integrity Measurement (USIM) enables interpreters to measure, log, and irrevocably anchor critical events in the TPM. We develop a software library in C which provides TPM-based measurement functionality as well as the USIM service, which provides concurrent access handling to the TPM based event logging. Further, we develop and implement a concept to realize highly frequent event logging on the slow TPM. We integrate this library into the Java Virtual Machine (JVM) to measure Java classes and show that it can be easily integrated into other interpreters. With performance measurements we demonstrate that our contribution is feasible and that overhead is negligible.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {138},
numpages = {11},
keywords = {Systems security, integrity verification, Trusted Computing},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1145/2742580.2742810,
author = {Karedla, Rama},
title = {Programming for the Intel Xeon Processor},
year = {2015},
isbn = {9781450335270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742580.2742810},
doi = {10.1145/2742580.2742810},
abstract = {Software programmers tend to focus on the software layer leaving performance on the table by not taking advantage of the underlying hardware. This talk will help the programmer take advantage of the underlying Intel Xeon server architecture to write more efficient programs. We broadly cover topics such as time measurement, memory ordering, making efficient use of the multi level caches, NUMA aware programming and the use of the many compute cores available in the Xeon architecture via multi-threading.We hope to show the benefit to both, latency and throughput oriented applications. The talk will also address using the new AVX vector registers to achieve higher performance, and briefly touch upon the recently announced Transactional Synchronization Extensions (TSX) features. Examples of application profiling will demonstrate the benefit of optimizing for performance in parallel with code development.},
booktitle = {Applicative 2015},
location = {New York, NY, USA},
series = {Applicative 2015}
}

@inproceedings{10.1145/2766498.2774989,
author = {Podiyan, Pradeep and Butakov, Sergey and Zavarsky, Pavol},
title = {Study of Compliance of Android Location APIs with Geopriv},
year = {2015},
isbn = {9781450336239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766498.2774989},
doi = {10.1145/2766498.2774989},
abstract = {This paper carefully examines the location APIs of Android OS as well as the Geopriv standard architecture to study measures that are being taken by Android OS to protect the location privacy of a user. Android offers various location APIs in its architecture for the app developers to work on location based services (LBS). The results of this evaluation will be compared with Geopriv standard architecture and its ways to enhance location information privacy on mobile platforms. The review of functionality of location APIs shows that Android has limited features such as Geofencing to have some extent of location privacy for a typical user. Only few of the recommendation in distribution segment of Geopriv with slightly different approach are similar to the protection mechanisms offered by location APIs in Android. The paper proposes general steps that can be taken to address location privacy issues on mobile devices.},
booktitle = {Proceedings of the 8th ACM Conference on Security \&amp; Privacy in Wireless and Mobile Networks},
articleno = {30},
numpages = {2},
keywords = {Geopriv, Android, location privacy},
location = {New York, New York},
series = {WiSec '15}
}

@inproceedings{10.1145/3139531.3139537,
author = {Liu, Ling},
title = {Keynote: Privacy and Trust: Friend or Foe},
year = {2017},
isbn = {9781450353939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139531.3139537},
doi = {10.1145/3139531.3139537},
abstract = {Internet of Things (IoT) and Big Data have fueled the development of fully distributed computational architectures for future cyber systems from data analytics, machine learning (ML) to artificial intelligence (AI). Trust and Privacy become two vital and necessary measures for distributed management of IoT powered big data learning systems and services. However, these two measures have been studied independently in computer science, social science and law.Trust is widely considered as a critical measure for the correctness, predictability, and resiliency (with respect to reliability and security) of software systems, be it big data systems, IoT systems, machine learning systems, or Artificial Intelligence systems. Privacy on the other hand is commonly recognized as a personalization measure for imposing control on the ways of how data is captured, accessed and analyzed, and the ways of how data analytic results from ML models and AI systems should be released and shared.Broadly speaking, in human society, we rely on three types of trust in our everyday work and life to achieve a peaceful mind: (1) verifiable belief-driven trust, (2) statistical evidence based trust, and (3) complex systemwide cognitive trust. Interestingly, privacy has been a more controversial subject. On one hand, privacy is an important built-in dimension of trust, which is deep rooted in human society, and a highly valued virtue in Western civilization. Even though different human beings may have diverse levels of privacy sensitivity, we all trust that our privacy is respected in our social and professional environments, including at home, at work and in social commons. Thus, Privacy is a perfect example of three-fold trust: belief-driven, statistical evident, and complex cognitive trust. On the other hand, many view privacy (and privacy protection) as an antagonistic measure of trust and one is often asked to show trust at the cost of giving up on privacy.Are Privacy and Trust friend or foe? This keynote will share my view to this question from multiple perspectives. I conjecture that the answer to this question can fundamentally change the ways we conduct research in privacy and trust in the next generation of big data enhanced cyber learning systems from data mining, machine learning to artificial intelligence.},
booktitle = {Proceedings of the 2017 Workshop on Women in Cyber Security},
pages = {11},
numpages = {1},
keywords = {privacy, big data, internet of things, trust, deep learning},
location = {Dallas, Texas, USA},
series = {CyberW '17}
}

@inproceedings{10.5555/2602339.2602341,
author = {Buevich, Maxim and Schnitzer, Dan and Escalada, Tristan and Jacquiau-Chamski, Arthur and Rowe, Anthony},
title = {Fine-Grained Remote Monitoring, Control and Pre-Paid Electrical Service in Rural Microgrids},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {In this paper, we present the architecture, design and experiences from a wirelessly managed microgrid deployment in rural Les Anglais, Haiti. The system consists of a three-tiered architecture with a cloud-based monitoring and control service, a local embedded gateway infrastructure and a mesh network of wireless smart meters deployed at 52 buildings. Each smart meter device has an 802.15.4 radio that enables remote monitoring and control of electrical service. The meters communicate over a scalable multi-hop TDMA network back to a central gateway that manages load within the system. The gateway also provides an 802.11 interface for an on-site operator and a cellular modem connection to a cloud-backend that manages and stores billing and usage data. The cloud backend allows occupants in each home to pre-pay for electricity at a particular peak power limit using a text messaging service. The system activates each meter within seconds and locally enforces power limits with provisioning for theft detection. We believe that this fine-grained micro-payment model can enable sustainable power in otherwise unfeasible areas.This paper provides a chronology of our deployment and installation strategy that involved GPS-based site mapping along with various network conditioning actions required as the network evolved. Finally, we summarize key lessons learned and hypothesis about additional hardware that could be used to ease the tracing of faults like short circuits and downed lines within microgrids.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {1–12},
numpages = {12},
keywords = {sensor networks, wireless local area networks, microgrid},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3288599.3295582,
author = {Shah, Ryan and Nagaraja, Shishir},
title = {Do We Have the Time for IRM? Service Denial Attacks and SDN-Based Defences},
year = {2019},
isbn = {9781450360944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288599.3295582},
doi = {10.1145/3288599.3295582},
abstract = {Distributed sensor networks such as IoT deployments generate large quantities of measurement data. Often, the analytics that runs on this data is available as a web service which can be purchased for a fee. A major concern in the analytics ecosystem is ensuring the security of the data. Often, companies offer Information Rights Management (IRM) as a solution to the problem of managing usage and access rights of the data that transits administrative boundaries. IRM enables individuals and corporations to create restricted IoT data, which can have its flow from organisation to individual control - disabling copying, forwarding, and allowing timed expiry. We describe our investigations into this functionality and uncover a weak-spot in the architecture - its dependence upon the accurate global availability of time. We present an amplified denial-of-service attack which attacks time synchronisation and could prevent all the users in an organisation from reading any sort of restricted data until their software has been re-installed and re-configured. We argue that IRM systems built on current technology will be too fragile for businesses to risk widespread use. We also present defences that leverage the capabilities of Software-Defined Networks to apply a simple filter-based approach to detect and isolate attack traffic.},
booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
pages = {496–501},
numpages = {6},
location = {Bangalore, India},
series = {ICDCN '19}
}

@inproceedings{10.1145/3176258.3176328,
author = {Alshehri, Asma and Benson, James and Patwa, Farhan and Sandhu, Ravi},
title = {Access Control Model for Virtual Objects (Shadows) Communication for AWS Internet of Things},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176328},
doi = {10.1145/3176258.3176328},
abstract = {The concept of Internet of Things (IoT) has received considerable attention and development in recent years. There have been significant studies on access control models for IoT in academia, while companies have already deployed several cloud-enabled IoT platforms. However, there is no consensus on a formal access control model for cloud-enabled IoT. The access-control oriented (ACO) architecture was recently proposed for cloud-enabled IoT, with virtual objects (VOs) and cloud services in the middle layers. Building upon ACO, operational and administrative access control models have been published for virtual object communication in cloud-enabled IoT illustrated by a use case of sensing speeding cars as a running example.In this paper, we study AWS IoT as a major commercial cloud-IoT platform and investigate its suitability for implementing the afore-mentioned academic models of ACO and VO communication control. While AWS IoT has a notion of digital shadows closely analogous to VOs, it lacks explicit capability for VO communication and thereby for VO communication control. Thus there is a significant mismatch between AWS IoT and these academic models. The principal contribution of this paper is to reconcile this mismatch by showing how to use the mechanisms of AWS IoT to effectively implement VO communication models. To this end, we develop an access control model for virtual objects (shadows) communication in AWS IoT called AWS-IoT-ACMVO. We develop a proof-of-concept implementation of the speeding cars use case in AWS IoT under guidance of this model, and provide selected performance measurements. We conclude with a discussion of possible alternate implementations of this use case in AWS IoT.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {175–185},
numpages = {11},
keywords = {internet of things (iot), acl, virtual objects, aws iot, security, abac, iot architecture, devices, access control, rbac},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1145/3409390.3409402,
author = {Monfared, Saleh Khalaj and Hajihassani, Omid and Kiarostami, Mohammad Sina and Zanjani, Soroush Meghdadi and Rahmati, Dara and Gorgin, Saeid},
title = {BSRNG: A High Throughput Parallel BitSliced Approach for Random Number Generators},
year = {2020},
isbn = {9781450388689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409390.3409402},
doi = {10.1145/3409390.3409402},
abstract = {In this work, a high throughput method for generating high-quality Pseudo-Random Numbers using the bitslicing technique is proposed. In such a technique, instead of the conventional row-major data representation, column-major data representation is employed, which allows the bitslicing implementation to take full advantage of all the available datapath of the hardware platform. By employing this data representation as building blocks of algorithms, we showcase the capability and scalability of our proposed method in various PRNG methods in the category of block and stream ciphers. The LFSR-based (Linear Feedback Shift Register) nature of the PRNG in our implementation perfectly suits the GPU’s many-core structure due to its register oriented architecture. In the proposed SIMD vectorized GPU implementation, each GPU thread can generate several 32 pseudo-random bits in each LFSR clock cycle. We then compare our implementation with some of the most significant PRNGs that display a satisfactory performance throughput and randomness criteria. The proposed implementation successfully passes the NIST test for statistical randomness and bit-wise correlation criteria. For computer-based PRNG and the optical solutions in terms of performance and performance per cost, this technique is efficient while maintaining an acceptable randomness measure. Our highest performance among all of the implemented CPRNGs with the proposed method is achieved by the MICKEY 2.0 algorithm, which shows 40\% improvement over state of the art NVIDIA’s proprietary high-performance PRNG, cuRAND library, achieving 2.72 Tb/s of throughput on the affordable NVIDIA GTX 2080 Ti.},
booktitle = {Workshop Proceedings of the 49th International Conference on Parallel Processing},
articleno = {12},
numpages = {10},
keywords = {CUDA, PRNG, High-performance, cuRAND, Stream cipher, Bitslicing, Cryptography},
location = {Edmonton, AB, Canada},
series = {ICPP Workshops '20}
}

@inproceedings{10.1145/2609248.2609264,
author = {Lazarescu, Mihai T. and Cohen, Albert and Guatto, Adrien and L\^{e}, Nhat Minn and Lavagno, Luciano and Pop, Antoniu and Prieto, Manuel and Terechko, Andrei and Sutii, Alexandru},
title = {Energy-Aware Parallelization Flow and Toolset for C Code},
year = {2014},
isbn = {9781450329415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609248.2609264},
doi = {10.1145/2609248.2609264},
abstract = {Multicore architectures are increasingly used in embedded systems to achieve higher throughput with lower energy consumption. This trend accentuates the need to convert existing sequential code to effectively exploit the resources of these architectures. We present a parallelization flow and toolset for legacy C code that includes a performance estimation tool, a parallelization tool, and a streaming-oriented parallelization framework. These are part of the work-in-progress EU FP7 PHARAON project that aims to develop a complete set of techniques and tools to guide and assist software development for heterogeneous parallel architectures. We demonstrate the effectiveness of the use of the toolset in an experiment where we measure the parallelization quality and time for inexperienced users, and the parallelization flow and performance results for the parallelization of a practical example of a stereo vision application.},
booktitle = {Proceedings of the 17th International Workshop on Software and Compilers for Embedded Systems},
pages = {79–88},
numpages = {10},
keywords = {program parallelization, execution profiling, energy estimation, data dependency analysis},
location = {Sankt Goar, Germany},
series = {SCOPES '14}
}

@inproceedings{10.1145/3316615.3316622,
author = {Ming, Fan Xiu and Habeeb, Riyaz Ahamed Ariyaluran and Md Nasaruddin, Fariza Hanum Binti and Gani, Abdullah Bin},
title = {Real-Time Carbon Dioxide Monitoring Based on IoT \&amp; Cloud Technologies},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316622},
doi = {10.1145/3316615.3316622},
abstract = {In recent years, environment monitoring are of greater importance towards the area of climate monitoring, analysis, agricultural productivity management, quality assurance of water, air, alongside with other potential factors that are closely connected to industrial development and convenience of living. This research is motivated by creating awareness of smart home residents on indoor air quality, as well as providing insight of carbon dioxide emissions for industries and environmental organizations.This paper proposes an efficient solution towards environment monitoring of carbon dioxide integrated with Internet of Things capability and cloud computing technology. Aforementioned techniques will deliver highly accessible and real-time data visualization which would be greatly beneficial for Smart Homes efficiency of analysis actualization and counter-measures deployment. A monitoring architecture was developed to generate, accumulate, store and visualize carbon dioxide concentration using MQ135 carbon dioxide sensor, ESP8266 Wi-Fi module, Firebase Cloud Storage Service and Android mobile application Carbon Insight for data visualization. 2880 data points in the time frame of 10 days with a 30-second interval was collected, stored and visualized with the application of this system.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {517–521},
numpages = {5},
keywords = {cloud, Internet of things, environment monitoring},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@inproceedings{10.1145/2642668.2642673,
author = {Hatoum, Rima and Hatoum, Abbas and Ghaith, Alaa and Pujolle, Guy},
title = {Qos-Based Joint Resource Allocation with Link Adaptation for SC-FDMA Uplink in Heterogeneous Networks},
year = {2014},
isbn = {9781450330268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642668.2642673},
doi = {10.1145/2642668.2642673},
abstract = {The LTE-based femtocell network is a promising solution adopted today to cope with the huge cellular traffic requirements. In particular, the Uplink communication becomes an attractive issue especially with the emerging of the interactive services and large uploaded data volume. Intelligent allocation of the resources and interference management are the main challenges in such context. In this paper, we propose a linear optimization model for the SC-FDMA Uplink transmission aiming to adaptively allocate resources with respect to the link quality. Both power and modulation and coding schemes are independently assigned to each user over each allocated sub-channel. The cluster architecture is adopted as a hybrid centralized/distributed network. The user differentiation strategy ensures the QoS guarantee with respect to a priority level of each user. Taking into account the specifications of the uplink communication, we confirm through comparative simulations the outperformance of our proposal considering several metrics such as throughput satisfaction rate, transmitted power, outage probability, special spectrum reuse and others.},
booktitle = {Proceedings of the 12th ACM International Symposium on Mobility Management and Wireless Access},
pages = {59–66},
numpages = {8},
keywords = {link adaptation, interference mitigation, resource allocation, SC-FDMA-femtocell, uplink, QoS},
location = {Montreal, QC, Canada},
series = {MobiWac '14}
}

@inproceedings{10.1145/3384217.3386393,
author = {Petz, Adam},
title = {An Infrastructure for Faithful Execution of Remote Attestation Protocols},
year = {2020},
isbn = {9781450375610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384217.3386393},
doi = {10.1145/3384217.3386393},
abstract = {Experience shows that even with a well-intentioned user at the keyboard, a motivated attacker can compromise a computer system at a layer below or adjacent to the shallow forms of authentication that are now accepted as commonplace[3]. Therefore, rather than asking "Can we trust the person behind the keyboard", a still better question might be: "Can we trust the computer system underneath?". An emerging technology for gaining trust in a remote computing system is remote attestation. Remote attestation is the activity of making a claim about properties of a target by supplying evidence to an appraiser over a network[2]. Although many existing approaches to remote attestation wisely adopt a layered architecture-where the bottom layers measure layers above-the dependencies between components remain static and measurement orderings fixed. For modern computing environments with diverse topologies, we can no longer fix a target architecture any more than we can fix a protocol to measure that architecture.Copland [1] is a domain-specific language and formal framework that provides a vocabulary for specifying the goals of layered attestation protocols. It also provides a reference semantics that characterizes system measurement events and evidence handling; a foundation for comparing protocol alternatives. The aim of this work is to refine the Copland semantics to a more fine-grained notion of attestation manager execution-a high-privilege thread of control responsible for invoking attestation services and bundling evidence results. This refinement consists of two cooperating components called the Copland Compiler and the Attestation Virtual Machine (AVM). The Copland Compiler translates a Copland protocol description into a sequence of primitive attestation instructions to be executed in the AVM. When considered in combination with advances in virtualization, trusted hardware, and high-assurance system software components-like compilers, file-systems, and OS kernels-a formally verified remote attestation infrastructure creates exciting opportunities for building system-level security arguments.},
booktitle = {Proceedings of the 7th Symposium on Hot Topics in the Science of Security},
articleno = {17},
numpages = {1},
location = {Lawrence, Kansas},
series = {HotSoS '20}
}

@inproceedings{10.1145/3318265.3318280,
author = {Hang, Zijun and Shi, Yang and Wen, Mei and Quan, Wei and Zhang, Chunyuan},
title = {SWAP: A Sliding Window Algorithm for in-Network Packet Measurement},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318280},
doi = {10.1145/3318265.3318280},
abstract = {Network traffic measurement is a fundamental part of many network applications, such as DDOS detection, capacity planning, and quality-of-service improvement. To achieve this, we need to count the number of packets passed during a past time interval. Traditionally, switches sample the packets and send them to the CPU for analysis. It is unavoidable that the sampling will sacrifice the measuring accuracy. Nowadays, programmable switches can keep the counters in the data plane. However, they still rely on the CPU to drain and clear the records periodically, which brings in too much communication latency. To overcome these disadvantages, we propose a metering mechanism under the RMT architectural model called SWAP. SWAP is carefully designed to count the number of packets during an interval accurately with little hardware resource usage. We prototype it using P4 and simulation results show SWAP achieves high efficiency and moderate accuracy at line speed.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {84–89},
numpages = {6},
keywords = {P4, network algorithm, software-defined networks, programmable switches},
location = {Xi'an, China},
series = {HP3C '19}
}

@proceedings{10.1145/2898375,
title = {HotSos '16: Proceedings of the Symposium and Bootcamp on the Science of Security},
year = {2016},
isbn = {9781450342773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Science of Security (SoS) emphasizes the advancement of research methods as well as the development of new research results. This dual focus is intended to improve both the confidence we gain from scientific results and also the capacity and efficiency through which we address increasingly challenging technical problems.The HotSoS conferences have focused on work related to one or more of the five Hard Problems identified by the Science of Security community:•Scalability and composability in the construction of secure systems•Policy-governed collaboration in handling data across different domains of authority for security and privacy protection•Predictive security metrics to guide choice-making in security engineering and response•Resilient architectures that can deliver service despite compromised components•Human behavior, modeling users, operators, and adversaries to support improved design and analysisA second and equally major focus of the conferences is on the advancement of scientific methods, including data gathering and analysis, experimental methods, and mathematical models for modeling and reasoning. This includes the exploration of interactions among these methods to enhance validity.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/2985766.2985772,
author = {Edoh, Thierry Oscar C. and Atchome, Athanase and Alahassa, Bidossessi R.U. and Pawar, Pravin},
title = {Evaluation of a Multi-Tier Heterogeneous Sensor Network for Patient Monitoring: The Case of Benin},
year = {2016},
isbn = {9781450345187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2985766.2985772},
doi = {10.1145/2985766.2985772},
abstract = {In this paper we propose and evaluate a wireless sensor network (WSN) system in order to improve an existing patient-monitoring and surveillance system at the cardiologic intensive care unit (CICU) of a large university clinic (Centre Hospitalier Universitaire Hubert Koutoukou Maga - CHU-HKM) in Cotonou city of Benin (a West-African country). We have designed a multi-tier architecture and simulated a heterogeneous, autonomous, and energy efficient wireless sensor network system to overcome issues faced by existing patient monitoring system in CICU such as manual collection and processing of data. The improvement of the patient monitoring system has the objectives of providing affordable and better health care service provision as well as autonomous and automatic collection and processing of patient's bio-signals and environmental data. The proposed Wireless Sensor Network consists of wireless heterogeneous nodes which sense patient bio-signals, measure environmental parameters in the hospitalization rooms such as ambient temperature, quality of air and send collected data to a gateway (central node) for processing and storage. The conducted simulation experiments show that the proposed sensor network architecture which uses ZigBee wireless standard and protocol highly improves the patience monitoring and surveillance experience at CICU. It promotes collection and autonomous processing of patient physiological data and room ambient temperature data. Incorporating such system in CICU will be highly beneficial for taking a correct decision during treatment. Beyond the accuracy and quality of the collected medical data, proposed WSN is also designed to reduce the energy consumption within the sensor network system.},
booktitle = {Proceedings of the 2016 ACM Workshop on Multimedia for Personal Health and Health Care},
pages = {23–29},
numpages = {7},
keywords = {cardiology, patient monitorin, intensive care unit, zigbee standards, cooperative sensors, wireless sensors network},
location = {Amsterdam, The Netherlands},
series = {MMHealth '16}
}

@article{10.1109/TNET.2018.2793892,
author = {Araldo, Andrea and Dan, Gyorgy and Rossi, Dario},
title = {Caching Encrypted Content Via Stochastic Cache Partitioning},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2793892},
doi = {10.1109/TNET.2018.2793892},
abstract = {In-network caching is an appealing solution to cope with the increasing bandwidth demand of video, audio, and data transfer over the Internet. Nonetheless, in order to protect consumer privacy and their own business, content providers CPs increasingly deliver encrypted content, thereby preventing Internet service providers ISPs from employing traditional caching strategies, which require the knowledge of the objects being transmitted. To overcome this emerging tussle between security and efficiency, in this paper we propose an architecture in which the ISP partitions the cache space into slices, assigns each slice to a different CP, and lets the CPs remotely manage their slices. This architecture enables transparent caching of encrypted content and can be deployed in the very edge of the ISP’s network i.e., base stations and femtocells, while allowing CPs to maintain exclusive control over their content. We propose an algorithm, called SDCP, for partitioning the cache storage into slices so as to maximize the bandwidth savings provided by the cache. A distinctive feature of our algorithm is that ISPs only need to measure the aggregated miss rates of each CP, but they need not know the individual objects that are requested. We prove that the SDCP algorithm converges to a partitioning that is close to the optimal, and we bound its optimality gap. We use simulations to evaluate SDCP’s convergence rate under stationary and nonstationary content popularity. Finally, we show that SDCP significantly outperforms traditional reactive caching techniques, considering both CPs with perfect and with imperfect knowledge of their content popularity.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {548–561},
numpages = {14}
}

@article{10.1145/3372136,
author = {Lao, Laphou and Li, Zecheng and Hou, Songlin and Xiao, Bin and Guo, Songtao and Yang, Yuanyuan},
title = {A Survey of IoT Applications in Blockchain Systems: Architecture, Consensus, and Traffic Modeling},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3372136},
doi = {10.1145/3372136},
abstract = {Blockchain technology can be extensively applied in diverse services, including online micro-payments, supply chain tracking, digital forensics, health-care record sharing, and insurance payments. Extending the technology to the Internet of things (IoT), we can obtain a verifiable and traceable IoT network. Emerging research in IoT applications exploits blockchain technology to record transaction data, optimize current system performance, or construct next-generation systems, which can provide additional security, automatic transaction management, decentralized platforms, offline-to-online data verification, and so on. In this article, we conduct a systematic survey of the key components of IoT blockchain and examine a number of popular blockchain applications.In particular, we first give an architecture overview of popular IoT-blockchain systems by analyzing their network structures and protocols. Then, we discuss variant consensus protocols for IoT blockchains, and make comparisons among different consensus algorithms. Finally, we analyze the traffic model for P2P and blockchain systems and provide several metrics. We also provide a suitable traffic model for IoT-blockchain systems to illustrate network traffic distribution.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {18},
numpages = {32},
keywords = {traffic modeling, IoT, consensus, architecture, Blockchain}
}

@proceedings{10.1145/2737182,
title = {QoSA '15: Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 11th International ACM Sigsoft Conference on the Quality of Software Architectures -- QoSA 2015. For more than a decade, QoSA has strived to advance the state of the art of quality aspects of software architecture, focusing broadly on its quality characteristics and how these relate to the design of software architectures. Specific issues of interest are defining and modeling quality measures, evaluating and managing architecture quality, linking architecture to requirements and implementation, and preserving architecture quality throughout the system lifetime. Past themes for QoSA include Architecting for Adaptivity (2014), The System View (2013), Evolving Architectures (2012), Quality throughout the Software Lifecycle (2011), and Research into Practice -- Reality and Gaps (2010).QoSA 2015's theme is "Software Architecture for the 4th Industrial Revolution". After mechanization, mass production, and electronics, the Internet is about to enable a new level of productivity in manufacturing. This shall be enabled by smart cyber-physical systems connected to cloud computing services and communicating using standardized semantics. In the near future, industrial big data analytics on monitored sensor data shall improve the efficiency and individualization of production facilities. This year's QoSA conference solicited contributions that explore the various implications of this upcoming industrial revolution on software architecture. This included reference architectures, software architectures adapting at run time, architecture styles and patterns for cyber-physical and distributed systems.The call for papers attracted 42 initial submissions from Asia, North America, Africa, and Europe and 28 final submissions were considered during the review process. The program committee accepted 11 full papers and 2 short papers that cover topics, such as new architecture modeling approaches, architectural tactics for mobile computing, cloud computing architectures, and cyberphysical systems. QoSA's 2015 proceedings also include 2 papers from the WCOP 2015, the 20th International Doctoral Symposium on Components and Architecture.QoSA 2015 is part of the federated events on component-based software engineering and software architecture (CompArch 2015), which include WICSA 2015 (12th Working IEEE / IFIP Conference on Software Architecture) and CBSE 2015 (18th International ACM SIGSOFT Symposium on Component-Based Software Engineering).},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1145/3405837.3411375,
author = {Khooi, Xin Zhe and Csikor, Levente and Kang, Min Suk and Divakaran, Dinil Mon},
title = {In-Network Defense against AR-DDoS Attacks},
year = {2021},
isbn = {9781450380485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405837.3411375},
doi = {10.1145/3405837.3411375},
abstract = {The prevalence of the disruptive amplified reflection DDoS (AR-DDoS) attacks is one of the biggest concerns of all network operators today. The increasing magnitude of new attacks are rendering existing measures (e.g., scrubbing services) inefficient. This work demonstrates DIDA, an efficient, topology independent, in-line AR-DDoS detection and mitigation architecture that operates entirely in the data plane.},
booktitle = {Proceedings of the SIGCOMM '20 Poster and Demo Sessions},
pages = {18–20},
numpages = {3},
keywords = {in-network, detection and mitigation, amplification attacks, denial-of-service attacks, reflection attacks, programmable switches},
location = {Virtual event},
series = {SIGCOMM '20}
}

@inproceedings{10.5555/2893711.2893715,
author = {Rauter, Tobias and H\"{o}ller, Andrea and Iber, Johannes and Kreiner, Christian},
title = {Thingtegrity: A Scalable Trusted Computing Architecture for the Internet of Things},
year = {2016},
isbn = {9780994988607},
publisher = {Junction Publishing},
address = {USA},
abstract = {Remote attestation is used to prove the integrity of one system (prover) to another (challenger). The prover measures its configuration and transmits the result to the challenger for verification. Common attestation methods lead to complex configuration measurements (e.g., hash of all executables), which are updated every time one of the software modules changes. The updated configuration has to be distributed to all possible challengers since they need a reference to enable the verification. Recently, an idea of reducing the complexity of the configuration measurement by taking into account privileges of software modules has been presented. However, this approach has not been exhaustively analyzed since, as yet, no implementation exists. Especially in the Internet of Things (IoT) domain, where resources are constrained strictly while devices are potentially physically exposed to adversaries, attestation methodologies with reduced overhead are desireable. In this work we combine binary-, property- and privilege-based remote attestation to integrate a trusted computing architecture transparently into iotivity, an existing IoT middleware. As a first step, we aim to enable to attestation of the integrity of complex devices with different services to constrained devices. With the help of an illustrative simulated environment, we show that our architecture reduces the effort of bootstrapping trusted relations, as well as updating single modules in the whole system, even if software and devices from different vendors are combined.},
booktitle = {Proceedings of the 2016 International Conference on Embedded Wireless Systems and Networks},
pages = {23–34},
numpages = {12},
location = {Graz, Austria},
series = {EWSN '16}
}

@inproceedings{10.1145/3210259.3210262,
author = {Erb, Benjamin and Mei\ss{}ner, Dominik and Kargl, Frank and Steer, Benjamin A. and Cuadrado, Felix and Margan, Domagoj and Pietzuch, Peter},
title = {Graphtides: A Framework for Evaluating Stream-Based Graph Processing Platforms},
year = {2018},
isbn = {9781450356954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210259.3210262},
doi = {10.1145/3210259.3210262},
abstract = {Stream-based graph systems continuously ingest graph-changing events via an established input stream, performing the required computation on the corresponding graph. While there are various benchmarking and evaluation approaches for traditional, batch-oriented graph processing systems, there are no common procedures for evaluating stream-based graph systems. We, therefore, present GraphTides, a generic framework which includes the definition of an appropriate system model, an exploration of the parameter space, suitable workloads, and computations required for evaluating such systems. Furthermore, we propose a methodology and provide an architecture for running experimental evaluations. With our framework, we hope to systematically support system development, performance measurements, engineering, and comparisons of stream-based graph systems.},
booktitle = {Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences \&amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {3},
numpages = {10},
keywords = {evolving graphs, evaluation, temporal graphs, stream-based graphs, measurements, graph processing, graph analytics},
location = {Houston, Texas},
series = {GRADES-NDA '18}
}

@inproceedings{10.1145/3240508.3240642,
author = {Pang, Haitian and Zhang, Cong and Wang, Fangxin and Hu, Han and Wang, Zhi and Liu, Jiangchuan and Sun, Lifeng},
title = {Optimizing Personalized Interaction Experience in Crowd-Interactive Livecast: A Cloud-Edge Approach},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240642},
doi = {10.1145/3240508.3240642},
abstract = {Enabling users to interact with broadcasters and audience, the crowd-interactive livecast greatly improves viewer's quality of experience (QoE) and attracts millions of daily active users recently. In addition to striking the balance between resource utilization and viewers' QoE met in the traditional video streaming service, this novel service needs to take supererogatory efforts to improve the interaction QoE, which reflects the viewer interaction experience. To tackle this issue, we conduct measurement studies over a large-scale dataset crawled from a representative livecast service provider. We observe that the individual's interaction pattern is quite heterogeneous: only 10\% viewers proactively participate in the interaction, and the rest viewers usually watch passively. Incorporating the insight into the emerging cloud-edge architecture, we propose a framework PIECE, which optimizes the Personalized Interaction Experience with Cloud-Edge architecture (PIECE) for intelligent user access control and livecast distribution. In particular, we first devise a novel deep neural network based algorithm to predict users' interaction intensity using the historical viewer pattern. We then design an algorithm to maximize the individual's QoE, by strategically matching viewer sessions and transcoding-delivery paths over cloud-edge infrastructure. Finally, we use trace-driven experiments to verify the effectiveness of PIECE. Our results show that our prediction algorithm outperforms the state-of-the-art algorithms with a much smaller mean absolute error (40\% reduction). Furthermore, in comparison with the cloud-based video delivery strategy, the proposed framework can simultaneously improve the average viewers QoE (26\% improvement) and interaction QoE (21\% improvement), while maintaining a high streaming bitrate.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1217–1225},
numpages = {9},
keywords = {viewer interaction, cloud-edge, interactive live streaming},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3414045.3415941,
author = {Wazid, Mohammad and Bera, Basudeb and Mitra, Ankush and Das, Ashok Kumar and Ali, Rashid},
title = {Private Blockchain-Envisioned Security Framework for AI-Enabled IoT-Based Drone-Aided Healthcare Services},
year = {2020},
isbn = {9781450381055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414045.3415941},
doi = {10.1145/3414045.3415941},
abstract = {Internet of Drones (IoD) architecture is designed to support a co-ordinated access for the airspace using the unmanned aerial vehicles (UAVs) known as drones. Recently, IoD communication environment is extremely useful for various applications in our daily activities. Artificial intelligence (AI)-enabled Internet of Things (IoT)-based drone-aided healthcare service is a specialized environment which can be used for different types of tasks, for instance, blood and urine samples collections, medicine delivery and for the delivery of other medical needs including the current pandemic of COVID-19. Due to wireless nature of communication among the deployed drones and their ground station server, several attacks (for example, replay, man-in-the-middle, impersonation and privileged-insider attacks) can be easily mounted by malicious attackers. To protect such attacks, the deployment of effective authentication, access control and key management schemes are extremely important in the IoD environment. Furthermore, combining the blockchain mechanism with deployed authentication make it more robust against various types of attacks. To mitigate such issues, we propose a private-blockchain based framework for secure communication in an IoT-enabled drone-aided healthcare environment. The blockchain-based simulation of the proposed framework has been carried out to measure its impact on various performance parameters.},
booktitle = {Proceedings of the 2nd ACM MobiCom Workshop on Drone Assisted Wireless Communications for 5G and Beyond},
pages = {37–42},
numpages = {6},
keywords = {healthcare, authentication, security, blockchain, privacy, internet of drones (IoD)},
location = {London, United Kingdom},
series = {DroneCom '20}
}

@inproceedings{10.1145/2676723.2691890,
author = {Hoffman, Mark E.},
title = {Student Board-Writing to Integrate Communication Skills and Content to Enhance Student Learning (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2691890},
doi = {10.1145/2676723.2691890},
abstract = {Students frequently use a whiteboard to individually demonstrate understanding or interactively develop understanding in groups. The practice is employed to develop content knowledge; however, an opportunity to intentionally develop communication skills is overlooked. On the other hand, instructors carefully integrate instructional organization and communication to maximize student content learning. Taken together, this presents an opportunity for students to intentionally improve their communication skills in the service of content learning. This poster details a "work in progress" project where students follow organizational guidelines for written homework and board-writing to facilitate in-class, problem solution presentation. Problem solution presentations occur during one class period each week. Students are given colored pencils for written homework and colored markers for board-writing. Student work including written homework and board-writing was gathered from the 2013 and 2014 iterations of a sophomore-level computer architecture course. Preliminary analysis of student work shows that students either adopt the guidelines from the start or learn to use them through feedback and practice. On the semester-end survey, students report that adopting guidelines for written homework, board-writing, and color scheme improve presentation, and board-writing improves student learning. Future work includes gathering data from more students including recorded student presentations, developing quantitative scores to analyze student work, and developing measures of student learning.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {684},
numpages = {1},
keywords = {student board-writing, content learning, communication skills},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1145/2641798.2641814,
author = {Xu, Yi and Helal, Sumi},
title = {Application Caching for Cloud-Sensor Systems},
year = {2014},
isbn = {9781450330305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641798.2641814},
doi = {10.1145/2641798.2641814},
abstract = {Driven by critical and pressing smart city applications, accessing massive numbers of sensors by cloud-hosted services is becoming an emerging and inevitable situation. Na\"{\i}vely connecting massive numbers of sensors to the cloud raises major scalability and energy challenges. An architecture embodying distributed optimization is needed to manage the scale and to allow limited energy sensors to last longer in such a dynamic and high-velocity big data system. We developed a multi-tier architecture which we call Cloud, Edge and Beneath (CEB). Based on CEB, we propose an Application Fragment Caching Algorithm (AFCA) which selectively caches application fragments from the cloud to lower layers of CEB to improve cloud scalability. Through experiments, we show and measure the effect of AFCA on cloud scalability.},
booktitle = {Proceedings of the 17th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {303–306},
numpages = {4},
keywords = {cloud-sensor systems, application caching, cloud computing},
location = {Montreal, QC, Canada},
series = {MSWiM '14}
}

@inproceedings{10.1145/3308558.3313551,
author = {Yoon, Changhoon and Kim, Kwanwoo and Kim, Yongdae and Shin, Seungwon and Son, Sooel},
title = {Doppelg\"{a}ngers on the Dark Web: A Large-Scale Assessment on Phishing Hidden Web Services},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313551},
doi = {10.1145/3308558.3313551},
abstract = {Anonymous network services on the World Wide Web have emerged as a new web architecture, called the Dark Web. The Dark Web has been notorious for harboring cybercriminals abusing anonymity. At the same time, the Dark Web has been a last resort for people who seek freedom of the press as well as avoid censorship. This anonymous nature allows website operators to conceal their identity and thereby leads users to have difficulties in determining the authenticity of websites. Phishers abuse this perplexing authenticity to lure victims; however, only a little is known about the prevalence of phishing attacks on the Dark Web. We conducted an in-depth measurement study to demystify the prevalent phishing websites on the Dark Web. We analyzed the text content of 28,928 HTTP Tor hidden services hosting 21 million dark webpages and confirmed 901 phishing domains. We also discovered a trend on the Dark Web in which service providers perceive dark web domains as their service brands. This trend exacerbates the risk of phishing for their service users who remember only a partial Tor hidden service address. Our work facilitates a better understanding of the phishing risks on the Dark Web and encourages further research on establishing an authentic and reliable service on the Dark Web.},
booktitle = {The World Wide Web Conference},
pages = {2225–2235},
numpages = {11},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3286062.3286086,
author = {Sharma, Puneet and Raghuramu, Arun and Lee, David and Saxena, Vinay and Chuah, Chen-Nee},
title = {We Don't Need No Licensing Server},
year = {2018},
isbn = {9781450361200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286062.3286086},
doi = {10.1145/3286062.3286086},
abstract = {Cloudification of edge to core infrastructure has led to new and rich application and service deployment and operational models. These ecosystems have complex relationships between the application vendors, infrastructure operators and application users. Traditional licensing and compliance enforcement methods such as those based on in person audits and dynamic issuing of license keys inhibit the resource provisioning and consumption flexibility offered by cloudified services due to scalability and management overheads. In this work, we argue the need for a trusted framework for application usage rights compliance. This new architecture named "Metered Boot" provides a way to realize trusted, capacity/usage based rights compliance for service deployments that allows decoupling of usage rights governed by application vendors from the resource provisioning by the infrastructure provider. We have built a Metered Boot prototype for a particular usecase of NFV usage rights compliance.},
booktitle = {Proceedings of the 17th ACM Workshop on Hot Topics in Networks},
pages = {162–168},
numpages = {7},
location = {Redmond, WA, USA},
series = {HotNets '18}
}

@inproceedings{10.1145/3319535.3363279,
author = {Mo, Fan and Shahin Shamsabadi, Ali and Katevas, Kleomenis and Cavallaro, Andrea and Haddadi, Hamed},
title = {Poster: Towards Characterizing and Limiting Information Exposure in DNN Layers},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363279},
doi = {10.1145/3319535.3363279},
abstract = {Pre-trained Deep Neural Network (DNN) models are increasingly used in smartphones and other user devices to enable prediction services, leading to potential disclosures of (sensitive) information from training data captured inside these models. Based on the concept of generalization error, we propose a framework to measure the amount of sensitive information memorized in each layer of a DNN. Our results show that, when considered individually, the last layers encode a larger amount of information from the training data compared to the first layers. We find that the same DNN architecture trained with different datasets has similar exposure per layer. We evaluate an architecture to protect the most sensitive layers within an on-device Trusted Execution Environment (TEE) against potential white-box membership inference attacks without the significant computational overhead.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2653–2655},
numpages = {3},
keywords = {sensitive information exposure, privacy, deep learning, trusted execution environment, training data},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1145/3014431,
author = {Hu, Han and Wen, Yonggang and Chua, Tat-Seng and Li, Xuelong},
title = {Cost-Optimized Microblog Distribution over Geo-Distributed Data Centers: Insights from Cross-Media Analysis},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3014431},
doi = {10.1145/3014431},
abstract = {The unprecedent growth of microblog services poses significant challenges on network traffic and service latency to the underlay infrastructure (i.e., geo-distributed data centers). Furthermore, the dynamic evolution in microblog status generates a huge workload on data consistence maintenance. In this article, motivated by insights of cross-media analysis-based propagation patterns, we propose a novel cache strategy for microblog service systems to reduce the inter-data center traffic and consistence maintenance cost, while achieving low service latency. Specifically, we first present a microblog classification method, which utilizes the external knowledge from correlated domains, to categorize microblogs. Then we conduct a large-scale measurement on a representative online social network system to study the category-based propagation diversity on region and time scales. These insights illustrate social common habits on creating and consuming microblogs and further motivate our architecture design. Finally, we formulate the content cache problem as a constrained optimization problem. By jointly using the Lyapunov optimization framework and simplex gradient method, we find the optimal online control strategy. Extensive trace-driven experiments further demonstrate that our algorithm reduces the system cost by 24.5\% against traditional approaches with the same service latency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {40},
numpages = {18},
keywords = {Social media analytics, data center, cross-media analysis, performance optimization}
}

@inproceedings{10.1145/3019612.3019724,
author = {Lautner, Douglas and Hua, Xiayu and Debates, Scott and Song, Miao and Shah, Jagat and Ren, Shangping},
title = {BaaS (Bluetooth-as-a-Sensor): Conception, Design and Implementation on Mobile Platforms},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019724},
doi = {10.1145/3019612.3019724},
abstract = {As network connectivity becomes more capable, mobile devices are evolving into sensory data accumulators. Bluetooth (BT) components, which are widely used for communication purposes, also have the potential to become contextual sensors by constantly listening to information broadcast by nearby Bluetooth Low Energy (BLE) beacons. Compared to traditional Micro-Electro-Mechanical (MEMs) based contextual sensors, Bluetooth-as-a-Sensor (BaaS) provides a wider sensing spectrum and more comprehensive environmental information. However, current implementations of BT are optimized as a data transmitter, therefore deploying BaaS on a traditional mobile platform would cause an unacceptably high current drain and hence a significant reduction in battery life. Our objective is to conquer the current drain problem associated with having continuous wireless BT sensing. We provide a novel BaaS-based architecture which utilizes an energy-efficient sensor fusion core (SFC) to execute heavy-duty and long-standing tasks. We also present an optimized duty cycle algorithm that minimizes the duty cycle while guaranteeing an application's QoS requirements. Both BaaS architecture and algorithm are implemented and deployed on a Moto X platform and then applied to an indoor location service for consumer use validation. The performance of the BaaS-based architecture is evaluated for both average current drain and location accuracy. Data measured on Moto X shows that when using the BaaS architecture, the battery life is 5 times longer than using the traditional BT architecture.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {550–556},
numpages = {7},
keywords = {embedded system, cellphone development, mobile sensing, mobile device, energy efficiency},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/2809826.2809836,
author = {Khan, Yasir Imtiaz and Al-shaer, Ehab and Rauf, Usman},
title = {Cyber Resilience-by-Construction: Modeling, Measuring \&amp; Verifying},
year = {2015},
isbn = {9781450338219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809826.2809836},
doi = {10.1145/2809826.2809836},
abstract = {The need of cyber security is increasing as cyber attacks are escalating day by day. Cyber attacks are now so many and sophisticated that many will unavoidably get through. Therefore, there is an immense need to employ resilient architectures to defend known or unknown threats. Engineer- ing resilient system/infrastructure is a challenging task, that implies how to measure the resilience and how to obtain sufficient resilience necessary to maintain its service delivery under diverse situations. This paper has two fold objective, the first is to propose a formal approach to measure cyber resilience from different aspects (i.e., attacks, failures) and at different levels (i.e., pro-active, resistive and reactive). To achieve the first objective, we propose a formal frame- work named as: Cyber Resilience Engineering Framework (CREF). The second objective is to build a resilient system by construction. The idea is to build a formal model of a cyber system, which is initially not resilient with respect to attacks. Then by systematic refinements of the formal model and by its model checking, we attain resiliency. We exemplify our technique through the case study of simple cyber security device (i.e., network firewall).},
booktitle = {Proceedings of the 2015 Workshop on Automated Decision Making for Active Cyber Defense},
pages = {9–14},
numpages = {6},
keywords = {cyber resilience, algebraic petri nets, model checking, firewall},
location = {Denver, Colorado, USA},
series = {SafeConfig '15}
}

