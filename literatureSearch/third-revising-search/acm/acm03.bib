@inproceedings{10.1145/3374664.3375750,
author = {Nguyen, Hoai Viet and Lo Iacono, Luigi},
title = {CREHMA: Cache-Aware REST-Ful HTTP Message Authentication},
year = {2020},
isbn = {9781450371070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374664.3375750},
doi = {10.1145/3374664.3375750},
abstract = {Scalability and security are two important elements of contemporary distributed software systems. The Web vividly shows that while complying with the constraints defined by the architectural style REST, the layered design of software with intermediate systems enables to scale at large. Intermediaries such as caches, however, interfere with the security guarantees of the industry standard for protecting data in transit on the Web, TLS, as in these circumstances the TLS channel already terminates at the intermediate system's server. For more in-depth defense strategies, service providers require message-oriented security means in addition to TLS. These are hardly available and only in the form of HTTP signature schemes that do not take caches into account either. In this paper we introduce CREHMA, a REST-ful HTTP message signature scheme that guarantees the integrity and authenticity of Web assets from end-to-end while simultaneous allowing service providers to enjoy the benefits of Web caches. Decisively, CREHMA achieves these guarantees without having to trust on the integrity of the cache and without requiring making changes to existing Web caching systems. In extensive experiments we evaluated CREHMA and found that it only introduces marginal impacts on metrics such as latency and data expansion while providing integrity protection from end to end. CREHMA thus extends the possibilities of service providers to achieve an appropriate balance between scalability and security.},
booktitle = {Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy},
pages = {49–60},
numpages = {12},
keywords = {rest, web caching, http, end-to-end security, signature},
location = {New Orleans, LA, USA},
series = {CODASPY '20}
}

@inproceedings{10.1145/2568088.2568098,
author = {Ewing, John M. and Menasc\'{e}, Daniel A.},
title = {A Meta-Controller Method for Improving Run-Time Self-Architecting in SOA Systems},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2568098},
doi = {10.1145/2568088.2568098},
abstract = {This paper builds on SASSY, a system for automatically generating SOA software architectures that optimize a given utility function of multiple QoS metrics. In SASSY, SOA software systems are automatically re-architected when services fail or degrade. Optimizing both architecture and service provider selection presents a pair of nested NP-hard problems. Here we adapt hill-climbing, beam search, simulated annealing, and evolutionary programming to both architecture optimization and service provider selection. Each of these techniques has several parameters that influence their efficiency. We introduce in this paper a meta-controller that automates the run-time selection of heuristic search techniques and their parameters. We examine two different meta-controller implementations that each use online learning. The first implementation identifies the best heuristic search combination from various prepared combinations. The second implementation analyzes the current self-architecting problem (e.g. changes in performance metrics, service degradations/failures) and looks for similar, previously encountered re-architecting problems to find an effective heuristic search combination for the current problem. A large set of experiments demonstrates the effectiveness of the first meta-controller implementation and indicates opportunities for improving the second meta-controller implementation.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {173–184},
numpages = {12},
keywords = {combinatorial search techniques, heuristic search, soa, metaheuristics, meta-controlled qos optimization, autonomic computing, automated run-time software architecting},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1145/2661714.2661726,
author = {Stohr, Denny and Wilk, Stefan and Effelsberg, Wolfgang},
title = {Monitoring of User Generated Video Broadcasting Services},
year = {2014},
isbn = {9781450331579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661714.2661726},
doi = {10.1145/2661714.2661726},
abstract = {Mobile video broadcasting services offer users the opportunity to instantly share content from their mobile handhelds to a large audience over the Internet. However, existing data caps in cellular network contracts and limitations in their upload capabilities restrict the adoption of mobile video broadcasting services. Additionally, the quality of those video streams is often reduced by the lack of skills of recording users and the technical limitations of the video capturing devices. Our research focuses on large-scale events that attract dozens of users to record video in parallel. In many cases, available network infrastructure is not capable to upload all video streams in parallel. To make decisions on how to appropriately transmit those video streams, a suitable monitoring of the video generation process is required. For this scenario, a measurement framework is proposed that allows Internet-scale mobile broadcasting services to deliver samples in an optimized way. Our framework architecture analyzes three zones for effectively monitoring user-generated video. Besides classical Quality of Service metrics on the network state, video quality indicators and additional auxiliary sensor information is gathered. Aim of this framework is an efficient coordination of devices and their uploads based on the currently observed system state.},
booktitle = {Proceedings of the First International Workshop on Internet-Scale Multimedia Management},
pages = {39–42},
numpages = {4},
keywords = {video composition, network monitoring, mobile, measurement, mix, cellular networks, video broadcast},
location = {Orlando, Florida, USA},
series = {WISMM '14}
}

@inproceedings{10.5555/2821357.2821365,
author = {Nikravesh, Ali Yadavar and Ajila, Samuel A. and Lung, Chung-Horng},
title = {Towards an Autonomic Auto-Scaling Prediction System for Cloud Resource Provisioning},
year = {2015},
publisher = {IEEE Press},
abstract = {This paper investigates the accuracy of predictive auto-scaling systems in the Infrastructure as a Service (IaaS) layer of cloud computing. The hypothesis in this research is that prediction accuracy of auto-scaling systems can be increased by choosing an appropriate time-series prediction algorithm based on the performance pattern over time. To prove this hypothesis, an experiment has been conducted to compare the accuracy of time-series prediction algorithms for different performance patterns. In the experiment, workload was considered as the performance metric, and Support Vector Machine (SVM) and Neural Networks (NN) were utilized as time-series prediction techniques. In addition, we used Amazon EC2 as the experimental infrastructure and TPC-W as the benchmark to generate different workload patterns. The results of the experiment show that prediction accuracy of SVM and NN depends on the incoming workload pattern of the system under study. Specifically, the results show that SVM has better prediction accuracy in the environments with periodic and growing workload patterns, while NN outperforms SVM in forecasting unpredicted workload pattern. Based on these experimental results, this paper proposes an architecture for a self-adaptive prediction suite using an autonomic system approach. This suite can choose the most suitable prediction technique based on the performance pattern, which leads to more accurate prediction results.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {35–45},
numpages = {11},
keywords = {resource provisioning, auto-scaling, workload pattern, neural networks, cloud computing, support vector machine, autonomic},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/2619239.2631456,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
title = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2631456},
doi = {10.1145/2619239.2631456},
abstract = {After more than two decades of evolution, TCP and its end host based modifications can still suffer from severely degraded performance under real-world challenging network conditions. The reason, as we observe, is due to TCP family's fundamental architectural deficiency, which hardwires packet-level events to control responses and ignores emprical performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented Congestion Control (PCC), a new congestion control architecture in which each sender controls its sending strategy based on empirically observed performance metrics. We show through preliminary experimental results that PCC achieves consistently high performance under various challenging network conditions.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {365–366},
numpages = {2},
keywords = {congestion control},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@article{10.1145/2740070.2631456,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
title = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2631456},
doi = {10.1145/2740070.2631456},
abstract = {After more than two decades of evolution, TCP and its end host based modifications can still suffer from severely degraded performance under real-world challenging network conditions. The reason, as we observe, is due to TCP family's fundamental architectural deficiency, which hardwires packet-level events to control responses and ignores emprical performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented Congestion Control (PCC), a new congestion control architecture in which each sender controls its sending strategy based on empirically observed performance metrics. We show through preliminary experimental results that PCC achieves consistently high performance under various challenging network conditions.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {365–366},
numpages = {2},
keywords = {congestion control}
}

@inproceedings{10.1145/2970276.2970338,
author = {Peldszus, Sven and Kulcs\'{a}r, G\'{e}za and Lochau, Malte and Schulze, Sandro},
title = {Continuous Detection of Design Flaws in Evolving Object-Oriented Programs Using Incremental Multi-Pattern Matching},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970338},
doi = {10.1145/2970276.2970338},
abstract = {Design flaws in object-oriented programs may seriously corrupt code quality thus increasing the risk for introducing subtle errors during software maintenance and evolution. Most recent approaches identify design flaws in an ad-hoc manner, either focusing on software metrics, locally restricted code smells, or on coarse-grained architectural anti-patterns. In this paper, we utilize an abstract program model capturing high-level object-oriented code entities, further augmented with qualitative and quantitative design-related information such as coupling/cohesion. Based on this model, we propose a comprehensive methodology for specifying object-oriented design flaws by means of compound rules integrating code metrics, code smells and anti-patterns in a modular way. This approach allows for efficient, automated design-flaw detection through incremental multi-pattern matching, by facilitating systematic information reuse among multiple detection rules as well as between subsequent detection runs on continuously evolving programs. Our tool implementation comprises well-known anti-patterns for Java programs. The results of our experimental evaluation show high detection precision, scalability to real-size programs, as well as a remarkable gain in efficiency due to information reuse.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {578–589},
numpages = {12},
keywords = {object-oriented software architecture, continuous software evolution, design-flaw detection},
location = {Singapore, Singapore},
series = {ASE '16}
}

@article{10.1145/2975161,
author = {Bouraoui, Hasna and Jerad, Chadlia and Chattopadhyay, Anupam and Hadj-Alouane, Nejib Ben},
title = {Hardware Architectures for Embedded Speaker Recognition Applications: A Survey},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/2975161},
doi = {10.1145/2975161},
abstract = {Authentication technologies based on biometrics, such as speaker recognition, are attracting more and more interest thanks to the elevated level of security offered by these technologies. Despite offering many advantages, such as remote use and low vulnerability, speaker recognition applications are constrained by the heavy computational effort and the hard real-time constraints. When such applications are run on an embedded platform, the problem becomes more challenging, as additional constraints inherent to this specific domain are added. In the literature, different hardware architectures were used/designed for implementing a process with a focus on a given particular metric. In this article, we give a survey of the state-of-the-art works on implementations of embedded speaker recognition applications. Our aim is to provide an overview of the different approaches dealing with acceleration techniques oriented towards speaker and speech recognition applications and attempt to identify the past, current, and future research trends in the area. Indeed, on the one hand, many flexible solutions were implemented, using either General Purpose Processors or Digital Signal Processors. In general, these types of solutions suffer from low area and energy efficiency. On the other hand, high-performance solutions were implemented on Application Specific Integrated Circuits or Field Programmable Gate Arrays but at the expense of flexibility. Based on the available results, we compare the application requirements vis-\`{a}-vis the performance achieved by the systems. This leads to the projection of new research trends that can be undertaken in the future.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {apr},
articleno = {78},
numpages = {28},
keywords = {acceleration, speaker recognition, Embedded hardware, classification algorithms and implementations}
}

@inproceedings{10.1145/2668322.2668327,
author = {Papalambrou, Andreas and Stefanidis, Kyriakos and Gialelis, John and Serpanos, Dimitrios},
title = {Detection, Traceback and Filtering of Denial of Service Attacks in Networked Embedded Systems},
year = {2014},
isbn = {9781450329323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668322.2668327},
doi = {10.1145/2668322.2668327},
abstract = {This work presents a composite scheme for detection, traceback and filtering of distributed denial of service (DDoS) attacks in networked embedded systems. A method based on algorithmic analysis of various node and network parameters is used to detect attacks while a packet marking method is used to mitigate the effects of the attack by filtering the incoming traffic that is part of this attack and trace back to the origin of the attack. The combination of the detection and mitigation methods provide an increased level of security in comparison to approaches based on a single method. Furthermore, the scheme is developed in a way to comply with the novel SHIELD secure architecture being developed, which aims at providing interoperability with other secure components as well as metrics to quantify their security properties.},
booktitle = {Proceedings of the 9th Workshop on Embedded Systems Security},
articleno = {5},
numpages = {8},
keywords = {denial of service attacks, embedded systems security},
location = {New Delhi, India},
series = {WESS '14}
}

@inproceedings{10.1145/3297663.3310307,
author = {van der Sar, Jerom and Donkervliet, Jesse and Iosup, Alexandru},
title = {Yardstick: A Benchmark for Minecraft-like Services},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3310307},
doi = {10.1145/3297663.3310307},
abstract = {Online gaming applications entertain hundreds of millions of daily active players and often feature vastly complex architecture. Among online games, Minecraft-like games simulate unique (e.g., modifiable) environments, are virally popular, and are increasingly provided as a service. However, the performance of Minecraft-like services, and in particular their scalability, is not well understood. Moreover, currently no benchmark exists for Minecraft-like games. Addressing this knowledge gap, in this work we design and use the Yardstick benchmark to analyze the performance of Minecraft-like services. Yardstick is based on an operational model that captures salient characteristics of Minecraft-like services. As input workload, Yardstick captures important features, such as the most-popular maps used within the Minecraft community. Yardstick captures system- and application-level metrics, and derives from them service-level metrics such as frequency of game-updates under scalable workload. We implement Yardstick, and, through real-world experiments in our clusters, we explore the performance and scalability of popular Minecraft-like servers, including the official vanilla server, and the community-developed servers Spigot and Glowstone. Our findings indicate the scalability limits of these servers, that Minecraft-like services are poorly parallelized, and that Glowstone is the least viable option among those tested.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {243–253},
numpages = {11},
keywords = {minecraft, as a service, distributed systems, online gaming, yardstick, benchmark},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/2747470.2747471,
author = {Dudouet, Florian and Edmonds, Andrew and Erne, Michael},
title = {Reliable Cloud-Applications: An Implementation through Service Orchestration},
year = {2015},
isbn = {9781450334761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2747470.2747471},
doi = {10.1145/2747470.2747471},
abstract = {As cloud-deployed applications became more and more mainstream, continuously more complex services started to be deployed; indeed where initially monolithic applications were simply ported to the cloud, applications are now more and more often composed of micro-services. This improves the flexibility of an application but also makes it more complex due to the sheer number of services comprising it.As deployment and runtime management becomes more complex, orchestration software are becoming necessary to completely manage the lifecycle of cloud applications. One crucial problem remaining is how these applications can be made reliable in the cloud, a naturally unreliable environment.In this paper we propose concepts and architectures which were implemented in our orchestration software to guarantee reliability. Our initial implementation also relies on Monasca, a well-known monitoring software for Open-Stack, to gather proper metric and execute threshold-based actions. This allows us to show how service reliability can be ensured using orchestration and how a proper incident-management software feeding decisions to the orchestration engine ensures high-availability of all components of managed applications.},
booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
pages = {1–6},
numpages = {6},
keywords = {incident management, orchestration, cloud computing, reliability},
location = {Bordeaux, France},
series = {AIMC '15}
}

@inproceedings{10.1145/3377813.3381349,
author = {Diamantopoulos, Nikos and Wong, Jeffrey and Mattos, David Issa and Gerostathopoulos, Ilias and Wardrop, Matthew and Mao, Tobias and McFarland, Colin},
title = {Engineering for a Science-Centric Experimentation Platform},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381349},
doi = {10.1145/3377813.3381349},
abstract = {Netflix is an internet entertainment service that routinely employs experimentation to guide strategy around product innovations. As Netflix grew, it had the opportunity to explore increasingly specialized improvements to its service, which generated demand for deeper analyses supported by richer metrics and powered by more diverse statistical methodologies. To facilitate this, and more fully harness the skill sets of both engineering and data science, Netflix engineers created a science-centric experimentation platform that leverages the expertise of scientists from a wide range of backgrounds working on data science tasks by allowing them to make direct code contributions in the languages used by them (Python and R). Moreover, the same code that runs in production is able to be run locally, making it straightforward to explore and graduate both metrics and causal inference methodologies directly into production services.In this paper, we provide two main contributions. Firstly, we report on the architecture of this platform, with a special emphasis on its novel aspects: how it supports science-centric end-to-end workflows without compromising engineering requirements. Secondly, we describe its approach to causal inference, which leverages the potential outcomes conceptual framework to provide a unified abstraction layer for arbitrary statistical models and methodologies.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {191–200},
numpages = {10},
keywords = {software architecture, causal inference, science-centric, experimentation, A/B testing},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@article{10.1145/3233182,
author = {Ji, Kecheng and Ling, Ming and Shi, Longxing and Pan, Jianping},
title = {An Analytical Cache Performance Evaluation Framework for Embedded Out-of-Order Processors Using Software Characteristics},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3233182},
doi = {10.1145/3233182},
abstract = {Utilizing analytical models to evaluate proposals or provide guidance in high-level architecture decisions is been becoming more and more attractive. A certain number of methods have emerged regarding cache behaviors and quantified insights in the last decade, such as the stack distance theory and the memory level parallelism (MLP) estimations. However, prior research normally oversimplified the factors that need to be considered in out-of-order processors, such as the effects triggered by reordered memory instructions, and multiple dependences among memory instructions, along with the merged accesses in the same MSHR entry. These ignored influences actually result in low and unstable precisions of recent analytical models.By quantifying the aforementioned effects, this article proposes a cache performance evaluation framework equipped with three analytical models, which can more accurately predict cache misses, MLPs, and the average cache miss service time, respectively. Similar to prior studies, these analytical models are all fed with profiled software characteristics in which case the architecture evaluation process can be accelerated significantly when compared with cycle-accurate simulations.We evaluate the accuracy of proposed models compared with gem5 cycle-accurate simulations with 16 benchmarks chosen from Mobybench Suite 2.0, Mibench 1.0, and Mediabench II. The average root mean square errors for predicting cache misses, MLPs, and the average cache miss service time are around 4\%, 5\%, and 8\%, respectively. Meanwhile, the average error of predicting the stall time due to cache misses by our framework is as low as 8\%. The whole cache performance estimation can be sped by about 15 times versus gem5 cycle-accurate simulations and 4 times when compared with recent studies. Furthermore, we have shown and studied the insights between different performance metrics and the reorder buffer sizes by using our models. As an application case of the framework, we also demonstrate how to use our framework combined with McPAT to find out Pareto optimal configurations for cache design space explorations.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {aug},
articleno = {79},
numpages = {25},
keywords = {Analytical models, cache misses, cache miss service time, software characteristics, memory level parallelism}
}

@inproceedings{10.1145/3452296.3472922,
author = {Fayed, Marwan and Bauer, Lorenz and Giotsas, Vasileios and Kerola, Sami and Majkowski, Marek and Odintsov, Pavel and Sitnicki, Jakub and Chung, Taejoong and Levin, Dave and Mislove, Alan and Wood, Christopher A. and Sullivan, Nick},
title = {The Ties That Un-Bind: Decoupling IP from Web Services and Sockets for Robust Addressing Agility at CDN-Scale},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472922},
doi = {10.1145/3452296.3472922},
abstract = {The couplings between IP addresses, names of content or services, and socket interfaces, are too tight. This impedes system manageability, growth, and overall provisioning. In turn, large-scale content providers are forced to use staggering numbers of addresses, ultimately leading to address exhaustion (IPv4) and inefficiency (IPv6).In this paper, we revisit IP bindings, entirely. We attempt to evolve addressing conventions by decoupling IP in DNS and from network sockets. Alongside technologies such as SNI and ECMP, a new architecture emerges that ``unbinds'' IP from services and servers, thereby returning IP's role to merely that of reachability. The architecture is under evaluation at a major CDN in multiple datacenters. We show that addresses can be generated randomly emph{per-query}, for 20M+ domains and services, from as few as ~4K addresses, 256 addresses, and even emph{one} IP address. We explain why this approach is transparent to routing, L4/L7 load-balancers, distributed caching, and all surrounding systems -- and is emph{highly desirable}. Our experience suggests that many network-oriented systems and services (e.g., route leak mitigation, denial of service, measurement) could be improved, and new ones designed, if built with addressing agility.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {433–446},
numpages = {14},
keywords = {provisioning, programmable sockets, addressing, content distribution},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up Architectural SW Health Builds in a New Product Line Generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start from scratch but takes into consideration the architecture and assets of the former product line generation. Being able to accommodate legacy and 3rd party code is one of the major product line qualities to be met. On the other side, product line qualities like reusability, maintainability and alterability, i.e. being able to cope up with a large amount of variability, with configurability and fast integratability are major drivers.While setting up a new product line generation and thus a new corresponding architecture, we this time focused on architectural software (SW) health and tracking of architectural metrics from the very beginning. Taking the definition of "architecture being a set of design decisions" [18] literally, we attempt to implement an architectural check for every design decision taken. Architectural design decisions in our understanding do not only - and even not mainly - deal with the definition of components and their interaction but with patterns and rules or anti-patterns. The rules and anti-patterns, "what not to do" or more often also "what not to do &lt;u&gt;any more&lt;/u&gt;", is even more important in setting up a new product line generation because developers are not only used to the old style of developing and the old architecture, but also still have to develop assets for both generations.In this article we describe selected architectural checks that we have implemented, the layered architecture check and the check for usage of obsolete services. Additionally we discuss selected architectural metrics: the coupling coefficient metrics and the instability metrics. In the summary and outlook we describe our experiences and still open topics in setting up architectural SW health checks for a large-scale product line.The real-world examples are taken from the domain of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {embedded software, architectural checks, technical debt, software architecture, software erosion, architectural technical debt, product line development},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.1109/CCGrid.2014.103,
author = {Wu, Jie and Jansen, Christoph and Beier, Maximilian and Witt, Michael and Krefting, Dagmar},
title = {Extending XNAT towards a Cloud-Based Quality Assessment Platform for Retinal Optical Coherence Tomographies},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.103},
doi = {10.1109/CCGrid.2014.103},
abstract = {Neurosciencific research is increasingly based on image analysis methods. Large sets of imaging data are processed using complex image analysis tools. While today magnetic resonance imaging (MRI) is widely used for both functional and anatomical analysis of the human brain, new imaging modalities are beginning to prove their capabilities for neurological research. Among them, optical coherence tomography (OCT) allows for noninvasive visualization of anatomical structures on a micrometer scale. Becoming a standard diagnostic tool in ophthalmology, it is of rising interest for neurological research. Crucial to all data analysis methods is the quality of the input data. The platform presented in this paper is designed for automatic quality assessment of retinal OCTs. It extends the image management platform XNAT by services to calculate and store quality measures. It is also extensible regarding new quality measure algorithms, allowing the developer to upload Matlab code, compile it for the infrastructure's hardware architecture and test it in the system. The image processing tools to calculate the quality measures are provided as a cloud-based service employing OpenStack as underlying IT infrastructure. The prototype implementation encompassing security and performance aspects are presented.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {764–773},
numpages = {10},
keywords = {SaaS, XNAT, cloud, medical imaging, neuroimaging, IaaS, OCT},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1109/ICSE-NIER.2019.00037,
author = {Aniche, Maur\'{\i}cio and Yoder, Joseph W. and Kon, Fabio},
title = {Current Challenges in Practical Object-Oriented Software Design},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00037},
doi = {10.1109/ICSE-NIER.2019.00037},
abstract = {According to the extensive 50-year-old body of knowledge in object-oriented programming and design, good software designs are, among other characteristics, lowly coupled, highly cohesive, extensible, comprehensible, and not fragile. However, with the increased complexity and heterogeneity of contemporary software, this might not be enough.This paper discusses the practical challenges of object-oriented design in modern software development. We focus on three main challenges: (1) how technologies, frameworks, and architectures pressure developers to make design decisions that they would not take in an ideal scenario, (2) the complexity of current real-world problems require developers to devise not only a single, but several models for the same problem that live and interact together, and (3) how existing quality assessment techniques for object-oriented design should go beyond high-level metrics.Finally, we propose an agenda for future research that should be tackled by both scientists and practitioners soon. This paper is a call for arms for more reality-oriented research on the object-oriented software design field.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {113–116},
numpages = {4},
keywords = {software architecture, domain modeling, software design, object-oriented design, class design, software engineering, object-oriented programming},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/3238147.3240463,
author = {Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin},
title = {Achieving Test Automation with Testers without Coding Skills: An Industrial Report},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240463},
doi = {10.1145/3238147.3240463},
abstract = {We present a process driven test automation solution which enables delegating (part of) automation tasks from test automation engineer (expensive resource) to test analyst (non-developer, less expensive). In our approach, a test automation engineer implements test steps (or actions) which are executed automatically. Such automated test steps represent user actions in the system under test and specified by a natural language which is understandable by a non-technical person. Then, a test analyst with a domain knowledge organizes automated steps combined with test input to create an automated test case. It should be emphasized that the test analyst does not need to possess programming skills to create, modify or execute automated test cases. We refine benchmark test automation architecture to be better suitable for an effective separation and sharing of responsibilities between the test automation engineer (with coding skills) and test analyst (with a domain knowledge). In addition, we propose a metric to empirically estimate cooperation between test automation engineer and test analyst's works. The proposed automation solution has been defined based on our experience in the development and maintenance of Helsenorg, the national electronic health services in Norway which has had over one million of visits per month past year, and we still use it to automate the execution of regression tests.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {749–756},
numpages = {8},
keywords = {process-driven test automation, DSL for test automation, keyword-driven test automation, Helsenorge, Test automation},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3343031.3350592,
author = {Galteri, Leonardo and Seidenari, Lorenzo and Bertini, Marco and Uricchio, Tiberio and Del Bimbo, Alberto},
title = {Fast Video Quality Enhancement Using GANs},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350592},
doi = {10.1145/3343031.3350592},
abstract = {Video compression algorithms result in a reduction of image quality, because of their lossy approach to reduce the required bandwidth. This affects commercial streaming services such as Netflix, or Amazon Prime Video, but affects also video conferencing and video surveillance systems. In all these cases it is possible to improve the video quality, both for human view and for automatic video analysis, without changing the compression pipeline, through a post-processing that eliminates the visual artifacts created by the compression algorithms. Generative Adversarial Networks have obtained extremely high quality results in image enhancement tasks; however, to obtain such results large generators are usually employed, resulting in high computational costs and processing time. In this work we present an architecture that can be used to reduce the computational cost and that has been implemented on mobile devices. A possible application is to improve video conferencing, or live streaming. In these cases there is no original uncompressed video stream available. Therefore, we report results using no-reference video quality metric showing high naturalness and quality even for efficient networks.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1065–1067},
numpages = {3},
keywords = {video compression, real-time enhancement, video streaming, video quality enhancement, gans},
location = {Nice, France},
series = {MM '19}
}

@article{10.1145/3308897.3308943,
author = {Luong, Doanh Kim and Ali, Muhammad and Benamrane, Fouad and Ammar, Ibrahim and Hu, Yim-Fun},
title = {Seamless Handover for Video Streaming over an SDN-Based Aeronautical Communications Network},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3308897.3308943},
doi = {10.1145/3308897.3308943},
abstract = {There have been increasing interests in applying Software Defined Networking (SDN) to aeronautical communications primarily for air traffic management purposes. From the service passenger communications' point of view, a major goal is to improve passengers' perception of quality of experience on the infotainment services being provided for them. Due to the high speed of aircrafts and the use of multiple radio technologies during different flight phases and across different areas, vertical handovers between these different radio technologies are envisaged. This poses a challenge to maintain the quality of service during such handovers, especially for high bandwidth applications such as video streaming. This paper proposes an SDN-based aeronautical communications architecture consisting of both satellite and terrestrial-based radio technology. In addition, an experimental implementation of the Locator ID Separation Protocol (LISP) protocol with built-in multi-homing capability over the SDN-based architecture was proposed to handle vertical handovers between the satellite and other radio technologies onboard the aircraft. By using both objective and subjective Quality of Experience (QoE) metrics, the simulation experiments show the benefit of combining LISP with SDN to improve the video streaming quality during the handover in the aeronautical communication environment.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {98–99},
numpages = {2},
keywords = {sdn, aeronautical communications, lisp mobility, vertical handovers, multi-homing}
}

@inproceedings{10.1145/2815317.2815321,
author = {da Silva, Madalena P. and Dantas, Mario A.R. and Gon\c{c}alves, Alexandre L. and Pinto, Alex R.},
title = {A Managing QoE Approach for Provisioning User Experience Aware Services Using SDN},
year = {2015},
isbn = {9781450337571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815317.2815321},
doi = {10.1145/2815317.2815321},
abstract = {Provision and delivery of services with quality is a classic research problem, however the computational resources available in the network infrastructure of providers are, usually, managed with conventional Quality of Service (QoS) parameters. This paper presents an approach of Quality of Experience (QoE) management for providing services aware of the user experience. QoE modeling and architecture are proposed, with a semantic engine able to learn the user's experience during the use of a service, detecting violations of QoS metrics and providing information, allowing the controller to perform actions in the elements of the Software Defined Networking. The experimental results demonstrate that the proposal is feasible and functional and that the time spent between QoE detection and adaptation of policies in network resources do not influence the quality perceived by the user.},
booktitle = {Proceedings of the 11th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {51–58},
numpages = {8},
keywords = {semantic engine, software-defined network, quality of service, quality of experience},
location = {Cancun, Mexico},
series = {Q2SWinet '15}
}

@inproceedings{10.1145/3364641.3364680,
author = {Couto, Christian Marlon Souza and Terra, Ricardo},
title = {A Quality-Oriented Approach to Recommend Move Method Refactorings},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364680},
doi = {10.1145/3364641.3364680},
abstract = {Refactoring processes are common in large software systems, especially when developers neglect architectural erosion process for long periods. Even though there are many refactoring approaches, very few consider the refactoring impact on the software quality.Given this scenario, we propose a refactoring approach to software systems oriented to software quality metrics. Based on the QMOOD (Quality Model for Object Oriented Design), the main idea is to move methods between classes in order to maximize the values of the quality metrics. Using a formal notation, we describe the problem as follows. Given a software system S, our approach recommends a sequence of refactorings R1, R2,..., Rn that result in system versions S1, S2,..., Sn, where quality(Si+1) &gt; quality(Si).We performed three types of evaluation to verify the usefulness of our implemented tool, called QMove. First, we applied our approach on 13 open-source systems that we modified by randomly moving a subset of its methods to other classes, then checking if our approach would recommend the moved methods to return to their original place, and we achieve 84\% recall, on average. Second, we compared QMove against two state-of-art refactoring tools (JMove and JDeodorant) on the 13 previously evaluated systems, and QMove showed better recall value (84\%) than the other two (30\% and 29\%, respectively). Third, we conducted the same comparison among QMove, JMove, and JDeodorant applied in two proprietary systems where experts evaluated the quality of the recommendations. QMove obtained eight positively evaluated recommendations from the experts, against two and none of JMove and JDeodorant, respectively.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {315},
numpages = {1},
keywords = {software architecture, refactoring, quality metrics},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.1145/3386367.3431306,
author = {Marques, Jonatas and Levchenko, Kirill and Gaspary, Luciano},
title = {IntSight: Diagnosing SLO Violations with in-Band Network Telemetry},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431306},
doi = {10.1145/3386367.3431306},
abstract = {Performance requirements for many of today's high-perfor-mance networks are expressed as service-level objectives (SLOs), i.e., precise guarantees, typically on latency and bandwidth, that a user can expect from the network. For network operators, monitoring their own SLO compliance, and quickly diagnosing any violations, is a critical element for effective operations. Unfortunately, existing network architectures are not engineered for this purpose; there is no mechanism, for example, for the operator to monitor the 95th per-centile latency experienced by a customer. Data plane programmability has made per-packet measurements possible but brings the challenge of keeping the monitoring overhead low and practical. In this paper, we present IntSight, a system for highly accurate and fine-grained detection and diagnosis of SLO violations. The main contribution of IntSight is, building upon in-band telemetry, introducing path-wise computation of network metrics and selective generation of reports. We show the effectiveness of IntSight by way of two use cases. Our evaluation using real networks also shows that IntSight generates up to two orders of magnitude less monitoring traffic than state-of-the-art approaches. Furthermore, its processing and memory requirements are low and therefore compatible with currently existing programmable platforms.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {421–434},
numpages = {14},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@inproceedings{10.1145/3437120.3437292,
author = {Maikantis, Theodoros and Tsintzira, Angeliki-Agathi and Ampatzoglou, Apostolos and Arvanitou, Elvira-Maria and Chatzigeorgiou, Alexander and Stamelos, Ioannis and Bibi, Stamatia and Deligiannis, Ignatios},
title = {Software Architecture Reconstruction via a Genetic Algorithm: Applying the Move Class Refactoring},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437292},
doi = {10.1145/3437120.3437292},
abstract = {Modularity is one of the four key principles of software design and architecture. According to this principle, software should be organized into modules that are tightly linked internally (high cohesion), whereas at the same time as independent from other modules as possible (low coupling). However, in practice, this principle is violated due to poor architecting design decisions, lack of time, or coding shortcuts, leading to a phenomenon termed as architectural technical debt (ATD). To alleviate this problem (lack of architectural modularity), the most common solution is the application of a software refactoring, namely Move Class—i.e., moving classes (the core artifact in object-oriented systems) from one module to another. To identify Move Class refactoring opportunities, we employ a search-based optimization process, relying on optimization metrics, through which optimal moves are derived. Given the extensive search space required for applying a brute-force search strategy, in this paper, we propose the use of a genetic algorithm that re-arranges existing software classes into existing or new modules (software packages in Java, or folders in C++). To validate the usefulness of the proposed refactorings, we performed an industrial case study on three projects (from the Aviation, Healthcare, and Manufacturing application domains). The results of the study indicate that the proposed architecture reconstruction is able to improve modularity, improving both coupling and cohesion. The obtained results can be useful to practitioners through an open source tool; whereas at the same point, they open interesting future work directions.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {135–139},
numpages = {5},
location = {Athens, Greece},
series = {PCI '20}
}

@inproceedings{10.1145/2841113.2841114,
author = {Forget, Alain and Chiasson, Sonia and Biddle, Robert},
title = {Choose Your Own Authentication},
year = {2015},
isbn = {9781450337540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2841113.2841114},
doi = {10.1145/2841113.2841114},
abstract = {To solve the long-standing problems users have in creating and remembering text passwords, a wide variety of alternative authentication schemes have been proposed. Some of these schemes outperform others by various metrics in various contexts. However, none unilaterally outperform all others, and so text passwords persist as the main scheme applications depend upon. In this paper, we challenge the long-standing assumption that only one authentication scheme can be offered by an application service. We propose Choose Your Own Authentication (CYOA): a novel authentication architecture that enables users to choose a scheme amongst several available alternatives. CYOA would enable users to select whichever scheme best suits their preferences, abilities, and usage context. Existing text password systems could easily be replaced. Furthermore, the three-party architecture would enable delegating the management of authentication systems to trusted-third parties. The architecture allows rapid deployment and testing of novel authentication technologies. Our two-week usability study suggests that participants were willing to leverage alternative schemes. Participants were confident that CYOA could keep their financial information secure.},
booktitle = {Proceedings of the 2015 New Security Paradigms Workshop},
pages = {1–15},
numpages = {15},
keywords = {survey, user study, Authentication, usable security},
location = {Twente, Netherlands},
series = {NSPW '15}
}

@inproceedings{10.1145/3291533.3291540,
author = {Dalgkitsis, Anestis and Louta, Malamati and Karetsos, George T.},
title = {Traffic Forecasting in Cellular Networks Using the LSTM RNN},
year = {2018},
isbn = {9781450366106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291533.3291540},
doi = {10.1145/3291533.3291540},
abstract = {In this work we design and implement a Neural Network that can identify recurrent patterns in various metrics which can be then used for cellular network traffic forecasting. Based on a custom architecture and memory, this Neural Network can handle prediction tasks faster and more accurately in real life scenarios. This approach offers a solution for service providers to enhance cellular network performance, by utilizing effectively the available resources. In order to provide a robust conclusion about the performance and precision of the proposed Neural Network, multiple predictions were made using the same data-set and the results were compared against other similar algorithms from the literature.},
booktitle = {Proceedings of the 22nd Pan-Hellenic Conference on Informatics},
pages = {28–33},
numpages = {6},
keywords = {recurrent neural networks, cellular networks, traffic forecasting, long-short term memory},
location = {Athens, Greece},
series = {PCI '18}
}

@inproceedings{10.1145/3323716.3323729,
author = {Berba, Elizalde M. and Palaoag, Thelma D.},
title = {Improving Customer Satisfaction on Internet Services in L-NU Using Virtualized AAA Network Architecture},
year = {2019},
isbn = {9781450361040},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323716.3323729},
doi = {10.1145/3323716.3323729},
abstract = {This study mainly aims to improve the satisfaction level on internet services in Lyceum-Northwestern University (L-NU). From a traditional network architecture, the researchers made use of a virtualized Authentication, Authorization and Accounting (AAA) network architecture to improve the internet services provided to the students of L-NU. For the methodology of this study, the researchers had to make use of a network lifecycle called Prepare, Plan, Design, Implement, Operate, and Optimize (PPDIOO) and there was also a need to combine both quantitative and qualitative research approach. To make this happen, the researchers had to fulfill the following objectives: a) determine the current network setup of L-NU, b) measure the current satisfaction level of the users, c) design, develop and implement a virtualized AAA network architecture, d) measure the satisfaction level of the users who have used the AAA network setup, and e) compare the measured satisfaction level from the users who used the internet facilities using the traditional network architecture and satisfaction level from users who have used the internet facilities after the implementation of AAA network architecture. As a result, it was found out that the implementation of the new network architecture has significantly improved the internet service level of L-NU which is reflected by a higher customer satisfaction rating. Therefore, the researchers conclude that it is most essential that AAA network architecture be implemented to enterprise type of network setup such as but not limited to education institutions in managing their internet services. Consequently, this kind of network architecture lead to a more effective and more efficient way of managing network resources of an institution or an organization while further improving the satisfaction level. In order to optimize the AAA network architecture and gain more implementation advantages, virtualization technology was used to contain and run numerous operating system instances such as four physical servers into one single physical server which favors to saving resources such as energy, space, money and of which also leads to simplified administration.},
booktitle = {Proceedings of the 8th International Conference on Informatics, Environment, Energy and Applications},
pages = {178–183},
numpages = {6},
keywords = {authorization, hypervisor, AAA, virtualization, accounting, PPDIOO, authentication, customer satisfaction},
location = {Osaka, Japan},
series = {IEEA '19}
}

@article{10.1109/TASLP.2019.2915922,
author = {Xu, Zhen and Sun, Chengjie and Long, Yinong and Liu, Bingquan and Wang, Baoxun and Wang, Mingjiang and Zhang, Min and Wang, Xiaolong},
title = {Dynamic Working Memory for Context-Aware Response Generation},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2915922},
doi = {10.1109/TASLP.2019.2915922},
abstract = {In human-to-human conversations, the context generally provides several backgrounds and strategic points for the following response. Therefore, many response generation approaches have explored the methodologies to incorporate the context into the encoder–decoder architecture, to generate context-aware responses that are remarkably relevant and cohesive to the given context. However, most approaches pay less attention to semantic interactions implicitly existing within contextual utterances, which are of great importance to capture semantic clues of the given dialog context, indeed. This paper proposes a dynamic working memory mechanism to model long-term semantic hints in the conversation context, by performing semantic interactions between utterances and updating context representation dynamically. Then, the outputs of the dynamic working memory are employed to provide helpful clues for the encoder–decoder architecture to generate responses to the given dialog. We have evaluated the proposed approach on Twitter Customer Service Corpus and OpenSubtitles Corpus, with several automatic evaluation metrics and the human evaluation, and the empirical results show the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {1419–1431},
numpages = {13}
}

@article{10.1145/3377138,
author = {Wu, Hao and Liu, Weizhi and Lin, Huanxin and Wang, Cho-Li},
title = {A Model-Based Software Solution for Simultaneous Multiple Kernels on GPUs},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3377138},
doi = {10.1145/3377138},
abstract = {As a critical computing resource in multiuser systems such as supercomputers, data centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking is an intuitive solution to underutilization in GPGPU computing. Recently proposed solutions of multitasking GPUs can be classified into two categories: (1) spatially partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels simultaneously within a CU. Compared to SPS, SMK can improve resource utilization even further due to the interleaving of instructions from kernels with low dynamic resource contentions.However, it is hard to implement SMK on current GPU architecture, because (1) techniques for applying SMK on top of GPU hardware scheduling policy are scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences of concurrently executed kernels. In this article, we propose a lightweight and effective performance model to evaluate the complex interferences of SMK. Based on the probability of independent events, our performance model is built from a totally new angle and contains limited parameters. Then, we propose a metric, symbiotic factor, which can evaluate an SMK scheme so that kernels with complementary resource utilization can corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing and kernel stretching techniques and integrate them to apply SMK on GPUs instead of simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based concurrent kernel execution whose kernel launching order brings fast execution time, the results of corunning kernel pairs show 11\%, 18\%, and 12\% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the results show 29\%, 18\%, and 51\% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average.},
journal = {ACM Trans. Archit. Code Optim.},
month = {mar},
articleno = {7},
numpages = {26},
keywords = {concurrent kernel execution, GPGPU}
}

@inproceedings{10.1145/3459104.3459148,
author = {Pradhan, Ayush and Joy, Eldhose and Jawagal, Harsha and Prasad Jayaraman, Sundar},
title = {A Framework for Leveraging Contextual Information in Automated Domain Specific Comprehension},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459148},
doi = {10.1145/3459104.3459148},
abstract = {When it comes to information, Enterprises today are seen as a black hole, a mass of it goes in but gets difficult to extract the practical knowledge out of it. An automated system that has the ability to consume this large mass of information and provide specific, knowledgeable, domain-oriented responses back, will go a long way in unlocking the value of this large-scale unstructured information. In a bid to enrich the answering system's accuracy in Machine Reading Comprehension (MRC), we propose a domain-specific Question Answers (QuAns) framework that specifically aims to auto-generate questions from a domain-based document using an improvised Sequence to Sequence (Seq2Seq) technique equipped with Attention and Copy mechanism. The generated questions are conditioned on a set of candidate answers, derived using a combination of heuristic-driven and graph-based techniques. Further, it also leverages the contextual information by pooling strategy to build an automated response system using a deep custom fine-tuned Bidirectional Encoder Representations from Transformers (BERT) framework and retrieving the top-k contexts for a user query. The evaluation of the QuAns architecture is performed in combination with human supervision as at times, the automated metrics like BLEU, Exact Match (EM), F1 score, etc. fail to gauge the diverse semantic and structural aspects of a generated response. Primarily, the proffered ensemble technique has leveraged the augmented domain knowledge to enrich the answering response efficacy and improving the EM and F1 score by 14.86\% and 12.76\% respectively over Vanilla BERT architecture. To enhance the user experience, the conversational system is equipped with Natural Language Generation (NLG) to present a human-readable response. Our architectural pipeline aims to provide a one-stop solution for the organizations in processing huge volumes of multidisciplinary data by significantly reducing the human introspection and the overhead cost.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {263–270},
numpages = {8},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3422392.3422501,
author = {Barros, Daniel D. R. and Horita, Fl\'{a}vio and Fantinato, Denis G.},
title = {Data Mining Tool to Discover DevOps Trends from Public Repositories: Predicting Release Candidates with Gthbmining.Rc},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422501},
doi = {10.1145/3422392.3422501},
abstract = {Public repositories have been performing an essential role in bringing software and services to technical communities and general users. Most of the cases, public repositories have a DevOps tool, with a live and historical database behind it, to support delivering and all steps this software or service should adopt before going to production. This paper introduces gthbmining, a data mining set of tools to discover DevOps trends from public repositories, and presents the module gthbmining.rc. Considering the premise of a GitHub public repository, the main contribution here is predicting release candidates, an important label a software release has. The methodology, architecture, components and interfaces are explained, as well as potential users. The results show a reliable and flexible tool, as classifiers metrics and graphics are provided, along with the possibility to add new data mining algorithms in the open source module presented. Related works are also supplied, and a conclusion shows the outcomes gthbmining.rc can provide.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {658–663},
numpages = {6},
keywords = {Data Mining, Release Candidate, DevOps, GitHub Mining Tool},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2851613.2851874,
author = {Abderrahim, Wiem and Choukair, Zied},
title = {PaaS Dependability Integration Architecture Based on Cloud Brokering},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851874},
doi = {10.1145/2851613.2851874},
abstract = {Cloud computing has revolutionized the way IT is provisioned nowadays since it exposes computing capabilities as rental resources to consumers. The emergence of cloud computing services hasn't though prevented outages in these environments even among high profile ranked cloud providers. Tremendous efforts concentrated on fault management measures have been applied for these environments. But they have been focused mainly on the IaaS service model and have been operated on the cloud provider side alone. In this context, this paper proposes an architecture for cloud brokering that implements dependability properties in an end to end way involving different cloud actors and all over the cloud service models SaaS, PaaS and IaaS.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {484–487},
numpages = {4},
keywords = {cloud provider, fault tolerance, fault forecasting, PaaS, IaaS, cloud broker, SaaS, fault management, dependability},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2856636.2876471,
author = {Zodik, Gabi},
title = {Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2876471},
doi = {10.1145/2856636.2876471},
abstract = {The second wave of change presented by the age of mobility, wearables, and IoT focuses on how organizations and enterprises, from a wide variety of commercial areas and industries, will use and leverage the new technologies available. Businesses and industries that don't change with the times will simply cease to exist.Applications need to be powered by cognitive and contextual technologies to support real-time proactive decisions. These decisions will be based on the mobile context of a specific user or group of users, incorporating location, time of day, current user task, and more. Driven by the huge amounts of data produced by mobile and wearables devices, and influenced by privacy concerns, the next wave in computing will need to exploit data and computing at the edge of the network. Future mobile apps will have to be cognitive to 'understand' user intentions based on all the available interactions and unstructured data.Mobile applications are becoming increasingly ubiquitous, going beyond what end users can easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business (B2B) apps, only about 30\% of the development efforts appear in the interface of the mobile app. For example, areas such as the collaborative nature of the software or the shortened development cycle and time-to-market are not apparent to end users. The other 70\% of the effort invested is dedicated to integrating the applications with back-office systems and developing those aspects of the application that operate behind the scenes.An important, yet often complex, part of the solution and mobile app takes place far from the public eye-in the back-office environment. It is there that various aspects of customer relationship management must be addressed: tracking usage data, pushing out messaging as needed, distributing apps to employees within the enterprise, and handling the wide variety of operational and management tasks-often involving the collection and monitoring of data from sensors and wearable devices. All this must be carried out while addressing security concerns that range from verifying user identities, to data protection, to blocking attempted breaches of the organization, and activation of malicious code. Of course, these tasks must be augmented by a systematic approach and vigilant maintenance of user privacy.The first wave of the mobile revolution focused on development platforms, run-time platforms, deployment, activation, and management tools for multi-platform environments, including comprehensive mobile device management (MDM). To realize the full potential of this revolution, we must capitalize on information about the context within which mobile devices are used. With both employees and customers, this context could be a simple piece of information such as the user location or time of use, the hour of the day, or the day of the week. The context could also be represented by more complex data, such as the amount of time used, type of activity performed, or user preferences. Further insight could include the relationship history with the user and the user's behavior as part of that relationship, as well as a long list of variables to be considered in various scenarios. Today, with the new wave of wearables, the definition of context is being further extended to include environmental factors such as temperature, weather, or pollution, as well as personal factors such as heart rate, movement, or even clothing worn.In both B2E and B2C situations, a context-dependent approach, based on the appropriate context for each specific user, offers a superior tool for working with both employees and clients alike. This mode of operation does not start and end with the individual user. Rather, it takes into account the people surrounding the user, the events taking place nearby, appliances or equipment activated, the user's daily schedule, as well as other, more general information, such as the environment and weather.Developing enterprise-wide, context-dependent, mobile solutions is still a complex challenge. A system of real added-value services must be developed, as well as a comprehensive architecture. These four-tier architectures comprise end-user devices like wearables and smartphones, connected to systems of engagement (SoEs), and systems of record (SoRs). All this is needed to enable data analytics and collection in the context where it is created. The data collected will allow further interaction with employees or customers, analytics, and follow-up actions based on the results of that analysis. We also need to ensure end-to-end (E2E) security across these four tiers, and to keep the data and application contexts in sync. These are just some of the challenges being addressed by IBM Research.As an example, these technologies could be deployed in the retail space, especially in brick-and-mortar stores. Identifying a customer entering a store, detecting her location among the aisles, and cross-referencing that data with the customer's transaction history, could lead to special offers tailor-made for that specific customer or suggestions relevant to her purchasing process. This technology enables real-world implementation of metrics, analytics, and other tools familiar to us from the online realm. We can now measure visits to physical stores in the same way we measure web page hits: analyze time spent in the store, the areas visited by the customer, and the results of those visits. In this way, we can also identify shoppers wandering around the store and understand when they are having trouble finding the product they want to purchase. We can also gain insight into the standard traffic patterns of shoppers and how they navigate a store's floors and departments. We might even consider redesigning the store layout to take advantage of this insight to enhance sales.In healthcare, the context can refer to insight extracted from data received from sensors on the patient, from either his mobile device or wearable technology, and information about the patient's environment and location at that moment in time. This data can help determine if any assistance is required. For example, if a patient is discharged from the hospital for continued at-home care, doctors can continue to remotely monitor his condition via a system of sensors and analytic tools that interpret the sensor readings.This approach can also be applied to the area of safety. Scientists at IBM Research are developing a platform that collects and analyzes data from wearable technology to protect the safety of employees working in construction, heavy industry, manufacturing, or out in the field. This solution can serve as a real-time warning system by analyzing information gathered from wearable sensors embedded in personal protective equipment, such as smart safety helmets and protective vests, and in the workers' individual smartphones. These sensors can continuously monitor a worker's pulse rate, movements, body temperature, and hydration level, as well as environmental factors such as noise level, and other parameters. The system can provide immediate alerts to the worker about any dangers in the work environment to prevent possible injury. It can also be used to prevent accidents before they happen or detect accidents once they occur. For example, with sophisticated algorithms, we can detect if a worker falls based on a sudden difference in elevations detected by an accelerometer, and then send an alert to notify her peers and supervisor or call for help. Monitoring can also help ensure safety in areas where continuous exposure to heat or dangerous materials must be limited based on regulated time periods.Mobile technologies can also help manage events with massive numbers of participants, such as professional soccer games, music festivals, and even large-scale public demonstrations, by sending alerts concerning long and growing lines or specific high-traffic areas. These technologies can be used to detect accidents typical of large-scale gatherings, send warnings about overcrowding, and alert the event organizers. In the same way, they can alleviate parking problems or guide public transportation operators- all via analysis and predictive analytics.IBM Research - Haifa is currently involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers have a special expertise in time- and location-based intelligent applications, including visual maps that display activity contexts and predictive analytics systems for mobile data and users. In another area, IBM researchers in Haifa are developing new cognitive services driven from the unique data available on mobile and wearable devices. Looking to the future, the IBM Research team is further advancing the integration of wearable technology, augmented reality systems, and biometric tools for mobile user identity validation.Managing contextual data and analyzing the interaction between the different kinds of data presents fascinating challenges for the development of next-generation programming. For example, we need to rethink when and where data processing and computations should occur: Is it best to leave them at the user-device level, or perhaps they should be moved to the back-office systems, servers, and/or the cloud infrastructures with which the user device is connected? New-age applications are becoming more and more distributed. They operate on a wide range of devices, such as wearable technologies, use a variety of sensors, and depend on cloud-based systems.As a result, a new distributed programming paradigm is emerging to meet the needs of these use-cases and real-time scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and data in business systems, and must be able to shift computation from the cloud to the edge, based on context in close to real-time. By processing data at the edge of the network, close to where the interactions and processing are happening, we can help reduce latency and offer new opportunities for improved privacy and security.Despite all these interactions, data collection, and the analytic insights based upon them-we cannot forget the issues of privacy. Without a proper and reliable solution that offers more control over what personal data is shared and how it is used, people will refrain from sharing information. Such sharing is necessary for developing and understanding the context in which people are carrying out various actions, and to offer them tools and services to enhance their actions.In the not-so-distant future, we anticipate the appearance of ad-hoc networks for wearable technology systems that will interact with one another to further expand the scope and value of available context-dependent data.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {11–12},
numpages = {2},
location = {Goa, India},
series = {ISEC '16}
}

@inproceedings{10.1145/3466933.3466945,
author = {Oliveira, Breno Silva and Ara\'{u}jo, \'{I}talo L. and Paiva, Joseane O. V. and Junior, Evilasio C. and Andrade, Rossana M. C.},
title = {Refactoring Decision Based on Measurements for IoHT Apps},
year = {2021},
isbn = {9781450384919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466933.3466945},
doi = {10.1145/3466933.3466945},
abstract = {Internet of Things (IoT) provides smart objects with the ability to connect to the Internet, allowing the exchange of information among them to provide a certain service and the development of innovative applications in several domains, including e-Health, in which it is called Internet of Health Things (IoHT). This domain can be critical specially when the application deals with the monitoring of the user health in real-time, what demands software quality assurance, even more than in other applications. Measures can be used to support that, for example, measures can suggest which components need refactoring to improve the software code, thus improving the application. In this work, we report how to do that with two existing measures that guide the refactoring process of an IoHT application for fall detection, called WatchAlert. These measures indicate that changes in both the architecture and the algorithms for fall detection should occur. After the refactoring, the app accuracy was improved from 73.3\% to 92.7\%. We believe that this work can contribute to other studies focusing on developing applications on the IoHT domain using a methodology, a set of refactoring techniques, and lessons learned that could be replicated to improve the quality of this type of application.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Information Systems},
articleno = {12},
numpages = {9},
keywords = {Refactoring, Internet of Things, e-Health, Measures, Fall detection},
location = {Uberl\^{a}ndia, Brazil},
series = {SBSI '21}
}

@inproceedings{10.5555/2821327.2821330,
author = {Zimmermann, Olaf},
title = {Metrics for Architectural Synthesis and Evaluation: Requirements and Compilation by Viewpoint: An Industrial Experience Report},
year = {2015},
publisher = {IEEE Press},
abstract = {During architectural analysis and synthesis, architectural metrics are established tacitly or explicitly. In architectural evaluation, these metrics are then consulted to assess whether architectures are fit for purpose and in line with recommended practices and published architectural knowledge. This experience report presents a personal retrospective of the author's use of architectural metrics during 20 years in IT architect roles in professional services as well as research and development. This reflection drives the identification of use cases, critical success factors and elements of risk for architectural metrics management. An initial catalog of architectural metrics is compiled next, which is organized by viewpoints and domains. The report concludes with a discussion of practical impact of architectural metrics and potential research topics in this area.},
booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
pages = {8–14},
numpages = {7},
keywords = {architectural metrics management, viewpoints, architectural reviews, integration, architectural metrics, enterprise information systems, patterns},
location = {Florence, Italy},
series = {SAM '15}
}

@inproceedings{10.1145/3341105.3374026,
author = {Araldo, Andrea and Stefano, Alessandro Di and Stefano, Antonella Di},
title = {Resource Allocation for Edge Computing with Multiple Tenant Configurations},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374026},
doi = {10.1145/3341105.3374026},
abstract = {Edge Computing (EC) consists in deploying computational resources, e.g., memory, CPUs, at the Edge of the network, e.g., base stations, access points, and run there a part of the computation currently running on the Cloud. This approach promises to reduce latency, inter-domain traffic and enhance user experience. Since resources at the Edge are scarce, resource allocation is crucial for EC. While most of the studies assume users interact directly with the Edge submitting a sequence of tasks, we instead consider that users will interact with different Service Providers (SPs), as they currently do in the Web. We therefore consider the case of a Network Operator (NO) that owns the resources at the Edge and must decide how much resource to allocate to the different tenants (SPs).We propose MORA, a polynomial time strategy which allows the NO to maximize its utility, which can be inter-domain traffic savings, improved users' QoE or other metrics of interest. The core of MORA is that (i) it exploits service elasticity, i.e., the fact that services can adapt to the resources allocated by the NO and rely on a remote Cloud for the excess of computation, (ii) it is suitable for micro-services architecture, which decomposes a single service in a set of components, which MORA places in the different computational nodes of the Edge and (iii) it copes with multi-dimensional resources, e.g., memory and CPUs. After analyzing the properties of the algorithm, we show numerically that it performs close to the optimum. To guarantee reproducibility, the numerical evaluation is performed on publicly available traces from Google and Alibaba clusters and in synthetic scenarios and our code is open source.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1190–1199},
numpages = {10},
keywords = {container systems, resource allocation, edge computing, network optimization, cloud computing},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.5555/2820518.2820566,
author = {Mirakhorli, Mehdi and Cleland-Huang, Jane},
title = {Modifications, Tweaks, and Bug Fixes in Architectural Tactics},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Architectural qualities such as reliability, performance, and security, are often realized in a software system through the adoption of tactical design decisions such as the decision to use redundant processes, a heartbeat monitor, or a specific authentication mechanism. Such decisions are critical for delivering a system that meets its quality requirements. Despite the stability of high-level decisions, our analysis has shown that tactic-related classes tend to be modified more frequently than other classes and are therefore stronger predictors of change than traditional Object-Oriented coupling and cohesion metrics. In this paper we present the results from this initial study, including an analysis of why tactic-related classes are changed, and a discussion of the implications of these findings for maintaining architectural quality over the lifetime of a software system.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {377–380},
numpages = {4},
keywords = {modifications, architectural decisions, metrics, tactics, bugs},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.5555/2840819.2840915,
author = {Kim, Yeseong and Imani, Mohsen and Patil, Shruti and Rosing, Tajana S.},
title = {CAUSE: Critical Application Usage-Aware Memory System Using Non-Volatile Memory for Mobile Devices},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {Mobile devices are severely limited in memory, which affects critical user-experience metrics such as application service time. Emerging non-volatile memory (NVM) technologies such as STT-RAM and PCM are ideal candidates to provide higher memory capacity with negligible energy overhead. However, existing memory management systems overlook mobile users application usage which provides crucial cues for improving user experience. In this paper, we propose CAUSE, a novel memory system based on DRAM-NVM hybrid memory architecture. CAUSE takes explicit account of the application usage patterns to distinguish data criticality and identify suitable swap candidates. We also devise NVM hardware design optimized for the access characteristics of the swapped pages. We evaluate CAUSE on a real Android smartphone and NVSim simulator using user application usage logs. Our experimental results show that the proposed technique achieves 32\% faster launch time for mobile applications while reducing energy cost by 90\% and 44\% on average over non-optimized STT-RAM and PCM, respectively.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {690–696},
numpages = {7},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1145/2628588.2628598,
author = {Sharakhov, Nikita and Marojevic, Vuk and Romano, Ferdinando and Polys, Nicholas and Dietrich, Carl},
title = {Visualizing Real-Time Radio Spectrum Access with CORNET3D},
year = {2014},
isbn = {9781450330152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628588.2628598},
doi = {10.1145/2628588.2628598},
abstract = {Modern web technology enables the 3D portrayal of real-time data. WebSocket connections provide data over the web without the time-consuming overhead of HTTP requests. The server-side "push" paradigm is particularly useful for creating novel tools such as CORNET3D, where real-time 3D visualization is required. CORNET3D is an innovative Web3D interface to a research and education test bed for Dynamic Spectrum Access (DSA). Our system can drive several 2D and 3D portrayals of spectral data and radio performance metrics from a live, online system. The testbed can further integrate the data portrayals into a multi-user "serious game" to teach students about strategies for the optimal use of spectrum resources by providing them with real-time scoring based on their choices of radio transmission parameters. This paper describes the web service architecture and Webd3D front end for our DSA testbed, detailing new methods for spectrum visualization and the applications they enable.},
booktitle = {Proceedings of the 19th International ACM Conference on 3D Web Technologies},
pages = {109–116},
numpages = {8},
keywords = {web applications, computer graphics, WebSockets, WebGL, HTML5},
location = {Vancouver, British Columbia, Canada},
series = {Web3D '14}
}

@inproceedings{10.1145/3366424.3382670,
author = {Tiwary, Mayank and Mishra, Pritish and Jain, Shashank and Puthal, Deepak},
title = {Data Aware Web-Assembly Function Placement},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382670},
doi = {10.1145/3366424.3382670},
abstract = {Existing container based serverless computing systems are limited by cold-start problems and complex architecture for stateful services, multi-tenancy, etc. This paper presents serverless functions to be placed as per data locality and executed as a web-assembly sandbox, which results better execution latency and reduced network usage as compared to the existing architectures. The designed serverless runtime features resource isolation in terms of CPU, Memory, and file-system isolation and falicitates multi-tenancy executions. The proposed architecture is evaluated using IoT workloads with different performance metrics.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {4–5},
numpages = {2},
keywords = {Servelress, Data Locality, Web-Assembly, Multi-Tenancy},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3338466.3358916,
author = {Alder, Fritz and Asokan, N. and Kurnikov, Arseny and Paverd, Andrew and Steiner, Michael},
title = {S-FaaS: Trustworthy and Accountable Function-as-a-Service Using Intel SGX},
year = {2019},
isbn = {9781450368261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338466.3358916},
doi = {10.1145/3338466.3358916},
abstract = {Function-as-a-Service (FaaS) is a recent and popular cloud computing paradigm in which the function provider specifies a function to be run and is billed only for the computational resources used by that function. Compared to other cloud paradigms, FaaS requires significantly more fine-grained measurement of functions' compute time and memory usage. Since functions are short and stateless, small ephemeral entities (e.g. individuals or underutilized data centers) can become FaaS service providers. However, this exacerbates the already substantial challenges of 1) ensuring integrity of computation, 2) minimizing information revealed to the service provider, and 3) accurately measuring computational resource usage.To address these challenges, we introduce S-FaaS, the first architecture and implementation of FaaS to provide strong security and accountability guarantees using Intel SGX. To match the dynamic event-driven nature of FaaS, we introduce a new key distribution enclave and a novel transitive attestation protocol. A core contribution of S-FaaS is our set of reusable resource measurement mechanisms that securely measure compute time and memory usage inside an enclave. We have integrated S-FaaS into the OpenWhisk FaaS framework and provide this as open source software.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
pages = {185–199},
numpages = {15},
keywords = {function-as-a-service, resource measurement, intel sgx},
location = {London, United Kingdom},
series = {CCSW'19}
}

@inproceedings{10.1145/3209914.3209918,
author = {Zou, Luyao and Rui, Xuhua and Nguyen, Tuan Anh and Min, Dugki and Choi, Eunmi and Thang, Tran Duc and Son, Nguyen Nhu},
title = {A Scalable Network Area Storage with Virtualization: Modelling and Evaluation Using Stochastic Reward Nets},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3209918},
doi = {10.1145/3209914.3209918},
abstract = {Modelling and analysis of storage system in data centers for availability prediction is of paramount importance. Many studies in literature proposed different architectures and techniques to enhance availability of the storage system. In this paper, we proposed to incorporate virtualization techniques on a network area storage. We used stochastic reward nets to model the system's architecture and operational scenarios. Furthermore, we investigated various measures of interests including steady state availability, downtime and downtime cost, and sensitivity of the system availability with respect to impacting parameters. The analysis results show that the proposed storage system with virtualization can obtain an acceptable level of service availability. Furthermore, the sensitivity analysis also points out complicated dependences of service availability upon system parameters. This paper presents a preliminary study to help guide the development of a scalable network area storage with virtualization in practice.},
booktitle = {Proceedings of the 1st International Conference on Information Science and Systems},
pages = {225–233},
numpages = {9},
keywords = {Stochastic Reward Nets, Network Attached Storage, Availability, Reliability},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@inproceedings{10.1145/3276774.3276777,
author = {Coffman, Austin R. and Bu\v{s}i\'{c}, Ana and Barooah, Prabir},
title = {Virtual Energy Storage from TCLs Using QoS Preserving Local Randomized Control},
year = {2018},
isbn = {9781450359511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276774.3276777},
doi = {10.1145/3276774.3276777},
abstract = {We propose a control architecture for distributed coordination of a collection of on/off TCLs (thermostatically controlled loads), such as residential air conditioners, to provide the same service to the power grid as a large battery. This involves a collection of loads to coordinate their on/off decisions so that the aggregate power consumption profile tracks a grid-supplied reference. A key constraint is to maintain each consumer's quality of service (QoS). Recent works have proposed randomization at the loads. Thermostats at the loads are replaced by a randomized controller, and the grid broadcasts a scalar to all loads, which tunes the probability of turning on or off at each load depending on its state. In this paper we propose a modification of a previous design by Meyn and Bu\v{s}i\'{c}. The previous design by Meyn and Bu\v{s}i\'{c} ensures that the indoor temperature remains within a pre-specified bound, but other QoS metrics, especially the frequency of turning on and off was not limited. The controller we propose can be tuned to reduce the cycling rate of a TCL to any desired degree. The proposed design is compared against the design by Meyn and Bu\v{s}i\'{c} and another well cited design in the literature on control of TCL populations, by Mathieu et al. We show through simulations that the proposed controller is able to reduce the cycling of individual ACs compared to the previous designs with little loss in tracking of the grid-supplied reference signal.},
booktitle = {Proceedings of the 5th Conference on Systems for Built Environments},
pages = {93–102},
numpages = {10},
keywords = {virtual energy storage, demand response, distributed control, randomized control},
location = {Shenzen, China},
series = {BuildSys '18}
}

@inproceedings{10.1145/3379310.3379320,
author = {Lumba, Ester and Waworuntu, Alexander},
title = {Application of Lecturer Performance Report in Indonesia with Model View Controller (MVC) Architecture},
year = {2020},
isbn = {9781450376853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379310.3379320},
doi = {10.1145/3379310.3379320},
abstract = {Lecturers in Indonesia have a fundamental obligation to conduct Tri Dharma activities consisting of teaching, research and community service. Most higher education institutions use Tri Dharma as a measure of lecturer's performance. In addition, lecturer activity data related to Tri Dharma is needed by the head of study program and department related to research, publication and community service to be stored which will be used as a source of data during the accreditation process. This paper discusses the application development of lecturer performance reports using the Model View Controller (MVC) architecture with Java programming language. The result is a desktop-based application that will be used by the head of the study program and the lecturers.},
booktitle = {Proceedings of the 2020 2nd Asia Pacific Information Technology Conference},
pages = {23–28},
numpages = {6},
keywords = {Indonesia higher-education, MVC architecture, application development, desktop-based application},
location = {Bali Island, Indonesia},
series = {APIT '20}
}

@inproceedings{10.1109/UCC.2014.167,
author = {Wagle, Shyam S.},
title = {SLA Assured Brokering (SAB) and CSP Certification in Cloud Computing},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.167},
doi = {10.1109/UCC.2014.167},
abstract = {Due to lack of information of the cloud service providers (CSPs), customers can not easily choose services according to their requirement and due to vendor lock-in and lack of interoperability standards among cloud service providers, customers cannot switch the providers once services are subscribed from CSPs. Recently proposed third party architecture which is called cloud broker can access inter-cloud and provides services to the customers according to their requirement but providing SLA based cloud services as per their requirement is still missing in current researches. In our work, we propose the SLA assured brokering framework which matches the requirements of the customer with SLA offered by CSPs using similarity matching algorithm and willingness to pay capacity for the services. It also measures the services offered by CSPs for certifying and ranking the CSPs.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {1016–1017},
numpages = {2},
keywords = {Certifying, Similarity Matching, SLA, Cloud Brokering},
series = {UCC '14}
}

@inproceedings{10.1109/MODELS-C.2019.00032,
author = {Burdusel, Alexandru and Zschaler, Steffen},
title = {Towards Scalable Search-Based Model Engineering with MDEOptimiser Scale},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00032},
doi = {10.1109/MODELS-C.2019.00032},
abstract = {Running scientific experiments using search-based model engineering (SBME) tools is a complex task, that poses a number of challenges, starting from defining an experiment workflow, to parameter tuning, finding optimal computational resources to run on, collecting and interpreting metrics and making the entire process easily reproducible.Despite the proliferation of easily accessible hardware, as a result of the increased availability of infrastructure-as-a-service providers, many SBME tools are rarely using this technology for accelerating experimentation. Running many experiments on a single machine implies much longer waiting times and reduces the ability to increase the speed of iterations when doing SBME research, thus, slowing down the entire process.In this paper, we introduce a domain-specific language (DSL) and a framework that can be used to configure and run experiments at scale, on cloud infrastructure, in a reproducible way. We will describe our DSL and framework architecture along with an example to showcase how a case study can be evaluated using two different model optimisation tools.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {189–195},
numpages = {7},
keywords = {search based model engineering, reproducible research, middleware, model driven engineering, cloud, workflow, evolutionary search},
location = {Munich, Germany},
series = {MODELS '19}
}

@inproceedings{10.1145/2933349.2933359,
author = {Sirin, Utku and Appuswamy, Raja and Ailamaki, Anastasia},
title = {OLTP on a Server-Grade ARM: Power, Throughput and Latency Comparison},
year = {2016},
isbn = {9781450343190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933349.2933359},
doi = {10.1145/2933349.2933359},
abstract = {Although scaling out of low-power cores is an alternative to power-hungry Intel Xeon processors for reducing the power overheads, they have proven inadequate for complex, non-parallelizable workloads. On the other hand, by the introduction of the 64-bit ARMv8 architecture, traditionally low power ARM processors have become powerful enough to run computationally intensive server-class applications.In this study, we compare a high-performance Intel x86 processor with a commercial implementation of the ARM Cortex-A57. We measure the power used, throughput delivered and latency quantified when running OLTP workloads. Our results show that the ARM processor consumes 3 to 15 times less power than the x86, while penalizing OLTP throughput by a much lower factor (1.7 to 3). As a result, the significant power savings deliver up to 9 times higher energy efficiency. The x86's heavily optimized power-hungry micro-architectural structures contribute to throughput only marginally. As a result, the x86 wastes power when utilization is low, while lightweight ARM processor consumes only as much power as it is utilized, achieving energy proportionality. On the other hand, ARM's quantified latency can be up to 11x higher than x86 towards to the tail of latency distribution, making x86 more suitable for certain type of service-level agreements.},
booktitle = {Proceedings of the 12th International Workshop on Data Management on New Hardware},
articleno = {10},
numpages = {7},
location = {San Francisco, California},
series = {DaMoN '16}
}

@inproceedings{10.1145/3286685.3286686,
author = {Saurez, Enrique and Balasubramanian, Bharath and Schlichting, Richard and Tschaen, Brendan and Huang, Zhe and Narayanan, Shankaranarayanan Puzhavakath and Ramachandran, Umakishore},
title = {METRIC: A Middleware for Entry Transactional Database Clustering at the Edge},
year = {2018},
isbn = {9781450361170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286685.3286686},
doi = {10.1145/3286685.3286686},
abstract = {A geo-distributed database for edge architectures spanning thousands of sites needs to assure efficient local updates while replicating sufficient state across sites to enable global management and support mobility, failover etc. To address this requirement, a new paradigm for database clustering that achieves a better balance than existing solutions between performance and strength of semantics called entry transactionality is introduced. Inspired by entry consistency in shared memory systems, entry transactionality guarantees that only a client that owns a range of keys in the database has a sequentially consistent value of the keys and can perform local and, hence, efficient transactions across these keys. Important use cases enabled by entry transactionality such as federated controllers and state management for edge applications are identified. The semantics of entry transactionality incorporating the complex failure modes in geo-distributed services are defined, and the difficult challenges in realizing these semantics are outlined. Then, a novel Middleware for Entry Transactional Clustering (METRIC) that combines existing SQL databases with an underlying geo-distributed entry consistent store to realize entry transactionality is described. This paper provides initial findings from an on-going effort.},
booktitle = {Proceedings of the 3rd Workshop on Middleware for Edge Clouds \&amp; Cloudlets},
pages = {2–7},
numpages = {6},
location = {Rennes, France},
series = {MECC'18}
}

@inproceedings{10.1145/3447545.3451180,
author = {Klaver, Luuk and van der Knaap, Thijs and van der Geest, Johan and Harmsma, Edwin and van der Waaij, Bram and Pileggi, Paolo},
title = {Towards Independent Run-Time Cloud Monitoring},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451180},
doi = {10.1145/3447545.3451180},
abstract = {Cloud computing services are integral to the digital transformation. They deliver greater connectivity, tremendous savings, and lower total cost of ownership. Despite such benefits and benchmarking advances, costs are still quite unpredictable, performance is unclear, security is inconsistent, and there is minimal control over aspects like data and service locality. Estimating performance of cloud environments is very hard for cloud consumers. They would like to make informed decisions about which provider better suits their needs using specialized evaluation mechanisms. Providers have their own tools reporting specific metrics, but they are potentially biased and often incomparable across providers. Current benchmarking tools allow comparison but consumers need more flexibility to evaluate environments under actual operating conditions for specialized applications. Ours is early stage work and a step towards a monitoring solution that enables independent evaluation of clouds for very specific application needs. In this paper, we present our initial architecture of the Cloud Monitor that aims to integrate existing and new benchmarks in a flexible and extensible way. By way of a simplistic demonstrator, we illustrate the concept. We report some preliminary monitoring results after a brief time of monitoring and are able to observe unexpected anomalies. The results suggest an independent monitoring solution is a powerful enabler of next generation cloud computing, not only for the consumer but potentially the whole ecosystem.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {21–26},
numpages = {6},
keywords = {performance evaluation, cloud computing, run-time monitoring, benchmarking},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3358695.3361753,
author = {Mohammadi, Farnaz and Panou, Angeliki and Ntantogian, Christoforos and Karapistoli, Eirini and Panaousis, Emmanouil and Xenakis, Christos},
title = {CUREX: SeCUre and PRivate HEalth Data EXchange},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3361753},
doi = {10.1145/3358695.3361753},
abstract = {The Health sector's increasing dependence on digital information and communication infrastructures renders it vulnerable to privacy and cybersecurity threats, especially as the theft of health data has become lucrative for cyber criminals. CUREX comprehensively addresses the protection of the confidentiality and integrity of health data by producing a novel, flexible and scalable situational awareness-oriented platform. It allows a healthcare provider to assess cybersecurity and privacy risks that are exposed to and suggest optimal strategies for addressing these risks with safeguards tailored to each business case and application. CUREX is fully GDPR compliant by design. At its core, a decentralised architecture enhanced by a private blockchain infrastructure ensures the integrity of the data and –most importantly- the patient safety. Crucially, CUREX expands beyond technical measures and improves cyber hygiene through training and awareness activities for healthcare personnel. Its validation focuses on highly challenging cases of health data exchange, spanning patient cross-border mobility, remote healthcare, and data exchange for research.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {263–268},
numpages = {6},
keywords = {Cybersecurity, Risk assessment, eHealth, Blockchain, Cyber hygiene},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.1145/3204949.3204976,
author = {Mekuria, Rufael and McGrath, Michael J. and Riccobene, Vincenzo and Bayon-Molino, Victor and Tselios, Christos and Thomson, John and Dobrodub, Artem},
title = {Automated Profiling of Virtualized Media Processing Functions Using Telemetry and Machine Learning},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204976},
doi = {10.1145/3204949.3204976},
abstract = {Most media streaming services are composed by different virtualized processing functions such as encoding, packaging, encryption, content stitching etc. Deployment of these functions in the cloud is attractive as it enables flexibility in deployment options and resource allocation for the different functions. Yet, most of the time overprovisioning of cloud resources is necessary in order to meet demand variability. This can be costly, especially for large scale deployments. Prior art proposes resource allocation based on analytical models that minimize the costs of cloud deployments under a quality of service (QoS) constraint. However, these models do not sufficiently capture the underlying complexity of services composed of multiple processing functions. Instead, we introduce a novel methodology based on full-stack telemetry and machine learning to profile virtualized or cloud native media processing functions individually. The basis of the approach consists of investigating 4 categories of performance metrics: throughput, anomaly, latency and entropy (TALE) in offline (stress tests) and online setups using cloud telemetry. Machine learning is then used to profile the media processing function in the targeted cloud/NFV environment and to extract the most relevant cloud level Key Performance Indicators (KPIs) that relate to the final perceived quality and known client side performance indicators. The results enable more efficient monitoring, as only KPI related metrics need to be collected, stored and analyzed, reducing the storage and communication footprints by over 85\%. In addition a detailed overview of the functions behavior was obtained, enabling optimized initial configuration and deployment, and more fine-grained dynamic online resource allocation reducing overprovisioning and avoiding function collapse. We further highlight the next steps towards cloud native carrier grade virtualized processing functions relevant for future network architectures such as in emerging 5G architectures.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {150–161},
numpages = {12},
keywords = {performance, characterization, video streaming, experimentation, telemetry, cloud computing},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

