@inproceedings{10.1145/3439231.3440604,
author = {Mercier, Julien and Whissell-Turner, Kathleen and Paradis, Ariane and Avaca, Ivan},
title = {Good Vibrations: Tuning a Systems Dynamics Model of Affect and Cognition in Learning to the Appropriate Frequency Bands of Fine-Grained Temporal Sequences of Data: Frequency Bands of Affect and Cognition},
year = {2021},
isbn = {9781450389372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439231.3440604},
doi = {10.1145/3439231.3440604},
abstract = {Process-oriented studies of cooperative learning from an educational neuroscience perspective has not been firmly quantified experimentally. Within a modeling approach aimed at the development of a systems dynamics model of affect and cognition, the goal of this exploratory study is to identify typical timescales of variation for continuous metrics of affect (Frontal Alpha Asymmetry (FAA): valence) and cognition (Cognitive Load (CL); Index of Cognitive Engagement (ICE); Frontal Midline Theta (FMT): attention). These metrics were obtained from 72 participants paired in dyads (player and watcher) from whom electroencephalography (EEG) was recorded for 2 hours while one participant was playing a serious game to learn Physics, and the other one was watching passively. The results show rather slow cyclical variation for every metric tested, accompanied in certain cases by short bursts of faster variations. This result converges with [Newell 1990] cognitive architecture assuming that psychophysiological measures capture activity at higher levels such as operation tasks and operations. Theoretical, methodological and applied implications are discussed. Also, the need for further fine-grained analyses of the context and other atypical analyses are expressed.},
booktitle = {Proceedings of the 9th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {194–202},
numpages = {9},
keywords = {online measures of learning, game-based learning, educational neuroscience},
location = {Online, Portugal},
series = {DSAI '20}
}

@inproceedings{10.1145/3344948.3344991,
author = {Santos, Nuno and Salgado, Carlos E. and Morais, Francisco and Melo, M\'{o}nica and Silva, Sara and Martins, Raquel and Pereira, Marco and Rodrigues, Helena and Machado, Ricardo J. and Ferreira, Nuno and Pereira, Manuel},
title = {A Logical Architecture Design Method for Microservices Architectures},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344991},
doi = {10.1145/3344948.3344991},
abstract = {The use of microservices architectures has been widely adopted in software development, especially for cloud-based solutions. Developing such solutions faces several challenges beyond typical architecture and service design concerns, including service exposition (API), inter-service communication, and infrastructure deployment, among others. Although model-driven approaches allow abstracting microservices behavior from the business domain, there is a lack of proper methods for addressing the referred challenges. In this paper, the elicitation of microservices, their identification uses using functionally decomposed UML use cases as input within a logical architecture derivation method, namely an adapted version of the Four Step Rule Set (4SRS), using SoaML diagrams, that responds to microservices specific characteristics. We demonstrate the approach using a scenario within a live industrial project.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {145–151},
numpages = {7},
keywords = {logical architectures, service participants, decomposition, SoaML, UML, microservices},
location = {Paris, France},
series = {ECSA '19}
}

@inproceedings{10.1145/3468264.3473915,
author = {Kalia, Anup K. and Xiao, Jin and Krishna, Rahul and Sinha, Saurabh and Vukovic, Maja and Banerjee, Debasish},
title = {Mono2Micro: A Practical and Effective Tool for Decomposing Monolithic Java Applications to Microservices},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473915},
doi = {10.1145/3468264.3473915},
abstract = {In migrating production workloads to cloud, enterprises often face the daunting task of evolving monolithic applications toward a microservice architecture. At IBM, we developed a tool called Mono2Micro to assist with this challenging task. Mono2Micro performs spatio-temporal decomposition, leveraging well-defined business use cases and runtime call relations to create functionally cohesive partitioning of application classes. Our preliminary evaluation of Mono2Micro showed promising results.  How well does Mono2Micro perform against other decomposition techniques, and how do practitioners perceive the tool? This paper describes the technical foundations of Mono2Micro and presents results to answer these two questions. To answer the first question, we evaluated Mono2Micro against four existing techniques on a set of open-source and proprietary Java applications and using different metrics to assess the quality of decomposition and tool’s efficiency. Our results show that Mono2Micro significantly outperforms state-of-the-art baselines in specific metrics well-defined for the problem domain. To answer the second question, we conducted a survey of twenty-one practitioners in various industry roles who have used Mono2Micro. This study highlights several benefits of the tool, interesting practitioner perceptions, and scope for further improvements. Overall, these results show that Mono2Micro can provide a valuable aid to practitioners in creating functionally cohesive and explainable microservice decompositions.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1214–1224},
numpages = {11},
keywords = {dynamic analysis, microservices, clustering},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3452383.3452385,
author = {Dasgupta, Gargi B.},
title = {AI and Its Applications in the Cloud Strategy},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452385},
doi = {10.1145/3452383.3452385},
abstract = {The fourth industrial revolution identifies cloud computing, data, and artificial intelligence (AI) as opportunity clusters with double digit growth in the next couple of years. As part of the cloud and digital transformation, the role of AI is crucial in enabling that transformation as well as creating the new breed of applications on top. AI mechanisms can help accelerate the modernization of applications, their management, and the testing on cloud architectures. I will focus on two sub-problems: 1) Refactoring of massive monolith applications using AI techniques. This problem statement is particularly relevant in understanding legacy un-optimized code and transforming them to be more cloud-ready. Microservices are indeed becoming the de-facto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently [2]. It also provides natural benefits when deployed on the cloud since resources can be allocated dynamically to necessary components based on demand. We are exploring how AI can help accelerate the transformation of existing applications to microservices. 2) Detecting faults in application behavior at runtime from operational data. This problem statement is particularly relevant in understanding how to manage this new architecture of multiple microservices across the cloud stack [1], [3]. Operational data artifacts span across logs, metrics, tickets, and traces. Looking at signals across the artifacts and across the stack presents a challenging data correlation problem. AI mechanisms can help accelerate problem determination in these complex environments. I will also share my thoughts on how fundamental breakthroughs in AI Research will be needed as we address some of the core problems of cloud computing.},
booktitle = {14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {2},
numpages = {1},
keywords = {modernization, AI Ops, code refactoring, log anomalies, hybrid cloud},
location = {Bhubaneswar, Odisha, India},
series = {ISEC 2021}
}

@inproceedings{10.1145/3415088.3415097,
author = {Mlotshwa, Likhwa Lothar and Makura, Sheunesu M. and Karie, Nickson M. and Kebande, Victor R.},
title = {Opportunistic Security Architecture for Osmotic Computing Paradigm in Dynamic IoT-Edge's Resource Diffusion},
year = {2020},
isbn = {9781450375580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415088.3415097},
doi = {10.1145/3415088.3415097},
abstract = {Increased heterogeneity of physical resources has had positive and negative effects in Internet of Things (IoT) through the existence of edge computing. As a result, there has been a need for effective dynamic management of IoT, cloud and edge resources, in order to address the existence of low-level constraints during resource migration. Nevertheless, the explosion of IoT devices and data has allowed orchestration of microservices to adopt an opportunistic approach to how applications and services are deployed in the edge in IoT platform. A notable approach has been osmotic computing that allows resources from a federated cloud to be able to diffuse from an ecosystem of higher solute (network properties and entities) concentration to solvent (applications, layered interfaces and services). We posit that, while computing resources and applications are able to move from the federated environment, to the cloud deployable models, to the edge, then to IoT ecosystem, there is a higher chance of susceptibility of threats and attacks that may be directed to the emerging edge applications/data due to dynamic emergent configurations. This paper proposes a 5-layer opportunistic architecture that adds security metrics across different levels of osmotic computing paradigm. The proposed 5-layer security architecture addresses the need for autonomously securing resources-edge computation, edge storage and emerging edge configurations as the computing resources move to a higher solute in heterogenous edge and cloud datacenters across IoT devices. This has been achieved by proposing security metrics that address the prevailing challenge with a degree of certainty.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent and Innovative Computing Applications},
articleno = {9},
numpages = {7},
keywords = {opportunistic, osmotic, IoT, edge, security architecture},
location = {Plaine Magnien, Mauritius},
series = {ICONIC '20}
}

@inproceedings{10.5555/3172795.3172823,
author = {Khazaei, Hamzeh and Ravichandiran, Rajsimman and Park, Byungchul and Bannazadeh, Hadi and Tizghadam, Ali and Leon-Garcia, Alberto},
title = {Elascale: Autoscaling and Monitoring as a Service},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Auto-scalability has become an evident feature for cloud software systems including but not limited to big data and IoT applications. Cloud application providers now are in full control over their applications' microservices and macroservices; virtual machines and containers can be provisioned or deprovisioned on demand at run-time. Elascale strives to adjust both micro/macro resources with respect to workload and changes in the internal state of the whole application stack. Elascale leverages Elasticsearch stack for collection, analysis and storage of performance metrics. Elascale then uses its default scaling engine to elastically adapt the managed application. Extendibility is guaranteed through provider, schema, plug-in and policy elements in the Elascale by which flexible scalability algorithms, including both reactive and proactive techniques, can be designed and implemented for various technologies, infrastructures and software stacks. In this paper, we present the architecture and initial implementation of Elascale; an instance will be leveraged to add auto-scalability to a generic IoT application. Due to zero dependency to the target software system, Elascale can be leveraged to provide auto-scalability and monitoring as-a-service for any type of cloud software system.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {234–240},
numpages = {7},
keywords = {microservices, cloud application, scalability as a service, docker, macroservices, monitoring, elasticsearch, containers, auto-scalability},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/2797022.2797039,
author = {Anwar, Ali and Sailer, Anca and Kochut, Andrzej and Butt, Ali R.},
title = {Anatomy of Cloud Monitoring and Metering: A Case Study and Open Problems},
year = {2015},
isbn = {9781450335546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797022.2797039},
doi = {10.1145/2797022.2797039},
abstract = {Microservices based architecture has recently gained traction among the cloud service providers in quest for a more scalable and reliable modular architecture. In parallel with this architectural choice, cloud providers are also facing the market demand for fine grained usage based prices. Both the management of the microservices complex dependencies, as well as the fine grained metering require the providers to track and log detailed monitoring data from their deployed cloud setups. Hence, on one hand, the providers need to record all such performance changes and events, while on the other hand, they are concerned with the additional cost associated with the resources required to store and process this ever increasing amount of collected data.In this paper, we analyze the design of the monitoring subsystem provided by open source cloud solutions, such as OpenStack. Specifically, we analyze how the monitoring data is collected by OpenStack and assess the characteristics of the data it collects, aiming to pinpoint the limitations of the current approach and suggest alternate solutions. Our preliminary evaluation of the proposed solutions reveals that it is possible to reduce the monitored data size by up to 80\% and missed anomaly detection rate from 3\% to as low as 0.05\% to 0.1\%.},
booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
articleno = {6},
numpages = {7},
location = {Tokyo, Japan},
series = {APSys '15}
}

@inproceedings{10.1145/2747470.2747474,
author = {Toffetti, Giovanni and Brunner, Sandro and Bl\"{o}chlinger, Martin and Dudouet, Florian and Edmonds, Andrew},
title = {An Architecture for Self-Managing Microservices},
year = {2015},
isbn = {9781450334761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2747470.2747474},
doi = {10.1145/2747470.2747474},
abstract = {Running applications in the cloud efficiently requires much more than deploying software in virtual machines. Cloud applications have to be continuously managed: 1) to adjust their resources to the incoming load and 2) to face transient failures replicating and restarting components to provide resiliency on unreliable infrastructure. Continuous management monitors application and infrastructural metrics to provide automated and responsive reactions to failures (health management) and changing environmental conditions (auto-scaling) minimizing human intervention.In the current practice, management functionalities are provided as infrastructural or third party services. In both cases they are external to the application deployment. We claim that this approach has intrinsic limits, namely that separating management functionalities from the application prevents them from naturally scaling with the application and requires additional management code and human intervention. Moreover, using infrastructure provider services for management functionalities results in vendor lock-in effectively preventing cloud applications to adapt and run on the most effective cloud for the job.In this position paper we propose a novel architecture that enables scalable and resilient self-management of microservices applications on cloud.},
booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
pages = {19–24},
numpages = {6},
location = {Bordeaux, France},
series = {AIMC '15}
}

@inproceedings{10.1145/3219819.3219834,
author = {Staar, Peter W J and Dolfi, Michele and Auer, Christoph and Bekas, Costas},
title = {Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219834},
doi = {10.1145/3219819.3219834},
abstract = {Over the past few decades, the amount of scientific articles and technical literature has increased exponentially in size. Consequently, there is a great need for systems that can ingest these documents at scale and make the contained knowledge discoverable. Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images) as well as the presentation of the data (e.g. complex tables) make the extraction of qualitative and quantitive data extremely challenging. In this paper, we present a modular, cloud-based platform to ingest documents at scale. This platform, called the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse and annotate documents (i.e. collect ground-truth), train machine-learning classification algorithms and ultimately convert any type of PDF or bitmap-documents to a structured content representation format. We will show that each of the modules is scalable due to an asynchronous microservice architecture and can therefore handle massive amounts of documents. Furthermore, we will show that our capability to gather groundtruth is accelerated by machine-learning algorithms by at least one order of magnitude. This allows us to both gather large amounts of ground-truth in very little time and obtain very good precision/recall metrics in the range of 99\% with regard to content conversion to structured output. The CCS platform is currently deployed on IBM internal infrastructure and serving more than 250 active users for knowledge-engineering project engagements.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {774–782},
numpages = {9},
keywords = {asynchronous architecture, ibm research, artificial intelligence, machine learning, deep learning, cloud computing, ibm, knowledge ingestion, cloud architecture, ai, table processing, pdf, document conversion},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/3293455,
author = {Arcuri, Andrea},
title = {RESTful API Automated Test Case Generation with EvoMaster},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3293455},
doi = {10.1145/3293455},
abstract = {RESTful APIs are widespread in industry, especially in enterprise applications developed with a microservice architecture. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, because often the tested API is a remote service whose code is not available. In this article, we consider testing from the point of view of the developers, who have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding metrics. However, REST is not a protocol but rather a set of guidelines on how to design resources accessed over HTTP endpoints. For example, there are guidelines on how related resources should be structured with hierarchical URIs and how the different HTTP verbs should be used to represent well-defined actions on those resources. Test-case generation for RESTful APIs that only rely on white-box information of the source code might not be able to identify how to create prerequisite resources needed before being able to test some of the REST endpoints. Smart sampling techniques that exploit the knowledge of best practices in RESTful API design are needed to generate tests with predefined structures to speed up the search. We implemented our technique in a tool called EvoMaster, which is open source. Experiments on five open-source, yet non-trivial, RESTful services show that our novel technique automatically found 80 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such an approach are therefore discussed, such as the handling of SQL databases.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
articleno = {3},
numpages = {37},
keywords = {REST, Software engineering, web service, testing}
}

@inproceedings{10.5555/3101282.3101285,
author = {Aderaldo, Carlos M. and Mendon\c{c}a, Nabor C. and Pahl, Claus and Jamshidi, Pooyan},
title = {Benchmark Requirements for Microservices Architecture Research},
year = {2017},
isbn = {9781538604175},
publisher = {IEEE Press},
abstract = {Microservices have recently emerged as a new architectural style in which distributed applications are broken up into small independently deployable services, each running in its own process and communicating via lightweight mechanisms. However, there is still a lack of repeatable empirical research on the design, development and evaluation of microservices applications. As a first step towards filling this gap, this paper proposes, discusses and illustrates the use of an initial set of requirements that may be useful in selecting a community-owned architecture benchmark to support repeatable microservices research.},
booktitle = {Proceedings of the 1st International Workshop on Establishing the Community-Wide Infrastructure for Architecture-Based Software Engineering},
pages = {8–13},
numpages = {6},
keywords = {software architecture, microservices, research benchmark},
location = {Buenos Aires, Argentina},
series = {ECASE '17}
}

@article{10.1145/2829950,
author = {Tomusk, Erik and Dubach, Christophe and O’boyle, Michael},
title = {Four Metrics to Evaluate Heterogeneous Multicores},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2829950},
doi = {10.1145/2829950},
abstract = {Semiconductor device scaling has made single-ISA heterogeneous processors a reality. Heterogeneous processors contain a number of different CPU cores that all implement the same Instruction Set Architecture (ISA). This enables greater flexibility and specialization, as runtime constraints and workload characteristics can influence which core a given workload is run on. A major roadblock to the further development of heterogeneous processors is the lack of appropriate evaluation metrics. Existing metrics can be used to evaluate individual cores, but to evaluate a heterogeneous processor, the cores must be considered as a collective. Without appropriate metrics, it is impossible to establish design goals for processors, and it is difficult to accurately compare two different heterogeneous processors.We present four new metrics to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity, gap overhead, set overhead, and generality. The metrics consider sets rather than individual cores. We use examples to demonstrate each metric, and show that the metrics can be used to quantify intuitions about heterogeneous cores.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {37},
numpages = {25},
keywords = {single-ISA, gap overhead, set overhead, effective speed, generality, Localized nonuniformity}
}

@inproceedings{10.1145/3195546.3195549,
author = {Schmidts, Oliver and Kraft, Bodo and Schreiber, Marc and Z\"{u}ndorf, Albert},
title = {Continuously Evaluated Research Projects in Collaborative Decoupled Environments},
year = {2018},
isbn = {9781450357449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195546.3195549},
doi = {10.1145/3195546.3195549},
abstract = {Often, research results from collaboration projects are not transferred into productive environments even though approaches are proven to work in demonstration prototypes. These demonstration prototypes are usually too fragile and error-prone to be transferred easily into productive environments. A lot of additional work is required.Inspired by the idea of an incremental delivery process, we introduce an architecture pattern, which combines the approach of Metrics Driven Research Collaboration with microservices for the ease of integration. It enables keeping track of project goals over the course of the collaboration while every party may focus on their expert skills: researchers may focus on complex algorithms, practitioners may focus on their business goals.Through the simplified integration (intermediate) research results can be introduced into a productive environment which enables getting an early user feedback and allows for the early evaluation of different approaches. The practitioners' business model benefits throughout the full project duration.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice},
pages = {2–9},
numpages = {8},
keywords = {research collaboration management, metrics, lean software development, software architecture, research best practices},
location = {Gothenburg, Sweden},
series = {SER&amp;IP '18}
}

@inproceedings{10.1145/2973839.2973846,
author = {Aniche, Maur\'{\i}cio and Gerosa, Marco Aur\'{e}lio and Treude, Christoph},
title = {Developers' Perceptions on Object-Oriented Design and Architectural Roles},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973846},
doi = {10.1145/2973839.2973846},
abstract = {Software developers commonly rely on well-known software architecture patterns, such as MVC, to build their applications. In many of these patterns, classes play specific roles in the system, such as Controllers or Entities, which means that each of these classes has specific characteristics in terms of object-oriented class design and implementation. Indeed, as we have shown in a previous study, architectural roles are different from each other in terms of code metrics. In this paper, we present a study in a software development company in which we captured developers' perceptions on object-oriented design aspects of the architectural roles in their system and whether these perceptions match the source code metric analysis. We found that their developers do not have a common perception of how their architectural roles behave in terms of object-oriented design aspects, and that their perceptions also do not match the results of the source code metric analysis. This phenomenon also does not seem to be related to developers' experience. We find these results alarming, and thus, we suggest software development teams to invest in education and knowledge sharing about how their system's architectural roles behave.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {software architecture, object-oriented design, code metrics},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1145/3011077.3011078,
author = {Mellouk, Abdelhamid},
title = {New Provider Services for Convergence Technologies Based on Quality of Experience and Quality of Service Paradigms},
year = {2016},
isbn = {9781450348157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011077.3011078},
doi = {10.1145/3011077.3011078},
abstract = {Based on a convergence of network technologies, the Next Generation Network (NGN) is being deployed to carry high quality video and voice data. In fact, the convergence of network technologies has been driven by the converging needs of end-users. The perceived end-to-end quality is becoming one of the main goals required by users that must be guaranteed by the network operators and the Internet Service Providers, through manufacturer equipment. This is referred to as the notion of Quality of Experience (QoE) and is becoming commonly used to represent user perception. The QoE is not a technical metric, but rather a concept consisting of all elements of a user's perception of the network services. In this talk, we focus on the idea of how to integrate the QoE into a control- command chain in order to construct an adaptive network system. More precisely, in the context of Content-Oriented Networks that is used to redesign the current Internet architecture to accommodate content-oriented applications and services, the talk aim to describe an end-to-end QoE model applied to a Content Distribution Network architecture and see relationships between Quality of service and Quality of Experience.},
booktitle = {Proceedings of the 7th Symposium on Information and Communication Technology},
pages = {1},
numpages = {1},
location = {Ho Chi Minh City, Vietnam},
series = {SoICT '16}
}

@inproceedings{10.1145/3411029.3411032,
author = {Kogias, Marios and Bugnion, Edouard},
title = {Tail-Tolerance as a Systems Principle Not a Metric},
year = {2020},
isbn = {9781450388764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411029.3411032},
doi = {10.1145/3411029.3411032},
abstract = {Tail-latency tolerance (or just simply tail-tolerance) is the ability for a system to deliver a response with low-latency nearly all the time. It it typically expressed as a system metric (e.g., the 99th or 99.99th percentile latency) or as a service-level objective (e.g., the maximum throughput so that the tail latency is below a desired threshold). We advocate instead that modern datacenter systems should incorporate tail-tolerance as a core systems design principle and not a metric to be observed, and that tail-tolerant systems can be built out of large and complex applications whose individual components may suffer from latency deviations. This is analogous to fault-tolerance, where a fault-tolerant system can be built out of unreliable components. The general solution is for the system to control the applied load and keep it under the threshold that violates the latency SLO. We propose to augment RPC semantics with an architectural layer that measures the observed tail latency and probabilistically rejects RPC requests maintaining throughput under the threshold that violates the SLO. Our design is application-independent, and does not make any assumptions about the request service time distribution. We implemented a proof of concept for such a tail-tolerant layer using programmable switches, called SVEN. We demonstrate that the approach is suitable even for microsecond-scale RPCs with variable service times. Moreover, our approach does not induce measurable overheads, and can maintain the maximum achieved throughput very close to the load level that would violate the SLO without SVEN.},
booktitle = {Proceedings of the 4th Asia-Pacific Workshop on Networking},
pages = {16–22},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {APNet '20}
}

@inproceedings{10.5555/2821327.2821338,
author = {L\"{u}bke, Daniel},
title = {Using Metric Time-Lines for Identifying Architecture Shortcomings in Process Execution Architectures},
year = {2015},
publisher = {IEEE Press},
abstract = {Process Execution with Service Orchestrations is an emerging architectural style for developing business software systems. However, few special metrics for guiding software architecture decisions have been proposed and no existing business process metrics have been evaluated for their suitability. By following static code metrics over time, architects can gain a better understanding, how processes and the whole system evolve and whether the metrics evolve as expected. This allows architects to recogniize when to intervene in the development and make architecture adjustments or refactorings. This paper presents an explatory study that uses time-lines of static process size metrics for constant feedback to software architects that deal with process-oriented architectures.},
booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
pages = {55–58},
numpages = {4},
location = {Florence, Italy},
series = {SAM '15}
}

@inproceedings{10.1145/3401025.3401740,
author = {Scrocca, Mario and Tommasini, Riccardo and Margara, Alessandro and Valle, Emanuele Della and Sakr, Sherif},
title = {The Kaiju Project: Enabling Event-Driven Observability},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3401740},
doi = {10.1145/3401025.3401740},
abstract = {Microservices architectures are getting momentum. Even small and medium-size companies are migrating towards cloud-based distributed solutions supported by lightweight virtualization techniques, containers, and orchestration systems. In this context, understanding the system behavior at runtime is critical to promptly react to errors. Unfortunately, traditional monitoring techniques are not adequate for such complex and dynamic environments. Therefore, a new challenge, namely observability, emerged from precise industrial needs: expose and make sense of the system behavior at runtime. In this paper, we investigate observability as a research problem. We discuss the benefits of events as a unified abstraction for metrics, logs, and trace data, and the advantages of employing event stream processing techniques and tools in this context. We show that an event-based approach enables understanding the system behavior in near real-time more effectively than state-of-the-art solutions in the field. We implement our model in the Kaiju system and we validate it against a realistic deployment supported by a software company.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {85–96},
numpages = {12},
keywords = {orchestration systems, observability, event-based systems, event stream processing},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3164541.3164576,
author = {Kim, Heejin and Jeon, Seil and Raza, Syed M. and Lee, Joohyun and Choo, Hyunseung},
title = {Service-Aware Split Point Selection for User-Centric Mobility Enhancement in SDN},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164576},
doi = {10.1145/3164541.3164576},
abstract = {IP mobility anchor works as the redirection/split point of the packet destined to the mobile terminal (MT), as well as IP address/prefix assignment and mobility binding management in the legacy mobility management protocols. In software-defined networking (SDN), the split point can be managed by the SDN controller or controller application, as the control of the network is separated from the forwarding entities. The demand of user QoE is ever increasing and they always want to get the best service continuity served in mobility management. Differentiated split point selection per service type could be one of the effective measures to enhance user QoE in a mobility management environment. In this paper, we propose a service-aware split point selection mechanism for user-centric mobility management enhancement in SDN. Specifically, we propose the mobility control architecture, which can classify service flow type and determine advantageous split point depending on a service flow type. We analyze the performance of the proposed split point selection mechanism compared to target mechanisms. We also measure the performance metrics on an ONOS-based SDN testbed to identify the superiority of the proposed mechanism.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {95},
numpages = {8},
keywords = {split point selection, mobility management, software-defined networking},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@inproceedings{10.1145/3098954.3098977,
author = {Boukoros, Spyros and Katzenbeisser, Stefan},
title = {Measuring Privacy in High Dimensional Microdata Collections},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3098977},
doi = {10.1145/3098954.3098977},
abstract = {Microdata is collected by companies in order to enhance their quality of service as well as the accuracy of their recommendation systems. These data often become publicly available after they have been sanitized. Recent reidentification attacks on publicly available, sanitized datasets illustrate the privacy risks involved in microdata collections. Currently, users have to trust the provider that their data will be safe in case data is published or if a privacy breach occurs. In this work, we empower users by developing a novel, user-centric tool for privacy measurement and a new lightweight privacy metric. The goal of our tool is to estimate users' privacy level prior to sharing their data with a provider. Hence, users can consciously decide whether to contribute their data. Our tool estimates an individuals' privacy level based on published popularity statistics regarding the items in the provider's database, and the users' microdata. In this work, we describe the architecture of our tool as well as a novel privacy metric, which is necessary for our setting where we do not have access to the provider's database. Our tool is user friendly, relying on smart visual results that raise privacy awareness. We evaluate our tool using three real world datasets, collected from major providers. We demonstrate strong correlations between the average anonymity set per user and the privacy score obtained by our metric. Our results illustrate that our tool which uses minimal information from the provider, estimates users' privacy levels comparably well, as if it had access to the actual database.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {15},
numpages = {8},
keywords = {user empowerment, privacy metrics, privacy, microdata},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/3258045,
author = {Savola, Reijo and Abie, Habtamu and Kanstr\'{e}n, Teemu},
title = {Session Details: Fourth International Workshop on Measurability of Security in Software Architectures (MeSSa 2017)},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3258045},
doi = {10.1145/3258045},
abstract = {Cybersecurity incidents are increasing, and at the same time, our society depends more and more on cyber-physical systems. Systematic approaches to measure cybersecurity are needed in order to support efficient construction and maintenance of secure software systems. Security measurement of software architectures is needed to produce sufficient evidence of security level as early as in the design phase. Design-time security measuring should support "security by design" approach. Moreover, software architectures have to support runtime security measurement to obtain up-to-date security information from an online software system, service or product. Security metrics and measurements are exploited in situational awareness monitoring and self-adaptive security solutions. The area of security metrics and security assurance metrics research is evolving, but still lacks widely accepted metrics definitions and applicable measuring techniques. Strong collaboration between security experts, software architects and system developers is needed to address this. MeSSa2017 workshop addresses these and other related topics to increase the importance of the overall picture, requiring sets of design patterns, measurements, metrics, best practices, and means to integrate this cost-effectively in the overall design and operational profiles.The outcome of the workshop will be an increased shared understanding of challenges and opportunities in systematic approaches to measure cybersecurity, which are needed in order to support efficient construction and maintenance of secure software systems.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/3368691.3368704,
author = {Albataineh, Abdallah and Al-Qassas, Raad S. and Qasaimeh, Malik},
title = {A New Architecture for Voice Interconnection Using Packet Switched Network},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368704},
doi = {10.1145/3368691.3368704},
abstract = {Interconnecting voice service providers require a mutual trust between communicating entities, which are built either using bilateral agreements or intermediary service provider. To achieve such relationship between Anonymous Service Providers we should have an automated mechanism. In this paper, we propose a conceptual architecture that can build such relationship between communicating Anonymous Service Providers. By applying this architecture, we argue that we can increase efficiency, security, and performance of service provider's networks. The impact of internet speed on the interconnection network is measured using key metrics including ACD, ASR, PDD, NER, and MOS.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {13},
numpages = {7},
keywords = {PDD, ACD, SIP, ASR, SS7, voice network architecture},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3359789.3359843,
author = {Nagendra, Vasudevan and Yegneswaran, Vinod and Porras, Phillip and Das, Samir R},
title = {Coordinated Dataflow Protection for Ultra-High Bandwidth Science Networks},
year = {2019},
isbn = {9781450376280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359789.3359843},
doi = {10.1145/3359789.3359843},
abstract = {The Science DMZ (SDMZ) is a special purpose network architecture proposed by ESnet (Energy Sciences Network) to facilitate distributed science experimentation on terabyte- (or petabyte-) scale data, exchanged over ultra-high bandwidth WAN links. Critical security challenges faced by these networks include: (i) network monitoring at high bandwidths, (ii) reconciling site-specific policies with project-level policies for conflict-free policy enforcement, (iii) dealing with geographically-distributed datasets with varying levels of sensitivity, and (iv) dynamically enforcing appropriate security rules. To address these challenges, we develop a fine-grained dataflow-based security enforcement system, called CoordiNetZ (CNZ), that provides coordinated situational awareness, i.e., the use of context-aware tagging for policy enforcement using the dynamic contextual information derived from hosts and network elements. We also developed tag and IP-based security microservices that incur minimal overheads in enforcing security to data flows exchanged across geographically-distributed SDMZ sites. We evaluate our prototype implementation across two geographically distributed SDMZ sites with SDN-based case studies, and present performance measurements that respectively highlight the utility of our framework and demonstrate efficient implementation of security policies across distributed SDMZ networks.},
booktitle = {Proceedings of the 35th Annual Computer Security Applications Conference},
pages = {568–583},
numpages = {16},
keywords = {usability and human-centric aspects of security, SDN, distributed systems security, software-defined programmable security, big data security, network security, NFV},
location = {San Juan, Puerto Rico, USA},
series = {ACSAC '19}
}

@inproceedings{10.1145/3338466.3358917,
author = {Heinl, Michael P. and Giehl, Alexander and Wiedermann, Norbert and Plaga, Sven and Kargl, Frank},
title = {MERCAT: A Metric for the Evaluation and Reconsideration of Certificate Authority Trustworthiness},
year = {2019},
isbn = {9781450368261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338466.3358917},
doi = {10.1145/3338466.3358917},
abstract = {Public key infrastructures (PKIs) build the foundation for secure communication of a vast majority of cloud services. In the recent past, there has been a series of security incidents leading to increasing concern regarding the trust model currently employed by PKIs. One of the key criticisms is the architecture's implicit assumption that certificate authorities (CAs) are trustworthy a priori.This work proposes a holistic metric to compensate this assumption by a differentiating assessment of a CA's individual trustworthiness based on objective criteria. The metric utilizes a wide range of technical and non-technical factors derived from existing policies, technical guidelines, and research. It consists of self-contained submetrics allowing the simple extension of the existing set of criteria. The focus is thereby on aspects which can be assessed by employing practically applicable methods of independent data collection.The metric is meant to help organizations, individuals, and service providers deciding which CAs to trust or distrust. For this, the modularized submetrics are clustered into coherent submetric groups covering a CA's different properties and responsibilities. By applying individually chosen weightings to these submetric groups, the metric's outcomes can be adapted to tailored protection requirements according to an exemplifying attacker model.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
pages = {1–15},
numpages = {15},
keywords = {cloud security, ca, metric, digital certificate, x.509, trustworthiness assessment, pki},
location = {London, United Kingdom},
series = {CCSW'19}
}

@inproceedings{10.5555/3191835.3191902,
author = {Lin, Chih-Lu and Chen, Ying-Liang and Kao, Hung-Yu},
title = {Question Difficulty Evaluation by Knowledge Gap Analysis in Question Answer Communities},
year = {2014},
isbn = {9781479958764},
publisher = {IEEE Press},
abstract = {The Community Question Answer (CQA) service is a typical forum of Web 2.0 that shares knowledge among people. There are thousands of questions that are posted and solved every day. Because of the various users of the CQA service, question search and ranking are the most important topics of research in the CQA portal. In this study, we addressed the problem of identifying questions as being hard or easy by means of a probability model. In addition, we observed the phenomenon called knowledge gap that is related to the habit of users and used a knowledge gap diagram to illustrate how much of a knowledge gap exists in different categories. To this end, we proposed an approach called the knowledge-gap-based difficulty rank (KG-DRank) algorithm, which combines the user-user network and the architecture of the CQA service to find hard questions. We used f-measure, AUC, MAP, NDCG, precision@Top5 and concordance analysis to evaluate the experimental results. Our results show that our approach leads to better performance than other baseline approaches across all evaluation metrics.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {336–339},
numpages = {4},
keywords = {social network, link analysis, knowledge gap, expert finding, difficulty, CQA portal},
location = {Beijing, China},
series = {ASONAM '14}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Software Development and Research with the HADAS Toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {software product line, CVL, energy efficiency, optimisation, clafer, metrics, repository, variability},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@article{10.1145/3351278,
author = {Yu, Tuo and Nahrstedt, Klara},
title = {ShoesHacker: Indoor Corridor Map and User Location Leakage through Force Sensors in Smart Shoes},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351278},
doi = {10.1145/3351278},
abstract = {The past few years have witnessed the rise of smart shoes, the wearable devices that measure foot force or track foot motion. However, people are not aware of the possible privacy leakage from in-shoe force sensors. In this paper, we explore the possibility of locating an indoor victim based on the force signals leaked from smart shoes. We present ShoesHacker, an attack scheme that reconstructs the corridor map of the building that the victim walks in based on force data only. The corridor map enables the attacker to recognize the building, and thus locate the victim on a global map. To handle the lack of training data, we design the stair landing detection algorithm, based on which we extract training data when victims are walking in stairwells. We estimate the trajectory of each walk, and propose the path merging algorithm to merge the trajectories. Moreover, we design a metric to quantify the similarity between corridor maps, which makes building recognition possible. Our experimental results show that, the building recognition accuracy reaches 77.5\% in a 40-building dataset, and the victim can be located with an average error lower than 6 m, which reveals the danger of privacy leakage through smart shoes. CCS Concepts: • Information systems~Mobile information processing systems; Location based services; • Human-centered computing~Mobile devices; Ubiquitous and mobile computing systems and tools; • Security and privacy~Domain-specific security and privacy architectures.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {120},
numpages = {29},
keywords = {Corridor map reconstruction, force sensors, smart shoes}
}

@inproceedings{10.1145/2745802.2745822,
author = {Stevanetic, Srdjan and Zdun, Uwe},
title = {Software Metrics for Measuring the Understandability of Architectural Structures: A Systematic Mapping Study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745822},
doi = {10.1145/2745802.2745822},
abstract = {The main idea of software architecture is to concentrate on the "big picture" of a software system. In the context of object-oriented software systems higher-level architectural structures or views above the level of classes are frequently used to capture the "big picture" of the system. One of the critical aspects of these higher-level views is understandability, as one of their main purposes is to enable designers to abstract away fine-grained details. In this article we present a systematic mapping study on software metrics related to the understandability concepts of such higher-level software structures with regard to their relations to the system implementation. In our systematic mapping study, we started from 3951 studies obtained using an electronic search in the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our inclusion/exclusion criteria as well as the snowballing technique we selected 268 studies for in-depth study. From those, we selected 25 studies that contain relevant metrics. We classify the identified studies and metrics with regard to the measured artefacts, attributes, quality characteristics, and representation model used for the metrics definitions. Additionally, we present the assessment of the maturity level of the identified studies. Overall, there is a lack of maturity in the studies. We discuss possible techniques how to mitigate the identified problems. From the academic point of view we believe that our study is a good starting point for future studies aiming at improving the existing works. From a practitioner's point of view, the results of our study can be used as a catalogue and an indication of the maturity of the existing research results.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {21},
numpages = {14},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1145/2601248.2601264,
author = {Stevanetic, Srdjan and Zdun, Uwe},
title = {Exploring the Relationships between the Understandability of Components in Architectural Component Models and Component Level Metrics},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601264},
doi = {10.1145/2601248.2601264},
abstract = {Architectural component models represent high level designs and are frequently used as a central view of architectural descriptions of software systems. The components in those models represent important high level organization units that group other components and classes in object-oriented design views. Hence, understandability of components and their interactions plays a key role in supporting the architectural understanding of a software system. In this paper we present a study we carried out to examine the relationships between the effort required to understand a component, measured through the time that participants spent on studying a component, and component level metrics that describe component's size, complexity and coupling in terms of the number of classes in a component and the classes' relationships. The participants were 49 master students, and they had to fully understand the components' functionalities in order to answer 4 true/false questions for each of the 7 components in the architecture of the Soomla Android store system. Correlation, collinearity and multivariate regression analysis were performed. The results of the analysis show a statistically significant correlation between three of the metrics, number of classes, number of incoming dependencies, and number of internal dependencies, on one side, and the effort required to understand a component, on the other side. In a multivariate regression analysis we obtained 3 reasonably well-fitting models that can be used to estimate the effort required to understand a component. In our future work we plan to study more components and investigate more metrics and their relationships to the understandability of components and architectural component models.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {32},
numpages = {10},
keywords = {software metrics, empirical evaluation, architectural component models, understandability},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1145/3267955.3267965,
author = {Khan, Junaid Ahmed and Westphal, Cedric and Garcia-Luna-Aceves, J. J. and Ghamri-Doudane, Yacine},
title = {NICE: Network-Oriented Information-Centric Centrality for Efficiency in Cache Management},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3267965},
doi = {10.1145/3267955.3267965},
abstract = {All Information-Centric Networking (ICN) architectures proposed to date aim at connecting users to content directly, rather than connecting clients to servers. Surprisingly, however, although content caching is an integral of any information-Centric Network, limited work has been reported on information-centric management of caches in the context of an ICN. Indeed, approaches to cache management in networks of caches have focused on network connectivity rather than proximity to content.We introduce the Network-oriented Information-centric Centrality for Efficiency (NICE) as a new metric for cache management in information-centric networks. We propose a method to compute information-centric centrality that scales with the number of caches in a network rather than the number of content objects, which is many orders of magnitude larger. Furthermore, it can be pre-processed offline and ahead of time. We apply the NICE metric to a content replacement policy in caches, and show that a content replacement based on NICE exhibits better performances than LRU and other policies based on topology-oriented definitions of centrality.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {31–42},
numpages = {12},
keywords = {content offloading, graph centrality, ICN, cache management},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@inproceedings{10.1145/3375555.3384936,
author = {Avritzer, Alberto},
title = {Automated Scalability Assessment in DevOps Environments},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3384936},
doi = {10.1145/3375555.3384936},
abstract = {In this extended abstract, we provide an outline of the presentation planned for WOSP-C 2020. The goal of the presentation is to provide an overview of the challenges and approaches for automated scalability assessment in the context of DevOps and microservices. The focus of this presentation is on approaches that employ automated identification of performance problems because these approaches can leverage performance anti-pattern[5] detection technology. In addition, we envision extending the approach to recommend component refactoring. In our previous work[1,2] we have designed a methodology and associated tool support for the automated scalability assessment of micro-service architectures, which included the automation of all the steps required for scalability assessment. The presentation starts with an introduction to dependability, operational Profile Data, and DevOps. Specifically, we provide an overview of the state of the art in continuous performance monitoring technologies[4] that are used for obtaining operational profile data using APM tools. We then present an overview of selected approaches for production and performance testing based on the application monitoring tool (PPTAM) as introduced in [1,2]. The presentation concludes by outlining a vision for automated performance anti-pattern[5] detection. Specifically, we present the approach introduced for automated anti-pattern detection based on load testing results and profiling introduced in[6] and provide recommendations for future research.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {10},
numpages = {1},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3204949.3204956,
author = {Mangla, Tarun and Zegura, Ellen and Ammar, Mostafa and Halepovic, Emir and Hwang, Kyung-Wook and Jana, Rittwik and Platania, Marco},
title = {VideoNOC: Assessing Video QoE for Network Operators Using Passive Measurements},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204956},
doi = {10.1145/3204949.3204956},
abstract = {Video streaming traffic is rapidly growing in mobile networks. Mobile Network Operators (MNOs) are expected to keep up with this growing demand, while maintaining a high video Quality of Experience (QoE). This makes it critical for MNOs to have a solid understanding of users' video QoE with a goal to help with network planning, provisioning and traffic management. However, designing a system to measure video QoE has several challenges: i) large scale of video traffic data and diversity of video streaming services, ii) cross-layer constraints due to complex cellular network architecture, and iii) extracting QoE metrics from network traffic. In this paper, we present VideoNOC, a prototype of a flexible and scalable platform to infer objective video QoE metrics (e.g., bitrate, rebuffering) for MNOs. We describe the design and architecture of VideoNOC, and outline the methodology to generate a novel data source for fine-grained video QoE monitoring. We then demonstrate some of the use cases of such a monitoring system. VideoNOC reveals video demand across the entire network, provides valuable insights on a number of design choices by content providers (e.g., OS-dependent performance, video player parameters like buffer size, range of encoding bitrates, etc.) and helps analyze the impact of network conditions on video QoE (e.g., mobility and high demand).},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {101–112},
numpages = {12},
keywords = {passive measurement, cellular network, video streaming, QoE},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/3234944.3234952,
author = {Kim, Yubin and Callan, Jamie},
title = {Measuring the Effectiveness of Selective Search Index Partitions without Supervision},
year = {2018},
isbn = {9781450356565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234944.3234952},
doi = {10.1145/3234944.3234952},
abstract = {Selective search architectures partition a document collection into topic-oriented index shards, usually using algorithms that have random components. Different mappings of documents into index shards (shard maps) produce different search accuracy and consistency, however identifying which shard maps will deliver the highest average effectiveness is an open problem. This paper presents a new metric, Area Under Recall Curve (AUReC), to evaluate and compare shard maps. AUReC is the first such metric that is independent of resource selection and shard cut-off estimation. It does not require an end-to-end evaluation or manual gold-standard judgements. Experiments show that its predictions are highly-correlated with evaluating end-to-end systems of various configurations, while being easier to implement and computationally inexpensive.},
booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {91–98},
numpages = {8},
keywords = {distributed search, selective search, clustering, evaluation, cluster-based retrieval},
location = {Tianjin, China},
series = {ICTIR '18}
}

@inproceedings{10.1109/CCGrid.2015.152,
author = {Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
title = {Modeling Cross-Architecture Co-Tenancy Performance Interference},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.152},
doi = {10.1109/CCGrid.2015.152},
abstract = {Cloud computing has become a dominant computing paradigm to provide elastic, affordable computing resources to end users. Due to the increased computing power of modern machines powered by multi/many-core computing, data centers often co-locate multiple virtual machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing and competition. Applications or VMs co-locating in one physical machine can interfere with each other despite of the promise of performance isolation through virtualization. Modeling and predicting co-run interference therefore becomes critical for data center job scheduling and QoS (Quality of Service) assurance. Co-run interference can be categorized into two metrics, sensitivity and pressure, where the former denotes how an application's performance is affected by its co-run applications, and the latter measures how it impacts the performance of its co-run applications. This paper shows that sensitivity and pressure are both application- and architecture-dependent. Further, we propose a regression model that predicts an application's sensitivity and pressure across architectures with high accuracy. This regression model enables a data center scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate with another VMs/applications.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {231–240},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3357384.3357956,
author = {Boiarov, Andrei and Tyantov, Eduard},
title = {Large Scale Landmark Recognition via Deep Metric Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357956},
doi = {10.1145/3357384.3357956},
abstract = {This paper presents a novel approach for landmark recognition in images that we've successfully deployed at Mail.ru. This method enables us to recognize famous places, buildings, monuments, and other landmarks in user photos. The main challenge lies in the fact that it's very complicated to give a precise definition of what is and what is not a landmark. Some buildings, statues and natural objects are landmarks; others are not. There's also no database with a fairly large number of landmarks to train a recognition model. A key feature of using landmark recognition in a production environment is that the number of photos containing landmarks is extremely small. This is why the model should have a very low false positive rate as well as high recognition accuracy. We propose a metric learning-based approach that successfully deals with existing challenges and efficiently handles a large number of landmarks. Our method uses a deep neural network and requires a single pass inference that makes it fast to use in production. We also describe an algorithm for cleaning landmarks database which is essential for training a metric learning model. We provide an in-depth description of basic components of our method like neural network architecture, the learning strategy, and the features of our metric learning approach. We show the results of proposed solutions in tests that emulate the distribution of photos with and without landmarks from a user collection. We compare our method with others during these tests. The described system has been deployed as a part of a photo recognition solution at Cloud Mail.ru, which is the photo sharing and storage service at Mail.ru Group.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {169–178},
numpages = {10},
keywords = {metric learning, landmark recognition, deep learning},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3368235.3370269,
author = {Spillner, Josef},
title = {Serverless Computing and Cloud Function-Based Applications},
year = {2019},
isbn = {9781450370448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368235.3370269},
doi = {10.1145/3368235.3370269},
abstract = {Serverless computing is a growing industry trend with corresponding rise in interest by scholars and tinkerers. Increasingly, open source and academic system prototypes are being proposed especially in relation with cloud, edge and fog computing among other distributed computing specialisations. Due to the strict separation between elastically scalable stateless microservices bound to stateful backend services prevalent in this computing paradigm, the resulting applications are inherently distributed with favourable characteristics such as elastic scalability and disposability. Still, software application developers are confronted with a multitude of different methods and tools to build, test and deploy their function-based applications in today's serverless ecosystems. The logical next step is therefore a methodical development approach with key enablers based on a classification of languages, tools, systems, system behaviours, patterns, pitfalls, application architectures, compositions and cloud services around the serverless application development process.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
pages = {177–178},
numpages = {2},
keywords = {tutorial, artefact quality, serverless computing, cloud functions},
location = {Auckland, New Zealand},
series = {UCC '19 Companion}
}

@inproceedings{10.1145/3412841.3441899,
author = {Torquato, Matheus and Maciel, Paulo and Vieira, Marco},
title = {Analysis of VM Migration Scheduling as Moving Target Defense against Insider Attacks},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441899},
doi = {10.1145/3412841.3441899},
abstract = {As cybersecurity threats evolve, cloud computing defenses must adapt to face new challenges. Unfortunately, due to resource sharing, cloud computing platforms open the door for insider attacks, which consist of malicious actions from cloud authorized users (e.g., clients of an Infrastructure-as-a-Service (IaaS) cloud) targeting the co-hosted users or the underlying provider environment. Virtual machine (VM) migration is a Moving Target Defense (MTD) technique to mitigate insider attacks effects, as it provides VMs positioning manageability. However, there is a clear demand for studies quantifying the security benefits of VM migration-based MTD considering different system architecture configurations. This paper tries to fill such a gap by presenting a Stochastic Reward Net model for the security evaluation of a VM migration-based MTD. The security metric of interest is the probability of attack success. We consider multiple architectures, ranging from one physical machine pool (without MTD) up to four physical machine pools. The evaluation also considers the unavailability due to VM migration. The key contributions are i) a set of results highlighting the probability of insider attacks success over time in different architectures and VM migration schedules, and ii) suggestions for selecting VMs as candidates for MTD deployment based on the tolerance levels of the attack success probability. The results are validated against simulation results to confirm the accuracy of the model.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {194–202},
numpages = {9},
keywords = {VM migration, stochastic petri nets, migration-based dynamic platform, moving target defense, availability},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/3021955.3022022,
author = {L., Marcelo Dornbusch and Rauta, Leonardo R.P. and Silva, Paulo H. and Silva, Rodrigo C. and Irigoite, Adriano M. and Wangham, Michelle S.},
title = {Remote and Continuous Monitoring of Electrical Quantities Using Web of Things and Cloud Computing},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {The remote monitoring and control of machines are essential in industrial environments. Emerging communication technologies such as Web of Things and Machine to Machine Communication can meet this demand for automation. This paper aims to introduce a solution, called Smart Meter, for continuous and remote monitoring of electrical quantities, in smart industrial environments with three-phase systems. The proposed solution uses a resource-oriented architecture and makes use of a Smart Gateway for communication, RESTful web services and cloud computing. The solution was integrated with a real case study and evaluated by software testing. The results obtained demonstrate the feasibility of the solution, and the correctness of measurements persisted in the cloud.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {393–400},
numpages = {8},
keywords = {Cloud Computing, Web of Things, M2M, Remote Monitoring of Electrical Quantities},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI '16}
}

@inproceedings{10.1145/3131151.3131153,
author = {Durelli, Rafael S. and Viana, Matheus C. and de S. Landi, Andr\'{e} and Durelli, Vinicius H. S. and Delamaro, Marcio E. and de Camargo, Valter V.},
title = {Improving the Structure of KDM Instances via Refactorings: An Experimental Study Using KDM-RE},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131153},
doi = {10.1145/3131151.3131153},
abstract = {Architecture-Driven Modernization (ADM) is an initiative of the Object Management Group (OMG) whose main purpose is to provide standard metamodels for software modernization activities. The most important metamodel is the Knowledge Discovery Metamodel (KDM), which represents software artifacts in a language-agnostic fashion. A fundamental step in software modernization is refactoring. However, there is a lack of tools that address how refactoring can be applied in conjunction with ADM. We developed a tool, called KDM-RE, that supports refactorings in KDM instances through: (i) a set of wizards that aid the software modernization engineer during refactoring activities; (ii) a change propagation module that keeps the internal metamodels synchronized; and (iii) the selection and application of refactorings available in its repository. This paper evaluates the application of refactorings to KDM instances in an experiment involving seven systems implemented in Java. We compared the pre-refactoring versions of these systems with the refactored ones using the Quality Model for Object-Oriented Design (QMOOD) metric set. The results from this evaluation suggest that KDM-RE provides advantages to software modernization engineers refactoring systems represented as KDMs.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {174–183},
numpages = {10},
keywords = {Model-Driven Development, Knowledge-Discovery Metamodel, Architecture-Driven Modernization, Refactoring},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@inproceedings{10.1145/3209978.3210005,
author = {Mohammad, Hafeezul Rahman and Xu, Keyang and Callan, Jamie and Culpepper, J. Shane},
title = {Dynamic Shard Cutoff Prediction for Selective Search},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210005},
doi = {10.1145/3209978.3210005},
abstract = {Selective search architectures use resource selection algorithms such as Rank-S or Taily to rank index shards and determine how many to search for a given query. Most prior research evaluated solutions by their ability to improve efficiency without significantly reducing early-precision metrics such as P@5 and NDCG@10. This paper recasts selective search as an early stage of a multi-stage retrieval architecture, which makes recall-oriented metrics more appropriate. A new algorithm is presented that predicts the number of shards that must be searched for a given query in order to meet recall-oriented goals. Decoupling shard ranking from deciding how many shards to search clarifies efficiency vs. effectiveness trade-offs, and enables them to be optimized independently. Experiments on two corpora demonstrate the value of this approach.},
booktitle = {The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
pages = {85–94},
numpages = {10},
keywords = {distributed search, resource selection, selective search},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3178461.3178462,
author = {Meryem, Amar and Samira, Douzi and Bouabid, El Ouahidi},
title = {Enhancing Cloud Security Using Advanced MapReduce K-Means on Log Files},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178462},
doi = {10.1145/3178461.3178462},
abstract = {Many customers ranked cloud security as a major challenge that threaten their work and reduces their trust on cloud service's provider. Hence, a significant improvement is required to establish better adaptations of security measures that suit recent technologies and especially distributed architectures.Considering the meaningful recorded data in cloud generated log files, making analysis on them, mines insightful value about hacker's activities. It identifies malicious user behaviors and predicts new suspected events. Not only that, but centralizing log files, prevents insiders from causing damage to system. In this paper, we proposed to take away sensitive log files into a single server provider and combining both MapReduce programming and k-means on the same algorithm to cluster observed events into classes having similar features. To label unknown user behaviors and predict new suspected activities this approach considers cosine distances and deviation metrics.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {63–67},
numpages = {5},
keywords = {MapReduce, Cloud Security, log files, K-means, Deviation metric},
location = {Casablanca, Morocco},
series = {ICSIM '18}
}

@article{10.1145/2659118.2659135,
author = {Tiwari, Umesh and Kumar, Santosh},
title = {In-out Interaction Complexity Metrics for Component-Based Software},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2659118.2659135},
doi = {10.1145/2659118.2659135},
abstract = {In the current state of software engineering, component-based software development is one of the most alluring paradigms for developing large and complex software products. In this software engineering methodology pre-engineered, pre-tested, context-based, adaptable, deployable software components are assembled according to a predefined architecture. Rather than developing a system from scratch, component-based software development emphasizes the integration of these components according to the user's requirements and specifications. In component-based software, the components interact to access and provide services and functionality to each other. Currently, the emphasis of industry and researchers is on developing impressive and efficient metrics and measurement tools to analyze the interaction complexity among these components. To represent the request and the response of services among components, we have used outgoing edges and incoming edges respectively. In this paper we have defined these interactions as In-Interactions and Out-Interactions. The metrics proposed in this paper are solely based on the interactions among the components. In this work some simple methods and metrics for computing the complexity of composable components are suggested. The metrics discussed in this paper include the computation of interaction complexities as Total-Interactions of a component, Total- Interactions of component-based software, Interaction-Ratio of a component, Interaction-Ratio of component-based software, Average- Interaction among components and Interaction-Percentage of components.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–4},
numpages = {4},
keywords = {component-based software development, pre-engineered, adaptable, in-interactions, context-based, out-interactions, metrics}
}

@inproceedings{10.1145/3102304.3102334,
author = {Calcina-Ccori, Pablo and Costa, Laisa and Fedrecheski, Geovane and Esquiagola, John and Zuffo, Marcelo and da Silva, Fl\'{a}vio Corr\^{e}a},
title = {Agile Servient Integration with the Swarm: Automatic Code Generation for Nodes in the Internet of Things},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102334},
doi = {10.1145/3102304.3102334},
abstract = {Swarm vision, consists in an organic ecosystem of heterogeneous devices that communicate and collaborate to achieve complex results. In previous work, we have proposed an architecture to implement this vision based on web technologies. In this paper, we have proposed a framework that makes the creation of Swarm-ready servients (devices that acts both as server and client) easier, by generating a ready-to-run project from a high-level description of the service. The project generated contains all dependencies and libraries needed to integrate an IoT device into the Swarm, thus saving development and configuration time. We compared the development effort of creating a servient by hand and by using our framework, having the number of lines of code as a metric. Our results show a reduction of 500\% in the development effort to connect a device to the Swarm. The next steps include a semantic high-level description for participating services and support for resource-constrained devices.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {30},
numpages = {6},
keywords = {Servient, automatic code generation, Internet of Things, Swarm},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/3167486.3167534,
author = {Seraoui, Youssef and Belmekki, Mostafa and Bellafkih, Mostafa and Raouyane, Brahim},
title = {ETOM Mapping onto NFV Framework: IMS Use Case},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167534},
doi = {10.1145/3167486.3167534},
abstract = {Telecom professionals have a strong interest in the proposition and adaptation of innovate network management models and frameworks to help mobile network operators (MNOs) to improve their business processes and get more agile in the telecoms industry that evolves with great speed. The model being established by the TeleManagement Forum (TM Forum) is the Enhanced Telecom Operations MAP (eTOM) business process framework on which we rely in this work to propose a mapping of the eTOM model onto the network functions virtualization (NFV) framework with the projection of this function mapping onto the IP Multimedia Subsystem (IMS) use case. This mapping covers essentially four main components playing important rules in the MNO's business processes, including customers, services, infrastructure resources, and also service providers. The main goal, thereby, is to design a combined architecture in a virtualized environment for dynamic delivery of services with quality of service (QoS) and improved resource performance so as to meet the purposes of the 5G network in terms of a proposed, virtual telecom environment managed and orchestrated by the conjunction of the aforementioned paradigms. Indeed, we conducted simulations to evaluate part of this function mapping in an IMS setting for static service chain provisioning. Thus, results showed possible provisioning of services in this context in measuring SIP related key performance indicators and performance metrics. Results showed the feasibility of our approach. In addition, resource performance improved obviously in the NFV context in accordance with eTOM business processes.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {45},
numpages = {8},
keywords = {five generation (5G), service chain provisioning, network functions virtualization (NFV), IP Multimedia Subsystem (IMS), Business process, quality of service (QoS), New Generation Operations Systems and Software (NGOSS), resource performance, Enhanced Telecom Operations Map (eTOM)},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@inproceedings{10.1145/3465481.3470091,
author = {Komisarek, Miko\l{}aj and Pawlicki, Marek and Kowalski, Miko\l{}aj and Marzecki, Adrian and Kozik, Rafa\l{} and Chora\'{s}, Micha\l{}},
title = {Network Intrusion Detection in the Wild - the Orange Use Case in the SIMARGL Project},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470091},
doi = {10.1145/3465481.3470091},
abstract = {There is a profuse abundance of network security incidents around the world every day. Increasingly, services and data stored on servers fall victim to sophisticated techniques that cause all sorts of damage. Hackers invent new ways to bypass security measures and modify the existing viruses in order to deceive defense systems. Therefore, in response to these illegal procedures, new ways to defend against them are being developed. In this paper, a method for anomaly detection based on machine learning technique is presented and a near real-time processing system architecture is proposed. The main contribution is a test-run of ML algorithms on real-world data coming from a world-class telecom operator. This work investigates the effectiveness of detecting malicious behaviour in network packets using several machine learning techniques. The results achieved are expressed with a set of metrics. For better clarity on the classifier performance, 10-fold cross-validation was used.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {65},
numpages = {7},
keywords = {machine learning, network intrusion detection},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1145/3238147.3240467,
author = {Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin},
title = {Experiences Applying Automated Architecture Analysis Tool Suites},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240467},
doi = {10.1145/3238147.3240467},
abstract = {In this paper, we report our experiences of applying three complementary automated software architecture analysis techniques, supported by a tool suite, called DV8, to 8 industrial projects within a large company. DV8 includes two state-of-the-art architecture-level maintainability metrics—Decoupling Level and Propagation Cost, an architecture flaw detection tool, and an architecture root detection tool. We collected development process data from the project teams as input to these tools, reported the results back to the practitioners, and followed up with telephone conferences and interviews. Our experiences revealed that the metrics scores, quantitative debt analysis, and architecture flaw visualization can effectively bridge the gap between management and development, help them decide if, when, and where to refactor. In particular, the metrics scores, compared against industrial benchmarks, faithfully reflected the practitioners’ intuitions about the maintainability of their projects, and enabled them to better understand the maintainability relative to other projects internal to their company, and to other industrial products. The automatically detected architecture flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify the “hotspots" within the systems that are responsible for high maintenance costs. Except for the two smallest projects for which both architecture metrics indicated high maintainability, all other projects are planning or have already begun refactorings to address the problems detected by our analyses. We are working on further automating the tool chain, and transforming the analysis suite into deployable services accessible by all projects within the company.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {779–789},
numpages = {11},
keywords = {Software Architecture, Software Maintenance, Software Quality},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1145/2983637,
author = {Miao, Wang and Min, Geyong and Wu, Yulei and Wang, Haozhe and Hu, Jia},
title = {Performance Modelling and Analysis of Software-Defined Networking under Bursty Multimedia Traffic},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2983637},
doi = {10.1145/2983637},
abstract = {Software-Defined Networking (SDN) is an emerging architecture for the next-generation Internet, providing unprecedented network programmability to handle the explosive growth of big data driven by the popularisation of smart mobile devices and the pervasiveness of content-rich multimedia applications. In order to quantitatively investigate the performance characteristics of SDN networks, several research efforts from both simulation experiments and analytical modelling have been reported in the current literature. Among those studies, analytical modelling has demonstrated its superiority in terms of cost-effectiveness in the evaluation of large-scale networks. However, for analytical tractability and simplification, existing analytical models are derived based on the unrealistic assumptions that the network traffic follows the Poisson process, which is suitable to model nonbursty text data, and the data plane of SDN is modelled by one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies have shown that, due to the features of heavy volume and high velocity, the multimedia big data generated by real-world multimedia applications reveals the bursty and correlated nature in the network transmission. With the aim of capturing such features of realistic traffic patterns and obtaining a comprehensive and deeper understanding of the performance behaviour of SDN networks, this article presents a new analytical model to investigate the performance of SDN in the presence of the bursty and correlated arrivals modelled by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance metrics in terms of the average latency and average network throughput of the SDN networks are derived based on the developed analytical model. To consider a realistic multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted to model the SDN data plane. To address the challenging problem of obtaining the key performance metrics, for example, queue-length distribution of a PQ system with a given service capacity, a versatile methodology extending the Empty Buffer Approximation (EBA) method is proposed to facilitate the decomposition of such a PQ system to two SSSQ systems. The validity of the proposed model is demonstrated through extensive simulation experiments. To illustrate its application, the developed model is then utilised to study the strategy of the network configuration and resource allocation in SDN networks.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {77},
numpages = {19},
keywords = {performance modelling and analysis, resource allocation, multimedia big data, Software-defined networking, queueing decomposition}
}

@article{10.1145/3394956,
author = {Sahoo, Kshira Sagar and Puthal, Deepak},
title = {SDN-Assisted DDoS Defense Framework for the Internet of Multimedia Things},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3394956},
doi = {10.1145/3394956},
abstract = {The Internet of Things is visualized as a fundamental networking model that bridges the gap between the cyber and real-world entity. Uniting the real-world object with virtualization technology is opening further opportunities for innovation in nearly every individual’s life. Moreover, the usage of smart heterogeneous multimedia devices is growing extensively. These multimedia devices that communicate among each other through the Internet form a unique paradigm called the Internet of Multimedia Things (IoMT). As the volume of the collected data in multimedia application increases, the security, reliability of communications, and overall quality of service need to be maintained. Primarily, distributed denial of service attacks unveil the pervasiveness of vulnerabilities in IoMT systems. However, the Software Defined Network (SDN) is a new network architecture that has the central visibility of the entire network, which helps to detect any attack effectively. In this regard, the combination of SDN and IoMT, termed SD-IoMT, has the immense ability to improve the network management and security capabilities of the IoT system. This article proposes an SDN-assisted two-phase detection framework, namely SD-IoMT-Protector, in which the first phase utilizes the entropy technique as the detection metric to verify and alert about the malicious traffic. The second phase has trained with an optimized machine learning technique for classifying different attacks. The outcomes of the experimental results signify the usefulness and effectiveness of the proposed framework for addressing distributed denial of service issues of the SD-IoMT system.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {dec},
articleno = {98},
numpages = {18},
keywords = {entropy, SDN, Control plane, IoMT, machine learning, security}
}

@article{10.1145/3319498,
author = {Izadpanah, Ramin and Allan, Benjamin A. and Dechev, Damian and Brandt, Jim},
title = {Production Application Performance Data Streaming for System Monitoring},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3319498},
doi = {10.1145/3319498},
abstract = {In this article, we present an approach to streaming collection of application performance data. Practical application performance tuning and troubleshooting in production high-performance computing (HPC) environments requires an understanding of how applications interact with the platform, including (but not limited to) parallel programming libraries such as Message Passing Interface (MPI). Several profiling and tracing tools exist that collect heavy runtime data traces either in memory (released only at application exit) or on a file system (imposing an I/O load that may interfere with the performance being measured). Although these approaches are beneficial in development stages and post-run analysis, a systemwide and low-overhead method is required to monitor deployed applications continuously. This method must be able to collect information at both the application and system levels to yield a complete performance picture.In our approach, an application profiler collects application event counters. A sampler uses an efficient inter-process communication method to periodically extract the application counters and stream them into an infrastructure for performance data collection. We implement a tool-set based on our approach and integrate it with the Lightweight Distributed Metric Service (LDMS) system, a monitoring system used on large-scale computational platforms. LDMS provides the infrastructure to create and gather streams of performance data in a low overhead manner. We demonstrate our approach using applications implemented with MPI, as it is one of the most common standards for the development of large-scale scientific applications.We utilize our tool-set to study the impact of our approach on an open source HPC application, Nalu. Our tool-set enables us to efficiently identify patterns in the behavior of the application without source-level knowledge. We leverage LDMS to collect system-level performance data and explore the correlation between the system and application events. Also, we demonstrate how our tool-set can help detect anomalies with a low latency. We run tests on two different architectures: a system enabled with Intel Xeon Phi and another system equipped with Intel Xeon processor. Our overhead study shows our method imposes at most 0.5\% CPU usage overhead on the application in realistic deployment scenarios.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {apr},
articleno = {8},
numpages = {25},
keywords = {Application and system monitoring, application profiling, performance data streaming}
}

@inproceedings{10.1145/3130218.3130225,
author = {Wang, Zicong and Chen, Xiaowen and Li, Chen and Guo, Yang},
title = {Fairness-Oriented and Location-Aware NUCA for Many-Core SoC},
year = {2017},
isbn = {9781450349840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3130218.3130225},
doi = {10.1145/3130218.3130225},
abstract = {Non-uniform cache architecture (NUCA) is often employed to organize the last level cache (LLC) by Networks-on-Chip (NoC). However, along with the scaling up for network size of Systems-on-Chip (SoC), two trends gradually begin to emerge. First, the network latency is becoming the major source of the cache access latency. Second, the communication distance and latency gap between different cores is increasing. Such gap can seriously cause the network latency imbalance problem, aggravate the degree of non-uniform for cache access latencies, and then worsen the system performance.In this paper, we propose a novel NUCA-based scheme, named fairness-oriented and location-aware NUCA (FL-NUCA), to alleviate the network latency imbalance problem and achieve more uniform cache access. We strive to equalize network latencies which are measured by three metrics: average latency (AL), latency standard deviation (LSD), and maximum latency (ML). In FL-NUCA, the memory-to-LLC mapping and links are both non-uniform distributed to better fit the network topology and traffics, thereby equalizing network latencies from two aspects, i.e., non-contention latencies and contention latencies, respectively. The experimental results show that FL-NUCA can effectively improve the fairness of network latencies. Compared with the traditional static NUCA (S-NUCA), in simulation with synthetic traffics, the average improvements for AL, LSD, and ML are 20.9\%, 36.3\%, and 35.0\%, respectively. In simulation with PARSEC benchmarks, the average improvements for AL, LSD, and ML are 6.3\%, 3.6\%, and 11.2\%, respectively.},
booktitle = {Proceedings of the Eleventh IEEE/ACM International Symposium on Networks-on-Chip},
articleno = {13},
numpages = {8},
keywords = {non-uniform cache architecture, memory mapping, Networks-on-chip},
location = {Seoul, Republic of Korea},
series = {NOCS '17}
}

