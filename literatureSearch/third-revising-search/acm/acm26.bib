@inproceedings{10.1145/3172944.3172975,
author = {Vanderdonckt, Jean and Bouzit, Sara and Calvary, Ga\"{e}lle and Ch\^{e}ne, Denis},
title = {Cloud Menus: A Circular Adaptive Menu for Small Screens},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172975},
doi = {10.1145/3172944.3172975},
abstract = {This paper presents Cloud Menus, a split adaptive menu for small screens where the predicted menu items are arranged in a circular tag cloud with a location consistent with their corresponding position in the static menu and a font size depending on their prediction level. This layout results from a 3-step design process: (i) defining an initial design space on Bertin's 8 visual variables and 4 quality properties, (ii) identifying the most preferred layout based on agreement rate, and (iii) implementing it into Cloud Menus, a new widget for Android with circular layout. An empirical study suggests that cloud menus reduce item selection time and error rate when prediction is correct without penalizing it when prediction is incorrect, compared to two baselines: a non-adaptive static menu and an adaptive linear menu. From this study, design guidelines for cloud menus are elaborated.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {317–328},
numpages = {12},
keywords = {tag cloud, prediction window, split menu, adaptive menu},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/2742854.2742858,
author = {Han, Rui and Wang, Junwei and Ge, Fengming and Vazquez-Poletti, Jose Luis and Zhan, Jianfeng},
title = {SARP: Producing Approximate Results with Small Correctness Losses for Cloud Interactive Services},
year = {2015},
isbn = {9781450333580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742854.2742858},
doi = {10.1145/2742854.2742858},
abstract = {Despite the importance of providing fluid responsiveness to user requests for interactive services, such request processing is very resource expensive when dealing with large-scale input data. These often exceed the application owners' budget when services are deployed on a cloud, in which resources are charged in monetary terms. Providing approximate processing results is a feasible solution for such problem that trades off request correctness (quantified by output quality) for response time reduction. However, existing techniques in this area either use partial input data or skip expensive computations to produce approximate results, thus resulting in large losses in output quality on a tight resource budget. In this paper, we propose SARP, a Synopsis-based Approximate Request Processing framework to produce approximate results with small correctness losses even using small amount of resources. To achieve this, SARP conducts full computations over the statistical aggregation of the entire input data using two key ideas: (1) offline synopsis management that generates and maintains a set of synopses that represent the statistical aggregation of original input data at different approximation levels. (2) Online synopsis selection that considers both the current resource allocation and the workload status so as to select the synopsis with the maximal length that can be processed within the required response time. We demonstrate the effectiveness of our approach by testing the recommendation services in E-commerce sites using a large, real-world dataset. Using prediction accuracy as the output quality, the results demonstrate: (i) SARP achieves significant response time reduction with very small quality losses compared to the exact processing results.(ii) Using the same processing time, SARP demonstrates a considerable reduction in quality loss compared to existing approximation techniques.},
booktitle = {Proceedings of the 12th ACM International Conference on Computing Frontiers},
articleno = {22},
numpages = {8},
keywords = {synopsis, interactive service, output quality, approximate results, result correctness, cloud},
location = {Ischia, Italy},
series = {CF '15}
}

@inproceedings{10.1145/3323503.3349545,
author = {de Amorim, Irandir O. and de Melo, Jose F. V. and Balieiro, Andson M. and Santos, Bruno B. dos},
title = {An Evolutionary Approach for Video Application Energy Consumption Estimation in Mobile Devices},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3349545},
doi = {10.1145/3323503.3349545},
abstract = {In the last years, the multimedia traffic has increased significantly and the mobile devices (e.g. smart phones and tablets) have been widely used to consume this content type. Video applications demand high energy consumption of the device because they perform complex operations and deal with a large data amount. Although hardware improvements in the mobile devices have been achieved, the advances in battery technology have not kept the same pace. In this respect, the combination of video applications with the limited battery capacity of the mobile devices has challenged the academia and industry in the development of techniques for energy management and provision of quality of experience (QoE) to the user. Energy consumption estimation models may assist these techniques, as well as, the decision made process when the computational offloading from the mobile device to the cloud is considered. This paper presents an evolutionary approach based on Genetic Algorithms (GAs) and Swarm Particle Optimization (PSO) for energy consumption estimation in mobile devices running video applications. The proposal is directly applicable to different model types (linear and non-linear ones), without the linearization cost, and it is evaluated in terms of mean squared error (MSE), using energy consumption measurement data of videos with different configurations. Results show the superiority of our proposal in comparison to the literature that adopts the Ordinary Least Squares method.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {169–175},
numpages = {7},
keywords = {energy consumption model for mobile devices, video application, genetic algorithms, particle swarm optimization},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/2565585.2565607,
author = {Klugman, Noah and Rosa, Javier and Pannuto, Pat and Podolsky, Matthew and Huang, William and Dutta, Prabal},
title = {Grid Watch: Mapping Blackouts with Smart Phones},
year = {2014},
isbn = {9781450327428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2565585.2565607},
doi = {10.1145/2565585.2565607},
abstract = {The power grid is one of humanity's most significant engineering undertakings and it is essential in developed and developing nations alike. Currently, transparency into the power grid relies on utility companies and more fine-grained insight is provided by costly smart meter deployments. We claim that greater visibility into power grid conditions can be provided in an inexpensive and crowd-sourced manner independent of utility companies by leveraging existing smartphones. Our key insight is that an unmodified smartphone can detect power outages by monitoring changes to its own power state, locally verifying these outages using a variety of sensors that reduce the likelihood of false power outage reports, and corroborating actual reports with other phones through data aggregation in the cloud. The proposed approach enables a decentralized system that can scale, potentially providing researchers and concerned citizens with a powerful new tool to analyze the power grid and hold utility companies accountable for poor power quality. This paper demonstrates the viability of the basic idea, identifies a number of challenges that are specific to this application as well as ones that are common to many crowd-sourced applications, and highlights some improvements to smartphone operating systems that could better support such applications in the future.},
booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
articleno = {1},
numpages = {6},
keywords = {crowdsourcing, smartphone applications, smart grid, side channel information, power monitoring},
location = {Santa Barbara, California},
series = {HotMobile '14}
}

@inproceedings{10.1109/CCGRID.2018.00029,
author = {Chatterjee, Subarna and Morin, Christine},
title = {Experimental Study on the Performance and Resource Utilization of Data Streaming Frameworks},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00029},
doi = {10.1109/CCGRID.2018.00029},
abstract = {With the advent of the Internet of Things (IoT), data stream processing have gained increased attention due to the ever-increasing need to process heterogeneous and voluminous data streams. This work addresses the problem of selecting a correct stream processing framework for a given application to be executed within a specific physical infrastructure. For this purpose, we focus on a thorough comparative analysis of three data stream processing platforms - Apache Flink, Apache Storm, and Twitter Heron (the enhanced version of Apache Storm), that are chosen based on their potential to process both streams and batches in real-time. The goal of the work is to enlighten the cloud-clients and the cloud-providers with the knowledge of the choice of the resource-efficient and requirement-adaptive streaming platform for a given application so that they can plan during allocation or assignment of Virtual Machines for application execution. For the comparative performance analysis of the chosen platforms, we have experimented using 8-node clusters on Grid5000 experimentation testbed and have selected a wide variety of applications ranging from a conventional benchmark to sensor-based IoT application and statistical batch processing application. In addition to the various performance metrics related to the elasticity and resource usage of the platforms, this work presents a comparative study of the "green-ness" of the streaming platforms by analyzing their power consumption - one of the first attempts of its kind. The obtained results are thoroughly analyzed to illustrate the functional behavior of these platforms under different computing scenarios.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {143–152},
numpages = {10},
keywords = {Apache spark, stream processing, internet of things, Apache flink, Twitter heron},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/2716281.2836120,
author = {Kateja, Rajat and Baranasuriya, Nimantha and Navda, Vishnu and Padmanabhan, Venkata N.},
title = {DiversiFi: Robust Multi-Link Interactive Streaming},
year = {2015},
isbn = {9781450334129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2716281.2836120},
doi = {10.1145/2716281.2836120},
abstract = {Real-time, interactive streaming for applications such as audio-video conferencing (e.g., Skype) and cloud-based gaming depends critically on the network providing low latency, jitter, and packet loss, much more so than on-demand streaming (e.g., YouTube) does. However, WiFi networks pose a challenge; our analysis of data from a large VoIP provider and from our own measurements shows that the WiFi access link is a significant cause of poor streaming experience.To improve streaming quality over WiFi, we present DiversiFi, which takes advantage of the diversity of WiFi links available in the vicinity, even when the individual links are poor. Leveraging such cross-link spatial and channel diversity outperforms both traditional link selection and the temporal diversity arising from retransmissions on the same link. It also provides significant gains over and above the PHY-layer spatial diversity provided by MIMO. Our experimental evaluation shows that, for a client with two NICs, enabling replication across two WiFi links helps cut down the poor call rate (PCR) for VoIP by 2.24x.Finally, we present the design and implementation of DiversiFi, which enables it to operate with single-NIC clients, and with either minimally modified APs or unmodified APs augmented with a middlebox. Over 61 runs, where the baseline average PCR is 4.9\%, DiversiFi running with a single NIC, switching between two links, helps cut the PCR down to 0\%, while duplicating wastefully only 0.62\% of the packets and impacting competing TCP throughput by only 2.5\%. Thus, DiversiFi provides the benefit of multi-link diversity for real-time interactive streaming in a manner that is deployable and imposes little overhead, thereby ensuring coexistence with other applications.},
booktitle = {Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies},
articleno = {35},
numpages = {13},
keywords = {wi-fi, multi-path, VoIP, real-time streaming},
location = {Heidelberg, Germany},
series = {CoNEXT '15}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Software Product Line, Web System, Energy Aware, Machine Learning},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1109/CCGrid.2014.50,
author = {Tolosana-Calasanz, Rafael and Ba\~{n}ares, Jos\'{e} \'{A}ngel and Rana, Omer and Pham, Congduc and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
title = {Enforcing Quality of Service on OpenNebula-Based Shared Clouds},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.50},
doi = {10.1109/CCGrid.2014.50},
abstract = {With an increase in the number of monitoring sensors deployed on physical infrastructures, there is a corresponding increase in data volumes that need to be processed. Data measured or collected by sensors is typically processed at destination or "in-transit" (i.e. from data capture to delivery to a user). When such data are processed in-transit over a shared distributed computing infrastructure, it is useful to provide elastic computational capability which can be adapted based on processing requirements and demand. Where Service Level Agreements (SLAs) have been pre-agreed, such available computational capacity needs to be shared in such a way that any Quality of Service related constraints in such SLAs are not violated. This is particularly challenging for time critical applications and with highly variable and unpredictable rates of data generation (e.g. in Smart Grid applications where energy usage patterns may change unpredictably). Previously, we proposed a Reference net based architectural model for supporting QoS for multiple concurrent data streams being processed (prior to delivery to a user) over a shared infrastructure. In this paper, we describe a practical realisation of this architecture using the OpenNebula Cloud platform. We consider our infrastructure to be composed of a number of nodes, each of which has multiple processing units and data buffers. We utilize the "token bucket" model for regulating, on a per stream basis, the data injection rate into each node. We subsequently demonstrate how a streaming pipeline can be supported and managed using a dynamic control strategy at each node.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {651–659},
numpages = {9},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3307681.3325409,
author = {Li, Yusen and Shan, Chuxu and Chen, Ruobing and Tang, Xueyan and Cai, Wentong and Tang, Shanjiang and Liu, Xiaoguang and Wang, Gang and Gong, Xiaoli and Zhang, Ying},
title = {GAugur: Quantifying Performance Interference of Colocated Games for Improving Resource Utilization in Cloud Gaming},
year = {2019},
isbn = {9781450366700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307681.3325409},
doi = {10.1145/3307681.3325409},
abstract = {Cloud gaming has been very popular recently, but providing satisfactory gaming experiences to players at a modest cost is still challenging. Colocating several games onto one server could improve server utilization. To enable efficient colocations while providing Quality of Service (QoS) guarantees, a precise quantification of performance interference among colocated games is required. However, achieving such precise interference prediction is very challenging for games due to the complexity introduced by the contention on many shared resources across CPU and GPU. Moreover, the distinctive properties of cloud gaming require that the prediction model should be constructed beforehand and the prediction should be made instantaneously at request arrivals, which further increases the difficulty. The existing solutions are either not applicable or not effective due to many limitations. In this paper, we present GAugur, a novel methodology that enables highly accurate prediction of the performance interference among games arbitrarily colocated. By leveraging machine learning technologies, GAugur is able to capture the complex relationship between the interference and the contention features of colocated games. We evaluate GAugur through extensive experiments using a large number of real popular games. The results show that GAugur is able to identify whether a colocated game satisfies QoS requirement within an average error of 5\%, and is able to quantify the performance degradation of a colocated game within an average error of 7.9\%, which significantly outperforms the alternatives. Moreover, GAugur incurs an offline profiling cost linear to the number of games, and negligible overhead for online prediction. We apply GAugur to guiding efficient game colocations for cloud gaming. Experimental results show that GAugur is able to increase the resource utilization by 20\% to 60\%, and improve the overall performance by up to 15\%, compared to the state-of-the-art solutions.},
booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {231–242},
numpages = {12},
keywords = {machine learning, performance prediction, performance interference, game co-location, cloud gaming},
location = {Phoenix, AZ, USA},
series = {HPDC '19}
}

@proceedings{10.1145/2789168,
title = {MobiCom '15: Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM MobiCom 2015, the 21st Annual International Conference on Mobile Computing and Networking. MobiCom is the premier forum for publishing and presenting cutting-edge research in mobile systems and wireless networks. The technical program this year features 38 outstanding papers that cover a wide variety of topics including energy, sensing, security, wireless access, applications, localization, Internet of things, mobile cloud, measurement and analysis. We created a new Experience track this year to encourage authors to present extensive experiences with implementation, deployment, and operations of mobile ncomputing and wireless networks. One of the accepted papers is an Experience paper on cellular networks.This year's call for papers attracted 207 qualified submissions from across the globe that were carefully reviewed by 46 Technical Program Committee (TPC) members (+2 TPC chairs) along with a selected group of external experts. The TPC was formed with the goal of covering diverse research expertise as well as diverse perspectives and approaches. The TPC included researchers from 12 countries including China, France, Germany, India, Italy, Singapore, South Korea, Spain, Sweden, Switzerland, UK, and USA. 25\% of the members were female, the highest ever in the history of MobiCom. We also had broad industry participation with TPC members from Alcatel-Lucent, Google, HP, IBM, Microsoft, NEC, and Telefonica.The paper review process was double-blinded and carried out in three phases. In the first phase, each paper was reviewed by at least three TPC members, and the top 112 papers were selected for the second phase. In addition to reviewer scores, reviewer confidence and normalization with respect to other papers in a reviewer's pile, were also considered in selecting papers. In the second phase, each paper was reviewed by at least two more reviewers followed by an online, often intense, discussion, producing 68 papers for the final phase. The final TPC meeting was held on May 28th and 29th in Salt Lake City, Utah. These 68 papers were organized by their topic areas, and discussed at length at the meeting. Eventually, 38 papers were shortlisted for inclusion in the program and a shepherd from the TPC was assigned to each of these papers. As the last step, each of the shortlisted papers was shepherded through a "blind" process where the authors interacted with all the reviewers and the shepherd to address the review comments without knowing the reviewers' or the shepherds' identities. The end result is an exciting technical program composed of 38 very high quality papers.During the review process, Prof. Robin Kravets, the TPC co-chair of MobiCom 2013, handled the papers that were co-authored by TPC chairs, and those that had conflict-of-interest with both TPC chairs. To ensure fairness and preserve the anonymity of all authors and reviewers, the assignment of reviewers, the reviews and discussions of these papers were done out of band without any exposure to the TPC chairs.},
location = {Paris, France}
}

@inproceedings{10.1145/3094405.3094406,
author = {Zhao, Yang and Xia, Nai and Tian, Chen and Li, Bo and Tang, Yizhou and Wang, Yi and Zhang, Gong and Li, Rui and Liu, Alex X.},
title = {Performance of Container Networking Technologies},
year = {2017},
isbn = {9781450350587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094405.3094406},
doi = {10.1145/3094405.3094406},
abstract = {Container networking is now an important part of cloud virtualization architectures. It provides network access for containers by connecting both virtual and physical network interfaces. The performance of container networking has multiple dependencies, and each factor may significantly affect the performance. In this paper, we perform systematic experiments to study the performance of container networking technologies. For every measurement result, we try our best to qualify influencing factors.},
booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
pages = {1–6},
numpages = {6},
keywords = {Container, Measurement, Networking},
location = {Los Angeles, CA, USA},
series = {HotConNet '17}
}

@inproceedings{10.1145/3446132.3446412,
author = {Maskat, Ruhaila and Faizzuddin Zainal, Muhammad and Ismail, Nurrissammimayantie and Ardi, Norizah and Ahmad, Amirah and Daud, Noriza},
title = {Automatic Labelling of Malay Cyberbullying Twitter Corpus Using Combinations of Sentiment, Emotion and Toxicity Polarities},
year = {2021},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446412},
doi = {10.1145/3446132.3446412},
abstract = {Automatic labelling is essential in large corpuses. Engaging in human experts to label can be challenging. Semantic understanding can differ from one labeler to another based on individual's language ability. Platforms such as AmazonTurk are not able to ensure the quality of annotations in every domain. Extensive steps such as qualification and counter checking of labels may be implemented which will increase the cost of data annotation. Thus, the higher quality of labelled data expected, the greater the cost that needs to be expended. This scenario is made worse when the language is of low resource where in this work is the Malay language. Malay is a language used mostly in Malaysia, Indonesia, Singapore and Brunei. Unlike English which has large resources to tap into the semantics of sentences, making automatic labelling faster to mature, resources in Malay language are still limited. Further compounded is the use of social media data where the text is short, unnormalized and the inherent presence of code switching. The availability of qualified native Malay labelers is also scarce. To overcome this, we devised a method to automatically label a total of 219,444 Malay tweets by using a combination of sentiment, emotion and toxicity polarities. We extend the work from Arslan et al. who proposed the use of sentiment and emotion to identify cyberbullying text. Our work added toxicity polarity in the context of automatic labelling of cyberbully tweets in Malay. We were able to employ 5 experts with formal degrees in Malay language to label our training set. We applied this method to Malay cyberbullying corpus to determine “bully” and “not bully” labels. We have tested our method on 54,867 manually labelled data and achieved high accuracy.},
booktitle = {Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {85},
numpages = {6},
keywords = {Twitter, Cyberbullying, Malay language, Automatic labelling},
location = {Sanya, China},
series = {ACAI '20}
}

@inproceedings{10.1145/3307339.3343464,
author = {Humphrey, Marty and Lin, Vincent and Notani, Shweta and Mattos, Jose},
title = {Leveraging the Cloud for Intelligent Clinical Data Registries},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3343464},
doi = {10.1145/3307339.3343464},
abstract = {Public cloud platforms provide an amazing set of capabilities, but it can be an overwhelming challenge to create a design, implementation, and deployment that properly leverages today's existing public cloud capabilities while not precluding the use of near-future new services and infrastructure. We tackle this challenge in the context of clinical data registries, and create Cloud-based Patient Outcomes Platform (CPOP), our scalable public cloud application for clinical patient data. Doctors are able to visualize collected medical data in different chart formats and patients are able to check their data and submit medical survey forms. The specific domain of interest in this paper is Chronic Rhinosinusitis (CRS), a largely under-recognized chronic disease in our society. The primary barrier to quality improvement in CRS is the difficulty in collecting data from patients, tracking appropriate follow-up time intervals, and analyzing outcomes results in a prospective and ongoing fashion. We describe key aspects and design experiences of CPOP-CRS in Amazon Web Services.We also provide quantitative evaluation of a key feature of CPOP-CRS, which is the ability of CRS doctors to upload an audio clip of a doctor-patient interaction, and have the cloud render a text-based representation, and show a word error rate of 15.6\%. We outline next steps in the development of the CPOP/CPOP-CRS, and provide guidance for other users considering the public cloud for their next parallel and cloud-based Bioinformatics and Biomedicine project.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {675–682},
numpages = {8},
keywords = {cloud computing, amazon web services, clinical data registries},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/3152881.3152887,
author = {Rahman, Mahmudur and Hong, Hua-Jun and Rahman, Amatur and Tsai, Pei-Hsuan and Afrin, Afia and Uddin, Md Yusuf Sarwar and Venkatasubramanian, Nalini and Hsu, Cheng-Hsin},
title = {Adaptive Sensing Using Internet-of-Things with Constrained Communications},
year = {2017},
isbn = {9781450351683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152881.3152887},
doi = {10.1145/3152881.3152887},
abstract = {In this paper, we design and implement an Internet-of-Things (IoT) based platform for developing cities using environmental sensing as driving application with a set of air quality sensors that periodically upload sensor data to the cloud. Ubiquitous and free WiFi access is unavailable in most developing cities; IoT deployments must leverage 3G cellular connections that are expensive and metered. In order to best utilize the limited 3G data plan, we envision two adaptation strategies to drive sensing and sensemaking. The first technique is an infrastructure-level adaptation approach where we adjust sensing intervals of periodic sensors so that the data volume remains bounded within the plan. The second approach is at the information-level where application-specific analytics are deployed on board devices (or the edge) through container technologies (Docker and Kubernetes); the use case focuses on multimedia sensors that process captured raw information to lower volume semantic data that is communicated. This approach is implemented through the EnviroSCALE (Environmental Sensing and Community Alert Network) platform, an inexpensive Raspberry Pi based environmental sensing system that periodically publishes sensor data over a 3G connection with a limited data plan. We outline our deployment experience of EnviroSCALE in Dhaka city, the capital of Bangladesh. For information-level adaptation, we enhanced EnviroSCALE with Docker containers with rich media analytics, along Kubernetes for provisioning IoT devices and deploying the Docker images. To limit data communication overhead, the Docker images are preloaded in the board but a small footprint of analytic code is transferred whenever required. Our experiment results demonstrate the practicality of adaptive sensing and triggering rich sensing analytics via user-specified criteria, even over constrained data connections.},
booktitle = {Proceedings of the 16th Workshop on Adaptive and Reflective Middleware},
articleno = {6},
numpages = {6},
location = {Las Vegas, Nevada},
series = {ARM '17}
}

@inproceedings{10.1145/2593793.2593798,
author = {Bersani, Marcello M. and Bianculli, Domenico and Dustdar, Schahram and Gambi, Alessio and Ghezzi, Carlo and Krsti\'{c}, Sr\textcrd{}an},
title = {Towards the Formalization of Properties of Cloud-Based Elastic Systems},
year = {2014},
isbn = {9781450328418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593793.2593798},
doi = {10.1145/2593793.2593798},
abstract = {Cloud-based elastic systems run on a cloud infrastructure and have the capability of dynamically adjusting the allocation of their resources in response to changes in the workload, in a way that balances the trade-off between the desired quality-of-service and the operational costs. The actual elastic behavior of these systems is determined by a combination of factors, including the input workload, the logic of the elastic controller determining the type of resource adjustment, and the underlying technological platform implementing the cloud infrastructure. All these factors have to be taken into account to express the desired elastic behavior of a system, as well as to verify whether the system manifests or not such a behavior.  In this paper, we take a first step into these directions, by proposing a formalization, based on the CLTL^t(D) temporal logic, of several concepts and properties related to the behavior of cloud-based elastic systems. We also report on our preliminary evaluation of the feasibility to check the (formalized) properties on execution traces using an automated verification tool.},
booktitle = {Proceedings of the 6th International Workshop on Principles of Engineering Service-Oriented and Cloud Systems},
pages = {38–47},
numpages = {10},
keywords = {elastic systems, temporal logic, Cloud computing},
location = {Hyderabad, India},
series = {PESOS 2014}
}

@inproceedings{10.5555/2755753.2757193,
author = {Liu, Zhiqing and Liu, Chuangwen and Young, Evangeline F. Y.},
title = {An Effective Triple Patterning Aware Grid-Based Detailed Routing Approach},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Triple patterning lithography (TPL) is attracting more and more attention due to further scaling of the critical feature size. How fully the benefits of TPL can be utilized depends very much on both the decomposition and layout steps. However, it is non-trivial to perform detailed routing and layout decomposition simultaneously on a large-scale complicated circuit to achieve decomposability on one hand, and short wirelength, small number of stitches and small number of vias on the other hand. In our approach, routing and coloring are done iteratively but integrated closely to reduce the problem complexity. The routing step is able to detect and avoid native conflicts as much as possible. If any conflicts occur in the coloring step, the router will rip-up and re-route to get rid of them. This technique proves to be effective and efficient in improving the quality of the coloring assignment. Compared with previous works [1] on TPL using simultaneous routing and coloring, the number of stitches and the number of vias are reduced by 76.8\% and 2.1\% respectively while our running time is 36.6\% less and the wirelength is very comparable.},
booktitle = {Proceedings of the 2015 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition},
pages = {1641–1646},
numpages = {6},
location = {Grenoble, France},
series = {DATE '15}
}

@article{10.1145/3369875,
author = {Haller, Armin and Fern\'{a}ndez, Javier D. and Kamdar, Maulik R. and Polleres, Axel},
title = {What Are Links in Linked Open Data? A Characterization and Evaluation of Links between Knowledge Graphs on the Web},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3369875},
doi = {10.1145/3369875},
abstract = {Linked Open Data promises to provide guiding principles to publish interlinked knowledge graphs on the Web in the form of findable, accessible, interoperable, and reusable datasets. We argue that while as such, Linked Data may be viewed as a basis for instantiating the FAIR principles, there are still a number of open issues that cause significant data quality issues even when knowledge graphs are published as Linked Data. First, to define boundaries of single coherent knowledge graphs within Linked Data, a principled notion of what a dataset is, or, respectively, what links within and between datasets are, has been missing. Second, we argue that to enable FAIR knowledge graphs, Linked Data misses standardised findability and accessability mechanism via a single entry link. To address the first issue, we (i) propose a rigorous definition of a naming authority for a Linked Data dataset, (ii) define different link types for data in Linked datasets, (iii) provide an empirical analysis of linkage among the datasets of the Linked Open Data cloud, and (iv) analyse the dereferenceability of those links. We base our analyses and link computations on a scalable mechanism implemented on top of the HDT format, which allows us to analyse quantity and quality of different link types at scale.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {9},
numpages = {34},
keywords = {RDF, Linked Data}
}

@inproceedings{10.1145/3423336.3429345,
author = {Oehmcke, Stefan and Chen, Tzu-Hsin Karen and Prishchepov, Alexander V. and Gieseke, Fabian},
title = {Creating Cloud-Free Satellite Imagery from Image Time Series with Deep Learning},
year = {2020},
isbn = {9781450381628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423336.3429345},
doi = {10.1145/3423336.3429345},
abstract = {Optical satellite images are important for environmental monitoring. Unfortunately, such images are often affected by distortions, such as clouds, shadows, or missing data. This work proposes a deep learning approach for cleaning and imputing satellite images, which could serve as a reliable preprocessing step for spatial and spatio-temporal analyzes. More specifically, a coherent and cloud-free image for a specific target date and region is created based on a sequence of images of that region obtained at previous dates. Our model first extracts information from the previous time steps via a special gating function and then resorts to a modified version of the well-known U-Net architecture to obtain the desired output image. The model uses supplementary data, namely the approximate cloud coverage of input images, the temporal distance to the target time, and a missing data mask for each input time step. During the training phase we condition our model with the targets cloud coverage and missing values (disabled in production), which allows us to use data afflicted by distortion during training and thus does not require pre-selection of distortion-free data. Our experimental evaluation, conducted on data of the Landsat missions, shows that our approach outperforms the commonly utilized approach that resorts to taking the median of cloud-free pixels for a given position. This is especially the case when the quality of the data for the considered period is poor (e.g., lack of cloud free-images during the winter/fall periods). Our deep learning approach allows to improve the utility of the entire Landsat archive, the only existing global medium-resolution free-access satellite archive dating back to the 1970s. It therefore holds scientific and societal potential for future analyses conducted on data from this and other satellite imagery repositories.},
booktitle = {Proceedings of the 9th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
articleno = {3},
numpages = {10},
keywords = {satellite imagery, image reconstruction, remote sensing},
location = {Seattle, Washington},
series = {BigSpatial '20}
}

@inproceedings{10.1145/3208806.3208813,
author = {Klomann, Marcel and Englert, Michael and Weber, Kai and Grimm, Paul and Jung, Yvonne},
title = {Improving Mobile MR Applications Using a Cloud-Based Image Segmentation Approach with Synthetic Training Data},
year = {2018},
isbn = {9781450358002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208806.3208813},
doi = {10.1145/3208806.3208813},
abstract = {In this paper, we show how the quality of augmentation in mobile Mixed Reality applications can be improved using a cloud-based image segmentation approach with synthetic training data. Many modern Augmented Reality frameworks are based on visual inertial odometry on mobile devices and therefore have limited access to tracking hardware (e.g., depth sensor). Consequently, tracking still suffers from drift that makes it difficult to utilize in use cases that require a higher precision. To improve tracking quality, we propose a cloud tracking approach that uses machine learning based image segmentation to recognize known objects in a real scene, which allows us to estimate a precise camera pose. Augmented Reality applications that utilize our web service can use the resulting camera pose to correct drift from time to time, while still using local tracking between key frames. Moreover, the device's position in the real world, when starting the application, is usually used as reference coordinate system. Therefore, we simplify the authoring of MR applications significantly due to a well-defined coordinate system, which is context-based and not dependend on the starting position of a user. We present all steps from web-based initialization over the generation of synthetic training data up to usage in production. In addition, we describe the underlying algorithms in detail. Finally, we show a mobile Mixed Reality application, which is based on this novel approach and discuss its advantages.},
booktitle = {Proceedings of the 23rd International ACM Conference on 3D Web Technology},
articleno = {4},
numpages = {7},
keywords = {mobile mixed reality, AR authoring, computer vision, tracking, training data generation, image segmentation, machine learning},
location = {Pozna\'{n}, Poland},
series = {Web3D '18}
}

@inproceedings{10.1145/3447555.3465326,
author = {Hanafy, Walid A. and Molom-Ochir, Tergel and Shenoy, Rohan},
title = {Design Considerations for Energy-Efficient Inference on Edge Devices},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3465326},
doi = {10.1145/3447555.3465326},
abstract = {The emergence of low-power accelerators has enabled deep learning models to be executed on mobile or embedded edge devices without relying on cloud resources. The energy-constrained nature of these devices requires a judicious choice of a deep learning model and system configuration parameter to meet application needs while optimizing energy used during deep learning inference.In this paper, we carry out an experimental evaluation of more than 40 popular pretrained deep learning models to characterize trends in their accuracy, latency, and energy when running on edge accelerators. Our results show that as models have grown in size, the marginal increase in their accuracy has come at a much higher energy cost. Consequently, simply choosing the most accurate model for an application task comes at a higher energy cost; the application designer needs to consider the tradeoff between latency, accuracy, and energy use to make an appropriate choice. Since the relation between these metrics is non-linear, we present a recommendation algorithm to enable application designers to choose the best deep learning model for an application that meets energy budget constraints. Our results show that our technique can provide recommendations that are within 3 to 7\% of the specified budget while maximizing accuracy and minimizing energy.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {302–308},
numpages = {7},
keywords = {Deep learning Inference, Edge Computing, Energy-efficient Deep Learning},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@inproceedings{10.1145/3469263.3469858,
author = {Baldoni, Gabriele and Loudet, Julien and Cominardi, Luca and Corsaro, Angelo and He, Yong},
title = {Facilitating Distributed Data-Flow Programming with Eclipse Zenoh: The ERDOS Case},
year = {2021},
isbn = {9781450386036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469263.3469858},
doi = {10.1145/3469263.3469858},
abstract = {Data-flow programming is the computational model of choice for a large class of application domains, such as, real-time data processing, robotics platforms, and big-data analytics. Traditionally, dataflows are deployed and executed within well-defined system boundaries, such as robots, radars, or data-centers. These boundaries however are expected to blur with the advent of Edge Computing, which provides a multi-tier infrastructure spanning from the cloud to the things and enables for the distribution of applications across this continuum. In this paper we make a step towards the design of an Edge-native data-flow by mixing technologies coming from both worlds: ERDOS, a novel data-flow framework, and Eclipse Zenoh, a Named-Data-Networking built for the Edge Computing. More specifically, we (i) investigate how ERDOS can be expanded to cover Edge deployments by leveraging Zenoh, (ii) analyze the advantages provided by this integration, and (iii) evaluate the performance of a Zenoh-powered ERDOS. Our results show that ERDOS experiences a higher throughput and bounded latency when operating over Zenoh. Moreover, Zenoh enhances ERDOS with full location transparency, allowing developers and system designers to focus on the logic of their application as opposed to the topology deployment. Finally, our integration of Zenoh and ERDOS is available as open source at https://github.com/atolab/erdos-on-zenoh.},
booktitle = {Proceedings of the 1st Workshop on Serverless Mobile Networking for 6G Communications},
pages = {13–18},
numpages = {6},
keywords = {serverless, zenoh, ERDOS, data-flow, FaaS, NDN},
location = {Virtual, WI, USA},
series = {MobileServerless'21}
}

@article{10.1109/TNET.2021.3106937,
author = {Zhang, Sheng and Wang, Can and Jin, Yibo and Wu, Jie and Qian, Zhuzhong and Xiao, Mingjun and Lu, Sanglu},
title = {Adaptive Configuration Selection and Bandwidth Allocation for Edge-Based Video Analytics},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3106937},
doi = {10.1109/TNET.2021.3106937},
abstract = {Major cities worldwide have millions of cameras deployed for surveillance, business intelligence, traffic control, crime prevention, etc. Real-time analytics on video data demands intensive computation resources and high energy consumption. Traditional cloud-based video analytics relies on large centralized clusters to ingest video streams. With edge computing, we can offload compute-intensive analysis tasks to nearby servers, thus mitigating long latency incurred by data transmission via wide area networks. When offloading video frames from the front-end device to an edge server, the application configuration (i.e., frame sampling rate and frame resolution) will impact several metrics, such as energy consumption, analytics accuracy and user-perceived latency. In this paper, we study the configuration selection and bandwidth allocation for multiple video streams, which are connected to the same edge node sharing an upload link. We propose an efficient online algorithm, called JCAB, which jointly optimizes configuration adaption and bandwidth allocation to address a number of key challenges in edge-based video analytics systems, including edge capacity limitation, unknown network variation, intrusive dynamics of video contents. Our algorithm is developed based on Lyapunov optimization and Markov approximation, works online without requiring future information, and achieves a provable performance bound. We also extend the proposed algorithms to the multi-edge scenario in which each user or video stream has an additional choice about which edge server to connect. Extensive evaluation results show that the proposed solutions can effectively balance the analytics accuracy and energy consumption while keeping low system latency in a variety of settings.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {285–298},
numpages = {14}
}

@inproceedings{10.1145/3365609.3365865,
author = {Arnold, Todd and Calder, Matt and Cunha, Italo and Gupta, Arpit and Madhyastha, Harsha V. and Schapira, Michael and Katz-Bassett, Ethan},
title = {Beating BGP is Harder than We Thought},
year = {2019},
isbn = {9781450370202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365609.3365865},
doi = {10.1145/3365609.3365865},
abstract = {Online services all seek to provide their customers with the best Quality of Experience (QoE) possible. Milliseconds of delay can cause users to abandon a cat video or move onto a different shopping site, which translates into lost revenue. Thus, minimizing latency between users and content is crucial. To reduce latency, content and cloud providers have built massive, global networks. However, their networks must interact with customer ISPs via BGP, which has no concept of performance.The shortcomings of BGP are many and well documented, but in this paper we ask the community to take a step back and rethink what we know about BGP. We examine three separate studies of performance using large content and cloud provider networks and find that performance-aware routing schemes rarely achieve lower latency than BGP. We lay out a map for research to further study the idea that beating BGP may be more difficult than previously thought.},
booktitle = {Proceedings of the 18th ACM Workshop on Hot Topics in Networks},
pages = {9–16},
numpages = {8},
keywords = {BGP, performance, content delivery, traffic engineering},
location = {Princeton, NJ, USA},
series = {HotNets '19}
}

@inproceedings{10.1145/3375555.3384938,
author = {Zibitsker, Boris and Lupersolsky, Alex},
title = {How to Apply Modeling to Compare Options and Select the Appropriate Cloud Platform},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3384938},
doi = {10.1145/3375555.3384938},
abstract = {Organizations want to take advantage of the flexibility and scalability of Cloud platforms. By migrating to the Cloud, they hope to develop and implement new applications faster with lower cost. Amazon AWS, Microsoft Azure, Google, IBM, Oracle and others Cloud providers support different DBMS like Snowflake, Redshift, Teradata Vantage, and others. These platforms have different architectures, mechanisms of allocation and management of resources, and levels of sophistication of DBMS optimizers which affect performance, scalability and cost. As a result, the response time, CPU Service Time and the number of I/Os for the same query, accessing the similar table in the Cloud could be significantly different than On Prem. In order to select the appropriate Cloud platform as a first step we perform a Workload Characterization for On Prem Data Warehouse. Each Data Warehouse workload represents a specific line of business and includes activity of many users generating concurrently simple and complex queries accessing data from different tables. Each workload has different demands for resources and different Response Time and Throughput Service Level Goals. In this presentation we will review results of the workload characterization for an On Prem Data Warehouse environment. During the second step we collected measurement data for standard TPC-DS benchmark tests performed in AWS Vantage, Redshift and Snowflake Cloud platform for different sizes of the data sets and different number of concurrent users. During the third step we used the results of the workload characterization and measurement data collected during the benchmark to modify BEZNext On Prem Closed Queueing model to model individual Clouds. And finally, during the fourth step we used our Model to take into consideration differences in concurrency, priorities and resource allocation to different workloads. BEZNext optimization algorithms incorporating Graduate search mechanism are used to find the AWS instance type and minimum number of instances which will be required to meet SLGs for each of the workloads. Publicly available information about the cost of the different AWS instances is used to predict the cost of supporting workloads in the Cloud month by month during next 12 months.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {16},
numpages = {1},
keywords = {service level goals, optimization., workload characterization, cloud platform, seasonality determination, modeling, workload forecasting, benchmarking},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@article{10.1145/3450626.3459679,
author = {Ma, Xiaohe and Kang, Kaizhang and Zhu, Ruisheng and Wu, Hongzhi and Zhou, Kun},
title = {Free-Form Scanning of Non-Planar Appearance with Neural Trace Photography},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459679},
doi = {10.1145/3450626.3459679},
abstract = {We propose neural trace photography, a novel framework to automatically learn high-quality scanning of non-planar, complex anisotropic appearance. Our key insight is that free-form appearance scanning can be cast as a geometry learning problem on unstructured point clouds, each of which represents an image measurement and the corresponding acquisition condition. Based on this connection, we carefully design a neural network, to jointly optimize the lighting conditions to be used in acquisition, as well as the spatially independent reconstruction of reflectance from corresponding measurements. Our framework is not tied to a specific setup, and can adapt to various factors in a data-driven manner. We demonstrate the effectiveness of our framework on a number of physical objects with a wide variation in appearance. The objects are captured with a light-weight mobile device, consisting of a single camera and an RGB LED array. We also generalize the framework to other common types of light sources, including a point, a linear and an area light.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {124},
numpages = {13},
keywords = {illumination multiplexing, optimal lighting pattern, SVBRDF}
}

@inproceedings{10.1145/3105762.3105772,
author = {Mazumdar, Amrita and Alaghi, Armin and Barron, Jonathan T. and Gallup, David and Ceze, Luis and Oskin, Mark and Seitz, Steven M.},
title = {A Hardware-Friendly Bilateral Solver for Real-Time Virtual Reality Video},
year = {2017},
isbn = {9781450351010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105762.3105772},
doi = {10.1145/3105762.3105772},
abstract = {Rendering 3D-360° VR video from a camera rig is computation-intensive and typically performed offline. In this paper, we target the most time-consuming step of the VR video creation process, high-quality flow estimation with the bilateral solver. We propose a new algorithm, the hardware-friendly bilateral solver, that enables faster runtimes than existing algorithms of similar quality. Our algorithm is easily parallelized, achieving a 4\texttimes{} speedup on CPU and 32\texttimes{} speedup on GPU over a baseline CPU implementation. We also design an FPGA-based hardware accelerator that utilizes reduced-precision computation and the parallelism inherent in our algorithm to achieve further speedups over our CPU and GPU implementations while consuming an order of magnitude less power. The FPGA design's power efficiency enables practical real-time VR video processing at the camera rig or in the cloud.},
booktitle = {Proceedings of High Performance Graphics},
articleno = {13},
numpages = {10},
keywords = {hardware accelerators, parallelism, FPGA design, GPU algorithm, virtual reality, real-time image processing},
location = {Los Angeles, California},
series = {HPG '17}
}

@inproceedings{10.1145/3132211.3134452,
author = {Grassi, Giulio and Jamieson, Kyle and Bahl, Paramvir and Pau, Giovanni},
title = {Parkmaster: An in-Vehicle, Edge-Based Video Analytics Service for Detecting Open Parking Spaces in Urban Environments},
year = {2017},
isbn = {9781450350877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132211.3134452},
doi = {10.1145/3132211.3134452},
abstract = {We present the design and implementation of ParkMaster, a system that leverages the ubiquitous smartphone to help drivers find parking spaces in the urban environment. ParkMaster estimates parking space availability using video gleaned from drivers' dash-mounted smartphones on the network's edge, uploading analytics about the street to the cloud in real time as participants drive. Novel lightweight parked-car localization algorithms enable the system to estimate each parked car's approximate location by fusing information from phone's camera, GPS, and inertial sensors, tracking and counting parked cars as they move through the driving car's camera frame of view. To visually calibrate the system, ParkMaster relies only on the size of well-known objects in the urban environment for on-the-go calibration. We implement and deploy ParkMaster on Android smartphones, uploading parking analytics to the Azure cloud. On-the-road experiments in three different environments comprising Los Angeles, Paris and an Italian village measure the end-to-end accuracy of the system's parking estimates (close to 90\%) as well as the amount of cellular data usage the system requires (less than one mega-byte per hour). Drill-down microbenchmarks then analyze the factors contributing to this end-to-end performance, as video resolution, vision algorithm parameters, and CPU resources.},
booktitle = {Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
articleno = {16},
numpages = {14},
keywords = {fog computing, mobile systems, edge computing, visual analytics},
location = {San Jose, California},
series = {SEC '17}
}

@inproceedings{10.1145/3460231.3474615,
author = {Zhang, Yuxi and Xie, Kexin},
title = {Boosting Local Recommendations With Partially Trained Global Model},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474615},
doi = {10.1145/3460231.3474615},
abstract = {Building recommendation systems for enterprise software has many unique challenges that are different from consumer-facing systems. When applied to different organizations, the data used to power those recommendation systems vary substantially in both quality and quantity due to differences in their operational practices, marketing strategies, and targeted audiences. At Salesforce, as a cloud provider of such a system with data across many different organizations, naturally, it makes sense to pool data from different organizations to build a model that combines all values from different brands. However, multiple issues like how do we make sure a model trained with pooled data can still capture customer specific characteristics, how do we design the system to handle those data responsibly and ethically, i.e., respecting contractual agreements with our clients, legal and compliance requirements, and the privacy of all the consumers. In this proposal, We present a framework that not only utilizes enriched user-level data across organizations, but also boosts business-specific characteristics in generating personal recommendations. We will also walk through key privacy considerations when designing such a system.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {533–535},
numpages = {3},
keywords = {global model, recommendation system, privacy},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/2933349.2933356,
author = {Kyzirakos, Kostis and Alvanaki, Foteini and Kersten, Martin},
title = {In Memory Processing of Massive Point Clouds for Multi-Core Systems},
year = {2016},
isbn = {9781450343190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933349.2933356},
doi = {10.1145/2933349.2933356},
abstract = {LIDAR is a popular remote sensing method used to examine the surface of the Earth. LIDAR instruments use light in the form of a pulsed laser to measure ranges (variable distances) and generate vast amounts of precise three dimensional point data describing the shape of the Earth. Processing large collections of point cloud data and combining them with auxiliary GIS data remain an open research problem.Past research in the area of geographic information systems focused on handling large collections of complex geometric objects stored on disk and most algorithms have been designed and studied in a single-thread setting even though multi-core systems are well established. In this paper, we describe parallel alternatives of known algorithms for evaluating spatial selections over point clouds and spatial joins between point clouds and rectangle collections.},
booktitle = {Proceedings of the 12th International Workshop on Data Management on New Hardware},
articleno = {7},
numpages = {10},
location = {San Francisco, California},
series = {DaMoN '16}
}

@inproceedings{10.1145/2851613.2852009,
author = {Ullah, Amjad},
title = {Towards Workload-Aware Fine-Grained Control over Cloud Resources: Student Research Abstract},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2852009},
doi = {10.1145/2851613.2852009},
abstract = {The systems deployed over cloud are subject to unpredictable workload conditions that vary from time to time, e.g. an ecommerce website may face higher workloads than normal during festivals or promotional schemes. In order to maintain the performance of such systems, an efficient elastic resource provisioning strategy is required. However, providing such a strategy that determines the right amount of cloud resources that fulfills the Quality of Service (QoS) demand is a challenging task. Over the period, many proposals have been introduced using techniques like threshold based rules, reinforcement learning and control theory, etc. The existing proposals, however, suffer from issues like lack of expertise to appropriately set the quantitative specification of thresholds, online training time overhead of the algorithm, too specific to work well in particular situation like when there is sudden burst in workload or work well in nominal conditions for stable workload, etc. Moreover, the existing approaches do not address uncertainty. Our proposed framework is a step forward to address the mentioned issues for systems that hold time varying workload conditions.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {488–489},
numpages = {2},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3469096.3469864,
author = {Frieder, Ophir},
title = {Searching Harsh Documents},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469864},
doi = {10.1145/3469096.3469864},
abstract = {Conventional, textual document search is arguably well understood. Traditional and modern (neural) algorithms are available; benchmark collections and evaluation metrics are prevalent. However, not all documents are conventional or purely textual. We explore what is takes to search "harsh" document collections. Such collections comprise potentially of documents that are natively non-digital, are multilingual, include components that are not strictly textual, are corrupted, or are a combination thereof. We address machine readability and its implication on search. We overview component segmentation and integration as a search process. We describe the processing of search queries that are informationally deficient or corrupt. We then comment on the evaluation of the selected efforts presented and highlight their history from concept to practice. We conclude with a brief commentary on ongoing efforts.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {4},
numpages = {1},
keywords = {enhancement, non-digital text processing, benchmarks, corrupted text},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@inproceedings{10.5555/3172795.3172826,
author = {Pravato, Laura and Doyle, Thomas E.},
title = {IoT for Remote Wireless Electrophysiological Monitoring: Proof of Concept},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {The Internet of Things (IoT) offers integrated sensing of all aspects of daily life. The field of healthcare offers the greatest potential for IoT to benefit society, but also presents significant challenges. A key component of IoT is the development of intelligent ubiquitous sensing. Achieving this requires circuits and systems that require low power and efficient computation.As a proof of concept, we present a prototype design of a continuous wireless electrocardiogram (ECG) monitoring device that uses a small, low-cost IoT wi-fi module to upload real-time data to the cloud. Two IoT cloud services were evaluated to record and plot real-time ECG data: IBM Bluemix and ThingSpeak. Preliminary data quality was analyzed using kurtosis and spectral distribution ratio. Future development is necessary to improve battery power and to implement real-time data analysis.Remote medical and health monitoring is an important step in supporting personalized predictive analytics, smart homes, and chronic illness management. The presented device has the potential to provide health professionals with real-time ECG data allowing for diagnosis of cardiac pathologies, monitoring of patients suffering from heart disease and/or patients recovering from cardiac conditions.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {254–258},
numpages = {5},
keywords = {electrocardiogram, wearables, ESP32, IBM watson, IBM Bluemix, ECG, ThingSpeak, ESP8266, IoT, internet of things},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3243046.3243049,
author = {Caballero-Gil, Pino and Caballero-Gil, C\'{a}ndido and Molina-Gil, Jezabel},
title = {Ubiquitous System to Monitor Transport and Logistics},
year = {2018},
isbn = {9781450359610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243046.3243049},
doi = {10.1145/3243046.3243049},
abstract = {In the management of transport and logistics, which includes the delivery, movement and collection of goods through roads, ports and airports, participate, in general, many different actors. The most critical aspects of supply chain systems include time, space and interdependencies. Besides, there are several security challenges that can be caused both by unintentional and intentional errors. With all this in mind, this work proposes the combination of technologies such as RFID, GPS, WiFi Direct and LTE/3G to automate product authentication and merchandise tracking, reducing the negative effects caused either by mismanagement or attacks against the process of the supply chain. In this way, this work proposes a ubiquitous management scheme for the monitoring through the cloud of freight and logistics systems, including demand management, customization and automatic replenishment of out-of-stock goods. The proposal implies an improvement in the efficiency of the systems, which can be quantified in a reduction of time and cost in the inventory and distribution processes, and in a greater facility for the detection of counterfeit versions of branded articles. In addition, it can be used to create safer and more efficient schemes that help companies and organizations to improve the quality of the service and the traceability of the transported goods.},
booktitle = {Proceedings of the 15th ACM International Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, \&amp; Ubiquitous Networks},
pages = {71–75},
numpages = {5},
keywords = {ubiquitous system, security, logistics, wireless technologies},
location = {Montreal, QC, Canada},
series = {PE-WASUN'18}
}

@article{10.1145/3409772,
author = {Kaur, Kuljeet and Garg, Sahil and Kaddoum, Georges and Kumar, Neeraj},
title = {Energy and SLA-Driven MapReduce Job Scheduling Framework for Cloud-Based Cyber-Physical Systems},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3409772},
doi = {10.1145/3409772},
abstract = {Energy consumption minimization of cloud data centers (DCs) has attracted much attention from the research community in the recent years; particularly due to the increasing dependence of emerging Cyber-Physical Systems on them. An effective way to improve the energy efficiency of DCs is by using efficient job scheduling strategies. However, the most challenging issue in selection of efficient job scheduling strategy is to ensure service-level agreement (SLA) bindings of the scheduled tasks. Hence, an energy-aware and SLA-driven job scheduling framework based on MapReduce is presented in this article. The primary aim of the proposed framework is to explore task-to-slot/container mapping problem as a special case of energy-aware scheduling in deadline-constrained scenario. Thus, this problem can be viewed as a complex multi-objective problem comprised of different constraints. To address this problem efficiently, it is segregated into three major subproblems (SPs), namely, deadline segregation, map and reduce phase energy-aware scheduling. These SPs are individually formulated using Integer Linear Programming. To solve these SPs effectively, heuristics based on Greedy strategy along with classical Hungarian algorithm for serial and serial-parallel systems are used. Moreover, the proposed scheme also explores the potential of splitting Map/Reduce phase(s) into multiple stages to achieve higher energy reductions. This is achieved by leveraging the concepts of classical Greedy approach and priority queues. The proposed scheme has been validated using real-time data traces acquired from OpenCloud. Moreover, the performance of the proposed scheme is compared with the existing schemes using different evaluation metrics, namely, number of stages, total energy consumption, total makespan, and SLA violated. The results obtained prove the efficacy of the proposed scheme in comparison to the other schemes under different workload scenarios.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {31},
numpages = {24},
keywords = {energy optimization, and MapReduce, Hungarian algorithm, Cyber-physical systems, greedy approach, job scheduling}
}

@inproceedings{10.1109/CCGrid.2014.42,
author = {Glatard, Tristan and Rousseau, Marc-Etienne and Rioux, Pierre and Adalat, Reza and Evans, Alan C.},
title = {Controlling the Deployment of Virtual Machines on Clusters and Clouds for Scientific Computing in CBRAIN},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.42},
doi = {10.1109/CCGrid.2014.42},
abstract = {The emergence of hardware virtualization, notably exploited by cloud infrastructures, led to a paradigm shift in distributed computing by enabling complete software customization and elastic scaling of resources. However, new software architectures and deployment algorithms are still required to fully exploit virtualization in web platforms used for scientific computing, commonly called science gateways. We propose a software architecture and an algorithm to enable and optimize the deployment of virtual machines on clusters and clouds in science gateways. Our architecture is based on 3 design principles: (i) separation between resource provisioning and task scheduling (ii) encapsulation of VMs in regular computing tasks (iii) association of a virtual computing site to each disk image. Our algorithm submits and removes VMs on clusters and clouds based on the current system workload, the number of available job slots in active VMs, the cost and current performance of clouds clusters, and a parameter quantifying the performance-cost trade-off. To cope with variable queuing and booting times, it replicates VMs on independent computing sites selected from a minimization of a makespan-cost linear combination in the Pareto set of non-dominated solutions. Makespan and cost are estimated from the last measured queuing, booting, and task execution times, using an exponential model of the gain yielded by VM replication. We implement this algorithm in CBRAIN, a science gateway widely used for neuroimaging, and we evaluate it on an infrastructure of 2 clusters and 1 cloud. Results show that it is able to reach some points of the performance-cost trade-off associated to VM deployment.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {384–393},
numpages = {10},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3375555.3384939,
author = {Bondi, Andr\'{e} B.},
title = {WOSP-C 2020: Workshop on Challenges and Opportunities in Large-Scale Performance: Welcoming Remarks},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3384939},
doi = {10.1145/3375555.3384939},
abstract = {It is my great pleasure to welcome you to WOSP-C 2020, the Workshop on Challenges and Opportunities in Large Scale Performance. Our theme this year relates to the use of analytics to interpret system performance and resource usage measurements that can now be gathered rapidly on a large scale. Our four invited speakers hail from industry. All three presentations in the first session and the last presentation in the second session deal with modeling and measurement to automate the making of decisions about system configuration or the recognition of anomalies, especially for cloud-based systems. The other two papers in the second session address measurement and modeling issues at a granular level. These topics are highly relevant to the issues systems architects and other stakeholders face when deploying systems in the cloud, because doing so need not guarantee good performance. The recent emergence of the ability to gather vast numbers of performance and resource usage measurements facilitates the informed choice of target cloud platforms and their configurations. The presentations in this workshop deal with various aspects of how this can be achieved.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {5–6},
numpages = {2},
keywords = {cloud measurement, monitoring and tuning, software performance engineering, automated system performance modeling},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3341302.3342073,
author = {Jin, Yuchen and Renganathan, Sundararajan and Ananthanarayanan, Ganesh and Jiang, Junchen and Padmanabhan, Venkata N. and Schroder, Manuel and Calder, Matt and Krishnamurthy, Arvind},
title = {Zooming in on Wide-Area Latencies to a Global Cloud Provider},
year = {2019},
isbn = {9781450359566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341302.3342073},
doi = {10.1145/3341302.3342073},
abstract = {The network communications between the cloud and the client have become the weak link for global cloud services that aim to provide low latency services to their clients. In this paper, we first characterize WAN latency from the viewpoint of a large cloud provider Azure, whose network edges serve hundreds of billions of TCP connections a day across hundreds of locations worldwide. In particular, we focus on instances of latency degradation and design a tool, BlameIt, that enables cloud operators to localize the cause (i.e., faulty AS) of such degradation. BlameIt uses passive diagnosis, using measurements of existing connections between clients and the cloud locations, to localize the cause to one of cloud, middle, or client segments. Then it invokes selective active probing (within a probing budget) to localize the cause more precisely. We validate BlameIt by comparing its automatic fault localization results with that arrived at by network engineers manually, and observe that BlameIt correctly localized the problem in all the 88 incidents. Further, BlameIt issues 72X fewer active probes than a solution relying on active probing alone, and is deployed in production at Azure.},
booktitle = {Proceedings of the ACM Special Interest Group on Data Communication},
pages = {104–116},
numpages = {13},
keywords = {networkfault localization, network diagnosis, tomography, active network probes, internet latency measurement, wide-area network},
location = {Beijing, China},
series = {SIGCOMM '19}
}

@inproceedings{10.1145/2945292.2945311,
author = {Devaux, Alexandre and Br\'{e}dif, Mathieu},
title = {Realtime Projective Multi-Texturing of Pointclouds and Meshes for a Realistic Street-View Web Navigation},
year = {2016},
isbn = {9781450344289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945292.2945311},
doi = {10.1145/2945292.2945311},
abstract = {Street-view web applications have now gained widespread popularity. Targeting the general public, they offer ease of use, but while they allow efficient navigation from a pedestrian level, the immersive quality of such renderings is still low. The user is usually stuck at specific positions and transitions bring out artefacts, in particular parallax and aliasing. We propose a method to enhance the realism of street view navigation systems using a hybrid rendering based on realtime projective texturing on meshes and pointclouds with occlusion handling, requiring extremely minimized pre-processing steps allowing fast data update, progressive streaming (mesh-based approximation, with point cloud details) and unaltered raw data precise visualization.},
booktitle = {Proceedings of the 21st International Conference on Web3D Technology},
pages = {105–108},
numpages = {4},
keywords = {image based rendering, GIS, projective texturing, WebGL, street-view, point based rendering},
location = {Anaheim, California},
series = {Web3D '16}
}

@inproceedings{10.1145/3434581.3434632,
author = {Mao, Deng and Jie, Liu},
title = {Streak Image Compression},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434632},
doi = {10.1145/3434581.3434632},
abstract = {This Streak image contains a lot of noise signals and it is not easy to accurately detect the target signal position. This article gives a method of determining target signal region and achieving streak image compression, which uses multi-resolution wavelet denoising and detecting target signal considering its overall continuous distribution as a rule and using the stepped-like piecewise function of range as the detection object. In target signal region, convolution filtering is used to detect the peaks, and then the three-dimensional image is calculated. Experiments show that the method can reduce the scale of data for computing, improve the quality of generating point cloud data and reconstruct 3D images more accurately.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {450–454},
numpages = {5},
keywords = {stepped-like piecewise function, overall continuous distribution, Streak image compression, target signal},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1145/3437914.3437980,
author = {M. Pittman, Jason},
title = {DRAT - A Dynamic Resource Allocation Tool for Estimating Compute Power in a Cybersecurity Engineering Learning Facility},
year = {2021},
isbn = {9781450389594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437914.3437980},
doi = {10.1145/3437914.3437980},
abstract = {Cybersecurity laboratory infrastructure has direct impact on the quality of student learning experiences. Because of this, the computing education field has developed a variety of approaches to designing and implementing these learning facilities. Yet, little work has gone into how to properly size cybersecurity laboratory infrastructure relative to student population and curricular compute power demands. The result has been laboratory infrastructures that do not scale with degree programs. Consequently, laboratories are either underpowered, thus limiting learning experiences, or overpowered which wastes financial resources. Accordingly, this work presents DRAT, an open-source software tool, for estimating necessary compute power in a cybersecurity engineering learning facility. More specifically, DRAT is designed to estimate the required discrete compute power on a per exercise basis in a cybersecurity engineering learning facility operating in a private cloud model. Such discrete estimations are intended to communicate physical host hardware requirements such as physical CPU core count, virtual RAM, and total Hard Disk space. The first step in designing DRAT was to forge a model estimator function. Then, we identified a series of scalar abstractions representing learning facility hardware infrastructure and behaving as conversion factors between the model function and output. Because the goal of this work was to provide estimates for cloud compute power requirements, DRAT outputs the number of physical cores, total RAM, total Disk, and total (virtual or physical) Network interfaces required to run the indicated scenario. The implication is that such estimates can inform purchasing and configuration decisions which directly impact student learning outcomes.},
booktitle = {Proceedings of the 5th Conference on Computing Education Practice},
pages = {39},
numpages = {1},
location = {<conf-loc>, <city>Durham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CEP '21}
}

@inproceedings{10.1145/3176258.3176328,
author = {Alshehri, Asma and Benson, James and Patwa, Farhan and Sandhu, Ravi},
title = {Access Control Model for Virtual Objects (Shadows) Communication for AWS Internet of Things},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176328},
doi = {10.1145/3176258.3176328},
abstract = {The concept of Internet of Things (IoT) has received considerable attention and development in recent years. There have been significant studies on access control models for IoT in academia, while companies have already deployed several cloud-enabled IoT platforms. However, there is no consensus on a formal access control model for cloud-enabled IoT. The access-control oriented (ACO) architecture was recently proposed for cloud-enabled IoT, with virtual objects (VOs) and cloud services in the middle layers. Building upon ACO, operational and administrative access control models have been published for virtual object communication in cloud-enabled IoT illustrated by a use case of sensing speeding cars as a running example.In this paper, we study AWS IoT as a major commercial cloud-IoT platform and investigate its suitability for implementing the afore-mentioned academic models of ACO and VO communication control. While AWS IoT has a notion of digital shadows closely analogous to VOs, it lacks explicit capability for VO communication and thereby for VO communication control. Thus there is a significant mismatch between AWS IoT and these academic models. The principal contribution of this paper is to reconcile this mismatch by showing how to use the mechanisms of AWS IoT to effectively implement VO communication models. To this end, we develop an access control model for virtual objects (shadows) communication in AWS IoT called AWS-IoT-ACMVO. We develop a proof-of-concept implementation of the speeding cars use case in AWS IoT under guidance of this model, and provide selected performance measurements. We conclude with a discussion of possible alternate implementations of this use case in AWS IoT.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {175–185},
numpages = {11},
keywords = {abac, internet of things (iot), virtual objects, acl, iot architecture, devices, security, access control, aws iot, rbac},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1109/CCGRID.2017.118,
author = {Xhagjika, Vamis and Escoda, \`{O}scar Divorra and Navarro, Leandro and Vlassov, Vladimir},
title = {Load and Video Performance Patterns of a Cloud Based WebRTC Architecture},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.118},
doi = {10.1109/CCGRID.2017.118},
abstract = {Web Real-Time Communication or Realtime communication in the Web (WebRTC/RTCWeb) is a prolific new standard and technology stack, providing full audio/video agnostic communications for the Web. Service providers implementing such technology deal with various levels of complexity ranging anywhere from: high service distribution, multiclient integration, P2P and Cloud assisted communication backends, content delivery, real-time constraints and across clouds resource allocation. This work presents a study of the joint factors including multi-cloud distribution, network performance, media parameters and back-end resource loads, in Cloud based Media Selective Forwarding Units for WebRTC infrastructures. The monitored workload is sampled from a large population of real users of our testing infrastructure, additionally the performance data is sampled both by passive user measurements as well as server side measurements. Patterns correlating such factors enable designing adaptive resource allocation algorithms and defining media Service Level Objectives (SLO) spanning over multiple data-centers or servers. Based on our analysis, we discover strong periodical load patterns even though the nature of user interaction with the system is mostly not predetermined with variable user churn.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {739–744},
numpages = {6},
keywords = {media, webrtc, stream allocation, load measurements, bitrate, rtp/rtcp},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3366030.3366041,
author = {da Silva, Gabriela Oliveira Mota and Dur\~{a}o, Frederico Ara\'{u}jo and Capretz, Miriam},
title = {PLDSD: Personalized Linked Data Semantic Distance for LOD-Based Recommender Systems},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366041},
doi = {10.1145/3366030.3366041},
abstract = {A vast amount of data that can be easily read by machines have been published in freely accessible and interconnected datasets, creating the so-called Linked Open Data cloud. This phenomenon has opened opportunities for the development of semantic applications, including recommender systems. In this paper, we propose Personalized Linked Data Semantic Distance (PLDSD), a novel similarity measure for linked data that personalizes the RDF graph by adding weights to the edges, based on previous user's choices. Thus, our approach has the purpose of minimizing the sparsity problem by ranking the best features for a particular user, and also, of solving the item cold-start problem, since the feature ranking task is based on features shared between old items and the new item. We evaluate PLDSD in the context of a LOD-based Recommender System using mixed data from DBpedia and MovieLens, and the experimental results indicate better accuracy of recommendations compared to a non-personalized baseline similarity method.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {294–303},
numpages = {10},
keywords = {Semantic Similarity, Linked Open Data, Feature Selection, Recommender Systems, Graph Personalization},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1109/MICRO.2014.53,
author = {Zhang, Yunqi and Laurenzano, Michael A. and Mars, Jason and Tang, Lingjia},
title = {SMiTe: Precise QoS Prediction on Real-System SMT Processors to Improve Utilization in Warehouse Scale Computers},
year = {2014},
isbn = {9781479969982},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2014.53},
doi = {10.1109/MICRO.2014.53},
abstract = {One of the key challenges for improving efficiency in warehouse scale computers (WSCs) is to improve server utilization while guaranteeing the quality of service (QoS) of latency-sensitive applications. To this end, prior work has proposed techniques to precisely predict performance and QoS interference to identify 'safe' application co-locations. However, such techniques are only applicable to resources shared across cores. Achieving such precise interference prediction on real-system simultaneous multithreading (SMT) architectures has been a significantly challenging open problem due to the complexity introduced by sharing resources within a core.In this paper, we demonstrate through a real-system investigation that the fundamental difference between resource sharing behaviors on CMP and SMT architectures calls for a redesign of the way we model interference. For SMT servers, the interference on different shared resources, including private caches, memory ports, as well as integer and floating-point functional units, do not correlate with each other. This insight suggests the necessity of decoupling interference into multiple resource sharing dimensions. In this work, we propose SMiTe, a methodology that enables precise performance prediction for SMT co-location on real-system commodity processors. With a set of Rulers, which are carefully designed software stressors that apply pressure to a multidimensional space of shared resources, we quantify application sensitivity and contentiousness in a decoupled manner. We then establish a regression model to combine the sensitivity and contentiousness in different dimensions to predict performance interference. Using this methodology, we are able to precisely predict the performance interference in SMT co-location with an average error of 2.80\% on SPEC CPU2006 and 1.79\% on Cloud Suite. Our evaluation shows that SMiTe allows us to improve the utilization of WSCs by up to 42.57\% while enforcing an application's QoS requirements.},
booktitle = {Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {406–418},
numpages = {13},
keywords = {warehouse scale computer, datacenter, quality of service, simultaneous multithreading},
location = {Cambridge, United Kingdom},
series = {MICRO-47}
}

@inproceedings{10.1145/2631775.2631824,
author = {Kim, Suin and Weber, Ingmar and Wei, Li and Oh, Alice},
title = {Sociolinguistic Analysis of Twitter in Multilingual Societies},
year = {2014},
isbn = {9781450329545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631775.2631824},
doi = {10.1145/2631775.2631824},
abstract = {In a multilingual society, language not only reflects culture and heritage, but also has implications for social status and the degree of integration in society. Different languages can be a barrier between monolingual communities, and the dynamics of language choice could explain the prosperity or demise of local languages in an international setting. We study this interplay of language and network structure in diverse, multi-lingual societies, using Twitter. In our analysis, we are particularly interested in the role of bilinguals. Concretely, we attempt to quantify the degree to which users are the "bridge-builders" between monolingual language groups, while monolingual users cluster together. Also, with the revalidation of English as a lingua franca on Twitter, we reveal users of the native non-English language have higher influence than English users, and the language convergence pattern is consistent across the regions. Furthermore, we explore for which topics these users prefer their native language rather than English. To the best of our knowledge, this is the largest sociolinguistic study in a network setting.},
booktitle = {Proceedings of the 25th ACM Conference on Hypertext and Social Media},
pages = {243–248},
numpages = {6},
keywords = {multilingualism, social media, topic modeling, sociolinguistics},
location = {Santiago, Chile},
series = {HT '14}
}

@inproceedings{10.1145/2541940.2541946,
author = {Dall, Christoffer and Nieh, Jason},
title = {KVM/ARM: The Design and Implementation of the Linux ARM Hypervisor},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541946},
doi = {10.1145/2541940.2541946},
abstract = {As ARM CPUs become increasingly common in mobile devices and servers, there is a growing demand for providing the benefits of virtualization for ARM-based devices. We present our experiences building the Linux ARM hypervisor, KVM/ARM, the first full system ARM virtualization solution that can run unmodified guest operating systems on ARM multicore hardware. KVM/ARM introduces split-mode virtualization, allowing a hypervisor to split its execution across CPU modes and be integrated into the Linux kernel. This allows KVM/ARM to leverage existing Linux hardware support and functionality to simplify hypervisor development and maintainability while utilizing recent ARM hardware virtualization extensions to run virtual machines with comparable performance to native execution. KVM/ARM has been successfully merged into the mainline Linux kernel, ensuring that it will gain wide adoption as the virtualization platform of choice for ARM. We provide the first measurements on real hardware of a complete hypervisor using ARM hardware virtualization support. Our results demonstrate that KVM/ARM has modest virtualization performance and power costs, and can achieve lower performance and power costs compared to x86-based Linux virtualization on multicore hardware.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {333–348},
numpages = {16},
keywords = {multicore, virtualization, arm, operating systems, hypervisors, linux},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@article{10.1145/2644865.2541946,
author = {Dall, Christoffer and Nieh, Jason},
title = {KVM/ARM: The Design and Implementation of the Linux ARM Hypervisor},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2644865.2541946},
doi = {10.1145/2644865.2541946},
abstract = {As ARM CPUs become increasingly common in mobile devices and servers, there is a growing demand for providing the benefits of virtualization for ARM-based devices. We present our experiences building the Linux ARM hypervisor, KVM/ARM, the first full system ARM virtualization solution that can run unmodified guest operating systems on ARM multicore hardware. KVM/ARM introduces split-mode virtualization, allowing a hypervisor to split its execution across CPU modes and be integrated into the Linux kernel. This allows KVM/ARM to leverage existing Linux hardware support and functionality to simplify hypervisor development and maintainability while utilizing recent ARM hardware virtualization extensions to run virtual machines with comparable performance to native execution. KVM/ARM has been successfully merged into the mainline Linux kernel, ensuring that it will gain wide adoption as the virtualization platform of choice for ARM. We provide the first measurements on real hardware of a complete hypervisor using ARM hardware virtualization support. Our results demonstrate that KVM/ARM has modest virtualization performance and power costs, and can achieve lower performance and power costs compared to x86-based Linux virtualization on multicore hardware.},
journal = {SIGPLAN Not.},
month = {feb},
pages = {333–348},
numpages = {16},
keywords = {arm, multicore, hypervisors, linux, operating systems, virtualization}
}

@article{10.1145/2654822.2541946,
author = {Dall, Christoffer and Nieh, Jason},
title = {KVM/ARM: The Design and Implementation of the Linux ARM Hypervisor},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2654822.2541946},
doi = {10.1145/2654822.2541946},
abstract = {As ARM CPUs become increasingly common in mobile devices and servers, there is a growing demand for providing the benefits of virtualization for ARM-based devices. We present our experiences building the Linux ARM hypervisor, KVM/ARM, the first full system ARM virtualization solution that can run unmodified guest operating systems on ARM multicore hardware. KVM/ARM introduces split-mode virtualization, allowing a hypervisor to split its execution across CPU modes and be integrated into the Linux kernel. This allows KVM/ARM to leverage existing Linux hardware support and functionality to simplify hypervisor development and maintainability while utilizing recent ARM hardware virtualization extensions to run virtual machines with comparable performance to native execution. KVM/ARM has been successfully merged into the mainline Linux kernel, ensuring that it will gain wide adoption as the virtualization platform of choice for ARM. We provide the first measurements on real hardware of a complete hypervisor using ARM hardware virtualization support. Our results demonstrate that KVM/ARM has modest virtualization performance and power costs, and can achieve lower performance and power costs compared to x86-based Linux virtualization on multicore hardware.},
journal = {SIGARCH Comput. Archit. News},
month = {feb},
pages = {333–348},
numpages = {16},
keywords = {virtualization, linux, multicore, arm, operating systems, hypervisors}
}

@inproceedings{10.1145/3266157.3266208,
author = {Kempfle, Jochen and Van Laerhoven, Kristof},
title = {Respiration Rate Estimation with Depth Cameras: An Evaluation of Parameters},
year = {2018},
isbn = {9781450364874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266157.3266208},
doi = {10.1145/3266157.3266208},
abstract = {Depth cameras have been known to be capable of picking up the small changes in distance from users' torsos, to estimate respiration rate. Several studies have shown that under certain conditions, the respiration rate from a non-mobile user facing the camera can be accurately estimated from parts of the depth data. It is however to date not clear, what factors might hinder the application of this technology in any setting, what areas of the torso need to be observed, and how readings are affected for persons at larger distances from the RGB-D camera. In this paper, we present a benchmark dataset that consists of the point cloud data from a depth camera, which monitors 7 volunteers at variable distances, for variable methods to pin-point the person's torso, and at variable breathing rates. Our findings show that the respiration signal's signal-to-noise ratio becomes debilitating as the distance to the person approaches 4 metres, and that bigger windows over the person's chest work particularly well. The sampling rate of the depth camera was also found to impact the signal's quality significantly.},
booktitle = {Proceedings of the 5th International Workshop on Sensor-Based Activity Recognition and Interaction},
articleno = {4},
numpages = {10},
keywords = {Kinect v2, non-contact measurement, respiration measurement, respiratory rate, ToF sensing},
location = {Berlin, Germany},
series = {iWOAR '18}
}

@inproceedings{10.1145/3445814.3446699,
author = {Zha, Yue and Li, Jing},
title = {When Application-Specific ISA Meets FPGAs: A Multi-Layer Virtualization Framework for Heterogeneous Cloud FPGAs},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446699},
doi = {10.1145/3445814.3446699},
abstract = {While field-programmable gate arrays (FPGAs) have been widely deployed into cloud platforms, the high programming complexity and the inability to manage FPGA resources in an elastic/scalable manner largely limits the adoption of FPGA acceleration. Existing FPGA virtualization mechanisms partially address these limitations. Application-specific (AS) ISA provides a nice abstraction to enable a simple software programming flow that makes FPGA acceleration accessible by the mainstream software application developers. Nevertheless, existing AS ISA-based approaches can only manage FPGA resources at a per-device granularity, leading to a low resource utilization. Alternatively, hardware-specific (HS) abstraction improves the resource utilization by spatially sharing one FPGA among multiple applications. But it cannot reduce the programming complexity due to the lack of a high-level programming model. In this paper, we propose a virtualization mechanism for heterogeneous cloud FPGAs that combines AS ISA and HS abstraction to fully address aforementioned limitations. To efficiently combine these two abstractions, we provide a multi-layer virtualization framework with a new system abstraction as an indirection layer between them. This indirection layer hides the FPGA-specific resource constraints and leverages parallel pattern to effectively reduce the mapping complexity. It simplifies the mapping process into two steps, where the first step decomposes an AS ISA-based accelerator under no resource constraint to extract all fine-grained parallel patterns, and the second step leverages the extracted parallel patterns to simplify the process of mapping the decomposed accelerators onto the underlying HS abstraction. While system designers might be able to manually perform these steps for small accelerator designs, we develop a set of custom tools to automate this process and achieve a high mapping quality. By hiding FPGA-specific resource constraints, the proposed system abstraction provides a homogeneous view for the heterogeneous cloud FPGAs to simplify the runtime resource management. The extracted parallel patterns could also be leveraged by the runtime system to improve the performance of scale-out acceleration by maximally hiding the inter-FPGA communication latency. We use an AS ISA similar to the one proposed in BrainWave project and a recently proposed HS abstraction as a case study to demonstrate the effectiveness of the proposed virtualization framework. The performance is evaluated on a custom-built FPGA cluster with heterogeneous FPGA resources. Compared with the baseline system that only uses AS ISA, the proposed framework effectively combines these two abstractions and improves the aggregated system throughput by 2.54\texttimes{} with a marginal virtualization overhead.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {123–134},
numpages = {12},
keywords = {Virtualization, Parallel patterns, Application-specific ISA, Heterogeneous cloud FPGAs},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{10.1145/3269206.3271692,
author = {Wu, Xuan and Zhao, Lingxiao and Akoglu, Leman},
title = {A Quest for Structure: Jointly Learning the Graph Structure and Semi-Supervised Classification},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271692},
doi = {10.1145/3269206.3271692},
abstract = {Semi-supervised learning (SSL) is effectively used for numerous classification problems, thanks to its ability to make use of abundant unlabeled data. The main assumption of various SSL algorithms is that the nearby points on the data manifold are likely to share a label. Graph-based SSL constructs a graph from point-cloud data as an approximation to the underlying manifold, followed by label inference. It is no surprise that the quality of the constructed graph in capturing the essential structure of the data is critical to the accuracy of the subsequent inference step [6].How should one construct a graph from the input point-cloud data for graph-based SSL? In this work we introduce a new, parallel graph learning framework (called PG-learn) for the graph construction step of SSL. Our solution has two main ingredients: (1) a gradient-based optimization of the edge weights (more specifically, different kernel bandwidths in each dimension) based on a validation loss function, and (2) a parallel hyperparameter search algorithm with an adaptive resource allocation scheme. In essence, (1) allows us to search around a (random) initial hyperparameter configuration for a better one with lower validation loss. Since the search space of hyperparameters is huge for high-dimensional problems, (2) empowers our gradient-based search to go through as many different initial configurations as possible, where runs for relatively unpromising starting configurations are terminated early to allocate the time for others. As such, PG-learn is a carefully-designed hybrid of random and adaptive search. Through experiments on multi-class classification problems, we show that PG-learn significantly outperforms a variety of existing graph construction schemes in accuracy (per fixed time budget for hyperparameter tuning), and scales more effectively to high dimensional problems.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {87–96},
numpages = {10},
keywords = {hyperparamer optimization, hyperparamer inference, semi-supervised learning, graph learning, graph construction, parallel graph learning},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3011077.3011126,
author = {Van Sinh, Nguyen and Ha, Tran Manh and Thanh, Nguyen Tien},
title = {Filling Holes on the Surface of 3D Point Clouds Based on Tangent Plane of Hole Boundary Points},
year = {2016},
isbn = {9781450348157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011077.3011126},
doi = {10.1145/3011077.3011126},
abstract = {Filling the holes of a triangular mesh has been studied for many years in the field of geometric modeling. This research is one of the reconstructing steps of a triangular mesh (or called refinement of a mesh) in order to improve the quality of a 3D triangular surface. With the same idea of hole filling in a mesh, filling in the holes of a 3D point cloud is still a challenge to the researchers. This paper describes a method for filling holes in an elevation surface of 3D point clouds structured in a 3D grid. The novelty of the method is processed directly on the 3D point clouds consisting of two steps. In the first step, we determine the boundary of hole. In the second step, we fill the holes based on the computation of tangent plane for each boundary point. Following clock-wise direction on the hole boundary, we compute and insert missing points on each tangent plane. This process is repeated and refined ring by ring from the hole boundary to the inside of the hole. The obtained results show that the processing time of algorithm is very fast, the output surfaces preserve their initial shapes and local curvatures.},
booktitle = {Proceedings of the 7th Symposium on Information and Communication Technology},
pages = {331–338},
numpages = {8},
keywords = {hole filling, elevation surface, boundary point, tangent plane, 3D point cloud},
location = {Ho Chi Minh City, Vietnam},
series = {SoICT '16}
}

