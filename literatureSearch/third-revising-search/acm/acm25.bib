@inproceedings{10.1145/3412382.3458276,
author = {Klugman, Noah and Adkins, Joshua and Paszkiewicz, Emily and Hickman, Molly G. and Podolsky, Matthew and Taneja, Jay and Dutta, Prabal},
title = {Watching the Grid: Utility-Independent Measurements of Electricity Reliability in Accra, Ghana},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3458276},
doi = {10.1145/3412382.3458276},
abstract = {In much of the world, electricity grids are not instrumented at the customer level, limiting insights into the power quality experienced by utility customers. Moreover, to understand grid performance, regulators and investors must depend on utilities to self-report reliability data. To address these challenges, we introduce PowerWatch, an agile methodology to directly measure customer experience and aggregated grid performance without relying on the utility for deployment or management. PowerWatch employs a system of distributed sensors coupled with cloud-based analytics. We evaluate the PowerWatch methodology by deploying 462 sensors in homes and businesses in Accra, Ghana for over a year, yielding the largest open-source data set on electricity reliability at the customer-level in the region. We describe the architecture, design, and performance of PowerWatch, as well as the data that are collected, explaining how we determine the accuracy and coverage of our methodology without ground truth. Finally, we report on grid performance issues, finding nearly twice as many outages as the utility observed, suggesting a need for better grid performance monitoring.},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {341–356},
numpages = {16},
keywords = {Sensor deployments, Grid reliability metering},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@inproceedings{10.1145/3395027.3419595,
author = {Ughetta, William and Kernighan, Brian W.},
title = {The Old Bailey and OCR: Benchmarking AWS, Azure, and GCP with 180,000 Page Images},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419595},
doi = {10.1145/3395027.3419595},
abstract = {The Proceedings of the Old Bailey is a corpus of over 180,000 page images of court records printed from April 1674 to April 1913 and presents a comprehensive challenge for Optical Character Recognition (OCR) services. The Old Bailey is an ideal benchmark for historical document OCR, representing more than two centuries of variations in documents, including spellings, formats, and printing and preservation qualities. In addition to its historical and sociological significance, the Old Bailey is filled with imperfections that reflect the reality of coping with large-scale historical data. Most importantly, the Old Bailey contains human transcriptions for each page, which can be used to help measure OCR accuracy. Since humans do make mistakes in transcriptions, the relative performance of OCR services will be more informative than their absolute performance. This paper compares three leading commercial OCR cloud services: Amazon Web Services's Textract (AWS); Microsoft Azure's Cognitive Services (Azure); and Google Cloud Platform's Vision (GCP). Benchmarking involved downloading over 180,000 images, executing the OCR, and measuring the error rate of the OCR text against the human transcriptions. Our results found that AWS had the lowest median error rate, Azure had the lowest median round trip time, and GCP had the best combination of a low error rate and a low duration.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {19},
numpages = {4},
keywords = {Google Cloud Platform, Optical Character Recognition, Old Bailey, Amazon Web Services, Microsoft Azure, Historical Documents},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@inproceedings{10.1145/2676662.2676677,
author = {Patiniotakis, Ioannis and Verginadis, Yiannis and Mentzas, Gregoris},
title = {Preference-Based Cloud Service Recommendation as a Brokerage Service},
year = {2014},
isbn = {9781450332330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676662.2676677},
doi = {10.1145/2676662.2676677},
abstract = {As the multitude and complexity of cloud services increases, the role of cloud brokers in the cloud service ecosystems becomes increasingly important. In particular, the lack of standard mechanisms that allow for the comparison of cloud service specifications against user requirements taking into account the implicit uncertainty and vagueness is a major hindrance during the cloud service evaluation and selection. In this paper, we discuss the Preference-based cLoud Service Recommender (PuLSaR) that uses a holistic multi-criteria decision making (MCDM) approach for offering optimisation as brokerage service. The specification and implementation details of this dedicated software are thoroughly discussed while the background method used is summarised. Both method and brokerage service allow for the multi-objective assessment of cloud services in a unified way, taking into account precise and imprecise metrics and dealing with their fuzziness.},
booktitle = {Proceedings of the 2nd International Workshop on CrossCloud Systems},
articleno = {5},
numpages = {6},
keywords = {cloud service broker, service ranking, MCDM, optimisation},
location = {Bordeaux, France},
series = {CCB '14}
}

@inproceedings{10.1145/3386293.3397116,
author = {Sabet, Saeed Shafiee and Schmidt, Steven and Zadtootaghaj, Saman and Griwodz, Carsten and M\"{o}ller, Sebastian},
title = {Delay Sensitivity Classification of Cloud Gaming Content},
year = {2020},
isbn = {9781450379472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386293.3397116},
doi = {10.1145/3386293.3397116},
abstract = {Cloud Gaming is an emerging service that catches growing interest in the research community as well as industry. Cloud Gaming require a highly reliable and low latency network to achieve a satisfying Quality of Experience (QoE) for its users. Using a cloud gaming service with high latency would harm the interaction of the user with the game, leading to a decrease in playing performance and, thus players frustrations. However, the negative effect of delay on gaming QoE depends strongly on the game content. At a certain level of delay, a slow-paced card game is typically not as delay sensitive as a shooting game. For optimal resource allocation and quality estimation, it is highly important for cloud providers, game developers, and network planners to consider the impact of the game content. This paper contributes to a better understanding of the delay impact on QoE for cloud gaming applications by identifying game characteristics influencing the delay perception of the users. In addition, an expert evaluation methodology to quantify these characteristics as well as a delay sensitivity classification based on a decision tree are presented. The results indicated an excellent level of agreement, which demonstrates the reliability of the proposed method. Additionally, the decision tree reached an accuracy of 90\% on determining the delay sensitivity classes which were derived from a large dataset of subjective input quality ratings during a series of experiments.},
booktitle = {Proceedings of the 12th ACM International Workshop on Immersive Mixed and Virtual Environment Systems},
pages = {25–30},
numpages = {6},
keywords = {cloud gaming, delay, QoE, content classification},
location = {Istanbul, Turkey},
series = {MMVE '20}
}

@inproceedings{10.1145/2987550.2987584,
author = {Cano, Ignacio and Aiyar, Srinivas and Krishnamurthy, Arvind},
title = {Characterizing Private Clouds: A Large-Scale Empirical Analysis of Enterprise Clusters},
year = {2016},
isbn = {9781450345255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987550.2987584},
doi = {10.1145/2987550.2987584},
abstract = {There is an increasing trend in the use of on-premise clusters within companies. Security, regulatory constraints, and enhanced service quality push organizations to work in these so called private cloud environments. On the other hand, the deployment of private enterprise clusters requires careful consideration of what will be necessary or may happen in the future, both in terms of compute demands and failures, as they lack the public cloud's flexibility to immediately provision new nodes in case of demand spikes or node failures.In order to better understand the challenges and tradeoffs of operating in private settings, we perform, to the best of our knowledge, the first extensive characterization of on-premise clusters. Specifically, we analyze data ranging from hardware failures to typical compute/storage requirements and workload profiles, from a large number of Nutanix clusters deployed at various companies.We show that private cloud hardware failure rates are lower, and that load/demand needs are more predictable than in other settings. Finally, we demonstrate the value of the measurements by using them to provide an analytical model for computing durability in private clouds, as well as a machine learning-driven approach for characterizing private clouds' growth.},
booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
pages = {29–41},
numpages = {13},
keywords = {Reliability, Performance, Private clouds, Measurements},
location = {Santa Clara, CA, USA},
series = {SoCC '16}
}

@inproceedings{10.1145/3291064.3291070,
author = {Thirunavukkarasu, Gokul Sidarth and Champion, Benjamin and Horan, Ben and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
title = {IoT-Based System Health Management Infrastructure as a Service},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291070},
doi = {10.1145/3291064.3291070},
abstract = {Customization, enhanced quality of streamlined maintenance services and uplifted productivity are some of the key highlights from the rapidly evolving concept of Industry 4.0. IoT (Internet of things) based service infrastructure models designed for delivering enterprise services with capabilities of pro-actively sensing malfunctions and responding with preventive measures to streamline the automated service offered is one of the prime application of this concept. Continuous maintenance services increase the optimum through-life cost and in-service life cycle of the product providing the customer with the feel of full ownership. In-service feedbacks also help the manufactures to identify issues with respect to the designs and improve it in the future versions. In this paper, as a proof of concept a cloud-based IoT service infrastructure for providing real-time prognostic and supervised vehicle maintenance system is proposed. This proposed system aims at providing an enterprise service infrastructure to the registered vehicle service centers to keep track of the real-time vehicle diagnostic information of their client's vehicle over cloud and use prognostic algorithms to identify any malfunctions or abnormal behavior of the vehicles for automatically scheduling a service appointment and automating the maintenance cycle of the vehicle. In addition to this, the system provides features like remote supervision and diagnostics maintenance enabling technicians to fix issues remotely, ensuring streamlined and reliable service. Initially, before building the proposed prototype system, a few experimental trails where conducted for analyzing the use of different IoT models used in the development to identify the best-suited approach. The results indicated that the publisher-subscriber (NodeJS) based model outperforms the request-response (PHP) based model in terms of the hits per second and mean request time for an increased number of active users. The results of the initial tests justify the reason for the using the publisher-subscriber based IOT architecture. The conceptualized enterprise infrastructure illustrated in the manuscript aims at providing a streamlined maintenance service.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {55–61},
numpages = {7},
keywords = {streamlined remote supervision, internet of things, System health management infrastructure as a service, prognostic maintenance, vehicle diagnosis},
location = {Singapore, Singapore},
series = {CCIOT '18}
}

@inproceedings{10.1145/3293320.3293326,
author = {Kaliszan, Damian and F\"{u}rst, Steffen and Gienger, Michael and Gogolenko, Sergiy and Meyer, Norbert and Petruczynik, Sebastian},
title = {Comparative Benchmarking of HPC Systems for GSS Applications: GSS Applications in the HPC Ecosystem},
year = {2019},
isbn = {9781450366328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293320.3293326},
doi = {10.1145/3293320.3293326},
abstract = {The work undertaken in this paper was done in the Centre of Excellence for Global Systems Science (CoeGSS), an interdisciplinary project, funded by the European Commission.The project provides decision-support in the face of global challenges. It brings together HPC and global systems science. This paper presents a proposition of GSS benchmark with the aim to find the most suitable HPC architecture and the best HPC system which allows to run GSS applications effectively. The GSS provides evidence about global systems challenges, e.g. the network structure of the world economy, energy, water and food supply systems, the global financial system or the global city system, and the scientific community.The outcome of the analysis is defining a benchmark which represents the GSS environment in the best way. Three exemplary challenges were defined as pilot applications: Health Habits, Green Growth and Global Urbanisation extended with additional applications from GSS ecosystem: Iterative proportional fitting (IPF), Data rastering - a preprocessing process converting all vectorial representations of georeferenced data into raster files to be later used as simulation input, Weather Research and Forecasting (WRF) model, CMAQ/CCTM (Community Air Multiscale Quality Modelling System/The CMAQ Chemistry-Transport Mode), CM1 (Cloud Modelling), ABMS (Agent-based Modelling and Simulation), OpenSWPC (An Open-source Seismic Wave Propagation Code). The above list seems to be quite rich and reflects the real GSS world as much as possible, having in mind, for example the real-world applications availability.Additionally, the authors tested new HPC platforms based on Intel® Xeon® Gold 6140, AMD EpycTM, ARM Hi1616 and IBM Power8+. Due to the hardware availability, the testbed consisted of a limited number of nodes. This restricted the ability to provide full tests of scalability for given applications. However, this small number of available computational units (cores) can provide valuable outcome including architecture comparison for different applications based on execution times, TDPs1 and TCO2. These are the basic metrics used for providing a ranking of HPC architectures. Finally, this document is thought to be valuable information for the GSS community for future purposes and analysis to determine their specific demands as well as - in general - to help develop a mature final benchmark set reflecting the GSS environment requirements and specialty. As none of the existing benchmarks is dedicated to the GSS community, the authors decided to create one by calling it a GSS benchmark to serve and help GSS users in their future work.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {43–52},
numpages = {10},
keywords = {e-Infrastructure evaluation, Global Systems Science, HPC benchmarks, parallel applications},
location = {<conf-loc>, <city>Guangzhou</city>, <country>China</country>, </conf-loc>},
series = {HPCAsia '19}
}

@inproceedings{10.1109/UCC.2014.168,
author = {Almanea, Mohammed Ibrahim M.},
title = {Cloud Advisor - A Framework towards Assessing the Trustworthiness and Transparency of Cloud Providers},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.168},
doi = {10.1109/UCC.2014.168},
abstract = {We propose a Cloud Advisor framework that couples two salient features: trustworthiness and transparency measurement. It provides a mechanism to measure trustworthiness based on the history of the cloud provider taking into account evidence support and to measure transparency based on the Cloud Controls Matrix (CCM) framework. The selection process is based on a set of assurance requirements that if are met by the cloud provider or if it has been considered in a tool it could bring assurance and confidence to cloud customers.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {1018–1019},
numpages = {2},
keywords = {framework, cloud computing, trustworthiness, measurement, cloud providers, transparency, assurance requirements},
series = {UCC '14}
}

@inproceedings{10.1145/2594449.2579468,
author = {Huang, Chun-Ying and Hsu, Cheng-Hsin and Chen, De-Yu and Chen, Kuan-Ta},
title = {Quantifying User Satisfaction in Mobile Cloud Games},
year = {2014},
isbn = {9781450327077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594449.2579468},
doi = {10.1145/2594449.2579468},
abstract = {We conduct real experiments to quantify user satisfaction in mobile cloud games using a real cloud gaming system built on the open-sourced GamingAnywhere. We share our experiences in porting GamingAnywhere client to Android OS and perform extensive experiments on both the mobile and desktop clients. The experiment results reveal several new insights: (1) gamers are more satisfied with the graphics quality on mobile devices, while they are more satisfied with the control quality on desktops, (2) the bitrate, frame rate, and network delay significantly affect the graphics and smoothness quality, and (3) the control quality only depends on the client type (mobile versus desktop). To the best of our knowledge, such user studies have never been done in the literature.},
booktitle = {Proceedings of Workshop on Mobile Video Delivery},
articleno = {4},
numpages = {6},
keywords = {Cloud games, user studies, performance evaluation, mobile games},
location = {Singapore, Singapore},
series = {MoViD'14}
}

@inproceedings{10.1145/3240508.3240642,
author = {Pang, Haitian and Zhang, Cong and Wang, Fangxin and Hu, Han and Wang, Zhi and Liu, Jiangchuan and Sun, Lifeng},
title = {Optimizing Personalized Interaction Experience in Crowd-Interactive Livecast: A Cloud-Edge Approach},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240642},
doi = {10.1145/3240508.3240642},
abstract = {Enabling users to interact with broadcasters and audience, the crowd-interactive livecast greatly improves viewer's quality of experience (QoE) and attracts millions of daily active users recently. In addition to striking the balance between resource utilization and viewers' QoE met in the traditional video streaming service, this novel service needs to take supererogatory efforts to improve the interaction QoE, which reflects the viewer interaction experience. To tackle this issue, we conduct measurement studies over a large-scale dataset crawled from a representative livecast service provider. We observe that the individual's interaction pattern is quite heterogeneous: only 10\% viewers proactively participate in the interaction, and the rest viewers usually watch passively. Incorporating the insight into the emerging cloud-edge architecture, we propose a framework PIECE, which optimizes the Personalized Interaction Experience with Cloud-Edge architecture (PIECE) for intelligent user access control and livecast distribution. In particular, we first devise a novel deep neural network based algorithm to predict users' interaction intensity using the historical viewer pattern. We then design an algorithm to maximize the individual's QoE, by strategically matching viewer sessions and transcoding-delivery paths over cloud-edge infrastructure. Finally, we use trace-driven experiments to verify the effectiveness of PIECE. Our results show that our prediction algorithm outperforms the state-of-the-art algorithms with a much smaller mean absolute error (40\% reduction). Furthermore, in comparison with the cloud-based video delivery strategy, the proposed framework can simultaneously improve the average viewers QoE (26\% improvement) and interaction QoE (21\% improvement), while maintaining a high streaming bitrate.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1217–1225},
numpages = {9},
keywords = {cloud-edge, interactive live streaming, viewer interaction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/2579465.2579468,
author = {Huang, Chun-Ying and Hsu, Cheng-Hsin and Chen, De-Yu and Chen, Kuan-Ta},
title = {Quantifying User Satisfaction in Mobile Cloud Games},
year = {2018},
isbn = {9781450327077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2579465.2579468},
doi = {10.1145/2579465.2579468},
abstract = {We conduct real experiments to quantify user satisfaction in mobile cloud games using a real cloud gaming system built on the open-sourced GamingAnywhere. We share our experiences in porting GamingAnywhere client to Android OS and perform extensive experiments on both the mobile and desktop clients. The experiment results reveal several new insights: (1) gamers are more satisfied with the graphics quality on mobile devices, while they are more satisfied with the control quality on desktops, (2) the bitrate, frame rate, and network delay significantly affect the graphics and smoothness quality, and (3) the control quality only depends on the client type (mobile versus desktop). To the best of our knowledge, such user studies have never been done in the literature.},
booktitle = {Proceedings of Workshop on Mobile Video Delivery},
pages = {1–6},
numpages = {6},
keywords = {mobile games, performance evaluation, user studies, Cloud games},
location = {Singapore, Singapore},
series = {MoViD'14}
}

@inproceedings{10.1145/3462203.3475873,
author = {Agossou, B. Emmanuel and Toshiro, Takahara},
title = {IoT \&amp; AI Based System for Fish Farming: Case Study of Benin},
year = {2021},
isbn = {9781450384780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462203.3475873},
doi = {10.1145/3462203.3475873},
abstract = {Agriculture including aquaculture has been changing through multiple technological transformations in recent years. The Internet of Things (IoT) and Artificial Intelligence (AI) are providing remarkable technological innovations on fish farming. In this research, we present an automated IoT and AI-based system to improve fish farming. The proposed system uses multiple sensors to measure in real-time water quality chemical parameters such as: temperature, pH, turbidity, electrical conductivity, total dissolved solids, etc., from the fish pond and send them on a cloud database to allow fish farmers to access them in realtime with their devices (mobile phone, PC, tablets). The system contains three web applications which fish farmers can use. The first web application enables farmers with realtime visualizations of sensors data, issues alerts and remote pumps controls. Fish farmers can use the second web application for fish disease detection and to receive suggestions for diseases' care. This would help to classify two fish diseases which are: Epizootic Ulcerative Syndrome(EUS), and Ichthyophthirus(Ich). The third web application is a digital community platform for knowledge sharing, capacity building, market opportunities and collaboration among fish farmers. Our system can help reduce human efforts, reinforce capacity building, increase fish production and market opportunities for fish farmers.},
booktitle = {Proceedings of the Conference on Information Technology for Social Good},
pages = {259–264},
numpages = {6},
keywords = {eFish Farm, Convolutional Neural Network, IoT, Smart Fish Farming, Arduino, ESP32, MQTT, AI},
location = {Roma, Italy},
series = {GoodIT '21}
}

@article{10.1109/TCBB.2016.2566617,
author = {Zhang, Gui-Jun and Zhou, Xiao-Gen and Yu, Xu-Feng and Hao, Xiao-Hu and Yu, Li},
title = {Enhancing Protein Conformational Space Sampling Using Distance Profile-Guided Differential Evolution},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2566617},
doi = {10.1109/TCBB.2016.2566617},
abstract = {De novo protein structure prediction aims to search for low-energy conformations as it follows the thermodynamics hypothesis that places native conformations at the global minimum of the protein energy surface. However, the native conformation is not necessarily located in the lowest-energy regions owing to the inaccuracies of the energy model. This study presents a differential evolution algorithm using distance profile-based selection strategy to sample conformations with reasonable structure effectively. In the proposed algorithm, besides energy, the residue-residue distance is considered another measure of the conformation. The average distance errors of decoys between the distance of each residue pair and the corresponding distance in the distance profiles are first calculated when the trial conformation yields a larger energy value than that of the target. Then, the distance acceptance probability of the trial conformation is designed based on distance profiles if the trial conformation obtains a lower average distance error compared with that of the target conformation. The trial conformation is accepted to the next generation in accordance with its distance acceptance probability. By using the dual constraints of energy and distance in guiding sampling, the algorithm can sample conformations with lower energies and more reasonable structures. Experimental results of 28 benchmark proteins show that the proposed algorithm can effectively predict near-native protein structures.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {nov},
pages = {1288–1301},
numpages = {14}
}

@article{10.14778/2994509.2994527,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {On Measuring the Lattice of Commonalities among Several Linked Datasets},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994527},
doi = {10.14778/2994509.2994527},
abstract = {A big number of datasets has been published according to the principles of Linked Data and this number keeps increasing. Although the ultimate objective is linking and integration, it is not currently evident how connected the current LOD cloud is. Measurements (and indexes) that involve more than two datasets are not available although they are important: (a) for obtaining complete information about one particular URI (or set of URIs) with provenance (b) for aiding dataset discovery and selection, (c) for assessing the connectivity between any set of datasets for quality checking and for monitoring their evolution over time, (d) for constructing visualizations that provide more informative overviews. Since it would be prohibitively expensive to perform all these measurements in a na\"{\i}ve way, in this paper we introduce indexes (and their construction algorithms) that can speedup such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and finally (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. We discuss the speedup obtained by the introduced indexes and algorithms through comparative results and finally we report measurements about connectivity of the LOD cloud that have never been carried out so far.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1101–1112},
numpages = {12}
}

@inproceedings{10.1145/2980258.2980451,
author = {Bhattacharya, Adrija and Choudhury, Sankhayan},
title = {An Efficient Service Selection Approach through a Goodness Measure of the Participating QoS},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2980451},
doi = {10.1145/2980258.2980451},
abstract = {The service repository in cloud consists of atomic services those need to be composed as per the requirement of consumers. In general, various providers offer different atomic services with same functionalities. These are called similar services and the service selection is the process to choose the best one among them based on the associated Quality of Services. Thus a service selection problem for satisfying the requirement of a consumer with given constraints is conceptualized as a multi-objective optimization problem. Sometime it involves the objectives that have conflict among them and as a result the complexity of the problem increases. In such cases users are requested to provide the feedback on the required QoS and accordingly the solution is offered. This demands sufficient domain knowledge from a user that may not be feasible in real cases. As a result the offered solution may deviate from the intended one. In this work we have proposed a method to calculate an overall measure of a service considering all QoS. It converts the multi-objective problem to single objective. This reduces the exponential complexity of NP-Hard problem into a problem solvable in polynomial time. The proposed Service Selection algorithm does not require any feedback from the users. The algorithm is capable to offer a moderate solution to users considering all requested QoS. The experiment shows that almost in every case the proposed algorithm is able to deliver a solution satisfying all QoS as referred by a user.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {94},
numpages = {6},
keywords = {Service Selection, Goodness, QoS},
location = {Pondicherry, India},
series = {ICIA-16}
}

@inproceedings{10.5555/3233397.3233459,
author = {Connor, Thomas Richard and Southgate, Joel},
title = {Automated Cloud Brokerage Based upon Continuous Real-Time Benchmarking},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {Over the last few years there has been a massive proliferation of cloud providers, all using a set of different metrics to describe the service solutions that they offer. This results in a lack of comparability within and between services that precludes end users being able to select the most appropriate service for their needs, based upon their requirements. Here we outline an automated real-time benchmarking platform that can interact with cloud brokers to automatically select the most optimal cloud service provider for a given workload, based upon up to the minute benchmarking results generated, stored, collated and compared by the platform itself. This software package could save end users and enterprises significant amounts of time and money by ensuring that they always use the most appropriate VM flavor, on the most appropriate cloud service, every time they run a workload.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {372–375},
numpages = {4},
keywords = {virtualisation, cloud brokerage, cloud benchmarking},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1145/2683405.2683409,
author = {Nguyen, Hoang Minh and W\"{u}nsche, Burkhard and Delmas, Patrice and Lutteroth, Christof},
title = {Identifying Low Confidence Mesh Regions: Uncertainty Measures and Segmentation},
year = {2014},
isbn = {9781450331845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2683405.2683409},
doi = {10.1145/2683405.2683409},
abstract = {3D digital models have become an important part of diverse applications ranging from computer games, virtual reality, architectural design to visual impact studies. One common method to create 3D models is to create a point cloud using laser scanners, structured lighting sensors, or image-based modelling techniques, and then construct a 3D mesh, and texture-map it using photographs of the observed scene. Attributed to the inherent properties of general 3D scenes such as occluded or inaccessible parts, reflective surfaces, lighting conditions or poor-quality inputs, 3D models produced by these approaches often exhibit unsatisfactory and erroneous mesh regions. In many cases, it is desirable to identify and extract such regions so that they can be constructed or corrected through other means. While much effort has been invested into the problem of 3D reconstructions, the task of evaluating existing models and preparing them for subsequent enhancement processes has been largely neglected. In this paper, we present a novel method for automatically detecting and segmenting mesh regions with low confidence in their correctness. The confidence estimation is achieved by exploiting and integrating various uncertainty measures such as geometric distances, normal variations and texture discrepancies. Low-confidence mesh regions are isolated and removed in such a way that the extracted region's boundary is as simple as possible in order to facilitate subsequent automatic or manual improvement of these regions. Segmentation is achieved by minimising an energy function that takes the genus and boundary length and smoothness of the extracted regions into account.},
booktitle = {Proceedings of the 29th International Conference on Image and Vision Computing New Zealand},
pages = {48–53},
numpages = {6},
keywords = {Uncertainty Measure, 3D Reconstruction, Mesh Classification},
location = {Hamilton, New Zealand},
series = {IVCNZ '14}
}

@article{10.1145/3165713,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3165713},
doi = {10.1145/3165713},
abstract = {Although the ultimate objective of Linked Data is linking and integration, it is not currently evident how connected the current Linked Open Data (LOD) cloud is. In this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference, i.e., for obtaining complete information about a set of entities, including provenance information; (c) Data Quality Assessment and Improvement, i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d) Dataset Visualizations; and various other tasks. Since it would be prohibitively expensive to perform all these measurements in a na\"{\i}ve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. For enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. Finally, we report measurements about the connectivity of the (billion triples-sized) LOD cloud that have never been carried out so far.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {15},
numpages = {49},
keywords = {connectivity, dataset discovery, Data quality, spark, big data, linked data, dataset selection, mapreduce, lattice of measurements}
}

@article{10.1145/3089262.3089268,
author = {Hossfeld, Tobias},
title = {2016 International Teletraffic Congress (ITC 28) Report},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/3089262.3089268},
doi = {10.1145/3089262.3089268},
abstract = {The 28th International Teletraffic Congress (ITC 28) was held on 12--16 September 2016 at the University of W"urzburg, Germany. The conference was technically cosponsored by the IEEE Communications Society and the Information Technology Society within VDE, and in cooperation with ACM SIGCOMM. ITC 28 provided a forum for leading researchers from academia and industry to present and discuss the latest advances and developments in design, modelling, measurement, and performance evaluation of communication systems, networks, and services. The main theme of ITC 28, emph{Digital Connected World}, reflects the evolution of communications and networking, which is continually changing the world we are living in. The technical program was composed of 37 contributed full papers, 6 short demo papers and three keynote addresses. Three workshops dedicated to timely topics were sponsored: Programmability for Cloud Networks and Applications, Quality of Experience Centric Management, Quality Engineering for a Reliable Internet of Services.See ITC 28 Homepage: url{https://itc28.org/}},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {30–35},
numpages = {6},
keywords = {Clouds and Data Center, Information Centric Networks, Video Streaming, Softwarization, Virtualization, Traffic and Network Management, Wireless and Cellular, Caching, Measurements, Performance Analysis and Modeling}
}

@inproceedings{10.1145/2695664.2695814,
author = {Kunde, Shruti and Mukherjee, Tridib},
title = {Workload Characterization Model for Optimal Resource Allocation in Cloud Middleware},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695814},
doi = {10.1145/2695664.2695814},
abstract = {With increasing focus on inter-operability across cloud offerings to leverage their disparate capabilities, it has become more and more important to enable a flexible framework for sharing of heterogeneous resources in the cloud infrastructure. At the same time, it is imperative to be aware of the performance implications of hosting application workloads on different resources in order to guarantee Service Level Agreements (SLAs) to the applications. This paper focusses on experimental characterization of performance implications of different heterogeneous resources in hosting big-data analytics application workloads (one of the most critical applications in modern times). To create the knowledge, based on which the recommendations are provided, we benchmark the performance of big-data analytics applications, using a Hadoop cluster setup. Specifically, we study parameters of interest such as turnaround time and throughput, which are most likely to influence our choice of infrastructure for a particular application. Our experiments are conducted on varied platforms, both internal to Xerox and external cloud providers. We present a model based on our experiments, that facilitates the characterization of hetergeneous applications, thus enabling the cloud middleware to select an appropriate infrastructure and metrics in order to attain the desired SLA.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {442–447},
numpages = {6},
keywords = {resource sharing, cloud middleware},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3383812.3383823,
author = {Vani, K. Suvarna and M., Arul Raj and M., Padmaja and Kumar, K. Praveen and A., Jitendra and A., Ravi Raja},
title = {Detection and Extraction of Roads Using Cartosat-2 High Resolution Satellite Imagery},
year = {2020},
isbn = {9781450377201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383812.3383823},
doi = {10.1145/3383812.3383823},
abstract = {The extraction of roads from panchromatic images has many insightful applications in the fields of urban planning, setting up of transportation, disaster management and cartography in geographical information systems (GIS). The process of extracting roads from high-resolution images is composite, due to the presence of different noises (i.e., buildings, shadows, clouds etc.). Various image processing techniques and various quality measures are applied on the high-resolution remote sensing satellite images to improve the quality of the image and interactively extract the information of roads. Cartosat-2 images available in Bhuvan website of ISRO are taken for testing the validity of proposed method. The proposed method enhances the images using Contrast Limited Adaptive Histogram Equalization (CLAHE) and Line Detector for detecting road segments. Connected component analysis (CCA) is performed on segmented image for connection of disconnected objects in the segmented image. Morphological operations fill the holes caused by the presence of shadows, buildings and trees on the road surface.},
booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
pages = {7–11},
numpages = {5},
keywords = {morphological operations, line segment detector, cartosat-2 dataset, image processing, GIS},
location = {Singapore, Singapore},
series = {ICIGP '20}
}

@inproceedings{10.1109/CCGRID.2017.12,
author = {Davatz, Christian and Inzinger, Christian and Scheuner, Joel and Leitner, Philipp},
title = {An Approach and Case Study of Cloud Instance Type Selection for Multi-Tier Web Applications},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.12},
doi = {10.1109/CCGRID.2017.12},
abstract = {A challenging problem for users of Infrastructure-as-a-Service (IaaS) clouds is selecting cloud providers, regions, and instance types cost-optimally for a given desired service level. Issues such as hardware heterogeneity, contention, and virtual machine (VM) placement can result in considerably differing performance across supposedly equivalent cloud resources. Existing research on cloud benchmarking helps, but often the focus is on providing low-level microbenchmarks (e.g., CPU or network speed), which are hard to map to concrete business metrics of enterprise cloud applications, such as request throughput of a multi-tier Web application. In this paper, we propose Okta, a general approach for fairly and comprehensively benchmarking the performance and cost of a multi-tier Web application hosted in an IaaS cloud. We exemplify our approach for a case study based on the two-tier AcmeAir application, which we evaluate for 11 real-life deployment configurations on Amazon EC2 and Google Compute Engine. Our results show that for this application, choosing compute-optimized instance types in the Web layer and small bursting instances for the database tier leads to the overall most cost-effective deployments. This result held true for both cloud providers. The least cost-effective configuration in our study provides only about 67\% of throughput per US dollar spent. Our case study can serve as a blueprint for future industrial or academic application benchmarking projects.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {534–543},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/2642687.2642704,
author = {Palomares, Daniel and Migault, Daniel and Hendrik, Hendrik and Laurent, Maryline and Pujolle, Guy},
title = {Elastic Virtual Private Cloud},
year = {2014},
isbn = {9781450330275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642687.2642704},
doi = {10.1145/2642687.2642704},
abstract = {Several Virtual Private Networks are based on IPsec. However, IPsec has not been designed with elasticity in mind, which makes clusters of IPsec security gateways hard to manage for providing high Service Level Agreement (SLA). Thus, these SG clusters need management techniques to maintain their Quality of Service. For example, ISPs use VPNs to secure millions of communications when offloading End-Users from Radio Access Networks towards alternative access networks such as WLANs. Additionally, Virtual Private Cloud (VPC) providers also handle thousands of VPN connections when remote EUs access private clouds services. This paper describes how to provide Traffic Management (TM) and High Availability (HA) for VPN infrastructures by sharing or transferring an IPsec session. TM and HA have been implemented and evaluated over a 2-nodes cluster. We measured their impact on a real time audio streaming simulating a phone conversation. We found out that over a 2 minutes conversation, the impact on QoS measured with POLQA while applying TM or HA, is less than 3\%.},
booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {127–131},
numpages = {5},
keywords = {IKEV2, context transfer, high availability, QoS, virtual private cloud, VPN management, IPSEC, POLQA},
location = {Montreal, QC, Canada},
series = {Q2SWinet '14}
}

@inproceedings{10.1145/3448891.3448918,
author = {Du, Yifan and Sailhan, Fran\c{c}oise and Issarny, Val\'{e}rie},
title = {IAM&nbsp;– Interpolation and Aggregation on the Move: Collaborative Crowdsensing for Spatio-Temporal Phenomena},
year = {2021},
isbn = {9781450388405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448891.3448918},
doi = {10.1145/3448891.3448918},
abstract = {Crowdsensing allows citizens to contribute to the monitoring of their living environment using the sensors embedded in their mobile devices, e.g., smartphones. However, crowdsensing at scale involves significant communication, computation, and financial costs due to the dependence on cloud infrastructures for the analysis (e.g., interpolation and aggregation) of spatio-temporal data. This limits the adoption of crowdsensing by activists although sorely needed to inform our knowledge of the environment. As an alternative to the centralized analysis of crowdsensed observations, this paper introduces a fully distributed interpolation-mediated aggregation approach running on smartphones. To achieve so efficiently, we model the interpolation as a distributed tensor completion problem, and we introduce a lightweight aggregation strategy that anticipates the likelihood of future encounters according to the quality of the interpolation. Our approach thus shifts the centralized post-processing of crowdsensed data to distributed pre-processing on the move, based on opportunistic encounters of crowdsensors through state-of-the-art D2D networking. The evaluation using a dataset of quantitative environmental measurements collected from 550 crowdsensors over 1 year shows that our solution significantly reduces –and may even eliminate– the dependence on the cloud infrastructure, while it incurs a limited resource cost on end devices. Meanwhile, the overall data accuracy remains comparable to that of the centralized approach.},
booktitle = {MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {337–346},
numpages = {10},
keywords = {Opportunistic Relay, Aggregation, Ubiquitous Sensing, Interpolation, Pervasive Computing, Crowdsensing},
location = {Darmstadt, Germany},
series = {MobiQuitous '20}
}

@inproceedings{10.1145/2602576.2602580,
author = {Chavarriaga, Jaime and Noguera, Carlos A. and Casallas, Rubby and Jonckers, Viviane},
title = {Architectural Tactics Support in Cloud Computing Providers: The Jelastic Case},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602580},
doi = {10.1145/2602576.2602580},
abstract = {When developing and deploying applications in the cloud, architects face the challenge of conciliating architectural decisions with the options and restrictions imposed by the chosen cloud provider. An architectural decision can be seen as a two-step process: selecting architectural tactics to promote quality attributes and choosing design alternatives to implement those tactics. Available design alternatives are limited by the offer of the cloud provider. When configuring the cloud platform and its services as directed by the chosen tactics, the architect must be mindful of conflicts among the available alternatives. These trade-offs amongst the desired quality attributes can be difficult to detect, understand and ultimately solve. In this paper, we consider the case of Jelastic, a particular cloud platform provider, to illustrate: 1) the modeling of architectural tactics and their corresponding design alternatives using cloud configuration options, and 2) a process that exploits these models to determine which options to use in order to implement a combination of tactics. Furthermore, we present an analysis for this cloud provider that explains which combinations of tactics and configurations lead to trade-offs.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {13–22},
numpages = {10},
keywords = {architectural tactics, feature model, cloud computing, quality attributes},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/2729094.2742618,
author = {Pal, Yogendra and Iyer, Sridhar},
title = {Classroom Versus Screencast for Native Language Learners: Effect of Medium of Instruction on Knowledge of Programming},
year = {2015},
isbn = {9781450334402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729094.2742618},
doi = {10.1145/2729094.2742618},
abstract = {Students, who study in their native language in K-12 and go on to do their undergraduate education in English, have difficulty in acquiring programming knowledge. Solutions targeted towards improving their English proficiency take time, while those that continue with native language in the classroom limit the students' ability to compete in a global market. Another solution could be the use of video-based instructional material to empower a student for self-paced learning. In this paper, we present a comparative study of classroom instruction versus self-paced screencasts for native language learners' acquisition of programming concepts. We conducted four introductory programming workshops, each of six days duration. Two workshops were classroom based, one having Hindi (native language) as the medium of instruction and other in English. Two other workshops were screencast based, again one in Hindi and one in English. We measured differences between the groups using a post-test, across different content types such as fact, concepts and process. We found that when medium of instruction is different from language of K-12 instruction, there is an adverse impact on learning. However, when self-paced screencast is used instead of classroom environment, there is a statistically significant improvement in performance. Our work informs the choice of MoI and choice of environment for native language learners.},
booktitle = {Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {290–295},
numpages = {6},
keywords = {computer programming education, native language instruction, screencast},
location = {Vilnius, Lithuania},
series = {ITiCSE '15}
}

@inproceedings{10.1145/2851553.2851554,
author = {Matsuki, Tatsuma and Matsuoka, Naoki},
title = {A Resource Contention Analysis Framework for Diagnosis of Application Performance Anomalies in Consolidated Cloud Environments},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2851554},
doi = {10.1145/2851553.2851554},
abstract = {Cloud services have made large contributions to the agile developments and rapid revisions of various applications. However, the performance of these applications is still one of the largest concerns for developers. Although it has created many performance analysis frameworks, most of them have not been efficient for the rapid application revisions because they have required performance models, which may have had to be remodeled whenever application revisions occurred. We propose an analysis framework for diagnosis of application performance anomalies. We designed our framework so that it did not require any performance models to be efficient in rapid application revisions. That investigates the Pearson correlation and association rules between system metrics and application performance. The association rules are widely used in data-mining areas to find relations between variables in databases. We demonstrated through an experiment and testing on a real data set that our framework could select causal metrics even when the metrics were temporally correlated, which reduced the false negatives obtained from cause diagnosis. We evaluated our framework from the perspective of the expected remaining diagnostic costs of framework users. The results indicated that it was expected to reduce the diagnostic costs by 84.8\% at most, compared with a method that only used the Pearson correlation.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {173–184},
numpages = {12},
keywords = {performance diagnosis, association rule, cloud computing, correlation analysis},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1145/2737095.2742919,
author = {Nasser, Soliman and Barry, Andew and Doniec, Marek and Peled, Guy and Rosman, Guy and Rus, Daniela and Volkov, Mikhail and Feldman, Dan},
title = {Fleye on the Car: Big Data Meets the Internet of Things},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2742919},
doi = {10.1145/2737095.2742919},
abstract = {Vehicle-based vision algorithms, such as the collision alert systems [4], are able to interpret a scene in real-time and provide drivers with immediate feedback. However, such technologies are based on cameras on the car, limited to the vicinity of the car, severely limiting their potential. They cannot find empty parking slots, bypass traffic jams, or warn about dangers outside the car's immediate surrounding. An intelligent driving system augmented with additional sensors and network inputs may significantly reduce the number of accidents, improve traffic congestion, and care for the safety and quality of people's lives.We propose an open-code system, called Fleye, that consists of an autonomous drone (nano quadrotor) that carries a radio camera and flies few meters in front and above the car. The streaming video is transmitted in real time from the quadcopter to Amazon's EC2 cloud together with information about the driver, the drone, and the car's state. The output is then transmitted to the "smart glasses" of the driver. The control of the drone, as well as the sensor data collection from the driver, is done by low cost (&lt;30$) minicomputer. Most computation is done in the cloud, allowing straightforward integration of multiple vehicle behaviour and additional sensors, as well as greater computational capability.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {382–383},
numpages = {2},
keywords = {video streaming, quadrotors, internet of things, collision alert system},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/3357223.3365441,
author = {Liu, Yang and Xu, Huanle and Lau, Wing Cheong},
title = {Accordia: Adaptive Cloud Configuration Optimization for Recurring Data-Intensive Applications},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3365441},
doi = {10.1145/3357223.3365441},
abstract = {Recognizing the diversity of big data analytic jobs, cloud providers offer a wide range of virtual machine (VM) instances for different use cases. The choice of cloud instance configurations can have significant impact on the response time and running cost of data-intensive, recurring jobs for production. A poor choice of cloud instance-type/configuration can substantially degrade the response time by 5x, or increase the cost by 10x. Identifying the best cloud configuration under low search budget is a challenging problem due to i) the large and high-dimensional configuration-parameters space, ii) the dynamically varying price of some instance types, iii) job response time variation even given the same configuration, and iv) gradual drifts/ unexpected changes of the characteristics of the recurring jobs. To tackle this problem, we have designed and implemented Accordia, a system which enables Adaptive Cloud Configuration Optimization for Recurring Data-Intensive Applications.Accordia extends the Gaussian-Process Upper Confidence Bound (GP-UCB) approach in [3] to search for and track the potentially dynamic optimal cloud configuration within a high-dimensional para-meter-space. Unlike other state-of-the-art schemes, such as CherryPick[1] and Arrow[2], Accordia can handle time-varying instance pricing while providing a performance guarantee of sub-linear regret when comparing with the static, offline optimial solution.Figure 1 depicts the system architecture of our implementation of Accordia for Apache Spark running over Kubernetes. When a job is submitted, a Spark driver and multiple Spark executors are deployed as containers, each within its own Kubernetes pod. Accordia then dynamically adjusts the resource types/ allocation for the containers within their respective pods to minimize the job completion cost using the GP-UCB online-learning approach.To evaluate the performance of Accordia, we have run different mixes of recurring Spark jobs over the Google public cloud. In our experiments, Accordia dynamically learns the best cloud configuration from over 7000 candidate choices within a 5-dimensional parameter space, covering the number of executors, as well as the number of CPU cores and memory (RAM) allocation for the driver and the executor pods. Empirical measurements show that Accordia can find a near-cost-optimal configuration for a recurring job (i.e. within 10\% of the optimal cost) with fewer than 20 runs, which translates to a 2X-speedup and a 20.9\% cost-savings, when comparing to CherryPick. To highlight Accordia's capability to handle abrupt/unexpected changes of the characteristics of a recurring job, we even dynamically switch the type of a recurring job (without notifying Accordia) over exponentially-distributed time-intervals. Under such cases, Accordia can still achieve on average a cost-savings of 18.4\% over CherryPick. The full technical report is available at http://mobitec.ie.cuhk.edu.hk/cloudComputing/Accordia.pdf.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {479},
numpages = {1},
keywords = {Cloud configuration, Big data analytics, Kubernetes, Gaussian-Process UCB},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.5555/3233397.3233412,
author = {Perez-Palacin, Diego and Mirandola, Raffaela and Monterisi, Federico and Montoli, Andrea},
title = {QoS-Driven Probabilistic Runtime Evaluations of Virtual Machine Placement on Hosts},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {We tackle the cloud providers challenge of virtual machine placement when the client experienced Quality of Service (QoS) is of paramount importance and resource demand of virtual machines varies over time. To this end, this work investigates approaches that leverage measured dynamic data for placement decisions. Relying on dynamic data to guide decisions has, on the one hand, the potential to optimize hardware utilization, while, on the other hand, increases the risk on the provided QoS. In this context, we present three probabilistic methods for evaluation of host suitability to allocate new virtual machines. We also present experiments results that illustrate the differences in the outcomes of presented approaches.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {90–94},
numpages = {5},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1145/2668975.2669018,
author = {Ganihar, Syed Altaf and Joshi, Shreyas and Setty, Shankar and Mudenagudi, Uma},
title = {3D Object Decomposition and Super Resolution},
year = {2014},
isbn = {9781450327923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668975.2669018},
doi = {10.1145/2668975.2669018},
abstract = {In this paper we propose to address the problem of 3D object decomposition and super resolution. We model the 3D object as a set of Riemannian manifolds and propose metric tensor and Christoffel symbols as a novel set of features for 3D object decomposition using polynomial kernel SVM classifier. The super resolution of the 3D point clouds is carried out using the decomposed object by using selective interpolation techniques. The effectiveness of the proposed framework is demonstrated on 3D objects obtained from different datasets and achieve comparable results.},
booktitle = {SIGGRAPH Asia 2014 Posters},
articleno = {5},
numpages = {1},
location = {Shenzhen, China},
series = {SA '14}
}

@inproceedings{10.1145/3400286.3418281,
author = {Lee, Yena and An, Jae-Hoon and Kim, Younghwan},
title = {Scheduler for Distributed and Collaborative Container Clusters Based on Multi-Resource Metric},
year = {2020},
isbn = {9781450380256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400286.3418281},
doi = {10.1145/3400286.3418281},
abstract = {With the development of cloud technology, distributed and collaborative container platform technology has emerged to overcome the limitations of the existing stand-alone container platform, which has limitations in the mobility and resource scalability of cloud services. Distributed and collaborative container platform technology enables flexible expansion of resources and maximization of service mobility between container platforms distributed locally.In this paper, we propose a two-stage scheduler based on multi-resource metrics. The proposed scheduler determines the proper federated cluster where the request deployment can be deployed in a distributed and collaborative cluster environment. In order to select an proper federated cluster, filtering to select candidate clusters to which the scheduling request deployment can be deployed and scoring to evaluate the preference of each filtered cluster are performed.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {279–281},
numpages = {3},
keywords = {Distributed and Collaborative, Scheduling, Cloud Computing},
location = {Gwangju, Republic of Korea},
series = {RACS '20}
}

@inproceedings{10.1145/2976749.2978316,
author = {Shen, Yilin and Jin, Hongxia},
title = {EpicRec: Towards Practical Differentially Private Framework for Personalized Recommendation},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978316},
doi = {10.1145/2976749.2978316},
abstract = {Recommender systems typically require users' history data to provide a list of recommendations and such recommendations usually reside on the cloud/server. However, the release of such private data to the cloud has been shown to put users at risk. It is highly desirable to provide users high-quality personalized services while respecting their privacy. In this paper, we develop the first Enhanced Privacy-built-In Client for Personalized Recommendation (EpicRec) system that performs the data perturbation on the client side to protect users' privacy. Our system needs no assumption of trusted server and no change on the recommendation algorithms on the server side; and needs minimum user interaction in their preferred manner, which makes our solution fit very well into real world practical use.The design of EpicRec system incorporates three main modules: (1) usable privacy control interface that enables two user preferred privacy controls, overall and category-based controls, in the way they understand; (2) user privacy level quantification that automatically quantifies user privacy concern level from these user understandable inputs; (3) lightweight data perturbation algorithm that perturbs user private data with provable guarantees on both differential privacy and data utility.Using large-scale real world datasets, we show that, for both overall and category-based privacy controls, EpicRec performs best with respect to both perturbation quality and personalized recommendation, with negligible computational overhead. Therefore, EpicRec enables two contradictory goals, privacy preservation and recommendation accuracy. We also implement a proof-of-concept EpicRec system to demonstrate a privacy-preserving personal computer for movie recommendation with web-based privacy controls. We believe EpicRec is an important step towards designing a practical system that enables companies to monetize on user data using high quality personalized services with strong provable privacy protection to gain user acceptance and adoption of their services.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {180–191},
numpages = {12},
keywords = {privacy paradox, privacy-preserving recommendation, differential privacy},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/3277593.3277619,
author = {Belkaroui, Rami and Bertaux, Aur\'{e}lie and Labbani, Ouassila and Hugol-Gential, Cl\'{e}mentine and Nicolle, Christophe},
title = {Towards Events Ontology Based on Data Sensors Network for Viticulture Domain},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277619},
doi = {10.1145/3277593.3277619},
abstract = {Wine Cloud project is the first "Big Data" platform on the french viticulture value chain. The aim of this platform is to provide a complete traceability of the life cycle of the wine, from the wine-grower to the consumer. In particular, Wine Cloud may qualify as an agricultural decision platform that will be used for vine life cycle management in order to predict the occurrence of major risks (vine diseases, grape vine pests, physiological risks, fermentation stoppage, oxidation of vine, etc...). Also to make wine production more rational by offering winegrower a set of recommendation regarding their strategy's of production development.The proposed platform "Wine Cloud" is based on heterogeneous sensors network (agricultural machines, plant sensors and measuring stations) deployed throughout a vineyard. These sensors allow for capturing data from the agricultural process and remote monitoring vineyards in the Internet of Things (IoT) era. However, the sensors data from different source is hard to work together for lack of semantic. Therefore, the task of coherently combining heterogeneous sensors data becomes very challenging. The integration of heterogeneous data from sensors can be achieved by data mining algorithms able to build correlations. Nevertheless, the meaning and the value of these correlations is difficult to perceive without highlighting the meaning of the data and the semantic description of the measured environment.In order to bridge this gap and build causality relationships form heterogeneous sensor data, we propose an ontology-based approach, that consists in exploring heterogeneous sensor data (light, temperature, atmospheric pressure, etc) in terms of ontologies enriched with semantic meta-data describing the life cycle of the monitored environment.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {44},
numpages = {7},
keywords = {semantic sensor data, big data, smart viticulture, ontologies, event ontology, IoT},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/2896377.2901452,
author = {Zheng, Liang and Joe-Wong, Carlee and Brinton, Christopher G. and Tan, Chee Wei and Ha, Sangtae and Chiang, Mung},
title = {On the Viability of a Cloud Virtual Service Provider},
year = {2016},
isbn = {9781450342667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896377.2901452},
doi = {10.1145/2896377.2901452},
abstract = {Cloud service providers (CSPs) often face highly dynamic user demands for their resources, which can make it difficult for them to maintain consistent quality-of-service. Some CSPs try to stabilize user demands by offering sustained-use discounts to jobs that consume more instance-hours per month. These discounts present an opportunity for users to pool their usage together into a single ``job.'' In this paper, we examine the viability of a middleman, the cloud virtual service provider (CVSP), that rents cloud resources from a CSP and then resells them to users. We show that the CVSP's business model is only viable if the average job runtimes and thresholds for sustained-use discounts are sufficiently small; otherwise, the CVSP cannot simultaneously maintain low job waiting times while qualifying for a sustained-use discount. We quantify these viability conditions by modeling the CVSP's job scheduling and then use this model to derive users' utility-maximizing demands and the CVSP's profit-maximizing price, as well as the optimal number of instances that the CVSP should rent from the CSP. We verify our results on a one-month trace from Google's production compute cluster, through which we first validate our assumptions on the job arrival and runtime distributions, and then show that the CVSP is viable under these workload traces. Indeed, the CVSP can earn a positive profit without significantly impacting the CSP's revenue, indicating that the CSP and CVSP can coexist in the cloud market.},
booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
pages = {235–248},
numpages = {14},
keywords = {virtual service provider, cloud pricing, economic viability},
location = {Antibes Juan-les-Pins, France},
series = {SIGMETRICS '16}
}

@article{10.1145/2964791.2901452,
author = {Zheng, Liang and Joe-Wong, Carlee and Brinton, Christopher G. and Tan, Chee Wei and Ha, Sangtae and Chiang, Mung},
title = {On the Viability of a Cloud Virtual Service Provider},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2964791.2901452},
doi = {10.1145/2964791.2901452},
abstract = {Cloud service providers (CSPs) often face highly dynamic user demands for their resources, which can make it difficult for them to maintain consistent quality-of-service. Some CSPs try to stabilize user demands by offering sustained-use discounts to jobs that consume more instance-hours per month. These discounts present an opportunity for users to pool their usage together into a single ``job.'' In this paper, we examine the viability of a middleman, the cloud virtual service provider (CVSP), that rents cloud resources from a CSP and then resells them to users. We show that the CVSP's business model is only viable if the average job runtimes and thresholds for sustained-use discounts are sufficiently small; otherwise, the CVSP cannot simultaneously maintain low job waiting times while qualifying for a sustained-use discount. We quantify these viability conditions by modeling the CVSP's job scheduling and then use this model to derive users' utility-maximizing demands and the CVSP's profit-maximizing price, as well as the optimal number of instances that the CVSP should rent from the CSP. We verify our results on a one-month trace from Google's production compute cluster, through which we first validate our assumptions on the job arrival and runtime distributions, and then show that the CVSP is viable under these workload traces. Indeed, the CVSP can earn a positive profit without significantly impacting the CSP's revenue, indicating that the CSP and CVSP can coexist in the cloud market.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {235–248},
numpages = {14},
keywords = {cloud pricing, economic viability, virtual service provider}
}

@inproceedings{10.1145/2693561.2693563,
author = {Klein, John and Gorton, Ian},
title = {Runtime Performance Challenges in Big Data Systems},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693563},
doi = {10.1145/2693561.2693563},
abstract = {Big data systems are becoming pervasive. They are distributed systems that include redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient to assure runtime performance in production. This is due to the scale of the deployed system, the continually evolving workloads, and the unpredictable quality of service of the shared infrastructure. Consequently, a solution for addressing performance requirements needs sophisticated runtime observability and measurement. Observability gives real-time insights into a system's health and status, both at the system and application level, and provides historical data repositories for forensic analysis, capacity planning, and predictive analytics. Due to the scale and heterogeneity of big data systems, significant challenges exist in the design, customization and operations of observability capabilities. These challenges include economical creation and insertion of monitors into hundreds or thousands of computation and data nodes, efficient, low overhead collection and storage of measurements (which is itself a big data problem), and application-aware aggregation and visualization. In this paper we propose a reference architecture to address these challenges, which uses a model-driven engineering toolkit to generate architecture-aware monitors and application-specific visualizations.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {17–22},
numpages = {6},
keywords = {model-driven engineering, observability, big data},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@article{10.1109/TNET.2014.2354262,
author = {Adhikari, Vijay K. and Guo, Yang and Hao, Fang and Hilt, Volker and Zhang, Zhi-Li and Varvello, Matteo and Steiner, Moritz},
title = {Measurement Study of Netflix, Hulu, and a Tale of Three CDNs},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2014.2354262},
doi = {10.1109/TNET.2014.2354262},
abstract = {Netflix and Hulu are leading Over-the-Top (OTT) content service providers in the US and Canada. Netflix alone accounts for 29.7\% of the peak downstream traffic in the US in 2011. Understanding the system architectures and performance of Netflix and Hulu can shed light on the design of such large-scale video streaming platforms, and help improving the design of future systems. In this paper, we perform extensive measurement study to uncover their architectures and service strategies. Netflix and Hulu bear many similarities. Both Netflix and Hulu video streaming platforms rely heavily on the third-party infrastructures, with Netflix migrating that majority of its functions to the Amazon cloud, while Hulu hosts its services out of Akamai. Both service providers employ the same set of three content distribution networks (CDNs) in delivering the video contents. Using active measurement study, we dissect several key aspects of OTT streaming platforms of Netflix and Hulu, e.g., employed streaming protocols, CDN selection strategy, user experience reporting, etc. We discover that both platforms assign the CDN to a video request without considering the network conditions and optimizing the user-perceived video quality. We further conduct the performance measurement studies of the three CDNs employed by Netflix and Hulu. We show that the available bandwidths on all three CDNs vary significantly over the time and over the geographic locations. We propose a measurement-based adaptive CDN selection strategy and a multiple-CDN-based video delivery strategy that can significantly increase users' average available bandwidth.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {1984–1997},
numpages = {14},
keywords = {CDN selection strategy, Netflix, video streaming, content distribution networks (CDN), Hulu, over-the-top (OTT) content service}
}

@inproceedings{10.1145/2851613.2851727,
author = {Megyesi, P\'{e}ter and Botta, Alessio and Aceto, Giuseppe and Pescap\`{e}, Antonio and Moln\'{a}r, S\'{a}ndor},
title = {Available Bandwidth Measurement in Software Defined Networks},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851727},
doi = {10.1145/2851613.2851727},
abstract = {Software Defined Networking (SDN) is an emerging paradigm that is expected to revolutionize computer networks. With the decoupling of data and control plane and the introduction of open communication interfaces between layers, SDN enables programmability over the entire network, promising rapid innovation in this area. The SDN concept was already proven to work successfully in cloud and data center environments thus the proper monitoring of such networks is already in the focus of the research community. Methods for measuring Quality of Service (QoS) parameters such as bandwidth utilization, packet loss, and delay have been recently introduced in literature, but they lack a solution for tackling down the question of available bandwidth. In this paper, we attempt to fill this gap and introduce a novel mechanism for measuring available bandwidth in SDN networks. We take advantage of the SDN architecture and build an application over the Network Operating System (NOS). Our application can track the topology of the network and the bandwidth utilization over the network links, and thus it is able to calculate the available bandwidth between any two points in the network. We validate our method using the popular Mininet network emulation environment and the widely used NOS called Floodlight. We present results providing insights into the measurement accuracy and showing its relationship with the delay in the control network and the polling frequency.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {651–657},
numpages = {7},
keywords = {network operating system, software defined networks, available bandwidth, mininet, OpenFlow, floodlight},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3365245.3365247,
author = {Liu, Xiaofeng and Zou, Hui and Niu, Wanyu and Song, Yuqing and He, Wenzhang},
title = {An Approach of Traffic Accident Scene Reconstruction Using Unmanned Aerial Vehicle Photogrammetry},
year = {2019},
isbn = {9781450372435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365245.3365247},
doi = {10.1145/3365245.3365247},
abstract = {Accurate and detailed information of traffic accident scene is important for accident investigation, and the current investigation methods (tape measuring and total station survey) always need highway closure, and their working process is time-consuming. From different angles or at different altitudes, unmanned aerial vehicle (UAV) can monitor accident site without interrupting the traffic flow, therefore, UAV is introduced for accident scene reconstruction. Firstly, the method framework of accident scene reconstruction was proposed, in which UAV was used to take pictures of accident site, and imaging system was adopted to reconstruct the 2D and 3D accident scene. Then, 3D reconstruction, point cloud generation, and model optimization were presented. Next, a UAV flight experiment was conducted for traffic accident scene reconstruction, and two evaluation indexes, signal-to-noise ratio and structural similarity, were introduced to assess the image quality of accident scene reconstruction. The case study demonstrates that compared with current methods, the proposed method is efficient; moreover, the effect of accident scene reconstruction is satisfactory.},
booktitle = {Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing},
pages = {31–34},
numpages = {4},
keywords = {traffic investigation, Scene reconstruction, photogrammetry, unmanned aerial vehicle},
location = {Prague, Czech Republic},
series = {SSIP '19}
}

@inproceedings{10.1145/3316782.3322746,
author = {Haslwanter, Jean D. Hallewell and Heiml, Michael and Wolfartsberger, Josef},
title = {Lost in Translation: Machine Translation and Text-to-Speech in Industry 4.0},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3322746},
doi = {10.1145/3316782.3322746},
abstract = {Small lot sizes are becoming more common in modern manufacturing. Rather than automate every possible product variant, companies may rely on manual assembly to be more flexible. However, it can be difficult for people to remember the steps for every possible product variant. Assistive systems providing instructions can support workers. In this paper, we present a study investigating whether existing machine translation and text-to-speech engines provide sufficient quality to enable on-the-fly translations to provide assistance to workers in their native languages. The results of our tests indicate that machine translation is not yet sufficient for this application.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {333–342},
numpages = {10},
keywords = {TTS, assistive systems, MT, text to speech, manual assembly, machine translation, computer-assisted instruction},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/3307334.3326087,
author = {Shi, Shu and Gupta, Varun and Jana, Rittwik},
title = {Freedom: Fast Recovery Enhanced VR Delivery Over Mobile Networks},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326087},
doi = {10.1145/3307334.3326087},
abstract = {In this paper we design and implement Freedom, a mobile VR system that deliver high quality VR content on today's mobile devices using 4G/LTE cellular networks. Compared to existing state-of-the-art, Freedom does not rely on any video frame pre- rendering or viewpoint prediction. We send a latency-adaptive VAM frame that contains pixels around the FoV. This allows the clients to render locally at a high refresh rate of 60 Hz to accommodate and compensate for the user's head movements before the next server update arrives. We demonstrate that Freedom is the first system in the world that can support dynamic and live 8K resolution VR content, while adapting to the real-world latency variations experienced in cellular networks. Compared to streaming the whole 360° panoramic VR content, we show that Freedom achieves up to 80\% bandwidth savings. Finally, we provide detailed end to end latency measurements of actual VR systems by running extensive experiments in a private LTE testbed using a Mobile Edge Cloud (MEC).},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {130–141},
numpages = {12},
keywords = {remote rendering, mobile edge cloud, 360 video, mobile vr, motion-to-update latency},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@inproceedings{10.1145/2964284.2964327,
author = {Wu, Chao and Jia, Jia and Zhu, Wenwu and Chen, Xu and Yang, Bowen and Zhang, Yaoxue},
title = {Affective Contextual Mobile Recommender System},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2964327},
doi = {10.1145/2964284.2964327},
abstract = {Exponential growth of media consumption in online social networks demands effective recommendation to improve the quality of experience especially for on-the-go mobile users. By means of large-scale trace-driven measurements over mobile Twitter traces from users, we reveal the significance of affective features in shaping users' social media behaviors. Existing recommender systems however, rarely support this psychological effect in real-life. To capture this effect, in this paper we propose Kaleido, a real mobile system to achieve an affect-aware learning-based social media recommendation.Specifically, we design a machine learning mechanism to infer the affective feature within media contents. Furthermore, a cluster-based latent bias model is provided for jointly training the affect, behavior and social contexts. Our comprehensive experiments on Android prototype expose a superior prediction accuracy of 82\%, with more than 20\% accuracy improvement over existing mobile recommender systems. Moreover, by enabling users to offload their machine learning procedures to the deployed edge-cloud testbed, our system achieves speed-up of a factor of 1,000 against the local data training execution on smartphones.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1375–1384},
numpages = {10},
keywords = {mobile application, social networks, recommender system, affective computing},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.5555/2602339.2602357,
author = {Misra, Prasant Kumar and Hu, Wen and Jin, Yuzhe and Liu, Jie and Souza de Paula, Amanda and Wirstrom, Niklas and Voigt, Thiemo},
title = {Energy Efficient GPS Acquisition with Sparse-Gps},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {Following rising demands in positioning with GPS, low-cost receivers are becoming widely available; but their energy demands are still too high. For energy efficient GPS sensing in delay-tolerant applications, the possibility of offloading a few milliseconds of raw signal samples and leveraging the greater processing power of the cloud for obtaining a position fix is being actively investigated. In an attempt to reduce the energy cost of this data offloading operation, we propose Sparse-GPS1: a new computing framework for GPS acquisition via sparse approximation. Within the framework, GPS signals can be efficiently compressed by random ensembles. The sparse acquisition information, pertaining to the visible satellites that are embedded within these limited measurements, can subsequently be recovered by our proposed representation dictionary. By extensive empirical evaluations, we demonstrate the acquisition quality and energy gains of Sparse-GPS. We show that it is twice as energy efficient than offloading uncompressed data, and has 5-10 times lower energy costs than standalone GPS; with a median positioning accuracy of 40 m.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {155–166},
numpages = {12},
keywords = {gps, sparse approximation, synchronization, compressed sensing, location sensing, energy efficiency},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3330430.3333644,
author = {Bruechner, Dominik and Renz, Jan and Klingbeil, Mandy},
title = {Creating a Framework for User-Centered Development and Improvement of Digital Education},
year = {2019},
isbn = {9781450368049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330430.3333644},
doi = {10.1145/3330430.3333644},
abstract = {We investigate how the technology acceptance and learning experience of the digital education platform HPI Schul-Cloud (HPI School Cloud) for German secondary school teachers can be improved by proposing a user-centered research and development framework. We highlight the importance of developing digital learning technologies in a user-centered way to take differences in the requirements of educators and students into account. We suggest applying qualitative and quantitative methods to build a solid understanding of a learning platform's users, their needs, requirements, and their context of use. After concept development and idea generation of features and areas of opportunity based on the user research, we emphasize on the application of a multi-attribute utility analysis decision-making framework to prioritize ideas rationally, taking results of user research into account. Afterward, we recommend applying the principle build-learn-iterate to build prototypes in different resolutions while learning from user tests and improving the selected opportunities. Last but not least, we propose an approach for continuous short- and long-term user experience controlling and monitoring, extending existing web- and learning analytics metrics.},
booktitle = {Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale},
articleno = {31},
numpages = {4},
keywords = {HPI Schul-Cloud, user-centered design, learning platform, user research framework, evaluation, user experience},
location = {Chicago, IL, USA},
series = {L@S '19}
}

@inproceedings{10.1145/3240508.3240620,
author = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Eerik\"{a}inen, Jukka and Yl\"{a}-J\"{a}\"{a}ski, Antti},
title = {CloudVR: Cloud Accelerated Interactive Mobile Virtual Reality},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240620},
doi = {10.1145/3240508.3240620},
abstract = {High quality immersive Virtual Reality experience currently requires a PC setup with cable connected head mounted display, which is expensive and restricts user mobility. This paper presents CloudVR which is a system for cloud accelerated interactive mobile VR. It is designed to provide short rotation and interaction latencies through panoramic rendering and dynamic object placement. CloudVR also includes rendering optimizations to reduce server-side computational load and bandwidth requirements between the server and client. Performance measurements with a CloudVR prototype suggest that the optimizations make it possible to double the server's framerate and halve the amount of bandwidth required and that small objects can be quickly moved at run time to client device for rendering to provide shorter interaction latency. A small-scale user study indicates that CloudVR users do not notice small network latencies (20ms) and even much longer ones (100-200ms) become non-trivial to detect when they do not affect the interaction with objects. Finally, we present a design of CloudVR extension to multi-user scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1181–1189},
numpages = {9},
keywords = {optimization, rendering, unity, virtual reality, cloud, edge computing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/2639108.2639118,
author = {Li, Liqun and Shen, Guobin and Zhao, Chunshui and Moscibroda, Thomas and Lin, Jyh-Han and Zhao, Feng},
title = {Experiencing and Handling the Diversity in Data Density and Environmental Locality in an Indoor Positioning Service},
year = {2014},
isbn = {9781450327831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639108.2639118},
doi = {10.1145/2639108.2639118},
abstract = {Diversity in training data density and environment locality is intrinsic in the real-world deployment of indoor localization systems and has a major impact on the performance of existing localization approaches. In this paper, through micro-benchmarks, we find that fingerprint-based approaches are preferable in scenarios where a dense database is available; while model-based approaches are the method of choice in the case of sparse data. It should be noted, however, that practical situations are complex. A single deployment often features both sparse and dense sampled areas. Furthermore, the internal layout affects the propagation of radio signals and exhibits environmental impacts. A certain number of measurement samples may be sufficient for one part of the building, but entirely insufficient for another. Thus, finding the right indoor localization algorithm for a given large-scale deployment is challenging, if not impossible; there is no one-size-fits-all indoor localization approach.Realizing the fundamental fact that the quality of the location database capturing the actual radio map dictates localization accuracy, in this paper, we propose Modellet, an algorithmic approach that optimally approximates the actual radio map by unifying model-based and fingerprint-based approaches. Modellet represents the radio map using a fingerprint-cloud that incorporates both measured real fingerprints and virtual fingerprints, which are computed from models with a local support, based on the key concept of the supporting set. We evaluate Modellet with data collected from an office building as well as 13 large-scale deployment venues (shopping malls and airports), located across China, U.S., and Germany. Comparing Modellet with two representative baseline approaches, RADAR and EZPerfect, demonstrates that Modellet effectively adapts to different data densities and environmental conditions, substantially outperforming existing approaches.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Computing and Networking},
pages = {459–470},
numpages = {12},
keywords = {model, fingerprint, indoor localization},
location = {Maui, Hawaii, USA},
series = {MobiCom '14}
}

@inproceedings{10.1145/3036290.3036312,
author = {Jaiswal, Akshay and Mishra, R. B.},
title = {Cloud Service Selection Using TOPSIS and Fuzzy TOPSIS with AHP and ANP},
year = {2017},
isbn = {9781450348287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3036290.3036312},
doi = {10.1145/3036290.3036312},
abstract = {The growing demand and availability of cloud services have triggered the need for comparison of their features available to customers at different prices and performance. It is necessary to be said that relevant and fair comparison is still challenging due to diverse deployment options and unique features of different services.The aim of this paper is to rank cloud services based on quantified QoS (Quality of Service) attributes using Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) and fuzzy TOPSIS, and comparing them to find out which method suits more in different scenarios.A comparative study of Analytic Hierarchy Process (AHP) and Analytic Network process (ANP) is also done while extracting the weights of criteria for TOPSIS and fuzzy TOPSIS.},
booktitle = {Proceedings of the 2017 International Conference on Machine Learning and Soft Computing},
pages = {136–142},
numpages = {7},
keywords = {AHP, TOPSIS, Cloud Computing, Multi Attribute Decision Making, ANP, Fuzzy set theory},
location = {Ho Chi Minh City, Vietnam},
series = {ICMLSC '17}
}

@article{10.1145/3131778,
author = {Setty, Shankar and Mudenagudi, Uma},
title = {Region of Interest-Based 3D Inpainting of Cultural Heritage Artifacts},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3131778},
doi = {10.1145/3131778},
abstract = {In this article, we address the problem of 3D inpainting using an exemplar-based method for point clouds. 3D inpainting is a process of filling holes or missing regions in the reconstructed 3D models. Typically, inpainting methods addressed in the literature fill missing regions due to occlusions or inaccurate scanning of 3D models. However, we focus on scenarios involving naturally existing damaged models, which are partly broken or incomplete in the artifacts at cultural heritage sites. We propose an exemplar-based inpainting technique using the region of interest (ROI)-based method to inpaint the missing regions of the damaged model. The ROI of a 3D model is represented as a set of Riemannian manifolds, and metric tensor and Christoffel symbols are used as geometric features to capture the inherent geometry. We then decompose the ROI into basic shape regions, namely, spherical, conical, and cylindrical components, and identify the best-fit match for inpainting. Instead of using a single similar exemplar for inpainting, we select the most relevant best-fit region to fill the missing region from the basic shape regions library obtained from n similar exemplars. We demonstrate the performance of the proposed inpainting method on artifacts at UNESCO World Heritage site Hampi temples, India with varying complexities and sizes for both synthetically generated holes and real missing regions in 3D objects.},
journal = {J. Comput. Cult. Herit.},
month = {may},
articleno = {9},
numpages = {21},
keywords = {point cloud data, region of interest, cultural heritage artifacts, geometric features, 3D inpainting, Riemannian manifolds}
}

@inproceedings{10.1145/3204949.3208126,
author = {Taheri, Sajjad and Vedienbaum, Alexander and Nicolau, Alexandru and Hu, Ningxin and Haghighat, Mohammad R.},
title = {OpenCV.Js: Computer Vision Processing for the Open Web Platform},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3208126},
doi = {10.1145/3204949.3208126},
abstract = {The Web is the world's most ubiquitous compute platform and the foundation of digital economy. Ever since its birth in early 1990's, web capabilities have been increasing in both quantity and quality. However, in spite of all such progress, computer vision is not mainstream on the web yet. The reasons are historical and include lack of sufficient performance of JavaScript, lack of camera support in the standard web APIs, and lack of comprehensive computer-vision libraries. These problems are about to get solved, resulting in the potential of an immersive and perceptual web with transformational effects including in online shopping, education, and entertainment among others. This work aims to enable web with computer vision by bringing hundreds of OpenCV functions to the open web platform. OpenCV is the most popular computer-vision library with a comprehensive set of vision functions and a large developer community. OpenCV is implemented in C++ and up until now, it was not available in the web browsers without the help of unpopular native plugins. This work leverage OpenCV efficiency, completeness, API maturity, and its communitys collective knowledge. It is provided in a format that is easy for JavaScript engines to highly optimize and has an API that is easy for the web programmers to adopt and develop applications. In addition, OpenCV parallel implementations that target SIMD units and multiprocessors can be ported to equivalent web primitives, providing better performance for real-time and interactive use cases.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {478–483},
numpages = {6},
keywords = {multimedia, web, javascript, performance, computer vision, parallel processing},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1109/IPSN.2018.00025,
author = {Adkins, Joshua and Ghena, Branden and Jackson, Neal and Pannuto, Pat and Rohrer, Samuel and Campbell, Bradford and Dutta, Prabal},
title = {Applications on the Signpost Platform for City-Scale Sensing: Demo Abstract},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00025},
doi = {10.1109/IPSN.2018.00025},
abstract = {City-scale sensing holds the promise of enabling deeper insight into how our urban environments function. Applications such as observing air quality and measuring traffic flows can have powerful impacts, allowing city planners and citizen scientists alike to understand and improve their world. However, the path from conceiving applications to implementing them is fraught with difficulty. A successful city-scale deployment requires physical installation, power management, and communications---all challenging tasks standing between a good idea and a realized one.The Signpost platform, presented at IPSN 2018, has been created to address these challenges. Signpost enables easy deployment by relying on harvested, solar energy and wireless networking rather than their wired counterparts. To further lower the bar to deploying applications, the platform provides the key resources necessary to support its pluggable sensor modules in their distributed sensing tasks. In this demo, we present the Signpost hardware and several applications running on a deployment of Signposts on UC Berkeley's campus, including distributed, energy-adaptive traffic monitoring and fine grained weather reporting. Additionally we show the cloud infrastructure supporting the Signpost deployment, specifically the ability to push new applications and parameters down to existing sensors, with the goal of demonstrating that the existing deployment can serve as a future testbed.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {124–125},
numpages = {2},
location = {Porto, Portugal},
series = {IPSN '18}
}

