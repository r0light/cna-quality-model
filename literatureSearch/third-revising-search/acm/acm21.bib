@inproceedings{10.1145/3204949.3204976,
author = {Mekuria, Rufael and McGrath, Michael J. and Riccobene, Vincenzo and Bayon-Molino, Victor and Tselios, Christos and Thomson, John and Dobrodub, Artem},
title = {Automated Profiling of Virtualized Media Processing Functions Using Telemetry and Machine Learning},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204976},
doi = {10.1145/3204949.3204976},
abstract = {Most media streaming services are composed by different virtualized processing functions such as encoding, packaging, encryption, content stitching etc. Deployment of these functions in the cloud is attractive as it enables flexibility in deployment options and resource allocation for the different functions. Yet, most of the time overprovisioning of cloud resources is necessary in order to meet demand variability. This can be costly, especially for large scale deployments. Prior art proposes resource allocation based on analytical models that minimize the costs of cloud deployments under a quality of service (QoS) constraint. However, these models do not sufficiently capture the underlying complexity of services composed of multiple processing functions. Instead, we introduce a novel methodology based on full-stack telemetry and machine learning to profile virtualized or cloud native media processing functions individually. The basis of the approach consists of investigating 4 categories of performance metrics: throughput, anomaly, latency and entropy (TALE) in offline (stress tests) and online setups using cloud telemetry. Machine learning is then used to profile the media processing function in the targeted cloud/NFV environment and to extract the most relevant cloud level Key Performance Indicators (KPIs) that relate to the final perceived quality and known client side performance indicators. The results enable more efficient monitoring, as only KPI related metrics need to be collected, stored and analyzed, reducing the storage and communication footprints by over 85\%. In addition a detailed overview of the functions behavior was obtained, enabling optimized initial configuration and deployment, and more fine-grained dynamic online resource allocation reducing overprovisioning and avoiding function collapse. We further highlight the next steps towards cloud native carrier grade virtualized processing functions relevant for future network architectures such as in emerging 5G architectures.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {150–161},
numpages = {12},
keywords = {video streaming, performance, cloud computing, telemetry, experimentation, characterization},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/3267809.3275470,
author = {Nadgowda, Shripad and Isci, Canturk},
title = {Drishti: Disaggregated and Interoperable Security Analytics Framework for Cloud},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3275470},
doi = {10.1145/3267809.3275470},
abstract = {Application and platform security has always been critical for the success of any business. Traditionally, applications were deployed directly on physical servers. As a result, there are myriad traditional security solutions that were developed around this model to run as local agents on the systems they monitor and protect. These solutions are then refined and standardized with decades of experience. With the emergence of virtualization, cloud and particularly containerization, use of these solutions is becoming challenging with consolidation and scale. As we begin to deploy hundreds of cloud instances on a single node, traditional solutions, designed for local execution do not scale out. At the same time, the clean separation of a virtual machine (VM) or a container from the platform itself, and maturing introspection and inspection APIs provide a simple, practical way to decouple monitored from the monitors [3]. Furthermore, as the scope of cloud security expands from simple monitoring and auditing to more complex learning based analytics, analytics components are further offloaded to separate data services, where they can burn extensive cycles, and in some cases use specialized hardware for security analytics, out of the critical path of the monitored applications [5]. As a result, traditional agent-based tightly-coupled model is being replaced by a more dis-aggregated {system, observation, analytics, actions} architecture.To implement such dis-aggregated model in practice, first system state needs to be transferred from cloud platform to analytic platform. File system more generally is representative of the system state that persists features of interest for security analytics like processes, metrics, configurations, packages across various files. Remote replication or snapshotting [1] of whole file system is very in-efficient, since only small set of files are accessed during the analytics. As a result, a new family of cloud-native security solutions have recently emerged in the field that uses various specialized data collection techniques[2, 4]. These techniques perform out-of band introspection of systems to interpret and extract required system features from the file system to essentially serialize system state into data. This data is then transferred to an analytic platform for analysis. Unlike the traditional security solutions that work locally against the system's standard POSIXy file system interfaces, these emerging security analytics "work from data" on the analytic platform. However since the target system is now available as "data", existing agent-based security solutions become incompatible to work against the system. One mitigating solution is to rewrite all existing solutions, which requires huge amount of resources and effort.In Drishti, we address this challenge from a fundamentally different perspective. Instead of rewriting security solutions to work from data, we make the data work for traditional security applications. We achieve this by developing a pseudo-system interface over systems data collected from cloud instances. With this approach, existing solutions run unmodified, as "black box" software over this system interface, as if they were running on the actual cloud instance. Drishti framework is our realization of this approach. It is logically the inverse of the first step of cloud-native security analytics that convert system state into data. With Drishti we transform data back to system on the analytic platform by orchestrating two file system components. First, a standard native system interface is re-calibrated over the system data through our new FUSE file system, confuse or ClOud Native Filesystem in UserSpacE. Second, we mimic the "effect" of an agent installation via an overlay file system based on the the agent image. Within the Drishti framework the underlying data looks like a standard POSIX system to each on-boarded security solution. This allows us to run existing agent-based security solutions as is, but still decoupled from the actual system. Drishti also provides a standard and interoperable platform for designing new security analytic solutions.Overall, Drishti demonstrates a novel, pragmatic and highly-practical approach for bringing security analytics into the cloud. It enables us to leverage existing solutions built based on decades of experience by eliminating the need for reinventing the wheel for cloud.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {528},
numpages = {1},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@inproceedings{10.1145/2701126.2701226,
author = {Rizvi, Syed and Ryoo, Jungwoo and Kissell, John and Aiken, Bill},
title = {A Stakeholder-Oriented Assessment Index for Cloud Security Auditing},
year = {2015},
isbn = {9781450333771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701126.2701226},
doi = {10.1145/2701126.2701226},
abstract = {Cloud computing is an emerging computing model that provides numerous advantages to organizations (both service providers and customers) in terms of massive scalability, lower cost, and flexibility, to name a few. Despite these technical and economical advantages of cloud computing, many potential cloud consumers are still hesitant to adopt cloud computing due to security and privacy concerns. This paper describes some of the unique cloud computing security factors and subfactors that play a critical role in addressing cloud security and privacy concerns. To mitigate these concerns, we develop a security metric tool to provide information to cloud users about the security status of a given cloud vendor. The primary objective of the proposed metric is to produce a security index that describes the security level accomplished by an evaluated cloud computing vendor. The resultant security index will give confidence to different cloud stakeholders and is likely to help them in decision making, increase the predictability of the quality of service, and allow appropriate proactive planning if needed before migrating to the cloud. To show the practicality of the proposed metric, we provide two case studies based on the available security information about two well-known cloud service providers (CSP). The results of these case studies demonstrated the effectiveness of the security index in determining the overall security level of a CSP with respect to the security preferences of cloud users.},
booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication},
articleno = {55},
numpages = {7},
keywords = {cloud security, cloud auditing, security metrics, data privacy},
location = {Bali, Indonesia},
series = {IMCOM '15}
}

@inproceedings{10.1145/2668930.2688043,
author = {Becker, Matthias and Lehrig, Sebastian and Becker, Steffen},
title = {Systematically Deriving Quality Metrics for Cloud Computing Systems},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688043},
doi = {10.1145/2668930.2688043},
abstract = {In cloud computing, software architects develop systems for virtually unlimited resources that cloud providers account on a pay-per-use basis. Elasticity management systems provision these resources autonomously to deal with changing workload. Such changing workloads call for new objective metrics allowing architects to quantify quality properties like scalability, elasticity, and efficiency, e.g., for requirements/SLO engineering and software design analysis. In literature, initial metrics for these properties have been proposed. However, current metrics lack a systematic derivation and assume knowledge of implementation details like resource handling. Therefore, these metrics are inapplicable where such knowledge is unavailable.To cope with these lacks, this short paper derives metrics for scalability, elasticity, and efficiency properties of cloud computing systems using the goal question metric (GQM) method. Our derivation uses a running example that outlines characteristics of cloud computing systems. Eventually, this example allows us to set up a systematic GQM plan and to derive an initial set of six new metrics. We particularly show that our GQM plan allows to classify existing metrics.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {169–174},
numpages = {6},
keywords = {analysis, cloud computing, efficiency, slo, scalability, metric, elasticity, gqm},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/2980258.2982046,
author = {VasanthaAzhagu, A. Kannaki and Gnanasekar, J. M.},
title = {Cloud Computing Overview, Security Threats and Solutions-A Survey},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2982046},
doi = {10.1145/2980258.2982046},
abstract = {Cloud Computing aims to provide computing everywhere. It delivers computing resources on demand over internet in terms of anything anywhere anytime concept. It provides everything as a service to its users like infrastructure platform software hardware workplace data and security. Cloud computing has made revolutionary transformations in the government and business. Cloud Computing transforms the databases and application software to the huge data centers, where the management of the services and data may not be trustworthy. To verify the correctness, integrity, confidentially and availability of data in the cloud, in this paper, we focus on various cloud computing security threats and solution that have been used since security is an important measure for quality of service.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {109},
numpages = {6},
keywords = {Cloud Computing, Availability, Deployment Security threats, Integrity, Quality of Service (QoS)},
location = {Pondicherry, India},
series = {ICIA-16}
}

@article{10.1145/2557833.2557854,
author = {Yadav, Nikita and Khatri, Sujata and Singh, V. B.},
title = {Developing an Intelligent Cloud for Higher Education},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2557854},
doi = {10.1145/2557833.2557854},
abstract = {With rapid development in the IT world, technologies are becoming more dynamic and advanced. Today, technologies are changing with customer requirements. In the IT world, research is carried out to make technology better to meet the requirements that change with time. With the advancement in the IT world, online services have proliferated. Now a days, cloud computing is the hottest buzzword in the IT world. Cloud computing is not limited to the E-Governance and business worlds, but is also making a great impact in the education world. With growing demand for education, technologies and research, all universities and education institutions have their eyes on cloud computing. The main pillars of educational institutions are students, faculties, administrations and libraries. Faculty and students do research and need quality data while students of a particular field need a subject-oriented knowledge. Manually getting these kinds of data is time consuming as students depend on literature, books, different kind of software and hardware. With cloud computing in higher education, cost-effective measures can be taken to minimize the dependency on books, hardware and software. In this paper, we discuss how Artifical Intelligence based cloud computing in higher education will improve quality and ease the process of getting e-resources (software/hardware platform, storage etc.). This study will help in understanding effective cost-cutting measures. We also discuss how cloud computing in the library and administration will brighten the education prospects.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {1–5},
numpages = {5},
keywords = {cloud computing, E-learning, E-library, E-administration, higher education}
}

@inproceedings{10.1145/2737182.2737185,
author = {Lehrig, Sebastian and Eikerling, Hendrik and Becker, Steffen},
title = {Scalability, Elasticity, and Efficiency in Cloud Computing: A Systematic Literature Review of Definitions and Metrics},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737185},
doi = {10.1145/2737182.2737185},
abstract = {Context: In cloud computing, there is a multitude of definitions and metrics for scalability, elasticity, and efficiency. However, stakeholders have little guidance for choosing fitting definitions and metrics for these quality properties, thus leading to potential misunderstandings. For example, cloud consumers and providers cannot negotiate reliable and quantitative service level objectives directly understood by each stakeholder. Objectives: Therefore, we examine existing definitions and metrics for these quality properties from the viewpoint of cloud consumers, cloud providers, and software architects with regard to commonly used concepts. Methods: We execute a systematic literature review (SLR), reproducibly collecting common concepts in definitions and metrics for scalability, elasticity, and efficiency. As quality selection criteria, we assess whether existing literature differentiates the three properties, exemplifies metrics, and considers typical cloud characteristics and cloud roles. Results: Our SLR yields 418 initial results from which we select 20 for in-depth evaluation based on our quality selection criteria. In our evaluation, we recommend concepts, definitions, and metrics for each property. Conclusions: Software architects can use our recommendations to analyze the quality of cloud computing applications. Cloud providers and cloud consumers can specify service level objectives based on our metric suggestions.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {83–92},
numpages = {10},
keywords = {cloud, systematic literature review, scalability, metrics, elasticity, definitions, efficiency, cloud computing},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@inproceedings{10.1145/2896387.2896403,
author = {Al-Ghuwairi, Abdel-Rahman and Eid, Hazem and Aloran, Mohammad and Salah, Zaher and Baarah, Aladdin Hussein and Al-oqaily, Ahmad A.},
title = {A Mutation-Based Model to Rank Testing as a Service (TaaS) Providers in Cloud Computing},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2896403},
doi = {10.1145/2896387.2896403},
abstract = {With the increase of cloud computing service models, the need to measure and evaluate them are increased as well. In this paper, we proposed a novel measurement approach for the purpose of evaluating the quality of Testing as a Service (TaaS), which is considered as one of the most recent outstanding model within cloud computing environment. (TaaS) as outstanding model include the provision of multi-sub services, such as enabling cloud customer to verify his own code through the use of cloud provider resources. Its goes without questioning that testing over web environment requires high level of resources, time, and effort. Therefore, it should take high attention toward the quality of the used testing technique. Where, the quality of testing technique associated with set of attributes that has the ability to determine testing effectiveness. Thus, in this paper we propose a measurement approach to evaluate the effectiveness of TaaS, over cloud computing environment which relies on the use of mutation score. The main contribution of the proposed model represent in the use of mutation score to evaluate cloud providers ability to perform TaaS, and rank them according to the percentage of TaaS effectiveness.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {18},
numpages = {5},
keywords = {Effectiveness, Cloud computing, Testing as a services, Cloud services, Measurement, Mutation},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/3234781.3234787,
author = {Tse, Daniel and Yuen, Hok Hin and He, Qiran and Wang, Chaoya and Yu, Jiheng},
title = {The Security Vulnerabilities of On-Demand and Sharing Economy},
year = {2018},
isbn = {9781450364904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234781.3234787},
doi = {10.1145/3234781.3234787},
abstract = {The cloud computing has been widely in on-demand-and-sharing service economy and has become a hotspot in recent years especially in the IT industry which really lead to some changes in human's daily life. However, many users and researchers believed that the information security is the most significant challenge in cloud computing. Therefore, this paper aims to discover the threats and vulnerabilities of the cloud storage which is the most common application originating from the cloud computing. This research utilized a quantitative approach and all qualified respondents were asked to complete an online questionnaire. The result shows that (1) Data loss and leakage is the biggest threat in using cloud storage application (2) Abuse use of cloud computational resources is the most severe impact in cloud storage application (3) Respondents with different backgrounds have the different perspectives towards the cloud service (4) The countermeasures to minimize the security vulnerability are flexibility in choosing the protective measures, strengthen the infrastructure, improve the password authentication and strengthen the authorization.},
booktitle = {Proceedings of the 2nd International Conference on E-Commerce, E-Business and E-Government},
pages = {47–53},
numpages = {7},
keywords = {on-demand-and-sharing economy, vulnerabilities, cloud storage application, threats, cloud computing},
location = {Hong Kong, Hong Kong},
series = {ICEEG '18}
}

@inproceedings{10.1145/3453187.3453396,
author = {Yang, QiZhen and Xie, XiaoLan},
title = {Research on Cloud Computing Task Scheduling Based on Improved Evolutionary Algorithm},
year = {2021},
isbn = {9781450389099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453187.3453396},
doi = {10.1145/3453187.3453396},
abstract = {In the research of cloud computing, the advantages and disadvantages of cloud task scheduling algorithm will affect the operation efficiency and service quality of the whole cloud computing system. Evolutionary algorithm is the sum of a series of specific algorithms inspired by the phenomenon of biological evolution in nature. One of the common points of these algorithms is that individuals must be mutated according to certain rules in the running process, so as to avoid falling into local optimum. In order to improve the efficiency of cloud task scheduling in cloud computing, this paper proposes a new mutation strategy which changes the genetic algorithm in evolutionary algorithm. It uses cloudsim platform to simulate cloud task scheduling in cloud computing, and uses particle swarm optimization algorithm to optimize its parameters. The experimental results show that the proposed evolutionary algorithm with improved mutation strategy has the function of cloud task scheduling, and its performance is also improved after the parameters are optimized by particle swarm optimization algorithm. The proposed algorithm improves the mutation step and explores the essence of mutation in evolutionary algorithm, which provides a reference for other research.},
booktitle = {Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science},
pages = {566–572},
numpages = {7},
keywords = {Particle swarm optimization algorithm, Cloud computing, Task scheduling, Evolutionary algorithm, Cloudsim},
location = {Wuhan, China},
series = {EBIMCS '20}
}

@inproceedings{10.1145/3447654.3447669,
author = {WANG, XIANZHI and HUANG, PINGGUO and ISHIBASHI, YUTAKA and OKUDA, TAKASHI and WATANABE, HISTOSHI},
title = {Influence of Network Delay on QoS Control Using Neural Network in Remote Robot Systems with Force Feedback},
year = {2021},
isbn = {9781450388566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447654.3447669},
doi = {10.1145/3447654.3447669},
abstract = {In this paper, we focus on the application of big data, cloud computing, and AI (Artificial Intelligence) to QoS (Quality of Service) control to remote robot systems with force feedback. As the first step of our research, we investigate the influence of cloud delay on the remote robot systems while using big data, cloud computing, and AI technology by experiment. In the experiment, we deal with a task in which two robot arms of the two remote robot systems grasp an object and carry the object together. By using big data, cloud computing, and neural network, we predict the optimum value for the robot position control using force information, which we previously proposed as QoS control, in the system, and investigate the influence of cloud delay. Experimental results show that our method is effective, and the feedback force becomes larger as the delay increases.},
booktitle = {Proceedings of the 2020 9th International Conference on Networks, Communication and Computing},
pages = {104–111},
numpages = {8},
keywords = {QoS control, Experiment, Robot position control, Force feedback, Remote robot systems, Neural network},
location = {Tokyo, Japan},
series = {ICNCC '20}
}

@inproceedings{10.5555/3233397.3233523,
author = {Kirsal, Yonal and Ever, Yoney Kirsal and Mostarda, Leonardo and Gemikonakli, Orhan},
title = {Analytical Modelling and Performability Analysis for Cloud Computing Using Queuing System},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {In recent years, cloud computing becomes a new computing model emerged from the rapid development of the internet. Users can reach their resources with high flexibility using the cloud computing systems all over the world. However, such systems are prone to failures. In order to obtain realistic quality of service (QoS) measurements, failure and recovery behaviours of the system should be considered. System's failures and repairs are associated with availability context in QoS measurements. In this paper, performance issues are considered with the availability of the system. Markov Reward Model (MRM) method is used to get QoS measurements. The mean queue length (MQL) results are calculated using the MRM. The results explicitly show that failures and repairs affect the system performance significantly.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {643–647},
numpages = {5},
keywords = {quality of service, queuing system, analytical modelling, performability analysis, cloud computing},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1145/2590651.2590675,
author = {Coutinho, Emanuel F. and Moreira, Leonardo O. and Paillard, Gabriel A. L. and Maia, Jos\'{e} G. R.},
title = {How to Deploy a Virtual Learning Environment in the Cloud?},
year = {2014},
isbn = {9781450324359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590651.2590675},
doi = {10.1145/2590651.2590675},
abstract = {Cloud computing is a trend of technology aimed at providing on-demand services with payment based on usage. Virtual Learning Environments (VLEs) are applications that require a highly scalable architecture that provides for its users an acceptable level of Quality of Service (QoS). This work aims to show the steps needed to install a VLE in a cloud computing infrastructure. The VLE's migration to this new type of execution environment allows the increase of its use but also brings some performance issues that must be considered. The case study will consider the Moodle VLE which was chosen for its widespread use.},
booktitle = {Proceedings of the 7th Euro American Conference on Telematics and Information Systems},
articleno = {25},
numpages = {4},
keywords = {virtual learning environment, moodle, cloud computing},
location = {Valparaiso, Chile},
series = {EATIS '14}
}

@proceedings{10.1145/2755979,
title = {VTDC '15: Proceedings of the 8th International Workshop on Virtualization Technologies in Distributed Computing},
year = {2015},
isbn = {9781450335737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {During the past few years, we have begun to see a convergence of cloud computing and high performance computing (HPC) infrastructures, technologies, and applications. In HPC, applications have since long predominantly been parallel batch jobs with execution times measured in hours or even days, managed by mature batch and scheduling systems developed and refined over decades. With the introduction of clouds, providing elastic capacity and new programming models for internet-type applications, also traditional HPC users have begun to explore new methods to solve their problems. Cloud applications often come with large numbers of shorter tasks, frequently pipelined and sometimes combined with long-running service-type application components, not too different from what has been seen in HPC since long. Building on previous successful and highly attended VTDC workshops, this 8th edition is a forum to dwell from these synergies and exchange ideas among researchers in the broad area of virtualization technologies in distributed computing in order to further the forefronts of both HPC and cloud computing.The technical program is what defines the workshop. For the establishment of the program, we are grateful to all authors and to the program committee that has provided each paper with an average of over 6 high quality reviews, hopefully contributing both to the paper quality and to each author's future research. We thank the invited speakers, Dr. John Russell Lange (University of Pittsburgh, USA), Dr. Abhishek Gupta (Intel Corp, USA), and Prof. Guillaume Pierre (IRISA / Rennes 1 University, France), for their presentations, each highlighting a different aspect on the convergence of HPC and cloud computing.},
location = {Portland, Oregon, USA}
}

@inproceedings{10.1145/3328020.3353936,
author = {Gao, Zhijun and Gao, Yuxin and Xu, Jingjing},
title = {Designing Metrics to Evaluate the Help Center of Baidu Cloud},
year = {2019},
isbn = {9781450367905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328020.3353936},
doi = {10.1145/3328020.3353936},
abstract = {Help centers are mainly designed to assist users with their product uses. The question as to how we measure the quality of a help center remains unanswered. As the first step of a joint research initiated by Peking University and Baidu Cloud that aims to develop a set of computable metrics to evaluate the quality of help centers, this experience report shares the results of data analysis on correlation between user behavioral data and technical documentation quality. The documents and data we use are a suite of cloud computing services provided by Baidu Cloud. The report begins with an introduction of the research goal; following reviews on the related work, it then lays out the design of the experiments with user data collected from Baidu Cloud. In our experiments, we categorize all documents into three groups and try to identify which metrics would affect documentation quality most. The result shows that the key index that contributes most to the model is PV/UV. At last, the report concludes with our current experimental efforts and future work in our plan.},
booktitle = {Proceedings of the 37th ACM International Conference on the Design of Communication},
articleno = {28},
numpages = {7},
keywords = {help center evaluation, technical information, quality evaluation, web metrics},
location = {Portland, Oregon},
series = {SIGDOC '19}
}

@inproceedings{10.1145/3412841.3441899,
author = {Torquato, Matheus and Maciel, Paulo and Vieira, Marco},
title = {Analysis of VM Migration Scheduling as Moving Target Defense against Insider Attacks},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441899},
doi = {10.1145/3412841.3441899},
abstract = {As cybersecurity threats evolve, cloud computing defenses must adapt to face new challenges. Unfortunately, due to resource sharing, cloud computing platforms open the door for insider attacks, which consist of malicious actions from cloud authorized users (e.g., clients of an Infrastructure-as-a-Service (IaaS) cloud) targeting the co-hosted users or the underlying provider environment. Virtual machine (VM) migration is a Moving Target Defense (MTD) technique to mitigate insider attacks effects, as it provides VMs positioning manageability. However, there is a clear demand for studies quantifying the security benefits of VM migration-based MTD considering different system architecture configurations. This paper tries to fill such a gap by presenting a Stochastic Reward Net model for the security evaluation of a VM migration-based MTD. The security metric of interest is the probability of attack success. We consider multiple architectures, ranging from one physical machine pool (without MTD) up to four physical machine pools. The evaluation also considers the unavailability due to VM migration. The key contributions are i) a set of results highlighting the probability of insider attacks success over time in different architectures and VM migration schedules, and ii) suggestions for selecting VMs as candidates for MTD deployment based on the tolerance levels of the attack success probability. The results are validated against simulation results to confirm the accuracy of the model.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {194–202},
numpages = {9},
keywords = {moving target defense, stochastic petri nets, availability, VM migration, migration-based dynamic platform},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/2872550.2872554,
author = {Yu, Ning and Gu, Feng and Guo, Xuan and He, Zaobo},
title = {A Fine-Grained Flow Control Model for Cloud-Assisted Data Broadcasting},
year = {2015},
isbn = {9781510801004},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Cloud-assisted data broadcasting is an emerging application where cloud computing assists data broadcasting to extend the capacity of system computing and improve the interactivity of the conventional media. However, with the increase in scale, it brings the difficulty on the complexity to provide the sufficient quality of service for diverse receivers. In order to obtain a fine-grained flow rate as well as the system stability, we propose a model based on parallel scheduling, fair queue and Proportional-Integral-Derivative (PID) controller to cope with these challenges. PID controller takes advantage of the feedback of the statistical output stream and automatically adjusts the transmission flow so that the system can achieve the fine-grained multiplexing performance. Meanwhile, we adopt a set of novel metrics to monitor and measure the quality of flow control in order to weaken the negative impact of coarse-grained flow to user-end devices to the minimum level. Extensive simulations and evaluations have illustrated the superiority of the proposed model in the performance and the quality of service in terms of proposed measurement metrics.},
booktitle = {Proceedings of the 18th Symposium on Communications \&amp; Networking},
pages = {24–31},
numpages = {8},
keywords = {impact energy, fine-grained flow control, time division multiplexing, proportional-integral-derivative (PID) controller, impact power, heterogeneous network, user-end devices, fair queue, quality of service, cloud-assisted data broadcasting, energy metric},
location = {Alexandria, Virginia},
series = {CNS '15}
}

@inproceedings{10.1109/CCGrid.2016.83,
author = {Farias, Victor A. E. and Sousa, Fl\'{a}vio R. C. and Maia, Jos\'{e} G. R. and Gomes, Jo\~{a}o P. P. and Machado, Javam C.},
title = {Machine Learning Approach for Cloud NoSQL Databases Performance Modeling},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.83},
doi = {10.1109/CCGrid.2016.83},
abstract = {Cloud computing is a successful, emerging paradigm that supports on-demand services with pay-as-you-go model. With the exponential growth of data, NoSQL databases have been used to manage data in the cloud. In these newly emerging settings, mechanisms to guarantee Quality of Service heavily relies on performance predictability, i.e., the ability to estimate the impact of concurrent query execution on the performance of individual queries in a continuously evolving workload. This paper presents a performance modeling approach for NoSQL databases in terms of performance metrics which is capable of capturing the non-linear effects caused by concurrency and distribution aspects. Experimental results confirm that our performance modeling can accurately predict mean response time measurements under a wide range of workload configurations.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {617–620},
numpages = {4},
keywords = {performance modeling, cloud computing, NoSQL},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/2695664.2695921,
author = {Silva, Francisco Airton and Maciel, Paulo and Filho, Gileno and Matos, Rubens},
title = {A Scheduler for Mobile Cloud Based on Weighted Metrics and Dynamic Context Evaluation},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695921},
doi = {10.1145/2695664.2695921},
abstract = {Resource scarcity is a major obstacle for many mobile applications, since devices have limited energy power and processing potential. As an example, there are applications that seamlessly augment human cognition and typically require resources that far outstrip mobile hardware's capabilities, such as language translation, speech recognition, and face recognition. The use of cloud computing may tackle this problem. This study presents SmartRank, a scheduling framework to perform load partitioning and offloading for mobile applications using cloud computing to increase performance in terms of response time. We first explore a benchmarking of face recognition application using mobile cloud and confirms its suitability to be used as case study with SmartRank. We have applied the approach to a face recognition process based on two strategies: cloudlet federation and resource ranking through balanced metrics (level of CPU utilization and round-trip time). Second, using a full factorial experimental design we tuned the SmartRank with the most suitable partitioning decision calibrating scheduling parameters. Nevertheless, SmartRank uses an equation that is extensible to include new parameters and make it applicable to other scenarios.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {569–576},
numpages = {8},
keywords = {performance evaluation, mobile cloud computing, offloading, partitioning},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3267357.3267362,
author = {Arias-Cabarcos, Patricia and Almen\'{a}rez, Florina and D\'{\i}az-S\'{a}nchez, Daniel and Mar\'{\i}n, Andr\'{e}s},
title = {FRiCS: A Framework for Risk-Driven Cloud Selection},
year = {2018},
isbn = {9781450359887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267357.3267362},
doi = {10.1145/3267357.3267362},
abstract = {Our devices and interactions in a world where physical and digital realities are more and more blended, generate a continuum of multimedia data that needs to be stored, shared and processed to provide services that enrich our daily lives. Cloud computing plays a key role in these tasks, dissolving resource allocation and computational boundaries, but it also requires advanced security mechanisms to protect the data and provide privacy guarantees. Therefore, security assurance must be evaluated before offloading tasks to a cloud provider, a process which is currently manual, complex and inadequate for dynamic scenarios. However, though there are many tools for evaluating cloud providers according to quality of service criteria, automated categorization and selection based on risk metrics is still challenging. To address this gap, we present FRiCS, a Framework for Risk-driven Cloud Selection, which contributes with: 1) a set of cloud security metrics and risk-based weighting policies, 2) distributed components for metric extraction and aggregation, and 3) decision-making plugins for ranking and selection. We have implemented the whole system and conducted a case-study validation based on public cloud providers' security data, showing the benefits of the proposed approach.},
booktitle = {Proceedings of the 2nd International Workshop on Multimedia Privacy and Security},
pages = {18–26},
numpages = {9},
keywords = {security metrics, cloud computing, cloud-based multimedia systems, decision making, risk-driven security},
location = {Toronto, Canada},
series = {MPS '18}
}

@inproceedings{10.1145/2797143.2797145,
author = {Stephanakis, Ioannis M. and Chochliouros, Ioannis P. and Sfakianakis, Evangelos and Shirazi, Noorulhassan},
title = {Anomaly Detection In Secure Cloud Environments Using a Self-Organizing Feature Map (SOFM) Model For Clustering Sets of R-Ordered Vector-Structured Features},
year = {2015},
isbn = {9781450335805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797143.2797145},
doi = {10.1145/2797143.2797145},
abstract = {Cloud computing delivers services over virtualized networks to many end-users. Cloud services are characterized by such attributes as on-demand self-service, broad network access, resource pooling, rapid and elastic resource provisioning and metered services of various qualities. Cloud networks provide data as well as multimedia and video services. Cloud computing for critical structure IT is a relative new area of potential applications. Cloud networks are classified into private cloud networks, public cloud networks and hybrid cloud networks. Anomaly detection systems are defined as a branch of intrusion detection systems that deal with identifying anomalous events with respect to normal system behavior. A novel application of a Self-Organizing-Feature Map (SOFM) of reduced/aggregate sets of ordered vector structured features that are used for detecting anomalies in the context of secure cloud environments is herein proposed. Multivalue inputs consist of reduced/aggregate ordered sets of vector and binary features. The nodes of the SOFM - after training - are indicative of local distributions of feature measurements during normal cloud operation. Anomalies are detected as outliers of the trained SOFM. Each structured vector consists of binary as well as histogram data. The aggregated Canberra distance is used to order histogram data whereas the Jaccard distance is used for multivalue binary data. The so-called Cross-Order Distance Matrix is defined for both cases. The distance depends upon the selection of a similarity/distance measure and a method for operating upon the elements of the Cross-Order Distance Matrix. Several methods of estimating the distance between two ordered sets of features are investigated in the course of this paper.},
booktitle = {Proceedings of the 16th International Conference on Engineering Applications of Neural Networks (INNS)},
articleno = {27},
numpages = {9},
keywords = {Canberra distance, Jaccard distance, Reduced/aggregate-ordering, Secure cloud networks, Self-Organizing Feature Maps (SOFMs), clustering, intrusion detection},
location = {Rhodes, Island, Greece},
series = {EANN '15}
}

@inproceedings{10.1145/3090354.3090367,
author = {Aladwani, Tahani},
title = {Impact of Selecting Virtual Machine with Least Load on Tasks Scheduling Algorithms in Cloud Computing},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090367},
doi = {10.1145/3090354.3090367},
abstract = {Tasks scheduling algorithms consider the first and basic factors in controlling the cloud computing performance. In this paper, we attempt to improve scheduling algorithm's performance by a focus on the load balance factor due to its impact on distributing tasks across multiple virtual machine (VMs) to get best resources utilization, reducing waiting and execution time and enhancing cloud computing performance. This attempt to improve scheduling algorithm's performance by proposing a new strategy called selecting VM with least load (SVLL) can be applied in conjunction with any task scheduling algorithm to improve algorithms performance and increase its load balance. SVLL based on calculating the total load in each VM without taking in consideration number of tasks assigned to it. In order to measure the performance achieved by this method, it will be applied on a set of simple scheduling algorithms, such as First Come First Service (FCFS), Shortest Job First (SJF), and Max-Min scheduling algorithms.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {13},
numpages = {7},
keywords = {Cloud Computing, load balance, Task scheduling algorithms},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3447545.3451190,
author = {Henning, S\"{o}ren and Hasselbring, Wilhelm},
title = {How to Measure Scalability of Distributed Stream Processing Engines?},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451190},
doi = {10.1145/3447545.3451190},
abstract = {Scalability is promoted as a key quality feature of modern big data stream processing engines. However, even though research made huge efforts to provide precise definitions and corresponding metrics for the term scalability, experimental scalability evaluations or benchmarks of stream processing engines apply different and inconsistent metrics. With this paper, we aim to establish general metrics for scalability of stream processing engines. Derived from common definitions of scalability in cloud computing, we propose two metrics: a load capacity function and a resource demand function. Both metrics relate provisioned resources and load intensities, while requiring specific service level objectives to be fulfilled. We show how these metrics can be employed for scalability benchmarking and discuss their advantages in comparison to other metrics, used for stream processing engines and other software systems.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {85–88},
numpages = {4},
keywords = {cloud computing, stream processing, scalability, metrics},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3339186.3339207,
author = {Xia, Qiufen and Bai, Luyao and Liang, Weifa and Xu, Zichuan and Yao, Lin and Wang, Lei},
title = {QoS-Aware Proactive Data Replication for Big Data Analytics in Edge Clouds},
year = {2019},
isbn = {9781450371964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339186.3339207},
doi = {10.1145/3339186.3339207},
abstract = {We are in the era of big data and cloud computing, large quantity of computing resource is desperately needed to detect invaluable information hidden in the coarse big data through query evaluation. Users demand big data analytic services with various Quality of Service (QoS) requirements. However, cloud computing is facing new challenges in meeting stringent QoS requirements of users due to the remoteness from its users. Edge computing has emerged as a new paradigm to address such shortcomings by bringing cloud services to the edge of the operation network in proximity of users for performance improvement. To satisfy the QoS requirements of users for big data analytics in edge computing, the data replication and placement problem must be properly dealt with such that user requests can be efficiently and promptly responded. In this paper, we consider data replication and placement for big data analytic query evaluation. We first cast a novel proactive data replication and placement problem of big data analytics in a two-tier edge cloud environment, we then devise an approximation algorithm with an approximation ratio for it, we finally evaluate the proposed algorithm against existing benchmarks, using both simulation and experiment in a testbed based on real datasets, the evaluation results show that the proposed algorithm is promising.},
booktitle = {Workshop Proceedings of the 48th International Conference on Parallel Processing},
articleno = {26},
numpages = {10},
keywords = {Data replication and placement, query evaluation, big data analytics, edge clouds},
location = {Kyoto, Japan},
series = {ICPP Workshops '19}
}

@inproceedings{10.1145/3147213.3149214,
author = {Aske, Austin and Zhao, Xinghui},
title = {An Actor-Based Framework for Edge Computing},
year = {2017},
isbn = {9781450351492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147213.3149214},
doi = {10.1145/3147213.3149214},
abstract = {The Actor model provides inherent parallelism, along with other convenient features to build large-scale distributed systems. In this paper, we present ActorEdge, an Actor based distributed framework for edge computing. ActorEdge provides straitforward integration with existing technologies, while enabling application developers to dynamically utilize computational resources on the edge of the clouds. ActorEdge has proven to outperform cloud computing options by providing superior quality of service, measuring a 10x lower latency, 30\% less jitter, and greater bandwidth. Using this framework, programmers can easily develop and deploy their applications on a heterogeneous system, including cloud servers/data centers, edge servers, and mobile devices.},
booktitle = {Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {199–200},
numpages = {2},
keywords = {actors, edge computing, cloud computing, mobile clouds},
location = {Austin, Texas, USA},
series = {UCC '17}
}

@inproceedings{10.1145/3132847.3133045,
author = {Fang, Zhou and Yu, Tong and Mengshoel, Ole J. and Gupta, Rajesh K.},
title = {QoS-Aware Scheduling of Heterogeneous Servers for Inference in Deep Neural Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133045},
doi = {10.1145/3132847.3133045},
abstract = {Deep neural networks (DNNs) are popular in diverse fields such as computer vision and natural language processing. DNN inference tasks are emerging as a service provided by cloud computing environments. However, cloud-hosted DNN inference faces new challenges in workload scheduling for the best Quality of Service (QoS), due to dependence on batch size, model complexity and resource allocation. This paper represents the QoS metric as a utility function of response delay and inference accuracy. We first propose a simple and effective heuristic approach that keeps low response delay and satisfies the requirement on processing throughput. Then we describe an advanced deep reinforcement learning (RL) approach that learns to schedule from experience. The RL scheduler is trained to maximize QoS, using a set of system statuses as the input to the RL policy model. Our approach performs scheduling actions only when there are free GPUs, thus reduces scheduling overhead over common RL schedulers that run at every continuous time step. We evaluate the schedulers on a simulation platform and demonstrate the advantages of RL over heuristics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2067–2070},
numpages = {4},
keywords = {deep neural networks inference, reinforcement learning, deep reinforcement learning, web service, qos aware scheduling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/2668930.2688818,
author = {Lehrig, Sebastian and Becker, Steffen},
title = {The CloudScale Method for Software Scalability, Elasticity, and Efficiency Engineering: A Tutorial},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688818},
doi = {10.1145/2668930.2688818},
abstract = {In cloud computing, software engineers design systems for virtually unlimited resources that cloud providers account on a pay-per-use basis. Elasticity management systems provision these resource autonomously to deal with changing workloads. Such workloads call for new objective metrics allowing engineers to quantify quality properties like scalability, elasticity, and efficiency. However, software engineers currently lack engineering methods that aid them in engineering their software regarding such properties. Therefore, the CloudScale project developed tools for such engineering tasks. These tools cover reverse engineering of architectural models from source code, editors for manual design/adaption of such models, as well as tools for the analysis of modeled and operating software regarding scalability, elasticity, and efficiency. All tools are interconnected via ScaleDL, a common architectural language, and the CloudScale Method that leads through the engineering process. In this tutorial, we execute our method step-by-step such that every tool and ScaleDL are briefly introduced.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {329–331},
numpages = {3},
keywords = {elasticity, cloud computing, tutorial, metrics, cloudscale, scalability, engineering, efficiency, software analysis, method},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1109/UCC.2014.49,
author = {Keller, Matthias and Robbert, Christoph and Karl, Holger},
title = {Template Embedding: Using Application Architecture to Allocate Resources in Distributed Clouds},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.49},
doi = {10.1109/UCC.2014.49},
abstract = {In distributed cloud computing, application deployment across multiple sites can improve quality of service. Recent research developed algorithms to find optimal locations for virtual machines. However, those algorithms assume to have either single-tier applications or a fixed number of virtual machines--a strong simplification of reality. This paper investigates the placement and scaling of complex application architectures. An application is dynamically scaled to fit both the current demand situation and the currently available infrastructure resources. We compare two approaches: The first one is based on virtual network embedding. The second approach is a novel method called Template Embedding. It is based on a hierarchical 1-allocation hub flow problem and combines application scaling and embedding in one step. Extensive experiments on 43200 network configurations showed that Template Embedding outperforms virtual network embedding in all cases in three metrics: success rate, solution quality, and runtime. This positive result shows that template embedding is a promising approach for distributed cloud resource allocation.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {387–395},
numpages = {9},
keywords = {Flow Problem, Cloud Resource Allocation, Application Architecture, Hub Problem, Distributed Cloud Computing},
series = {UCC '14}
}

@inproceedings{10.1109/CCGrid.2015.152,
author = {Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
title = {Modeling Cross-Architecture Co-Tenancy Performance Interference},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.152},
doi = {10.1109/CCGrid.2015.152},
abstract = {Cloud computing has become a dominant computing paradigm to provide elastic, affordable computing resources to end users. Due to the increased computing power of modern machines powered by multi/many-core computing, data centers often co-locate multiple virtual machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing and competition. Applications or VMs co-locating in one physical machine can interfere with each other despite of the promise of performance isolation through virtualization. Modeling and predicting co-run interference therefore becomes critical for data center job scheduling and QoS (Quality of Service) assurance. Co-run interference can be categorized into two metrics, sensitivity and pressure, where the former denotes how an application's performance is affected by its co-run applications, and the latter measures how it impacts the performance of its co-run applications. This paper shows that sensitivity and pressure are both application- and architecture-dependent. Further, we propose a regression model that predicts an application's sensitivity and pressure across architectures with high accuracy. This regression model enables a data center scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate with another VMs/applications.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {231–240},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@proceedings{10.1145/2737182,
title = {QoSA '15: Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 11th International ACM Sigsoft Conference on the Quality of Software Architectures -- QoSA 2015. For more than a decade, QoSA has strived to advance the state of the art of quality aspects of software architecture, focusing broadly on its quality characteristics and how these relate to the design of software architectures. Specific issues of interest are defining and modeling quality measures, evaluating and managing architecture quality, linking architecture to requirements and implementation, and preserving architecture quality throughout the system lifetime. Past themes for QoSA include Architecting for Adaptivity (2014), The System View (2013), Evolving Architectures (2012), Quality throughout the Software Lifecycle (2011), and Research into Practice -- Reality and Gaps (2010).QoSA 2015's theme is "Software Architecture for the 4th Industrial Revolution". After mechanization, mass production, and electronics, the Internet is about to enable a new level of productivity in manufacturing. This shall be enabled by smart cyber-physical systems connected to cloud computing services and communicating using standardized semantics. In the near future, industrial big data analytics on monitored sensor data shall improve the efficiency and individualization of production facilities. This year's QoSA conference solicited contributions that explore the various implications of this upcoming industrial revolution on software architecture. This included reference architectures, software architectures adapting at run time, architecture styles and patterns for cyber-physical and distributed systems.The call for papers attracted 42 initial submissions from Asia, North America, Africa, and Europe and 28 final submissions were considered during the review process. The program committee accepted 11 full papers and 2 short papers that cover topics, such as new architecture modeling approaches, architectural tactics for mobile computing, cloud computing architectures, and cyberphysical systems. QoSA's 2015 proceedings also include 2 papers from the WCOP 2015, the 20th International Doctoral Symposium on Components and Architecture.QoSA 2015 is part of the federated events on component-based software engineering and software architecture (CompArch 2015), which include WICSA 2015 (12th Working IEEE / IFIP Conference on Software Architecture) and CBSE 2015 (18th International ACM SIGSOFT Symposium on Component-Based Software Engineering).},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1145/3128128.3128161,
author = {Marwan, M. and Kartit, A. and Ouahmane, H.},
title = {Protecting Medical Data in Cloud Storage Using Fault-Tolerance Mechanism},
year = {2017},
isbn = {9781450352819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128128.3128161},
doi = {10.1145/3128128.3128161},
abstract = {Given the fact that cloud computing offers cost-efficient storage systems, medical organizations are more interested in using this alternative solution to safeguard their patients' data. Equally interestingly, users are charged based typically on the amount of occupied storage space. Basically, this concept is meant to cut costs and improve the quality of healthcare services. Consequently, implementing cloud storage would help clients to manage their data efficiently. Besides, it allows users to outsource the storage process by using virtual storage systems instead of local ones. Despite its significant impact in healthcare domain, adopting this paradigm to save medical data on remote servers poses serious challenges, especially security risks. Currently, various cryptographic techniques have been used to ensure data confidentiality and to avoid data disclosure. Globally, this model uses traditional cryptosystems such as AES, RSA to address security issues in cloud storage. As far as we know, there are only a few works in literature that deal with availability and data recovery in cloud computing. In general, the classical approach which is based on backup or replication is not suitable for cloud environment due to the highly dynamic nature of this model. The intent of this work is to enhance the reliability of cloud storage in order to meet security requirements. In this study, we propose a novel method based on Shamir's Secret Share Scheme and multi-cloud concept to avoid data loss and unauthorized access. More precisely, this technique seeks to divide consumers' data into several portions using Shamir's Secret Share to prevent privacy disclosure. Based on these considerations, we store these created portions in different nodes to minimize security risks, particularly internal attacks. To sum up, this method is designed to ensure fault-tolerance, which is the main subject of this study. In fact, we need just certain shares to reconstruct the secret data rather than using all parts. The experimental results are in accordance with the theoretical assumptions behind this model, and hence, confirm that the proposed framework provides necessary measures for preventing data loss in cloud storage.},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
pages = {214–219},
numpages = {6},
keywords = {fault tolerance, medical image, security, cloud computing},
location = {Rabat, Morocco},
series = {ICSDE '17}
}

@inproceedings{10.1145/3452383.3452385,
author = {Dasgupta, Gargi B.},
title = {AI and Its Applications in the Cloud Strategy},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452385},
doi = {10.1145/3452383.3452385},
abstract = {The fourth industrial revolution identifies cloud computing, data, and artificial intelligence (AI) as opportunity clusters with double digit growth in the next couple of years. As part of the cloud and digital transformation, the role of AI is crucial in enabling that transformation as well as creating the new breed of applications on top. AI mechanisms can help accelerate the modernization of applications, their management, and the testing on cloud architectures. I will focus on two sub-problems: 1) Refactoring of massive monolith applications using AI techniques. This problem statement is particularly relevant in understanding legacy un-optimized code and transforming them to be more cloud-ready. Microservices are indeed becoming the de-facto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently [2]. It also provides natural benefits when deployed on the cloud since resources can be allocated dynamically to necessary components based on demand. We are exploring how AI can help accelerate the transformation of existing applications to microservices. 2) Detecting faults in application behavior at runtime from operational data. This problem statement is particularly relevant in understanding how to manage this new architecture of multiple microservices across the cloud stack [1], [3]. Operational data artifacts span across logs, metrics, tickets, and traces. Looking at signals across the artifacts and across the stack presents a challenging data correlation problem. AI mechanisms can help accelerate problem determination in these complex environments. I will also share my thoughts on how fundamental breakthroughs in AI Research will be needed as we address some of the core problems of cloud computing.},
booktitle = {14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {2},
numpages = {1},
keywords = {hybrid cloud, log anomalies, AI Ops, modernization, code refactoring},
location = {Bhubaneswar, Odisha, India},
series = {ISEC 2021}
}

@article{10.1145/3284553,
author = {Avgeris, Marios and Dechouniotis, Dimitrios and Athanasopoulos, Nikolaos and Papavassiliou, Symeon},
title = {Adaptive Resource Allocation for Computation Offloading: A Control-Theoretic Approach},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3284553},
doi = {10.1145/3284553},
abstract = {Although mobile devices today have powerful hardware and networking capabilities, they fall short when it comes to executing compute-intensive applications. Computation offloading (i.e., delegating resource-consuming tasks to servers located at the edge of the network) contributes toward moving to a mobile cloud computing paradigm. In this work, a two-level resource allocation and admission control mechanism for a cluster of edge servers offers an alternative choice to mobile users for executing their tasks. At the lower level, the behavior of edge servers is modeled by a set of linear systems, and linear controllers are designed to meet the system’s constraints and quality of service metrics, whereas at the upper level, an optimizer tackles the problems of load balancing and application placement toward the maximization of the number the offloaded requests. The evaluation illustrates the effectiveness of the proposed offloading mechanism regarding the performance indicators, such as application average response time, and the optimal utilization of the computational resources of edge servers.},
journal = {ACM Trans. Internet Technol.},
month = {apr},
articleno = {23},
numpages = {20},
keywords = {Edge computing, feedback control, linear modeling}
}

@inproceedings{10.1145/3425269.3425272,
author = {Silva, Jorge Luiz Machado da and de Fran\c{c}a, Breno B. Nicolau and Rubira, Cec\'{\i}lia Mary Fischer},
title = {Generating Trustworthiness Adaptation Plans Based on Quality Models for Cloud Platforms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425272},
doi = {10.1145/3425269.3425272},
abstract = {Cloud computing platforms can offer many benefits related to the provision of service processing and storage for hosting client applications. Trustworthiness can be defined as the trust of a customer in a cloud service and its provider; however, the assurance of this property is not trivial. First, trustworthiness in general is not composed by a single quality attribute, but by the combination of multiple attributes, such as data privacy, performance, reliability, etc. Second, during runtime clients can experience a change of the trustworthiness level required by their application due to the degradation of the cloud service. This article presents a solution that monitors during runtime the set of quality attributes of a specific application and generates adaptation plans in order to certify that an adequate resource amount be provided by the cloud in order to keep its trustworthiness level. Our solution is based on quality models to compute the metric associated to each non-functional requirement and their combination them into different types of trustworthiness levels. The main contribution of the solution is to provide an approach which deals with multiple requirements at the same time (or simultaneously) during runtime in order to adapt the cloud resources to keep the trustworthiness level required by the application. The solution was evaluated by an experiment considering a scenario where the application trustworthiness level was composed by three quality attributes: data privacy, performance and reliability. Initial results have shown that the approach is feasible in terms of the execution of the adaptation plans during runtime to certify the trustworthiness level required by the application.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {141–150},
numpages = {10},
keywords = {Cloud Computing, Self-adaptive Systems, Adaptation Planning, Trustworthiness},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1145/3331148,
author = {Magrofuoco, Nathan and Vanderdonckt, Jean},
title = {Gelicit: A Cloud Platform for Distributed Gesture Elicitation Studies},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {EICS},
url = {https://doi.org/10.1145/3331148},
doi = {10.1145/3331148},
abstract = {A gesture elicitation study, as originally defined, consists of gathering a sample of participants in a room, instructing them to produce gestures they would use for a particular set of tasks, materialized through a representation called referent, and asking them to fill in a series of tests, questionnaires, and feedback forms. Until now, this procedure is conducted manually in a single, physical, and synchronous setup. To relax the constraints imposed by this manual procedure and to support stakeholders in defining and conducting such studies in multiple contexts of use, this paper presents Gelicit, a cloud computing platform that supports gesture elicitation studies distributed in time and space structured into six stages: (1) define a study: a designer defines a set of tasks with their referents for eliciting gestures and specifies an experimental protocol by parameterizing its settings; (2) conduct a study: any participant receiving the invitation to join the study conducts the experiment anywhere, anytime, anyhow, by eliciting gestures and filling forms; (3) classify gestures: an experimenter classifies elicited gestures according to selected criteria and a vocabulary; (4) measure gestures: an experimenter computes gesture measures, like agreement, frequency, to understand their configuration; (5) discuss gestures: a designer discusses resulting gestures with the participants to reach a consensus; (6) export gestures: the consensus set of gestures resulting from the discussion is exported to be used with a gesture recognizer. The paper discusses Gelicit advantages and limitations with respect to three main contributions: as a conceptual model for gesture management, as a method for distributed gesture elicitation based on this model, and as a cloud computing platform supporting this distributed elicitation. We illustrate Gelicit through a study for eliciting 2D gestures executing Internet of Things tasks on a smartphone.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {6},
numpages = {41},
keywords = {elicitation technique, workflow analysis, gesture elicitation study, gesture interaction}
}

@inproceedings{10.1145/3316615.3316622,
author = {Ming, Fan Xiu and Habeeb, Riyaz Ahamed Ariyaluran and Md Nasaruddin, Fariza Hanum Binti and Gani, Abdullah Bin},
title = {Real-Time Carbon Dioxide Monitoring Based on IoT \&amp; Cloud Technologies},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316622},
doi = {10.1145/3316615.3316622},
abstract = {In recent years, environment monitoring are of greater importance towards the area of climate monitoring, analysis, agricultural productivity management, quality assurance of water, air, alongside with other potential factors that are closely connected to industrial development and convenience of living. This research is motivated by creating awareness of smart home residents on indoor air quality, as well as providing insight of carbon dioxide emissions for industries and environmental organizations.This paper proposes an efficient solution towards environment monitoring of carbon dioxide integrated with Internet of Things capability and cloud computing technology. Aforementioned techniques will deliver highly accessible and real-time data visualization which would be greatly beneficial for Smart Homes efficiency of analysis actualization and counter-measures deployment. A monitoring architecture was developed to generate, accumulate, store and visualize carbon dioxide concentration using MQ135 carbon dioxide sensor, ESP8266 Wi-Fi module, Firebase Cloud Storage Service and Android mobile application Carbon Insight for data visualization. 2880 data points in the time frame of 10 days with a 30-second interval was collected, stored and visualized with the application of this system.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {517–521},
numpages = {5},
keywords = {environment monitoring, cloud, Internet of things},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@inproceedings{10.1145/2857546.2857552,
author = {Anand, Priya and Ryoo, Jungwoo and Kim, Hyoungshick and Kim, Eunhyun},
title = {Threat Assessment in the Cloud Environment: A Quantitative Approach for Security Pattern Selection},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857552},
doi = {10.1145/2857546.2857552},
abstract = {Cloud computing has emerged as a fast-growing technology in the past few years. It provides a great flexibility for storing, sharing and delivering data over the Internet without investing on new technology or resources. In spite of the development and wide array of cloud usage, security perspective of cloud computing still remains its infancy. Security challenges faced by cloud environment becomes more complicated when we include various stakeholders' perspectives. In a cloud environment, security perspectives and requirements are usually designed by software engineers or security experts. Sometimes clients' requirements are either ignored or given a very high importance. In order to implement cloud security by providing equal importance to client organizations, software engineers and security experts, we propose a new methodology in this paper. We use Microsoft's STRIDE-DREAD model to assess threats existing in the cloud environment and also to measure its consequences. Our aim is to rank the threats based on the nature of its severity, and also giving a significant importance for clients' requirements on security perspective. Our methodology would act as a guiding tool for security experts and software engineers to proceed with securing process especially for a private or a hybrid cloud. Once threats are ranked, we provide a link to a well-known security pattern classification. Although we have some security pattern classification schemes in the literature, we need a methodology to select a particular category of patterns. In this paper, we provide a novel methodology to select a set of security patterns for securing a cloud software. This methodology could aid a security expert or a software professional to assess the current vulnerability condition and prioritize by also including client's security requirements in a cloud environment.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {5},
numpages = {8},
keywords = {Cloud Computing, Risk Analysis, STRIDE-DREAD Model, Security Patterns, Threat Assessment},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@article{10.1145/3236332,
author = {Herbst, Nikolas and Bauer, Andr\'{e} and Kounev, Samuel and Oikonomou, Giorgos and Eyk, Erwin Van and Kousiouris, George and Evangelinou, Athanasia and Krebs, Rouven and Brecht, Tim and Abad, Cristina L. and Iosup, Alexandru},
title = {Quantifying Cloud Performance and Dependability: Taxonomy, Metric Design, and Emerging Challenges},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3236332},
doi = {10.1145/3236332},
abstract = {In only a decade, cloud computing has emerged from a pursuit for a service-driven information and communication technology (ICT), becoming a significant fraction of the ICT market. Responding to the growth of the market, many alternative cloud services and their underlying systems are currently vying for the attention of cloud users and providers. To make informed choices between competing cloud service providers, permit the cost-benefit analysis of cloud-based systems, and enable system DevOps to evaluate and tune the performance of these complex ecosystems, appropriate performance metrics, benchmarks, tools, and methodologies are necessary. This requires re-examining old system properties and considering new system properties, possibly leading to the re-design of classic benchmarking metrics such as expressing performance as throughput and latency (response time). In this work, we address these requirements by focusing on four system properties: (i) elasticity of the cloud service, to accommodate large variations in the amount of service requested, (ii)&nbsp;performance isolation between the tenants of shared cloud systems and resulting performance variability, (iii)&nbsp;availability of cloud services and systems, and (iv) the operational risk of running a production system in a cloud environment. Focusing on key metrics for each of these properties, we review the state-of-the-art, then select or propose new metrics together with measurement approaches. We see the presented metrics as a foundation toward upcoming, future industry-standard cloud benchmarks.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {aug},
articleno = {19},
numpages = {36},
keywords = {Metrics, cloud, performance variability, performance isolation, elasticity, benchmarking, availability, operational risk}
}

@inproceedings{10.1145/2955193.2955208,
author = {Aman, Mortada A. and \c{C}etinkaya, Egemen K.},
title = {DSB-SEIS: A Deduplicating Secure Backup System with Encryption Intensity Selection},
year = {2016},
isbn = {9781450342209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2955193.2955208},
doi = {10.1145/2955193.2955208},
abstract = {Cloud computing is an emerging service that enables users to store and manage their data easily at a low cost. We propose a Deduplicating Secure Backup System with Encryption Intensity Selection (DSB-SEIS) that combines features to amend security and performance of cloud-based backup services. Our scheme introduces the concept of encryption intensity selection to cloud backup systems, which allows users to select the encryption intensity of their files. We also combine features such as deduplication, assured deletion, and multi-aspect awareness to further enhance our scheme. The DSB-SEIS performance is measured over an OpenStack cloud installed on CloudLab resources demonstrating that DSB-SEIS can improve the backup service.},
booktitle = {Proceedings of the 4th Workshop on Distributed Cloud Computing},
articleno = {11},
numpages = {1},
keywords = {security, cloud, deduplication, CloudLab, backup, integrity},
location = {Chicago, Illinois},
series = {DCC '16}
}

@inproceedings{10.1145/3030207.3030214,
author = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Papadopoulos, Alessandro V. and Ghit, Bogdan and Epema, Dick and Iosup, Alexandru},
title = {An Experimental Performance Evaluation of Autoscaling Policies for Complex Workflows},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030214},
doi = {10.1145/3030207.3030214},
abstract = {Simplifying the task of resource management and scheduling for customers, while still delivering complex Quality-of-Service (QoS), is key to cloud computing. Many autoscaling policies have been proposed in the past decade to decide on behalf of cloud customers when and how to provision resources to a cloud application utilizing cloud elasticity features. However, in prior work, when a new policy is proposed, it is seldom compared to the state-of-the-art, and is often compared only to static provisioning using a predefined QoS target. This reduces the ability of cloud customers and of cloud operators to choose and deploy an autoscaling policy. In our work, we conduct an experimental performance evaluation of autoscaling policies, using as application model workflows, a commonly used formalism for automating resource management for applications with well-defined yet complex structure. We present a detailed comparative study of general state-of-the-art autoscaling policies, along with two new workflow-specific policies. To understand the performance differences between the 7 policies, we conduct various forms of pairwise and group comparisons. We report both individual and aggregated metrics. Our results highlight the trade-offs between the suggested policies, and thus enable a better understanding of the current state-of-the-art.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {75–86},
numpages = {12},
keywords = {workflows, demand, directed acyclic graph, clouds, spec, dag, cloud computing, performance, auto-scaling, scheduling, metrics, workloads, autoscaling, opennebula, elasticity, supply, level of parallelism},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/3386367.3431670,
author = {Sacco, Alessio and Esposito, Flavio and Marchetto, Guido},
title = {A Distributed Reinforcement Learning Approach for Energy and Congestion-Aware Edge Networks},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431670},
doi = {10.1145/3386367.3431670},
abstract = {The abiding attempt of automation has also pervaded computer networks, with the ability to measure, analyze, and control themselves in an automated manner, by reacting to changes in the environment (e.g., demand) while exploiting existing flexibilities. When provided with these features, networks are often referred to as "self-driving". Network virtualization and machine learning are the drivers. In this regard, the provision and orchestration of physical or virtual resources are crucial for both Quality of Service guarantees and cost management in the edge/cloud computing ecosystem. Auto-scaling mechanisms are hence essential to effectively manage the lifecycle of network resources. In this poster, we propose Relevant, a distributed reinforcement learning approach to enable distributed automation for network orchestrators. Our solution aims at solving the congestion control problem within Software-Defined Network infrastructures, while being mindful of the energy consumption, helping resources to scale up and down as traffic demands fluctuate and energy optimization opportunities arise.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {546–547},
numpages = {2},
keywords = {reinforcement learning, self-driving networks, auto-scaling},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@article{10.1145/3460197,
author = {Nemati, Hani and Azhari, Seyed Vahid and Shakeri, Mahsa and Dagenais, Michel},
title = {Host-Based Virtual Machine Workload Characterization Using Hypervisor Trace Mining},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2376-3639},
url = {https://doi.org/10.1145/3460197},
doi = {10.1145/3460197},
abstract = {Cloud computing is a fast-growing technology that provides on-demand access to a pool of shared resources. This type of distributed and complex environment requires advanced resource management solutions that could model virtual machine (VM) behavior. Different workload measurements, such as CPU, memory, disk, and network usage, are usually derived from each VM to model resource utilization and group similar VMs. However, these course workload metrics require internal access to each VM with the available performance analysis toolkit, which is not feasible with many cloud environments privacy policies.In this article, we propose a non-intrusive host-based virtual machine workload characterization using hypervisor tracing. VM blockings duration, along with virtual interrupt injection rates, are derived as features to reveal multiple levels of resource intensiveness. In addition, the VM exit reason is considered, as well as the resource contention rate due to the host and other VMs. Moreover, the processes and threads preemption rates in each VM are extracted using the collected tracing logs. Our proposed approach further improves the selected features by exploiting a page ranking based algorithm to filter non-important processes running on each VM. Once the metric features are defined, a two-stage VM clustering technique is employed to perform both coarse- and fine-grain workload characterization. The inter-cluster and intra-cluster similarity metrics of the silhouette score is used to reveal distinct VM workload groups, as well as the ones with significant overlap. The proposed framework can provide a detailed vision of the underlying behavior of the running VMs. This can assist infrastructure administrators in efficient resource management, as well as root cause analysis.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {jun},
articleno = {4},
numpages = {25},
keywords = {machine learning, time series, workload characterization, PageRank, vCPU states, K-Means, performance analysis, VM clustering, tracing, virtual interrupts}
}

@inproceedings{10.1145/2996890.3007870,
author = {Uhlir, Vojtech and Tomanek, Ondrej and Kencl, Lukas},
title = {Latency-Based Benchmarking of Cloud Service Providers},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.3007870},
doi = {10.1145/2996890.3007870},
abstract = {With the ever-increasing trend of migration of applications to the Cloud environment, there is a growing need to thoroughly evaluate quality of the Cloud service itself, before deciding upon a hosting provider. Benchmarking the Cloud services is difficult though, due to the complex nature of the Cloud Computing setup and the diversity of locations, of applications and of their specific service requirements. However, such comparison may be crucial for decision making and for troubleshooting of services offered by the intermediate businesses - the so-called Cloud tenants. Existing cross-sectional studies and benchmarking methodologies provide only a shallow comparison of Cloud services, whereas state-of-the-art tooling for specific comparisons of application-performance parameters, such as for example latency, is insufficient. In this work, we propose a novel methodology for benchmarking of Cloud-service providers, which is based on latency measurements collected via active probing, and can be tailored to specific application needs. Furthermore, we demonstrate its applicability on a practical longitudinal study of real measurements of two major Cloud-service providers - Amazon and Microsoft.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {263–268},
numpages = {6},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1109/CCGRID.2017.39,
author = {Evangelidis, Alexandros and Parker, David and Bahsoon, Rami},
title = {Performance Modelling and Verification of Cloud-Based Auto-Scaling Policies},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.39},
doi = {10.1109/CCGRID.2017.39},
abstract = {Auto-scaling, a key property of cloud computing, allows application owners to acquire and release resources on demand. However, the shared environment, along with the exponentially large configuration space of available parameters, makes configuration of auto-scaling policies a challenging task. In particular, it is difficult to quantify, a priori, the impact of a policy on Quality of Service (QoS) provision. To address this problem, we propose a novel approach based on performance modelling and formal verification to produce performance guarantees on particular rule-based auto-scaling policies. We demonstrate the usefulness and efficiency of our model through a detailed validation process on the Amazon EC2 cloud, using two types of load patterns. Our experimental results show that it can be very effective in helping a cloud application owner configure an auto-scaling policy in order to minimise the QoS violations.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {355–364},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/2910017.2910602,
author = {Slivar, Ivan and Skorin-Kapov, Lea and Suznjevic, Mirko},
title = {Cloud Gaming QoE Models for Deriving Video Encoding Adaptation Strategies},
year = {2016},
isbn = {9781450342971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910017.2910602},
doi = {10.1145/2910017.2910602},
abstract = {Cloud gaming has been recognized as a promising shift in the online game industry, with the aim being to deliver high-quality graphics games to any type of end user device. The concepts of cloud computing are leveraged to render the game scene as a video stream which is then delivered to players in real-time. Given high bandwidth and strict latency requirements, a key challenge faced by cloud game providers lies in configuring the video encoding parameters so as to maximize player Quality of Experience (QoE) while meeting bandwidth availability constraints. In this paper we address this challenge by conducting a subjective laboratory study involving 52 players and two different games aimed at identifying QoE-driven video encoding adaptation strategies. Empirical results are used to derive analytical QoE estimation models as functions of bitrate and framerate, while also taking into account game type and player skill. Results have shown that under certain identified bandwidth conditions, reductions of framerate lead to QoE improvements due to improved graphics quality. Given that results indicate that different QoE-driven video adaptation policies should likely be applied for different types of games, we further report on objective video metrics that may be used to classify games for the purpose of choosing an appropriate and QoE-driven video codec configuration strategy.},
booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
articleno = {18},
numpages = {12},
keywords = {QoE modeling, cloud gaming QoE, QoE, cloud gaming},
location = {Klagenfurt, Austria},
series = {MMSys '16}
}

@inproceedings{10.1145/3302541.3310294,
author = {Scheuner, Joel and Leitner, Philipp},
title = {Performance Benchmarking of Infrastructure-as-a-Service (IaaS) Clouds with Cloud WorkBench},
year = {2019},
isbn = {9781450362863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302541.3310294},
doi = {10.1145/3302541.3310294},
abstract = {The continuing growth of the cloud computing market has led to an unprecedented diversity of cloud services with different performance characteristics. To support service selection, researchers and practitioners conduct cloud performance benchmarking by measuring and objectively comparing the performance of different providers and configurations (e.g., instance types in different data center regions). In this tutorial, we demonstrate how to write performance tests for IaaS clouds using the Web-based benchmarking tool Cloud WorkBench (CWB). We will motivate and introduce benchmarking of IaaS cloud in general, demonstrate the execution of a simple benchmark in a public cloud environment, summarize the CWB tool architecture, and interactively develop and deploy a more advanced benchmark together with the participants.},
booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {53–56},
numpages = {4},
keywords = {performance, cloud computing, benchmarking},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3005745.3005749,
author = {Mukerjee, Matthew K. and Bozkurt, Ilker Nadi and Maggs, Bruce and Seshan, Srinivasan and Zhang, Hui},
title = {The Impact of Brokers on the Future of Content Delivery},
year = {2016},
isbn = {9781450346610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3005745.3005749},
doi = {10.1145/3005745.3005749},
abstract = {Various trends are reshaping content delivery on the Internet: the explosive growth of traffic due to video, users' increasing expectations for higher quality of experience (QoE), and the proliferation of server capacity from a variety of sources (e.g., cloud computing, content provider-owned datacenters, and ISP-owned CDNs). In order to meet the scale and quality demands imposed by users, content providers have started to spread demand across a variety of CDNs using a broker. Brokers break many traditional CDN assumptions (e.g., unexpected traffic skew, significant variance in demand over short timescales, etc.). Through an analysis of data from a leading broker and a leading CDN, we show the potential challenges and opportunities that brokers impart on content delivery. We take the first steps towards improvement through a redesigned broker-CDN interface.},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
pages = {127–133},
numpages = {7},
location = {Atlanta, GA, USA},
series = {HotNets '16}
}

@inproceedings{10.1145/3463677.3463732,
author = {Ahn, Michael and Chu, Shengli},
title = {What Matters in Maintaining Effective Open Government Data Systems? The Role of Government Managerial Capacity, and Political and Legal Environment},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463732},
doi = {10.1145/3463677.3463732},
abstract = {This paper aims to identify key institutional factors that contribute to effective open data systems. Rapid advancement in new technologies such as machine learning, algorithms, IoT, and Cloud Computing has amplified the importance of national open data systems. The availability of relevant public data has become a crucial factor in creating sophisticated machine learning platforms or algorithms that will have a considerable impact on national competitiveness. Effective national open data strategies will matter in shaping an environment that will facilitate data production, dissemination, and utilization. Using multiple sources of data that measure the qualities of open data systems and various political, governmental, and legal attributes at the national level, we seek to identify key institutional factors that contribute to robust open data policies and outcomes. Our findings point to the importance of the existence of a national open data strategy and support (especially "open by default" strategy), pre-existing e-government capability, and countries operating under full democracy with its guarantees to civil liberties and political freedom. In addition, the nature of the open data matters as different managerial, political, and demographic conditions affected the quality of different open data systems. Policy implications of our findings are discussed.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {444–457},
numpages = {14},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@inproceedings{10.1145/2898445.2898446,
author = {Sun, Degang and Zhang, Jie and Fan, Wei and Wang, Tingting and Liu, Chao and Huang, Weiqing},
title = {SPLM: Security Protection of Live Virtual Machine Migration in Cloud Computing},
year = {2016},
isbn = {9781450342858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2898445.2898446},
doi = {10.1145/2898445.2898446},
abstract = {Virtual machine live migration technology, as an important support for cloud computing, has become a central issue in recent years. The virtual machines' runtime environment is migrated from the original physical server to another physical server, maintaining the virtual machines running at the same time. Therefore, it can make load balancing among servers and ensure the quality of service. However, virtual machine migration security issue cannot be ignored due to the immature development of it. This paper we analyze the security threats of the virtual machine migration, and compare the current proposed protection measures. While, these methods either rely on hardware, or lack adequate security and expansibility. In the end, we propose a security model of live virtual machine migration based on security policy transfer and encryption, named as SPLM (Security Protection of Live Migration) and analyze its security and reliability, which proves that SPLM is better than others. This paper can be useful for the researchers to work on this field. The security study of live virtual machine migration in this paper provides a certain reference for the research of virtualization security, and is of great significance.},
booktitle = {Proceedings of the 4th ACM International Workshop on Security in Cloud Computing},
pages = {2–9},
numpages = {8},
keywords = {virtualization, cloud computing, virtual machine, security, live migration},
location = {Xi'an, China},
series = {SCC '16}
}

@inproceedings{10.1109/CCGrid.2015.91,
author = {Naskos, Athanasios and Stachtiari, Emmanouela and Gounaris, Anastasios and Katsaros, Panagiotis and Tsoumakos, Dimitrios and Konstantinou, Ioannis and Sioutas, Spyros},
title = {Dependable Horizontal Scaling Based on Probabilistic Model Checking},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.91},
doi = {10.1109/CCGrid.2015.91},
abstract = {The focus of this work is the on-demand resource provisioning in cloud computing, which is commonly referred to as cloud elasticity. Although a lot of effort has been invested in developing systems and mechanisms that enable elasticity, the elasticity decision policies tend to be designed without quantifying or guaranteeing the quality of their operation. We present an approach towards the development of more formalized and dependable elasticity policies. We make two distinct contributions. First, we propose an extensible approach to enforcing elasticity through the dynamic instantiation and online quantitative verification of Markov Decision Processes (MDP) using probabilistic model checking. Second, various concrete elasticity models and elasticity policies are studied. We evaluate the decision policies using traces from a real NoSQL database cluster under constantly evolving external load. We reason about the behaviour of different modeling and elasticity policy options and we show that our proposal can improve upon the state-of-the-art in significantly decreasing under-provisioning while avoiding over-provisioning.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {31–40},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

