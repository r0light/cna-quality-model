@inproceedings{10.1145/2656075.2656086,
author = {Hsieh, Chih-Ming and Samie, Farzad and Srouji, M. Sammer and Wang, Manyi and Wang, Zhonglei and Henkel, J\"{o}rg},
title = {Hardware/Software Co-Design for a Wireless Sensor Network Platform},
year = {2014},
isbn = {9781450330510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656075.2656086},
doi = {10.1145/2656075.2656086},
abstract = {Wireless sensor networks have become shared resources providing sensing services to
monitor ambient environment. The tasks performed by the sensor nodes and the network
structure are becoming more and more complex so that they cannot be handled efficiently
by traditional sensor nodes any more. The traditional sensor node architecture, which
has software implementation running on a fixed hardware design, is no longer fit to
the changing requirements when new applications with complex computation are added
to this shared infrastructure due to several reasons. First, the operation behavior
changes because of the application requirements and the environmental conditions which
makes a fixed architecture not efficient all the time. Second, to collaborate with
other already deployed sensor networks and to maintain an efficient network structure,
the sensor nodes require flexible communication capabilities. Furthermore, the information
required to determine an efficient hardware/software co-design under the system constraints
cannot be known a priori. Therefore a platform which can adapt to run-time situations
will play an important role in wireless sensor networks. In this paper, we present
a hardware/software co-design framework for a wireless sensor platform, which can
adaptively change its hardware/software configuration to accelerate complex operations
and provides a flexible communication mechanism to deal with complex network structures.
We perform real-world measurements on our prototype to analyze its capabilities. In
addition, our case studies with prototype implementation and network simulations show
the energy savings of the sensor network application by using the proposed design
with run-time adaptivity.},
booktitle = {Proceedings of the 2014 International Conference on Hardware/Software Codesign and System Synthesis},
articleno = {1},
numpages = {10},
keywords = {FPGA, sensor networks, multi-radio, reconfiguration, hardware accelerator, low power},
location = {New Delhi, India},
series = {CODES '14}
}

@inproceedings{10.1145/2789168.2790089,
author = {Kurose, James F.},
title = {Research Challenges and Opportunities in a Mobility-Centric World},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2790089},
doi = {10.1145/2789168.2790089},
abstract = {The Internet recently passed an historic inflection point, with the number of broadband
mobile devices surpassing the number of wired PCs and servers connected to the Internet.
Mobility now profoundly affects the architecture, services and applications in both
the wireless and wired domains. In this "bottom up" talk, we begin by discussing several
specific mobility-related challenges and recent results in areas including mobility
measurement (including privacy considerations) and modeling, and context-sensitive
services. We then take a broader look at current and future challenges, and conclude
by discussing several NSF investments in programs and projects in area of mobile networking.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {290},
numpages = {1},
keywords = {mobility, computer networks, architecture, measurement modeling},
location = {Paris, France},
series = {MobiCom '15}
}

@inproceedings{10.1145/3240765.3243485,
author = {Wei, Tianshu and Chen, Xiaoming and Li, Xin and Zhu, Qi},
title = {Model-Based and Data-Driven Approaches for Building Automation and Control},
year = {2018},
isbn = {9781450359504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240765.3243485},
doi = {10.1145/3240765.3243485},
abstract = {Smart buildings in the future are complex cyber-physical-human systems that involve
close interactions among embedded platform (for sensing, computation, communication
and control), mechanical components, physical environment, building architecture,
and occupant activities. The design and operation of such buildings require a new
set of methodologies and tools that can address these heterogeneous domains in a holistic,
quantitative and automated fashion. In this paper, we will present our design automation
methods for improving building energy efficiency and offering comfortable services
to occupants at low cost. In particular, we will highlight our work in developing
both model-based and data-driven approaches for building automation and control, including
methods for co-scheduling heterogeneous energy demands and supplies, for integrating
intelligent building energy management with grid optimization through a proactive
demand response framework, for optimizing HVAC control with deep reinforcement learning,
and for accurately measuring in-building temperature by combining prior modeling information
with few sensor measurements based upon Bayesian inference.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
articleno = {26},
numpages = {8},
keywords = {smart buildings, deep reinforcement learning, model-based design, Bayesian inference, data-driven, model predictive control},
location = {San Diego, California},
series = {ICCAD '18}
}

@inproceedings{10.1145/2627566.2627575,
author = {Antonescu, Alexandru-Florian and Braun, Torsten},
title = {Modeling and Simulation of Concurrent Workload Processing in Cloud-Distributed Enterprise Information Systems},
year = {2014},
isbn = {9781450329927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627566.2627575},
doi = {10.1145/2627566.2627575},
abstract = {Cloud Computing enables provisioning and distribution of highly scalable services
in a reliable, on-demand and sustainable manner. Distributed Enterprise Information
Systems (dEIS) are a class of applications with important economic value and with
strong requirements in terms of performance and reliability. In order to validate
dEIS architectures, stability, scaling and SLA compliance, large testing deployments
are necessary, adding complexity to the design and testing of such systems. To fill
this gap, we present and validate a methodology for modeling and simulating such complex
distributed systems using the CloudSim cloud computing simulator, based on measurement
data from an actual distributed system. We present an approach for creating a performance-based
model of a distributed cloud application using recorded service performance traces.
We then show how to integrate the created model into CloudSim. We validate the CloudSim
simulation model by comparing performance traces gathered during distributed concurrent
experiments with simulation results using different VM configurations. We demonstrate
the usefulness of using a cloud simulator for modeling properties of real cloud-distributed
applications.},
booktitle = {Proceedings of the 2014 ACM SIGCOMM Workshop on Distributed Cloud Computing},
pages = {11–16},
numpages = {6},
keywords = {cloud computing, performance profiling, distributed applications, modelling and simulation},
location = {Chicago, Illinois, USA},
series = {DCC '14}
}

@inproceedings{10.1145/2789168.2790094,
author = {Cui, Yong and Lai, Zeqi and Wang, Xin and Dai, Ningwei and Miao, Congcong},
title = {QuickSync: Improving Synchronization Efficiency for Mobile Cloud Storage Services},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2790094},
doi = {10.1145/2789168.2790094},
abstract = {Mobile cloud storage services have gained phenomenal success in recent few years.
In this paper, we identify, analyze and address the synchronization (sync) inefficiency
problem of modern mobile cloud storage services. Our measurement results demonstrate
that existing commercial sync services fail to make full use of available bandwidth,
and generate a large amount of unnecessary sync traffic in certain circumstance even
though the incremental sync is implemented. These issues are caused by the inherent
limitations of the sync protocol and the distributed architecture. Based on our findings,
we propose QuickSync, a system with three novel techniques to improve the sync efficiency
for mobile cloud storage services, and build the system on two commercial sync services.
Our experimental results using representative workloads show that QuickSync is able
to reduce up to 52.9% sync time in our experiment settings.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {592–603},
numpages = {12},
keywords = {performance, mobile cloud storage, measurement},
location = {Paris, France},
series = {MobiCom '15}
}

@inproceedings{10.1145/2987443.2987482,
author = {Orsini, Chiara and King, Alistair and Giordano, Danilo and Giotsas, Vasileios and Dainotti, Alberto},
title = {BGPStream: A Software Framework for Live and Historical BGP Data Analysis},
year = {2016},
isbn = {9781450345262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987443.2987482},
doi = {10.1145/2987443.2987482},
abstract = {We present BGPStream, an open-source software framework for the analysis of both historical
and real-time Border Gateway Protocol (BGP) measurement data. Although BGP is a crucial
operational component of the Internet infrastructure, and is the subject of research
in the areas of Internet performance, security, topology, protocols, economics, etc.,
there is no efficient way of processing large amounts of distributed and/or live BGP
measurement data. BGPStream fills this gap, enabling efficient investigation of events,
rapid prototyping, and building complex tools and large-scale monitoring applications
(e.g., detection of connectivity disruptions or BGP hijacking attacks). We discuss
the goals and architecture of BGPStream. We apply the components of the framework
to different scenarios, and we describe the development and deployment of complex
services for global Internet monitoring that we built on top of it.},
booktitle = {Proceedings of the 2016 Internet Measurement Conference},
pages = {429–444},
numpages = {16},
keywords = {bgp measurement, network monitoring, internet measurement, bgp monitoring, internet routing, network measurement, real-time monitoring},
location = {Santa Monica, California, USA},
series = {IMC '16}
}

@inproceedings{10.1145/3307334.3326070,
author = {Xiao, Ao and Liu, Yunhao and Li, Yang and Qian, Feng and Li, Zhenhua and Bai, Sen and Liu, Yao and Xu, Tianyin and Xin, Xianlong},
title = {An In-Depth Study of Commercial MVNO: Measurement and Optimization},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326070},
doi = {10.1145/3307334.3326070},
abstract = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs),
which operate on top of the existing cellular infrastructures of base carriers while
offering cheaper or more flexible data plans compared to those of the base carriers.
In this paper, we present a nearly two-year measurement study towards understanding
various key aspects of today's MVNO ecosystem, including its architecture, performance,
economics, customers, and the complex interplay with the base carrier. Our study focuses
on a large commercial MVNO with reviseabout 1 million customers, operating atop a
nation-wide base carrier. Our measurements clarify several key concerns raised by
MVNO customers, such as inaccurate billing and potential performance discrimination
with the base carrier. We also leverage big data analytics and machine learning to
optimize an MVNO's key businesses such as data plan reselling and customer churn mitigation.
Our proposed techniques can help achieve %will lead to higher revenues and improved
services for commercial MVNOs.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {457–468},
numpages = {12},
keywords = {mvno, network performance, machine learning, churn mitigation, data prediction},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@article{10.1109/TNET.2020.2981514,
author = {Li, Yang and Zheng, Jianwei and Li, Zhenhua and Liu, Yunhao and Qian, Feng and Bai, Sen and Liu, Yao and Xin, Xianlong},
title = {Understanding the Ecosystem and Addressing the Fundamental Concerns of Commercial MVNO},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2981514},
doi = {10.1109/TNET.2020.2981514},
abstract = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs),
which operate on top of existing cellular infrastructures of base carriers, while
offering cheaper or more flexible data plans compared to those of the base carriers.
In this paper, we present a two-year measurement study towards understanding various
fundamental aspects of today's MVNO ecosystem, including its architecture, customers,
performance, economics, and the complex interplay with the base carrier. Our study
focuses on a large commercial MVNO with one million customers, operating atop a nation-wide
base carrier. Our measurements clarify several key concerns raised by MVNO customers,
such as inaccurate billing and potential performance discrimination with the base
carrier. We also leverage big data analytics, statistical modeling, and machine learning
to address the MVNO's key concerns with regard to data usage prediction, data plan
reselling, customer churn mitigation, and billing delay reduction. Our proposed techniques
can help achieve higher revenues and improved services for commercial MVNOs.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {1364–1377},
numpages = {14}
}

@inproceedings{10.1145/3331076.3331108,
author = {Buccafurri, Francesco and Musarella, Lorenzo and Nardone, Roberto},
title = {Enabling Propagation in Web of Trust by Ethereum},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331108},
doi = {10.1145/3331076.3331108},
abstract = {Web of Trust offers a way to bind identities with the corresponding public keys. It
relies on a distributed architecture, where each user could play the role of certificate
signer. With the widespread diffusion of social networks, the trust propagation is
a matter of growing interest. This paper proposes an approach enabling the propagation
in Web of Trust by means of Ethereum. The usage of Ethereum eliminates the necessity
of single-organization trusted services, which is, in general, not realistic. Although
the information stored on Ethereum is public, the privacy of users is protected because
trust chains involve only Ethereum addresses and strong measures are implemented to
contrast their malicious de-anonymization. The approach relies on the usage of a smart
contract for storing the status of certificate signatures and to manage revocations.
When a user u wants to trust another user v, the smart contract checks the presence
of trust chains originating from root nodes of u.},
booktitle = {Proceedings of the 23rd International Database Applications &amp; Engineering Symposium},
articleno = {9},
numpages = {6},
keywords = {trust propagation, blockchain, smart contract, social network, pretty good privacy, Ethereum},
location = {Athens, Greece},
series = {IDEAS '19}
}

@inproceedings{10.1145/3267809.3275470,
author = {Nadgowda, Shripad and Isci, Canturk},
title = {Drishti: Disaggregated and Interoperable Security Analytics Framework for Cloud},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3275470},
doi = {10.1145/3267809.3275470},
abstract = {Application and platform security has always been critical for the success of any
business. Traditionally, applications were deployed directly on physical servers.
As a result, there are myriad traditional security solutions that were developed around
this model to run as local agents on the systems they monitor and protect. These solutions
are then refined and standardized with decades of experience. With the emergence of
virtualization, cloud and particularly containerization, use of these solutions is
becoming challenging with consolidation and scale. As we begin to deploy hundreds
of cloud instances on a single node, traditional solutions, designed for local execution
do not scale out. At the same time, the clean separation of a virtual machine (VM)
or a container from the platform itself, and maturing introspection and inspection
APIs provide a simple, practical way to decouple monitored from the monitors [3].
Furthermore, as the scope of cloud security expands from simple monitoring and auditing
to more complex learning based analytics, analytics components are further offloaded
to separate data services, where they can burn extensive cycles, and in some cases
use specialized hardware for security analytics, out of the critical path of the monitored
applications [5]. As a result, traditional agent-based tightly-coupled model is being
replaced by a more dis-aggregated {system, observation, analytics, actions} architecture.To
implement such dis-aggregated model in practice, first system state needs to be transferred
from cloud platform to analytic platform. File system more generally is representative
of the system state that persists features of interest for security analytics like
processes, metrics, configurations, packages across various files. Remote replication
or snapshotting [1] of whole file system is very in-efficient, since only small set
of files are accessed during the analytics. As a result, a new family of cloud-native
security solutions have recently emerged in the field that uses various specialized
data collection techniques[2, 4]. These techniques perform out-of band introspection
of systems to interpret and extract required system features from the file system
to essentially serialize system state into data. This data is then transferred to
an analytic platform for analysis. Unlike the traditional security solutions that
work locally against the system's standard POSIXy file system interfaces, these emerging
security analytics "work from data" on the analytic platform. However since the target
system is now available as "data", existing agent-based security solutions become
incompatible to work against the system. One mitigating solution is to rewrite all
existing solutions, which requires huge amount of resources and effort.In Drishti,
we address this challenge from a fundamentally different perspective. Instead of rewriting
security solutions to work from data, we make the data work for traditional security
applications. We achieve this by developing a pseudo-system interface over systems
data collected from cloud instances. With this approach, existing solutions run unmodified,
as "black box" software over this system interface, as if they were running on the
actual cloud instance. Drishti framework is our realization of this approach. It is
logically the inverse of the first step of cloud-native security analytics that convert
system state into data. With Drishti we transform data back to system on the analytic
platform by orchestrating two file system components. First, a standard native system
interface is re-calibrated over the system data through our new FUSE file system,
confuse or ClOud Native Filesystem in UserSpacE. Second, we mimic the "effect" of
an agent installation via an overlay file system based on the the agent image. Within
the Drishti framework the underlying data looks like a standard POSIX system to each
on-boarded security solution. This allows us to run existing agent-based security
solutions as is, but still decoupled from the actual system. Drishti also provides
a standard and interoperable platform for designing new security analytic solutions.Overall,
Drishti demonstrates a novel, pragmatic and highly-practical approach for bringing
security analytics into the cloud. It enables us to leverage existing solutions built
based on decades of experience by eliminating the need for reinventing the wheel for
cloud.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {528},
numpages = {1},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@article{10.1109/TNET.2020.2976129,
author = {Xie, Kun and Chen, Yuxiang and Wang, Xin and Xie, Gaogang and Cao, Jiannong and Wen, Jigang},
title = {Accurate and Fast Recovery of Network Monitoring Data: A GPU Accelerated Matrix Completion},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2976129},
doi = {10.1109/TNET.2020.2976129},
abstract = {Gaining a full knowledge of end-to-end network performance is important for some advanced
network management and services. Although it becomes increasingly critical, end-to-end
network monitoring usually needs active probing of the path and the overhead will
increase quadratically with the number of network nodes. To reduce the measurement
overhead, matrix completion is proposed recently to predict the end-to-end network
performance among all node pairs by only measuring a small set of paths. Despite its
potential, applying matrix completion to recover the missing data suffers from low
recovery accuracy and long recovery time. To address the issues, we propose MC-GPU
to exploit Graphics Processing Units (GPUs) to enable parallel matrix factorization
for high-speed and highly accurate Matrix Completion. To well exploit the special
architecture features of GPUs for both task independent and data-independent parallel
task execution, we propose several novel techniques: similar OD (origin and destination)
pairs reordering taking advantage of the locality-sensitive hash (LSH) functions,
balanced matrix partition, and parallel matrix completion. We implement the proposed
MC-GPU on the GPU platform and evaluate the performance using real trace data. We
compare the proposed MC-GPU with the state of the art matrix completion algorithms,
and our results demonstrate that MC-GPU can achieve significantly faster speed with
high data recovery accuracy.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {958–971},
numpages = {14}
}

@inproceedings{10.1145/3197768.3201560,
author = {Tzallas, Alexandros T. and Katertsidis, Nikolaos and Glykos, Konstantinos and Segkouli, Sofia and Votis, Konstantinos and Tzovaras, Dimitrios and Barru\'{e}, Cristian and Paliokas, Ioannis and Cort\'{e}s, Ulises},
title = {Designing a Gamified Social Platform for People Living with Dementia and Their Live-in Family Caregivers},
year = {2018},
isbn = {9781450363907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197768.3201560},
doi = {10.1145/3197768.3201560},
abstract = {In the current paper, a social gamified platform for people living with dementia and
their live-in family caregivers, integrating a broader diagnostic approach and interactive
interventions is presented. The CAREGIVERSPRO-MMD (C-MMD) platform constitutes a support
tool for the patient and the informal caregiver - also referred to as the dyad - that
strengthens self-care, and builds community capacity and engagement at the point of
care. The platform is implemented to improve social collaboration, adherence to treatment
guidelines through gamification, recognition of progress indicators and measures to
guide management of patients with dementia, and strategies and tools to improve treatment
interventions and medication adherence. Moreover, particular attention was provided
on guidelines, considerations and user requirements for the design of a User-Centered
Design (UCD) platform. The design of the platform has been based on a deep understanding
of users, tasks and contexts in order to improve platform usability, and provide adaptive
and intuitive User Interfaces with high accessibility. In this paper, the architecture
and services of the C-MMD platform are presented, and specifically the gamification
aspects.},
booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
pages = {476–481},
numpages = {6},
keywords = {Dementia, Cloud Platform, Interventions, Social Networking, Caregivers, Gamification, Self-management},
location = {Corfu, Greece},
series = {PETRA '18}
}

@article{10.1145/3057857,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Visual Simple Transformations: Empowering End-Users to Wire Internet of Things Objects},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3057857},
doi = {10.1145/3057857},
abstract = {Empowering end-users to wire Internet of Things (IoT) objects (things and services)
together would allow them to more easily conceive and realize interesting IoT solutions.
A challenge lies in devising a simple end-user development approach to support the
specification of transformations, which can bridge the mismatch in the data being
exchanged among IoT objects. To tackle this challenge, we present Visual Simple Transformations
(ViSiT) as an approach that allows end-users to use a jigsaw puzzle metaphor for specifying
transformations that are automatically converted into underlying executable workflows.
ViSiT is explained by presenting meta-models and an architecture for implementing
a system of connected IoT objects. A tool is provided for supporting end-users in
visually developing and testing transformations. Another tool is also provided for
allowing software developers to modify, if they wish, a transformation's underlying
implementation. This work was evaluated from a technical perspective by developing
transformations and measuring ViSiT's efficiency and scalability and by constructing
an example application to show ViSiT's practicality. A study was conducted to evaluate
this work from an end-user perspective, and its results showed positive indications
of perceived usability, learnability, and the ability to conceive real-life scenarios
for ViSiT.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {10},
numpages = {43},
keywords = {End-user development, transformations, internet of things}
}

@inproceedings{10.1145/3053600.3053634,
author = {Walter, J\"{u}rgen and Stier, Christian and Koziolek, Heiko and Kounev, Samuel},
title = {An Expandable Extraction Framework for Architectural Performance Models},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053634},
doi = {10.1145/3053600.3053634},
abstract = {Providing users with Quality of Service (QoS) guarantees and the prevention of performance
problems are challenging tasks for software systems. Architectural performance models
can be applied to explore performance properties of a software system at design time
and run time. At design time, architectural performance models support reasoning on
effects of design decisions. At run time, they enable automatic reconfigurations by
reasoning on the effects of changing user behavior. In this paper, we present a framework
for the extraction of architectural performance models based on monitoring log files
generalizing over the targeted architectural modeling language. Using the presented
framework, the creation of a performance model extraction tool for a specific modeling
formalism requires only the implementation of a key set of object creation routines
specific to the formalism. Our framework integrates them with extraction techniques
that apply to many architectural performance models, e.g., resource demand estimation
techniques. This lowers the effort to implement performance model extraction tools
tremendously through a high level of reuse. We evaluate our framework presenting builders
for the Descartes Modeling Language (DML) and the Palladio Component Model(PCM). For
the extracted models we compare simulation results with measurements receiving accurate
results.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {165–170},
numpages = {6},
keywords = {descartes modeling language, palladio component model, automated performance model extraction, builder pattern},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1145/2968216,
author = {Maqsood, Tahir and Khalid, Osman and Irfan, Rizwana and Madani, Sajjad A. and Khan, Samee U.},
title = {Scalability Issues in Online Social Networks},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/2968216},
doi = {10.1145/2968216},
abstract = {The last decade witnessed a tremendous increase in popularity and usage of social
network services, such as Facebook, Twitter, and YouTube. Moreover, advances in Web
technologies coupled with social networks has enabled users to not only access, but
also generate, content in many forms. The overwhelming amount of produced content
and resulting network traffic gives rise to precarious scalability issues for social
networks, such as handling a large number of users, infrastructure management, internal
network traffic, content dissemination, and data storage. There are few surveys conducted
to explore the different dimensions of social networks, such as security, privacy,
and data acquisition. Most of the surveys focus on privacy or security-related issues
and do not specifically address scalability challenges faced by social networks. In
this survey, we provide a comprehensive study of social networks along with their
significant characteristics and categorize social network architectures into three
broad categories: (a) centralized, (b) decentralized, and (c) hybrid. We also highlight
various scalability issues faced by social network architectures. Finally, a qualitative
comparison of presented architectures is provided, which is based on various scalability
metrics, such as availability, latency, interserver communication, cost of resources,
and energy consumption, just to name a few.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {40},
numpages = {42},
keywords = {social network, Scalability, hybrid social networks, decentralized social networks, centralized social networks}
}

@article{10.1109/TNET.2013.2253797,
author = {De Cicco, Luca and Mascolo, Saverio},
title = {An Adaptive Video Streaming Control System: Modeling, Validation, and Performance Evaluation},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2013.2253797},
doi = {10.1109/TNET.2013.2253797},
abstract = {Adaptive video streaming is a relevant advancement with respect to classic progressive
download streaming a la YouTube. Among the different approaches, the video stream-switching
technique is getting wide acceptance, being adopted by Microsoft, Apple, and popular
video streaming services such as Akamai, Netflix, Hulu, Vudu, and Livestream. In this
paper, we present a model of the automatic video stream-switching employed by one
of these leading video streaming services along with a description of the client-side
communication and control protocol. From the control architecture point of view, the
automatic adaptation is achieved by means of two interacting control loops having
the controllers at the client and the actuators at the server: One loop is the buffer
controller, which aims at steering the client playout buffer to a target length by
regulating the server sending rate; the other one implements the stream-switching
controller and aims at selecting the video level. A detailed validation of the proposed
model has been carried out through experimental measurements in an emulated scenario.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {526–539},
numpages = {14},
keywords = {adaptive video streaming, modeling, stream-switching, performance evaluation}
}

@article{10.1145/3457143,
author = {Burny, Nicolas and Vanderdonckt, Jean},
title = {UiLab, a Workbench for Conducting and Reproducing Experiments in GUI Visual Design},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3457143},
doi = {10.1145/3457143},
abstract = {With the continuously increasing number and variety of devices, the study of visual
design of their Graphical User Interfaces grows in importance and scope, particularly
for new devices, including smartphones, tablets, and large screens. Conducting a visual
design experiment typically requires defining and building a GUI dataset with different
resolutions for different devices, computing visual design measures for the various
configurations, and analyzing their results. This workflow is very time- and resource-consuming,
therefore limiting its reproducibility. To address this problem, we present UiLab,
a cloud-based workbench that parameterizes the settings for conducting an experiment
on visual design of Graphical User Interfaces, for facilitating the design of such
experiments by automating some workflow stages, and for fostering their reproduction
by automating their deployment. Based on requirements elicited for UiLab, we define
its conceptual model to delineate the borders of services of the software architecture
to support the new workflow. We exemplify it by demonstrating a system walkthrough
and we assess its impact on experiment reproducibility in terms of design and development
time saved with respect to a classical workflow. Finally, we discuss potential benefits
brought by this workbench with respect to reproducing experiments in GUI visual design
and existing shortcomings to initiate future avenues. We publicly release UiLab source
code on a GitHub repository.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {196},
numpages = {31},
keywords = {usability evaluation, visual design, user interface evaluation, aesthetics}
}

@inproceedings{10.1145/2976767.2987689,
author = {Falkner, Katrina and Szabo, Claudia and Chiprianov, Vanea},
title = {Model-Driven Performance Prediction of Systems of Systems},
year = {2016},
isbn = {9781450343213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976767.2987689},
doi = {10.1145/2976767.2987689},
abstract = {Systems of Systems exhibit characteristics that pose difficulty in modelling and predicting
their overall performance capabilities, including the presence of operational independence,
emergent behaviour, and evolutionary development. When considering Systems of Systems
within the autonomous defence systems context, these aspects become increasingly critical,
as performance constraints are typically driven by hard constraints on space, weight
and power.System execution modelling languages and tools permit early prediction of
the performance of model-driven systems, however the focus to date has been on understanding
the performance of a model rather than determining if it meets performance requirements,
and only subsequently carrying out analysis to reveal the causes of any requirement
violations. Such an analysis is even more difficult when applied to several systems
cooperating to achieve a common goal - a System of Systems (SoS).The successful integration
of systems within a SoS context has been identified as one of the most substantial
challenges facing military systems development [2]. Accordingly, there is a critical
need to understand the non-functional aspects of the SoS (such as quality of service,
power, size, cost and scalable management of communications), and to explore how these
non-functional aspects evolve under new conditions and deployment scenarios. It is
crucial that we develop methodologies for modelling and understanding non-functional
properties early in the development and integration cycle to better inform our understanding
of the impact of emergent behaviour and evolution within the SoS.We propose an integrated
approach to performance prediction of model-driven real time embedded defence systems
and systems of systems [1]. Our architectural prototyping system supports a scenario-driven
experimental platform for evaluating model suitability within a set of deployment
and real-time performance constraints. We present an overview of our performance prediction
system, demonstrating the integration of modelling, execution and performance analysis,
and discuss a case study to illustrate our approach. Our work employs state-of-the-art
model-driven engineering techniques to facilitate SoS performance prediction and analysis
at design time, either before the SoS is built and deployed, or during its lifetime
when required to evolve.Our model-driven performance prediction platform supports
a scenario-driven experimental environment for evaluating a SoS within the context
of a specific deployment (modelling geographical distribution) and integration constraints.
The main contributions of our work are: (a) a modeling methodology that captures diverse
perspectives of the performance modeling of Systems of Systems; (b) a performance
analysis engine that captures metrics associated with these perspectives and (c) a
case study showing the performance evaluaton of a system of systems and its evolution
as a result of the performance analysis. We discuss how our approach to modelling
supports the specific characteristics of an SoS, and illustrate this through a case
study, based on a "Blue Ocean" scenario, demonstrating how we may obtain performance
predictions within a SoS with emergent and evolutionary properties. Within the context
of our environment, we define models for the individual systems within our System
of Systems, defined for representative workload to predict execution costs, i.e. CPU,
memory usage and network usage, within a generic situation. Our modelling environment
supports the generation of executable forms of these models, which may then be executed
above realistic deployment scenarios in order to obtain predictions of System of System
performance.},
booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
pages = {44},
numpages = {1},
location = {Saint-malo, France},
series = {MODELS '16}
}

@inproceedings{10.1145/3277868.3277880,
author = {Klugman, Noah and Dutta, Prabal},
title = {Set and Forget Sensing with Applets on IFTTT},
year = {2018},
isbn = {9781450360494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277868.3277880},
doi = {10.1145/3277868.3277880},
abstract = {Rich data sets can be collected trivially by bootstrapping off mobile phones and cloud
services. We describe an end-to-end system built with IFTTT that requires no code
to collect arrival and departure times from a geographic area on the campus of the
University of California, Berkeley. This system was configured and deployed in less
than one half hour, cost nothing to deploy or run, and functioned without interruption
for seven months, taking 463 measurements of a single participant. Along with providing
the data set, which provides some insight into the working life of a graduate student,
we describe each part of the system architecture and discuss how a model of sensing-as-an-applet
enables data streams with de-facto standardized, high reliability, and close-to-no-barrier
of entry.},
booktitle = {Proceedings of the First Workshop on Data Acquisition To Analysis},
pages = {23–24},
numpages = {2},
keywords = {sensing at scale, trigger-action programming, IFTTT},
location = {Shenzhen, China},
series = {DATA '18}
}

@inproceedings{10.1145/3301551.3301582,
author = {Ruangvanich, Supparang and Nilsook, Prachyanun},
title = {Personality Learning Analytics System in Intelligent Virtual Learning Environment},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301582},
doi = {10.1145/3301551.3301582},
abstract = {In this paper, the researchers propose a conceptual for system architecture of learning
analytics process in the intelligent learning environment. Within this concept, today's
competitive business environment need for businesses in order to implement the monitor
and analyze the user-generated data on their own and their competitors. The achievement
of competitive advantage is often necessary to listen to and understand what customers
are saying about competitors' products and services. Not only personality analytics
but also the conceptual description can capture an intelligent learning environment,
and it is the analytic tools that are used to improve learning and education. The
researchers also discuss how learning analytics is developed in different fields.
It closely tied to, a series of other fields of study including business intelligence,
web analytics, academic analytics, educational data mining, and action analytics.
The researchers believe that conceptual of personality analytics in the intelligent
learning environment can play an essential role in managing and analyzing personality
and contribute to the concept of personality analytics in the intelligent learning
environment. The results of this research could be summarized as follows: learning
analytics process should be used as measuring and collecting data about learners and
learning with the aim of improving teaching and learning practice through analysis
of the data. By achieving this process, it should collect data to report or analyze
the happening about the learner. Then, instructors monitor learning what is happening
now, while as learning analytics should get what is going to happen in the future
for learners. Finally, instructors take action to feedback learners.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {245–250},
numpages = {6},
keywords = {Virtual Learning Environment, Learning Analytics, Intelligent Environment, Personal Analytics, System Architecture},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@inproceedings{10.1145/2645884.2645890,
author = {Irish, Andrew T. and Iland, Daniel and Isaacs, Jason T. and Hespanha, Jo\~{a}o P. and Belding, Elizabeth M. and Madhow, Upamanyu},
title = {Using Crowdsourced Satellite SNR Measurements for 3D Mapping and Real-Time GNSS Positioning Improvement},
year = {2014},
isbn = {9781450330732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645884.2645890},
doi = {10.1145/2645884.2645890},
abstract = {Geopositioning using Global Navigation Satellite Systems (GNSS), such as the Global
Positioning System (GPS), is inaccurate in urban environments due to frequent non-line-of-sight
(NLOS) signal reception. This poses a major problem for mobile services that benefit
from accurate urban localization, such as navigation, hyperlocal advertising, and
geofencing applications. However, urban NLOS signal reception can be exploited in
two ways. First, one can use satellite signal-to-noise ratio (SNR) measurements crowdsourced
from mobile devices to create 3D environment maps. This is possible because, for example,
the SNR of signals obstructed by buildings is lower on average than that of line-of-sight
(LOS) signals. Second, in a sort of reverse process called Shadow Matching, SNR readings
from a particular device at an instant in time can be compared to 3D maps to provide
real-time localization improvement. In this paper we give a brief overview of how
such a system works and describe a scalable, low-cost, software-only architecture
that implements it.},
booktitle = {Proceedings of the 6th Annual Workshop on Wireless of the Students, by the Students, for the Students},
pages = {5–8},
numpages = {4},
keywords = {localization improvement, shadow matching, 3d mapping, crowdsourcing, gnss, gps},
location = {Maui, Hawaii, USA},
series = {S3 '14}
}

@inproceedings{10.1145/3229556.3229563,
author = {Kastanakis, Savvas and Sermpezis, Pavlos and Kotronis, Vasileios and Dimitropoulos, Xenofontas},
title = {CABaRet: Leveraging Recommendation Systems for Mobile Edge Caching},
year = {2018},
isbn = {9781450359061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229556.3229563},
doi = {10.1145/3229556.3229563},
abstract = {Joint caching and recommendation has been recently proposed for increasing the efficiency
of mobile edge caching. While previous works assume collaboration between mobile network
operators and content providers (who control the recommendation systems), this might
be challenging in today's economic ecosystem, with existing protocols and architectures.
In this paper, we propose an approach that enables cache-aware recommendations without
requiring a network and content provider collaboration. We leverage information provided
publicly by the recommendation system, and build a system that provides cache-friendly
and high-quality recommendations. We apply our approach to the YouTube service, and
conduct measurements on YouTube video recommendations and experiments with video requests,
to evaluate the potential gains in the cache hit ratio. Finally, we analytically study
the problem of caching optimization under our approach. Our results show that significant
caching gains can be achieved in practice; 8 to 10 times increase in the cache hit
ratio from cache-aware recommendations, and an extra 2 times increase from caching
optimization.},
booktitle = {Proceedings of the 2018 Workshop on Mobile Edge Communications},
pages = {19–24},
numpages = {6},
keywords = {Recommendation Systems, Joint Caching and Recommendation, Mobile Edge Networks},
location = {Budapest, Hungary},
series = {MECOMM'18}
}

@inproceedings{10.1145/3368235.3368838,
author = {Harsh, Piyush and Ribera Laszkowski, Juan Francisco and Edmonds, Andy and Quang Thanh, Tran and Pauls, Michael and Vlaskovski, Radoslav and Avila-Garc\'{\i}a, Orlando and Pages, Enric and Gort\'{a}zar Bellas, Francisco and Gallego Carrillo, Micael},
title = {Cloud Enablers For Testing Large-Scale Distributed Applications},
year = {2019},
isbn = {9781450370448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368235.3368838},
doi = {10.1145/3368235.3368838},
abstract = {Testing large-scale distributed systems (also known as testing in the large) is a
challenge that spreads across different technical domains and areas of expertise.
Current methods and tools provide some minimal guarantees in relation to the correctness
of their functional properties and have serious limitations when evaluating their
extra-functional properties in realistic conditions, such as scalability, availability
and performance efficiency. Cloud Testing and more specifically "testing in the cloud''
has arisen to tackle those challenges. In this new paradigm, cloud-based environment
and infrastructure are used to run realistic end-to-end and/or system-level tests,
collect test data and analyse them. In this paper we present a set of cloud-native
services to take from the tester the responsibility of managing the resources and
complementary services required to simulate realistic operational conditions and production
environments. Specifically, they provide cloud testing capabilities such as logs and
measurements collection from both testing jobs and system under test; test data analytics
and visualization; provisioning and operation of additional services and processes
to replicate realistic production ecosystems; support to scalability and diversity
of underlying testing infrastructure; and replication of the operational conditions
of the software under test through its instrumentation. We present the architecture
of the cloud testing solution and the detailed design of each of the services; we
also evaluate their relative contribution to satisfy different needs in the context
of test execution.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
pages = {35–42},
numpages = {8},
keywords = {reliability, cloud testing, continuous testing, testing, continuous integration, large-scale distributed systems, scalability},
location = {Auckland, New Zealand},
series = {UCC '19 Companion}
}

@inproceedings{10.5555/3395101.3395117,
author = {Patan\'{e}, Giancarlo M. M. and Valastro, Gianluca C. and Sambo, Yusuf A. and Ozturk, Metin and Hussain, Sajjad and Imran, Muhammad A. and Panno, Daniela},
title = {Flexible SDN/NFV-Based SON Testbed for 5G Mobile Networks},
year = {2019},
isbn = {9781728129235},
publisher = {IEEE Press},
abstract = {In the next few years, a considerable innovation concerning the design of the future
5G mobile networks will be a concrete step towards enabling effective high throughput
and low latency services. Software Defined Networking (SDN), Network Function Virtualization
(NFV) and Self Organizing Network (SON) are considered the enabling technologies to
achieve these goals. In this paper, assuming a Control-Data Separation Architecture
(CDSA), we propose a flexible SDN/NFV-based SON testbed, for future 5G mobile networks.
The main contribution of our work is to cover the need for a CDSA based testbed, enabling
the investigation of the NG-SON capabilities for practical implementations. We implement
two different testbed setups, a real one and a virtualized one, both based on the
FlexRAN and OpenAirInterface software tools. First, we implement a specific case study,
i.e., the RAN entities activation/deactivation procedures. Next, we carry out time
measurements, concerning the aforementioned procedures, in order to prove proper testbed
functioning. Finally, we validate the C-SON and D-SON capabilities of our testbed,
considering the features of the results.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Symposium on Distributed Simulation and Real Time Applications},
pages = {79–86},
numpages = {8},
keywords = {OpenAirInterface, NFV, Cloud-RAN, FlexRAN, 5G, SDN},
location = {Cosenza, Italy},
series = {DS-RT '19}
}

@proceedings{10.1145/2619239,
title = {SIGCOMM '14: Proceedings of the 2014 ACM Conference on SIGCOMM},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SIGCOMM 2014!This year's conference continues the SIGCOMM tradition
of being the premier forum for the presentation of research on networking and communications.
The technical program this year features a set of outstanding papers that cover a
wide variety of areas including network architecture, software defined networks, data
center networks, wireless networks, network services, congestion management, security,
privacy, measurement and analysis.This year's call for papers attracted 242 submissions
from all over the world. The 54 member Technical Program Committee along with a selected
group of external experts carefully considered all of the submissions over two rounds
of reviewing including an author feedback period - with a total of 968 detailed reviews
completed. The TPC meeting to select the final program was held at ICSI, Berkeley,
in late April 2014. At the conclusion of the meeting, the committee had assembled
a wonderful program composed of 45 papers, to be presented over three days at the
conference. The quality of submissions was extremely high as reflected in the final
technical program.},
location = {Chicago, Illinois, USA}
}

@article{10.1145/3428151,
author = {Bibi, Iram and Akhunzada, Adnan and Malik, Jahanzaib and Khan, Muhammad Khurram and Dawood, Muhammad},
title = {Secure Distributed Mobile Volunteer Computing with Android},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3428151},
doi = {10.1145/3428151},
abstract = {Volunteer Computing provision of seamless connectivity that enables convenient and
rapid deployment of greener and cheaper computing infrastructure is extremely promising
to complement next-generation distributed computing systems. Undoubtedly, without
tactile Internet and secure VC ecosystems, harnessing its full potentials and making
it an alternative viable and reliable computing infrastructure is next to impossible.
Android-enabled smart devices, applications, and services are inevitable for Volunteer
computing. Contrarily, the progressive developments of sophisticated Android malware
may reduce its exponential growth. Besides, Android malwares are considered the most
potential and persistent cyber threat to mobile VC systems. To secure Android-based
mobile volunteer computing, the authors proposed MulDroid, an efficient and self-learning
autonomous hybrid (Long-Short-Term Memory, Convolutional Neural Network, Deep Neural
Network) multi-vector Android malware threat detection framework. The proposed mechanism
is highly scalable with well-coordinated infrastructure and self-optimizing capabilities
to proficiently tackle fast-growing dynamic variants of sophisticated malware threats
and attacks with 99.01% detection accuracy. For a comprehensive evaluation, the authors
employed current state-of-the-art malware datasets (Android Malware Dataset, Androzoo)
with standard performance evaluation metrics. Moreover, MulDroid is compared with
our constructed contemporary hybrid DL-driven architectures and benchmark algorithms.
Our proposed mechanism outperforms in terms of detection accuracy with a trivial tradeoff
speed efficiency. Additionally, a 10-fold cross-validation is performed to explicitly
show unbiased results.},
journal = {ACM Trans. Internet Technol.},
month = sep,
articleno = {2},
numpages = {21},
keywords = {deep learning (DL), Volunteer computing (VC), android malware, tactile internet}
}

@inproceedings{10.1145/3143434.3143443,
author = {Bogner, Justus and Wagner, Stefan and Zimmermann, Alfred},
title = {Automatically Measuring the Maintainability of Service- and Microservice-Based Systems: A Literature Review},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143443},
doi = {10.1145/3143434.3143443},
abstract = {In a time of digital transformation, the ability to quickly and efficiently adapt
software systems to changed business requirements becomes more important than ever.
Measuring the maintainability of software is therefore crucial for the long-term management
of such products. With Service-based Systems (SBSs) being a very important form of
enterprise software, we present a holistic overview of such metrics specifically designed
for this type of system, since traditional metrics - e.g. object-oriented ones - are
not fully applicable in this case. The selected metric candidates from the literature
review were mapped to 4 dominant design properties: size, complexity, coupling, and
cohesion. Microservice-based Systems (μSBSs) emerge as an agile and fine-grained variant
of SBSs. While the majority of identified metrics are also applicable to this specialization
(with some limitations), the large number of services in combination with technological
heterogeneity and decentralization of control significantly impacts automatic metric
collection in such a system. Our research therefore suggest that specialized tool
support is required to guarantee the practical applicability of the presented metrics
to μSBSs.},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {107–115},
numpages = {9},
keywords = {maintainability, metrics, service-based systems, SOA, microservices},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@article{10.1007/s00779-012-0618-y,
author = {Belsis, Petros and Pantziou, Grammati},
title = {A K-Anonymity Privacy-Preserving Approach in Wireless Medical Monitoring Environments},
year = {2014},
issue_date = {January   2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-012-0618-y},
doi = {10.1007/s00779-012-0618-y},
abstract = {With the proliferation of wireless sensor networks and mobile technologies in general,
it is possible to provide improved medical services and also to reduce costs as well
as to manage the shortage of specialized personnel. Monitoring a person's health condition
using sensors provides a lot of benefits but also exposes personal sensitive information
to a number of privacy threats. By recording user-related data, it is often feasible
for a malicious or negligent data provider to expose these data to an unauthorized
user. One solution is to protect the patient's privacy by making difficult a linkage
between specific measurements with a patient's identity. In this paper we present
a privacy-preserving architecture which builds upon the concept of k-anonymity; we
present a clustering-based anonymity scheme for effective network management and data
aggregation, which also protects user's privacy by making an entity indistinguishable
from other k similar entities. The presented algorithm is resource aware, as it minimizes
energy consumption with respect to other more costly, cryptography-based approaches.
The system is evaluated from an energy-consuming and network performance perspective,
under different simulation scenarios.},
journal = {Personal Ubiquitous Comput.},
month = jan,
pages = {61–74},
numpages = {14},
keywords = {Clustering, Remote medical monitoring, Sensors, Anonymity}
}

@article{10.1145/3412821.3412823,
author = {Cerina, L. and Santambrogio, M. D.},
title = {SAGe: A Configurable Code Generator for Efficient Symbolic Analysis of Time-Series},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
url = {https://doi.org/10.1145/3412821.3412823},
doi = {10.1145/3412821.3412823},
abstract = {Some of the most recent applications and services revolve around the analysis of time-series,
which generally exhibits chaotic characteristics. This behavior brought back the necessity
to simplify their representation to discover meaningful patterns and extract information
efficiently. Furthermore, recent trends show how computation is moving back from the
Cloud to the Edge of network, meaning that algorithms should be compatible with low-power
embedded devices. A family of methods called Symbolic Analysis (SA) tries to solve
this issue, reducing the dimensionality of the original data in a set of symbolic
words and providing distance metrics for the obtained symbols. However, SA is usually
implemented using application-specific tools, which are not easily adaptable, or mathematical
environments (e.g. R, Julia) that do not ensure portability, or that require additional
work to maximize computing performance. We propose here SAGe: a code generation tool
that helps the user to prototype efficient and portable code, starting from a high-level
representation of SA requirements. Other than exploiting similarities between SA pipelines,
SAGe employs general code templates to build and deploy the code on different architectures,
such as embedded devices, microcontrollers, and FPGAs. Preliminary results show a
speedup up to 223x against Python implementations running on an x86 desktop machine
and a notable increase in computational efficiency on a reconfigurable device.},
journal = {SIGBED Rev.},
month = jul,
pages = {12–17},
numpages = {6}
}

@inproceedings{10.1145/3090354.3090463,
author = {Rafii, Fadoua and Hassani, Badr Dine Rossi and Kbir, M'hamed A\"{\i}t},
title = {New Approach for Microarray Data Decision Making with Respect to Multiple Sources},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090463},
doi = {10.1145/3090354.3090463},
abstract = {Microarray technology is an innovative technology, which has brought changes to the
biological fields. It is considered as an interesting advent for worthwhile researches.
It has permitted simultaneous measurements of the hundreds of activities of genes.
However, most of users and specifically researchers and biologists find difficulties
while extracting and interpreting this kind of data, also the results of Microarray
experiments are stored in multiple and different databases. The present paper focuses
on providing a global architecture for making decisions on Microarray data, by taking
advantages from the semantic web technologies and the data mining techniques. The
major goal consists on getting decisions about a given disease from many experiment
data distributed on many sources over the net. The input dataset, real elements array
form, is retrieved from the integrated experiments designed for cancer studies. This
work is interested to two huge Microarray databases: GEO and ArrayExpress. The integration
was based on semantic web technologies used to integrate data from several Web sites
and Microarray data sources. This can be done by a user to combine several experiments
that treat the same disease or phenomenon in order to have more significant results.
Also a user can upload a specific dataset, via Web services provided by a laboratory,
that can be combined with other data, containing the same genes and treating the same
disease, and receive results of data mining techniques proposed by this laboratory.
We suppose that each laboratory has its own Web services that can receive data which
respects a predefined format.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {106},
numpages = {5},
keywords = {Data mining, Microarray, Semantic web},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3199902.3199904,
author = {\v{S}ljivo, Amina and Kerkhove, Dwight and Moerman, Ingrid and De Poorter, Eli and Hoebeke, Jeroen},
title = {Interactive Web Visualizer for IEEE 802.11ah Ns-3 Module},
year = {2018},
isbn = {9781450364133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199902.3199904},
doi = {10.1145/3199902.3199904},
abstract = {The main purpose of running ns-3 simulations is to generate relevant data sets for
further study. There are two strategies to generate output from ns-3, either using
generic predefined bulk output mechanisms or using the ns-3's Tracing system. Both
require parsing the raw output data to extract and process the data of interest to
obtain meaningful information. However, parsing such output is in most cases time
consuming and prone to mistakes. Post-processing is even harder when a large number
of simulations needs to be analyzed and even the tracing system cannot simplify this
task. Moreover, results obtained this way are only available once the simulation is
finished.Therefore, we developed a user-friendly interactive visualization and post-processing
tool for IEEE 802.11ah called ahVisualizer. Beside the topology and MAC configuration,
ahVisualizer also plots our traces for each node over time during the simulation,
as well as averages and standard deviations for each traced parameter. It can compare
all the measured values across different simulations. Users can easily download figures
and data in various formats. Moreover, it includes a post-processing tool which plots
desired series, with desired fixed parameters, from a large set of simulations. This
paper presents the ahVisualizer, its services and its architecture and shows how this
tool enables much faster and easier data analysis and monitoring of ns-3 simulations
with 802.11ah.},
booktitle = {Proceedings of the 10th Workshop on Ns-3},
pages = {23–29},
numpages = {7},
keywords = {visualization, Wi-Fi HaLow, ns-3, post-processing, analysis, distributed simulations, IEEE 802.11ah},
location = {Surathkal, India},
series = {WNS3 '18}
}

@article{10.1145/3059149,
author = {Ying, Xuhang and Zhang, Jincheng and Yan, Lichao and Chen, Yu and Zhang, Guanglin and Chen, Minghua and Chandra, Ranveer},
title = {Exploring Indoor White Spaces in Metropolises},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3059149},
doi = {10.1145/3059149},
abstract = {It is a promising vision to exploit white spaces, that is, vacant VHF and UHF TV channels,
to meet the rapidly growing demand for wireless data services in both outdoor and
indoor scenarios. While most prior works have focused on outdoor white space, the
indoor story is largely open for investigation. Motivated by this observation and
discovering that 70% of the spectrum demand comes from indoor environment, we carry
out a comprehensive study to explore indoor white spaces. We first conduct a large-scale
measurement study and compare outdoor and indoor TV spectrum occupancy at 30+ diverse
locations in a typical metropolis—Hong Kong. Our results show that abundant white
spaces are available in different areas in Hong Kong, which account for more than
50% and 70% of the entire TV spectrum in outdoor and indoor scenarios, respectively.
Although there are substantially more white spaces indoors than outdoors, there have
been very few solutions for identifying indoor white space. To fill in this gap, we
develop the first data-driven, low-cost indoor white space identification system for
White-space Indoor Spectrum EnhanceR (WISER), to allow secondary users to identify
white spaces for communication without sensing the spectrum themselves. We design
the architecture and algorithms to address the inherent challenges. We build a WISER
prototype and carry out real-world experiments to evaluate its performance. Our results
show that WISER can identify 30%--40% more indoor white spaces with negligible false
alarms, as compared to alternative baseline approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {9},
numpages = {25},
keywords = {TV white spaces, sensor placement, clustering algorithms}
}

@article{10.1145/3448738,
author = {Shi, Cong and Liu, Jian and Liu, Hongbo and Chen, Yingying},
title = {WiFi-Enabled User Authentication through Deep Learning in Daily Activities},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1914},
url = {https://doi.org/10.1145/3448738},
doi = {10.1145/3448738},
abstract = {User authentication is a critical process in both corporate and home environments
due to the ever-growing security and privacy concerns. With the advancement of smart
cities and home environments, the concept of user authentication is evolved with a
broader implication by not only preventing unauthorized users from accessing confidential
information but also providing the opportunities for customized services corresponding
to a specific user. Traditional approaches of user authentication either require specialized
device installation or inconvenient wearable sensor attachment. This article supports
the extended concept of user authentication with a device-free approach by leveraging
the prevalent WiFi signals made available by IoT devices, such as smart refrigerator,
smart TV, and smart thermostat, and so on. The proposed system utilizes the WiFi signals
to capture unique human physiological and behavioral characteristics inherited from
their daily activities, including both walking and stationary ones. Particularly,
we extract representative features from channel state information (CSI) measurements
of WiFi signals, and develop a deep-learning-based user authentication scheme to accurately
identify each individual user. To mitigate the signal distortion caused by surrounding
people’s movements, our deep learning model exploits a CNN-based architecture that
constructively combines features from multiple receiving antennas and derives more
reliable feature abstractions. Furthermore, a transfer-learning-based mechanism is
developed to reduce the training cost for new users and environments. Extensive experiments
in various indoor environments are conducted to demonstrate the effectiveness of the
proposed authentication system. In particular, our system can achieve over 94% authentication
accuracy with 11 subjects through different activities.},
journal = {ACM Trans. Internet Things},
month = may,
articleno = {13},
numpages = {25},
keywords = {User authentication, IoT, WiFi signals}
}

@inproceedings{10.1145/3132340.3132358,
author = {Olariu, Stephan and Florin, Ryan},
title = {Vehicular Clouds Research: What is Missing?},
year = {2017},
isbn = {9781450351645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132340.3132358},
doi = {10.1145/3132340.3132358},
abstract = {Vehicular Clouds (VCs) have become an active research topic. However, even a cursory
look reveals that the VC literature of recent years is full of papers discussing fanciful
VC architectures and services that often seem too good to be true. And many of them
are. It seems to us that promoting VC models without any regard to their practical
feasibility is apt to discredit the VC concept altogether. Part of the problem stems
from the fact that some authors do not seem to be concerned with the obvious fact
that moving vehicles' residency times in the VC may, indeed, be very short and, therefore,
so is their contribution to the amount of useful work performed. Should a vehicle
running a user job leave the VC prematurely, the amount of work performed by that
vehicle may be lost, unless special precautions are taken. Such precautionary measures
involve either some flavor of checkpointing or some form of redundant job assignment.
Both approaches have consequences in terms of overhead and impact job completion time.
The success of conventional cloud computing (CC) is attributable to the ability to
provide quantifiable functional characteristics such as scalability, reliability and
availability. By the same token, if the VCs are to see a widespread adoption, the
same quantitative aspects have to be addressed here, too. Feasibility issues in terms
of sufficient compute power, communication bandwidth, reliability, availability, and
job duration time are all fundamental quantitative aspects of VCs that need to be
studied and understood before one can claim with any degree of certainty that they
can support the workload for which they are intended. The first contribution of this
paper is to make a case for the stringent need to address quantitatively the performance
characteristics of VC architectures and proposed services. Our second contribution
is to point out directions and challenges facing the VC community.},
booktitle = {Proceedings of the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {77–84},
numpages = {8},
keywords = {availability, acm proceedings, vehicular clouds, reliability, redundancy, cloud computing},
location = {Miami, Florida, USA},
series = {DIVANet '17}
}

@inproceedings{10.1145/3394885.3431616,
author = {Chakaravarthy, Ravikumar V. and Kwon, Hyun and Jiang, Hua},
title = {Vision Control Unit in Fully Self Driving Vehicles Using Xilinx MPSoC and Opensource Stack},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431616},
doi = {10.1145/3394885.3431616},
abstract = {Fully self-driving (FSD) vehicles are becoming increasing popular over the last few
years and companies are investing significantly into its research and development.
In the recent years, FSD technology innovators like Tesla, Google etc. have been working
on proprietary autonomous driving stacks and have been able to successfully bring
the vehicle to the roads. On the other end, organizations like Autoware Foundation
and Baidu are fueling the growth of self-driving mobility using open source stacks.
These organizations firmly believe in enabling autonomous driving technology for everyone
and support developing software stacks through the open source community that is SoC
vendor agnostic. In this proposed solution we describe a vision control unit for a
fully self-driving vehicle developed on Xilinx MPSoC platform using open source software
components.The vision control unit of an FSD vehicle is responsible for camera video
capture, image processing and rendering, AI algorithm processing, data and meta-data
transfer to next stage of the FSD pipeline. In this proposed solution we have used
many open source stacks and frameworks for video and AI processing. The processing
of the video pipeline and algorithms take full advantage of the pipelining and parallelism
using all the heterogenous cores of the Xilinx MPSoC. In addition, we have developed
an extensible, scalable, adaptable and configurable AI backend framework, XTA, for
acceleration purposes that is derived from a popular, open source AI backend framework,
TVM-VTA. XTA uses all the MPSoC cores for its computation in a parallel and pipelined
fashion. XTA also adapts to the compute and memory parameters of the system and can
scale to achieve optimal performance for any given AI problem. The FSD system design
is based on a distributed system architecture and uses open source components like
Autoware for autonomous driving algorithms, ROS and Distributed Data Services as a
messaging middleware between the functional nodes and a real-time kernel to coordinate
the actions. The details of image capture, rendering and AI processing of the vision
perception pipeline will be presented along with the performance measurements of the
vision pipeline.In this proposed solution we will demonstrate some of the key use
cases of vision perception unit like surround vision and object detection. In addition,
we will also show the capability of Xilinx MPSoC technology to handle multiple channels
of real time camera and the integration with the Lidar/Radar point cloud data to feed
into the decision-making unit of the overall system. The system is also designed with
the capability to update the vision control unit through Over the Air Update (OTA).
It is also envisioned that the core AI engine will require regular updates with the
latest training values; hence a built-in platform level mechanism supporting such
capability is essential for real world deployment.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {311–317},
numpages = {7},
keywords = {MPSoC, XTA, Vision Pipeline, Autoware, AI, Heterogenous Processing, FSD, ROS},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@inproceedings{10.1109/ICCPS.2014.6843740,
author = {Zhang, Jiaxing and Qiu, Hanjiao and Shamsabadi, Salar Shahini and Birken, Ralf and Schirner, Gunar},
title = {WiP Abstract: System-Level Integration of Mobile Multi-Modal Multi-Sensor Systems},
year = {2014},
isbn = {9781479949304},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCPS.2014.6843740},
doi = {10.1109/ICCPS.2014.6843740},
abstract = {Heterogeneous roaming sensor systems have gained significant importance in many domains
of civil infrastructure performance inspection as they accelerate data collection
and analysis. However, designing such systems is challenging due to the immense complexity
in the heterogeneity and processing demands of the involved sensors. Unifying frameworks
are needed to simplify development, deployment and operation of roaming sensors and
computing units.To address the sensing needs, we propose SIROM3, a Scalable Intelligent
Roaming Multi-Modal Multi-Sensor framework. SIROM3 incorporates a CPS approach for
infrastructure performance monitoring to address the following challenges: 1. Scalability
and expandability. It offers a scalable and expandable solution enabling diversity
in sensing and the growth in processing platforms from sensors to control centers.
2. Fusion foundations. SIROM3 enables fusion of data collected by logically and geo-spatially
distributed sensors. 3. Big data handling. Automatic collection, categorization, storage
and manipulation of heterogeneous large volume of data streams. 4. Automation. SIROM3
minimizes human interaction through full automation from data acquisition to visualization
of the fused results.Illustrated in Fig. 1, SIROM3 realizes scalability and expandability
in a system-level design approach encapsulating common functionality across hierarchical
components in a Run-Time Environment (RTE). The RTE deploys a layered design paradigm
defining services in both software and hardware architectures. Equipped with multiple
RTE-enabled Multi-Sensor Aggregators (MSA), an array of Roaming Sensor Systems (RSS)
operate as mobile agents attached to vehicles to provide distributed computing services
regulated by Fleet Control and Management (FCM) center via communication network.
A series of foundational services including the Precise Timing Protocol (PTP), GPS
timing systems, Distance Measurement Instruments (DMI) through middleware services
(CORBA) embedded in the RTE build the fusion foundations for data correlation and
analysis. A Heterogeneous Stream File-system Overlay (HSFO) alleviates the big data
challenge. It facilitates storing, processing, categorizing and fusing large heterogeneous
data stream collected by versatile sensors. A GIS visualization module is integrated
for visual analysis and monitoring.SIROM3 enables coordination and collaboration across
sensors, MSAs and RSSes, which produce high volume of heterogeneous data stored in
HSFO. To fuse the data efficiently, SIROM3 contains an expandable plugin system (part
of RTE) for rapid algorithm prototyping using data streams in the architectural hierarchy
(i.e. from MSAs to FCM) via the HSFO API. This makes an ideal test-bed to develop
new algorithms and methodologies expanding CPS principles to civil infrastructure
performance monitoring. In result, SIROM3 simplifies the development, construction
and operation of roaming multi-modal multi-sensor systems.We demonstrate the efficiency
of SIROM3 by automating the assessment of road surface conditions at the city scale.
We realized an RSS with 6 MSAs and 30 heterogeneous sensors, including radars, microphones,
GPS and cameras, all deployed onto a van sponsored by the VOTERS (Versatile Onboard
Traffic Embedded Roaming Sensors) project. Over 20 terabytes of data have been collected,
aggregated, fused, analyzed and geo-spatially visualized using SIROM3 for studying
the pavement conditions of the city of Brockton, MA covering 300 miles. The expandability
of SIROM3 is shown by adding a millimeter-wave radar needing less than 50 lines of
C++ code for system integration. SIROM3 offers a unified solution for comprehensive
roadway assessment and evaluation. The integrated management of big data (from collection
to automated processing) is an ideal research platform for automated assessment of
civil infrastructure performance.},
booktitle = {ICCPS '14: ACM/IEEE 5th International Conference on Cyber-Physical Systems (with CPS Week 2014)},
pages = {227},
numpages = {1},
location = {Berlin, Germany},
series = {ICCPS '14}
}

@article{10.1145/2935634.2935640,
author = {Orwat, Carsten and Bless, Roland},
title = {Values and Networks: Steps Toward Exploring Their Relationships},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2935634.2935640},
doi = {10.1145/2935634.2935640},
abstract = {Many technical systems of the Information and Communication Technology (ICT) sector
enable, structure and/or constrain social interactions. Thereby, they influence or
implement certain values, including human rights, and affect or raise conflicts among
values. The ongoing developments toward an "Internet of everything'' is likely to
lead to further value conflicts. This trend illustrates that a better understanding
of the relationships between social values and networks is urgently needed because
it is largely unknown what values lie behind protocols, design principles, or technical
and organizational options of the Internet. This paper focuses on the complex steps
of realizing human rights in Internet architectures and protocols as well as in Internet-based
products and services. Besides direct implementation of values in Internet protocols,
there are several other options that can indirectly contribute to realizing human
rights via political processes and market choices. Eventually, a better understanding
of what values can be realized by networks in general, what technical measures may
affect certain values, and where complementary institutional developments are needed
may lead toward a methodology for considering technical and institutional systems
together.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = may,
pages = {25–31},
numpages = {7},
keywords = {network design, rules, human rights, values, communication protocols, institutions, governance}
}

@inproceedings{10.1145/3447545.3451195,
author = {Calzarossa, Maria Carla and Massari, Luisa and Tessera, Daniele},
title = {Performance Monitoring Guidelines},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451195},
doi = {10.1145/3447545.3451195},
abstract = {Monitoring, that is, the process of collecting measurements on infrastructures and
services, is an important subject of performance engineering. Although monitoring
is not a new education topic, nowadays its relevance is rapidly increasing and its
application is particularly demanding due to the complex distributed architectures
of new and emerging technologies. As a consequence, monitoring has become a "must
have" skill for students majoring in computer science and in computing-related fields.
In this paper, we present a set of guidelines and recommendations to plan, design
and setup sound monitoring projects. Moreover, we investigate and discuss the main
challenges to be faced to build confidence in the entire monitoring process and ensure
measurement quality. Finally, we describe practical applications of these concepts
in teaching activities.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {109–114},
numpages = {6},
keywords = {performance monitoring, active measurements, measurement platforms, passive measurements, performance engineering, measurement quality},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1145/3390605,
author = {Ismail, Leila and Materwala, Huned},
title = {Computing Server Power Modeling in a Data Center: Survey, Taxonomy, and Performance Evaluation},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3390605},
doi = {10.1145/3390605},
abstract = {Data centers are large-scale, energy-hungry infrastructure serving the increasing
computational demands as the world is becoming more connected in smart cities. The
emergence of advanced technologies such as cloud-based services, internet of things
(IoT), and big data analytics has augmented the growth of global data centers, leading
to high energy consumption. This upsurge in energy consumption of the data centers
not only incurs the issue of surging high cost (operational and maintenance) but also
has an adverse effect on the environment. Dynamic power management in a data center
environment requires the cognizance of the correlation between the system and hardware-level
performance counters and the power consumption. Power consumption modeling exhibits
this correlation and is crucial in designing energy-efficient optimization strategies
based on resource utilization. Several works in power modeling are proposed and used
in the literature. However, these power models have been evaluated using different
benchmarking applications, power-measurement techniques, and error-calculation formulas
on different machines. In this work, we present a taxonomy and evaluation of 24 software-based
power models using a unified environment, benchmarking applications, power-measurement
techniques, and error formulas, with the aim of achieving an objective comparison.
We use different server architectures to assess the impact of heterogeneity on the
models’ comparison. The performance analysis of these models is elaborated in the
article.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {58},
numpages = {34},
keywords = {Data center, machine learning, server power consumption modeling, green computing, energy-efficiency, resource utilization}
}

@inproceedings{10.1145/3267955.3267972,
author = {Sardara, Mauro and Muscariello, Luca and Compagno, Alberto},
title = {A Transport Layer and Socket API for (h)ICN: Design, Implementation and Performance Analysis},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3267972},
doi = {10.1145/3267955.3267972},
abstract = {In this paper we present the design of a transport layer and socket API that can be
used in several ICN architectures such as NDN, CCN and hICN. The current design makes
it possible to expose an API that is simple to insert in current applications and
easy to use to develop novel ones. The proliferation of connected applications for
very different use cases and services with wide spectrum of requirements suggests
that several transport services will coexist in the Internet. This is just about to
happen with QUIC, MPTCP, LEDBAT as the most notable ones but is expected to grow and
diversify with the advent of applications for 5G, IoT, MEC with heterogeneous connectivity.
The advantages of ICN have to be measurable from the application, end-services and
in the network, with relevant key performance indicators. We have implemented an high
speed transport stack with most of the designed features that we present in this paper
with extensive experiments and benchmarks to show the scalability of the current systems
in different use cases.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {137–147},
numpages = {11},
keywords = {socket API, ICN, transport services},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@inproceedings{10.1145/3079856.3080210,
author = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
title = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080210},
doi = {10.1145/3079856.3080210},
abstract = {With increasing deployment of virtual machines for cloud services and server applications,
memory address translation overheads in virtualized environments have received great
attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss
necessitates up to 24 memory references for one guest to host translation. While dedicated
page walk caches and such recent enhancements eliminate many of these memory references,
our measurements on the Intel Skylake processors indicate that many programs in virtualized
mode of execution still spend hundreds of cycles for translations that do not hit
in the TLBs.This paper presents an innovative scheme to reduce the cost of address
translations by using a very large Translation Lookaside Buffer that is part of memory,
the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses
required in commonly used 2D walks with radix-4 type of page tables. Even if many
of the 24 accesses may hit in the page walk caches, the aggregated cost of the many
hits plus the overhead of occasional misses from page walk caches still exceeds the
cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space,
TLB entries (as opposed to multiple page table entries) can be cached in large L2
and L3 data caches, yielding significant benefits. Through detailed evaluation running
SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves
performance by approximately 10% on average. The improvement is more than 16% for
5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate
nearly all TLB misses in 8-core systems.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {469–480},
numpages = {12},
keywords = {Die-Stacked DRAM, Virtualization, Very Large TLB, Address Translation},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1145/3140659.3080210,
author = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
title = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080210},
doi = {10.1145/3140659.3080210},
abstract = {With increasing deployment of virtual machines for cloud services and server applications,
memory address translation overheads in virtualized environments have received great
attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss
necessitates up to 24 memory references for one guest to host translation. While dedicated
page walk caches and such recent enhancements eliminate many of these memory references,
our measurements on the Intel Skylake processors indicate that many programs in virtualized
mode of execution still spend hundreds of cycles for translations that do not hit
in the TLBs.This paper presents an innovative scheme to reduce the cost of address
translations by using a very large Translation Lookaside Buffer that is part of memory,
the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses
required in commonly used 2D walks with radix-4 type of page tables. Even if many
of the 24 accesses may hit in the page walk caches, the aggregated cost of the many
hits plus the overhead of occasional misses from page walk caches still exceeds the
cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space,
TLB entries (as opposed to multiple page table entries) can be cached in large L2
and L3 data caches, yielding significant benefits. Through detailed evaluation running
SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves
performance by approximately 10% on average. The improvement is more than 16% for
5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate
nearly all TLB misses in 8-core systems.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {469–480},
numpages = {12},
keywords = {Die-Stacked DRAM, Address Translation, Very Large TLB, Virtualization}
}

@inproceedings{10.1145/2876019.2876023,
author = {Pan, Xiang and Yegneswaran, Vinod and Chen, Yan and Porras, Phillip and Shin, Seungwon},
title = {HogMap: Using SDNs to Incentivize Collaborative Security Monitoring},
year = {2016},
isbn = {9781450340786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876019.2876023},
doi = {10.1145/2876019.2876023},
abstract = {Cyber Threat Intelligence (CTI) sharing facilitates a comprehensive understanding
of adversary activity and enables enterprise networks to prioritize their cyber defense
technologies. To that end, we introduce HogMap, a novel software-defined infrastructure
that simplifies and incentivizes collaborative measurement and monitoring of cyber-threat
activity. HogMap proposes to transform the cyber-threat monitoring landscape by integrating
several novel SDN-enabled capabilities: (i) intelligent in-place filtering of malicious
traffic, (ii) dynamic migration of interesting and extraordinary traffic and (iii)
a software-defined marketplace where various parties can opportunistically subscribe
to and publish cyber-threat intelligence services in a flexible manner.We present
the architectural vision and summarize our preliminary experience in developing and
operating an SDN-based HoneyGrid, which spans three enterprises and implements several
of the enabling capabilities (e.g., traffic filtering, traffic forwarding and connection
migration). We find that SDN technologies greatly simplify the design and deployment
of such globally distributed and elastic HoneyGrids.},
booktitle = {Proceedings of the 2016 ACM International Workshop on Security in Software Defined Networks &amp; Network Function Virtualization},
pages = {7–12},
numpages = {6},
keywords = {cyber threat intelligence, honeygrid, marketplace, sdn, honeynet},
location = {New Orleans, Louisiana, USA},
series = {SDN-NFV Security '16}
}

