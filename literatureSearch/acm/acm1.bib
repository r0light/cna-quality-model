@inproceedings{10.1145/2661829.2661991,
author = {Anchuri, Pranay and Sumbaly, Roshan and Shah, Sam},
title = {Hotspot Detection in a Service-Oriented Architecture},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661991},
doi = {10.1145/2661829.2661991},
abstract = {Large-scale websites are predominantly built as a service-oriented architecture. Here,
services are specialized for a certain task, run on multiple machines, and communicate
with each other to serve a user's request. Reducing latency and improving the cost
to serve is quite important, but optimizing this service call graph is particularly
challenging due to the volume of data and the graph's non-uniform and dynamic nature.In
this paper, we present a framework to detect hotspots in a service-oriented architecture.
The framework is general, in that it can handle arbitrary objective functions. We
show that finding the optimal set of hotspots for a metric, such as latency, is NP-complete
and propose a greedy algorithm by relaxing some constraints. We use a pattern mining
algorithm to rank hotspots based on the impact and consistency. Experiments on real
world service call graphs from LinkedIn, the largest online professional social network,
show that our algorithm consistently outperforms baseline methods.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {1749–1758},
numpages = {10},
keywords = {monitoring, call graph, service-oriented architecture, hotspots},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/2593501.2593505,
author = {Miranda, Breno and Bertolino, Antonia},
title = {Social Coverage for Customized Test Adequacy and Selection Criteria},
year = {2014},
isbn = {9781450328586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593501.2593505},
doi = {10.1145/2593501.2593505},
abstract = { Test coverage information can be very useful for guiding testers in enhancing their
test suites to exercise possible uncovered entities and in deciding when to stop testing.
However, for complex applications that are reused in different contexts and for emerging
paradigms (e.g., component-based development, service-oriented architecture, and cloud
computing), traditional coverage metrics may no longer provide meaningful information
to help testers on these tasks. Various proposals are advocating to leverage information
that come from the testing community in a collaborative testing approach. In this
work we introduce a coverage metric, the Social Coverage, that customizes coverage
information in a given context based on coverage data collected from similar users.
To evaluate the potential of our proposed approach, we instantiated the social coverage
metric in the context of a real world service oriented application. In this exploratory
study, we were able to predict the entities that would be of interest for a given
user with an average precision of 97% and average recall of 75%. Our results suggest
that, in similar environments, social coverage can provide a better support to testers
than traditional coverage. },
booktitle = {Proceedings of the 9th International Workshop on Automation of Software Test},
pages = {22–28},
numpages = {7},
keywords = {User Similarity, Coverage Testing, Service-Oriented Application, Relative Coverage},
location = {Hyderabad, India},
series = {AST 2014}
}

@article{10.1145/2579281.2579294,
author = {Castelluccia, Daniela and Boffoli, Nicola},
title = {Service-Oriented Product Lines: A Systematic Mapping Study},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579294},
doi = {10.1145/2579281.2579294},
abstract = {Software product line engineering and service-oriented architectures both enable organizations
to capitalize on reuse of existing software assets and capabilities and improve competitive
advantage in terms of development savings, product flexibility, time-to-market. Both
approaches accommodate variation of assets, including services, by changing the software
being reused or composing services according a new orchestration. Therefore, variability
management in Service-oriented Product Lines (SoPL) is one of the main challenges
today. In order to highlight the emerging evidence-based results from the research
community, we apply the well-defined method of systematic mapping in order to populate
a classification scheme for the SoPL field of interest. The analysis of results throws
light on the current open issues. Moreover, different facets of the scheme can be
combined to answer more specific research questions. The report reveals the need for
more empirical research able to provide new metrics measuring efficiency and efficacy
of the proposed models, new methods and tools supporting variability management in
SoPL, especially during maintenance and verification and validation. The mapping study
about SoPL opens further investigations by means of a complete systematic review to
select and validate the most efficient solutions to variability management in SoPL.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6},
keywords = {empirical study, mapping study, service-oriented computing, product line development, service-oriented architecture, software product line, variability management}
}

@inproceedings{10.5555/3021955.3022024,
author = {Oliveira, Joyce Aline and Junior, Jose J.L.D.},
title = {A Three-Dimensional View of Reuse in Service Oriented Architecture},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {The reuse in Service Oriented Architecture (SOA) has been used strategically in organizations
to reduce development costs and increase the quality of applications. This article
reports a qualitative research realized with experts in order to identify goals, barriers,
facilitators, strategies, metrics and benefits associated with reuse in SOA. The results
were summarized in three dimensions (management, architecture, operation) and represented
by a conceptual model that can serve as a preliminary roadmap to manage the reuse
in SOA.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {409–416},
numpages = {8},
keywords = {qualitative research, SOA reuse, Services Oriented Architecture},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI 2016}
}

@inproceedings{10.1145/3034950.3034975,
author = {Ke, Weimao},
title = {Distributed Search Efficiency and Robustness in Service Oriented Multi-Agent Networks},
year = {2017},
isbn = {9781450348348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3034950.3034975},
doi = {10.1145/3034950.3034975},
abstract = {We study decentralized searches in a large service-oriented agent network and investigate
the influences of multiple factors on search efficiency. In this study we focus on
overall system robustness and examine search performance in unstable environments
where individual agents may fail or a system-wide attack may occur. Experimental results
show that searches continue to be efficient when a large number of service agents
become unavailable. Surprisingly, overall system performance in terms of a search
path length metric improves with an increasing number of unavailable agents. Service
unavailability also has an impact on the load balance of service agents. We plan to
conduct further research to verify observed patterns and to understand related implications
on system architecture design.},
booktitle = {Proceedings of the 2017 International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {9–18},
numpages = {10},
keywords = {distributed search, information network, robustness, service agents},
location = {Wuhan, China},
series = {ICMSS '17}
}

@inproceedings{10.1145/3018896.3018961,
author = {Lehmann, Martin and Sandnes, Frode Eika},
title = {A Framework for Evaluating Continuous Microservice Delivery Strategies},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018961},
doi = {10.1145/3018896.3018961},
abstract = {The emergence of service-oriented computing, and in particular microservice architecture,
has introduced a new layer of complexity to the already challenging task of continuously
delivering changes to the end users. Cloud computing has turned scalable hardware
into a commodity, but also imposes some requirements on the software development process.
Yet, the literature mainly focuses on quantifiable metrics such as number of manual
steps and lines of code required to make a change. The industry, on the other hand,
appears to focus more on qualitative metrics such as increasing the productivity of
their developers. These are common goals, but must be measured using different approaches.
Therefore, based on interviews of industry stakeholders a framework for evaluating
and comparing approaches to continuous microservice delivery is proposed. We show
that it is possible to efficiently evaluate and compare strategies for continuously
delivering microservices.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {64},
numpages = {9},
keywords = {microservices, microservice architectures, deployment strategy, cloud computing, evaluation framework, continuous deployment},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3229345.3229419,
author = {Oliveira, Joyce Aline and Vargas, Matheus and Rodrigues, Roni},
title = {SOA Reuse: Systematic Literature Review Updating and Research Directions},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229419},
doi = {10.1145/3229345.3229419},
abstract = {Service Oriented Architecture (SOA) reuse has been used strategically in organizations
to reduce development costs and increase the quality of applications. This article
analyzes a systematic literature review in order to identify concepts, goals, strategies,
and metrics of SOA reuse. The results show that the main goal of SOA reuse is to decrease
development costs. The factor that most negatively influences SOA reuse is the existence
of legacy systems. The strategy used most to potentialize SOA reuse is business process
management. Metrics proposed by studies to measure SOA reuse are related to modularity
and adaptability indicators. The study is relevant because it increases the body of
knowledge of the area. Additionally, a set of gaps to be addressed by researchers
and reuse practitioners was identified.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {71},
numpages = {8},
keywords = {Service Oriented Architecture, systematic literature review, SOA reuse},
location = {Caxias do Sul, Brazil},
series = {SBSI'18}
}

@inproceedings{10.1145/3284557.3284713,
author = {Rudorfer, Martin and Pannen, Tessa J. and Kr\"{u}ger, J\"{o}rg},
title = {A Case Study on Granularity of Industrial Vision Services},
year = {2018},
isbn = {9781450366281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284557.3284713},
doi = {10.1145/3284557.3284713},
abstract = {Software engineering paradigms such as service-oriented architectures are increasingly
often applied in the field of factory automation. Functions like robot motion planning
or object recognition are provided by cloud services. A crucial architectural aspect
is the granularity, i.e. the scope and size of individual services. In our case study,
we examine a service-based object recognition application for a robotic assembly use
case. We implement three different granularity levels, measure their communication
and computation times and discuss further architectural features. The fine-granular
approach encapsulates individual image processing operations as services, which have
high reusability but impose large communication overheads. The medium granularity
approach is object-wise and offers best reuse efficiency and cohesion. The coarse
solution offers the best performance.},
booktitle = {Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control},
articleno = {59},
numpages = {6},
keywords = {Object Recognition, Granularity, Service-Oriented Architecture},
location = {Stockholm, Sweden},
series = {ISCSIC '18}
}

@inproceedings{10.1145/3143434.3143457,
author = {Ilin, I. and Levina, A. and Abran, A. and Iliashenko, O.},
title = {Measurement of Enterprise Architecture (EA) from an IT Perspective: Research Gaps and Measurement Avenues},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143457},
doi = {10.1145/3143434.3143457},
abstract = {Reorganizational projects in general and software-related projects in particular,
are often implemented with a focus only on the reorganized components within an organizational
management system, not taking into account relationships with the other components
of an enterprise architecture (EA). This paper first looks at the current state of
EA measurement to identify weaknesses and gaps in aligning and measuring EA components,
EA structures and EA interrelationships from an IT perspective. It then identifies
from related works available innovative measurement concepts that could contribute
for aligning, measuring and monitoring software-related projects within an EA strategy.
This includes measurement avenues within a Balanced Scorecard (BSC), contributions
of functional size measurement to the BSC, and measurement of software structures
and functionality within a service-oriented architecture (SOA).},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {232–243},
numpages = {12},
keywords = {balanced scorecard (BSC), function points (FP), enterprise architecture (EA), enterprise architecture measurement, service-oriented architecture (SOA), functional size measurement (FSM)},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@inproceedings{10.1145/2568225.2568230,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Integrating Adaptive User Interface Capabilities in Enterprise Applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568230},
doi = {10.1145/2568225.2568230},
abstract = { Many existing enterprise applications are at a mature stage in their development
and are unable to easily benefit from the usability gains offered by adaptive user
interfaces (UIs). Therefore, a method is needed for integrating adaptive UI capabilities
into these systems without incurring a high cost or significantly disrupting the way
they function. This paper presents a method for integrating adaptive UI behavior in
enterprise applications based on CEDAR, a model-driven, service-oriented, and tool-supported
architecture for devising adaptive enterprise application UIs. The proposed integration
method is evaluated with a case study, which includes establishing and applying technical
metrics to measure several of the method’s properties using the open-source enterprise
application OFBiz as a test-case. The generality and flexibility of the integration
method are also evaluated based on an interview and discussions with practitioners
about their real-life projects. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {712–723},
numpages = {12},
keywords = {Adaptive user interfaces, integration, software architectures, model-driven engineering, software metrics, enterprise systems},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3018896.3036365,
author = {Ashrafi, Tasnia H. and Arefin, Sayed E. and Das, Kowshik D. J. and Hossain, Md. A. and Chakrabarty, Amitabha},
title = {FOG Based Distributed IoT Infrastructure},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3036365},
doi = {10.1145/3018896.3036365},
abstract = {The Internet of Things(IoT) can be defined as a network connectivity bridge between
people, systems and physical world. With the increasing number of IoT devices and
networks, dealing with enormous number of data efficiently is becoming more and more
challenging for the present infrastructure which is a very big matter of concern.
In this paper, we depicted the current infrastructure and proposed another model of
IoT infrastructure to surpass the difficulties of the existing infrastructure, which
will be a coordinated effort of Fog computing amalgamation with Machine-to-Machine(M2M)
intelligent communication protocol followed by incorporation of Service Oriented Architecture(SOA)
and finally integration of Agent based SOA. This model will have the capacity to exchange
data by breaking down dependably and methodically with low latency, less bandwidth,
heterogeneity in less measure of time maintaining the Quality of Service(QoS) precisely.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {124},
numpages = {13},
keywords = {fog computing, M2M communication, heterogeneous devices, agent based SOA, internet of things (IoT), quality of service (QoS), service oriented architecture(SOA)},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3068126.3068128,
author = {Filelis-Papadopoulos, C. K. and Gravvanis, G. A. and Morrison, J. P.},
title = {CloudLightning Simulation and Evaluation Roadmap},
year = {2017},
isbn = {9781450349369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3068126.3068128},
doi = {10.1145/3068126.3068128},
abstract = {The CloudLightning (CL) system, designed in the frame of the CloudLightning project,
is a service-oriented architecture for the emerging large scale heterogeneous cloud.
It facilitates a clear distinction between service-lifecyle management and resource-lifecycle
management. This separation of concerns is used to make resource management issues
tractable at scale and to enable functionality that is currently not naturally covered
by the cloud paradigm. In particular, the CL project seeks to maximize computational
efficiency of the cloud in a number of specific ways; by exploiting prebuilt HPC environments,
by dynamically building HPC instances, by improving server utilization, by reducing
power consumption and by improving service delivery. Given the scale and complexity
of this project, its utility can presently only be measured through simulation. This
paper outlines the parameters, constraints and limitation being considered as part
of the design and construction of that simulation environment.},
booktitle = {Proceedings of the 1st International Workshop on Next Generation of Cloud Architectures},
articleno = {2},
numpages = {6},
keywords = {CloudLightning, Self - Organisation, Self - Management, Evaluation},
location = {Belgrade, Serbia},
series = {CloudNG:17}
}

@inproceedings{10.1145/3194164.3194166,
author = {Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred},
title = {Limiting Technical Debt with Maintainability Assurance: An Industry Survey on Used Techniques and Differences with Service- and Microservice-Based Systems},
year = {2018},
isbn = {9781450357135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194164.3194166},
doi = {10.1145/3194164.3194166},
abstract = {Maintainability assurance techniques are used to control this quality attribute and
limit the accumulation of potentially unknown technical debt. Since the industry state
of practice and especially the handling of Service- and Microservice-Based Systems
in this regard are not well covered in scientific literature, we created a survey
to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related
treatment of systems based on service-orientation, and c) influences on developer
satisfaction w.r.t. maintainability. 60 software professionals responded to our online
questionnaire. The results indicate that using explicit and systematic techniques
has benefits for maintainability. The more sophisticated the applied methods the more
satisfied participants were with the maintainability of their software while no link
to a hindrance in productivity could be established. Other important findings were
the absence of architecture-level evolvability control mechanisms as well as a significant
neglect of service-oriented particularities for quality assurance. The results suggest
that industry has to improve its quality control in these regards to avoid problems
with long-living service-based software systems.},
booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
pages = {125–133},
numpages = {9},
keywords = {microservice-based systems, maintainability, industry, survey, software quality control, service-based systems},
location = {Gothenburg, Sweden},
series = {TechDebt '18}
}

@inproceedings{10.1145/3019612.3019805,
author = {Abid, Ahmed and Messai, Nizar and Rouached, Mohsen and Abid, Mohamed and Devogele, Thomas},
title = {Semantic Similarity Based Web Services Composition Framework},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019805},
doi = {10.1145/3019612.3019805},
abstract = {Computing similarities between Web services is a main concern in Service Oriented
Architecture as it allows to decide which services are likely to be matched into a
composite workflow, or in other cases, which services can be substituted in order
to ensure continuous service availability. With the high maturity achieved by the
standards, tools and frameworks in the Semantic Web domain, measuring Web services
similarities relies more than ever on semantic descriptions of services as well as
on semantic relationships these descriptions may hold. In this paper we present a
Framework for Web services composition based on computing semantic similarity between
Web services. We particularly focus on Services Matching engine which uses the considered
similarity measure first to classify Web services into classes of functionally similar
Web services and then to propose a composite sequence of services that matches a requested
goal. In both tasks, the presented framework appeals for best known techniques of
similarity computing and data and knowledge extraction, respectively.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1319–1325},
numpages = {7},
keywords = {matching, semantic similarity, web services, discovery and composition},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/2642937.2653471,
author = {Miranda, Breno},
title = {A Proposal for Revisiting Coverage Testing Metrics},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2653471},
doi = {10.1145/2642937.2653471},
abstract = {Test coverage information can be very useful for guiding testers in enhancing their
test suites to exercise possible uncovered entities and in deciding when to stop testing.
Since the concept of test criterion was born, several contributions have been made
by both academia and industry in the definition and adaptation of adequacy criteria
aiming at ensuring the discovery of more failures. Numerous contributions have also
been done in the development of coverage tools. However, for complex applications
that are reused in different contexts and for emerging paradigms (e.g., component-based
development, service-oriented architecture, and cloud computing), traditional coverage
metrics may no longer provide meaningful information to help testers on these tasks.
Inspired by the idea of relative coverage this research focuses on the introduction
of meaningful coverage metrics to cope with the challenges imposed by the current
programming paradigms as well as on the definition of a theoretical framework for
the development of relative coverage metrics.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {899–902},
numpages = {4},
keywords = {traditional coverage, coverage testing, relative coverage},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3127479.3132020,
author = {Suresh, Lalith and Bodik, Peter and Menache, Ishai and Canini, Marco and Ciucu, Florin},
title = {Distributed Resource Management across Process Boundaries},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3132020},
doi = {10.1145/3127479.3132020},
abstract = {Multi-tenant distributed systems composed of small services, such as Service-oriented
Architectures (SOAs) and Micro-services, raise new challenges in attaining high performance
and efficient resource utilization. In these systems, a request execution spans tens
to thousands of processes, and the execution paths and resource demands on different
services are generally not known when a request first enters the system. In this paper,
we highlight the fundamental challenges of regulating load and scheduling in SOAs
while meeting end-to-end performance objectives on metrics of concern to both tenants
and operators. We design Wisp, a framework for building SOAs that transparently adapts
rate limiters and request schedulers system-wide according to operator policies to
satisfy end-to-end goals while responding to changing system conditions. In evaluations
against production as well as synthetic workloads, Wisp successfully enforces a range
of end-to-end performance objectives, such as reducing average latencies, meeting
deadlines, providing fairness and isolation, and avoiding system overload.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {611–623},
numpages = {13},
keywords = {resource management, service-oriented architectures, microservices, scheduling, rate limiting},
location = {Santa Clara, California},
series = {SoCC '17}
}

@article{10.5555/3288897.3288933,
author = {Wu, Yumei and Fang, Yuanyuan and Liu, Bin and Zhao, Zehui},
title = {A Novel Service Deployment Approach Based on Resilience Metrics for Service-Oriented System},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {5–6},
issn = {1617-4909},
abstract = {Service-Oriented Architecture (SOA) has been widely used in IT areas and is expected
to bring a lot of benefits. However, the SOA system developers have to address new
challenging issues such as computational resource failure before such benefits can
be realized. This paper develops a graph-theoretic model for the SOA system and proposes
metrics that quantify the resilience of such system under resource failures. It explores
two service deployment strategies to optimize resilience by taking not only communication
costs among services but also the computation costs of services into consideration.
Among them, two types of undirected graphs are developed to model the relationships
between services, including Service Dependence Graph (SDG) and Service Concurrence
Graph (SCG). Then, these two graphs are integrated into Service Relationship Graph
(SRG) and adopt the k-cut optimization theory to complete the service deployment.
Finally, this paper verifies the effectiveness of the above methods in improving the
resilience of the system through a series of experiments, which indicate that our
methods perform better than the previous methods in improving resilience of the SOA
system.},
journal = {Personal Ubiquitous Comput.},
month = oct,
pages = {1099–1107},
numpages = {9},
keywords = {Resilience, SOA, Service relationship graph, Service deployment}
}

@inproceedings{10.1145/2739480.2754724,
author = {Ouni, Ali and Gaikovina Kula, Raula and Kessentini, Marouane and Inoue, Katsuro},
title = {Web Service Antipatterns Detection Using Genetic Programming},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754724},
doi = {10.1145/2739480.2754724},
abstract = {Service-Oriented Architecture (SOA) is an emerging paradigm that has radically changed
the way software applications are architected, designed and implemented. SOA allows
developers to structure their systems as a set of ready-made, reusable and compostable
services. The leading technology used today for implementing SOA is Web Services.
Indeed, like all software, Web services are prone to change constantly to add new
user requirements or to adapt to environment changes. Poorly planned changes may risk
introducing antipatterns into the system. Consequently, this may ultimately leads
to a degradation of software quality, evident by poor quality of service (QoS). In
this paper, we introduce an automated approach to detect Web service antipatterns
using genetic programming. Our approach consists of using knowledge from real-world
examples of Web service antipatterns to generate detection rules based on combinations
of metrics and threshold values. We evaluate our approach on a benchmark of 310 Web
services and a variety of five types of Web service antipatterns. The statistical
analysis of the obtained results provides evidence that our approach is efficient
to detect most of the existing antipatterns with a score of 85% of precision and 87%
of recall.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1351–1358},
numpages = {8},
keywords = {web services, search-based software engineering, antipatterns},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/3330204.3330259,
author = {Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel},
title = {Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330259},
doi = {10.1145/3330204.3330259},
abstract = {In the last decade the (big) data-driven science paradigm became a wide-spread reality.
However, this approach has some limitations such as a performance dependency on the
quality of the data and the lack of reproducibility of the results. In order to enable
this reproducibility, many tools such as Workflow Management Systems were developed
to formalize process pipelines and capture execution traces. However, interoperating
data generated by these solutions became a problem, since most systems adopted proprietary
data models. To support interoperability across heterogeneous provenance data, we
propose a Service Oriented Architecture with a polystore storage design in which provenance
is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible
for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover,
we propose a query layer that provides location and access transparency to users.
Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios.
Firstly, we illustrate how two research groups can compare their processes and results.
Secondly, we show how our architecture can be used as a queriable provenance repository.
We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology.
Finally, we show our solution usability and extensibility appeal by comparing it to
similar approaches.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {49},
numpages = {8},
keywords = {polystore, Workflows interoperability, heterogeneous provenance data integration},
location = {Aracaju, Brazil},
series = {SBSI'19}
}

@inproceedings{10.1145/3230833.3233278,
author = {Mathas, Christos M. and Segou, Olga E. and Xylouris, Georgios and Christinakis, Dimitris and Kourtis, Michail-Alexandros and Vassilakis, Costas and Kourtis, Anastasios},
title = {Evaluation of Apache Spot's Machine Learning Capabilities in an SDN/NFV Enabled Environment},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233278},
doi = {10.1145/3230833.3233278},
abstract = {Software Defined Networking (SDN) and Network Function Virtualisation (NFV) are transforming
modern networks towards a service-oriented architecture. At the same time, the cybersecurity
industry is rapidly adopting Machine Learning (ML) algorithms to improve detection
and mitigation of complex attacks. Traditional intrusion detection systems perform
signature-based detection, based on well-known malicious traffic patterns that signify
potential attacks. The main drawback of this method is that attack patterns need to
be known in advance and signatures must be preconfigured. Hence, typical systems fail
to detect a zero-day attack or an attack with unknown signature. This work considers
the use of machine learning for advanced anomaly detection, and specifically deploys
the Apache Spot ML framework on an SDN/NFV-enabled testbed running cybersecurity services
as Virtual Network Functions (VNFs). VNFs are used to capture traffic for ingestion
by the ML algorithm and apply mitigation measures in case of a detected anomaly. Apache
Spot utilises Latent Dirichlet Allocation to identify anomalous traffic patterns in
Netflow, DNS and proxy data. The overall performance of Apache Spot is evaluated by
deploying Denial of Service (Slowloris, BoNeSi) and a Data Exfiltration attack (iodine).},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {52},
numpages = {10},
keywords = {Apache Spot, Software Defined Networking, Network Function Virtualisation, Latent Dirichlet Allocation, Penetration Testing, Machine Learning, SHIELD Project},
location = {Hamburg, Germany},
series = {ARES 2018}
}

@article{10.14778/2733004.2733009,
author = {Poess, Meikel and Rabl, Tilmann and Jacobsen, Hans-Arno and Caufield, Brian},
title = {TPC-DI: The First Industry Benchmark for Data Integration},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733009},
doi = {10.14778/2733004.2733009},
abstract = {Historically, the process of synchronizing a decision support system with data from
operational systems has been referred to as Extract, Transform, Load (ETL) and the
tools supporting such process have been referred to as ETL tools. Recently, ETL was
replaced by the more comprehensive acronym, data integration (DI). DI describes the
process of extracting and combining data from a variety of data source formats, transforming
that data into a unified data model representation and loading it into a data store.
This is done in the context of a variety of scenarios, such as data acquisition for
business intelligence, analytics and data warehousing, but also synchronization of
data between operational applications, data migrations and conversions, master data
management, enterprise data sharing and delivery of data services in a service-oriented
architecture context, amongst others. With these scenarios relying on up-to-date information
it is critical to implement a highly performing, scalable and easy to maintain data
integration system. This is especially important as the complexity, variety and volume
of data is constantly increasing and performance of data integration systems is becoming
very critical. Despite the significance of having a highly performing DI system, there
has been no industry standard for measuring and comparing their performance. The TPC,
acknowledging this void, has released TPC-DI, an innovative benchmark for data integration.
This paper motivates the reasons behind its development, describes its main characteristics
including workload, run rules, metric, and explains key decisions.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1367–1378},
numpages = {12}
}

@inproceedings{10.1145/2642668.2642679,
author = {Hassam, Mickael and Kara, Nadjia and Belqasmi, Fatna and Glitho, Roch},
title = {Virtualized Infrastructure for Video Game Applications in Cloud Environments},
year = {2014},
isbn = {9781450330268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642668.2642679},
doi = {10.1145/2642668.2642679},
abstract = {Mobile video games are fast-growing and fast-evolving. Cloud computing's paradigm
can bring several benefits to mobile video games, like cost reduction through an efficient
usage of resources, or an easier and faster on-demand deployment of new applications.
This paper focuses on the architecture aspects of mobile cloud-based video gaming
and proposes a new service oriented and virtualized paradigm. The idea is to provide
reusable game engines sub modules like the rendering or physics engines as cloud computing
services. We call these sub modules substrates. Offered by different substrates providers
as services, they can be dynamically discovered, used and composed. There are several
motivations for substrates virtualization, including the rapid introduction of new
video game applications and cost efficiency through resource sharing. This paper also
describes the implementation of a prototype and the measurements performed to validate
some aspects of our paradigm. As a preliminary validation of this solution, we analyze
the effects of different parameters like virtualization or inner latency on the QoS.
The performance analysis shows that the overhead introduced by substrate virtualization
is acceptable, and reveals how the low-latency connectivity between substrates that
compose a video game application and the limitation of the amount of these substrates
are crucial to achieve a satisfactory level of QoS.},
booktitle = {Proceedings of the 12th ACM International Symposium on Mobility Management and Wireless Access},
pages = {109–114},
numpages = {6},
keywords = {infrastructure and platform as services, mobile video game applications, virtualization, cloud computing, substrate},
location = {Montreal, QC, Canada},
series = {MobiWac '14}
}

@inproceedings{10.1109/CCGrid.2015.148,
author = {Beier, Maximilian and Jansen, Christoph and Mayer, Geert and Penzel, Thomas and Rodenbeck, Andrea and Siewert, Ren\'{e} and Wu, Jie and Krefting, Dagmar},
title = {Multicenter Data Sharing for Collaboration in Sleep Medicine},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.148},
doi = {10.1109/CCGrid.2015.148},
abstract = {Clinical Sleep Research is an inherent multidisciplinary field, as many health issues
may affect a person's sleep conditions and sleep disorders may cause several health
problems. Many patients with chronic sleep disorders suffer from different further
medical conditions - called multimorbidity. Due to the high variety of the reasons
and the courses of sleep disorders, individual cases are difficult to compare. Therefore
there is a high demand for sleep researchers to collaborate with each other to reach
necessary participant numbers and multidisciplinary expertise. To date, inter-institutional
sleep research is poorly supported by IT systems. In particular the heterogeneity
and the quality variations within the acquired biosignal data - caused by different
biosignal recorders or different measurement procedures - are impeding common biosignal
data processing. In this manuscript we introduce a virtual research platform supporting
inter-institutional data sharing and processing. The infrastructure is based on XNAT
- a free and open-source neuroimaging research platform - a loosely coupled service
oriented architecture and scalable virtualization in the backend. The system is capable
of local pseudonymization of biosignal data, mapping to a standardized set of parameters
and automatic quality assessment. Terms and quality measures are derived from the
"Manual for the Scoring of Sleep and Associated Events" of the American Academy of
Sleep Medicine, the de-facto standard for diagnostic biosignal analysis in sleep medicine.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {880–889},
numpages = {10},
keywords = {REST, biosignal, XNAT, cloud, sleep, polysomnography, OpenStack},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@article{10.1145/3386041,
author = {Shi, Min and Tang, Yufei and Zhu, Xingquan and Liu, Jianxun},
title = {Topic-Aware Web Service Representation Learning},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3386041},
doi = {10.1145/3386041},
abstract = {The advent of Service-Oriented Architecture (SOA) has brought a fundamental shift
in the way in which distributed applications are implemented. An overwhelming number
of Web-based services (e.g., APIs and Mashups) have leveraged this shift and furthered
development. Applications designed with SOA principles are typically characterized
by frequent dependencies with one another in the form of heterogeneous networks, i.e.,
annotation relations between tags and services, and composition relations between
Mashups and APIs. Although prior work has shown the utility gained by exploring these
networks, their analysis is still in its infancy. This article develops an approach
to learning representations of the Web service network, which seeks to embed Web services
in low-dimensional continuous vectors with preserved information of the network structure,
functional tags, and service descriptions, such that services with similar functional
properties and network structures are mapped together in the learned latent space.
We first propose a topic generative model for constructing two topic distribution
networks (Mashup-Topic and API-Topic) from the service content. Then, we present an
efficient optimization process to derive low-dimensional vector representations of
Web services from a tri-layer bipartite network with the Mashup-Topic and API-Topic
networks on two ends and the Mashup-API composition network in the middle. Experiments
on real-word datasets have verified that our approach is effective to learn robust
low-rank service representations, i.e., 25% F1-measure gain over the state-of-the-art
in Web service recommendation task.},
journal = {ACM Trans. Web},
month = apr,
articleno = {9},
numpages = {23},
keywords = {network embedding, service representation, Web services, probabilistic topic model, Mashups}
}

@inproceedings{10.1145/3385032.3385042,
author = {Tummalapalli, Sahithi and Kumar, Lov and Murthy, N. L. Bhanu},
title = {Prediction of Web Service Anti-Patterns Using Aggregate Software Metrics and Machine Learning Techniques},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385042},
doi = {10.1145/3385032.3385042},
abstract = {Service-Oriented Architecture(SOA) can be characterized as an approximately coupled
engineering intended to meet the business needs of an association/organization. Service-Based
Systems (SBSs) are inclined to continually change to enjoy new client necessities
and adjust the execution settings, similar to some other huge and complex frameworks.
These changes may lead to the evolution of designs/products with poor Quality of Service
(QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns.
Anti-patterns makes the evolution and maintenance of the software systems hard and
complex. Early identification of modules, classes, or source code regions where anti-patterns
are more likely to occur can help in amending and maneuvering testing efforts leading
to the improvement of software quality. In this work, we investigate the application
of three sampling techniques, three feature selection techniques, and sixteen different
classification techniques to develop the models for web service anti-pattern detection.
We report the results of an empirical study by evaluating the approach proposed, on
a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of
five types of web-service anti-patterns. Experimental results demonstrated that SMOTE
is the best performing data sampling techniques. The experimental results also reveal
that the model developed by considering Uncorrelated Significant Predictors(SUCP)
as the input obtained better performance compared to the model developed by other
metrics. Experimental results also show that the Least Square Support Vector Machine
with Linear(LSLIN) function has outperformed all other classifier techniques.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
articleno = {8},
numpages = {11},
keywords = {Anti-pattern, Classifiers, Service-Based Systems(SBS), Aggregation measures, WSDL, Feature Selection, Class imbalance distribution, Machine Learning, Web-Services, Source Code Metrics},
location = {Jabalpur, India},
series = {ISEC 2020}
}

@inproceedings{10.1145/3011141.3011179,
author = {de Camargo, Andr\'{e} and Salvadori, Ivan and Mello, Ronaldo dos Santos and Siqueira, Frank},
title = {An Architecture to Automate Performance Tests on Microservices},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011179},
doi = {10.1145/3011141.3011179},
abstract = {The microservices architecture provides a new approach to develop applications. As
opposed to monolithic applications, in which the application comprises a single software
artifact, an application based on the microservices architecture is composed by a
set of services, each one designed to perform a single and well-defined task. These
services allow the development team to decouple several parts of the application using
different frameworks, languages and hardware for each part of the system. One of the
drawbacks for adopting the microservices architecture to develop applications is testability.
In a single application test boundaries can be more easily established and tend to
be more stable as the application evolves, while with microservices we can have a
set of hundreds of services that operate together and are prone to change more rapidly.
Each one of these services needs to be tested and updated as the service changes.
In addition, the different characteristics of these services such as languages, frameworks
or the used infrastructure have to be considered in the testing phase. Performance
tests are applied to assure that a particular software complies with a set of non-functional
requirements such as throughput and response time. These metrics are important to
ensure that business constraints are respected and to help finding performance bottlenecks.
In this paper, we present a new approach to allow the performance tests to be executed
in an automated way, with each microservice providing a test specification that is
used to perform tests. Along with the architecture, we also provide a framework that
implements some key concepts of this architecture. This framework is available as
an open source project1.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {422–429},
numpages = {8},
keywords = {test automation, performance test, microservices},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@inproceedings{10.1145/3412841.3442016,
author = {Brito, Miguel and Cunha, J\'{a}come and Saraiva, Jo\~{a}o},
title = {Identification of Microservices from Monolithic Applications through Topic Modelling},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442016},
doi = {10.1145/3412841.3442016},
abstract = {Microservices emerged as one of the most popular architectural patterns in the recent
years given the increased need to scale, grow and flexibilize software projects accompanied
by the growth in cloud computing and DevOps. Many software applications are being
submitted to a process of migration from its monolithic architecture to a more modular,
scalable and flexible architecture of microservices. This process is slow and, depending
on the project's complexity, it may take months or even years to complete.This paper
proposes a new approach on microservice identification by resorting to topic modelling
in order to identify services according to domain terms. This approach in combination
with clustering techniques produces a set of services based on the original software.
The proposed methodology is implemented as an open-source tool for exploration of
monolithic architectures and identification of microservices. A quantitative analysis
using the state of the art metrics on independence of functionality and modularity
of services was conducted on 200 open-source projects collected from GitHub. Cohesion
at message and domain level metrics' showed medians of roughly 0.6. Interfaces per
service exhibited a median of 1.5 with a compact interquartile range. Structural and
conceptual modularity revealed medians of 0.2 and 0.4 respectively.Our first results
are positive demonstrating beneficial identification of services due to overall metrics'
results.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1409–1418},
numpages = {10},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3366423.3380111,
author = {Ma, Meng and Xu, Jingmin and Wang, Yuan and Chen, Pengfei and Zhang, Zonghua and Wang, Ping},
title = {AutoMAP: Diagnose Your Microservice-Based Web Applications Automatically},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380111},
doi = {10.1145/3366423.3380111},
abstract = {The high complexity and dynamics of the microservice architecture make its application
diagnosis extremely challenging. Static troubleshooting approaches may fail to obtain
reliable model applies for frequently changing situations. Even if we know the calling
dependency of services, we lack a more dynamic diagnosis mechanism due to the existence
of indirect fault propagation. Besides, algorithm based on single metric usually fail
to identify the root cause of anomaly, as single type of metric is not enough to characterize
the anomalies occur in diverse services. In view of this, we design a novel tool,
named AutoMAP, which enables dynamic generation of service correlations and automated
diagnosis leveraging multiple types of metrics. In AutoMAP, we propose the concept
of anomaly behavior graph to describe the correlations between services associated
with different types of metrics. Two binary operations, as well as a similarity function
on behavior graph are defined to help AutoMAP choose appropriate diagnosis metric
in any particular scenario. Following the behavior graph, we design a heuristic investigation
algorithm by using forward, self, and backward random walk, with an objective to identify
the root cause services. To demonstrate the strengths of AutoMAP, we develop a prototype
and evaluate it in both simulated environment and real-work enterprise cloud system.
Experimental results clearly indicate that AutoMAP achieves over 90% precision, which
significantly outperforms other selected baseline methods. AutoMAP can be quickly
deployed in a variety of microservice-based systems without any system knowledge.
It also supports introduction of various expert knowledge to improve accuracy.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {246–258},
numpages = {13},
keywords = {anomaly diagnosis, web application, Microservice architecture, root cause, cloud computing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3147234.3148111,
author = {L\'{o}pez, Manuel Ram\'{\i}rez and Spillner, Josef},
title = {Towards Quantifiable Boundaries for Elastic Horizontal Scaling of Microservices},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3148111},
doi = {10.1145/3147234.3148111},
abstract = {One of the most useful features of a microservices architecture is its versatility
to scale horizontally. However, not all services scale in or out uniformly. The performance
of an application composed of microservices depends largely on a suitable combination
of replica count and resource capacity. In practice, this implies limitations to the
efficiency of autoscalers which often overscale based on an isolated consideration
of single service metrics. Consequently, application providers pay more than necessary
despite zero gain in overall performance. Solving this issue requires an application-specific
determination of scaling limits due to the general infeasibility of an application-agnostic
solution. In this paper, we study microservices scalability, the auto-scaling of containers
as microservice implementations and the relation between the number of replicas and
the resulting application task performance. We contribute a replica count determination
solution with a mathematical approach. Furthermore, we offer a calibration software
tool which places scalability boundaries into declarative composition descriptions
of applications ready to be consumed by cloud platforms.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {35–40},
numpages = {6},
keywords = {scalability, replication, optimization, microservices},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@inproceedings{10.1145/2797022.2797039,
author = {Anwar, Ali and Sailer, Anca and Kochut, Andrzej and Butt, Ali R.},
title = {Anatomy of Cloud Monitoring and Metering: A Case Study and Open Problems},
year = {2015},
isbn = {9781450335546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797022.2797039},
doi = {10.1145/2797022.2797039},
abstract = {Microservices based architecture has recently gained traction among the cloud service
providers in quest for a more scalable and reliable modular architecture. In parallel
with this architectural choice, cloud providers are also facing the market demand
for fine grained usage based prices. Both the management of the microservices complex
dependencies, as well as the fine grained metering require the providers to track
and log detailed monitoring data from their deployed cloud setups. Hence, on one hand,
the providers need to record all such performance changes and events, while on the
other hand, they are concerned with the additional cost associated with the resources
required to store and process this ever increasing amount of collected data.In this
paper, we analyze the design of the monitoring subsystem provided by open source cloud
solutions, such as OpenStack. Specifically, we analyze how the monitoring data is
collected by OpenStack and assess the characteristics of the data it collects, aiming
to pinpoint the limitations of the current approach and suggest alternate solutions.
Our preliminary evaluation of the proposed solutions reveals that it is possible to
reduce the monitored data size by up to 80% and missed anomaly detection rate from
3% to as low as 0.05% to 0.1%.},
booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
articleno = {6},
numpages = {7},
location = {Tokyo, Japan},
series = {APSys '15}
}

@inproceedings{10.1145/3425269.3425273,
author = {de Freitas Apolin\'{a}rio, Daniel Rodrigo and de Fran\c{c}a, Breno Bernard Nicolau},
title = {Towards a Method for Monitoring the Coupling Evolution of Microservice-Based Architectures},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425273},
doi = {10.1145/3425269.3425273},
abstract = {The microservice architecture is claimed to satisfy ongoing software development demands,
such as resilience, flexibility, and velocity. However, developing applications based
on microservices also brings some drawbacks, such as the increased software operational
complexity. Recent studies have also pointed out the lack of methods to prevent problems
related to the maintainability of these solutions. Disregarding established design
principles during the software evolution may lead to the so-called architectural erosion,
which can end up in a condition of unfeasible maintenance. As microservices can be
considered a new architecture style, there are few initiatives to monitoring the evolution
of software microservice-based architectures. In this paper, we introduce the SYMBIOTE
method for monitoring the coupling evolution of microservice-based systems. More specifically,
this method collects coupling metrics during runtime (staging or production environments)
and monitors them throughout software evolution. The longitudinal analysis of the
collected measures allows detecting an upward trend in coupling metrics that could
be signs of architectural erosion. To develop the proposed method, we performed an
experimental analysis of the coupling metrics behavior using artificially-generated
data.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {71–80},
numpages = {10},
keywords = {maintainability, coupling metrics, software evolution, microservices},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3011077.3011078,
author = {Mellouk, Abdelhamid},
title = {New Provider Services for Convergence Technologies Based on Quality of Experience and Quality of Service Paradigms},
year = {2016},
isbn = {9781450348157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011077.3011078},
doi = {10.1145/3011077.3011078},
abstract = {Based on a convergence of network technologies, the Next Generation Network (NGN)
is being deployed to carry high quality video and voice data. In fact, the convergence
of network technologies has been driven by the converging needs of end-users. The
perceived end-to-end quality is becoming one of the main goals required by users that
must be guaranteed by the network operators and the Internet Service Providers, through
manufacturer equipment. This is referred to as the notion of Quality of Experience
(QoE) and is becoming commonly used to represent user perception. The QoE is not a
technical metric, but rather a concept consisting of all elements of a user's perception
of the network services. In this talk, we focus on the idea of how to integrate the
QoE into a control- command chain in order to construct an adaptive network system.
More precisely, in the context of Content-Oriented Networks that is used to redesign
the current Internet architecture to accommodate content-oriented applications and
services, the talk aim to describe an end-to-end QoE model applied to a Content Distribution
Network architecture and see relationships between Quality of service and Quality
of Experience.},
booktitle = {Proceedings of the Seventh Symposium on Information and Communication Technology},
pages = {1},
numpages = {1},
location = {Ho Chi Minh City, Vietnam},
series = {SoICT '16}
}

@inproceedings{10.1145/3297280.3297400,
author = {Cardarelli, Mario and Iovino, Ludovico and Di Francesco, Paolo and Di Salle, Amleto and Malavolta, Ivano and Lago, Patricia},
title = {An Extensible Data-Driven Approach for Evaluating the Quality of Microservice Architectures},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297400},
doi = {10.1145/3297280.3297400},
abstract = {Microservice architecture (MSA) is defined as an architectural style where the software
system is developed as a suite of small services, each running in its own process
and communicating with lightweight mechanisms. The benefits of MSA are many, ranging
from an increase in development productivity, to better business-IT alignment, agility,
scalability, and technology flexibility. The high degree of microservices distribution
and decoupling is, however, imposing a number of relevant challenges from an architectural
perspective. In this context, measuring, controlling, and keeping a satisfactory level
of quality of the system architecture is of paramount importance.In this paper we
propose an approach for the specification, aggregation, and evaluation of software
quality attributes for the architecture of microservice-based systems. The proposed
approach allows developers to (i) produce architecture models of the system, either
manually or automatically via recovering techniques, (ii) contribute to an ecosystem
of well-specified and automatically-computable software quality attributes for MSAs,
and (iii) continuously measure and evaluate the architecture of their systems by (re-)using
the software quality attributes defined in the ecosystem. The approach is implemented
by using Model-Driven Engineering techniques.The current implementation of the approach
has been validated by assessing the maintainability of a third-party, publicly available
benchmark system.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1225–1234},
numpages = {10},
keywords = {microservices, architecture recovery, software quality, model-driven},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3126858.3126873,
author = {Brilhante, Jonathan and Costa, Rostand and Maritan, Tiago},
title = {Asynchronous Queue Based Approach for Building Reactive Microservices},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126873},
doi = {10.1145/3126858.3126873},
abstract = {To achieve scalability and flexibility in larger applications a new approach arises,
named by Microservices (MS). However MS architectures are at their inception and are
even more a concept than a fully mature design pattern. One of the hardest topics
in this approach is how to properly migrate or develop a single microservice, in terms
of scope, efficiency and dependability. In this sense, this work proposes a new architectural
model based on high-level architecture pattern of reactive programming to the internal
structure of a new microservice. The new model of microservices are internally coordinated
by asynchronous queues, which allowed to preserve compatibility with most monolithic
components and provide an encapsulation process to enable its continuity. A comparative
study between the standard approach and the proposed architecture was carried out
to measure the eventual performance improvement of the new strategy.},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {373–380},
numpages = {8},
keywords = {asynchronous queues, reactive approach, micro services, refactoring},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.1145/2973839.2973846,
author = {Aniche, Maur\'{\i}cio and Gerosa, Marco Aur\'{e}lio and Treude, Christoph},
title = {Developers' Perceptions on Object-Oriented Design and Architectural Roles},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973846},
doi = {10.1145/2973839.2973846},
abstract = {Software developers commonly rely on well-known software architecture patterns, such
as MVC, to build their applications. In many of these patterns, classes play specific
roles in the system, such as Controllers or Entities, which means that each of these
classes has specific characteristics in terms of object-oriented class design and
implementation. Indeed, as we have shown in a previous study, architectural roles
are different from each other in terms of code metrics. In this paper, we present
a study in a software development company in which we captured developers' perceptions
on object-oriented design aspects of the architectural roles in their system and whether
these perceptions match the source code metric analysis. We found that their developers
do not have a common perception of how their architectural roles behave in terms of
object-oriented design aspects, and that their perceptions also do not match the results
of the source code metric analysis. This phenomenon also does not seem to be related
to developers' experience. We find these results alarming, and thus, we suggest software
development teams to invest in education and knowledge sharing about how their system's
architectural roles behave.},
booktitle = {Proceedings of the 30th Brazilian Symposium on Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {object-oriented design, code metrics, software architecture},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1145/3452383.3452385,
author = {Dasgupta, Gargi B.},
title = {AI and Its Applications in the Cloud Strategy},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452385},
doi = {10.1145/3452383.3452385},
abstract = { The fourth industrial revolution identifies cloud computing, data, and artificial
intelligence (AI) as opportunity clusters with double digit growth in the next couple
of years. As part of the cloud and digital transformation, the role of AI is crucial
in enabling that transformation as well as creating the new breed of applications
on top. AI mechanisms can help accelerate the modernization of applications, their
management, and the testing on cloud architectures. I will focus on two sub-problems:
1) Refactoring of massive monolith applications using AI techniques. This problem
statement is particularly relevant in understanding legacy un-optimized code and transforming
them to be more cloud-ready. Microservices are indeed becoming the de-facto design
choice for software architecture. It involves partitioning the software components
into finer modules such that the development can happen independently [2]. It also
provides natural benefits when deployed on the cloud since resources can be allocated
dynamically to necessary components based on demand. We are exploring how AI can help
accelerate the transformation of existing applications to microservices. 2) Detecting
faults in application behavior at runtime from operational data. This problem statement
is particularly relevant in understanding how to manage this new architecture of multiple
microservices across the cloud stack [1], [3]. Operational data artifacts span across
logs, metrics, tickets, and traces. Looking at signals across the artifacts and across
the stack presents a challenging data correlation problem. AI mechanisms can help
accelerate problem determination in these complex environments. I will also share
my thoughts on how fundamental breakthroughs in AI Research will be needed as we address
some of the core problems of cloud computing. },
booktitle = {14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {2},
numpages = {1},
keywords = {modernization, AI Ops, code refactoring, hybrid cloud, log anomalies},
location = {Bhubaneswar, Odisha, India},
series = {ISEC 2021}
}

@inproceedings{10.5555/3172795.3172823,
author = {Khazaei, Hamzeh and Ravichandiran, Rajsimman and Park, Byungchul and Bannazadeh, Hadi and Tizghadam, Ali and Leon-Garcia, Alberto},
title = {Elascale: Autoscaling and Monitoring as a Service},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Auto-scalability has become an evident feature for cloud software systems including
but not limited to big data and IoT applications. Cloud application providers now
are in full control over their applications' microservices and macroservices; virtual
machines and containers can be provisioned or deprovisioned on demand at run-time.
Elascale strives to adjust both micro/macro resources with respect to workload and
changes in the internal state of the whole application stack. Elascale leverages Elasticsearch
stack for collection, analysis and storage of performance metrics. Elascale then uses
its default scaling engine to elastically adapt the managed application. Extendibility
is guaranteed through provider, schema, plug-in and policy elements in the Elascale
by which flexible scalability algorithms, including both reactive and proactive techniques,
can be designed and implemented for various technologies, infrastructures and software
stacks. In this paper, we present the architecture and initial implementation of Elascale;
an instance will be leveraged to add auto-scalability to a generic IoT application.
Due to zero dependency to the target software system, Elascale can be leveraged to
provide auto-scalability and monitoring as-a-service for any type of cloud software
system.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {234–240},
numpages = {7},
keywords = {macroservices, elasticsearch, monitoring, microservices, auto-scalability, cloud application, scalability as a service, containers, docker},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3460319.3464805,
author = {Pan, Yicheng and Ma, Meng and Jiang, Xinrui and Wang, Ping},
title = {Faster, Deeper, Easier: Crowdsourcing Diagnosis of Microservice Kernel Failure from User Space},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464805},
doi = {10.1145/3460319.3464805},
abstract = {With the widespread use of cloud-native architecture, increasing web applications
(apps) choose to build on microservices. Simultaneously, troubleshooting becomes full
of challenges owing to the high dynamics and complexity of anomaly propagation. Existing
diagnostic methods rely heavily on monitoring metrics collected from the kernel side
of microservice systems. Without a comprehensive monitoring infrastructure, application
owners and even cloud operators cannot resort to these kernel-space solutions. This
paper summarizes several insights on operating a top commercial cloud platform. Then,
for the first time, we put forward the idea of user-space diagnosis for microservice
kernel failures. To this end, we develop a crowdsourcing solution - DyCause, to resolve
the asymmetric diagnostic information problem. DyCause deploys on the application
side in a distributed manner. Through lightweight API log sharing, apps collect the
operational status of kernel services collaboratively and initiate diagnosis on demand.
Deploying DyCause is fast and lightweight as we do not have any architectural and
functional requirements for the kernel. To reveal more accurate correlations from
asymmetric diagnostic information, we design a novel statistical algorithm that can
efficiently discover the time-varying causalities between services. This algorithm
also helps us build the temporal order of the anomaly propagation. Therefore, by using
DyCause, we can obtain more in-depth and interpretable diagnostic clues with limited
indicators. We apply and evaluate DyCause on both a simulated test-bed and a real-world
cloud system. Experimental results verify that DyCause running in the user-space outperforms
several state-of-the-art algorithms running in the kernel on accuracy. Besides, DyCause
shows superior advantages in terms of algorithmic efficiency and data sensitivity.
Simply put, DyCause produces a significantly better result than other baselines when
analyzing much fewer or sparser metrics. To conclude, DyCause is faster to act, deeper
in analysis, and easier to deploy.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {646–657},
numpages = {12},
keywords = {granger causal intervals, root cause analysis, microservice system, dynamic service dependency},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3439231.3440604,
author = {Mercier, Julien and Whissell-Turner, Kathleen and Paradis, Ariane and Avaca, Ivan},
title = {Good Vibrations: Tuning a Systems Dynamics Model of Affect and Cognition in Learning to the Appropriate Frequency Bands of Fine-Grained Temporal Sequences of Data: Frequency Bands of Affect and Cognition},
year = {2020},
isbn = {9781450389372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439231.3440604},
doi = {10.1145/3439231.3440604},
abstract = {Process-oriented studies of cooperative learning from an educational neuroscience
perspective has not been firmly quantified experimentally. Within a modeling approach
aimed at the development of a systems dynamics model of affect and cognition, the
goal of this exploratory study is to identify typical timescales of variation for
continuous metrics of affect (Frontal Alpha Asymmetry (FAA): valence) and cognition
(Cognitive Load (CL); Index of Cognitive Engagement (ICE); Frontal Midline Theta (FMT):
attention). These metrics were obtained from 72 participants paired in dyads (player
and watcher) from whom electroencephalography (EEG) was recorded for 2 hours while
one participant was playing a serious game to learn Physics, and the other one was
watching passively. The results show rather slow cyclical variation for every metric
tested, accompanied in certain cases by short bursts of faster variations. This result
converges with [Newell 1990] cognitive architecture assuming that psychophysiological
measures capture activity at higher levels such as operation tasks and operations.
Theoretical, methodological and applied implications are discussed. Also, the need
for further fine-grained analyses of the context and other atypical analyses are expressed.},
booktitle = {9th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {194–202},
numpages = {9},
keywords = {educational neuroscience, online measures of learning, game-based learning},
location = {Online, Portugal},
series = {DSAI 2020}
}

@inproceedings{10.1145/3308558.3313653,
author = {Shan, Huasong and Chen, Yuan and Liu, Haifeng and Zhang, Yunpeng and Xiao, Xiao and He, Xiaofeng and Li, Min and Ding, Wei},
title = {??-Diagnosis: Unsupervised and Real-Time Diagnosis of Small- Window Long-Tail Latency in Large-Scale Microservice Platforms},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313653},
doi = {10.1145/3308558.3313653},
abstract = {Microservice architectures and container technologies are broadly adopted by giant
internet companies to support their web services, which typically have a strict service-level
objective (SLO), tail latency, rather than average latency. However, diagnosing SLO
violations, e.g., long tail latency problem, is non-trivial for large-scale web applications
in shared microservice platforms due to million-level operational data and complex
operational environments. We identify a new type of tail latency problem for web services,
small-window long-tail latency (SWLT), which is typically aggregated during a small
statistical window (e.g., 1-minute or 1-second). We observe SWLT usually occurs in
a small number of containers in microservice clusters and sharply shifts among different
containers at different time points. To diagnose root-causes of SWLT, we propose an
unsupervised and low-cost diagnosis algorithm-?-Diagnosis, using two-sample test algorithm
and ?-statistics for measuring similarity of time series to identify root-cause metrics
from millions of metrics. We implement and deploy a real-time diagnosis system in
our real-production microservice platforms. The evaluation using real web application
datasets demonstrates that ?-Diagnosis can identify all the actual root-causes at
runtime and significantly reduce the candidate problem space, outperforming other
time-series distance based root-cause analysis algorithms.},
booktitle = {The World Wide Web Conference},
pages = {3215–3222},
numpages = {8},
keywords = {Root-cause analysis, time series similarity, tail latency},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3415088.3415097,
author = {Mlotshwa, Likhwa Lothar and Makura, Sheunesu M. and Karie, Nickson M. and Kebande, Victor R.},
title = {Opportunistic Security Architecture for Osmotic Computing Paradigm in Dynamic IoT-Edge's Resource Diffusion},
year = {2020},
isbn = {9781450375580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415088.3415097},
doi = {10.1145/3415088.3415097},
abstract = {Increased heterogeneity of physical resources has had positive and negative effects
in Internet of Things (IoT) through the existence of edge computing. As a result,
there has been a need for effective dynamic management of IoT, cloud and edge resources,
in order to address the existence of low-level constraints during resource migration.
Nevertheless, the explosion of IoT devices and data has allowed orchestration of microservices
to adopt an opportunistic approach to how applications and services are deployed in
the edge in IoT platform. A notable approach has been osmotic computing that allows
resources from a federated cloud to be able to diffuse from an ecosystem of higher
solute (network properties and entities) concentration to solvent (applications, layered
interfaces and services). We posit that, while computing resources and applications
are able to move from the federated environment, to the cloud deployable models, to
the edge, then to IoT ecosystem, there is a higher chance of susceptibility of threats
and attacks that may be directed to the emerging edge applications/data due to dynamic
emergent configurations. This paper proposes a 5-layer opportunistic architecture
that adds security metrics across different levels of osmotic computing paradigm.
The proposed 5-layer security architecture addresses the need for autonomously securing
resources-edge computation, edge storage and emerging edge configurations as the computing
resources move to a higher solute in heterogenous edge and cloud datacenters across
IoT devices. This has been achieved by proposing security metrics that address the
prevailing challenge with a degree of certainty.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent and Innovative Computing Applications},
articleno = {9},
numpages = {7},
keywords = {edge, security architecture, opportunistic, osmotic, IoT},
location = {Plaine Magnien, Mauritius},
series = {ICONIC '20}
}

@inproceedings{10.5555/3021955.3022022,
author = {L., Marcelo Dornbusch and Rauta, Leonardo R.P. and Silva, Paulo H. and Silva, Rodrigo C. and Irigoite, Adriano M. and Wangham, Michelle S.},
title = {Remote and Continuous Monitoring of Electrical Quantities Using Web of Things and Cloud Computing},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {The remote monitoring and control of machines are essential in industrial environments.
Emerging communication technologies such as Web of Things and Machine to Machine Communication
can meet this demand for automation. This paper aims to introduce a solution, called
Smart Meter, for continuous and remote monitoring of electrical quantities, in smart
industrial environments with three-phase systems. The proposed solution uses a resource-oriented
architecture and makes use of a Smart Gateway for communication, RESTful web services
and cloud computing. The solution was integrated with a real case study and evaluated
by software testing. The results obtained demonstrate the feasibility of the solution,
and the correctness of measurements persisted in the cloud.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {393–400},
numpages = {8},
keywords = {Cloud Computing, M2M, Remote Monitoring of Electrical Quantities, Web of Things},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI 2016}
}

@inproceedings{10.1145/2747470.2747474,
author = {Toffetti, Giovanni and Brunner, Sandro and Bl\"{o}chlinger, Martin and Dudouet, Florian and Edmonds, Andrew},
title = {An Architecture for Self-Managing Microservices},
year = {2015},
isbn = {9781450334761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2747470.2747474},
doi = {10.1145/2747470.2747474},
abstract = {Running applications in the cloud efficiently requires much more than deploying software
in virtual machines. Cloud applications have to be continuously managed: 1) to adjust
their resources to the incoming load and 2) to face transient failures replicating
and restarting components to provide resiliency on unreliable infrastructure. Continuous
management monitors application and infrastructural metrics to provide automated and
responsive reactions to failures (health management) and changing environmental conditions
(auto-scaling) minimizing human intervention.In the current practice, management functionalities
are provided as infrastructural or third party services. In both cases they are external
to the application deployment. We claim that this approach has intrinsic limits, namely
that separating management functionalities from the application prevents them from
naturally scaling with the application and requires additional management code and
human intervention. Moreover, using infrastructure provider services for management
functionalities results in vendor lock-in effectively preventing cloud applications
to adapt and run on the most effective cloud for the job.In this position paper we
propose a novel architecture that enables scalable and resilient self-management of
microservices applications on cloud.},
booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
pages = {19–24},
numpages = {6},
location = {Bordeaux, France},
series = {AIMC '15}
}

@article{10.1145/3418899,
author = {Brondolin, Rolando and Santambrogio, Marco D.},
title = {A Black-Box Monitoring Approach to Measure Microservices Runtime Performance},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3418899},
doi = {10.1145/3418899},
abstract = {Microservices changed cloud computing by moving the applications’ complexity from
one monolithic executable to thousands of network interactions between small components.
Given the increasing deployment sizes, the architectural exploitation challenges,
and the impact on data-centers’ power consumption, we need to efficiently track this
complexity. Within this article, we propose a black-box monitoring approach to track
microservices at scale, focusing on architectural metrics, power consumption, application
performance, and network performance. The proposed approach is transparent w.r.t.
the monitored applications, generates less overhead w.r.t. black-box approaches available
in the state-of-the-art, and provides fine-grain accurate metrics.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {34},
numpages = {26},
keywords = {network performance monitoring, cloud computing, performance monitoring, docker, power attribution, kubernetes, Microservices}
}

@inproceedings{10.1145/3098954.3098977,
author = {Boukoros, Spyros and Katzenbeisser, Stefan},
title = {Measuring Privacy in High Dimensional Microdata Collections},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3098977},
doi = {10.1145/3098954.3098977},
abstract = {Microdata is collected by companies in order to enhance their quality of service as
well as the accuracy of their recommendation systems. These data often become publicly
available after they have been sanitized. Recent reidentification attacks on publicly
available, sanitized datasets illustrate the privacy risks involved in microdata collections.
Currently, users have to trust the provider that their data will be safe in case data
is published or if a privacy breach occurs. In this work, we empower users by developing
a novel, user-centric tool for privacy measurement and a new lightweight privacy metric.
The goal of our tool is to estimate users' privacy level prior to sharing their data
with a provider. Hence, users can consciously decide whether to contribute their data.
Our tool estimates an individuals' privacy level based on published popularity statistics
regarding the items in the provider's database, and the users' microdata. In this
work, we describe the architecture of our tool as well as a novel privacy metric,
which is necessary for our setting where we do not have access to the provider's database.
Our tool is user friendly, relying on smart visual results that raise privacy awareness.
We evaluate our tool using three real world datasets, collected from major providers.
We demonstrate strong correlations between the average anonymity set per user and
the privacy score obtained by our metric. Our results illustrate that our tool which
uses minimal information from the provider, estimates users' privacy levels comparably
well, as if it had access to the actual database.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {15},
numpages = {8},
keywords = {microdata, user empowerment, privacy metrics, privacy},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/3411029.3411032,
author = {Kogias, Marios and Bugnion, Edouard},
title = {Tail-Tolerance as a Systems Principle Not a Metric},
year = {2020},
isbn = {9781450388764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411029.3411032},
doi = {10.1145/3411029.3411032},
abstract = {Tail-latency tolerance (or just simply tail-tolerance) is the ability for a system
to deliver a response with low-latency nearly all the time. It it typically expressed
as a system metric (e.g., the 99th or 99.99th percentile latency) or as a service-level
objective (e.g., the maximum throughput so that the tail latency is below a desired
threshold). We advocate instead that modern datacenter systems should incorporate
tail-tolerance as a core systems design principle and not a metric to be observed,
and that tail-tolerant systems can be built out of large and complex applications
whose individual components may suffer from latency deviations. This is analogous
to fault-tolerance, where a fault-tolerant system can be built out of unreliable components.
The general solution is for the system to control the applied load and keep it under
the threshold that violates the latency SLO. We propose to augment RPC semantics with
an architectural layer that measures the observed tail latency and probabilistically
rejects RPC requests maintaining throughput under the threshold that violates the
SLO. Our design is application-independent, and does not make any assumptions about
the request service time distribution. We implemented a proof of concept for such
a tail-tolerant layer using programmable switches, called SVEN. We demonstrate that
the approach is suitable even for microsecond-scale RPCs with variable service times.
Moreover, our approach does not induce measurable overheads, and can maintain the
maximum achieved throughput very close to the load level that would violate the SLO
without SVEN. },
booktitle = {4th Asia-Pacific Workshop on Networking},
pages = {16–22},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {APNet '20}
}

@inproceedings{10.1145/3267955.3267965,
author = {Khan, Junaid Ahmed and Westphal, Cedric and Garcia-Luna-Aceves, J. J. and Ghamri-Doudane, Yacine},
title = {NICE: Network-Oriented Information-Centric Centrality for Efficiency in Cache Management},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3267965},
doi = {10.1145/3267955.3267965},
abstract = {All Information-Centric Networking (ICN) architectures proposed to date aim at connecting
users to content directly, rather than connecting clients to servers. Surprisingly,
however, although content caching is an integral of any information-Centric Network,
limited work has been reported on information-centric management of caches in the
context of an ICN. Indeed, approaches to cache management in networks of caches have
focused on network connectivity rather than proximity to content.We introduce the
Network-oriented Information-centric Centrality for Efficiency (NICE) as a new metric
for cache management in information-centric networks. We propose a method to compute
information-centric centrality that scales with the number of caches in a network
rather than the number of content objects, which is many orders of magnitude larger.
Furthermore, it can be pre-processed offline and ahead of time. We apply the NICE
metric to a content replacement policy in caches, and show that a content replacement
based on NICE exhibits better performances than LRU and other policies based on topology-oriented
definitions of centrality.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {31–42},
numpages = {12},
keywords = {content offloading, ICN, graph centrality, cache management},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@inproceedings{10.1145/3338466.3358917,
author = {Heinl, Michael P. and Giehl, Alexander and Wiedermann, Norbert and Plaga, Sven and Kargl, Frank},
title = {MERCAT: A Metric for the Evaluation and Reconsideration of Certificate Authority Trustworthiness},
year = {2019},
isbn = {9781450368261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338466.3358917},
doi = {10.1145/3338466.3358917},
abstract = {Public key infrastructures (PKIs) build the foundation for secure communication of
a vast majority of cloud services. In the recent past, there has been a series of
security incidents leading to increasing concern regarding the trust model currently
employed by PKIs. One of the key criticisms is the architecture's implicit assumption
that certificate authorities (CAs) are trustworthy a priori.This work proposes a holistic
metric to compensate this assumption by a differentiating assessment of a CA's individual
trustworthiness based on objective criteria. The metric utilizes a wide range of technical
and non-technical factors derived from existing policies, technical guidelines, and
research. It consists of self-contained submetrics allowing the simple extension of
the existing set of criteria. The focus is thereby on aspects which can be assessed
by employing practically applicable methods of independent data collection.The metric
is meant to help organizations, individuals, and service providers deciding which
CAs to trust or distrust. For this, the modularized submetrics are clustered into
coherent submetric groups covering a CA's different properties and responsibilities.
By applying individually chosen weightings to these submetric groups, the metric's
outcomes can be adapted to tailored protection requirements according to an exemplifying
attacker model.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
pages = {1–15},
numpages = {15},
keywords = {metric, digital certificate, pki, trustworthiness assessment, cloud security, ca, x.509},
location = {London, United Kingdom},
series = {CCSW'19}
}

@inproceedings{10.1145/3234944.3234952,
author = {Kim, Yubin and Callan, Jamie},
title = {Measuring the Effectiveness of Selective Search Index Partitions without Supervision},
year = {2018},
isbn = {9781450356565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234944.3234952},
doi = {10.1145/3234944.3234952},
abstract = {Selective search architectures partition a document collection into topic-oriented
index shards, usually using algorithms that have random components. Different mappings
of documents into index shards (shard maps) produce different search accuracy and
consistency, however identifying which shard maps will deliver the highest average
effectiveness is an open problem. This paper presents a new metric, Area Under Recall
Curve (AUReC), to evaluate and compare shard maps. AUReC is the first such metric
that is independent of resource selection and shard cut-off estimation. It does not
require an end-to-end evaluation or manual gold-standard judgements. Experiments show
that its predictions are highly-correlated with evaluating end-to-end systems of various
configurations, while being easier to implement and computationally inexpensive.},
booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {91–98},
numpages = {8},
keywords = {clustering, cluster-based retrieval, evaluation, distributed search, selective search},
location = {Tianjin, China},
series = {ICTIR '18}
}

@article{10.1145/2829950,
author = {Tomusk, Erik and Dubach, Christophe and O’boyle, Michael},
title = {Four Metrics to Evaluate Heterogeneous Multicores},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2829950},
doi = {10.1145/2829950},
abstract = {Semiconductor device scaling has made single-ISA heterogeneous processors a reality.
Heterogeneous processors contain a number of different CPU cores that all implement
the same Instruction Set Architecture (ISA). This enables greater flexibility and
specialization, as runtime constraints and workload characteristics can influence
which core a given workload is run on. A major roadblock to the further development
of heterogeneous processors is the lack of appropriate evaluation metrics. Existing
metrics can be used to evaluate individual cores, but to evaluate a heterogeneous
processor, the cores must be considered as a collective. Without appropriate metrics,
it is impossible to establish design goals for processors, and it is difficult to
accurately compare two different heterogeneous processors.We present four new metrics
to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity,
gap overhead, set overhead, and generality. The metrics consider sets rather than
individual cores. We use examples to demonstrate each metric, and show that the metrics
can be used to quantify intuitions about heterogeneous cores.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {37},
numpages = {25},
keywords = {gap overhead, effective speed, single-ISA, generality, Localized nonuniformity, set overhead}
}

