@inproceedings{10.1145/2661829.2661991,
author = {Anchuri, Pranay and Sumbaly, Roshan and Shah, Sam},
title = {Hotspot Detection in a Service-Oriented Architecture},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661991},
doi = {10.1145/2661829.2661991},
abstract = {Large-scale websites are predominantly built as a service-oriented architecture. Here,
services are specialized for a certain task, run on multiple machines, and communicate
with each other to serve a user's request. Reducing latency and improving the cost
to serve is quite important, but optimizing this service call graph is particularly
challenging due to the volume of data and the graph's non-uniform and dynamic nature.In
this paper, we present a framework to detect hotspots in a service-oriented architecture.
The framework is general, in that it can handle arbitrary objective functions. We
show that finding the optimal set of hotspots for a metric, such as latency, is NP-complete
and propose a greedy algorithm by relaxing some constraints. We use a pattern mining
algorithm to rank hotspots based on the impact and consistency. Experiments on real
world service call graphs from LinkedIn, the largest online professional social network,
show that our algorithm consistently outperforms baseline methods.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {1749–1758},
numpages = {10},
keywords = {monitoring, call graph, service-oriented architecture, hotspots},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/2593501.2593505,
author = {Miranda, Breno and Bertolino, Antonia},
title = {Social Coverage for Customized Test Adequacy and Selection Criteria},
year = {2014},
isbn = {9781450328586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593501.2593505},
doi = {10.1145/2593501.2593505},
abstract = { Test coverage information can be very useful for guiding testers in enhancing their
test suites to exercise possible uncovered entities and in deciding when to stop testing.
However, for complex applications that are reused in different contexts and for emerging
paradigms (e.g., component-based development, service-oriented architecture, and cloud
computing), traditional coverage metrics may no longer provide meaningful information
to help testers on these tasks. Various proposals are advocating to leverage information
that come from the testing community in a collaborative testing approach. In this
work we introduce a coverage metric, the Social Coverage, that customizes coverage
information in a given context based on coverage data collected from similar users.
To evaluate the potential of our proposed approach, we instantiated the social coverage
metric in the context of a real world service oriented application. In this exploratory
study, we were able to predict the entities that would be of interest for a given
user with an average precision of 97% and average recall of 75%. Our results suggest
that, in similar environments, social coverage can provide a better support to testers
than traditional coverage. },
booktitle = {Proceedings of the 9th International Workshop on Automation of Software Test},
pages = {22–28},
numpages = {7},
keywords = {User Similarity, Coverage Testing, Service-Oriented Application, Relative Coverage},
location = {Hyderabad, India},
series = {AST 2014}
}

@article{10.1145/2579281.2579294,
author = {Castelluccia, Daniela and Boffoli, Nicola},
title = {Service-Oriented Product Lines: A Systematic Mapping Study},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579294},
doi = {10.1145/2579281.2579294},
abstract = {Software product line engineering and service-oriented architectures both enable organizations
to capitalize on reuse of existing software assets and capabilities and improve competitive
advantage in terms of development savings, product flexibility, time-to-market. Both
approaches accommodate variation of assets, including services, by changing the software
being reused or composing services according a new orchestration. Therefore, variability
management in Service-oriented Product Lines (SoPL) is one of the main challenges
today. In order to highlight the emerging evidence-based results from the research
community, we apply the well-defined method of systematic mapping in order to populate
a classification scheme for the SoPL field of interest. The analysis of results throws
light on the current open issues. Moreover, different facets of the scheme can be
combined to answer more specific research questions. The report reveals the need for
more empirical research able to provide new metrics measuring efficiency and efficacy
of the proposed models, new methods and tools supporting variability management in
SoPL, especially during maintenance and verification and validation. The mapping study
about SoPL opens further investigations by means of a complete systematic review to
select and validate the most efficient solutions to variability management in SoPL.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6},
keywords = {empirical study, mapping study, service-oriented computing, product line development, service-oriented architecture, software product line, variability management}
}

@inproceedings{10.5555/3021955.3022024,
author = {Oliveira, Joyce Aline and Junior, Jose J.L.D.},
title = {A Three-Dimensional View of Reuse in Service Oriented Architecture},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {The reuse in Service Oriented Architecture (SOA) has been used strategically in organizations
to reduce development costs and increase the quality of applications. This article
reports a qualitative research realized with experts in order to identify goals, barriers,
facilitators, strategies, metrics and benefits associated with reuse in SOA. The results
were summarized in three dimensions (management, architecture, operation) and represented
by a conceptual model that can serve as a preliminary roadmap to manage the reuse
in SOA.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {409–416},
numpages = {8},
keywords = {qualitative research, SOA reuse, Services Oriented Architecture},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI 2016}
}

@inproceedings{10.1145/3034950.3034975,
author = {Ke, Weimao},
title = {Distributed Search Efficiency and Robustness in Service Oriented Multi-Agent Networks},
year = {2017},
isbn = {9781450348348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3034950.3034975},
doi = {10.1145/3034950.3034975},
abstract = {We study decentralized searches in a large service-oriented agent network and investigate
the influences of multiple factors on search efficiency. In this study we focus on
overall system robustness and examine search performance in unstable environments
where individual agents may fail or a system-wide attack may occur. Experimental results
show that searches continue to be efficient when a large number of service agents
become unavailable. Surprisingly, overall system performance in terms of a search
path length metric improves with an increasing number of unavailable agents. Service
unavailability also has an impact on the load balance of service agents. We plan to
conduct further research to verify observed patterns and to understand related implications
on system architecture design.},
booktitle = {Proceedings of the 2017 International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {9–18},
numpages = {10},
keywords = {distributed search, information network, robustness, service agents},
location = {Wuhan, China},
series = {ICMSS '17}
}

@inproceedings{10.1145/3018896.3018961,
author = {Lehmann, Martin and Sandnes, Frode Eika},
title = {A Framework for Evaluating Continuous Microservice Delivery Strategies},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018961},
doi = {10.1145/3018896.3018961},
abstract = {The emergence of service-oriented computing, and in particular microservice architecture,
has introduced a new layer of complexity to the already challenging task of continuously
delivering changes to the end users. Cloud computing has turned scalable hardware
into a commodity, but also imposes some requirements on the software development process.
Yet, the literature mainly focuses on quantifiable metrics such as number of manual
steps and lines of code required to make a change. The industry, on the other hand,
appears to focus more on qualitative metrics such as increasing the productivity of
their developers. These are common goals, but must be measured using different approaches.
Therefore, based on interviews of industry stakeholders a framework for evaluating
and comparing approaches to continuous microservice delivery is proposed. We show
that it is possible to efficiently evaluate and compare strategies for continuously
delivering microservices.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {64},
numpages = {9},
keywords = {microservices, microservice architectures, deployment strategy, cloud computing, evaluation framework, continuous deployment},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3229345.3229419,
author = {Oliveira, Joyce Aline and Vargas, Matheus and Rodrigues, Roni},
title = {SOA Reuse: Systematic Literature Review Updating and Research Directions},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229419},
doi = {10.1145/3229345.3229419},
abstract = {Service Oriented Architecture (SOA) reuse has been used strategically in organizations
to reduce development costs and increase the quality of applications. This article
analyzes a systematic literature review in order to identify concepts, goals, strategies,
and metrics of SOA reuse. The results show that the main goal of SOA reuse is to decrease
development costs. The factor that most negatively influences SOA reuse is the existence
of legacy systems. The strategy used most to potentialize SOA reuse is business process
management. Metrics proposed by studies to measure SOA reuse are related to modularity
and adaptability indicators. The study is relevant because it increases the body of
knowledge of the area. Additionally, a set of gaps to be addressed by researchers
and reuse practitioners was identified.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {71},
numpages = {8},
keywords = {Service Oriented Architecture, systematic literature review, SOA reuse},
location = {Caxias do Sul, Brazil},
series = {SBSI'18}
}

@inproceedings{10.1145/3284557.3284713,
author = {Rudorfer, Martin and Pannen, Tessa J. and Kr\"{u}ger, J\"{o}rg},
title = {A Case Study on Granularity of Industrial Vision Services},
year = {2018},
isbn = {9781450366281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284557.3284713},
doi = {10.1145/3284557.3284713},
abstract = {Software engineering paradigms such as service-oriented architectures are increasingly
often applied in the field of factory automation. Functions like robot motion planning
or object recognition are provided by cloud services. A crucial architectural aspect
is the granularity, i.e. the scope and size of individual services. In our case study,
we examine a service-based object recognition application for a robotic assembly use
case. We implement three different granularity levels, measure their communication
and computation times and discuss further architectural features. The fine-granular
approach encapsulates individual image processing operations as services, which have
high reusability but impose large communication overheads. The medium granularity
approach is object-wise and offers best reuse efficiency and cohesion. The coarse
solution offers the best performance.},
booktitle = {Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control},
articleno = {59},
numpages = {6},
keywords = {Object Recognition, Granularity, Service-Oriented Architecture},
location = {Stockholm, Sweden},
series = {ISCSIC '18}
}

@inproceedings{10.1145/3143434.3143457,
author = {Ilin, I. and Levina, A. and Abran, A. and Iliashenko, O.},
title = {Measurement of Enterprise Architecture (EA) from an IT Perspective: Research Gaps and Measurement Avenues},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143457},
doi = {10.1145/3143434.3143457},
abstract = {Reorganizational projects in general and software-related projects in particular,
are often implemented with a focus only on the reorganized components within an organizational
management system, not taking into account relationships with the other components
of an enterprise architecture (EA). This paper first looks at the current state of
EA measurement to identify weaknesses and gaps in aligning and measuring EA components,
EA structures and EA interrelationships from an IT perspective. It then identifies
from related works available innovative measurement concepts that could contribute
for aligning, measuring and monitoring software-related projects within an EA strategy.
This includes measurement avenues within a Balanced Scorecard (BSC), contributions
of functional size measurement to the BSC, and measurement of software structures
and functionality within a service-oriented architecture (SOA).},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {232–243},
numpages = {12},
keywords = {balanced scorecard (BSC), function points (FP), enterprise architecture (EA), enterprise architecture measurement, service-oriented architecture (SOA), functional size measurement (FSM)},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@inproceedings{10.1145/2568225.2568230,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Integrating Adaptive User Interface Capabilities in Enterprise Applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568230},
doi = {10.1145/2568225.2568230},
abstract = { Many existing enterprise applications are at a mature stage in their development
and are unable to easily benefit from the usability gains offered by adaptive user
interfaces (UIs). Therefore, a method is needed for integrating adaptive UI capabilities
into these systems without incurring a high cost or significantly disrupting the way
they function. This paper presents a method for integrating adaptive UI behavior in
enterprise applications based on CEDAR, a model-driven, service-oriented, and tool-supported
architecture for devising adaptive enterprise application UIs. The proposed integration
method is evaluated with a case study, which includes establishing and applying technical
metrics to measure several of the method’s properties using the open-source enterprise
application OFBiz as a test-case. The generality and flexibility of the integration
method are also evaluated based on an interview and discussions with practitioners
about their real-life projects. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {712–723},
numpages = {12},
keywords = {Adaptive user interfaces, integration, software architectures, model-driven engineering, software metrics, enterprise systems},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3018896.3036365,
author = {Ashrafi, Tasnia H. and Arefin, Sayed E. and Das, Kowshik D. J. and Hossain, Md. A. and Chakrabarty, Amitabha},
title = {FOG Based Distributed IoT Infrastructure},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3036365},
doi = {10.1145/3018896.3036365},
abstract = {The Internet of Things(IoT) can be defined as a network connectivity bridge between
people, systems and physical world. With the increasing number of IoT devices and
networks, dealing with enormous number of data efficiently is becoming more and more
challenging for the present infrastructure which is a very big matter of concern.
In this paper, we depicted the current infrastructure and proposed another model of
IoT infrastructure to surpass the difficulties of the existing infrastructure, which
will be a coordinated effort of Fog computing amalgamation with Machine-to-Machine(M2M)
intelligent communication protocol followed by incorporation of Service Oriented Architecture(SOA)
and finally integration of Agent based SOA. This model will have the capacity to exchange
data by breaking down dependably and methodically with low latency, less bandwidth,
heterogeneity in less measure of time maintaining the Quality of Service(QoS) precisely.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {124},
numpages = {13},
keywords = {fog computing, M2M communication, heterogeneous devices, agent based SOA, internet of things (IoT), quality of service (QoS), service oriented architecture(SOA)},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3068126.3068128,
author = {Filelis-Papadopoulos, C. K. and Gravvanis, G. A. and Morrison, J. P.},
title = {CloudLightning Simulation and Evaluation Roadmap},
year = {2017},
isbn = {9781450349369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3068126.3068128},
doi = {10.1145/3068126.3068128},
abstract = {The CloudLightning (CL) system, designed in the frame of the CloudLightning project,
is a service-oriented architecture for the emerging large scale heterogeneous cloud.
It facilitates a clear distinction between service-lifecyle management and resource-lifecycle
management. This separation of concerns is used to make resource management issues
tractable at scale and to enable functionality that is currently not naturally covered
by the cloud paradigm. In particular, the CL project seeks to maximize computational
efficiency of the cloud in a number of specific ways; by exploiting prebuilt HPC environments,
by dynamically building HPC instances, by improving server utilization, by reducing
power consumption and by improving service delivery. Given the scale and complexity
of this project, its utility can presently only be measured through simulation. This
paper outlines the parameters, constraints and limitation being considered as part
of the design and construction of that simulation environment.},
booktitle = {Proceedings of the 1st International Workshop on Next Generation of Cloud Architectures},
articleno = {2},
numpages = {6},
keywords = {CloudLightning, Self - Organisation, Self - Management, Evaluation},
location = {Belgrade, Serbia},
series = {CloudNG:17}
}

@inproceedings{10.1145/3194164.3194166,
author = {Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred},
title = {Limiting Technical Debt with Maintainability Assurance: An Industry Survey on Used Techniques and Differences with Service- and Microservice-Based Systems},
year = {2018},
isbn = {9781450357135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194164.3194166},
doi = {10.1145/3194164.3194166},
abstract = {Maintainability assurance techniques are used to control this quality attribute and
limit the accumulation of potentially unknown technical debt. Since the industry state
of practice and especially the handling of Service- and Microservice-Based Systems
in this regard are not well covered in scientific literature, we created a survey
to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related
treatment of systems based on service-orientation, and c) influences on developer
satisfaction w.r.t. maintainability. 60 software professionals responded to our online
questionnaire. The results indicate that using explicit and systematic techniques
has benefits for maintainability. The more sophisticated the applied methods the more
satisfied participants were with the maintainability of their software while no link
to a hindrance in productivity could be established. Other important findings were
the absence of architecture-level evolvability control mechanisms as well as a significant
neglect of service-oriented particularities for quality assurance. The results suggest
that industry has to improve its quality control in these regards to avoid problems
with long-living service-based software systems.},
booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
pages = {125–133},
numpages = {9},
keywords = {microservice-based systems, maintainability, industry, survey, software quality control, service-based systems},
location = {Gothenburg, Sweden},
series = {TechDebt '18}
}

@inproceedings{10.1145/3019612.3019805,
author = {Abid, Ahmed and Messai, Nizar and Rouached, Mohsen and Abid, Mohamed and Devogele, Thomas},
title = {Semantic Similarity Based Web Services Composition Framework},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019805},
doi = {10.1145/3019612.3019805},
abstract = {Computing similarities between Web services is a main concern in Service Oriented
Architecture as it allows to decide which services are likely to be matched into a
composite workflow, or in other cases, which services can be substituted in order
to ensure continuous service availability. With the high maturity achieved by the
standards, tools and frameworks in the Semantic Web domain, measuring Web services
similarities relies more than ever on semantic descriptions of services as well as
on semantic relationships these descriptions may hold. In this paper we present a
Framework for Web services composition based on computing semantic similarity between
Web services. We particularly focus on Services Matching engine which uses the considered
similarity measure first to classify Web services into classes of functionally similar
Web services and then to propose a composite sequence of services that matches a requested
goal. In both tasks, the presented framework appeals for best known techniques of
similarity computing and data and knowledge extraction, respectively.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1319–1325},
numpages = {7},
keywords = {matching, semantic similarity, web services, discovery and composition},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/2642937.2653471,
author = {Miranda, Breno},
title = {A Proposal for Revisiting Coverage Testing Metrics},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2653471},
doi = {10.1145/2642937.2653471},
abstract = {Test coverage information can be very useful for guiding testers in enhancing their
test suites to exercise possible uncovered entities and in deciding when to stop testing.
Since the concept of test criterion was born, several contributions have been made
by both academia and industry in the definition and adaptation of adequacy criteria
aiming at ensuring the discovery of more failures. Numerous contributions have also
been done in the development of coverage tools. However, for complex applications
that are reused in different contexts and for emerging paradigms (e.g., component-based
development, service-oriented architecture, and cloud computing), traditional coverage
metrics may no longer provide meaningful information to help testers on these tasks.
Inspired by the idea of relative coverage this research focuses on the introduction
of meaningful coverage metrics to cope with the challenges imposed by the current
programming paradigms as well as on the definition of a theoretical framework for
the development of relative coverage metrics.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {899–902},
numpages = {4},
keywords = {traditional coverage, coverage testing, relative coverage},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3127479.3132020,
author = {Suresh, Lalith and Bodik, Peter and Menache, Ishai and Canini, Marco and Ciucu, Florin},
title = {Distributed Resource Management across Process Boundaries},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3132020},
doi = {10.1145/3127479.3132020},
abstract = {Multi-tenant distributed systems composed of small services, such as Service-oriented
Architectures (SOAs) and Micro-services, raise new challenges in attaining high performance
and efficient resource utilization. In these systems, a request execution spans tens
to thousands of processes, and the execution paths and resource demands on different
services are generally not known when a request first enters the system. In this paper,
we highlight the fundamental challenges of regulating load and scheduling in SOAs
while meeting end-to-end performance objectives on metrics of concern to both tenants
and operators. We design Wisp, a framework for building SOAs that transparently adapts
rate limiters and request schedulers system-wide according to operator policies to
satisfy end-to-end goals while responding to changing system conditions. In evaluations
against production as well as synthetic workloads, Wisp successfully enforces a range
of end-to-end performance objectives, such as reducing average latencies, meeting
deadlines, providing fairness and isolation, and avoiding system overload.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {611–623},
numpages = {13},
keywords = {resource management, service-oriented architectures, microservices, scheduling, rate limiting},
location = {Santa Clara, California},
series = {SoCC '17}
}

@article{10.5555/3288897.3288933,
author = {Wu, Yumei and Fang, Yuanyuan and Liu, Bin and Zhao, Zehui},
title = {A Novel Service Deployment Approach Based on Resilience Metrics for Service-Oriented System},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {5–6},
issn = {1617-4909},
abstract = {Service-Oriented Architecture (SOA) has been widely used in IT areas and is expected
to bring a lot of benefits. However, the SOA system developers have to address new
challenging issues such as computational resource failure before such benefits can
be realized. This paper develops a graph-theoretic model for the SOA system and proposes
metrics that quantify the resilience of such system under resource failures. It explores
two service deployment strategies to optimize resilience by taking not only communication
costs among services but also the computation costs of services into consideration.
Among them, two types of undirected graphs are developed to model the relationships
between services, including Service Dependence Graph (SDG) and Service Concurrence
Graph (SCG). Then, these two graphs are integrated into Service Relationship Graph
(SRG) and adopt the k-cut optimization theory to complete the service deployment.
Finally, this paper verifies the effectiveness of the above methods in improving the
resilience of the system through a series of experiments, which indicate that our
methods perform better than the previous methods in improving resilience of the SOA
system.},
journal = {Personal Ubiquitous Comput.},
month = oct,
pages = {1099–1107},
numpages = {9},
keywords = {Resilience, SOA, Service relationship graph, Service deployment}
}

@inproceedings{10.1145/2739480.2754724,
author = {Ouni, Ali and Gaikovina Kula, Raula and Kessentini, Marouane and Inoue, Katsuro},
title = {Web Service Antipatterns Detection Using Genetic Programming},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754724},
doi = {10.1145/2739480.2754724},
abstract = {Service-Oriented Architecture (SOA) is an emerging paradigm that has radically changed
the way software applications are architected, designed and implemented. SOA allows
developers to structure their systems as a set of ready-made, reusable and compostable
services. The leading technology used today for implementing SOA is Web Services.
Indeed, like all software, Web services are prone to change constantly to add new
user requirements or to adapt to environment changes. Poorly planned changes may risk
introducing antipatterns into the system. Consequently, this may ultimately leads
to a degradation of software quality, evident by poor quality of service (QoS). In
this paper, we introduce an automated approach to detect Web service antipatterns
using genetic programming. Our approach consists of using knowledge from real-world
examples of Web service antipatterns to generate detection rules based on combinations
of metrics and threshold values. We evaluate our approach on a benchmark of 310 Web
services and a variety of five types of Web service antipatterns. The statistical
analysis of the obtained results provides evidence that our approach is efficient
to detect most of the existing antipatterns with a score of 85% of precision and 87%
of recall.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1351–1358},
numpages = {8},
keywords = {web services, search-based software engineering, antipatterns},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/3330204.3330259,
author = {Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel},
title = {Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330259},
doi = {10.1145/3330204.3330259},
abstract = {In the last decade the (big) data-driven science paradigm became a wide-spread reality.
However, this approach has some limitations such as a performance dependency on the
quality of the data and the lack of reproducibility of the results. In order to enable
this reproducibility, many tools such as Workflow Management Systems were developed
to formalize process pipelines and capture execution traces. However, interoperating
data generated by these solutions became a problem, since most systems adopted proprietary
data models. To support interoperability across heterogeneous provenance data, we
propose a Service Oriented Architecture with a polystore storage design in which provenance
is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible
for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover,
we propose a query layer that provides location and access transparency to users.
Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios.
Firstly, we illustrate how two research groups can compare their processes and results.
Secondly, we show how our architecture can be used as a queriable provenance repository.
We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology.
Finally, we show our solution usability and extensibility appeal by comparing it to
similar approaches.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {49},
numpages = {8},
keywords = {polystore, Workflows interoperability, heterogeneous provenance data integration},
location = {Aracaju, Brazil},
series = {SBSI'19}
}

@inproceedings{10.1145/3230833.3233278,
author = {Mathas, Christos M. and Segou, Olga E. and Xylouris, Georgios and Christinakis, Dimitris and Kourtis, Michail-Alexandros and Vassilakis, Costas and Kourtis, Anastasios},
title = {Evaluation of Apache Spot's Machine Learning Capabilities in an SDN/NFV Enabled Environment},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233278},
doi = {10.1145/3230833.3233278},
abstract = {Software Defined Networking (SDN) and Network Function Virtualisation (NFV) are transforming
modern networks towards a service-oriented architecture. At the same time, the cybersecurity
industry is rapidly adopting Machine Learning (ML) algorithms to improve detection
and mitigation of complex attacks. Traditional intrusion detection systems perform
signature-based detection, based on well-known malicious traffic patterns that signify
potential attacks. The main drawback of this method is that attack patterns need to
be known in advance and signatures must be preconfigured. Hence, typical systems fail
to detect a zero-day attack or an attack with unknown signature. This work considers
the use of machine learning for advanced anomaly detection, and specifically deploys
the Apache Spot ML framework on an SDN/NFV-enabled testbed running cybersecurity services
as Virtual Network Functions (VNFs). VNFs are used to capture traffic for ingestion
by the ML algorithm and apply mitigation measures in case of a detected anomaly. Apache
Spot utilises Latent Dirichlet Allocation to identify anomalous traffic patterns in
Netflow, DNS and proxy data. The overall performance of Apache Spot is evaluated by
deploying Denial of Service (Slowloris, BoNeSi) and a Data Exfiltration attack (iodine).},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {52},
numpages = {10},
keywords = {Apache Spot, Software Defined Networking, Network Function Virtualisation, Latent Dirichlet Allocation, Penetration Testing, Machine Learning, SHIELD Project},
location = {Hamburg, Germany},
series = {ARES 2018}
}

@article{10.14778/2733004.2733009,
author = {Poess, Meikel and Rabl, Tilmann and Jacobsen, Hans-Arno and Caufield, Brian},
title = {TPC-DI: The First Industry Benchmark for Data Integration},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733009},
doi = {10.14778/2733004.2733009},
abstract = {Historically, the process of synchronizing a decision support system with data from
operational systems has been referred to as Extract, Transform, Load (ETL) and the
tools supporting such process have been referred to as ETL tools. Recently, ETL was
replaced by the more comprehensive acronym, data integration (DI). DI describes the
process of extracting and combining data from a variety of data source formats, transforming
that data into a unified data model representation and loading it into a data store.
This is done in the context of a variety of scenarios, such as data acquisition for
business intelligence, analytics and data warehousing, but also synchronization of
data between operational applications, data migrations and conversions, master data
management, enterprise data sharing and delivery of data services in a service-oriented
architecture context, amongst others. With these scenarios relying on up-to-date information
it is critical to implement a highly performing, scalable and easy to maintain data
integration system. This is especially important as the complexity, variety and volume
of data is constantly increasing and performance of data integration systems is becoming
very critical. Despite the significance of having a highly performing DI system, there
has been no industry standard for measuring and comparing their performance. The TPC,
acknowledging this void, has released TPC-DI, an innovative benchmark for data integration.
This paper motivates the reasons behind its development, describes its main characteristics
including workload, run rules, metric, and explains key decisions.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1367–1378},
numpages = {12}
}

@inproceedings{10.1145/2642668.2642679,
author = {Hassam, Mickael and Kara, Nadjia and Belqasmi, Fatna and Glitho, Roch},
title = {Virtualized Infrastructure for Video Game Applications in Cloud Environments},
year = {2014},
isbn = {9781450330268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642668.2642679},
doi = {10.1145/2642668.2642679},
abstract = {Mobile video games are fast-growing and fast-evolving. Cloud computing's paradigm
can bring several benefits to mobile video games, like cost reduction through an efficient
usage of resources, or an easier and faster on-demand deployment of new applications.
This paper focuses on the architecture aspects of mobile cloud-based video gaming
and proposes a new service oriented and virtualized paradigm. The idea is to provide
reusable game engines sub modules like the rendering or physics engines as cloud computing
services. We call these sub modules substrates. Offered by different substrates providers
as services, they can be dynamically discovered, used and composed. There are several
motivations for substrates virtualization, including the rapid introduction of new
video game applications and cost efficiency through resource sharing. This paper also
describes the implementation of a prototype and the measurements performed to validate
some aspects of our paradigm. As a preliminary validation of this solution, we analyze
the effects of different parameters like virtualization or inner latency on the QoS.
The performance analysis shows that the overhead introduced by substrate virtualization
is acceptable, and reveals how the low-latency connectivity between substrates that
compose a video game application and the limitation of the amount of these substrates
are crucial to achieve a satisfactory level of QoS.},
booktitle = {Proceedings of the 12th ACM International Symposium on Mobility Management and Wireless Access},
pages = {109–114},
numpages = {6},
keywords = {infrastructure and platform as services, mobile video game applications, virtualization, cloud computing, substrate},
location = {Montreal, QC, Canada},
series = {MobiWac '14}
}

@inproceedings{10.1109/CCGrid.2015.148,
author = {Beier, Maximilian and Jansen, Christoph and Mayer, Geert and Penzel, Thomas and Rodenbeck, Andrea and Siewert, Ren\'{e} and Wu, Jie and Krefting, Dagmar},
title = {Multicenter Data Sharing for Collaboration in Sleep Medicine},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.148},
doi = {10.1109/CCGrid.2015.148},
abstract = {Clinical Sleep Research is an inherent multidisciplinary field, as many health issues
may affect a person's sleep conditions and sleep disorders may cause several health
problems. Many patients with chronic sleep disorders suffer from different further
medical conditions - called multimorbidity. Due to the high variety of the reasons
and the courses of sleep disorders, individual cases are difficult to compare. Therefore
there is a high demand for sleep researchers to collaborate with each other to reach
necessary participant numbers and multidisciplinary expertise. To date, inter-institutional
sleep research is poorly supported by IT systems. In particular the heterogeneity
and the quality variations within the acquired biosignal data - caused by different
biosignal recorders or different measurement procedures - are impeding common biosignal
data processing. In this manuscript we introduce a virtual research platform supporting
inter-institutional data sharing and processing. The infrastructure is based on XNAT
- a free and open-source neuroimaging research platform - a loosely coupled service
oriented architecture and scalable virtualization in the backend. The system is capable
of local pseudonymization of biosignal data, mapping to a standardized set of parameters
and automatic quality assessment. Terms and quality measures are derived from the
"Manual for the Scoring of Sleep and Associated Events" of the American Academy of
Sleep Medicine, the de-facto standard for diagnostic biosignal analysis in sleep medicine.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {880–889},
numpages = {10},
keywords = {REST, biosignal, XNAT, cloud, sleep, polysomnography, OpenStack},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@article{10.1145/3386041,
author = {Shi, Min and Tang, Yufei and Zhu, Xingquan and Liu, Jianxun},
title = {Topic-Aware Web Service Representation Learning},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3386041},
doi = {10.1145/3386041},
abstract = {The advent of Service-Oriented Architecture (SOA) has brought a fundamental shift
in the way in which distributed applications are implemented. An overwhelming number
of Web-based services (e.g., APIs and Mashups) have leveraged this shift and furthered
development. Applications designed with SOA principles are typically characterized
by frequent dependencies with one another in the form of heterogeneous networks, i.e.,
annotation relations between tags and services, and composition relations between
Mashups and APIs. Although prior work has shown the utility gained by exploring these
networks, their analysis is still in its infancy. This article develops an approach
to learning representations of the Web service network, which seeks to embed Web services
in low-dimensional continuous vectors with preserved information of the network structure,
functional tags, and service descriptions, such that services with similar functional
properties and network structures are mapped together in the learned latent space.
We first propose a topic generative model for constructing two topic distribution
networks (Mashup-Topic and API-Topic) from the service content. Then, we present an
efficient optimization process to derive low-dimensional vector representations of
Web services from a tri-layer bipartite network with the Mashup-Topic and API-Topic
networks on two ends and the Mashup-API composition network in the middle. Experiments
on real-word datasets have verified that our approach is effective to learn robust
low-rank service representations, i.e., 25% F1-measure gain over the state-of-the-art
in Web service recommendation task.},
journal = {ACM Trans. Web},
month = apr,
articleno = {9},
numpages = {23},
keywords = {network embedding, service representation, Web services, probabilistic topic model, Mashups}
}

@inproceedings{10.1145/3385032.3385042,
author = {Tummalapalli, Sahithi and Kumar, Lov and Murthy, N. L. Bhanu},
title = {Prediction of Web Service Anti-Patterns Using Aggregate Software Metrics and Machine Learning Techniques},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385042},
doi = {10.1145/3385032.3385042},
abstract = {Service-Oriented Architecture(SOA) can be characterized as an approximately coupled
engineering intended to meet the business needs of an association/organization. Service-Based
Systems (SBSs) are inclined to continually change to enjoy new client necessities
and adjust the execution settings, similar to some other huge and complex frameworks.
These changes may lead to the evolution of designs/products with poor Quality of Service
(QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns.
Anti-patterns makes the evolution and maintenance of the software systems hard and
complex. Early identification of modules, classes, or source code regions where anti-patterns
are more likely to occur can help in amending and maneuvering testing efforts leading
to the improvement of software quality. In this work, we investigate the application
of three sampling techniques, three feature selection techniques, and sixteen different
classification techniques to develop the models for web service anti-pattern detection.
We report the results of an empirical study by evaluating the approach proposed, on
a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of
five types of web-service anti-patterns. Experimental results demonstrated that SMOTE
is the best performing data sampling techniques. The experimental results also reveal
that the model developed by considering Uncorrelated Significant Predictors(SUCP)
as the input obtained better performance compared to the model developed by other
metrics. Experimental results also show that the Least Square Support Vector Machine
with Linear(LSLIN) function has outperformed all other classifier techniques.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
articleno = {8},
numpages = {11},
keywords = {Anti-pattern, Classifiers, Service-Based Systems(SBS), Aggregation measures, WSDL, Feature Selection, Class imbalance distribution, Machine Learning, Web-Services, Source Code Metrics},
location = {Jabalpur, India},
series = {ISEC 2020}
}

@inproceedings{10.1145/3011141.3011179,
author = {de Camargo, Andr\'{e} and Salvadori, Ivan and Mello, Ronaldo dos Santos and Siqueira, Frank},
title = {An Architecture to Automate Performance Tests on Microservices},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011179},
doi = {10.1145/3011141.3011179},
abstract = {The microservices architecture provides a new approach to develop applications. As
opposed to monolithic applications, in which the application comprises a single software
artifact, an application based on the microservices architecture is composed by a
set of services, each one designed to perform a single and well-defined task. These
services allow the development team to decouple several parts of the application using
different frameworks, languages and hardware for each part of the system. One of the
drawbacks for adopting the microservices architecture to develop applications is testability.
In a single application test boundaries can be more easily established and tend to
be more stable as the application evolves, while with microservices we can have a
set of hundreds of services that operate together and are prone to change more rapidly.
Each one of these services needs to be tested and updated as the service changes.
In addition, the different characteristics of these services such as languages, frameworks
or the used infrastructure have to be considered in the testing phase. Performance
tests are applied to assure that a particular software complies with a set of non-functional
requirements such as throughput and response time. These metrics are important to
ensure that business constraints are respected and to help finding performance bottlenecks.
In this paper, we present a new approach to allow the performance tests to be executed
in an automated way, with each microservice providing a test specification that is
used to perform tests. Along with the architecture, we also provide a framework that
implements some key concepts of this architecture. This framework is available as
an open source project1.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {422–429},
numpages = {8},
keywords = {test automation, performance test, microservices},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@inproceedings{10.1145/3412841.3442016,
author = {Brito, Miguel and Cunha, J\'{a}come and Saraiva, Jo\~{a}o},
title = {Identification of Microservices from Monolithic Applications through Topic Modelling},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442016},
doi = {10.1145/3412841.3442016},
abstract = {Microservices emerged as one of the most popular architectural patterns in the recent
years given the increased need to scale, grow and flexibilize software projects accompanied
by the growth in cloud computing and DevOps. Many software applications are being
submitted to a process of migration from its monolithic architecture to a more modular,
scalable and flexible architecture of microservices. This process is slow and, depending
on the project's complexity, it may take months or even years to complete.This paper
proposes a new approach on microservice identification by resorting to topic modelling
in order to identify services according to domain terms. This approach in combination
with clustering techniques produces a set of services based on the original software.
The proposed methodology is implemented as an open-source tool for exploration of
monolithic architectures and identification of microservices. A quantitative analysis
using the state of the art metrics on independence of functionality and modularity
of services was conducted on 200 open-source projects collected from GitHub. Cohesion
at message and domain level metrics' showed medians of roughly 0.6. Interfaces per
service exhibited a median of 1.5 with a compact interquartile range. Structural and
conceptual modularity revealed medians of 0.2 and 0.4 respectively.Our first results
are positive demonstrating beneficial identification of services due to overall metrics'
results.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1409–1418},
numpages = {10},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3366423.3380111,
author = {Ma, Meng and Xu, Jingmin and Wang, Yuan and Chen, Pengfei and Zhang, Zonghua and Wang, Ping},
title = {AutoMAP: Diagnose Your Microservice-Based Web Applications Automatically},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380111},
doi = {10.1145/3366423.3380111},
abstract = {The high complexity and dynamics of the microservice architecture make its application
diagnosis extremely challenging. Static troubleshooting approaches may fail to obtain
reliable model applies for frequently changing situations. Even if we know the calling
dependency of services, we lack a more dynamic diagnosis mechanism due to the existence
of indirect fault propagation. Besides, algorithm based on single metric usually fail
to identify the root cause of anomaly, as single type of metric is not enough to characterize
the anomalies occur in diverse services. In view of this, we design a novel tool,
named AutoMAP, which enables dynamic generation of service correlations and automated
diagnosis leveraging multiple types of metrics. In AutoMAP, we propose the concept
of anomaly behavior graph to describe the correlations between services associated
with different types of metrics. Two binary operations, as well as a similarity function
on behavior graph are defined to help AutoMAP choose appropriate diagnosis metric
in any particular scenario. Following the behavior graph, we design a heuristic investigation
algorithm by using forward, self, and backward random walk, with an objective to identify
the root cause services. To demonstrate the strengths of AutoMAP, we develop a prototype
and evaluate it in both simulated environment and real-work enterprise cloud system.
Experimental results clearly indicate that AutoMAP achieves over 90% precision, which
significantly outperforms other selected baseline methods. AutoMAP can be quickly
deployed in a variety of microservice-based systems without any system knowledge.
It also supports introduction of various expert knowledge to improve accuracy.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {246–258},
numpages = {13},
keywords = {anomaly diagnosis, web application, Microservice architecture, root cause, cloud computing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3147234.3148111,
author = {L\'{o}pez, Manuel Ram\'{\i}rez and Spillner, Josef},
title = {Towards Quantifiable Boundaries for Elastic Horizontal Scaling of Microservices},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3148111},
doi = {10.1145/3147234.3148111},
abstract = {One of the most useful features of a microservices architecture is its versatility
to scale horizontally. However, not all services scale in or out uniformly. The performance
of an application composed of microservices depends largely on a suitable combination
of replica count and resource capacity. In practice, this implies limitations to the
efficiency of autoscalers which often overscale based on an isolated consideration
of single service metrics. Consequently, application providers pay more than necessary
despite zero gain in overall performance. Solving this issue requires an application-specific
determination of scaling limits due to the general infeasibility of an application-agnostic
solution. In this paper, we study microservices scalability, the auto-scaling of containers
as microservice implementations and the relation between the number of replicas and
the resulting application task performance. We contribute a replica count determination
solution with a mathematical approach. Furthermore, we offer a calibration software
tool which places scalability boundaries into declarative composition descriptions
of applications ready to be consumed by cloud platforms.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {35–40},
numpages = {6},
keywords = {scalability, replication, optimization, microservices},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@inproceedings{10.1145/2797022.2797039,
author = {Anwar, Ali and Sailer, Anca and Kochut, Andrzej and Butt, Ali R.},
title = {Anatomy of Cloud Monitoring and Metering: A Case Study and Open Problems},
year = {2015},
isbn = {9781450335546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797022.2797039},
doi = {10.1145/2797022.2797039},
abstract = {Microservices based architecture has recently gained traction among the cloud service
providers in quest for a more scalable and reliable modular architecture. In parallel
with this architectural choice, cloud providers are also facing the market demand
for fine grained usage based prices. Both the management of the microservices complex
dependencies, as well as the fine grained metering require the providers to track
and log detailed monitoring data from their deployed cloud setups. Hence, on one hand,
the providers need to record all such performance changes and events, while on the
other hand, they are concerned with the additional cost associated with the resources
required to store and process this ever increasing amount of collected data.In this
paper, we analyze the design of the monitoring subsystem provided by open source cloud
solutions, such as OpenStack. Specifically, we analyze how the monitoring data is
collected by OpenStack and assess the characteristics of the data it collects, aiming
to pinpoint the limitations of the current approach and suggest alternate solutions.
Our preliminary evaluation of the proposed solutions reveals that it is possible to
reduce the monitored data size by up to 80% and missed anomaly detection rate from
3% to as low as 0.05% to 0.1%.},
booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
articleno = {6},
numpages = {7},
location = {Tokyo, Japan},
series = {APSys '15}
}

@inproceedings{10.1145/3425269.3425273,
author = {de Freitas Apolin\'{a}rio, Daniel Rodrigo and de Fran\c{c}a, Breno Bernard Nicolau},
title = {Towards a Method for Monitoring the Coupling Evolution of Microservice-Based Architectures},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425273},
doi = {10.1145/3425269.3425273},
abstract = {The microservice architecture is claimed to satisfy ongoing software development demands,
such as resilience, flexibility, and velocity. However, developing applications based
on microservices also brings some drawbacks, such as the increased software operational
complexity. Recent studies have also pointed out the lack of methods to prevent problems
related to the maintainability of these solutions. Disregarding established design
principles during the software evolution may lead to the so-called architectural erosion,
which can end up in a condition of unfeasible maintenance. As microservices can be
considered a new architecture style, there are few initiatives to monitoring the evolution
of software microservice-based architectures. In this paper, we introduce the SYMBIOTE
method for monitoring the coupling evolution of microservice-based systems. More specifically,
this method collects coupling metrics during runtime (staging or production environments)
and monitors them throughout software evolution. The longitudinal analysis of the
collected measures allows detecting an upward trend in coupling metrics that could
be signs of architectural erosion. To develop the proposed method, we performed an
experimental analysis of the coupling metrics behavior using artificially-generated
data.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {71–80},
numpages = {10},
keywords = {maintainability, coupling metrics, software evolution, microservices},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3011077.3011078,
author = {Mellouk, Abdelhamid},
title = {New Provider Services for Convergence Technologies Based on Quality of Experience and Quality of Service Paradigms},
year = {2016},
isbn = {9781450348157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011077.3011078},
doi = {10.1145/3011077.3011078},
abstract = {Based on a convergence of network technologies, the Next Generation Network (NGN)
is being deployed to carry high quality video and voice data. In fact, the convergence
of network technologies has been driven by the converging needs of end-users. The
perceived end-to-end quality is becoming one of the main goals required by users that
must be guaranteed by the network operators and the Internet Service Providers, through
manufacturer equipment. This is referred to as the notion of Quality of Experience
(QoE) and is becoming commonly used to represent user perception. The QoE is not a
technical metric, but rather a concept consisting of all elements of a user's perception
of the network services. In this talk, we focus on the idea of how to integrate the
QoE into a control- command chain in order to construct an adaptive network system.
More precisely, in the context of Content-Oriented Networks that is used to redesign
the current Internet architecture to accommodate content-oriented applications and
services, the talk aim to describe an end-to-end QoE model applied to a Content Distribution
Network architecture and see relationships between Quality of service and Quality
of Experience.},
booktitle = {Proceedings of the Seventh Symposium on Information and Communication Technology},
pages = {1},
numpages = {1},
location = {Ho Chi Minh City, Vietnam},
series = {SoICT '16}
}

@inproceedings{10.1145/3297280.3297400,
author = {Cardarelli, Mario and Iovino, Ludovico and Di Francesco, Paolo and Di Salle, Amleto and Malavolta, Ivano and Lago, Patricia},
title = {An Extensible Data-Driven Approach for Evaluating the Quality of Microservice Architectures},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297400},
doi = {10.1145/3297280.3297400},
abstract = {Microservice architecture (MSA) is defined as an architectural style where the software
system is developed as a suite of small services, each running in its own process
and communicating with lightweight mechanisms. The benefits of MSA are many, ranging
from an increase in development productivity, to better business-IT alignment, agility,
scalability, and technology flexibility. The high degree of microservices distribution
and decoupling is, however, imposing a number of relevant challenges from an architectural
perspective. In this context, measuring, controlling, and keeping a satisfactory level
of quality of the system architecture is of paramount importance.In this paper we
propose an approach for the specification, aggregation, and evaluation of software
quality attributes for the architecture of microservice-based systems. The proposed
approach allows developers to (i) produce architecture models of the system, either
manually or automatically via recovering techniques, (ii) contribute to an ecosystem
of well-specified and automatically-computable software quality attributes for MSAs,
and (iii) continuously measure and evaluate the architecture of their systems by (re-)using
the software quality attributes defined in the ecosystem. The approach is implemented
by using Model-Driven Engineering techniques.The current implementation of the approach
has been validated by assessing the maintainability of a third-party, publicly available
benchmark system.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1225–1234},
numpages = {10},
keywords = {microservices, architecture recovery, software quality, model-driven},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3126858.3126873,
author = {Brilhante, Jonathan and Costa, Rostand and Maritan, Tiago},
title = {Asynchronous Queue Based Approach for Building Reactive Microservices},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126873},
doi = {10.1145/3126858.3126873},
abstract = {To achieve scalability and flexibility in larger applications a new approach arises,
named by Microservices (MS). However MS architectures are at their inception and are
even more a concept than a fully mature design pattern. One of the hardest topics
in this approach is how to properly migrate or develop a single microservice, in terms
of scope, efficiency and dependability. In this sense, this work proposes a new architectural
model based on high-level architecture pattern of reactive programming to the internal
structure of a new microservice. The new model of microservices are internally coordinated
by asynchronous queues, which allowed to preserve compatibility with most monolithic
components and provide an encapsulation process to enable its continuity. A comparative
study between the standard approach and the proposed architecture was carried out
to measure the eventual performance improvement of the new strategy.},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {373–380},
numpages = {8},
keywords = {asynchronous queues, reactive approach, micro services, refactoring},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.1145/2973839.2973846,
author = {Aniche, Maur\'{\i}cio and Gerosa, Marco Aur\'{e}lio and Treude, Christoph},
title = {Developers' Perceptions on Object-Oriented Design and Architectural Roles},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973846},
doi = {10.1145/2973839.2973846},
abstract = {Software developers commonly rely on well-known software architecture patterns, such
as MVC, to build their applications. In many of these patterns, classes play specific
roles in the system, such as Controllers or Entities, which means that each of these
classes has specific characteristics in terms of object-oriented class design and
implementation. Indeed, as we have shown in a previous study, architectural roles
are different from each other in terms of code metrics. In this paper, we present
a study in a software development company in which we captured developers' perceptions
on object-oriented design aspects of the architectural roles in their system and whether
these perceptions match the source code metric analysis. We found that their developers
do not have a common perception of how their architectural roles behave in terms of
object-oriented design aspects, and that their perceptions also do not match the results
of the source code metric analysis. This phenomenon also does not seem to be related
to developers' experience. We find these results alarming, and thus, we suggest software
development teams to invest in education and knowledge sharing about how their system's
architectural roles behave.},
booktitle = {Proceedings of the 30th Brazilian Symposium on Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {object-oriented design, code metrics, software architecture},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1145/3452383.3452385,
author = {Dasgupta, Gargi B.},
title = {AI and Its Applications in the Cloud Strategy},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452385},
doi = {10.1145/3452383.3452385},
abstract = { The fourth industrial revolution identifies cloud computing, data, and artificial
intelligence (AI) as opportunity clusters with double digit growth in the next couple
of years. As part of the cloud and digital transformation, the role of AI is crucial
in enabling that transformation as well as creating the new breed of applications
on top. AI mechanisms can help accelerate the modernization of applications, their
management, and the testing on cloud architectures. I will focus on two sub-problems:
1) Refactoring of massive monolith applications using AI techniques. This problem
statement is particularly relevant in understanding legacy un-optimized code and transforming
them to be more cloud-ready. Microservices are indeed becoming the de-facto design
choice for software architecture. It involves partitioning the software components
into finer modules such that the development can happen independently [2]. It also
provides natural benefits when deployed on the cloud since resources can be allocated
dynamically to necessary components based on demand. We are exploring how AI can help
accelerate the transformation of existing applications to microservices. 2) Detecting
faults in application behavior at runtime from operational data. This problem statement
is particularly relevant in understanding how to manage this new architecture of multiple
microservices across the cloud stack [1], [3]. Operational data artifacts span across
logs, metrics, tickets, and traces. Looking at signals across the artifacts and across
the stack presents a challenging data correlation problem. AI mechanisms can help
accelerate problem determination in these complex environments. I will also share
my thoughts on how fundamental breakthroughs in AI Research will be needed as we address
some of the core problems of cloud computing. },
booktitle = {14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {2},
numpages = {1},
keywords = {modernization, AI Ops, code refactoring, hybrid cloud, log anomalies},
location = {Bhubaneswar, Odisha, India},
series = {ISEC 2021}
}

@inproceedings{10.5555/3172795.3172823,
author = {Khazaei, Hamzeh and Ravichandiran, Rajsimman and Park, Byungchul and Bannazadeh, Hadi and Tizghadam, Ali and Leon-Garcia, Alberto},
title = {Elascale: Autoscaling and Monitoring as a Service},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Auto-scalability has become an evident feature for cloud software systems including
but not limited to big data and IoT applications. Cloud application providers now
are in full control over their applications' microservices and macroservices; virtual
machines and containers can be provisioned or deprovisioned on demand at run-time.
Elascale strives to adjust both micro/macro resources with respect to workload and
changes in the internal state of the whole application stack. Elascale leverages Elasticsearch
stack for collection, analysis and storage of performance metrics. Elascale then uses
its default scaling engine to elastically adapt the managed application. Extendibility
is guaranteed through provider, schema, plug-in and policy elements in the Elascale
by which flexible scalability algorithms, including both reactive and proactive techniques,
can be designed and implemented for various technologies, infrastructures and software
stacks. In this paper, we present the architecture and initial implementation of Elascale;
an instance will be leveraged to add auto-scalability to a generic IoT application.
Due to zero dependency to the target software system, Elascale can be leveraged to
provide auto-scalability and monitoring as-a-service for any type of cloud software
system.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {234–240},
numpages = {7},
keywords = {macroservices, elasticsearch, monitoring, microservices, auto-scalability, cloud application, scalability as a service, containers, docker},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3460319.3464805,
author = {Pan, Yicheng and Ma, Meng and Jiang, Xinrui and Wang, Ping},
title = {Faster, Deeper, Easier: Crowdsourcing Diagnosis of Microservice Kernel Failure from User Space},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464805},
doi = {10.1145/3460319.3464805},
abstract = {With the widespread use of cloud-native architecture, increasing web applications
(apps) choose to build on microservices. Simultaneously, troubleshooting becomes full
of challenges owing to the high dynamics and complexity of anomaly propagation. Existing
diagnostic methods rely heavily on monitoring metrics collected from the kernel side
of microservice systems. Without a comprehensive monitoring infrastructure, application
owners and even cloud operators cannot resort to these kernel-space solutions. This
paper summarizes several insights on operating a top commercial cloud platform. Then,
for the first time, we put forward the idea of user-space diagnosis for microservice
kernel failures. To this end, we develop a crowdsourcing solution - DyCause, to resolve
the asymmetric diagnostic information problem. DyCause deploys on the application
side in a distributed manner. Through lightweight API log sharing, apps collect the
operational status of kernel services collaboratively and initiate diagnosis on demand.
Deploying DyCause is fast and lightweight as we do not have any architectural and
functional requirements for the kernel. To reveal more accurate correlations from
asymmetric diagnostic information, we design a novel statistical algorithm that can
efficiently discover the time-varying causalities between services. This algorithm
also helps us build the temporal order of the anomaly propagation. Therefore, by using
DyCause, we can obtain more in-depth and interpretable diagnostic clues with limited
indicators. We apply and evaluate DyCause on both a simulated test-bed and a real-world
cloud system. Experimental results verify that DyCause running in the user-space outperforms
several state-of-the-art algorithms running in the kernel on accuracy. Besides, DyCause
shows superior advantages in terms of algorithmic efficiency and data sensitivity.
Simply put, DyCause produces a significantly better result than other baselines when
analyzing much fewer or sparser metrics. To conclude, DyCause is faster to act, deeper
in analysis, and easier to deploy.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {646–657},
numpages = {12},
keywords = {granger causal intervals, root cause analysis, microservice system, dynamic service dependency},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3439231.3440604,
author = {Mercier, Julien and Whissell-Turner, Kathleen and Paradis, Ariane and Avaca, Ivan},
title = {Good Vibrations: Tuning a Systems Dynamics Model of Affect and Cognition in Learning to the Appropriate Frequency Bands of Fine-Grained Temporal Sequences of Data: Frequency Bands of Affect and Cognition},
year = {2020},
isbn = {9781450389372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439231.3440604},
doi = {10.1145/3439231.3440604},
abstract = {Process-oriented studies of cooperative learning from an educational neuroscience
perspective has not been firmly quantified experimentally. Within a modeling approach
aimed at the development of a systems dynamics model of affect and cognition, the
goal of this exploratory study is to identify typical timescales of variation for
continuous metrics of affect (Frontal Alpha Asymmetry (FAA): valence) and cognition
(Cognitive Load (CL); Index of Cognitive Engagement (ICE); Frontal Midline Theta (FMT):
attention). These metrics were obtained from 72 participants paired in dyads (player
and watcher) from whom electroencephalography (EEG) was recorded for 2 hours while
one participant was playing a serious game to learn Physics, and the other one was
watching passively. The results show rather slow cyclical variation for every metric
tested, accompanied in certain cases by short bursts of faster variations. This result
converges with [Newell 1990] cognitive architecture assuming that psychophysiological
measures capture activity at higher levels such as operation tasks and operations.
Theoretical, methodological and applied implications are discussed. Also, the need
for further fine-grained analyses of the context and other atypical analyses are expressed.},
booktitle = {9th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {194–202},
numpages = {9},
keywords = {educational neuroscience, online measures of learning, game-based learning},
location = {Online, Portugal},
series = {DSAI 2020}
}

@inproceedings{10.1145/3308558.3313653,
author = {Shan, Huasong and Chen, Yuan and Liu, Haifeng and Zhang, Yunpeng and Xiao, Xiao and He, Xiaofeng and Li, Min and Ding, Wei},
title = {??-Diagnosis: Unsupervised and Real-Time Diagnosis of Small- Window Long-Tail Latency in Large-Scale Microservice Platforms},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313653},
doi = {10.1145/3308558.3313653},
abstract = {Microservice architectures and container technologies are broadly adopted by giant
internet companies to support their web services, which typically have a strict service-level
objective (SLO), tail latency, rather than average latency. However, diagnosing SLO
violations, e.g., long tail latency problem, is non-trivial for large-scale web applications
in shared microservice platforms due to million-level operational data and complex
operational environments. We identify a new type of tail latency problem for web services,
small-window long-tail latency (SWLT), which is typically aggregated during a small
statistical window (e.g., 1-minute or 1-second). We observe SWLT usually occurs in
a small number of containers in microservice clusters and sharply shifts among different
containers at different time points. To diagnose root-causes of SWLT, we propose an
unsupervised and low-cost diagnosis algorithm-?-Diagnosis, using two-sample test algorithm
and ?-statistics for measuring similarity of time series to identify root-cause metrics
from millions of metrics. We implement and deploy a real-time diagnosis system in
our real-production microservice platforms. The evaluation using real web application
datasets demonstrates that ?-Diagnosis can identify all the actual root-causes at
runtime and significantly reduce the candidate problem space, outperforming other
time-series distance based root-cause analysis algorithms.},
booktitle = {The World Wide Web Conference},
pages = {3215–3222},
numpages = {8},
keywords = {Root-cause analysis, time series similarity, tail latency},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3415088.3415097,
author = {Mlotshwa, Likhwa Lothar and Makura, Sheunesu M. and Karie, Nickson M. and Kebande, Victor R.},
title = {Opportunistic Security Architecture for Osmotic Computing Paradigm in Dynamic IoT-Edge's Resource Diffusion},
year = {2020},
isbn = {9781450375580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415088.3415097},
doi = {10.1145/3415088.3415097},
abstract = {Increased heterogeneity of physical resources has had positive and negative effects
in Internet of Things (IoT) through the existence of edge computing. As a result,
there has been a need for effective dynamic management of IoT, cloud and edge resources,
in order to address the existence of low-level constraints during resource migration.
Nevertheless, the explosion of IoT devices and data has allowed orchestration of microservices
to adopt an opportunistic approach to how applications and services are deployed in
the edge in IoT platform. A notable approach has been osmotic computing that allows
resources from a federated cloud to be able to diffuse from an ecosystem of higher
solute (network properties and entities) concentration to solvent (applications, layered
interfaces and services). We posit that, while computing resources and applications
are able to move from the federated environment, to the cloud deployable models, to
the edge, then to IoT ecosystem, there is a higher chance of susceptibility of threats
and attacks that may be directed to the emerging edge applications/data due to dynamic
emergent configurations. This paper proposes a 5-layer opportunistic architecture
that adds security metrics across different levels of osmotic computing paradigm.
The proposed 5-layer security architecture addresses the need for autonomously securing
resources-edge computation, edge storage and emerging edge configurations as the computing
resources move to a higher solute in heterogenous edge and cloud datacenters across
IoT devices. This has been achieved by proposing security metrics that address the
prevailing challenge with a degree of certainty.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent and Innovative Computing Applications},
articleno = {9},
numpages = {7},
keywords = {edge, security architecture, opportunistic, osmotic, IoT},
location = {Plaine Magnien, Mauritius},
series = {ICONIC '20}
}

@inproceedings{10.5555/3021955.3022022,
author = {L., Marcelo Dornbusch and Rauta, Leonardo R.P. and Silva, Paulo H. and Silva, Rodrigo C. and Irigoite, Adriano M. and Wangham, Michelle S.},
title = {Remote and Continuous Monitoring of Electrical Quantities Using Web of Things and Cloud Computing},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {The remote monitoring and control of machines are essential in industrial environments.
Emerging communication technologies such as Web of Things and Machine to Machine Communication
can meet this demand for automation. This paper aims to introduce a solution, called
Smart Meter, for continuous and remote monitoring of electrical quantities, in smart
industrial environments with three-phase systems. The proposed solution uses a resource-oriented
architecture and makes use of a Smart Gateway for communication, RESTful web services
and cloud computing. The solution was integrated with a real case study and evaluated
by software testing. The results obtained demonstrate the feasibility of the solution,
and the correctness of measurements persisted in the cloud.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {393–400},
numpages = {8},
keywords = {Cloud Computing, M2M, Remote Monitoring of Electrical Quantities, Web of Things},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI 2016}
}

@inproceedings{10.1145/2747470.2747474,
author = {Toffetti, Giovanni and Brunner, Sandro and Bl\"{o}chlinger, Martin and Dudouet, Florian and Edmonds, Andrew},
title = {An Architecture for Self-Managing Microservices},
year = {2015},
isbn = {9781450334761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2747470.2747474},
doi = {10.1145/2747470.2747474},
abstract = {Running applications in the cloud efficiently requires much more than deploying software
in virtual machines. Cloud applications have to be continuously managed: 1) to adjust
their resources to the incoming load and 2) to face transient failures replicating
and restarting components to provide resiliency on unreliable infrastructure. Continuous
management monitors application and infrastructural metrics to provide automated and
responsive reactions to failures (health management) and changing environmental conditions
(auto-scaling) minimizing human intervention.In the current practice, management functionalities
are provided as infrastructural or third party services. In both cases they are external
to the application deployment. We claim that this approach has intrinsic limits, namely
that separating management functionalities from the application prevents them from
naturally scaling with the application and requires additional management code and
human intervention. Moreover, using infrastructure provider services for management
functionalities results in vendor lock-in effectively preventing cloud applications
to adapt and run on the most effective cloud for the job.In this position paper we
propose a novel architecture that enables scalable and resilient self-management of
microservices applications on cloud.},
booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
pages = {19–24},
numpages = {6},
location = {Bordeaux, France},
series = {AIMC '15}
}

@article{10.1145/3418899,
author = {Brondolin, Rolando and Santambrogio, Marco D.},
title = {A Black-Box Monitoring Approach to Measure Microservices Runtime Performance},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3418899},
doi = {10.1145/3418899},
abstract = {Microservices changed cloud computing by moving the applications’ complexity from
one monolithic executable to thousands of network interactions between small components.
Given the increasing deployment sizes, the architectural exploitation challenges,
and the impact on data-centers’ power consumption, we need to efficiently track this
complexity. Within this article, we propose a black-box monitoring approach to track
microservices at scale, focusing on architectural metrics, power consumption, application
performance, and network performance. The proposed approach is transparent w.r.t.
the monitored applications, generates less overhead w.r.t. black-box approaches available
in the state-of-the-art, and provides fine-grain accurate metrics.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {34},
numpages = {26},
keywords = {network performance monitoring, cloud computing, performance monitoring, docker, power attribution, kubernetes, Microservices}
}

@inproceedings{10.1145/3098954.3098977,
author = {Boukoros, Spyros and Katzenbeisser, Stefan},
title = {Measuring Privacy in High Dimensional Microdata Collections},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3098977},
doi = {10.1145/3098954.3098977},
abstract = {Microdata is collected by companies in order to enhance their quality of service as
well as the accuracy of their recommendation systems. These data often become publicly
available after they have been sanitized. Recent reidentification attacks on publicly
available, sanitized datasets illustrate the privacy risks involved in microdata collections.
Currently, users have to trust the provider that their data will be safe in case data
is published or if a privacy breach occurs. In this work, we empower users by developing
a novel, user-centric tool for privacy measurement and a new lightweight privacy metric.
The goal of our tool is to estimate users' privacy level prior to sharing their data
with a provider. Hence, users can consciously decide whether to contribute their data.
Our tool estimates an individuals' privacy level based on published popularity statistics
regarding the items in the provider's database, and the users' microdata. In this
work, we describe the architecture of our tool as well as a novel privacy metric,
which is necessary for our setting where we do not have access to the provider's database.
Our tool is user friendly, relying on smart visual results that raise privacy awareness.
We evaluate our tool using three real world datasets, collected from major providers.
We demonstrate strong correlations between the average anonymity set per user and
the privacy score obtained by our metric. Our results illustrate that our tool which
uses minimal information from the provider, estimates users' privacy levels comparably
well, as if it had access to the actual database.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {15},
numpages = {8},
keywords = {microdata, user empowerment, privacy metrics, privacy},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/3411029.3411032,
author = {Kogias, Marios and Bugnion, Edouard},
title = {Tail-Tolerance as a Systems Principle Not a Metric},
year = {2020},
isbn = {9781450388764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411029.3411032},
doi = {10.1145/3411029.3411032},
abstract = {Tail-latency tolerance (or just simply tail-tolerance) is the ability for a system
to deliver a response with low-latency nearly all the time. It it typically expressed
as a system metric (e.g., the 99th or 99.99th percentile latency) or as a service-level
objective (e.g., the maximum throughput so that the tail latency is below a desired
threshold). We advocate instead that modern datacenter systems should incorporate
tail-tolerance as a core systems design principle and not a metric to be observed,
and that tail-tolerant systems can be built out of large and complex applications
whose individual components may suffer from latency deviations. This is analogous
to fault-tolerance, where a fault-tolerant system can be built out of unreliable components.
The general solution is for the system to control the applied load and keep it under
the threshold that violates the latency SLO. We propose to augment RPC semantics with
an architectural layer that measures the observed tail latency and probabilistically
rejects RPC requests maintaining throughput under the threshold that violates the
SLO. Our design is application-independent, and does not make any assumptions about
the request service time distribution. We implemented a proof of concept for such
a tail-tolerant layer using programmable switches, called SVEN. We demonstrate that
the approach is suitable even for microsecond-scale RPCs with variable service times.
Moreover, our approach does not induce measurable overheads, and can maintain the
maximum achieved throughput very close to the load level that would violate the SLO
without SVEN. },
booktitle = {4th Asia-Pacific Workshop on Networking},
pages = {16–22},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {APNet '20}
}

@inproceedings{10.1145/3267955.3267965,
author = {Khan, Junaid Ahmed and Westphal, Cedric and Garcia-Luna-Aceves, J. J. and Ghamri-Doudane, Yacine},
title = {NICE: Network-Oriented Information-Centric Centrality for Efficiency in Cache Management},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3267965},
doi = {10.1145/3267955.3267965},
abstract = {All Information-Centric Networking (ICN) architectures proposed to date aim at connecting
users to content directly, rather than connecting clients to servers. Surprisingly,
however, although content caching is an integral of any information-Centric Network,
limited work has been reported on information-centric management of caches in the
context of an ICN. Indeed, approaches to cache management in networks of caches have
focused on network connectivity rather than proximity to content.We introduce the
Network-oriented Information-centric Centrality for Efficiency (NICE) as a new metric
for cache management in information-centric networks. We propose a method to compute
information-centric centrality that scales with the number of caches in a network
rather than the number of content objects, which is many orders of magnitude larger.
Furthermore, it can be pre-processed offline and ahead of time. We apply the NICE
metric to a content replacement policy in caches, and show that a content replacement
based on NICE exhibits better performances than LRU and other policies based on topology-oriented
definitions of centrality.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {31–42},
numpages = {12},
keywords = {content offloading, ICN, graph centrality, cache management},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@inproceedings{10.1145/3338466.3358917,
author = {Heinl, Michael P. and Giehl, Alexander and Wiedermann, Norbert and Plaga, Sven and Kargl, Frank},
title = {MERCAT: A Metric for the Evaluation and Reconsideration of Certificate Authority Trustworthiness},
year = {2019},
isbn = {9781450368261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338466.3358917},
doi = {10.1145/3338466.3358917},
abstract = {Public key infrastructures (PKIs) build the foundation for secure communication of
a vast majority of cloud services. In the recent past, there has been a series of
security incidents leading to increasing concern regarding the trust model currently
employed by PKIs. One of the key criticisms is the architecture's implicit assumption
that certificate authorities (CAs) are trustworthy a priori.This work proposes a holistic
metric to compensate this assumption by a differentiating assessment of a CA's individual
trustworthiness based on objective criteria. The metric utilizes a wide range of technical
and non-technical factors derived from existing policies, technical guidelines, and
research. It consists of self-contained submetrics allowing the simple extension of
the existing set of criteria. The focus is thereby on aspects which can be assessed
by employing practically applicable methods of independent data collection.The metric
is meant to help organizations, individuals, and service providers deciding which
CAs to trust or distrust. For this, the modularized submetrics are clustered into
coherent submetric groups covering a CA's different properties and responsibilities.
By applying individually chosen weightings to these submetric groups, the metric's
outcomes can be adapted to tailored protection requirements according to an exemplifying
attacker model.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
pages = {1–15},
numpages = {15},
keywords = {metric, digital certificate, pki, trustworthiness assessment, cloud security, ca, x.509},
location = {London, United Kingdom},
series = {CCSW'19}
}

@inproceedings{10.1145/3234944.3234952,
author = {Kim, Yubin and Callan, Jamie},
title = {Measuring the Effectiveness of Selective Search Index Partitions without Supervision},
year = {2018},
isbn = {9781450356565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234944.3234952},
doi = {10.1145/3234944.3234952},
abstract = {Selective search architectures partition a document collection into topic-oriented
index shards, usually using algorithms that have random components. Different mappings
of documents into index shards (shard maps) produce different search accuracy and
consistency, however identifying which shard maps will deliver the highest average
effectiveness is an open problem. This paper presents a new metric, Area Under Recall
Curve (AUReC), to evaluate and compare shard maps. AUReC is the first such metric
that is independent of resource selection and shard cut-off estimation. It does not
require an end-to-end evaluation or manual gold-standard judgements. Experiments show
that its predictions are highly-correlated with evaluating end-to-end systems of various
configurations, while being easier to implement and computationally inexpensive.},
booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {91–98},
numpages = {8},
keywords = {clustering, cluster-based retrieval, evaluation, distributed search, selective search},
location = {Tianjin, China},
series = {ICTIR '18}
}

@article{10.1145/2829950,
author = {Tomusk, Erik and Dubach, Christophe and O’boyle, Michael},
title = {Four Metrics to Evaluate Heterogeneous Multicores},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2829950},
doi = {10.1145/2829950},
abstract = {Semiconductor device scaling has made single-ISA heterogeneous processors a reality.
Heterogeneous processors contain a number of different CPU cores that all implement
the same Instruction Set Architecture (ISA). This enables greater flexibility and
specialization, as runtime constraints and workload characteristics can influence
which core a given workload is run on. A major roadblock to the further development
of heterogeneous processors is the lack of appropriate evaluation metrics. Existing
metrics can be used to evaluate individual cores, but to evaluate a heterogeneous
processor, the cores must be considered as a collective. Without appropriate metrics,
it is impossible to establish design goals for processors, and it is difficult to
accurately compare two different heterogeneous processors.We present four new metrics
to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity,
gap overhead, set overhead, and generality. The metrics consider sets rather than
individual cores. We use examples to demonstrate each metric, and show that the metrics
can be used to quantify intuitions about heterogeneous cores.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {37},
numpages = {25},
keywords = {gap overhead, effective speed, single-ISA, generality, Localized nonuniformity, set overhead}
}

@inproceedings{10.5555/2821327.2821338,
author = {L\"{u}bke, Daniel},
title = {Using Metric Time-Lines for Identifying Architecture Shortcomings in Process Execution Architectures},
year = {2015},
publisher = {IEEE Press},
abstract = {Process Execution with Service Orchestrations is an emerging architectural style for
developing business software systems. However, few special metrics for guiding software
architecture decisions have been proposed and no existing business process metrics
have been evaluated for their suitability. By following static code metrics over time,
architects can gain a better understanding, how processes and the whole system evolve
and whether the metrics evolve as expected. This allows architects to recogniize when
to intervene in the development and make architecture adjustments or refactorings.
This paper presents an explatory study that uses time-lines of static process size
metrics for constant feedback to software architects that deal with process-oriented
architectures.},
booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
pages = {55–58},
numpages = {4},
location = {Florence, Italy},
series = {SAM '15}
}

@inproceedings{10.1145/3357384.3357956,
author = {Boiarov, Andrei and Tyantov, Eduard},
title = {Large Scale Landmark Recognition via Deep Metric Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357956},
doi = {10.1145/3357384.3357956},
abstract = {This paper presents a novel approach for landmark recognition in images that we've
successfully deployed at Mail.ru. This method enables us to recognize famous places,
buildings, monuments, and other landmarks in user photos. The main challenge lies
in the fact that it's very complicated to give a precise definition of what is and
what is not a landmark. Some buildings, statues and natural objects are landmarks;
others are not. There's also no database with a fairly large number of landmarks to
train a recognition model. A key feature of using landmark recognition in a production
environment is that the number of photos containing landmarks is extremely small.
This is why the model should have a very low false positive rate as well as high recognition
accuracy. We propose a metric learning-based approach that successfully deals with
existing challenges and efficiently handles a large number of landmarks. Our method
uses a deep neural network and requires a single pass inference that makes it fast
to use in production. We also describe an algorithm for cleaning landmarks database
which is essential for training a metric learning model. We provide an in-depth description
of basic components of our method like neural network architecture, the learning strategy,
and the features of our metric learning approach. We show the results of proposed
solutions in tests that emulate the distribution of photos with and without landmarks
from a user collection. We compare our method with others during these tests. The
described system has been deployed as a part of a photo recognition solution at Cloud
Mail.ru, which is the photo sharing and storage service at Mail.ru Group.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {169–178},
numpages = {10},
keywords = {metric learning, landmark recognition, deep learning},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3195546.3195549,
author = {Schmidts, Oliver and Kraft, Bodo and Schreiber, Marc and Z\"{u}ndorf, Albert},
title = {Continuously Evaluated Research Projects in Collaborative Decoupled Environments},
year = {2018},
isbn = {9781450357449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195546.3195549},
doi = {10.1145/3195546.3195549},
abstract = {Often, research results from collaboration projects are not transferred into productive
environments even though approaches are proven to work in demonstration prototypes.
These demonstration prototypes are usually too fragile and error-prone to be transferred
easily into productive environments. A lot of additional work is required.Inspired
by the idea of an incremental delivery process, we introduce an architecture pattern,
which combines the approach of Metrics Driven Research Collaboration with microservices
for the ease of integration. It enables keeping track of project goals over the course
of the collaboration while every party may focus on their expert skills: researchers
may focus on complex algorithms, practitioners may focus on their business goals.Through
the simplified integration (intermediate) research results can be introduced into
a productive environment which enables getting an early user feedback and allows for
the early evaluation of different approaches. The practitioners' business model benefits
throughout the full project duration.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice},
pages = {2–9},
numpages = {8},
keywords = {lean software development, software architecture, metrics, research best practices, research collaboration management},
location = {Gothenburg, Sweden},
series = {SER&amp;IP '18}
}

@inproceedings{10.1145/3131151.3131153,
author = {Durelli, Rafael S. and Viana, Matheus C. and de S. Landi, Andr\'{e} and Durelli, Vinicius H. S. and Delamaro, Marcio E. and de Camargo, Valter V.},
title = {Improving the Structure of KDM Instances via Refactorings: An Experimental Study Using KDM-RE},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131153},
doi = {10.1145/3131151.3131153},
abstract = {Architecture-Driven Modernization (ADM) is an initiative of the Object Management
Group (OMG) whose main purpose is to provide standard metamodels for software modernization
activities. The most important metamodel is the Knowledge Discovery Metamodel (KDM),
which represents software artifacts in a language-agnostic fashion. A fundamental
step in software modernization is refactoring. However, there is a lack of tools that
address how refactoring can be applied in conjunction with ADM. We developed a tool,
called KDM-RE, that supports refactorings in KDM instances through: (i) a set of wizards
that aid the software modernization engineer during refactoring activities; (ii) a
change propagation module that keeps the internal metamodels synchronized; and (iii)
the selection and application of refactorings available in its repository. This paper
evaluates the application of refactorings to KDM instances in an experiment involving
seven systems implemented in Java. We compared the pre-refactoring versions of these
systems with the refactored ones using the Quality Model for Object-Oriented Design
(QMOOD) metric set. The results from this evaluation suggest that KDM-RE provides
advantages to software modernization engineers refactoring systems represented as
KDMs.},
booktitle = {Proceedings of the 31st Brazilian Symposium on Software Engineering},
pages = {174–183},
numpages = {10},
keywords = {Knowledge-Discovery Metamodel, Model-Driven Development, Refactoring, Architecture-Driven Modernization},
location = {Fortaleza, CE, Brazil},
series = {SBES'17}
}

@article{10.1145/3293455,
author = {Arcuri, Andrea},
title = {RESTful API Automated Test Case Generation with EvoMaster},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3293455},
doi = {10.1145/3293455},
abstract = {RESTful APIs are widespread in industry, especially in enterprise applications developed
with a microservice architecture. A RESTful web service will provide data via an API
over the network using HTTP, possibly interacting with databases and other web services.
Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP
requests/responses to a remote server. Many approaches in the literature do black-box
testing, because often the tested API is a remote service whose code is not available.
In this article, we consider testing from the point of view of the developers, who
have full access to the code that they are writing. Therefore, we propose a fully
automated white-box testing approach, where test cases are automatically generated
using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding
metrics. However, REST is not a protocol but rather a set of guidelines on how to
design resources accessed over HTTP endpoints. For example, there are guidelines on
how related resources should be structured with hierarchical URIs and how the different
HTTP verbs should be used to represent well-defined actions on those resources. Test-case
generation for RESTful APIs that only rely on white-box information of the source
code might not be able to identify how to create prerequisite resources needed before
being able to test some of the REST endpoints. Smart sampling techniques that exploit
the knowledge of best practices in RESTful API design are needed to generate tests
with predefined structures to speed up the search. We implemented our technique in
a tool called EvoMaster, which is open source. Experiments on five open-source, yet
non-trivial, RESTful services show that our novel technique automatically found 80
real bugs in those applications. However, obtained code coverage is lower than the
one achieved by the manually written test suites already existing in those services.
Research directions on how to further improve such an approach are therefore discussed,
such as the handling of SQL databases.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {3},
numpages = {37},
keywords = {Software engineering, testing, REST, web service}
}

@article{10.1145/3351278,
author = {Yu, Tuo and Nahrstedt, Klara},
title = {ShoesHacker: Indoor Corridor Map and User Location Leakage through Force Sensors in Smart Shoes},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351278},
doi = {10.1145/3351278},
abstract = {The past few years have witnessed the rise of smart shoes, the wearable devices that
measure foot force or track foot motion. However, people are not aware of the possible
privacy leakage from in-shoe force sensors. In this paper, we explore the possibility
of locating an indoor victim based on the force signals leaked from smart shoes. We
present ShoesHacker, an attack scheme that reconstructs the corridor map of the building
that the victim walks in based on force data only. The corridor map enables the attacker
to recognize the building, and thus locate the victim on a global map. To handle the
lack of training data, we design the stair landing detection algorithm, based on which
we extract training data when victims are walking in stairwells. We estimate the trajectory
of each walk, and propose the path merging algorithm to merge the trajectories. Moreover,
we design a metric to quantify the similarity between corridor maps, which makes building
recognition possible. Our experimental results show that, the building recognition
accuracy reaches 77.5% in a 40-building dataset, and the victim can be located with
an average error lower than 6 m, which reveals the danger of privacy leakage through
smart shoes. CCS Concepts: • Information systems~Mobile information processing systems;
Location based services; • Human-centered computing~Mobile devices; Ubiquitous and
mobile computing systems and tools; • Security and privacy~Domain-specific security
and privacy architectures.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {120},
numpages = {29},
keywords = {smart shoes, force sensors, Corridor map reconstruction}
}

@inproceedings{10.1145/3219819.3219834,
author = {Staar, Peter W J and Dolfi, Michele and Auer, Christoph and Bekas, Costas},
title = {Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219834},
doi = {10.1145/3219819.3219834},
abstract = {Over the past few decades, the amount of scientific articles and technical literature
has increased exponentially in size. Consequently, there is a great need for systems
that can ingest these documents at scale and make the contained knowledge discoverable.
Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images)
as well as the presentation of the data (e.g. complex tables) make the extraction
of qualitative and quantitive data extremely challenging. In this paper, we present
a modular, cloud-based platform to ingest documents at scale. This platform, called
the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse
and annotate documents (i.e. collect ground-truth), train machine-learning classification
algorithms and ultimately convert any type of PDF or bitmap-documents to a structured
content representation format. We will show that each of the modules is scalable due
to an asynchronous microservice architecture and can therefore handle massive amounts
of documents. Furthermore, we will show that our capability to gather groundtruth
is accelerated by machine-learning algorithms by at least one order of magnitude.
This allows us to both gather large amounts of ground-truth in very little time and
obtain very good precision/recall metrics in the range of 99% with regard to content
conversion to structured output. The CCS platform is currently deployed on IBM internal
infrastructure and serving more than 250 active users for knowledge-engineering project
engagements.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {774–782},
numpages = {9},
keywords = {ibm research, machine learning, deep learning, artificial intelligence, ibm, cloud architecture, table processing, knowledge ingestion, cloud computing, ai, asynchronous architecture, pdf, document conversion},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3102304.3102334,
author = {Calcina-Ccori, Pablo and Costa, Laisa and Fedrecheski, Geovane and Esquiagola, John and Zuffo, Marcelo and da Silva, Fl\'{a}vio Corr\^{e}a},
title = {Agile Servient Integration with the Swarm: Automatic Code Generation for Nodes in the Internet of Things},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102334},
doi = {10.1145/3102304.3102334},
abstract = {Swarm vision, consists in an organic ecosystem of heterogeneous devices that communicate
and collaborate to achieve complex results. In previous work, we have proposed an
architecture to implement this vision based on web technologies. In this paper, we
have proposed a framework that makes the creation of Swarm-ready servients (devices
that acts both as server and client) easier, by generating a ready-to-run project
from a high-level description of the service. The project generated contains all dependencies
and libraries needed to integrate an IoT device into the Swarm, thus saving development
and configuration time. We compared the development effort of creating a servient
by hand and by using our framework, having the number of lines of code as a metric.
Our results show a reduction of 500% in the development effort to connect a device
to the Swarm. The next steps include a semantic high-level description for participating
services and support for resource-constrained devices.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {30},
numpages = {6},
keywords = {Swarm, automatic code generation, Servient, Internet of Things},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/3292425.3292428,
author = {Liu, Jiaming and Xian, Cuiling},
title = {Extracting Information of Urban Land Surface with High Resolution Remote Sensing Data},
year = {2018},
isbn = {9781450365468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292425.3292428},
doi = {10.1145/3292425.3292428},
abstract = {With the advantages of high spatial resolution and definition, and rich information,
land classification and land utilization of urban surface can be reached by using
the high resolution remote sensing data. Based on the high resolution remote sensing
image data Worldview-1 and Worldview-2 as the main data source, using the methods
of stereo images, object-oriented land use classification technique and architecture
shadow, this study (1) extracts 5-meter resolution DEM data of Shilipu district in
Wuhan and the average relative error compared with the measured DEM data of ground
points is 3.71% with relatively high precision, (2) obtains the land use information
of this area with an accuracy rate of 90%, and (3) achieve the data of building height
in this area with the relative error of less than 20%. The results of this paper show
that the high speed and precision can meet the 3d modeling elevation precision and
plane precision of digital urban buildings when using high-resolution remote sensing
images to extract basic geographic information in small urban areas, which will play
an important role in the research of urban surface information extraction in the future.},
booktitle = {Proceedings of the 2018 International Conference on Information Hiding and Image Processing},
pages = {62–67},
numpages = {6},
keywords = {High resolution, urban surface, remote sensing},
location = {Manchester, United Kingdom},
series = {IHIP 2018}
}

@article{10.1145/3394956,
author = {Sahoo, Kshira Sagar and Puthal, Deepak},
title = {SDN-Assisted DDoS Defense Framework for the Internet of Multimedia Things},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3394956},
doi = {10.1145/3394956},
abstract = {The Internet of Things is visualized as a fundamental networking model that bridges
the gap between the cyber and real-world entity. Uniting the real-world object with
virtualization technology is opening further opportunities for innovation in nearly
every individual’s life. Moreover, the usage of smart heterogeneous multimedia devices
is growing extensively. These multimedia devices that communicate among each other
through the Internet form a unique paradigm called the Internet of Multimedia Things
(IoMT). As the volume of the collected data in multimedia application increases, the
security, reliability of communications, and overall quality of service need to be
maintained. Primarily, distributed denial of service attacks unveil the pervasiveness
of vulnerabilities in IoMT systems. However, the Software Defined Network (SDN) is
a new network architecture that has the central visibility of the entire network,
which helps to detect any attack effectively. In this regard, the combination of SDN
and IoMT, termed SD-IoMT, has the immense ability to improve the network management
and security capabilities of the IoT system. This article proposes an SDN-assisted
two-phase detection framework, namely SD-IoMT-Protector, in which the first phase
utilizes the entropy technique as the detection metric to verify and alert about the
malicious traffic. The second phase has trained with an optimized machine learning
technique for classifying different attacks. The outcomes of the experimental results
signify the usefulness and effectiveness of the proposed framework for addressing
distributed denial of service issues of the SD-IoMT system.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {98},
numpages = {18},
keywords = {machine learning, IoMT, entropy, Control plane, security, SDN}
}

@inproceedings{10.1145/3401025.3401740,
author = {Scrocca, Mario and Tommasini, Riccardo and Margara, Alessandro and Valle, Emanuele Della and Sakr, Sherif},
title = {The Kaiju Project: Enabling Event-Driven Observability},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3401740},
doi = {10.1145/3401025.3401740},
abstract = {Microservices architectures are getting momentum. Even small and medium-size companies
are migrating towards cloud-based distributed solutions supported by lightweight virtualization
techniques, containers, and orchestration systems. In this context, understanding
the system behavior at runtime is critical to promptly react to errors. Unfortunately,
traditional monitoring techniques are not adequate for such complex and dynamic environments.
Therefore, a new challenge, namely observability, emerged from precise industrial
needs: expose and make sense of the system behavior at runtime. In this paper, we
investigate observability as a research problem. We discuss the benefits of events
as a unified abstraction for metrics, logs, and trace data, and the advantages of
employing event stream processing techniques and tools in this context. We show that
an event-based approach enables understanding the system behavior in near real-time
more effectively than state-of-the-art solutions in the field. We implement our model
in the Kaiju system and we validate it against a realistic deployment supported by
a software company.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {85–96},
numpages = {12},
keywords = {event stream processing, orchestration systems, observability, event-based systems},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.5555/2602339.2602341,
author = {Buevich, Maxim and Schnitzer, Dan and Escalada, Tristan and Jacquiau-Chamski, Arthur and Rowe, Anthony},
title = {Fine-Grained Remote Monitoring, Control and Pre-Paid Electrical Service in Rural Microgrids},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {In this paper, we present the architecture, design and experiences from a wirelessly
managed microgrid deployment in rural Les Anglais, Haiti. The system consists of a
three-tiered architecture with a cloud-based monitoring and control service, a local
embedded gateway infrastructure and a mesh network of wireless smart meters deployed
at 52 buildings. Each smart meter device has an 802.15.4 radio that enables remote
monitoring and control of electrical service. The meters communicate over a scalable
multi-hop TDMA network back to a central gateway that manages load within the system.
The gateway also provides an 802.11 interface for an on-site operator and a cellular
modem connection to a cloud-backend that manages and stores billing and usage data.
The cloud backend allows occupants in each home to pre-pay for electricity at a particular
peak power limit using a text messaging service. The system activates each meter within
seconds and locally enforces power limits with provisioning for theft detection. We
believe that this fine-grained micro-payment model can enable sustainable power in
otherwise unfeasible areas.This paper provides a chronology of our deployment and
installation strategy that involved GPS-based site mapping along with various network
conditioning actions required as the network evolved. Finally, we summarize key lessons
learned and hypothesis about additional hardware that could be used to ease the tracing
of faults like short circuits and downed lines within microgrids.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {1–12},
numpages = {12},
keywords = {wireless local area networks, sensor networks, microgrid},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3359789.3359843,
author = {Nagendra, Vasudevan and Yegneswaran, Vinod and Porras, Phillip and Das, Samir R},
title = {Coordinated Dataflow Protection for Ultra-High Bandwidth Science Networks},
year = {2019},
isbn = {9781450376280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359789.3359843},
doi = {10.1145/3359789.3359843},
abstract = {The Science DMZ (SDMZ) is a special purpose network architecture proposed by ESnet
(Energy Sciences Network) to facilitate distributed science experimentation on terabyte-
(or petabyte-) scale data, exchanged over ultra-high bandwidth WAN links. Critical
security challenges faced by these networks include: (i) network monitoring at high
bandwidths, (ii) reconciling site-specific policies with project-level policies for
conflict-free policy enforcement, (iii) dealing with geographically-distributed datasets
with varying levels of sensitivity, and (iv) dynamically enforcing appropriate security
rules. To address these challenges, we develop a fine-grained dataflow-based security
enforcement system, called CoordiNetZ (CNZ), that provides coordinated situational
awareness, i.e., the use of context-aware tagging for policy enforcement using the
dynamic contextual information derived from hosts and network elements. We also developed
tag and IP-based security microservices that incur minimal overheads in enforcing
security to data flows exchanged across geographically-distributed SDMZ sites. We
evaluate our prototype implementation across two geographically distributed SDMZ sites
with SDN-based case studies, and present performance measurements that respectively
highlight the utility of our framework and demonstrate efficient implementation of
security policies across distributed SDMZ networks.},
booktitle = {Proceedings of the 35th Annual Computer Security Applications Conference},
pages = {568–583},
numpages = {16},
keywords = {usability and human-centric aspects of security, distributed systems security, software-defined programmable security, NFV, big data security, network security, SDN},
location = {San Juan, Puerto Rico, USA},
series = {ACSAC '19}
}

@article{10.1145/2975161,
author = {Bouraoui, Hasna and Jerad, Chadlia and Chattopadhyay, Anupam and Hadj-Alouane, Nejib Ben},
title = {Hardware Architectures for Embedded Speaker Recognition Applications: A Survey},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/2975161},
doi = {10.1145/2975161},
abstract = {Authentication technologies based on biometrics, such as speaker recognition, are
attracting more and more interest thanks to the elevated level of security offered
by these technologies. Despite offering many advantages, such as remote use and low
vulnerability, speaker recognition applications are constrained by the heavy computational
effort and the hard real-time constraints. When such applications are run on an embedded
platform, the problem becomes more challenging, as additional constraints inherent
to this specific domain are added. In the literature, different hardware architectures
were used/designed for implementing a process with a focus on a given particular metric.
In this article, we give a survey of the state-of-the-art works on implementations
of embedded speaker recognition applications. Our aim is to provide an overview of
the different approaches dealing with acceleration techniques oriented towards speaker
and speech recognition applications and attempt to identify the past, current, and
future research trends in the area. Indeed, on the one hand, many flexible solutions
were implemented, using either General Purpose Processors or Digital Signal Processors.
In general, these types of solutions suffer from low area and energy efficiency. On
the other hand, high-performance solutions were implemented on Application Specific
Integrated Circuits or Field Programmable Gate Arrays but at the expense of flexibility.
Based on the available results, we compare the application requirements vis-\`{a}-vis
the performance achieved by the systems. This leads to the projection of new research
trends that can be undertaken in the future.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = apr,
articleno = {78},
numpages = {28},
keywords = {Embedded hardware, acceleration, classification algorithms and implementations, speaker recognition}
}

@inproceedings{10.1145/3412841.3441899,
author = {Torquato, Matheus and Maciel, Paulo and Vieira, Marco},
title = {Analysis of VM Migration Scheduling as Moving Target Defense against Insider Attacks},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441899},
doi = {10.1145/3412841.3441899},
abstract = {As cybersecurity threats evolve, cloud computing defenses must adapt to face new challenges.
Unfortunately, due to resource sharing, cloud computing platforms open the door for
insider attacks, which consist of malicious actions from cloud authorized users (e.g.,
clients of an Infrastructure-as-a-Service (IaaS) cloud) targeting the co-hosted users
or the underlying provider environment. Virtual machine (VM) migration is a Moving
Target Defense (MTD) technique to mitigate insider attacks effects, as it provides
VMs positioning manageability. However, there is a clear demand for studies quantifying
the security benefits of VM migration-based MTD considering different system architecture
configurations. This paper tries to fill such a gap by presenting a Stochastic Reward
Net model for the security evaluation of a VM migration-based MTD. The security metric
of interest is the probability of attack success. We consider multiple architectures,
ranging from one physical machine pool (without MTD) up to four physical machine pools.
The evaluation also considers the unavailability due to VM migration. The key contributions
are i) a set of results highlighting the probability of insider attacks success over
time in different architectures and VM migration schedules, and ii) suggestions for
selecting VMs as candidates for MTD deployment based on the tolerance levels of the
attack success probability. The results are validated against simulation results to
confirm the accuracy of the model.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {194–202},
numpages = {9},
keywords = {VM migration, moving target defense, availability, stochastic petri nets, migration-based dynamic platform},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3164541.3164576,
author = {Kim, Heejin and Jeon, Seil and Raza, Syed M. and Lee, Joohyun and Choo, Hyunseung},
title = {Service-Aware Split Point Selection for User-Centric Mobility Enhancement in SDN},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164576},
doi = {10.1145/3164541.3164576},
abstract = {IP mobility anchor works as the redirection/split point of the packet destined to
the mobile terminal (MT), as well as IP address/prefix assignment and mobility binding
management in the legacy mobility management protocols. In software-defined networking
(SDN), the split point can be managed by the SDN controller or controller application,
as the control of the network is separated from the forwarding entities. The demand
of user QoE is ever increasing and they always want to get the best service continuity
served in mobility management. Differentiated split point selection per service type
could be one of the effective measures to enhance user QoE in a mobility management
environment. In this paper, we propose a service-aware split point selection mechanism
for user-centric mobility management enhancement in SDN. Specifically, we propose
the mobility control architecture, which can classify service flow type and determine
advantageous split point depending on a service flow type. We analyze the performance
of the proposed split point selection mechanism compared to target mechanisms. We
also measure the performance metrics on an ONOS-based SDN testbed to identify the
superiority of the proposed mechanism.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {95},
numpages = {8},
keywords = {mobility management, split point selection, software-defined networking},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@article{10.1145/3319498,
author = {Izadpanah, Ramin and Allan, Benjamin A. and Dechev, Damian and Brandt, Jim},
title = {Production Application Performance Data Streaming for System Monitoring},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3319498},
doi = {10.1145/3319498},
abstract = {In this article, we present an approach to streaming collection of application performance
data. Practical application performance tuning and troubleshooting in production high-performance
computing (HPC) environments requires an understanding of how applications interact
with the platform, including (but not limited to) parallel programming libraries such
as Message Passing Interface (MPI). Several profiling and tracing tools exist that
collect heavy runtime data traces either in memory (released only at application exit)
or on a file system (imposing an I/O load that may interfere with the performance
being measured). Although these approaches are beneficial in development stages and
post-run analysis, a systemwide and low-overhead method is required to monitor deployed
applications continuously. This method must be able to collect information at both
the application and system levels to yield a complete performance picture.In our approach,
an application profiler collects application event counters. A sampler uses an efficient
inter-process communication method to periodically extract the application counters
and stream them into an infrastructure for performance data collection. We implement
a tool-set based on our approach and integrate it with the Lightweight Distributed
Metric Service (LDMS) system, a monitoring system used on large-scale computational
platforms. LDMS provides the infrastructure to create and gather streams of performance
data in a low overhead manner. We demonstrate our approach using applications implemented
with MPI, as it is one of the most common standards for the development of large-scale
scientific applications.We utilize our tool-set to study the impact of our approach
on an open source HPC application, Nalu. Our tool-set enables us to efficiently identify
patterns in the behavior of the application without source-level knowledge. We leverage
LDMS to collect system-level performance data and explore the correlation between
the system and application events. Also, we demonstrate how our tool-set can help
detect anomalies with a low latency. We run tests on two different architectures:
a system enabled with Intel Xeon Phi and another system equipped with Intel Xeon processor.
Our overhead study shows our method imposes at most 0.5% CPU usage overhead on the
application in realistic deployment scenarios.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = apr,
articleno = {8},
numpages = {25},
keywords = {application profiling, Application and system monitoring, performance data streaming}
}

@inproceedings{10.1145/3452296.3472922,
author = {Fayed, Marwan and Bauer, Lorenz and Giotsas, Vasileios and Kerola, Sami and Majkowski, Marek and Odintsov, Pavel and Sitnicki, Jakub and Chung, Taejoong and Levin, Dave and Mislove, Alan and Wood, Christopher A. and Sullivan, Nick},
title = {The Ties That Un-Bind: Decoupling IP from Web Services and Sockets for Robust Addressing Agility at CDN-Scale},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472922},
doi = {10.1145/3452296.3472922},
abstract = {The couplings between IP addresses, names of content or services, and socket interfaces,
are too tight. This impedes system manageability, growth, and overall provisioning.
In turn, large-scale content providers are forced to use staggering numbers of addresses,
ultimately leading to address exhaustion (IPv4) and inefficiency (IPv6).In this paper,
we revisit IP bindings, entirely. We attempt to evolve addressing conventions by decoupling
IP in DNS and from network sockets. Alongside technologies such as SNI and ECMP, a
new architecture emerges that ``unbinds'' IP from services and servers, thereby returning
IP's role to merely that of reachability. The architecture is under evaluation at
a major CDN in multiple datacenters. We show that addresses can be generated randomly
emph{per-query}, for 20M+ domains and services, from as few as ~4K addresses, 256
addresses, and even emph{one} IP address. We explain why this approach is transparent
to routing, L4/L7 load-balancers, distributed caching, and all surrounding systems
-- and is emph{highly desirable}. Our experience suggests that many network-oriented
systems and services (e.g., route leak mitigation, denial of service, measurement)
could be improved, and new ones designed, if built with addressing agility.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {433–446},
numpages = {14},
keywords = {programmable sockets, provisioning, addressing, content distribution},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@inproceedings{10.5555/2821357.2821365,
author = {Nikravesh, Ali Yadavar and Ajila, Samuel A. and Lung, Chung-Horng},
title = {Towards an Autonomic Auto-Scaling Prediction System for Cloud Resource Provisioning},
year = {2015},
publisher = {IEEE Press},
abstract = {This paper investigates the accuracy of predictive auto-scaling systems in the Infrastructure
as a Service (IaaS) layer of cloud computing. The hypothesis in this research is that
prediction accuracy of auto-scaling systems can be increased by choosing an appropriate
time-series prediction algorithm based on the performance pattern over time. To prove
this hypothesis, an experiment has been conducted to compare the accuracy of time-series
prediction algorithms for different performance patterns. In the experiment, workload
was considered as the performance metric, and Support Vector Machine (SVM) and Neural
Networks (NN) were utilized as time-series prediction techniques. In addition, we
used Amazon EC2 as the experimental infrastructure and TPC-W as the benchmark to generate
different workload patterns. The results of the experiment show that prediction accuracy
of SVM and NN depends on the incoming workload pattern of the system under study.
Specifically, the results show that SVM has better prediction accuracy in the environments
with periodic and growing workload patterns, while NN outperforms SVM in forecasting
unpredicted workload pattern. Based on these experimental results, this paper proposes
an architecture for a self-adaptive prediction suite using an autonomic system approach.
This suite can choose the most suitable prediction technique based on the performance
pattern, which leads to more accurate prediction results.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {35–45},
numpages = {11},
keywords = {workload pattern, neural networks, resource provisioning, support vector machine, cloud computing, auto-scaling, autonomic},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/2747470.2747471,
author = {Dudouet, Florian and Edmonds, Andrew and Erne, Michael},
title = {Reliable Cloud-Applications: An Implementation through Service Orchestration},
year = {2015},
isbn = {9781450334761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2747470.2747471},
doi = {10.1145/2747470.2747471},
abstract = {As cloud-deployed applications became more and more mainstream, continuously more
complex services started to be deployed; indeed where initially monolithic applications
were simply ported to the cloud, applications are now more and more often composed
of micro-services. This improves the flexibility of an application but also makes
it more complex due to the sheer number of services comprising it.As deployment and
runtime management becomes more complex, orchestration software are becoming necessary
to completely manage the lifecycle of cloud applications. One crucial problem remaining
is how these applications can be made reliable in the cloud, a naturally unreliable
environment.In this paper we propose concepts and architectures which were implemented
in our orchestration software to guarantee reliability. Our initial implementation
also relies on Monasca, a well-known monitoring software for Open-Stack, to gather
proper metric and execute threshold-based actions. This allows us to show how service
reliability can be ensured using orchestration and how a proper incident-management
software feeding decisions to the orchestration engine ensures high-availability of
all components of managed applications.},
booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
pages = {1–6},
numpages = {6},
keywords = {incident management, orchestration, cloud computing, reliability},
location = {Bordeaux, France},
series = {AIMC '15}
}

@inproceedings{10.1145/3368691.3368704,
author = {Albataineh, Abdallah and Al-Qassas, Raad S. and Qasaimeh, Malik},
title = {A New Architecture for Voice Interconnection Using Packet Switched Network},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368704},
doi = {10.1145/3368691.3368704},
abstract = {Interconnecting voice service providers require a mutual trust between communicating
entities, which are built either using bilateral agreements or intermediary service
provider. To achieve such relationship between Anonymous Service Providers we should
have an automated mechanism. In this paper, we propose a conceptual architecture that
can build such relationship between communicating Anonymous Service Providers. By
applying this architecture, we argue that we can increase efficiency, security, and
performance of service provider's networks. The impact of internet speed on the interconnection
network is measured using key metrics including ACD, ASR, PDD, NER, and MOS.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {13},
numpages = {7},
keywords = {voice network architecture, SS7, ACD, PDD, ASR, SIP},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.5555/3191835.3191902,
author = {Lin, Chih-Lu and Chen, Ying-Liang and Kao, Hung-Yu},
title = {Question Difficulty Evaluation by Knowledge Gap Analysis in Question Answer Communities},
year = {2014},
isbn = {9781479958764},
publisher = {IEEE Press},
abstract = {The Community Question Answer (CQA) service is a typical forum of Web 2.0 that shares
knowledge among people. There are thousands of questions that are posted and solved
every day. Because of the various users of the CQA service, question search and ranking
are the most important topics of research in the CQA portal. In this study, we addressed
the problem of identifying questions as being hard or easy by means of a probability
model. In addition, we observed the phenomenon called knowledge gap that is related
to the habit of users and used a knowledge gap diagram to illustrate how much of a
knowledge gap exists in different categories. To this end, we proposed an approach
called the knowledge-gap-based difficulty rank (KG-DRank) algorithm, which combines
the user-user network and the architecture of the CQA service to find hard questions.
We used f-measure, AUC, MAP, NDCG, precision@Top5 and concordance analysis to evaluate
the experimental results. Our results show that our approach leads to better performance
than other baseline approaches across all evaluation metrics.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {336–339},
numpages = {4},
keywords = {social network, CQA portal, link analysis, knowledge gap, expert finding, difficulty},
location = {Beijing, China},
series = {ASONAM '14}
}

@inproceedings{10.1145/3343031.3350592,
author = {Galteri, Leonardo and Seidenari, Lorenzo and Bertini, Marco and Uricchio, Tiberio and Del Bimbo, Alberto},
title = {Fast Video Quality Enhancement Using GANs},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350592},
doi = {10.1145/3343031.3350592},
abstract = {Video compression algorithms result in a reduction of image quality, because of their
lossy approach to reduce the required bandwidth. This affects commercial streaming
services such as Netflix, or Amazon Prime Video, but affects also video conferencing
and video surveillance systems. In all these cases it is possible to improve the video
quality, both for human view and for automatic video analysis, without changing the
compression pipeline, through a post-processing that eliminates the visual artifacts
created by the compression algorithms. Generative Adversarial Networks have obtained
extremely high quality results in image enhancement tasks; however, to obtain such
results large generators are usually employed, resulting in high computational costs
and processing time. In this work we present an architecture that can be used to reduce
the computational cost and that has been implemented on mobile devices. A possible
application is to improve video conferencing, or live streaming. In these cases there
is no original uncompressed video stream available. Therefore, we report results using
no-reference video quality metric showing high naturalness and quality even for efficient
networks.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1065–1067},
numpages = {3},
keywords = {gans, video quality enhancement, real-time enhancement, video streaming, video compression},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3258045,
author = {Savola, Reijo and Abie, Habtamu and Kanstr\'{e}n, Teemu},
title = {Session Details: Fourth International Workshop on Measurability of Security in Software Architectures (MeSSa 2017)},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3258045},
doi = {10.1145/3258045},
abstract = {Cybersecurity incidents are increasing, and at the same time, our society depends
more and more on cyber-physical systems. Systematic approaches to measure cybersecurity
are needed in order to support efficient construction and maintenance of secure software
systems. Security measurement of software architectures is needed to produce sufficient
evidence of security level as early as in the design phase. Design-time security measuring
should support "security by design" approach. Moreover, software architectures have
to support runtime security measurement to obtain up-to-date security information
from an online software system, service or product. Security metrics and measurements
are exploited in situational awareness monitoring and self-adaptive security solutions.
The area of security metrics and security assurance metrics research is evolving,
but still lacks widely accepted metrics definitions and applicable measuring techniques.
Strong collaboration between security experts, software architects and system developers
is needed to address this. MeSSa2017 workshop addresses these and other related topics
to increase the importance of the overall picture, requiring sets of design patterns,
measurements, metrics, best practices, and means to integrate this cost-effectively
in the overall design and operational profiles.The outcome of the workshop will be
an increased shared understanding of challenges and opportunities in systematic approaches
to measure cybersecurity, which are needed in order to support efficient construction
and maintenance of secure software systems.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/3238147.3240463,
author = {Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin},
title = {Achieving Test Automation with Testers without Coding Skills: An Industrial Report},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240463},
doi = {10.1145/3238147.3240463},
abstract = {We present a process driven test automation solution which enables delegating (part
of) automation tasks from test automation engineer (expensive resource) to test analyst
(non-developer, less expensive). In our approach, a test automation engineer implements
test steps (or actions) which are executed automatically. Such automated test steps
represent user actions in the system under test and specified by a natural language
which is understandable by a non-technical person. Then, a test analyst with a domain
knowledge organizes automated steps combined with test input to create an automated
test case. It should be emphasized that the test analyst does not need to possess
programming skills to create, modify or execute automated test cases. We refine benchmark
test automation architecture to be better suitable for an effective separation and
sharing of responsibilities between the test automation engineer (with coding skills)
and test analyst (with a domain knowledge). In addition, we propose a metric to empirically
estimate cooperation between test automation engineer and test analyst's works. The
proposed automation solution has been defined based on our experience in the development
and maintenance of Helsenorg, the national electronic health services in Norway which
has had over one million of visits per month past year, and we still use it to automate
the execution of regression tests.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {749–756},
numpages = {8},
keywords = {Test automation, keyword-driven test automation, process-driven test automation, Helsenorge, DSL for test automation},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/2601248.2601264,
author = {Stevanetic, Srdjan and Zdun, Uwe},
title = {Exploring the Relationships between the Understandability of Components in Architectural Component Models and Component Level Metrics},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601264},
doi = {10.1145/2601248.2601264},
abstract = {Architectural component models represent high level designs and are frequently used
as a central view of architectural descriptions of software systems. The components
in those models represent important high level organization units that group other
components and classes in object-oriented design views. Hence, understandability of
components and their interactions plays a key role in supporting the architectural
understanding of a software system. In this paper we present a study we carried out
to examine the relationships between the effort required to understand a component,
measured through the time that participants spent on studying a component, and component
level metrics that describe component's size, complexity and coupling in terms of
the number of classes in a component and the classes' relationships. The participants
were 49 master students, and they had to fully understand the components' functionalities
in order to answer 4 true/false questions for each of the 7 components in the architecture
of the Soomla Android store system. Correlation, collinearity and multivariate regression
analysis were performed. The results of the analysis show a statistically significant
correlation between three of the metrics, number of classes, number of incoming dependencies,
and number of internal dependencies, on one side, and the effort required to understand
a component, on the other side. In a multivariate regression analysis we obtained
3 reasonably well-fitting models that can be used to estimate the effort required
to understand a component. In our future work we plan to study more components and
investigate more metrics and their relationships to the understandability of components
and architectural component models.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {32},
numpages = {10},
keywords = {software metrics, understandability, architectural component models, empirical evaluation},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1145/3468264.3473915,
author = {Kalia, Anup K. and Xiao, Jin and Krishna, Rahul and Sinha, Saurabh and Vukovic, Maja and Banerjee, Debasish},
title = {Mono2Micro: A Practical and Effective Tool for Decomposing Monolithic Java Applications to Microservices},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473915},
doi = {10.1145/3468264.3473915},
abstract = {In migrating production workloads to cloud, enterprises often face the daunting task
of evolving monolithic applications toward a microservice architecture. At IBM, we
developed a tool called Mono2Micro to assist with this challenging task. Mono2Micro
performs spatio-temporal decomposition, leveraging well-defined business use cases
and runtime call relations to create functionally cohesive partitioning of application
classes. Our preliminary evaluation of Mono2Micro showed promising results.  How well
does Mono2Micro perform against other decomposition techniques, and how do practitioners
perceive the tool? This paper describes the technical foundations of Mono2Micro and
presents results to answer these two questions. To answer the first question, we evaluated
Mono2Micro against four existing techniques on a set of open-source and proprietary
Java applications and using different metrics to assess the quality of decomposition
and tool’s efficiency. Our results show that Mono2Micro significantly outperforms
state-of-the-art baselines in specific metrics well-defined for the problem domain.
To answer the second question, we conducted a survey of twenty-one practitioners in
various industry roles who have used Mono2Micro. This study highlights several benefits
of the tool, interesting practitioner perceptions, and scope for further improvements.
Overall, these results show that Mono2Micro can provide a valuable aid to practitioners
in creating functionally cohesive and explainable microservice decompositions.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1214–1224},
numpages = {11},
keywords = {clustering, microservices, dynamic analysis},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Software Development and Research with the HADAS Toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is
a non-trivial task. Developers require energy metrics that support sustainable software
architectures reflecting quality attributes such as security, reliability, performance,
etc., identifying what are the concerns that impact more in the energy consumption.
A variability model of different designs and implementations of an energy model should
exist for this task, as well as a service that stores and compares the experimentation
results of energy and time consumption of each concern, finding out what is the most
eco-efficient solution. The experimental measurements are performed by energy experts
and researchers that share the energy model and metrics in a collaborative repository.
HADAS confronts these tasks modelling and reasoning with the variability of energy
consuming concerns for different energy contexts, connecting HADAS variability model
with its energy efficiency collaborative repository, establishing a Software Product
Line (SPL) service. Our main goal is to help developers to perform sustainability
analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit
prototype is implemented based on a Clafer model and Choco solver, and it has been
tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {software product line, optimisation, variability, clafer, CVL, repository, energy efficiency, metrics},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.5555/2821481.2821486,
author = {Van Landuyt, Dimitri and Joosen, Wouter},
title = {On the Role of Early Architectural Assumptions in Quality Attribute Scenarios: A Qualitative and Quantitative Study},
year = {2015},
publisher = {IEEE Press},
abstract = {Architectural assumptions are fundamentally different from architectural decisions
because they can not be traced directly to requirements, nor to domain, technical
or environmental constraints; they represent conditions under which the designed solution
is expected to be valid. Early architectural assumptions are similar in nature, with
the key difference that they are not made during architectural design but during requirement
elicitation, not by the software architect (a solution-oriented stakeholder), but
by the requirements engineer (a problem-oriented stakeholder). They represent initial
assumptions about the system's architecture, and allow the requirements engineer to
be more precise in documenting the requirements of the system.The role of early architectural
assumptions in the current practice of quality attribute scenario elicitation and
related development activities in the transition to architecture is unknown and under-investigated.
In this paper, we present the results of an exploratory study that focuses on the
role and nature of these assumptions in the early development stages. We studied a
reasonably large set of quality attribute scenarios for a realistic industrial case,
a smart metering system. Our study (i) confirms that quality attribute scenario elicitation
in practice does rely heavily on early architectural assumptions, and (ii) shows that
they do influence the perceived quality of the requirements body as a whole, in some
cases positively, in other cases negatively.These findings provide empirical arguments
in favor of making such assumptions explicit already during the requirements elicitation
activities. Especially in the context of iterative software development methodologies
such as the Twin Peaks model, a well-defined and -documented set of assumptions could
smoothen the transition between successive development iterations.},
booktitle = {Proceedings of the Fifth International Workshop on Twin Peaks of Requirements and Architecture},
pages = {9–15},
numpages = {7},
location = {Florence, Italy},
series = {TwinPeaks '15}
}

@inproceedings{10.1145/2745802.2745822,
author = {Stevanetic, Srdjan and Zdun, Uwe},
title = {Software Metrics for Measuring the Understandability of Architectural Structures: A Systematic Mapping Study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745822},
doi = {10.1145/2745802.2745822},
abstract = {The main idea of software architecture is to concentrate on the "big picture" of a
software system. In the context of object-oriented software systems higher-level architectural
structures or views above the level of classes are frequently used to capture the
"big picture" of the system. One of the critical aspects of these higher-level views
is understandability, as one of their main purposes is to enable designers to abstract
away fine-grained details. In this article we present a systematic mapping study on
software metrics related to the understandability concepts of such higher-level software
structures with regard to their relations to the system implementation. In our systematic
mapping study, we started from 3951 studies obtained using an electronic search in
the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our
inclusion/exclusion criteria as well as the snowballing technique we selected 268
studies for in-depth study. From those, we selected 25 studies that contain relevant
metrics. We classify the identified studies and metrics with regard to the measured
artefacts, attributes, quality characteristics, and representation model used for
the metrics definitions. Additionally, we present the assessment of the maturity level
of the identified studies. Overall, there is a lack of maturity in the studies. We
discuss possible techniques how to mitigate the identified problems. From the academic
point of view we believe that our study is a good starting point for future studies
aiming at improving the existing works. From a practitioner's point of view, the results
of our study can be used as a catalogue and an indication of the maturity of the existing
research results.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {21},
numpages = {14},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1145/2949550.2949652,
author = {Hu, Hao and Hong, Xingchen and Terstriep, Jeff and Liu, Yan Y. and Finn, Michael P. and Rush, Johnathan and Wendel, Jeffrey and Wang, Shaowen},
title = {TopoLens: Building a CyberGIS Community Data Service for Enhancing the Usability of High-Resolution National Topographic Datasets},
year = {2016},
isbn = {9781450347556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2949550.2949652},
doi = {10.1145/2949550.2949652},
abstract = {Geospatial data, often embedded with geographic references, are important to many
application and science domains, and represent a major type of big data. The increased
volume and diversity of geospatial data have caused serious usability issues for researchers
in various scientific domains, which call for innovative cyberGIS solutions. To address
these issues, this paper describes a cyberGIS community data service framework to
facilitate geospatial big data access, processing, and sharing based on a hybrid supercomputer
architecture. Through the collaboration between the CyberGIS Center at the University
of Illinois at Urbana-Champaign (UIUC) and the U.S. Geological Survey (USGS), a community
data service for accessing, customizing, and sharing digital elevation model (DEM)
and its derived datasets from the 10-meter national elevation dataset, namely TopoLens,
is created to demonstrate the workflow integration of geospatial big data sources,
computation, analysis needed for customizing the original dataset for end user needs,
and a friendly online user environment. TopoLens provides online access to precomputed
and on-demand computed high-resolution elevation data by exploiting the ROGER supercomputer.
The usability of this prototype service has been acknowledged in community evaluation.},
booktitle = {Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},
articleno = {39},
numpages = {8},
keywords = {web-based gateway environment, CyberGIS, microservices, geospatial big data, elevation data, data sharing},
location = {Miami, USA},
series = {XSEDE16}
}

@article{10.1145/3377138,
author = {Wu, Hao and Liu, Weizhi and Lin, Huanxin and Wang, Cho-Li},
title = {A Model-Based Software Solution for Simultaneous Multiple Kernels on GPUs},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3377138},
doi = {10.1145/3377138},
abstract = {As a critical computing resource in multiuser systems such as supercomputers, data
centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking
is an intuitive solution to underutilization in GPGPU computing. Recently proposed
solutions of multitasking GPUs can be classified into two categories: (1) spatially
partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of
compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels
simultaneously within a CU. Compared to SPS, SMK can improve resource utilization
even further due to the interleaving of instructions from kernels with low dynamic
resource contentions.However, it is hard to implement SMK on current GPU architecture,
because (1) techniques for applying SMK on top of GPU hardware scheduling policy are
scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences
of concurrently executed kernels. In this article, we propose a lightweight and effective
performance model to evaluate the complex interferences of SMK. Based on the probability
of independent events, our performance model is built from a totally new angle and
contains limited parameters. Then, we propose a metric, symbiotic factor, which can
evaluate an SMK scheme so that kernels with complementary resource utilization can
corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing
and kernel stretching techniques and integrate them to apply SMK on GPUs instead of
simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based
concurrent kernel execution whose kernel launching order brings fast execution time,
the results of corunning kernel pairs show 11%, 18%, and 12% speedup on AMD R9 290X,
RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the
results show 29%, 18%, and 51% speedup on AMD R9 290X, RX 480, and Vega 64, respectively,
on average.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {7},
numpages = {26},
keywords = {concurrent kernel execution, GPGPU}
}

@inproceedings{10.1145/3209978.3210005,
author = {Mohammad, Hafeezul Rahman and Xu, Keyang and Callan, Jamie and Culpepper, J. Shane},
title = {Dynamic Shard Cutoff Prediction for Selective Search},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210005},
doi = {10.1145/3209978.3210005},
abstract = {Selective search architectures use resource selection algorithms such as Rank-S or
Taily to rank index shards and determine how many to search for a given query. Most
prior research evaluated solutions by their ability to improve efficiency without
significantly reducing early-precision metrics such as P@5 and NDCG@10. This paper
recasts selective search as an early stage of a multi-stage retrieval architecture,
which makes recall-oriented metrics more appropriate. A new algorithm is presented
that predicts the number of shards that must be searched for a given query in order
to meet recall-oriented goals. Decoupling shard ranking from deciding how many shards
to search clarifies efficiency vs. effectiveness trade-offs, and enables them to be
optimized independently. Experiments on two corpora demonstrate the value of this
approach.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {85–94},
numpages = {10},
keywords = {resource selection, distributed search, selective search},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3204949.3204956,
author = {Mangla, Tarun and Zegura, Ellen and Ammar, Mostafa and Halepovic, Emir and Hwang, Kyung-Wook and Jana, Rittwik and Platania, Marco},
title = {VideoNOC: Assessing Video QoE for Network Operators Using Passive Measurements},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204956},
doi = {10.1145/3204949.3204956},
abstract = {Video streaming traffic is rapidly growing in mobile networks. Mobile Network Operators
(MNOs) are expected to keep up with this growing demand, while maintaining a high
video Quality of Experience (QoE). This makes it critical for MNOs to have a solid
understanding of users' video QoE with a goal to help with network planning, provisioning
and traffic management. However, designing a system to measure video QoE has several
challenges: i) large scale of video traffic data and diversity of video streaming
services, ii) cross-layer constraints due to complex cellular network architecture,
and iii) extracting QoE metrics from network traffic. In this paper, we present VideoNOC,
a prototype of a flexible and scalable platform to infer objective video QoE metrics
(e.g., bitrate, rebuffering) for MNOs. We describe the design and architecture of
VideoNOC, and outline the methodology to generate a novel data source for fine-grained
video QoE monitoring. We then demonstrate some of the use cases of such a monitoring
system. VideoNOC reveals video demand across the entire network, provides valuable
insights on a number of design choices by content providers (e.g., OS-dependent performance,
video player parameters like buffer size, range of encoding bitrates, etc.) and helps
analyze the impact of network conditions on video QoE (e.g., mobility and high demand).},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {101–112},
numpages = {12},
keywords = {video streaming, passive measurement, cellular network, QoE},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/3167486.3167534,
author = {Seraoui, Youssef and Belmekki, Mostafa and Bellafkih, Mostafa and Raouyane, Brahim},
title = {ETOM Mapping onto NFV Framework: IMS Use Case},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167534},
doi = {10.1145/3167486.3167534},
abstract = {Telecom professionals have a strong interest in the proposition and adaptation of
innovate network management models and frameworks to help mobile network operators
(MNOs) to improve their business processes and get more agile in the telecoms industry
that evolves with great speed. The model being established by the TeleManagement Forum
(TM Forum) is the Enhanced Telecom Operations MAP (eTOM) business process framework
on which we rely in this work to propose a mapping of the eTOM model onto the network
functions virtualization (NFV) framework with the projection of this function mapping
onto the IP Multimedia Subsystem (IMS) use case. This mapping covers essentially four
main components playing important rules in the MNO's business processes, including
customers, services, infrastructure resources, and also service providers. The main
goal, thereby, is to design a combined architecture in a virtualized environment for
dynamic delivery of services with quality of service (QoS) and improved resource performance
so as to meet the purposes of the 5G network in terms of a proposed, virtual telecom
environment managed and orchestrated by the conjunction of the aforementioned paradigms.
Indeed, we conducted simulations to evaluate part of this function mapping in an IMS
setting for static service chain provisioning. Thus, results showed possible provisioning
of services in this context in measuring SIP related key performance indicators and
performance metrics. Results showed the feasibility of our approach. In addition,
resource performance improved obviously in the NFV context in accordance with eTOM
business processes.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {45},
numpages = {8},
keywords = {New Generation Operations Systems and Software (NGOSS), service chain provisioning, network functions virtualization (NFV), IP Multimedia Subsystem (IMS), resource performance, Enhanced Telecom Operations Map (eTOM), five generation (5G), Business process, quality of service (QoS)},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@inproceedings{10.1145/3286685.3286686,
author = {Saurez, Enrique and Balasubramanian, Bharath and Schlichting, Richard and Tschaen, Brendan and Huang, Zhe and Narayanan, Shankaranarayanan Puzhavakath and Ramachandran, Umakishore},
title = {METRIC: A Middleware for Entry Transactional Database Clustering at the Edge},
year = {2018},
isbn = {9781450361170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286685.3286686},
doi = {10.1145/3286685.3286686},
abstract = {A geo-distributed database for edge architectures spanning thousands of sites needs
to assure efficient local updates while replicating sufficient state across sites
to enable global management and support mobility, failover etc. To address this requirement,
a new paradigm for database clustering that achieves a better balance than existing
solutions between performance and strength of semantics called entry transactionality
is introduced. Inspired by entry consistency in shared memory systems, entry transactionality
guarantees that only a client that owns a range of keys in the database has a sequentially
consistent value of the keys and can perform local and, hence, efficient transactions
across these keys. Important use cases enabled by entry transactionality such as federated
controllers and state management for edge applications are identified. The semantics
of entry transactionality incorporating the complex failure modes in geo-distributed
services are defined, and the difficult challenges in realizing these semantics are
outlined. Then, a novel Middleware for Entry Transactional Clustering (METRIC) that
combines existing SQL databases with an underlying geo-distributed entry consistent
store to realize entry transactionality is described. This paper provides initial
findings from an on-going effort.},
booktitle = {Proceedings of the 3rd Workshop on Middleware for Edge Clouds &amp; Cloudlets},
pages = {2–7},
numpages = {6},
location = {Rennes, France},
series = {MECC'18}
}

@inproceedings{10.1145/3130218.3130225,
author = {Wang, Zicong and Chen, Xiaowen and Li, Chen and Guo, Yang},
title = {Fairness-Oriented and Location-Aware NUCA for Many-Core SoC},
year = {2017},
isbn = {9781450349840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3130218.3130225},
doi = {10.1145/3130218.3130225},
abstract = {Non-uniform cache architecture (NUCA) is often employed to organize the last level
cache (LLC) by Networks-on-Chip (NoC). However, along with the scaling up for network
size of Systems-on-Chip (SoC), two trends gradually begin to emerge. First, the network
latency is becoming the major source of the cache access latency. Second, the communication
distance and latency gap between different cores is increasing. Such gap can seriously
cause the network latency imbalance problem, aggravate the degree of non-uniform for
cache access latencies, and then worsen the system performance.In this paper, we propose
a novel NUCA-based scheme, named fairness-oriented and location-aware NUCA (FL-NUCA),
to alleviate the network latency imbalance problem and achieve more uniform cache
access. We strive to equalize network latencies which are measured by three metrics:
average latency (AL), latency standard deviation (LSD), and maximum latency (ML).
In FL-NUCA, the memory-to-LLC mapping and links are both non-uniform distributed to
better fit the network topology and traffics, thereby equalizing network latencies
from two aspects, i.e., non-contention latencies and contention latencies, respectively.
The experimental results show that FL-NUCA can effectively improve the fairness of
network latencies. Compared with the traditional static NUCA (S-NUCA), in simulation
with synthetic traffics, the average improvements for AL, LSD, and ML are 20.9%, 36.3%,
and 35.0%, respectively. In simulation with PARSEC benchmarks, the average improvements
for AL, LSD, and ML are 6.3%, 3.6%, and 11.2%, respectively.},
booktitle = {Proceedings of the Eleventh IEEE/ACM International Symposium on Networks-on-Chip},
articleno = {13},
numpages = {8},
keywords = {Networks-on-chip, non-uniform cache architecture, memory mapping},
location = {Seoul, Republic of Korea},
series = {NOCS '17}
}

@inproceedings{10.1145/3323716.3323729,
author = {Berba, Elizalde M. and Palaoag, Thelma D.},
title = {Improving Customer Satisfaction on Internet Services in L-NU Using Virtualized AAA Network Architecture},
year = {2019},
isbn = {9781450361040},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323716.3323729},
doi = {10.1145/3323716.3323729},
abstract = {This study mainly aims to improve the satisfaction level on internet services in Lyceum-Northwestern
University (L-NU). From a traditional network architecture, the researchers made use
of a virtualized Authentication, Authorization and Accounting (AAA) network architecture
to improve the internet services provided to the students of L-NU. For the methodology
of this study, the researchers had to make use of a network lifecycle called Prepare,
Plan, Design, Implement, Operate, and Optimize (PPDIOO) and there was also a need
to combine both quantitative and qualitative research approach. To make this happen,
the researchers had to fulfill the following objectives: a) determine the current
network setup of L-NU, b) measure the current satisfaction level of the users, c)
design, develop and implement a virtualized AAA network architecture, d) measure the
satisfaction level of the users who have used the AAA network setup, and e) compare
the measured satisfaction level from the users who used the internet facilities using
the traditional network architecture and satisfaction level from users who have used
the internet facilities after the implementation of AAA network architecture. As a
result, it was found out that the implementation of the new network architecture has
significantly improved the internet service level of L-NU which is reflected by a
higher customer satisfaction rating. Therefore, the researchers conclude that it is
most essential that AAA network architecture be implemented to enterprise type of
network setup such as but not limited to education institutions in managing their
internet services. Consequently, this kind of network architecture lead to a more
effective and more efficient way of managing network resources of an institution or
an organization while further improving the satisfaction level. In order to optimize
the AAA network architecture and gain more implementation advantages, virtualization
technology was used to contain and run numerous operating system instances such as
four physical servers into one single physical server which favors to saving resources
such as energy, space, money and of which also leads to simplified administration.},
booktitle = {Proceedings of the 8th International Conference on Informatics, Environment, Energy and Applications},
pages = {178–183},
numpages = {6},
keywords = {authorization, AAA, authentication, hypervisor, accounting, customer satisfaction, PPDIOO, virtualization},
location = {Osaka, Japan},
series = {IEEA '19}
}

@inproceedings{10.1145/2619239.2631456,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
title = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2631456},
doi = {10.1145/2619239.2631456},
abstract = {After more than two decades of evolution, TCP and its end host based modifications
can still suffer from severely degraded performance under real-world challenging network
conditions. The reason, as we observe, is due to TCP family's fundamental architectural
deficiency, which hardwires packet-level events to control responses and ignores emprical
performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented
Congestion Control (PCC), a new congestion control architecture in which each sender
controls its sending strategy based on empirically observed performance metrics. We
show through preliminary experimental results that PCC achieves consistently high
performance under various challenging network conditions.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {365–366},
numpages = {2},
keywords = {congestion control},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@article{10.1145/2740070.2631456,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
title = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2631456},
doi = {10.1145/2740070.2631456},
abstract = {After more than two decades of evolution, TCP and its end host based modifications
can still suffer from severely degraded performance under real-world challenging network
conditions. The reason, as we observe, is due to TCP family's fundamental architectural
deficiency, which hardwires packet-level events to control responses and ignores emprical
performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented
Congestion Control (PCC), a new congestion control architecture in which each sender
controls its sending strategy based on empirically observed performance metrics. We
show through preliminary experimental results that PCC achieves consistently high
performance under various challenging network conditions.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {365–366},
numpages = {2},
keywords = {congestion control}
}

@inproceedings{10.1145/2568088.2568098,
author = {Ewing, John M. and Menasc\'{e}, Daniel A.},
title = {A Meta-Controller Method for Improving Run-Time Self-Architecting in SOA Systems},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2568098},
doi = {10.1145/2568088.2568098},
abstract = {This paper builds on SASSY, a system for automatically generating SOA software architectures
that optimize a given utility function of multiple QoS metrics. In SASSY, SOA software
systems are automatically re-architected when services fail or degrade. Optimizing
both architecture and service provider selection presents a pair of nested NP-hard
problems. Here we adapt hill-climbing, beam search, simulated annealing, and evolutionary
programming to both architecture optimization and service provider selection. Each
of these techniques has several parameters that influence their efficiency. We introduce
in this paper a meta-controller that automates the run-time selection of heuristic
search techniques and their parameters. We examine two different meta-controller implementations
that each use online learning. The first implementation identifies the best heuristic
search combination from various prepared combinations. The second implementation analyzes
the current self-architecting problem (e.g. changes in performance metrics, service
degradations/failures) and looks for similar, previously encountered re-architecting
problems to find an effective heuristic search combination for the current problem.
A large set of experiments demonstrates the effectiveness of the first meta-controller
implementation and indicates opportunities for improving the second meta-controller
implementation.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {173–184},
numpages = {12},
keywords = {heuristic search, meta-controlled qos optimization, autonomic computing, soa, combinatorial search techniques, metaheuristics, automated run-time software architecting},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1109/CCGrid.2015.152,
author = {Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
title = {Modeling Cross-Architecture Co-Tenancy Performance Interference},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.152},
doi = {10.1109/CCGrid.2015.152},
abstract = {Cloud computing has become a dominant computing paradigm to provide elastic, affordable
computing resources to end users. Due to the increased computing power of modern machines
powered by multi/many-core computing, data centers often co-locate multiple virtual
machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing
and competition. Applications or VMs co-locating in one physical machine can interfere
with each other despite of the promise of performance isolation through virtualization.
Modeling and predicting co-run interference therefore becomes critical for data center
job scheduling and QoS (Quality of Service) assurance. Co-run interference can be
categorized into two metrics, sensitivity and pressure, where the former denotes how
an application's performance is affected by its co-run applications, and the latter
measures how it impacts the performance of its co-run applications. This paper shows
that sensitivity and pressure are both application- and architecture-dependent. Further,
we propose a regression model that predicts an application's sensitivity and pressure
across architectures with high accuracy. This regression model enables a data center
scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate
with another VMs/applications.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {231–240},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3286062.3286086,
author = {Sharma, Puneet and Raghuramu, Arun and Lee, David and Saxena, Vinay and Chuah, Chen-Nee},
title = {We Don't Need No Licensing Server},
year = {2018},
isbn = {9781450361200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286062.3286086},
doi = {10.1145/3286062.3286086},
abstract = {Cloudification of edge to core infrastructure has led to new and rich application
and service deployment and operational models. These ecosystems have complex relationships
between the application vendors, infrastructure operators and application users. Traditional
licensing and compliance enforcement methods such as those based on in person audits
and dynamic issuing of license keys inhibit the resource provisioning and consumption
flexibility offered by cloudified services due to scalability and management overheads.
In this work, we argue the need for a trusted framework for application usage rights
compliance. This new architecture named "Metered Boot" provides a way to realize trusted,
capacity/usage based rights compliance for service deployments that allows decoupling
of usage rights governed by application vendors from the resource provisioning by
the infrastructure provider. We have built a Metered Boot prototype for a particular
usecase of NFV usage rights compliance.},
booktitle = {Proceedings of the 17th ACM Workshop on Hot Topics in Networks},
pages = {162–168},
numpages = {7},
location = {Redmond, WA, USA},
series = {HotNets '18}
}

@inproceedings{10.1109/ICSE-NIER.2019.00037,
author = {Aniche, Maur\'{\i}cio and Yoder, Joseph W. and Kon, Fabio},
title = {Current Challenges in Practical Object-Oriented Software Design},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00037},
doi = {10.1109/ICSE-NIER.2019.00037},
abstract = {According to the extensive 50-year-old body of knowledge in object-oriented programming
and design, good software designs are, among other characteristics, lowly coupled,
highly cohesive, extensible, comprehensible, and not fragile. However, with the increased
complexity and heterogeneity of contemporary software, this might not be enough.This
paper discusses the practical challenges of object-oriented design in modern software
development. We focus on three main challenges: (1) how technologies, frameworks,
and architectures pressure developers to make design decisions that they would not
take in an ideal scenario, (2) the complexity of current real-world problems require
developers to devise not only a single, but several models for the same problem that
live and interact together, and (3) how existing quality assessment techniques for
object-oriented design should go beyond high-level metrics.Finally, we propose an
agenda for future research that should be tackled by both scientists and practitioners
soon. This paper is a call for arms for more reality-oriented research on the object-oriented
software design field.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {113–116},
numpages = {4},
keywords = {domain modeling, software architecture, class design, object-oriented programming, software engineering, object-oriented design, software design},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/3176258.3176328,
author = {Alshehri, Asma and Benson, James and Patwa, Farhan and Sandhu, Ravi},
title = {Access Control Model for Virtual Objects (Shadows) Communication for AWS Internet of Things},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176328},
doi = {10.1145/3176258.3176328},
abstract = {The concept of Internet of Things (IoT) has received considerable attention and development
in recent years. There have been significant studies on access control models for
IoT in academia, while companies have already deployed several cloud-enabled IoT platforms.
However, there is no consensus on a formal access control model for cloud-enabled
IoT. The access-control oriented (ACO) architecture was recently proposed for cloud-enabled
IoT, with virtual objects (VOs) and cloud services in the middle layers. Building
upon ACO, operational and administrative access control models have been published
for virtual object communication in cloud-enabled IoT illustrated by a use case of
sensing speeding cars as a running example.In this paper, we study AWS IoT as a major
commercial cloud-IoT platform and investigate its suitability for implementing the
afore-mentioned academic models of ACO and VO communication control. While AWS IoT
has a notion of digital shadows closely analogous to VOs, it lacks explicit capability
for VO communication and thereby for VO communication control. Thus there is a significant
mismatch between AWS IoT and these academic models. The principal contribution of
this paper is to reconcile this mismatch by showing how to use the mechanisms of AWS
IoT to effectively implement VO communication models. To this end, we develop an access
control model for virtual objects (shadows) communication in AWS IoT called AWS-IoT-ACMVO.
We develop a proof-of-concept implementation of the speeding cars use case in AWS
IoT under guidance of this model, and provide selected performance measurements. We
conclude with a discussion of possible alternate implementations of this use case
in AWS IoT.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {175–185},
numpages = {11},
keywords = {devices, abac, security, iot architecture, acl, aws iot, internet of things (iot), access control, virtual objects, rbac},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1145/3453688.3461512,
author = {Gao, Chengsi and Li, Bing and Wang, Ying and Chen, Weiwei and Zhang, Lei},
title = {Tenet: A Neural Network Model Extraction Attack in Multi-Core Architecture},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453688.3461512},
doi = {10.1145/3453688.3461512},
abstract = {As neural networks (NNs) are being widely deployed in many cloud-oriented systems
for safety-critical tasks, the privacy and security of NNs become significant concerns
to users in the cloud platform that shares the computation infrastructure such as
memory resource. In this work, we observed that the memory timing channel in the shared
memory of cloud multi-core architecture poses the risk of network model information
leakage. Based on the observation, we propose a learning-based method to steal the
model architecture of the NNs by exploiting the memory timing channel without any
high-level privilege or physical access. We first trained an end-to-end measurement
network offline to learn the relation between memory timing information and NNs model
architecture. Then, we performed an online attack and reconstructed the target model
using the prediction from the measurement network. We evaluated the proposed attack
method on a multi-core architecture simulator. The experimental results show that
our learning-based attack method can reconstruct the target model with high accuracy
and improve the adversarial attack success rate by 42.4%.},
booktitle = {Proceedings of the 2021 on Great Lakes Symposium on VLSI},
pages = {21–26},
numpages = {6},
keywords = {deep learning security, multi-core, machine learning, memory timing channel},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@inproceedings{10.1145/2661714.2661726,
author = {Stohr, Denny and Wilk, Stefan and Effelsberg, Wolfgang},
title = {Monitoring of User Generated Video Broadcasting Services},
year = {2014},
isbn = {9781450331579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661714.2661726},
doi = {10.1145/2661714.2661726},
abstract = {Mobile video broadcasting services offer users the opportunity to instantly share
content from their mobile handhelds to a large audience over the Internet. However,
existing data caps in cellular network contracts and limitations in their upload capabilities
restrict the adoption of mobile video broadcasting services. Additionally, the quality
of those video streams is often reduced by the lack of skills of recording users and
the technical limitations of the video capturing devices. Our research focuses on
large-scale events that attract dozens of users to record video in parallel. In many
cases, available network infrastructure is not capable to upload all video streams
in parallel. To make decisions on how to appropriately transmit those video streams,
a suitable monitoring of the video generation process is required. For this scenario,
a measurement framework is proposed that allows Internet-scale mobile broadcasting
services to deliver samples in an optimized way. Our framework architecture analyzes
three zones for effectively monitoring user-generated video. Besides classical Quality
of Service metrics on the network state, video quality indicators and additional auxiliary
sensor information is gathered. Aim of this framework is an efficient coordination
of devices and their uploads based on the currently observed system state.},
booktitle = {Proceedings of the First International Workshop on Internet-Scale Multimedia Management},
pages = {39–42},
numpages = {4},
keywords = {cellular networks, mobile, mix, video broadcast, network monitoring, video composition, measurement},
location = {Orlando, Florida, USA},
series = {WISMM '14}
}

@article{10.1145/2983637,
author = {Miao, Wang and Min, Geyong and Wu, Yulei and Wang, Haozhe and Hu, Jia},
title = {Performance Modelling and Analysis of Software-Defined Networking under Bursty Multimedia Traffic},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2983637},
doi = {10.1145/2983637},
abstract = {Software-Defined Networking (SDN) is an emerging architecture for the next-generation
Internet, providing unprecedented network programmability to handle the explosive
growth of big data driven by the popularisation of smart mobile devices and the pervasiveness
of content-rich multimedia applications. In order to quantitatively investigate the
performance characteristics of SDN networks, several research efforts from both simulation
experiments and analytical modelling have been reported in the current literature.
Among those studies, analytical modelling has demonstrated its superiority in terms
of cost-effectiveness in the evaluation of large-scale networks. However, for analytical
tractability and simplification, existing analytical models are derived based on the
unrealistic assumptions that the network traffic follows the Poisson process, which
is suitable to model nonbursty text data, and the data plane of SDN is modelled by
one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies
have shown that, due to the features of heavy volume and high velocity, the multimedia
big data generated by real-world multimedia applications reveals the bursty and correlated
nature in the network transmission. With the aim of capturing such features of realistic
traffic patterns and obtaining a comprehensive and deeper understanding of the performance
behaviour of SDN networks, this article presents a new analytical model to investigate
the performance of SDN in the presence of the bursty and correlated arrivals modelled
by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance
metrics in terms of the average latency and average network throughput of the SDN
networks are derived based on the developed analytical model. To consider a realistic
multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted
to model the SDN data plane. To address the challenging problem of obtaining the key
performance metrics, for example, queue-length distribution of a PQ system with a
given service capacity, a versatile methodology extending the Empty Buffer Approximation
(EBA) method is proposed to facilitate the decomposition of such a PQ system to two
SSSQ systems. The validity of the proposed model is demonstrated through extensive
simulation experiments. To illustrate its application, the developed model is then
utilised to study the strategy of the network configuration and resource allocation
in SDN networks.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = sep,
articleno = {77},
numpages = {19},
keywords = {performance modelling and analysis, Software-defined networking, multimedia big data, queueing decomposition, resource allocation}
}

@inproceedings{10.1145/3437120.3437292,
author = {Maikantis, Theodoros and Tsintzira, Angeliki-Agathi and Ampatzoglou, Apostolos and Arvanitou, Elvira-Maria and Chatzigeorgiou, Alexander and Stamelos, Ioannis and Bibi, Stamatia and Deligiannis, Ignatios},
title = {Software Architecture Reconstruction via a Genetic Algorithm: Applying the Move Class Refactoring},
year = {2020},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437292},
doi = {10.1145/3437120.3437292},
abstract = {Modularity is one of the four key principles of software design and architecture.
According to this principle, software should be organized into modules that are tightly
linked internally (high cohesion), whereas at the same time as independent from other
modules as possible (low coupling). However, in practice, this principle is violated
due to poor architecting design decisions, lack of time, or coding shortcuts, leading
to a phenomenon termed as architectural technical debt (ATD). To alleviate this problem
(lack of architectural modularity), the most common solution is the application of
a software refactoring, namely Move Class—i.e., moving classes (the core artifact
in object-oriented systems) from one module to another. To identify Move Class refactoring
opportunities, we employ a search-based optimization process, relying on optimization
metrics, through which optimal moves are derived. Given the extensive search space
required for applying a brute-force search strategy, in this paper, we propose the
use of a genetic algorithm that re-arranges existing software classes into existing
or new modules (software packages in Java, or folders in C++). To validate the usefulness
of the proposed refactorings, we performed an industrial case study on three projects
(from the Aviation, Healthcare, and Manufacturing application domains). The results
of the study indicate that the proposed architecture reconstruction is able to improve
modularity, improving both coupling and cohesion. The obtained results can be useful
to practitioners through an open source tool; whereas at the same point, they open
interesting future work directions.},
booktitle = {24th Pan-Hellenic Conference on Informatics},
pages = {135–139},
numpages = {5},
location = {Athens, Greece},
series = {PCI 2020}
}

@inproceedings{10.1145/3405656.3418718,
author = {G\"{u}ndo\u{g}an, Cenk and Ams\"{u}ss, Christian and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
title = {Toward a RESTful Information-Centric Web of Things: A Deeper Look at Data Orientation in CoAP},
year = {2020},
isbn = {9781450380409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405656.3418718},
doi = {10.1145/3405656.3418718},
abstract = {The information-centric networking (ICN) paradigm offers replication of autonomously
verifiable content throughout a network, in which content is bound to names instead
of hosts. This has proven beneficial in particular for the constrained IoT. Several
approaches, the most prominent of which being Named Data Networking, propose access
to named content directly on the network layer. Independently, the IETF CoAP protocol
group started to develop mechanisms that support autonomous content processing and
in-network storage.In this paper, we explore the emerging CoAP protocol building blocks
and how they contribute to an information-centric network architecture for a data-oriented
RESTful Web of Things. We discuss design options and measure characteristic performances
of different network configurations, which deploy CoAP proxies and OSCORE content
object security, and compare with NDN. Our findings indicate an almost continuous
design space ranging from plain CoAP at the one end to NDN on the other. On both ends---ICN
and CoAP---we identify protocol features and aspects whose mutual transfer potentially
improves design and operation of the other.},
booktitle = {Proceedings of the 7th ACM Conference on Information-Centric Networking},
pages = {77–88},
numpages = {12},
keywords = {Internet of Things, ICN, protocol evaluation, content object security, CoAP Proxy, OSCORE},
location = {Virtual Event, Canada},
series = {ICN '20}
}

@article{10.1145/3151123.3151125,
author = {Zeinalipour-Yazti, Demetrios and Laoudias, Christos},
title = {The Anatomy of the Anyplace Indoor Navigation Service},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3151123.3151125},
doi = {10.1145/3151123.3151125},
abstract = {The pervasiveness of smartphones is leading to the uptake of a new class of Internet-based
Indoor Navigation (IIN) services, which might soon diminish the need of Satellite-based
localization technologies in urban environments. These services rely on geo-location
databases that store spatial models along with wireless, light and magnetic signals
used to localize users and provide better power efficiency and wider coverage than
predominant approaches. In this article we overview Anyplace, an open, modular, extensible
and scalable navigation architecture that exploits crowdsourced Wi-Fi data to develop
a novel navigation service that won several international research awards for its
utility and accuracy (i.e., less than 2 meters). Our MIT-licenced open-source software
stack has to this date been used by thaousands of researchers and practitioners around
the globe, with the public Anyplace service reaching over 100,000 real user interactions.},
journal = {SIGSPATIAL Special},
month = oct,
pages = {3–10},
numpages = {8}
}

@inproceedings{10.1145/3379310.3379320,
author = {Lumba, Ester and Waworuntu, Alexander},
title = {Application of Lecturer Performance Report in Indonesia with Model View Controller (MVC) Architecture},
year = {2020},
isbn = {9781450376853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379310.3379320},
doi = {10.1145/3379310.3379320},
abstract = {Lecturers in Indonesia have a fundamental obligation to conduct Tri Dharma activities
consisting of teaching, research and community service. Most higher education institutions
use Tri Dharma as a measure of lecturer's performance. In addition, lecturer activity
data related to Tri Dharma is needed by the head of study program and department related
to research, publication and community service to be stored which will be used as
a source of data during the accreditation process. This paper discusses the application
development of lecturer performance reports using the Model View Controller (MVC)
architecture with Java programming language. The result is a desktop-based application
that will be used by the head of the study program and the lecturers.},
booktitle = {Proceedings of the 2020 2nd Asia Pacific Information Technology Conference},
pages = {23–28},
numpages = {6},
keywords = {desktop-based application, application development, MVC architecture, Indonesia higher-education},
location = {Bali Island, Indonesia},
series = {APIT 2020}
}

@inproceedings{10.1145/3338466.3358916,
author = {Alder, Fritz and Asokan, N. and Kurnikov, Arseny and Paverd, Andrew and Steiner, Michael},
title = {S-FaaS: Trustworthy and Accountable Function-as-a-Service Using Intel SGX},
year = {2019},
isbn = {9781450368261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338466.3358916},
doi = {10.1145/3338466.3358916},
abstract = {Function-as-a-Service (FaaS) is a recent and popular cloud computing paradigm in which
the function provider specifies a function to be run and is billed only for the computational
resources used by that function. Compared to other cloud paradigms, FaaS requires
significantly more fine-grained measurement of functions' compute time and memory
usage. Since functions are short and stateless, small ephemeral entities (e.g. individuals
or underutilized data centers) can become FaaS service providers. However, this exacerbates
the already substantial challenges of 1) ensuring integrity of computation, 2) minimizing
information revealed to the service provider, and 3) accurately measuring computational
resource usage.To address these challenges, we introduce S-FaaS, the first architecture
and implementation of FaaS to provide strong security and accountability guarantees
using Intel SGX. To match the dynamic event-driven nature of FaaS, we introduce a
new key distribution enclave and a novel transitive attestation protocol. A core contribution
of S-FaaS is our set of reusable resource measurement mechanisms that securely measure
compute time and memory usage inside an enclave. We have integrated S-FaaS into the
OpenWhisk FaaS framework and provide this as open source software.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
pages = {185–199},
numpages = {15},
keywords = {intel sgx, function-as-a-service, resource measurement},
location = {London, United Kingdom},
series = {CCSW'19}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-Grained Test Case Prioritization for Integration Testing of Delta-Oriented Software Product Lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = { Software product line (SPL) testing is a challenging task, due to the huge number
of variants sharing common functionalities to be taken into account for efficient
testing. By adopting the concept of regression testing, incremental SPL testing strategies
cope with this challenge by exploiting the reuse potential of test artifacts between
subsequent variants under test. In previous work, we proposed delta-oriented test
case prioritization for incremental SPL integration testing, where differences between
architecture test model variants allow for reasoning about the order of reusable test
cases to be executed. However, the prioritization left two issues open, namely (1)
changes to component behavior are ignored, which may also influence component interactions
and, (2) the weighting and ordering of similar test cases result in an unintended
clustering of test cases. In this paper, we extend the test case prioritization technique
by (1) incorporating changes to component behavior allowing for a more fine-grained
analysis and (2) defining a dissimilarity measure to avoid clustered test case orders.
We prototyped our test case prioritization technique and evaluated its applicability
and effectiveness by means of a case study from the automotive domain showing positive
results. },
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3318265.3318280,
author = {Hang, Zijun and Shi, Yang and Wen, Mei and Quan, Wei and Zhang, Chunyuan},
title = {SWAP: A Sliding Window Algorithm for in-Network Packet Measurement},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318280},
doi = {10.1145/3318265.3318280},
abstract = {Network traffic measurement is a fundamental part of many network applications, such
as DDOS detection, capacity planning, and quality-of-service improvement. To achieve
this, we need to count the number of packets passed during a past time interval. Traditionally,
switches sample the packets and send them to the CPU for analysis. It is unavoidable
that the sampling will sacrifice the measuring accuracy. Nowadays, programmable switches
can keep the counters in the data plane. However, they still rely on the CPU to drain
and clear the records periodically, which brings in too much communication latency.
To overcome these disadvantages, we propose a metering mechanism under the RMT architectural
model called SWAP. SWAP is carefully designed to count the number of packets during
an interval accurately with little hardware resource usage. We prototype it using
P4 and simulation results show SWAP achieves high efficiency and moderate accuracy
at line speed.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {84–89},
numpages = {6},
keywords = {P4, programmable switches, network algorithm, software-defined networks},
location = {Xi'an, China},
series = {HP3C '19}
}

@inproceedings{10.1145/3320326.3320391,
author = {El Mrabet, Zakaria and Ezzari, Mehdi and Elghazi, Hassan and El Majd, Badr Abou},
title = {Deep Learning-Based Intrusion Detection System for Advanced Metering Infrastructure},
year = {2019},
isbn = {9781450366458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320326.3320391},
doi = {10.1145/3320326.3320391},
abstract = {Smart grid is an alternative solution of the conventional power grid which harnesses
the power of the information technology to save the energy and meet todays' environment
requirements. Due to the inherent vulnerabilities in the information technology, the
smart grid is exposed to wide variety of threats that could be translated into cyber-attacks.
In this paper, we develop a deep learning-based intrusion detection system to defend
against cyber-attacks in the advanced metering infrastructure network. The proposed
machine learning approach is trained and tested extensively on an empirical industrial
dataset which is composed of several attack' categories including the scanning, buffer
overflow, and denial of service attacks. Then, an experimental comparison in terms
of detection accuracy is conducted to evaluate the performance of the proposed approach
with Na\"{\i}ve Bayes, Support Vector Machine, and Random Forest. The obtained results
suggest that the proposed approaches produce optimal results comparing to the other
algorithms. Finally, we propose a network architecture to deploy the proposed anomaly-based
intrusion detection system across the Advanced metering infrastructure network. In
addition, we propose a network security architecture composed of two types of Intrusion
detection system types, Host and Network based, deployed across the Advanced Metering
Infrastructure network to inspect the traffic and detect the malicious one at all
the levels.},
booktitle = {Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security},
articleno = {58},
numpages = {7},
keywords = {Deep learning, Intrusion detection system, Advanced Metering Infrastructure, cross entropy loss, detection accuracy},
location = {Rabat, Morocco},
series = {NISS19}
}

@inproceedings{10.1145/2970276.2970338,
author = {Peldszus, Sven and Kulcs\'{a}r, G\'{e}za and Lochau, Malte and Schulze, Sandro},
title = {Continuous Detection of Design Flaws in Evolving Object-Oriented Programs Using Incremental Multi-Pattern Matching},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970338},
doi = {10.1145/2970276.2970338},
abstract = { Design flaws in object-oriented programs may seriously corrupt code quality thus
increasing the risk for introducing subtle errors during software maintenance and
evolution. Most recent approaches identify design flaws in an ad-hoc manner, either
focusing on software metrics, locally restricted code smells, or on coarse-grained
architectural anti-patterns. In this paper, we utilize an abstract program model capturing
high-level object-oriented code entities, further augmented with qualitative and quantitative
design-related information such as coupling/cohesion. Based on this model, we propose
a comprehensive methodology for specifying object-oriented design flaws by means of
compound rules integrating code metrics, code smells and anti-patterns in a modular
way. This approach allows for efficient, automated design-flaw detection through incremental
multi-pattern matching, by facilitating systematic information reuse among multiple
detection rules as well as between subsequent detection runs on continuously evolving
programs. Our tool implementation comprises well-known anti-patterns for Java programs.
The results of our experimental evaluation show high detection precision, scalability
to real-size programs, as well as a remarkable gain in efficiency due to information
reuse. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {578–589},
numpages = {12},
keywords = {object-oriented software architecture, design-flaw detection, continuous software evolution},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3459104.3459148,
author = {Pradhan, Ayush and Joy, Eldhose and Jawagal, Harsha and Prasad Jayaraman, Sundar},
title = {A Framework for Leveraging Contextual Information in Automated Domain Specific Comprehension},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459148},
doi = {10.1145/3459104.3459148},
abstract = {When it comes to information, Enterprises today are seen as a black hole, a mass of
it goes in but gets difficult to extract the practical knowledge out of it. An automated
system that has the ability to consume this large mass of information and provide
specific, knowledgeable, domain-oriented responses back, will go a long way in unlocking
the value of this large-scale unstructured information. In a bid to enrich the answering
system's accuracy in Machine Reading Comprehension (MRC), we propose a domain-specific
Question Answers (QuAns) framework that specifically aims to auto-generate questions
from a domain-based document using an improvised Sequence to Sequence (Seq2Seq) technique
equipped with Attention and Copy mechanism. The generated questions are conditioned
on a set of candidate answers, derived using a combination of heuristic-driven and
graph-based techniques. Further, it also leverages the contextual information by pooling
strategy to build an automated response system using a deep custom fine-tuned Bidirectional
Encoder Representations from Transformers (BERT) framework and retrieving the top-k
contexts for a user query. The evaluation of the QuAns architecture is performed in
combination with human supervision as at times, the automated metrics like BLEU, Exact
Match (EM), F1 score, etc. fail to gauge the diverse semantic and structural aspects
of a generated response. Primarily, the proffered ensemble technique has leveraged
the augmented domain knowledge to enrich the answering response efficacy and improving
the EM and F1 score by 14.86% and 12.76% respectively over Vanilla BERT architecture.
To enhance the user experience, the conversational system is equipped with Natural
Language Generation (NLG) to present a human-readable response. Our architectural
pipeline aims to provide a one-stop solution for the organizations in processing huge
volumes of multidisciplinary data by significantly reducing the human introspection
and the overhead cost.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {263–270},
numpages = {8},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@article{10.1145/3308897.3308943,
author = {Luong, Doanh Kim and Ali, Muhammad and Benamrane, Fouad and Ammar, Ibrahim and Hu, Yim-Fun},
title = {Seamless Handover for Video Streaming over an SDN-Based Aeronautical Communications Network},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3308897.3308943},
doi = {10.1145/3308897.3308943},
abstract = {There have been increasing interests in applying Software Defined Networking (SDN)
to aeronautical communications primarily for air traffic management purposes. From
the service passenger communications' point of view, a major goal is to improve passengers'
perception of quality of experience on the infotainment services being provided for
them. Due to the high speed of aircrafts and the use of multiple radio technologies
during different flight phases and across different areas, vertical handovers between
these different radio technologies are envisaged. This poses a challenge to maintain
the quality of service during such handovers, especially for high bandwidth applications
such as video streaming. This paper proposes an SDN-based aeronautical communications
architecture consisting of both satellite and terrestrial-based radio technology.
In addition, an experimental implementation of the Locator ID Separation Protocol
(LISP) protocol with built-in multi-homing capability over the SDN-based architecture
was proposed to handle vertical handovers between the satellite and other radio technologies
onboard the aircraft. By using both objective and subjective Quality of Experience
(QoE) metrics, the simulation experiments show the benefit of combining LISP with
SDN to improve the video streaming quality during the handover in the aeronautical
communication environment.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jan,
pages = {98–99},
numpages = {2},
keywords = {sdn, multi-homing, vertical handovers, aeronautical communications, lisp mobility}
}

@inproceedings{10.1145/2742580.2742810,
author = {Karedla, Rama},
title = {Programming for the Intel Xeon Processor},
year = {2015},
isbn = {9781450335270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742580.2742810},
doi = {10.1145/2742580.2742810},
abstract = {Software programmers tend to focus on the software layer leaving performance on the
table by not taking advantage of the underlying hardware. This talk will help the
programmer take advantage of the underlying Intel Xeon server architecture to write
more efficient programs. We broadly cover topics such as time measurement, memory
ordering, making efficient use of the multi level caches, NUMA aware programming and
the use of the many compute cores available in the Xeon architecture via multi-threading.We
hope to show the benefit to both, latency and throughput oriented applications. The
talk will also address using the new AVX vector registers to achieve higher performance,
and briefly touch upon the recently announced Transactional Synchronization Extensions
(TSX) features. Examples of application profiling will demonstrate the benefit of
optimizing for performance in parallel with code development.},
booktitle = {Applicative 2015},
location = {New York, NY, USA},
series = {Applicative 2015}
}

@inproceedings{10.1145/2609248.2609264,
author = {Lazarescu, Mihai T. and Cohen, Albert and Guatto, Adrien and L\^{e}, Nhat Minn and Lavagno, Luciano and Pop, Antoniu and Prieto, Manuel and Terechko, Andrei and Sutii, Alexandru},
title = {Energy-Aware Parallelization Flow and Toolset for C Code},
year = {2014},
isbn = {9781450329415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609248.2609264},
doi = {10.1145/2609248.2609264},
abstract = {Multicore architectures are increasingly used in embedded systems to achieve higher
throughput with lower energy consumption. This trend accentuates the need to convert
existing sequential code to effectively exploit the resources of these architectures.
We present a parallelization flow and toolset for legacy C code that includes a performance
estimation tool, a parallelization tool, and a streaming-oriented parallelization
framework. These are part of the work-in-progress EU FP7 PHARAON project that aims
to develop a complete set of techniques and tools to guide and assist software development
for heterogeneous parallel architectures. We demonstrate the effectiveness of the
use of the toolset in an experiment where we measure the parallelization quality and
time for inexperienced users, and the parallelization flow and performance results
for the parallelization of a practical example of a stereo vision application.},
booktitle = {Proceedings of the 17th International Workshop on Software and Compilers for Embedded Systems},
pages = {79–88},
numpages = {10},
keywords = {execution profiling, program parallelization, energy estimation, data dependency analysis},
location = {Sankt Goar, Germany},
series = {SCOPES '14}
}

@article{10.1145/2641361.2641374,
author = {Ohkawa, Takeshi and Uetake, Daichi and Yokota, Takashi and Ootsu, Kanemitsu and Baba, Takanobu},
title = {Reconfigurable and Hardwired ORB Engine on FPGA by Java-to-HDL Synthesizer for Realtime Application},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {5},
issn = {0163-5964},
url = {https://doi.org/10.1145/2641361.2641374},
doi = {10.1145/2641361.2641374},
abstract = {A platform for networked FPGA system design, which is named "ORB Engine", is proposed
to add more controllability and design productivity on FPGA-based systems composed
of software and hardwired IPs. A developer can define an object-oriented interface
for the circuit IP in FPGA, and implement the control sequence part using Java. The
circuit IP in FPGA can be handled through object-oriented interface from variety of
programing languages like C++, Java, Python, Ruby and so on. Application specific
and high-efficiency circuit for ORB (Object Request Broker) protocol processing is
synthesized from easy-handling Java code using JavaRock Java-to-HDL synthesizer within
the de-facto standard CORBA (Common Object Request Broker Architecture). The measurement
result shows a very low latency as low as 200us of UDP/IP packet in/out and exhibits
a fluctuation free delay performance, which is desirable for real-time applications.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {77–82},
numpages = {6}
}

@article{10.1145/2659118.2659135,
author = {Tiwari, Umesh and Kumar, Santosh},
title = {In-out Interaction Complexity Metrics for Component-Based Software},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2659118.2659135},
doi = {10.1145/2659118.2659135},
abstract = {In the current state of software engineering, component-based software development
is one of the most alluring paradigms for developing large and complex software products.
In this software engineering methodology pre-engineered, pre-tested, context-based,
adaptable, deployable software components are assembled according to a predefined
architecture. Rather than developing a system from scratch, component-based software
development emphasizes the integration of these components according to the user's
requirements and specifications. In component-based software, the components interact
to access and provide services and functionality to each other. Currently, the emphasis
of industry and researchers is on developing impressive and efficient metrics and
measurement tools to analyze the interaction complexity among these components. To
represent the request and the response of services among components, we have used
outgoing edges and incoming edges respectively. In this paper we have defined these
interactions as In-Interactions and Out-Interactions. The metrics proposed in this
paper are solely based on the interactions among the components. In this work some
simple methods and metrics for computing the complexity of composable components are
suggested. The metrics discussed in this paper include the computation of interaction
complexities as Total-Interactions of a component, Total- Interactions of component-based
software, Interaction-Ratio of a component, Interaction-Ratio of component-based software,
Average- Interaction among components and Interaction-Percentage of components.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–4},
numpages = {4},
keywords = {component-based software development, adaptable, in-interactions, out-interactions, context-based, metrics, pre-engineered}
}

@inproceedings{10.1145/3409390.3409402,
author = {Monfared, Saleh Khalaj and Hajihassani, Omid and Kiarostami, Mohammad Sina and Zanjani, Soroush Meghdadi and Rahmati, Dara and Gorgin, Saeid},
title = {BSRNG: A High Throughput Parallel BitSliced Approach for Random Number Generators},
year = {2020},
isbn = {9781450388689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409390.3409402},
doi = {10.1145/3409390.3409402},
abstract = { In this work, a high throughput method for generating high-quality Pseudo-Random
Numbers using the bitslicing technique is proposed. In such a technique, instead of
the conventional row-major data representation, column-major data representation is
employed, which allows the bitslicing implementation to take full advantage of all
the available datapath of the hardware platform. By employing this data representation
as building blocks of algorithms, we showcase the capability and scalability of our
proposed method in various PRNG methods in the category of block and stream ciphers.
The LFSR-based (Linear Feedback Shift Register) nature of the PRNG in our implementation
perfectly suits the GPU’s many-core structure due to its register oriented architecture.
In the proposed SIMD vectorized GPU implementation, each GPU thread can generate several
32 pseudo-random bits in each LFSR clock cycle. We then compare our implementation
with some of the most significant PRNGs that display a satisfactory performance throughput
and randomness criteria. The proposed implementation successfully passes the NIST
test for statistical randomness and bit-wise correlation criteria. For computer-based
PRNG and the optical solutions in terms of performance and performance per cost, this
technique is efficient while maintaining an acceptable randomness measure. Our highest
performance among all of the implemented CPRNGs with the proposed method is achieved
by the MICKEY 2.0 algorithm, which shows 40% improvement over state of the art NVIDIA’s
proprietary high-performance PRNG, cuRAND library, achieving 2.72 Tb/s of throughput
on the affordable NVIDIA GTX 2080 Ti.},
booktitle = {49th International Conference on Parallel Processing - ICPP : Workshops},
articleno = {12},
numpages = {10},
keywords = {Cryptography, Stream cipher, Bitslicing, cuRAND, PRNG, High-performance, CUDA},
location = {Edmonton, AB, Canada},
series = {ICPP Workshops '20}
}

@article{10.1145/3183517,
author = {Floris, Alessandro and Ahmad, Arslan and Atzori, Luigi},
title = {QoE-Aware OTT-ISP Collaboration in Service Management: Architecture and Approaches},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3183517},
doi = {10.1145/3183517},
abstract = {It is a matter of fact that quality of experience (QoE) has become one of the key
factors determining whether a new multimedia service will be successfully accepted
by the final users. Accordingly, several QoE models have been developed with the aim
of capturing the perception of the user by considering as many influencing factors
as possible. However, when it comes to adopting these models in the management of
the services and networks, it frequently happens that no single provider has access
to all of the tools to either measure all influencing factors parameters or control
over the delivered quality. In particular, it often happens to the over-the-top (OTT)
and Internet service providers (ISPs), which act with complementary roles in the service
delivery over the Internet. On the basis of this consideration, in this article we
first highlight the importance of a possible OTT-ISP collaboration for a joint service
management in terms of technical and economic aspects. Then we propose a general reference
architecture for a possible collaboration and information exchange among them. Finally,
we define three different approaches, namely joint venture, customer lifetime value
based, and QoE fairness based. The first aims to maximize the revenue by providing
better QoE to customers paying more. The second aims to maximize the profit by providing
better QoE to the most profitable customers (MPCs). The third aims to maximize QoE
fairness among all customers. Finally, we conduct simulations to compare the three
approaches in terms of QoE provided to the users, profit generated for the providers,
and QoE fairness.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {36},
numpages = {24},
keywords = {OTT, quality of experience, Internet service providers, ISP, Over The Top service providers, QoE management, OTT-ISP collaboration, QoE}
}

@inproceedings{10.1145/2695664.2695835,
author = {Rrushi, Julian L. and Farhangi, Hassan and Nikolic, Radina and Howey, Clay and Carmichael, Kelly and Palizban, Ali},
title = {By-Design Vulnerabilities in the ANSI C12.22 Protocol Specification},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695835},
doi = {10.1145/2695664.2695835},
abstract = {The ANSI C12.22 is a standard that specifies interfaces to data communication networks
in the smart grid. In this paper we discuss several vulnerabilities by design that
we discovered in the ANSI C12.22 protocol specification during an analysis of the
overall protocol architecture. The consequences of an exploitation of those vulnerabilities
consist of denial of service conditions and disruptions to ANSI C12.22 nodes and relays.
We developed attack code to experiment with exploitations of most of the vulnerabilities
that we discuss in this paper. Our research testbed consisted of meters that we emulated
via the Trilliant TstBench software. The emulated meters were running on virtual Windows
machines on a virtual network. In the paper, we provide details of the vulnerabilities
by design that we identified, and thus propose a series of revisions of the ANSI C12.22
protocol specification with the objective of mitigating those vulnerabilities.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {2231–2236},
numpages = {6},
keywords = {smartgrid security, vulnerabilities, ANSI C12.22 protocol},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2815317.2815321,
author = {da Silva, Madalena P. and Dantas, Mario A.R. and Gon\c{c}alves, Alexandre L. and Pinto, Alex R.},
title = {A Managing QoE Approach for Provisioning User Experience Aware Services Using SDN},
year = {2015},
isbn = {9781450337571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815317.2815321},
doi = {10.1145/2815317.2815321},
abstract = {Provision and delivery of services with quality is a classic research problem, however
the computational resources available in the network infrastructure of providers are,
usually, managed with conventional Quality of Service (QoS) parameters. This paper
presents an approach of Quality of Experience (QoE) management for providing services
aware of the user experience. QoE modeling and architecture are proposed, with a semantic
engine able to learn the user's experience during the use of a service, detecting
violations of QoS metrics and providing information, allowing the controller to perform
actions in the elements of the Software Defined Networking. The experimental results
demonstrate that the proposal is feasible and functional and that the time spent between
QoE detection and adaptation of policies in network resources do not influence the
quality perceived by the user.},
booktitle = {Proceedings of the 11th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {51–58},
numpages = {8},
keywords = {software-defined network, semantic engine, quality of service, quality of experience},
location = {Cancun, Mexico},
series = {Q2SWinet '15}
}

@inproceedings{10.1145/3220267.3220280,
author = {Odema, Mohanad and Adly, Ihab and El-Baz, Ahmed and Amin, Hani},
title = {A RESTful Architecture for Portable Remote Online Experimentation Services},
year = {2018},
isbn = {9781450364690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220267.3220280},
doi = {10.1145/3220267.3220280},
abstract = {In this paper, an architecture is proposed to deliver portable remote online experimentation
services. This can benefit the educational and academic sectors in terms of providing
remote online accessibility to real experiment setups. Thus, the users can be relieved
from geographical and time dependence for the experiment to be conducted. Nowadays,
almost all web services leverage the efficiency and prevalence of the REST (Representational
State Transfer) architecture. Hence, this proposed remote online service has been
implemented in compliance with the RESTful architectural style.Web-based experiments
require compatibility with any of the users' portable devices and accessibility at
any time. A RESTful architecture can fulfill these requirements. In addition, different
experiments can be made available online based on this architecture while sharing
the same infrastructure. A case study has been selected to obtain measurements of
different force components existing inside wind tunnels. The complete implementation
of this system is provided starting from the embedded controller retrieving sensor
measurements to the web server development and user interface design.},
booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
pages = {102–105},
numpages = {4},
keywords = {Online testing and experimentation, RESTful architecture, Remote testing facilities},
location = {Cairo, Egypt},
series = {ICSIE '18}
}

@inproceedings{10.1145/3364641.3364680,
author = {Couto, Christian Marlon Souza and Terra, Ricardo},
title = {A Quality-Oriented Approach to Recommend Move Method Refactorings},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364680},
doi = {10.1145/3364641.3364680},
abstract = {Refactoring processes are common in large software systems, especially when developers
neglect architectural erosion process for long periods. Even though there are many
refactoring approaches, very few consider the refactoring impact on the software quality.Given
this scenario, we propose a refactoring approach to software systems oriented to software
quality metrics. Based on the QMOOD (Quality Model for Object Oriented Design), the
main idea is to move methods between classes in order to maximize the values of the
quality metrics. Using a formal notation, we describe the problem as follows. Given
a software system S, our approach recommends a sequence of refactorings R1, R2,...,
Rn that result in system versions S1, S2,..., Sn, where quality(Si+1) &gt; quality(Si).We
performed three types of evaluation to verify the usefulness of our implemented tool,
called QMove. First, we applied our approach on 13 open-source systems that we modified
by randomly moving a subset of its methods to other classes, then checking if our
approach would recommend the moved methods to return to their original place, and
we achieve 84% recall, on average. Second, we compared QMove against two state-of-art
refactoring tools (JMove and JDeodorant) on the 13 previously evaluated systems, and
QMove showed better recall value (84%) than the other two (30% and 29%, respectively).
Third, we conducted the same comparison among QMove, JMove, and JDeodorant applied
in two proprietary systems where experts evaluated the quality of the recommendations.
QMove obtained eight positively evaluated recommendations from the experts, against
two and none of JMove and JDeodorant, respectively.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {315},
numpages = {1},
keywords = {refactoring, software architecture, quality metrics},
location = {Fortaleza, Brazil},
series = {SBQS'19}
}

@inproceedings{10.1145/3297663.3310307,
author = {van der Sar, Jerom and Donkervliet, Jesse and Iosup, Alexandru},
title = {Yardstick: A Benchmark for Minecraft-like Services},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3310307},
doi = {10.1145/3297663.3310307},
abstract = {Online gaming applications entertain hundreds of millions of daily active players
and often feature vastly complex architecture. Among online games, Minecraft-like
games simulate unique (e.g., modifiable) environments, are virally popular, and are
increasingly provided as a service. However, the performance of Minecraft-like services,
and in particular their scalability, is not well understood. Moreover, currently no
benchmark exists for Minecraft-like games. Addressing this knowledge gap, in this
work we design and use the Yardstick benchmark to analyze the performance of Minecraft-like
services. Yardstick is based on an operational model that captures salient characteristics
of Minecraft-like services. As input workload, Yardstick captures important features,
such as the most-popular maps used within the Minecraft community. Yardstick captures
system- and application-level metrics, and derives from them service-level metrics
such as frequency of game-updates under scalable workload. We implement Yardstick,
and, through real-world experiments in our clusters, we explore the performance and
scalability of popular Minecraft-like servers, including the official vanilla server,
and the community-developed servers Spigot and Glowstone. Our findings indicate the
scalability limits of these servers, that Minecraft-like services are poorly parallelized,
and that Glowstone is the least viable option among those tested.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {243–253},
numpages = {11},
keywords = {yardstick, distributed systems, as a service, online gaming, minecraft, benchmark},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/2856636.2876471,
author = {Zodik, Gabi},
title = {Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2876471},
doi = {10.1145/2856636.2876471},
abstract = {The second wave of change presented by the age of mobility, wearables, and IoT focuses
on how organizations and enterprises, from a wide variety of commercial areas and
industries, will use and leverage the new technologies available. Businesses and industries
that don't change with the times will simply cease to exist.Applications need to be
powered by cognitive and contextual technologies to support real-time proactive decisions.
These decisions will be based on the mobile context of a specific user or group of
users, incorporating location, time of day, current user task, and more. Driven by
the huge amounts of data produced by mobile and wearables devices, and influenced
by privacy concerns, the next wave in computing will need to exploit data and computing
at the edge of the network. Future mobile apps will have to be cognitive to 'understand'
user intentions based on all the available interactions and unstructured data.Mobile
applications are becoming increasingly ubiquitous, going beyond what end users can
easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business
(B2B) apps, only about 30% of the development efforts appear in the interface of the
mobile app. For example, areas such as the collaborative nature of the software or
the shortened development cycle and time-to-market are not apparent to end users.
The other 70% of the effort invested is dedicated to integrating the applications
with back-office systems and developing those aspects of the application that operate
behind the scenes.An important, yet often complex, part of the solution and mobile
app takes place far from the public eye-in the back-office environment. It is there
that various aspects of customer relationship management must be addressed: tracking
usage data, pushing out messaging as needed, distributing apps to employees within
the enterprise, and handling the wide variety of operational and management tasks-often
involving the collection and monitoring of data from sensors and wearable devices.
All this must be carried out while addressing security concerns that range from verifying
user identities, to data protection, to blocking attempted breaches of the organization,
and activation of malicious code. Of course, these tasks must be augmented by a systematic
approach and vigilant maintenance of user privacy.The first wave of the mobile revolution
focused on development platforms, run-time platforms, deployment, activation, and
management tools for multi-platform environments, including comprehensive mobile device
management (MDM). To realize the full potential of this revolution, we must capitalize
on information about the context within which mobile devices are used. With both employees
and customers, this context could be a simple piece of information such as the user
location or time of use, the hour of the day, or the day of the week. The context
could also be represented by more complex data, such as the amount of time used, type
of activity performed, or user preferences. Further insight could include the relationship
history with the user and the user's behavior as part of that relationship, as well
as a long list of variables to be considered in various scenarios. Today, with the
new wave of wearables, the definition of context is being further extended to include
environmental factors such as temperature, weather, or pollution, as well as personal
factors such as heart rate, movement, or even clothing worn.In both B2E and B2C situations,
a context-dependent approach, based on the appropriate context for each specific user,
offers a superior tool for working with both employees and clients alike. This mode
of operation does not start and end with the individual user. Rather, it takes into
account the people surrounding the user, the events taking place nearby, appliances
or equipment activated, the user's daily schedule, as well as other, more general
information, such as the environment and weather.Developing enterprise-wide, context-dependent,
mobile solutions is still a complex challenge. A system of real added-value services
must be developed, as well as a comprehensive architecture. These four-tier architectures
comprise end-user devices like wearables and smartphones, connected to systems of
engagement (SoEs), and systems of record (SoRs). All this is needed to enable data
analytics and collection in the context where it is created. The data collected will
allow further interaction with employees or customers, analytics, and follow-up actions
based on the results of that analysis. We also need to ensure end-to-end (E2E) security
across these four tiers, and to keep the data and application contexts in sync. These
are just some of the challenges being addressed by IBM Research.As an example, these
technologies could be deployed in the retail space, especially in brick-and-mortar
stores. Identifying a customer entering a store, detecting her location among the
aisles, and cross-referencing that data with the customer's transaction history, could
lead to special offers tailor-made for that specific customer or suggestions relevant
to her purchasing process. This technology enables real-world implementation of metrics,
analytics, and other tools familiar to us from the online realm. We can now measure
visits to physical stores in the same way we measure web page hits: analyze time spent
in the store, the areas visited by the customer, and the results of those visits.
In this way, we can also identify shoppers wandering around the store and understand
when they are having trouble finding the product they want to purchase. We can also
gain insight into the standard traffic patterns of shoppers and how they navigate
a store's floors and departments. We might even consider redesigning the store layout
to take advantage of this insight to enhance sales.In healthcare, the context can
refer to insight extracted from data received from sensors on the patient, from either
his mobile device or wearable technology, and information about the patient's environment
and location at that moment in time. This data can help determine if any assistance
is required. For example, if a patient is discharged from the hospital for continued
at-home care, doctors can continue to remotely monitor his condition via a system
of sensors and analytic tools that interpret the sensor readings.This approach can
also be applied to the area of safety. Scientists at IBM Research are developing a
platform that collects and analyzes data from wearable technology to protect the safety
of employees working in construction, heavy industry, manufacturing, or out in the
field. This solution can serve as a real-time warning system by analyzing information
gathered from wearable sensors embedded in personal protective equipment, such as
smart safety helmets and protective vests, and in the workers' individual smartphones.
These sensors can continuously monitor a worker's pulse rate, movements, body temperature,
and hydration level, as well as environmental factors such as noise level, and other
parameters. The system can provide immediate alerts to the worker about any dangers
in the work environment to prevent possible injury. It can also be used to prevent
accidents before they happen or detect accidents once they occur. For example, with
sophisticated algorithms, we can detect if a worker falls based on a sudden difference
in elevations detected by an accelerometer, and then send an alert to notify her peers
and supervisor or call for help. Monitoring can also help ensure safety in areas where
continuous exposure to heat or dangerous materials must be limited based on regulated
time periods.Mobile technologies can also help manage events with massive numbers
of participants, such as professional soccer games, music festivals, and even large-scale
public demonstrations, by sending alerts concerning long and growing lines or specific
high-traffic areas. These technologies can be used to detect accidents typical of
large-scale gatherings, send warnings about overcrowding, and alert the event organizers.
In the same way, they can alleviate parking problems or guide public transportation
operators- all via analysis and predictive analytics.IBM Research - Haifa is currently
involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers
have a special expertise in time- and location-based intelligent applications, including
visual maps that display activity contexts and predictive analytics systems for mobile
data and users. In another area, IBM researchers in Haifa are developing new cognitive
services driven from the unique data available on mobile and wearable devices. Looking
to the future, the IBM Research team is further advancing the integration of wearable
technology, augmented reality systems, and biometric tools for mobile user identity
validation.Managing contextual data and analyzing the interaction between the different
kinds of data presents fascinating challenges for the development of next-generation
programming. For example, we need to rethink when and where data processing and computations
should occur: Is it best to leave them at the user-device level, or perhaps they should
be moved to the back-office systems, servers, and/or the cloud infrastructures with
which the user device is connected? New-age applications are becoming more and more
distributed. They operate on a wide range of devices, such as wearable technologies,
use a variety of sensors, and depend on cloud-based systems.As a result, a new distributed
programming paradigm is emerging to meet the needs of these use-cases and real-time
scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and
data in business systems, and must be able to shift computation from the cloud to
the edge, based on context in close to real-time. By processing data at the edge of
the network, close to where the interactions and processing are happening, we can
help reduce latency and offer new opportunities for improved privacy and security.Despite
all these interactions, data collection, and the analytic insights based upon them-we
cannot forget the issues of privacy. Without a proper and reliable solution that offers
more control over what personal data is shared and how it is used, people will refrain
from sharing information. Such sharing is necessary for developing and understanding
the context in which people are carrying out various actions, and to offer them tools
and services to enhance their actions.In the not-so-distant future, we anticipate
the appearance of ad-hoc networks for wearable technology systems that will interact
with one another to further expand the scope and value of available context-dependent
data.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {11–12},
numpages = {2},
location = {Goa, India},
series = {ISEC '16}
}

@inproceedings{10.1109/CCGrid.2014.103,
author = {Wu, Jie and Jansen, Christoph and Beier, Maximilian and Witt, Michael and Krefting, Dagmar},
title = {Extending XNAT towards a Cloud-Based Quality Assessment Platform for Retinal Optical Coherence Tomographies},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.103},
doi = {10.1109/CCGrid.2014.103},
abstract = {Neurosciencific research is increasingly based on image analysis methods. Large sets
of imaging data are processed using complex image analysis tools. While today magnetic
resonance imaging (MRI) is widely used for both functional and anatomical analysis
of the human brain, new imaging modalities are beginning to prove their capabilities
for neurological research. Among them, optical coherence tomography (OCT) allows for
noninvasive visualization of anatomical structures on a micrometer scale. Becoming
a standard diagnostic tool in ophthalmology, it is of rising interest for neurological
research. Crucial to all data analysis methods is the quality of the input data. The
platform presented in this paper is designed for automatic quality assessment of retinal
OCTs. It extends the image management platform XNAT by services to calculate and store
quality measures. It is also extensible regarding new quality measure algorithms,
allowing the developer to upload Matlab code, compile it for the infrastructure's
hardware architecture and test it in the system. The image processing tools to calculate
the quality measures are provided as a cloud-based service employing OpenStack as
underlying IT infrastructure. The prototype implementation encompassing security and
performance aspects are presented.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {764–773},
numpages = {10},
keywords = {cloud, IaaS, neuroimaging, medical imaging, OCT, SaaS, XNAT},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3238147.3240467,
author = {Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin},
title = {Experiences Applying Automated Architecture Analysis Tool Suites},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240467},
doi = {10.1145/3238147.3240467},
abstract = {In this paper, we report our experiences of applying three complementary automated
software architecture analysis techniques, supported by a tool suite, called DV8,
to 8 industrial projects within a large company. DV8 includes two state-of-the-art
architecture-level maintainability metrics—Decoupling Level and Propagation Cost,
an architecture flaw detection tool, and an architecture root detection tool. We collected
development process data from the project teams as input to these tools, reported
the results back to the practitioners, and followed up with telephone conferences
and interviews. Our experiences revealed that the metrics scores, quantitative debt
analysis, and architecture flaw visualization can effectively bridge the gap between
management and development, help them decide if, when, and where to refactor. In particular,
the metrics scores, compared against industrial benchmarks, faithfully reflected the
practitioners’ intuitions about the maintainability of their projects, and enabled
them to better understand the maintainability relative to other projects internal
to their company, and to other industrial products. The automatically detected architecture
flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify
the “hotspots" within the systems that are responsible for high maintenance costs.
Except for the two smallest projects for which both architecture metrics indicated
high maintainability, all other projects are planning or have already begun refactorings
to address the problems detected by our analyses. We are working on further automating
the tool chain, and transforming the analysis suite into deployable services accessible
by all projects within the company.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {779–789},
numpages = {11},
keywords = {Software Quality, Software Maintenance, Software Architecture},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/3178461.3178462,
author = {Meryem, Amar and Samira, Douzi and Bouabid, El Ouahidi},
title = {Enhancing Cloud Security Using Advanced MapReduce K-Means on Log Files},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178462},
doi = {10.1145/3178461.3178462},
abstract = {Many customers ranked cloud security as a major challenge that threaten their work
and reduces their trust on cloud service's provider. Hence, a significant improvement
is required to establish better adaptations of security measures that suit recent
technologies and especially distributed architectures.Considering the meaningful recorded
data in cloud generated log files, making analysis on them, mines insightful value
about hacker's activities. It identifies malicious user behaviors and predicts new
suspected events. Not only that, but centralizing log files, prevents insiders from
causing damage to system. In this paper, we proposed to take away sensitive log files
into a single server provider and combining both MapReduce programming and k-means
on the same algorithm to cluster observed events into classes having similar features.
To label unknown user behaviors and predict new suspected activities this approach
considers cosine distances and deviation metrics.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {63–67},
numpages = {5},
keywords = {K-means, Deviation metric, MapReduce, Cloud Security, log files},
location = {Casablanca, Morocco},
series = {ICSIM2018}
}

@inproceedings{10.1145/2841113.2841114,
author = {Forget, Alain and Chiasson, Sonia and Biddle, Robert},
title = {Choose Your Own Authentication},
year = {2015},
isbn = {9781450337540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2841113.2841114},
doi = {10.1145/2841113.2841114},
abstract = {To solve the long-standing problems users have in creating and remembering text passwords,
a wide variety of alternative authentication schemes have been proposed. Some of these
schemes outperform others by various metrics in various contexts. However, none unilaterally
outperform all others, and so text passwords persist as the main scheme applications
depend upon. In this paper, we challenge the long-standing assumption that only one
authentication scheme can be offered by an application service. We propose Choose
Your Own Authentication (CYOA): a novel authentication architecture that enables users
to choose a scheme amongst several available alternatives. CYOA would enable users
to select whichever scheme best suits their preferences, abilities, and usage context.
Existing text password systems could easily be replaced. Furthermore, the three-party
architecture would enable delegating the management of authentication systems to trusted-third
parties. The architecture allows rapid deployment and testing of novel authentication
technologies. Our two-week usability study suggests that participants were willing
to leverage alternative schemes. Participants were confident that CYOA could keep
their financial information secure.},
booktitle = {Proceedings of the 2015 New Security Paradigms Workshop},
pages = {1–15},
numpages = {15},
keywords = {user study, usable security, survey, Authentication},
location = {Twente, Netherlands},
series = {NSPW '15}
}

@inproceedings{10.1145/3377813.3381349,
author = {Diamantopoulos, Nikos and Wong, Jeffrey and Mattos, David Issa and Gerostathopoulos, Ilias and Wardrop, Matthew and Mao, Tobias and McFarland, Colin},
title = {Engineering for a Science-Centric Experimentation Platform},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381349},
doi = {10.1145/3377813.3381349},
abstract = {Netflix is an internet entertainment service that routinely employs experimentation
to guide strategy around product innovations. As Netflix grew, it had the opportunity
to explore increasingly specialized improvements to its service, which generated demand
for deeper analyses supported by richer metrics and powered by more diverse statistical
methodologies. To facilitate this, and more fully harness the skill sets of both engineering
and data science, Netflix engineers created a science-centric experimentation platform
that leverages the expertise of scientists from a wide range of backgrounds working
on data science tasks by allowing them to make direct code contributions in the languages
used by them (Python and R). Moreover, the same code that runs in production is able
to be run locally, making it straightforward to explore and graduate both metrics
and causal inference methodologies directly into production services.In this paper,
we provide two main contributions. Firstly, we report on the architecture of this
platform, with a special emphasis on its novel aspects: how it supports science-centric
end-to-end workflows without compromising engineering requirements. Secondly, we describe
its approach to causal inference, which leverages the potential outcomes conceptual
framework to provide a unified abstraction layer for arbitrary statistical models
and methodologies.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {191–200},
numpages = {10},
keywords = {causal inference, A/B testing, experimentation, software architecture, science-centric},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.1145/3465481.3470091,
author = {Komisarek, Miko\l{}aj and Pawlicki, Marek and Kowalski, Miko\l{}aj and Marzecki, Adrian and Kozik, Rafa\l{} and Choraundefined, Micha\l{}},
title = {Network Intrusion Detection in the Wild - the Orange Use Case in the SIMARGL Project},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470091},
doi = {10.1145/3465481.3470091},
abstract = { There is a profuse abundance of network security incidents around the world every
day. Increasingly, services and data stored on servers fall victim to sophisticated
techniques that cause all sorts of damage. Hackers invent new ways to bypass security
measures and modify the existing viruses in order to deceive defense systems. Therefore,
in response to these illegal procedures, new ways to defend against them are being
developed. In this paper, a method for anomaly detection based on machine learning
technique is presented and a near real-time processing system architecture is proposed.
The main contribution is a test-run of ML algorithms on real-world data coming from
a world-class telecom operator. This work investigates the effectiveness of detecting
malicious behaviour in network packets using several machine learning techniques.
The results achieved are expressed with a set of metrics. For better clarity on the
classifier performance, 10-fold cross-validation was used.},
booktitle = {The 16th International Conference on Availability, Reliability and Security},
articleno = {65},
numpages = {7},
keywords = {network intrusion detection, machine learning},
location = {Vienna, Austria},
series = {ARES 2021}
}

@inproceedings{10.1145/3167132.3167178,
author = {Fernandes, Rodrigo and Sim\~{a}o, Jos\'{e} and Veiga, Lu\'{\i}s},
title = {EcoVMbroker: Energy-Aware Scheduling for Multi-Layer Datacenters},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167178},
doi = {10.1145/3167132.3167178},
abstract = {The cloud relies on efficient algorithms to find resources for jobs by fulfilling
the job's requirements and at the same time optimise an objective function. Utility
is a measure of the client satisfaction that can be seen as an objective function
maximised by schedulers based on the agreed service level agreement (SLA). We propose
EcoVM-Broker which can reduce energy consumption by using dynamic voltage frequency
scaling (DVFS) and applying reductions of utility, different for classes of users
and across ranges of resource allocations. Using efficient data structures and a hierarchical
architecture, we created a scalable solution for the fast growing heterogeneous cloud.
EcoVMBroker proved that we can delegate work in a hierarchical datacenter, make decisions
based on summaries of resource usage collected from several nodes and still be efficient.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {403–410},
numpages = {8},
keywords = {DVFS, virtual machice scheduling, partial utility, energy efficiency},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3210259.3210262,
author = {Erb, Benjamin and Mei\ss{}ner, Dominik and Kargl, Frank and Steer, Benjamin A. and Cuadrado, Felix and Margan, Domagoj and Pietzuch, Peter},
title = {Graphtides: A Framework for Evaluating Stream-Based Graph Processing Platforms},
year = {2018},
isbn = {9781450356954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210259.3210262},
doi = {10.1145/3210259.3210262},
abstract = {Stream-based graph systems continuously ingest graph-changing events via an established
input stream, performing the required computation on the corresponding graph. While
there are various benchmarking and evaluation approaches for traditional, batch-oriented
graph processing systems, there are no common procedures for evaluating stream-based
graph systems. We, therefore, present GraphTides, a generic framework which includes
the definition of an appropriate system model, an exploration of the parameter space,
suitable workloads, and computations required for evaluating such systems. Furthermore,
we propose a methodology and provide an architecture for running experimental evaluations.
With our framework, we hope to systematically support system development, performance
measurements, engineering, and comparisons of stream-based graph systems.},
booktitle = {Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {3},
numpages = {10},
keywords = {graph processing, graph analytics, evolving graphs, evaluation, temporal graphs, stream-based graphs, measurements},
location = {Houston, Texas},
series = {GRADES-NDA '18}
}

@inproceedings{10.1145/3459104.3459136,
author = {A. Panayiotou, Nikolaos and P. Stavrou, Vasileios and E. Stergiou, Konstantinos},
title = {Applying the Industry 4.0 in a Smart Gas Grid: The Greek Gas Distribution Network Case},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459136},
doi = {10.1145/3459104.3459136},
abstract = {The aim of this paper is to design and implement a series of actions regarding the
operation of DEDA S.A. (Natural Gas Distribution Networks), based on principles of
Industry 4.0. The Natural Gas Distribution sector is one of the most critical and
innovative areas where Industry 4.0 can be applied, being part of critical infrastructure
management. At first, company's business process architecture was developed, with
the aim to export DEDA's business process and functional specifications related to
the required information systems. Subsequently, company's communication network is
implemented alongside the company's gas network, in coordination with the company's
control room.In addition, modernization of metering system is taking place in order
to exchange information between smart meters and the control room. A number of Information
Systems, such as the pipeline surveillance system and the Business Intelligence system
will also be installed in order to ensure communication at different levels using
Cloud technologies. The implementation is expected to improve DEDA's organization,
increasing customers' service level. As a result, there will be an expected increase
in the operational efficiency of DEDA's network through the use of advanced technologies,
in cooperation with business process modelling techniques. The effort should be continued
in this direction in order to achieve even greater improvement in business processes,
information systems and pipeline automation.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {180–184},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up Architectural SW Health Builds in a New Product Line Generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start
from scratch but takes into consideration the architecture and assets of the former
product line generation. Being able to accommodate legacy and 3rd party code is one
of the major product line qualities to be met. On the other side, product line qualities
like reusability, maintainability and alterability, i.e. being able to cope up with
a large amount of variability, with configurability and fast integratability are major
drivers.While setting up a new product line generation and thus a new corresponding
architecture, we this time focused on architectural software (SW) health and tracking
of architectural metrics from the very beginning. Taking the definition of "architecture
being a set of design decisions" [18] literally, we attempt to implement an architectural
check for every design decision taken. Architectural design decisions in our understanding
do not only - and even not mainly - deal with the definition of components and their
interaction but with patterns and rules or anti-patterns. The rules and anti-patterns,
"what not to do" or more often also "what not to do <u>any more</u>", is even more
important in setting up a new product line generation because developers are not only
used to the old style of developing and the old architecture, but also still have
to develop assets for both generations.In this article we describe selected architectural
checks that we have implemented, the layered architecture check and the check for
usage of obsolete services. Additionally we discuss selected architectural metrics:
the coupling coefficient metrics and the instability metrics. In the summary and outlook
we describe our experiences and still open topics in setting up architectural SW health
checks for a large-scale product line.The real-world examples are taken from the domain
of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {architectural technical debt, software architecture, product line development, technical debt, software erosion, architectural checks, embedded software},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.1145/3358695.3361753,
author = {Mohammadi, Farnaz and Panou, Angeliki and Ntantogian, Christoforos and Karapistoli, Eirini and Panaousis, Emmanouil and Xenakis, Christos},
title = {CUREX: SeCUre and PRivate HEalth Data EXchange},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3361753},
doi = {10.1145/3358695.3361753},
abstract = {The Health sector's increasing dependence on digital information and communication
infrastructures renders it vulnerable to privacy and cybersecurity threats, especially
as the theft of health data has become lucrative for cyber criminals. CUREX comprehensively
addresses the protection of the confidentiality and integrity of health data by producing
a novel, flexible and scalable situational awareness-oriented platform. It allows
a healthcare provider to assess cybersecurity and privacy risks that are exposed to
and suggest optimal strategies for addressing these risks with safeguards tailored
to each business case and application. CUREX is fully GDPR compliant by design. At
its core, a decentralised architecture enhanced by a private blockchain infrastructure
ensures the integrity of the data and –most importantly- the patient safety. Crucially,
CUREX expands beyond technical measures and improves cyber hygiene through training
and awareness activities for healthcare personnel. Its validation focuses on highly
challenging cases of health data exchange, spanning patient cross-border mobility,
remote healthcare, and data exchange for research.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {263–268},
numpages = {6},
keywords = {Risk assessment, Blockchain, eHealth, Cybersecurity, Cyber hygiene},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.1145/3465481.3470018,
author = {Eckel, Michael and Riemann, Tim},
title = {Userspace Software Integrity Measurement},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470018},
doi = {10.1145/3465481.3470018},
abstract = {Todays computing systems are more interconnected and sophisticated than ever before.
Especially in healthcare 4.0, services and infrastructures rely on cyber-physical
systemss (CPSess) and Internet of Things (IoT) devices. This adds to the complexity
of these highly connected systems and their manageability. Even worse, the variety
of emerging cyber attacks is becoming more severe and sophisticated, making healthcare
one of the most important sectors with major security risks. The development of appropriate
countermeasures constitutes one of the most complex and difficult challenges in cyber
security research. Research areas include, among others, anomaly detection, network
security, multi-layer event detection, cyber resiliency, and integrity protection.
Securing the integrity of software running on a device is a desirable protection goal
in the context of systems security. With a Trusted Platform Module (TPM), measured
boot, and remote attestation there exist technologies to ensure that a system has
booted up correctly and runs only authentic software. The Linux Integrity Measurement
Architecture (IMA) extends these principles into the operating systems (OSes), measuring
native binaries before they are loaded. However, interpreted language files, such
as Java classes and Python scripts, are not considered executables and are not measured
as such. Contemporary OSess ship with many of these and it is vital to consider them
as security-critical as native binaries. In this paper, we introduce Userspace Software
Integrity Measurement (USIM) for the Linux OSes. Userspace Software Integrity Measurement
(USIM) enables interpreters to measure, log, and irrevocably anchor critical events
in the TPM. We develop a software library in C which provides TPM-based measurement
functionality as well as the USIM service, which provides concurrent access handling
to the TPM based event logging. Further, we develop and implement a concept to realize
highly frequent event logging on the slow TPM. We integrate this library into the
Java Virtual Machine (JVM) to measure Java classes and show that it can be easily
integrated into other interpreters. With performance measurements we demonstrate that
our contribution is feasible and that overhead is negligible. },
booktitle = {The 16th International Conference on Availability, Reliability and Security},
articleno = {138},
numpages = {11},
keywords = {integrity verification, Trusted Computing, Systems security},
location = {Vienna, Austria},
series = {ARES 2021}
}

@inproceedings{10.1145/3384217.3386393,
author = {Petz, Adam},
title = {An Infrastructure for Faithful Execution of Remote Attestation Protocols},
year = {2020},
isbn = {9781450375610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384217.3386393},
doi = {10.1145/3384217.3386393},
abstract = {Experience shows that even with a well-intentioned user at the keyboard, a motivated
attacker can compromise a computer system at a layer below or adjacent to the shallow
forms of authentication that are now accepted as commonplace[3]. Therefore, rather
than asking "Can we trust the person behind the keyboard", a still better question
might be: "Can we trust the computer system underneath?". An emerging technology for
gaining trust in a remote computing system is remote attestation. Remote attestation
is the activity of making a claim about properties of a target by supplying evidence
to an appraiser over a network[2]. Although many existing approaches to remote attestation
wisely adopt a layered architecture-where the bottom layers measure layers above-the
dependencies between components remain static and measurement orderings fixed. For
modern computing environments with diverse topologies, we can no longer fix a target
architecture any more than we can fix a protocol to measure that architecture.Copland
[1] is a domain-specific language and formal framework that provides a vocabulary
for specifying the goals of layered attestation protocols. It also provides a reference
semantics that characterizes system measurement events and evidence handling; a foundation
for comparing protocol alternatives. The aim of this work is to refine the Copland
semantics to a more fine-grained notion of attestation manager execution-a high-privilege
thread of control responsible for invoking attestation services and bundling evidence
results. This refinement consists of two cooperating components called the Copland
Compiler and the Attestation Virtual Machine (AVM). The Copland Compiler translates
a Copland protocol description into a sequence of primitive attestation instructions
to be executed in the AVM. When considered in combination with advances in virtualization,
trusted hardware, and high-assurance system software components-like compilers, file-systems,
and OS kernels-a formally verified remote attestation infrastructure creates exciting
opportunities for building system-level security arguments.},
booktitle = {Proceedings of the 7th Symposium on Hot Topics in the Science of Security},
articleno = {17},
numpages = {1},
location = {Lawrence, Kansas},
series = {HotSoS '20}
}

@article{10.1145/3233182,
author = {Ji, Kecheng and Ling, Ming and Shi, Longxing and Pan, Jianping},
title = {An Analytical Cache Performance Evaluation Framework for Embedded Out-of-Order Processors Using Software Characteristics},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3233182},
doi = {10.1145/3233182},
abstract = {Utilizing analytical models to evaluate proposals or provide guidance in high-level
architecture decisions is been becoming more and more attractive. A certain number
of methods have emerged regarding cache behaviors and quantified insights in the last
decade, such as the stack distance theory and the memory level parallelism (MLP) estimations.
However, prior research normally oversimplified the factors that need to be considered
in out-of-order processors, such as the effects triggered by reordered memory instructions,
and multiple dependences among memory instructions, along with the merged accesses
in the same MSHR entry. These ignored influences actually result in low and unstable
precisions of recent analytical models.By quantifying the aforementioned effects,
this article proposes a cache performance evaluation framework equipped with three
analytical models, which can more accurately predict cache misses, MLPs, and the average
cache miss service time, respectively. Similar to prior studies, these analytical
models are all fed with profiled software characteristics in which case the architecture
evaluation process can be accelerated significantly when compared with cycle-accurate
simulations.We evaluate the accuracy of proposed models compared with gem5 cycle-accurate
simulations with 16 benchmarks chosen from Mobybench Suite 2.0, Mibench 1.0, and Mediabench
II. The average root mean square errors for predicting cache misses, MLPs, and the
average cache miss service time are around 4%, 5%, and 8%, respectively. Meanwhile,
the average error of predicting the stall time due to cache misses by our framework
is as low as 8%. The whole cache performance estimation can be sped by about 15 times
versus gem5 cycle-accurate simulations and 4 times when compared with recent studies.
Furthermore, we have shown and studied the insights between different performance
metrics and the reorder buffer sizes by using our models. As an application case of
the framework, we also demonstrate how to use our framework combined with McPAT to
find out Pareto optimal configurations for cache design space explorations.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
articleno = {79},
numpages = {25},
keywords = {Analytical models, cache misses, cache miss service time, software characteristics, memory level parallelism}
}

@inproceedings{10.1145/3341105.3374026,
author = {Araldo, Andrea and Stefano, Alessandro Di and Stefano, Antonella Di},
title = {Resource Allocation for Edge Computing with Multiple Tenant Configurations},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374026},
doi = {10.1145/3341105.3374026},
abstract = {Edge Computing (EC) consists in deploying computational resources, e.g., memory, CPUs,
at the Edge of the network, e.g., base stations, access points, and run there a part
of the computation currently running on the Cloud. This approach promises to reduce
latency, inter-domain traffic and enhance user experience. Since resources at the
Edge are scarce, resource allocation is crucial for EC. While most of the studies
assume users interact directly with the Edge submitting a sequence of tasks, we instead
consider that users will interact with different Service Providers (SPs), as they
currently do in the Web. We therefore consider the case of a Network Operator (NO)
that owns the resources at the Edge and must decide how much resource to allocate
to the different tenants (SPs).We propose MORA, a polynomial time strategy which allows
the NO to maximize its utility, which can be inter-domain traffic savings, improved
users' QoE or other metrics of interest. The core of MORA is that (i) it exploits
service elasticity, i.e., the fact that services can adapt to the resources allocated
by the NO and rely on a remote Cloud for the excess of computation, (ii) it is suitable
for micro-services architecture, which decomposes a single service in a set of components,
which MORA places in the different computational nodes of the Edge and (iii) it copes
with multi-dimensional resources, e.g., memory and CPUs. After analyzing the properties
of the algorithm, we show numerically that it performs close to the optimum. To guarantee
reproducibility, the numerical evaluation is performed on publicly available traces
from Google and Alibaba clusters and in synthetic scenarios and our code is open source.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1190–1199},
numpages = {10},
keywords = {container systems, edge computing, network optimization, resource allocation, cloud computing},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.1109/TASLP.2019.2915922,
author = {Xu, Zhen and Sun, Chengjie and Long, Yinong and Liu, Bingquan and Wang, Baoxun and Wang, Mingjiang and Zhang, Min and Wang, Xiaolong},
title = {Dynamic Working Memory for Context-Aware Response Generation},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2915922},
doi = {10.1109/TASLP.2019.2915922},
abstract = {In human-to-human conversations, the context generally provides several backgrounds
and strategic points for the following response. Therefore, many response generation
approaches have explored the methodologies to incorporate the context into the encoder–decoder
architecture, to generate context-aware responses that are remarkably relevant and
cohesive to the given context. However, most approaches pay less attention to semantic
interactions implicitly existing within contextual utterances, which are of great
importance to capture semantic clues of the given dialog context, indeed. This paper
proposes a dynamic working memory mechanism to model long-term semantic hints in the
conversation context, by performing semantic interactions between utterances and updating
context representation dynamically. Then, the outputs of the dynamic working memory
are employed to provide helpful clues for the encoder–decoder architecture to generate
responses to the given dialog. We have evaluated the proposed approach on Twitter
Customer Service Corpus and OpenSubtitles Corpus, with several automatic evaluation
metrics and the human evaluation, and the empirical results show the effectiveness
of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1419–1431},
numpages = {13}
}

@inproceedings{10.1145/3386367.3431306,
author = {Marques, Jonatas and Levchenko, Kirill and Gaspary, Luciano},
title = {IntSight: Diagnosing SLO Violations with in-Band Network Telemetry},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431306},
doi = {10.1145/3386367.3431306},
abstract = {Performance requirements for many of today's high-perfor-mance networks are expressed
as service-level objectives (SLOs), i.e., precise guarantees, typically on latency
and bandwidth, that a user can expect from the network. For network operators, monitoring
their own SLO compliance, and quickly diagnosing any violations, is a critical element
for effective operations. Unfortunately, existing network architectures are not engineered
for this purpose; there is no mechanism, for example, for the operator to monitor
the 95th per-centile latency experienced by a customer. Data plane programmability
has made per-packet measurements possible but brings the challenge of keeping the
monitoring overhead low and practical. In this paper, we present IntSight, a system
for highly accurate and fine-grained detection and diagnosis of SLO violations. The
main contribution of IntSight is, building upon in-band telemetry, introducing path-wise
computation of network metrics and selective generation of reports. We show the effectiveness
of IntSight by way of two use cases. Our evaluation using real networks also shows
that IntSight generates up to two orders of magnitude less monitoring traffic than
state-of-the-art approaches. Furthermore, its processing and memory requirements are
low and therefore compatible with currently existing programmable platforms.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {421–434},
numpages = {14},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@inproceedings{10.5555/2820518.2820566,
author = {Mirakhorli, Mehdi and Cleland-Huang, Jane},
title = {Modifications, Tweaks, and Bug Fixes in Architectural Tactics},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Architectural qualities such as reliability, performance, and security, are often
realized in a software system through the adoption of tactical design decisions such
as the decision to use redundant processes, a heartbeat monitor, or a specific authentication
mechanism. Such decisions are critical for delivering a system that meets its quality
requirements. Despite the stability of high-level decisions, our analysis has shown
that tactic-related classes tend to be modified more frequently than other classes
and are therefore stronger predictors of change than traditional Object-Oriented coupling
and cohesion metrics. In this paper we present the results from this initial study,
including an analysis of why tactic-related classes are changed, and a discussion
of the implications of these findings for maintaining architectural quality over the
lifetime of a software system.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {377–380},
numpages = {4},
keywords = {modifications, tactics, bugs, architectural decisions, metrics},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1145/2699503,
author = {Patrignani, Marco and Agten, Pieter and Strackx, Raoul and Jacobs, Bart and Clarke, Dave and Piessens, Frank},
title = {Secure Compilation to Protected Module Architectures},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/2699503},
doi = {10.1145/2699503},
abstract = {A fully abstract compiler prevents security features of the source language from being
bypassed by an attacker operating at the target language level. Unfortunately, developing
fully abstract compilers is very complex, and it is even more so when the target language
is an untyped assembly language. To provide a fully abstract compiler that targets
untyped assembly, it has been suggested to extend the target language with a protected
module architecture—an assembly-level isolation mechanism which can be found in next-generation
processors. This article provides a fully abstract compilation scheme whose source
language is an object-oriented, high-level language and whose target language is such
an extended assembly language. The source language enjoys features such as dynamic
memory allocation and exceptions. Secure compilation of first-order method references,
cross-package inheritance, and inner classes is also presented. Moreover, this article
contains the formal proof of full abstraction of the compilation scheme. Measurements
of the overhead introduced by the compilation scheme indicate that it is negligible.},
journal = {ACM Trans. Program. Lang. Syst.},
month = apr,
articleno = {6},
numpages = {50},
keywords = {Fully abstract compilation, protected module architecture}
}

@inproceedings{10.5555/2665049.2665054,
author = {Kofler, Klaus and Davis, Gregory and Gesing, Sandra},
title = {SAMPO: An Agent-Based Mosquito Point Model in OpenCL},
year = {2014},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Agent-based modeling and simulations are applied for problems where population-level
patterns arise from the interaction of many autonomous individuals. These problems
are compute-intensive and excellent candidates for the use of parallel algorithms
and architectures. As a cross-platform software development framework for parallel
architectures, OpenCL appears as an ideal tool to implement such algorithms. However,
OpenCL does not natively support object-oriented development, which most of the toolkits
and frameworks used to build agent-based models require.The present work describes
an OpenCL implementation of an existing agent-based model, simulating populations
of the Anopheles gambiae mosquito, one of the most important vectors of malaria in
Africa. Discussed are the methods and techniques used to overcome the design challenges,
which arise when transitioning from an object-oriented program to an efficient OpenCL
implementation. In particular, the parallelism inside the program has been maximized,
dynamic divergent branching was reduced, and the number of data transfers between
the OpenCL host and device has been minimized as far as possible.Even though our implementation
was designed for this specific use case, the approach can be generalized to other
contexts, as most agent-based point models would benefit from the same basic design
decisions that we took for our implementation. Comparisons between the object-oriented
and the OpenCL implementation illustrate that using an OpenCL approach offers two
important performance benefits: an overall simulation time speedup of up to 576 with
no measurable loss of accuracy, and better scalability as the agent-population size
increases. The tradeoffs necessary to achieve these performance benefits and the implications
for future agent-based software development frameworks are discussed.},
booktitle = {Proceedings of the 2014 Symposium on Agent Directed Simulation},
articleno = {5},
numpages = {10},
keywords = {agent-based modelling, GPGPU, OpenCL},
location = {Tampa, Florida},
series = {ADS '14}
}

@inproceedings{10.1145/2641798.2641814,
author = {Xu, Yi and Helal, Sumi},
title = {Application Caching for Cloud-Sensor Systems},
year = {2014},
isbn = {9781450330305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641798.2641814},
doi = {10.1145/2641798.2641814},
abstract = {Driven by critical and pressing smart city applications, accessing massive numbers
of sensors by cloud-hosted services is becoming an emerging and inevitable situation.
Na\"{\i}vely connecting massive numbers of sensors to the cloud raises major scalability
and energy challenges. An architecture embodying distributed optimization is needed
to manage the scale and to allow limited energy sensors to last longer in such a dynamic
and high-velocity big data system. We developed a multi-tier architecture which we
call Cloud, Edge and Beneath (CEB). Based on CEB, we propose an Application Fragment
Caching Algorithm (AFCA) which selectively caches application fragments from the cloud
to lower layers of CEB to improve cloud scalability. Through experiments, we show
and measure the effect of AFCA on cloud scalability.},
booktitle = {Proceedings of the 17th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {303–306},
numpages = {4},
keywords = {cloud-sensor systems, cloud computing, application caching},
location = {Montreal, QC, Canada},
series = {MSWiM '14}
}

@inproceedings{10.1145/2985766.2985772,
author = {Edoh, Thierry Oscar C. and Atchome, Athanase and Alahassa, Bidossessi R.U. and Pawar, Pravin},
title = {Evaluation of a Multi-Tier Heterogeneous Sensor Network for Patient Monitoring: The Case of Benin},
year = {2016},
isbn = {9781450345187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2985766.2985772},
doi = {10.1145/2985766.2985772},
abstract = {In this paper we propose and evaluate a wireless sensor network (WSN) system in order
to improve an existing patient-monitoring and surveillance system at the cardiologic
intensive care unit (CICU) of a large university clinic (Centre Hospitalier Universitaire
Hubert Koutoukou Maga - CHU-HKM) in Cotonou city of Benin (a West-African country).
We have designed a multi-tier architecture and simulated a heterogeneous, autonomous,
and energy efficient wireless sensor network system to overcome issues faced by existing
patient monitoring system in CICU such as manual collection and processing of data.
The improvement of the patient monitoring system has the objectives of providing affordable
and better health care service provision as well as autonomous and automatic collection
and processing of patient's bio-signals and environmental data. The proposed Wireless
Sensor Network consists of wireless heterogeneous nodes which sense patient bio-signals,
measure environmental parameters in the hospitalization rooms such as ambient temperature,
quality of air and send collected data to a gateway (central node) for processing
and storage. The conducted simulation experiments show that the proposed sensor network
architecture which uses ZigBee wireless standard and protocol highly improves the
patience monitoring and surveillance experience at CICU. It promotes collection and
autonomous processing of patient physiological data and room ambient temperature data.
Incorporating such system in CICU will be highly beneficial for taking a correct decision
during treatment. Beyond the accuracy and quality of the collected medical data, proposed
WSN is also designed to reduce the energy consumption within the sensor network system.},
booktitle = {Proceedings of the 2016 ACM Workshop on Multimedia for Personal Health and Health Care},
pages = {23–29},
numpages = {7},
keywords = {wireless sensors network, cooperative sensors, patient monitorin, zigbee standards, intensive care unit, cardiology},
location = {Amsterdam, The Netherlands},
series = {MMHealth '16}
}

@inproceedings{10.1145/2851613.2851874,
author = {Abderrahim, Wiem and Choukair, Zied},
title = {PaaS Dependability Integration Architecture Based on Cloud Brokering},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851874},
doi = {10.1145/2851613.2851874},
abstract = {Cloud computing has revolutionized the way IT is provisioned nowadays since it exposes
computing capabilities as rental resources to consumers. The emergence of cloud computing
services hasn't though prevented outages in these environments even among high profile
ranked cloud providers. Tremendous efforts concentrated on fault management measures
have been applied for these environments. But they have been focused mainly on the
IaaS service model and have been operated on the cloud provider side alone. In this
context, this paper proposes an architecture for cloud brokering that implements dependability
properties in an end to end way involving different cloud actors and all over the
cloud service models SaaS, PaaS and IaaS.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {484–487},
numpages = {4},
keywords = {fault tolerance, cloud provider, PaaS, IaaS, SaaS, fault management, cloud broker, fault forecasting, dependability},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1109/TNET.2018.2793892,
author = {Araldo, Andrea and Dan, Gyorgy and Rossi, Dario},
title = {Caching Encrypted Content Via Stochastic Cache Partitioning},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2793892},
doi = {10.1109/TNET.2018.2793892},
abstract = {In-network caching is an appealing solution to cope with the increasing bandwidth
demand of video, audio, and data transfer over the Internet. Nonetheless, in order
to protect consumer privacy and their own business, content providers CPs increasingly
deliver encrypted content, thereby preventing Internet service providers ISPs from
employing traditional caching strategies, which require the knowledge of the objects
being transmitted. To overcome this emerging tussle between security and efficiency,
in this paper we propose an architecture in which the ISP partitions the cache space
into slices, assigns each slice to a different CP, and lets the CPs remotely manage
their slices. This architecture enables transparent caching of encrypted content and
can be deployed in the very edge of the ISP’s network i.e., base stations and femtocells,
while allowing CPs to maintain exclusive control over their content. We propose an
algorithm, called SDCP, for partitioning the cache storage into slices so as to maximize
the bandwidth savings provided by the cache. A distinctive feature of our algorithm
is that ISPs only need to measure the aggregated miss rates of each CP, but they need
not know the individual objects that are requested. We prove that the SDCP algorithm
converges to a partitioning that is close to the optimal, and we bound its optimality
gap. We use simulations to evaluate SDCP’s convergence rate under stationary and nonstationary
content popularity. Finally, we show that SDCP significantly outperforms traditional
reactive caching techniques, considering both CPs with perfect and with imperfect
knowledge of their content popularity.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {548–561},
numpages = {14}
}

@inproceedings{10.1145/3209914.3209918,
author = {Zou, Luyao and Rui, Xuhua and Nguyen, Tuan Anh and Min, Dugki and Choi, Eunmi and Thang, Tran Duc and Son, Nguyen Nhu},
title = {A Scalable Network Area Storage with Virtualization: Modelling and Evaluation Using Stochastic Reward Nets},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3209918},
doi = {10.1145/3209914.3209918},
abstract = {Modelling and analysis of storage system in data centers for availability prediction
is of paramount importance. Many studies in literature proposed different architectures
and techniques to enhance availability of the storage system. In this paper, we proposed
to incorporate virtualization techniques on a network area storage. We used stochastic
reward nets to model the system's architecture and operational scenarios. Furthermore,
we investigated various measures of interests including steady state availability,
downtime and downtime cost, and sensitivity of the system availability with respect
to impacting parameters. The analysis results show that the proposed storage system
with virtualization can obtain an acceptable level of service availability. Furthermore,
the sensitivity analysis also points out complicated dependences of service availability
upon system parameters. This paper presents a preliminary study to help guide the
development of a scalable network area storage with virtualization in practice.},
booktitle = {Proceedings of the 2018 International Conference on Information Science and System},
pages = {225–233},
numpages = {9},
keywords = {Stochastic Reward Nets, Availability, Reliability, Network Attached Storage},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@inproceedings{10.1145/3291064.3291070,
author = {Thirunavukkarasu, Gokul Sidarth and Champion, Benjamin and Horan, Ben and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
title = {IoT-Based System Health Management Infrastructure as a Service},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291070},
doi = {10.1145/3291064.3291070},
abstract = {Customization, enhanced quality of streamlined maintenance services and uplifted productivity
are some of the key highlights from the rapidly evolving concept of Industry 4.0.
IoT (Internet of things) based service infrastructure models designed for delivering
enterprise services with capabilities of pro-actively sensing malfunctions and responding
with preventive measures to streamline the automated service offered is one of the
prime application of this concept. Continuous maintenance services increase the optimum
through-life cost and in-service life cycle of the product providing the customer
with the feel of full ownership. In-service feedbacks also help the manufactures to
identify issues with respect to the designs and improve it in the future versions.
In this paper, as a proof of concept a cloud-based IoT service infrastructure for
providing real-time prognostic and supervised vehicle maintenance system is proposed.
This proposed system aims at providing an enterprise service infrastructure to the
registered vehicle service centers to keep track of the real-time vehicle diagnostic
information of their client's vehicle over cloud and use prognostic algorithms to
identify any malfunctions or abnormal behavior of the vehicles for automatically scheduling
a service appointment and automating the maintenance cycle of the vehicle. In addition
to this, the system provides features like remote supervision and diagnostics maintenance
enabling technicians to fix issues remotely, ensuring streamlined and reliable service.
Initially, before building the proposed prototype system, a few experimental trails
where conducted for analyzing the use of different IoT models used in the development
to identify the best-suited approach. The results indicated that the publisher-subscriber
(NodeJS) based model outperforms the request-response (PHP) based model in terms of
the hits per second and mean request time for an increased number of active users.
The results of the initial tests justify the reason for the using the publisher-subscriber
based IOT architecture. The conceptualized enterprise infrastructure illustrated in
the manuscript aims at providing a streamlined maintenance service.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {55–61},
numpages = {7},
keywords = {streamlined remote supervision, prognostic maintenance, vehicle diagnosis, internet of things, System health management infrastructure as a service},
location = {Singapore, Singapore},
series = {CCIOT 2018}
}

@inproceedings{10.1145/3319535.3363279,
author = {Mo, Fan and Shahin Shamsabadi, Ali and Katevas, Kleomenis and Cavallaro, Andrea and Haddadi, Hamed},
title = {Poster: Towards Characterizing and Limiting Information Exposure in DNN Layers},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363279},
doi = {10.1145/3319535.3363279},
abstract = {Pre-trained Deep Neural Network (DNN) models are increasingly used in smartphones
and other user devices to enable prediction services, leading to potential disclosures
of (sensitive) information from training data captured inside these models. Based
on the concept of generalization error, we propose a framework to measure the amount
of sensitive information memorized in each layer of a DNN. Our results show that,
when considered individually, the last layers encode a larger amount of information
from the training data compared to the first layers. We find that the same DNN architecture
trained with different datasets has similar exposure per layer. We evaluate an architecture
to protect the most sensitive layers within an on-device Trusted Execution Environment
(TEE) against potential white-box membership inference attacks without the significant
computational overhead.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2653–2655},
numpages = {3},
keywords = {trusted execution environment, privacy, deep learning, training data, sensitive information exposure},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1109/UCC.2014.167,
author = {Wagle, Shyam S.},
title = {SLA Assured Brokering (SAB) and CSP Certification in Cloud Computing},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.167},
doi = {10.1109/UCC.2014.167},
abstract = {Due to lack of information of the cloud service providers (CSPs), customers can not
easily choose services according to their requirement and due to vendor lock-in and
lack of interoperability standards among cloud service providers, customers cannot
switch the providers once services are subscribed from CSPs. Recently proposed third
party architecture which is called cloud broker can access inter-cloud and provides
services to the customers according to their requirement but providing SLA based cloud
services as per their requirement is still missing in current researches. In our work,
we propose the SLA assured brokering framework which matches the requirements of the
customer with SLA offered by CSPs using similarity matching algorithm and willingness
to pay capacity for the services. It also measures the services offered by CSPs for
certifying and ranking the CSPs.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {1016–1017},
numpages = {2},
keywords = {SLA, Cloud Brokering, Certifying, Similarity Matching},
series = {UCC '14}
}

@inproceedings{10.1145/2933349.2933359,
author = {Sirin, Utku and Appuswamy, Raja and Ailamaki, Anastasia},
title = {OLTP on a Server-Grade ARM: Power, Throughput and Latency Comparison},
year = {2016},
isbn = {9781450343190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933349.2933359},
doi = {10.1145/2933349.2933359},
abstract = {Although scaling out of low-power cores is an alternative to power-hungry Intel Xeon
processors for reducing the power overheads, they have proven inadequate for complex,
non-parallelizable workloads. On the other hand, by the introduction of the 64-bit
ARMv8 architecture, traditionally low power ARM processors have become powerful enough
to run computationally intensive server-class applications.In this study, we compare
a high-performance Intel x86 processor with a commercial implementation of the ARM
Cortex-A57. We measure the power used, throughput delivered and latency quantified
when running OLTP workloads. Our results show that the ARM processor consumes 3 to
15 times less power than the x86, while penalizing OLTP throughput by a much lower
factor (1.7 to 3). As a result, the significant power savings deliver up to 9 times
higher energy efficiency. The x86's heavily optimized power-hungry micro-architectural
structures contribute to throughput only marginally. As a result, the x86 wastes power
when utilization is low, while lightweight ARM processor consumes only as much power
as it is utilized, achieving energy proportionality. On the other hand, ARM's quantified
latency can be up to 11x higher than x86 towards to the tail of latency distribution,
making x86 more suitable for certain type of service-level agreements.},
booktitle = {Proceedings of the 12th International Workshop on Data Management on New Hardware},
articleno = {10},
numpages = {7},
location = {San Francisco, California},
series = {DaMoN '16}
}

@inproceedings{10.1145/3291533.3291540,
author = {Dalgkitsis, Anestis and Louta, Malamati and Karetsos, George T.},
title = {Traffic Forecasting in Cellular Networks Using the LSTM RNN},
year = {2018},
isbn = {9781450366106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291533.3291540},
doi = {10.1145/3291533.3291540},
abstract = {In this work we design and implement a Neural Network that can identify recurrent
patterns in various metrics which can be then used for cellular network traffic forecasting.
Based on a custom architecture and memory, this Neural Network can handle prediction
tasks faster and more accurately in real life scenarios. This approach offers a solution
for service providers to enhance cellular network performance, by utilizing effectively
the available resources. In order to provide a robust conclusion about the performance
and precision of the proposed Neural Network, multiple predictions were made using
the same data-set and the results were compared against other similar algorithms from
the literature.},
booktitle = {Proceedings of the 22nd Pan-Hellenic Conference on Informatics},
pages = {28–33},
numpages = {6},
keywords = {long-short term memory, cellular networks, traffic forecasting, recurrent neural networks},
location = {Athens, Greece},
series = {PCI '18}
}

@inproceedings{10.1145/3240508.3240642,
author = {Pang, Haitian and Zhang, Cong and Wang, Fangxin and Hu, Han and Wang, Zhi and Liu, Jiangchuan and Sun, Lifeng},
title = {Optimizing Personalized Interaction Experience in Crowd-Interactive Livecast: A Cloud-Edge Approach},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240642},
doi = {10.1145/3240508.3240642},
abstract = {Enabling users to interact with broadcasters and audience, the crowd-interactive livecast
greatly improves viewer's quality of experience (QoE) and attracts millions of daily
active users recently. In addition to striking the balance between resource utilization
and viewers' QoE met in the traditional video streaming service, this novel service
needs to take supererogatory efforts to improve the interaction QoE, which reflects
the viewer interaction experience. To tackle this issue, we conduct measurement studies
over a large-scale dataset crawled from a representative livecast service provider.
We observe that the individual's interaction pattern is quite heterogeneous: only
10% viewers proactively participate in the interaction, and the rest viewers usually
watch passively. Incorporating the insight into the emerging cloud-edge architecture,
we propose a framework PIECE, which optimizes the Personalized Interaction Experience
with Cloud-Edge architecture (PIECE) for intelligent user access control and livecast
distribution. In particular, we first devise a novel deep neural network based algorithm
to predict users' interaction intensity using the historical viewer pattern. We then
design an algorithm to maximize the individual's QoE, by strategically matching viewer
sessions and transcoding-delivery paths over cloud-edge infrastructure. Finally, we
use trace-driven experiments to verify the effectiveness of PIECE. Our results show
that our prediction algorithm outperforms the state-of-the-art algorithms with a much
smaller mean absolute error (40% reduction). Furthermore, in comparison with the cloud-based
video delivery strategy, the proposed framework can simultaneously improve the average
viewers QoE (26% improvement) and interaction QoE (21% improvement), while maintaining
a high streaming bitrate.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1217–1225},
numpages = {9},
keywords = {cloud-edge, interactive live streaming, viewer interaction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/2966986.2980075,
author = {Chang, Wanli and Roy, Debayan and Zhang, Licong and Chakraborty, Samarjit},
title = {Model-Based Design of Resource-Efficient Automotive Control Software},
year = {2016},
isbn = {9781450344661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2966986.2980075},
doi = {10.1145/2966986.2980075},
abstract = {Automotive platforms today run hundreds of millions of lines of software code implementing
a large number of different control applications spanning across safety-critical functionality
to driver assistance and comfort-related functions. While such control software today
is largely designed following model-based approaches, the underlying models do not
take into account the details of the implementation platforms, on which the software
would eventually run. Following the state-of-the-art in control theory, the focus
in such design is restricted to ensuring the stability of the designed controllers
and meeting control performance objectives, such as settling time or peak overshoot.
However, automotive platforms are highly cost-sensitive and the issue of designing
"resource-efficient" controllers has largely been ignored so far and is addressed
using very ad hoc techniques. In this paper, we will illustrate how, following traditional
embedded systems design oriented thinking, computation, communication and memory issues
can be incorporated in the controller design stage, thereby resulting in control software
not only satisfying the usual control performance metrics but also making efficient
utilization of the resources on distributed automotive architectures.},
booktitle = {Proceedings of the 35th International Conference on Computer-Aided Design},
articleno = {34},
numpages = {8},
location = {Austin, Texas},
series = {ICCAD '16}
}

@article{10.1145/2845082,
author = {\"{A}ij\"{o}, Tomi and J\"{a}\"{a}skel\"{a}inen, Pekka and Elomaa, Tapio and Kultala, Heikki and Takala, Jarmo},
title = {Integer Linear Programming-Based Scheduling for Transport Triggered Architectures},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2845082},
doi = {10.1145/2845082},
abstract = {Static multi-issue machines, such as traditional Very Long Instructional Word (VLIW)
architectures, move complexity from the hardware to the compiler. This is motivated
by the ability to support high degrees of instruction-level parallelism without requiring
complicated scheduling logic in the processor hardware. The simpler-control hardware
results in reduced area and power consumption, but leads to a challenge of engineering
a compiler with good code-generation quality.Transport triggered architectures (TTA),
and other so-called exposed datapath architectures, take the compiler-oriented philosophy
even further by pushing more details of the datapath under software control. The main
benefit of this is the reduced register file pressure, with a drawback of adding even
more complexity to the compiler side.In this article, we propose an Integer Linear
Programming (ILP)-based instruction scheduling model for TTAs. The model describes
the architecture characteristics, the particular processor resource constraints, and
the operation dependencies of the scheduled program. The model is validated and measured
by compiling application kernels to various TTAs with a different number of datapath
components and connectivity. In the best case, the cycle count is reduced to 52% when
compared to a heuristic scheduler. In addition to producing shorter schedules, the
number of register accesses in the compiled programs is generally notably less than
those with the heuristic scheduler; in the best case, the ILP scheduler reduced the
number of register file reads to 33% of the heuristic results and register file writes
to 18%. On the other hand, as expected, the ILP-based scheduler uses distinctly more
time to produce a schedule than the heuristic scheduler, but the compilation time
is within tolerable limits for production-code generation.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {59},
numpages = {22},
keywords = {Code generation, transport triggered architectures, instruction-level parallelism, integer linear programming, exposed datapath}
}

@inproceedings{10.1145/3422392.3422501,
author = {Barros, Daniel D. R. and Horita, Fl\'{a}vio and Fantinato, Denis G.},
title = {Data Mining Tool to Discover DevOps Trends from Public Repositories: Predicting Release Candidates with Gthbmining.Rc},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422501},
doi = {10.1145/3422392.3422501},
abstract = {Public repositories have been performing an essential role in bringing software and
services to technical communities and general users. Most of the cases, public repositories
have a DevOps tool, with a live and historical database behind it, to support delivering
and all steps this software or service should adopt before going to production. This
paper introduces gthbmining, a data mining set of tools to discover DevOps trends
from public repositories, and presents the module gthbmining.rc. Considering the premise
of a GitHub public repository, the main contribution here is predicting release candidates,
an important label a software release has. The methodology, architecture, components
and interfaces are explained, as well as potential users. The results show a reliable
and flexible tool, as classifiers metrics and graphics are provided, along with the
possibility to add new data mining algorithms in the open source module presented.
Related works are also supplied, and a conclusion shows the outcomes gthbmining.rc
can provide.},
booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
pages = {658–663},
numpages = {6},
keywords = {Data Mining, GitHub Mining Tool, DevOps, Release Candidate},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3308558.3313551,
author = {Yoon, Changhoon and Kim, Kwanwoo and Kim, Yongdae and Shin, Seungwon and Son, Sooel},
title = {Doppelg\"{a}Ngers on the Dark Web: A Large-Scale Assessment on Phishing Hidden Web Services},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313551},
doi = {10.1145/3308558.3313551},
abstract = {Anonymous network services on the World Wide Web have emerged as a new web architecture,
called the Dark Web. The Dark Web has been notorious for harboring cybercriminals
abusing anonymity. At the same time, the Dark Web has been a last resort for people
who seek freedom of the press as well as avoid censorship. This anonymous nature allows
website operators to conceal their identity and thereby leads users to have difficulties
in determining the authenticity of websites. Phishers abuse this perplexing authenticity
to lure victims; however, only a little is known about the prevalence of phishing
attacks on the Dark Web. We conducted an in-depth measurement study to demystify the
prevalent phishing websites on the Dark Web. We analyzed the text content of 28,928
HTTP Tor hidden services hosting 21 million dark webpages and confirmed 901 phishing
domains. We also discovered a trend on the Dark Web in which service providers perceive
dark web domains as their service brands. This trend exacerbates the risk of phishing
for their service users who remember only a partial Tor hidden service address. Our
work facilitates a better understanding of the phishing risks on the Dark Web and
encourages further research on establishing an authentic and reliable service on the
Dark Web.},
booktitle = {The World Wide Web Conference},
pages = {2225–2235},
numpages = {11},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3276774.3276777,
author = {Coffman, Austin R. and Bu\v{s}i\'{c}, Ana and Barooah, Prabir},
title = {Virtual Energy Storage from TCLs Using QoS Preserving Local Randomized Control},
year = {2018},
isbn = {9781450359511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276774.3276777},
doi = {10.1145/3276774.3276777},
abstract = {We propose a control architecture for distributed coordination of a collection of
on/off TCLs (thermostatically controlled loads), such as residential air conditioners,
to provide the same service to the power grid as a large battery. This involves a
collection of loads to coordinate their on/off decisions so that the aggregate power
consumption profile tracks a grid-supplied reference. A key constraint is to maintain
each consumer's quality of service (QoS). Recent works have proposed randomization
at the loads. Thermostats at the loads are replaced by a randomized controller, and
the grid broadcasts a scalar to all loads, which tunes the probability of turning
on or off at each load depending on its state. In this paper we propose a modification
of a previous design by Meyn and Bu\v{s}i\'{c}. The previous design by Meyn and Bu\v{s}i\'{c} ensures
that the indoor temperature remains within a pre-specified bound, but other QoS metrics,
especially the frequency of turning on and off was not limited. The controller we
propose can be tuned to reduce the cycling rate of a TCL to any desired degree. The
proposed design is compared against the design by Meyn and Bu\v{s}i\'{c} and another well
cited design in the literature on control of TCL populations, by Mathieu et al. We
show through simulations that the proposed controller is able to reduce the cycling
of individual ACs compared to the previous designs with little loss in tracking of
the grid-supplied reference signal.},
booktitle = {Proceedings of the 5th Conference on Systems for Built Environments},
pages = {93–102},
numpages = {10},
keywords = {randomized control, virtual energy storage, demand response, distributed control},
location = {Shenzen, China},
series = {BuildSys '18}
}

@article{10.1145/3340290,
author = {Wang, Ji and Bao, Weidong and Zheng, Lei and Zhu, Xiaomin and Yu, Philip S.},
title = {An Attention-Augmented Deep Architecture for Hard Drive Status Monitoring in Large-Scale Storage Systems},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/3340290},
doi = {10.1145/3340290},
abstract = {Data centers equipped with large-scale storage systems are critical infrastructures
in the era of big data. The enormous amount of hard drives in storage systems magnify
the failure probability, which may cause tremendous loss for both data service users
and providers. Despite a set of reactive fault-tolerant measures such as RAID, it
is still a tough issue to enhance the reliability of large-scale storage systems.
Proactive prediction is an effective method to avoid possible hard-drive failures
in advance. A series of models based on the SMART statistics have been proposed to
predict impending hard-drive failures. Nonetheless, there remain some serious yet
unsolved challenges like the lack of explainability of prediction results. To address
these issues, we carefully analyze a dataset collected from a real-world large-scale
storage system and then design an attention-augmented deep architecture for hard-drive
health status assessment and failure prediction. The deep architecture, composed of
a feature integration layer, a temporal dependency extraction layer, an attention
layer, and a classification layer, cannot only monitor the status of hard drives but
also assist in failure cause diagnoses. The experiments based on real-world datasets
show that the proposed deep architecture is able to assess the hard-drive status and
predict the impending failures accurately. In addition, the experimental results demonstrate
that the attention-augmented deep architecture can reveal the degradation progression
of hard drives automatically and assist administrators in tracing the cause of hard
drive failures.},
journal = {ACM Trans. Storage},
month = aug,
articleno = {21},
numpages = {26},
keywords = {SMART, recurrent neural network, attention mechanism, Hard drive failure, deep neural network}
}

@inproceedings{10.1145/2668322.2668327,
author = {Papalambrou, Andreas and Stefanidis, Kyriakos and Gialelis, John and Serpanos, Dimitrios},
title = {Detection, Traceback and Filtering of Denial of Service Attacks in Networked Embedded Systems},
year = {2014},
isbn = {9781450329323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668322.2668327},
doi = {10.1145/2668322.2668327},
abstract = {This work presents a composite scheme for detection, traceback and filtering of distributed
denial of service (DDoS) attacks in networked embedded systems. A method based on
algorithmic analysis of various node and network parameters is used to detect attacks
while a packet marking method is used to mitigate the effects of the attack by filtering
the incoming traffic that is part of this attack and trace back to the origin of the
attack. The combination of the detection and mitigation methods provide an increased
level of security in comparison to approaches based on a single method. Furthermore,
the scheme is developed in a way to comply with the novel SHIELD secure architecture
being developed, which aims at providing interoperability with other secure components
as well as metrics to quantify their security properties.},
booktitle = {Proceedings of the 9th Workshop on Embedded Systems Security},
articleno = {5},
numpages = {8},
keywords = {embedded systems security, denial of service attacks},
location = {New Delhi, India},
series = {WESS '14}
}

@article{10.1145/3014431,
author = {Hu, Han and Wen, Yonggang and Chua, Tat-Seng and Li, Xuelong},
title = {Cost-Optimized Microblog Distribution over Geo-Distributed Data Centers: Insights from Cross-Media Analysis},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3014431},
doi = {10.1145/3014431},
abstract = {The unprecedent growth of microblog services poses significant challenges on network
traffic and service latency to the underlay infrastructure (i.e., geo-distributed
data centers). Furthermore, the dynamic evolution in microblog status generates a
huge workload on data consistence maintenance. In this article, motivated by insights
of cross-media analysis-based propagation patterns, we propose a novel cache strategy
for microblog service systems to reduce the inter-data center traffic and consistence
maintenance cost, while achieving low service latency. Specifically, we first present
a microblog classification method, which utilizes the external knowledge from correlated
domains, to categorize microblogs. Then we conduct a large-scale measurement on a
representative online social network system to study the category-based propagation
diversity on region and time scales. These insights illustrate social common habits
on creating and consuming microblogs and further motivate our architecture design.
Finally, we formulate the content cache problem as a constrained optimization problem.
By jointly using the Lyapunov optimization framework and simplex gradient method,
we find the optimal online control strategy. Extensive trace-driven experiments further
demonstrate that our algorithm reduces the system cost by 24.5% against traditional
approaches with the same service latency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {40},
numpages = {18},
keywords = {cross-media analysis, performance optimization, Social media analytics, data center}
}

@inproceedings{10.1145/3473714.3473742,
author = {Xin, An},
title = {Research on Multi-Sensor Fusion Perception Method of Vehicle-Infrastructure Collaboration for Smart Automobiles},
year = {2021},
isbn = {9781450390231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473714.3473742},
doi = {10.1145/3473714.3473742},
abstract = {Traffic congestion should be solved, it reduce traffic safety potential risks, and
improve people's travel efficiency. The paper based on intelligent car's own operation
characteristics (Smart car perception is generally only 300 ~ 500 meters, there is
a perceived problem of super-visual distance and vision blind zone), In order to effectively
improve the safety operation level of smart cars, and combined with intelligent non-smart
cars on the road may encounter cars a smart car trip perception highway outside the
scope of frequently asked questions, such as over-site sensations, the whole scene
and area's perception, the blind area, the emergency corner, tunnel, bridge, other
highways travel common scenes and so on. This paper is based on new infrastructure
transformations or newly build's research and practical results such as road traffic
intelligence infrastructure, it deployed the current road traffic to intelligently
infrastructure, especially the technical difficulties existing in the process of common
perceptual equipment (smart cameras, radar) are synonymous with multiple sensor information
fusion perceptions, it proposed a multi-sensor fusion algorithm based on error variance,
and designed a multi-object multi-sensor data processing system architecture. This
paper also proposes traffic operation scheduling architecture based on the game theory
of car road synergies on the basis of multi-sensor data fusion. Finally, these architectures
were analyzed using computer simulation techniques. The results show that the traffic
operation schedule for multi-sensor fusion algorithm based on error variance and game
theory based on the study proposed this study can be more obvious. The safety and
efficiency of the road traffic environment of smart vehicles. Optimization, smart
vehicles equipped with smart vehicles are also more than 25% higher than traditional
common vehicles in terms of vehicle safety. All in all, this study proposed to synergistic
multi-sensor convergence method for smart cars, that based on smart car a smart car
perception, compared to non-smart roads after intelligent infrastructure construction
and transformation of road traffic intelligent transportation systems, Higher efficiency
and more intelligent can better solve the common problems in road traffic environment,
providing people with safer, efficient and high-quality traffic travel services.},
booktitle = {Proceedings of the 2021 International Conference on Control and Intelligent Robotics},
pages = {164–175},
numpages = {12},
keywords = {vehicle-infrastructure collaboration, perceptual method research, smart vehicle, multi-sensor fusion},
location = {Guangzhou, China},
series = {ICCIR 2021}
}

@inproceedings{10.1145/2809826.2809836,
author = {Khan, Yasir Imtiaz and Al-shaer, Ehab and Rauf, Usman},
title = {Cyber Resilience-by-Construction: Modeling, Measuring &amp; Verifying},
year = {2015},
isbn = {9781450338219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809826.2809836},
doi = {10.1145/2809826.2809836},
abstract = {The need of cyber security is increasing as cyber attacks are escalating day by day.
Cyber attacks are now so many and sophisticated that many will unavoidably get through.
Therefore, there is an immense need to employ resilient architectures to defend known
or unknown threats. Engineer- ing resilient system/infrastructure is a challenging
task, that implies how to measure the resilience and how to obtain sufficient resilience
necessary to maintain its service delivery under diverse situations. This paper has
two fold objective, the first is to propose a formal approach to measure cyber resilience
from different aspects (i.e., attacks, failures) and at different levels (i.e., pro-active,
resistive and reactive). To achieve the first objective, we propose a formal frame-
work named as: Cyber Resilience Engineering Framework (CREF). The second objective
is to build a resilient system by construction. The idea is to build a formal model
of a cyber system, which is initially not resilient with respect to attacks. Then
by systematic refinements of the formal model and by its model checking, we attain
resiliency. We exemplify our technique through the case study of simple cyber security
device (i.e., network firewall).},
booktitle = {Proceedings of the 2015 Workshop on Automated Decision Making for Active Cyber Defense},
pages = {9–14},
numpages = {6},
keywords = {model checking, algebraic petri nets, firewall, cyber resilience},
location = {Denver, Colorado, USA},
series = {SafeConfig '15}
}

@inproceedings{10.1145/3019612.3019724,
author = {Lautner, Douglas and Hua, Xiayu and Debates, Scott and Song, Miao and Shah, Jagat and Ren, Shangping},
title = {BaaS (Bluetooth-as-a-Sensor): Conception, Design and Implementation on Mobile Platforms},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019724},
doi = {10.1145/3019612.3019724},
abstract = {As network connectivity becomes more capable, mobile devices are evolving into sensory
data accumulators. Bluetooth (BT) components, which are widely used for communication
purposes, also have the potential to become contextual sensors by constantly listening
to information broadcast by nearby Bluetooth Low Energy (BLE) beacons. Compared to
traditional Micro-Electro-Mechanical (MEMs) based contextual sensors, Bluetooth-as-a-Sensor
(BaaS) provides a wider sensing spectrum and more comprehensive environmental information.
However, current implementations of BT are optimized as a data transmitter, therefore
deploying BaaS on a traditional mobile platform would cause an unacceptably high current
drain and hence a significant reduction in battery life. Our objective is to conquer
the current drain problem associated with having continuous wireless BT sensing. We
provide a novel BaaS-based architecture which utilizes an energy-efficient sensor
fusion core (SFC) to execute heavy-duty and long-standing tasks. We also present an
optimized duty cycle algorithm that minimizes the duty cycle while guaranteeing an
application's QoS requirements. Both BaaS architecture and algorithm are implemented
and deployed on a Moto X platform and then applied to an indoor location service for
consumer use validation. The performance of the BaaS-based architecture is evaluated
for both average current drain and location accuracy. Data measured on Moto X shows
that when using the BaaS architecture, the battery life is 5 times longer than using
the traditional BT architecture.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {550–556},
numpages = {7},
keywords = {mobile sensing, mobile device, energy efficiency, embedded system, cellphone development},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@article{10.1145/2660768,
author = {Kim, Lok-Won and Lee, Dong-U and Villasenor, John},
title = {Automated Iterative Pipelining for ASIC Design},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/2660768},
doi = {10.1145/2660768},
abstract = {We describe an automated pipelining approach for optimally balanced pipeline implementation
that achieves low area cost as well as meeting timing requirements. Most previous
automatic pipelining methods have focused on Instruction Set Architecture (ISA)-based
designs and the main goal of such methods generally has been maximizing performance
as measured in terms of instructions per clock (IPC). By contrast, we focus on datapath-oriented
designs (e.g., DSP filters for image or communication processing applications) in
ASIC design flows. The goal of the proposed pipelining approach is to find the optimally
pipelined design that not only meets the user-specified target clock frequency, but
also seeks to minimize area cost of a given design. Unlike most previous approaches,
the proposed methods incorporate the use of accurate area and timing information (iteratively
achieved by synthesizing every interim pipelined design) to achieve higher accuracy
during design exploration. When compared with exhaustive design exploration that considers
all possible pipeline patterns, the two heuristic pipelining methods presented here
involve only a small area penalty (typically under 5%) while offering dramatically
reduced computational complexity. Experimental validation is performed with commercial
ASIC design tools and described for applications including polynomial function evaluation,
FIR filters, matrix multiplication, and discrete wavelet transform filter designs
with a 90nm standard cell library.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = mar,
articleno = {28},
numpages = {24},
keywords = {pipelined hardware architecture, pipelining, ASIC designs, design area optimization, Timing error resolution}
}

@inproceedings{10.1145/3414045.3415941,
author = {Wazid, Mohammad and Bera, Basudeb and Mitra, Ankush and Das, Ashok Kumar and Ali, Rashid},
title = {Private Blockchain-Envisioned Security Framework for AI-Enabled IoT-Based Drone-Aided Healthcare Services},
year = {2020},
isbn = {9781450381055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414045.3415941},
doi = {10.1145/3414045.3415941},
abstract = {Internet of Drones (IoD) architecture is designed to support a co-ordinated access
for the airspace using the unmanned aerial vehicles (UAVs) known as drones. Recently,
IoD communication environment is extremely useful for various applications in our
daily activities. Artificial intelligence (AI)-enabled Internet of Things (IoT)-based
drone-aided healthcare service is a specialized environment which can be used for
different types of tasks, for instance, blood and urine samples collections, medicine
delivery and for the delivery of other medical needs including the current pandemic
of COVID-19. Due to wireless nature of communication among the deployed drones and
their ground station server, several attacks (for example, replay, man-in-the-middle,
impersonation and privileged-insider attacks) can be easily mounted by malicious attackers.
To protect such attacks, the deployment of effective authentication, access control
and key management schemes are extremely important in the IoD environment. Furthermore,
combining the blockchain mechanism with deployed authentication make it more robust
against various types of attacks. To mitigate such issues, we propose a private-blockchain
based framework for secure communication in an IoT-enabled drone-aided healthcare
environment. The blockchain-based simulation of the proposed framework has been carried
out to measure its impact on various performance parameters.},
booktitle = {Proceedings of the 2nd ACM MobiCom Workshop on Drone Assisted Wireless Communications for 5G and Beyond},
pages = {37–42},
numpages = {6},
keywords = {authentication, security, internet of drones (IoD), blockchain, privacy, healthcare},
location = {London, United Kingdom},
series = {DroneCom '20}
}

@inproceedings{10.1145/3219104.3219148,
author = {Ruan, Guangchen and Wernert, Eric and Gniady, Tassie and Tuna, Esen and Sherman, William},
title = {High Performance Photogrammetry for Academic Research},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3219148},
doi = {10.1145/3219104.3219148},
abstract = {Photogrammetry is the process of computationally extracting a three-dimensional surface
model from a set of two-dimensional photographs of an object or environment. It is
used to build models of everything from terrains to statues to ancient artifacts.
In the past, the computational process was done on powerful PCs and could take weeks
for large datasets. Even relatively small objects often required many hours of compute
time to stitch together. With the availability of parallel processing options in the
latest release of state-of-the-art photogrammetry software, it is possible to leverage
the power of high performance computing systems on large datasets. In this paper we
present a particular implementation of a high performance photogrammetry service.
Though the service is currently based on a specific software package (Agisoft's PhotoScan),
our system architecture is designed around a general photogrammetry process that can
be easily adapted to leverage other photogrammetry tools. In addition, we report on
an extensive performance study that measured the relative impacts of dataset size,
software quality settings, and processing cluster size. Furthermore, we share lessons
learned that are useful to system administrators looking to establish a similar service,
and we describe the user-facing support components that are crucial for the success
of the service.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {45},
numpages = {8},
keywords = {HPC, photogrammetry, benchmarking, distributed processing, scalability, performance evaluation},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.1109/CCGRID.2017.27,
author = {Colmant, Maxime and Felber, Pascal and Rouvoy, Romain and Seinturier, Lionel},
title = {WattsKit: Software-Defined Power Monitoring of Distributed Systems},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.27},
doi = {10.1109/CCGRID.2017.27},
abstract = {The design and the deployment of energy-efficient distributed systems is a challenging
task, which requires software engineers to consider all the layers of a system, from
hardware to software. In particular, monitoring and analyzing the power consumption
of a distributed system spanning several---potentially heterogeneous---nodes becomes
particularly tedious when aiming at a finer granularity than observing the power consumption
of hosting nodes. While the state-of-the-art in software-defined power meters fails
to deliver adaptive solutions to offer such service-level perspective and to cope
with the diversity of hardware CPU architectures, this paper proposes to automatically
learn the power models of the nodes supporting a distributed system, and then to use
these inferred power models to better understand how the power consumption of the
system's processes is distributed across nodes at runtime.Our solution, named WattsKit,
offers a modular toolkit to build software-defined power meters "\`{a} la carte", thus
dealing with the diversity of user and hardware requirements. Beyond the demonstrated
capability of covering a wide diversity of CPU architectures with high accuracy, we
illustrate the benefits of adopting software-defined power meters to analyze the power
consumption of complex layered and distributed systems. In particular, we illustrate
the capability of our approach to monitor the power consumption of a system composed
of Docker Swarm, Weave,Elasticsearch, and Apache ZooKeeper. Thanks to WattsKit, developers
and administrators are now able to identify potential power leaks in their software
infrastructure.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {514–523},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.5555/2665671.2665678,
author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
title = {A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services},
year = {2014},
isbn = {9781479943944},
publisher = {IEEE Press},
abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency,
and low cost. It is challenging to improve all of these factors simultaneously. To
advance datacenter capabilities beyond what commodity server designs can provide,
we have designed and built a composable, reconfigurablefabric to accelerate portions
of large-scale software services. Each instantiation of the fabric consists of a 6x8
2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One
FPGA is placed into each server, accessible through PCIe, and wired directly to other
FPGAs with pairs of 10 Gb SAS cablesIn this paper, we describe a medium-scale deployment
of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating
the Bing web search engine. We describe the requirements and architecture of the system,
detail the critical engineering challenges and solutions needed to make the system
robust in the presence of failures, and measure the performance, power, and resilience
of the system when ranking candidate documents. Under high load, the largescale reconfigurable
fabric improves the ranking throughput of each server by a factor of 95% for a fixed
latency distribution--- or, while maintaining equivalent throughput, reduces the tail
latency by 29%},
booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
pages = {13–24},
numpages = {12},
location = {Minneapolis, Minnesota, USA},
series = {ISCA '14}
}

@article{10.1145/2678373.2665678,
author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
title = {A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2678373.2665678},
doi = {10.1145/2678373.2665678},
abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency,
and low cost. It is challenging to improve all of these factors simultaneously. To
advance datacenter capabilities beyond what commodity server designs can provide,
we have designed and built a composable, reconfigurablefabric to accelerate portions
of large-scale software services. Each instantiation of the fabric consists of a 6x8
2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One
FPGA is placed into each server, accessible through PCIe, and wired directly to other
FPGAs with pairs of 10 Gb SAS cablesIn this paper, we describe a medium-scale deployment
of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating
the Bing web search engine. We describe the requirements and architecture of the system,
detail the critical engineering challenges and solutions needed to make the system
robust in the presence of failures, and measure the performance, power, and resilience
of the system when ranking candidate documents. Under high load, the largescale reconfigurable
fabric improves the ranking throughput of each server by a factor of 95% for a fixed
latency distribution--- or, while maintaining equivalent throughput, reduces the tail
latency by 29%},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {13–24},
numpages = {12}
}

@inproceedings{10.1145/3366424.3382670,
author = {Tiwary, Mayank and Mishra, Pritish and Jain, Shashank and Puthal, Deepak},
title = {Data Aware Web-Assembly Function Placement},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382670},
doi = {10.1145/3366424.3382670},
abstract = {Existing container based serverless computing systems are limited by cold-start problems
and complex architecture for stateful services, multi-tenancy, etc. This paper presents
serverless functions to be placed as per data locality and executed as a web-assembly
sandbox, which results better execution latency and reduced network usage as compared
to the existing architectures. The designed serverless runtime features resource isolation
in terms of CPU, Memory, and file-system isolation and falicitates multi-tenancy executions.
The proposed architecture is evaluated using IoT workloads with different performance
metrics.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {4–5},
numpages = {2},
keywords = {Multi-Tenancy, Servelress, Data Locality, Web-Assembly},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3302541.3310294,
author = {Scheuner, Joel and Leitner, Philipp},
title = {Performance Benchmarking of Infrastructure-as-a-Service (IaaS) Clouds with Cloud WorkBench},
year = {2019},
isbn = {9781450362863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302541.3310294},
doi = {10.1145/3302541.3310294},
abstract = {The continuing growth of the cloud computing market has led to an unprecedented diversity
of cloud services with different performance characteristics. To support service selection,
researchers and practitioners conduct cloud performance benchmarking by measuring
and objectively comparing the performance of different providers and configurations
(e.g., instance types in different data center regions). In this tutorial, we demonstrate
how to write performance tests for IaaS clouds using the Web-based benchmarking tool
Cloud WorkBench (CWB). We will motivate and introduce benchmarking of IaaS cloud in
general, demonstrate the execution of a simple benchmark in a public cloud environment,
summarize the CWB tool architecture, and interactively develop and deploy a more advanced
benchmark together with the participants.},
booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {53–56},
numpages = {4},
keywords = {performance, benchmarking, cloud computing},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.5555/3195638.3195647,
author = {Caulfield, Adrian M. and Chung, Eric S. and Putnam, Andrew and Angepat, Hari and Fowers, Jeremy and Haselman, Michael and Heil, Stephen and Humphrey, Matt and Kaur, Puneet and Kim, Joo-Young and Lo, Daniel and Massengill, Todd and Ovtcharov, Kalin and Papamichael, Michael and Woods, Lisa and Lanka, Sitaram and Chiou, Derek and Burger, Doug},
title = {A Cloud-Scale Acceleration Architecture},
year = {2016},
publisher = {IEEE Press},
abstract = {Hyperscale datacenter providers have struggled to balance the growing need for specialized
hardware (efficiency) with the economic benefits of homogeneity (manageability). In
this paper we propose a new cloud architecture that uses reconfigurable logic to accelerate
both network plane functions and applications. This Configurable Cloud architecture
places a layer of reconfigurable logic (FPGAs) between the network switches and the
servers, enabling network flows to be programmably transformed at line rate, enabling
acceleration of local applications running on the server, and enabling the FPGAs to
communicate directly, at datacenter scale, to harvest remote FPGAs unused by their
local servers. We deployed this design over a production server bed, and show how
it can be used for both service acceleration (Web search ranking) and network acceleration
(encryption of data in transit at high-speeds). This architecture is much more scalable
than prior work which used secondary rack-scale networks for inter-FPGA communication.
By coupling to the network plane, direct FPGA-to-FPGA messages can be achieved at
comparable latency to previous work, without the secondary network. Additionally,
the scale of direct inter-FPGA messaging is much larger. The average round-trip latencies
observed in our measurements among 24, 1000, and 250,000 machines are under 3, 9,
and 20 microseconds, respectively. The Configurable Cloud architecture has been deployed
at hyperscale in Microsoft's production datacenters worldwide.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {7},
numpages = {13},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@article{10.1109/TNET.2016.2621159,
author = {Chen, Min and Chen, Shigang and Cai, Zhiping},
title = {Counter Tree: A Scalable Counter Architecture for Per-Flow Traffic Measurement},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2621159},
doi = {10.1109/TNET.2016.2621159},
abstract = {Per-flow traffic measurement, which is to count the number of packets for each active
flow during a certain measurement period, has many applications in traffic engineering,
classification of routing distribution or network usage pattern, service provision,
anomaly detection, and network forensics. In order to keep up with the high throughput
of modern routers or switches, the online module for per-flow traffic measurement
should use high-bandwidth SRAM that allows fast memory accesses. Due to limited SRAM
space, exact counting, which requires to keep a counter for each flow, does not scale
to large networks consisting of numerous flows. Some recent work takes a different
approach to estimate the flow sizes using counter architectures that can fit into
tight SRAM. However, existing counter architectures have limitations, either still
requiring considerable SRAM space or having a small estimation range. In this paper,
we design a scalable counter architecture called Counter Tree, which leverages a 2-D
counter sharing scheme to achieve far better memory efficiency and in the meantime
extend estimation range significantly. Furthermore, we improve the performance of
Counter Tree by adding a status bit to each counter. Extensive experiments with real
network traces demonstrate that our counter architecture can produce accurate estimates
for flows of all sizes under very tight memory space.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {1249–1262},
numpages = {14}
}

@inproceedings{10.5555/2840819.2840915,
author = {Kim, Yeseong and Imani, Mohsen and Patil, Shruti and Rosing, Tajana S.},
title = {CAUSE: Critical Application Usage-Aware Memory System Using Non-Volatile Memory for Mobile Devices},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {Mobile devices are severely limited in memory, which affects critical user-experience
metrics such as application service time. Emerging non-volatile memory (NVM) technologies
such as STT-RAM and PCM are ideal candidates to provide higher memory capacity with
negligible energy overhead. However, existing memory management systems overlook mobile
users application usage which provides crucial cues for improving user experience.
In this paper, we propose CAUSE, a novel memory system based on DRAM-NVM hybrid memory
architecture. CAUSE takes explicit account of the application usage patterns to distinguish
data criticality and identify suitable swap candidates. We also devise NVM hardware
design optimized for the access characteristics of the swapped pages. We evaluate
CAUSE on a real Android smartphone and NVSim simulator using user application usage
logs. Our experimental results show that the proposed technique achieves 32% faster
launch time for mobile applications while reducing energy cost by 90% and 44% on average
over non-optimized STT-RAM and PCM, respectively.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {690–696},
numpages = {7},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1145/2628588.2628598,
author = {Sharakhov, Nikita and Marojevic, Vuk and Romano, Ferdinando and Polys, Nicholas and Dietrich, Carl},
title = {Visualizing Real-Time Radio Spectrum Access with CORNET3D},
year = {2014},
isbn = {9781450330152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628588.2628598},
doi = {10.1145/2628588.2628598},
abstract = {Modern web technology enables the 3D portrayal of real-time data. WebSocket connections
provide data over the web without the time-consuming overhead of HTTP requests. The
server-side "push" paradigm is particularly useful for creating novel tools such as
CORNET3D, where real-time 3D visualization is required. CORNET3D is an innovative
Web3D interface to a research and education test bed for Dynamic Spectrum Access (DSA).
Our system can drive several 2D and 3D portrayals of spectral data and radio performance
metrics from a live, online system. The testbed can further integrate the data portrayals
into a multi-user "serious game" to teach students about strategies for the optimal
use of spectrum resources by providing them with real-time scoring based on their
choices of radio transmission parameters. This paper describes the web service architecture
and Webd3D front end for our DSA testbed, detailing new methods for spectrum visualization
and the applications they enable.},
booktitle = {Proceedings of the 19th International ACM Conference on 3D Web Technologies},
pages = {109–116},
numpages = {8},
keywords = {HTML5, web applications, computer graphics, WebSockets, WebGL},
location = {Vancouver, British Columbia, Canada},
series = {Web3D '14}
}

@inproceedings{10.1145/3297663.3309668,
author = {Talreja, Disha and Lahiri, Kanishka and Kalambur, Subramaniam and Raghavendra, Prakash},
title = {Performance Scaling of Cassandra on High-Thread Count Servers},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3309668},
doi = {10.1145/3297663.3309668},
abstract = {NoSQL databases are commonly used today in cloud deployments due to their ability
to "scale-out" and effectively use distributed computing resources in a data center.
At the same time, cloud servers are also witnessing rapid growth in CPU core counts,
memory bandwidth, and memory capacity. Hence, apart from scaling out effectively,
it's important to consider how such workloads "scale-up" within a single system, so
that they can make the best use of available resources. In this paper, we describe
our experiences studying the performance scaling characteristics of Cassandra, a popular
open-source, column-oriented database, on a single high-thread count dual socket server.
We demonstrate that using commonly used benchmarking practices, Cassandra does not
scale well on such systems. Next, we show how by taking into account specific knowledge
of the underlying topology of the server architecture, we can achieve substantial
improvements in performance scalability. We report on how, during the course of our
work, we uncovered an area for performance improvement in the official open-source
implementation of the Java platform with respect to NUMA awareness. We show how optimizing
this resulted in 27% throughput gain for Cassandra under studied configurations. As
a result of these optimizations, using standard workload generators, we obtained up
to 1.44x and 2.55x improvements in Cassandra throughput over baseline single and dual-socket
performance measurements respectively. On wider testing across a variety of workloads,
we achieved excellent performance scaling, averaging 98% efficiency within a socket
and 90% efficiency at the system-level.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {179–187},
numpages = {9},
keywords = {nosql databases, performance scalability, cassandra, performance benchmarking},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3447545.3451180,
author = {Klaver, Luuk and van der Knaap, Thijs and van der Geest, Johan and Harmsma, Edwin and van der Waaij, Bram and Pileggi, Paolo},
title = {Towards Independent Run-Time Cloud Monitoring},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451180},
doi = {10.1145/3447545.3451180},
abstract = {Cloud computing services are integral to the digital transformation. They deliver
greater connectivity, tremendous savings, and lower total cost of ownership. Despite
such benefits and benchmarking advances, costs are still quite unpredictable, performance
is unclear, security is inconsistent, and there is minimal control over aspects like
data and service locality. Estimating performance of cloud environments is very hard
for cloud consumers. They would like to make informed decisions about which provider
better suits their needs using specialized evaluation mechanisms. Providers have their
own tools reporting specific metrics, but they are potentially biased and often incomparable
across providers. Current benchmarking tools allow comparison but consumers need more
flexibility to evaluate environments under actual operating conditions for specialized
applications. Ours is early stage work and a step towards a monitoring solution that
enables independent evaluation of clouds for very specific application needs. In this
paper, we present our initial architecture of the Cloud Monitor that aims to integrate
existing and new benchmarks in a flexible and extensible way. By way of a simplistic
demonstrator, we illustrate the concept. We report some preliminary monitoring results
after a brief time of monitoring and are able to observe unexpected anomalies. The
results suggest an independent monitoring solution is a powerful enabler of next generation
cloud computing, not only for the consumer but potentially the whole ecosystem.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {21–26},
numpages = {6},
keywords = {performance evaluation, run-time monitoring, benchmarking, cloud computing},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/2775088.2775100,
author = {You, Taewan and Martinez-Julia, Pedro and Skarmeta, Antonio and Jung, Heeyoung},
title = {Design and Deployment of Federation Testbed in EU-KR for Identifier-Based Communications},
year = {2015},
isbn = {9781450335645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2775088.2775100},
doi = {10.1145/2775088.2775100},
abstract = {SmartFIRE is the first intercontinental testbed, federating multiple small-scale testbeds
in South Korea and Europe, which exploits the benefits and building blocks of an OpenFlow-based
infrastructure. As a part of SmartFIRE, both ETRI and UMU designs and develops a federation
testbed for Identifier-based communications that all of communication services are
achieved by Identifier not by IP address. In order to manage and control the testbed,
we deploy a Measurement and Management Framework (OMF), further we will deploy SFA
aggregate manager to federate with other SmartFIRE testbed. In this paper we introduce
the federation testbed for ID-based communications including network connectivity,
architecture configuration, and federation architecture. Moreover, to exploit the
testbed, we design and implement two mobility use cases that we show seamless network
connection service under host's mobility, such as intra-domain handover and inter-domain
handover. Thus we can show result of the experimentation that the communication session
wouldn't be cut off even though communication entity moves to a different network.
Finally we refer future works for federation to cooperate with other SmartFIRE testbeds
and additional ID-based communication scenario.},
booktitle = {The 10th International Conference on Future Internet},
pages = {13–16},
numpages = {4},
keywords = {Deployment, Federation Testbed, EU, Identifier-based communications, SmartFire, KR},
location = {Seoul, Republic of Korea},
series = {CFI '15}
}

@inproceedings{10.1145/3123878.3131999,
author = {Szabo, Marton and Majdan, Andras and Pongracz, Gergely and Toka, Laszlo and Sonkoly, Balazs},
title = {Making the Data Plane Ready for NFV: An Effective Way of Handling Resources},
year = {2017},
isbn = {9781450350570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123878.3131999},
doi = {10.1145/3123878.3131999},
abstract = {In order to enable carrier grade network services constructed from software-based
network functions, we need a novel data plane supporting high performance packet processing,
low latency and flexible, fine granular programmability and control. The network functions
implemented as virtual machines or containers use the same hardware resources (cpu,
memory) as the elements responsible for networking, therefore, a low-level resource
orchestrator which is capable of jointly controlling these resources is an indispensable
component. In this demonstration, we showcase our novel resource orchestrator (FERO)
on top of a data plane making use of open-source components such as, Docker, DPDK
and OVS. It is capable of i) generating an abstract model of the underlying hardware
architecture during the bootstrap process, ii) mapping the incoming network service
requests to available resources based on our recently proposed Service Graph embedding
engine and the generated graph model. The impact of the orchestration decision is
shown on-the-fly by real-time performance measurements on a graphical dashboard.},
booktitle = {Proceedings of the SIGCOMM Posters and Demos},
pages = {97–99},
numpages = {3},
keywords = {DPDK, SFC, SDN, NFV, Docker},
location = {Los Angeles, CA, USA},
series = {SIGCOMM Posters and Demos '17}
}

@inproceedings{10.1109/MODELS-C.2019.00032,
author = {Burdusel, Alexandru and Zschaler, Steffen},
title = {Towards Scalable Search-Based Model Engineering with MDEOptimiser Scale},
year = {2019},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00032},
doi = {10.1109/MODELS-C.2019.00032},
abstract = {Running scientific experiments using search-based model engineering (SBME) tools is
a complex task, that poses a number of challenges, starting from defining an experiment
workflow, to parameter tuning, finding optimal computational resources to run on,
collecting and interpreting metrics and making the entire process easily reproducible.Despite
the proliferation of easily accessible hardware, as a result of the increased availability
of infrastructure-as-a-service providers, many SBME tools are rarely using this technology
for accelerating experimentation. Running many experiments on a single machine implies
much longer waiting times and reduces the ability to increase the speed of iterations
when doing SBME research, thus, slowing down the entire process.In this paper, we
introduce a domain-specific language (DSL) and a framework that can be used to configure
and run experiments at scale, on cloud infrastructure, in a reproducible way. We will
describe our DSL and framework architecture along with an example to showcase how
a case study can be evaluated using two different model optimisation tools.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {189–195},
numpages = {7},
keywords = {evolutionary search, workflow, reproducible research, middleware, search based model engineering, model driven engineering, cloud},
location = {Munich, Germany},
series = {MODELS '19}
}

@inproceedings{10.1145/3424978.3425039,
author = {Li, Hailing and Zhang, Xiaohang and Cao, Shoufeng and He, Longtao and Zhang, Hui},
title = {Active Measurement of Open Resolver Service Nodes},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425039},
doi = {10.1145/3424978.3425039},
abstract = {Driven by the growing number of DNS requests on the Internet, the architecture of
the recursive resolver has become more huge and complex, especially for open resolvers
that provide resolution services to the public. There are many service nodes with
different roles in the open resolver, and the nodes that directly communicate with
the authoritative server are called recursive egress nodes. This paper proposed a
distributed measurement system and performed active measurement and analysis on the
characteristics of the egress node of open resolvers collected from passive DNS traffic
and third party active scanning. The results from 65 vantage points show that (1)
most open resolvers have dozens of recursive egress nodes, and (2) most open resolvers
have deployed at least one IPv6 address egress node, while IPv4 address still dominates
the service node configuration. (3) A small amount of recursive egress nodes is reused
by a large number of open resolvers, so that a large amount of DNS request traffic
on the Internet is concentrated on limited recursive egress nodes, which will reduce
the redundancy of DNS and cause cyber security risks. (4) The median distances between
most open resolvers with multiple egress nodes and the users usually exceed 1000 kilometers,
which will bring negative effect on the scheduling accuracy of CDN.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {61},
numpages = {8},
keywords = {Open resolver, Recursive egress node, DNS redirection, Distributed active measurement},
location = {Sanya, China},
series = {CSAE 2020}
}

@proceedings{10.1145/2737182,
title = {QoSA '15: Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 11th International ACM Sigsoft Conference on the Quality of Software
Architectures -- QoSA 2015. For more than a decade, QoSA has strived to advance the
state of the art of quality aspects of software architecture, focusing broadly on
its quality characteristics and how these relate to the design of software architectures.
Specific issues of interest are defining and modeling quality measures, evaluating
and managing architecture quality, linking architecture to requirements and implementation,
and preserving architecture quality throughout the system lifetime. Past themes for
QoSA include Architecting for Adaptivity (2014), The System View (2013), Evolving
Architectures (2012), Quality throughout the Software Lifecycle (2011), and Research
into Practice -- Reality and Gaps (2010).QoSA 2015's theme is "Software Architecture
for the 4th Industrial Revolution". After mechanization, mass production, and electronics,
the Internet is about to enable a new level of productivity in manufacturing. This
shall be enabled by smart cyber-physical systems connected to cloud computing services
and communicating using standardized semantics. In the near future, industrial big
data analytics on monitored sensor data shall improve the efficiency and individualization
of production facilities. This year's QoSA conference solicited contributions that
explore the various implications of this upcoming industrial revolution on software
architecture. This included reference architectures, software architectures adapting
at run time, architecture styles and patterns for cyber-physical and distributed systems.The
call for papers attracted 42 initial submissions from Asia, North America, Africa,
and Europe and 28 final submissions were considered during the review process. The
program committee accepted 11 full papers and 2 short papers that cover topics, such
as new architecture modeling approaches, architectural tactics for mobile computing,
cloud computing architectures, and cyberphysical systems. QoSA's 2015 proceedings
also include 2 papers from the WCOP 2015, the 20th International Doctoral Symposium
on Components and Architecture.QoSA 2015 is part of the federated events on component-based
software engineering and software architecture (CompArch 2015), which include WICSA
2015 (12th Working IEEE / IFIP Conference on Software Architecture) and CBSE 2015
(18th International ACM SIGSOFT Symposium on Component-Based Software Engineering).},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1145/3383313.3412248,
author = {Hansen, Casper and Hansen, Christian and Maystre, Lucas and Mehrotra, Rishabh and Brost, Brian and Tomasi, Federico and Lalmas, Mounia},
title = {Contextual and Sequential User Embeddings for Large-Scale Music Recommendation},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412248},
doi = {10.1145/3383313.3412248},
abstract = {Recommender systems play an important role in providing an engaging experience on
online music streaming services. However, the musical domain presents distinctive
challenges to recommender systems: tracks are short, listened to multiple times, typically
consumed in sessions with other tracks, and relevance is highly context-dependent.
In this paper, we argue that modeling users’ preferences at the beginning of a session
is a practical and effective way to address these challenges. Using a dataset from
Spotify, a popular music streaming service, we observe that a) consumption from the
recent past and b) session-level contextual variables (such as the time of the day
or the type of device used) are indeed predictive of the tracks a user will stream—much
more so than static, average preferences. Driven by these findings, we propose CoSeRNN,
a neural network architecture that models users’ preferences as a sequence of embeddings,
one for each session. CoSeRNN predicts, at the beginning of a session, a preference
vector, based on past consumption history and current context. This preference vector
can then be used in downstream tasks to generate contextually relevant just-in-time
recommendations efficiently, by using approximate nearest-neighbour search algorithms.
We evaluate CoSeRNN on session and track ranking tasks, and find that it outperforms
the current state of the art by upwards of 10% on different ranking metrics. Dissecting
the performance of our approach, we find that sequential and contextual information
are both crucial. },
booktitle = {Fourteenth ACM Conference on Recommender Systems},
pages = {53–62},
numpages = {10},
keywords = {Context, Sequence, User Embeddings, Music Recommendation},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/2693561.2693563,
author = {Klein, John and Gorton, Ian},
title = {Runtime Performance Challenges in Big Data Systems},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693563},
doi = {10.1145/2693561.2693563},
abstract = {Big data systems are becoming pervasive. They are distributed systems that include
redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient
to assure runtime performance in production. This is due to the scale of the deployed
system, the continually evolving workloads, and the unpredictable quality of service
of the shared infrastructure. Consequently, a solution for addressing performance
requirements needs sophisticated runtime observability and measurement. Observability
gives real-time insights into a system's health and status, both at the system and
application level, and provides historical data repositories for forensic analysis,
capacity planning, and predictive analytics. Due to the scale and heterogeneity of
big data systems, significant challenges exist in the design, customization and operations
of observability capabilities. These challenges include economical creation and insertion
of monitors into hundreds or thousands of computation and data nodes, efficient, low
overhead collection and storage of measurements (which is itself a big data problem),
and application-aware aggregation and visualization. In this paper we propose a reference
architecture to address these challenges, which uses a model-driven engineering toolkit
to generate architecture-aware monitors and application-specific visualizations.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {17–22},
numpages = {6},
keywords = {observability, big data, model-driven engineering},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@inproceedings{10.1145/3364544.3364824,
author = {Faye, S\'{e}bastien and Melakessou, Foued and Mtalaa, Wassila and Gautier, Prune and AlNaffakh, Neamah and Khadraoui, Djamel},
title = {SWAM: A Novel Smart Waste Management Approach for Businesses Using IoT},
year = {2019},
isbn = {9781450370158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364544.3364824},
doi = {10.1145/3364544.3364824},
abstract = {The waste recycling industry has grown considerably in the recent years and many solutions
have become democratized around smart waste collection. However, existing decision
support systems generally rely on a limited flow of information and offer an often
static or statistically based approach, focusing on specific use-cases (e.g., individuals,
municipalities). This paper introduces SWAM, a new smart waste collection platform
currently being elaborated in Luxembourg that targets businesses and large entities
(e.g., restaurants, shopping centers). SWAM aims to consider multiple sources of combined
information in its decision-making process and go further in the routing optimization
process. The platform notably uses ultrasonic sensors to measure the filling level
of containers, and smartphones with embedded intelligence to understand a driver's
actions. This paper presents our experience on the development and deployment of this
platform in Luxembourg, as well as the relevant options on the choice of sensing and
communication technologies available in the market. It also presents the system architecture
and two fundamental components. Firstly, a data management layer, which implements
models for analyzing and predicting the filling patterns of the containers. Secondly,
a multi-objective optimization layer, which compiles the collection routes that minimize
the impact on the environment and maximize the service quality. This paper is intended
to serve as a practical reference for the deployment of waste management systems,
which have many technological components and can greatly fluctuate depending on the
use cases to be covered.},
booktitle = {Proceedings of the 1st ACM International Workshop on Technology Enablers and Innovative Applications for Smart Cities and Communities},
pages = {38–45},
numpages = {8},
keywords = {Smart Waste Collection, Multi-Objective Optimization, IoT, WSN},
location = {New York, NY, USA},
series = {TESCA'19}
}

@inproceedings{10.1145/3426744.3431328,
author = {V\"{o}r\"{o}s, P\'{e}ter and Pongr\'{a}cz, Gergely and Laki, S\'{a}ndor},
title = {Towards a Hybrid Next Generation NodeB},
year = {2020},
isbn = {9781450381819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426744.3431328},
doi = {10.1145/3426744.3431328},
abstract = {5G Radio Access Networks consists of two key services: User Plane Function (UPF) and
next generation NodeB (gNB). Though several papers have recently demonstrated that
the high-level UPF can be described in P4, for the lowest-level gNB service it is
more challenging and cannot purely be done with existing programmable switches. In
this paper, we show that gNB requires functionalities such as Automatic Repeat Request
(ARQ) and ciphering/deciphering that are not supported by the high speed P4-programmable
switches available in the market. To overcome these limitations, we propose a hybrid
approach where the majority of packet processing is done by a high speed P4-programmable
switch while the additional functionalities are solved by external services implemented
in DPDK. The coordination of packets among the services is also handled by the P4-switch.
Our preliminary results include the identification of functionalities required by
a gNB node for delivering user data, the design of a hybrid architecture, and the
performance evaluation of the buffering and re-transmission service. Finally, our
measurements demonstrate that the proposed hybrid approach is scalable and could be
an alternative to existing gNB solutions in the future.},
booktitle = {Proceedings of the 3rd P4 Workshop in Europe},
pages = {56–58},
numpages = {3},
location = {Barcelona, Spain},
series = {EuroP4'20}
}

@inproceedings{10.1145/2996913.2996917,
author = {Chatterjee, Abhranil and Anjaria, Janit and Roy, Sourav and Ganguli, Arnab and Seal, Krishanu},
title = {SAGEL: Smart Address Geocoding Engine for Supply-Chain Logistics},
year = {2016},
isbn = {9781450345897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996913.2996917},
doi = {10.1145/2996913.2996917},
abstract = {With the recent explosion of e-commerce industry in India, the problem of address
geocoding, that is, transforming textual address descriptions to geographic reference,
such as latitude, longitude coordinates, has emerged as a core problem for supply
chain management. Some of the major areas that rely on precise and accurate address
geocoding are supply chain fulfilment, supply chain analytics and logistics. In this
paper, we present some of the challenges faced in practice while building an address
geocoding engine as a core capability at Flipkart. We discuss the unique challenges
of building a geocoding engine for a rapidly developing country like India, such as,
fuzzy region boundaries, dynamic topography and lack of convention in spellings of
toponyms, to name a few. We motivate the need for building a reliable and precise
address geocoding system from a business perspective and argue why some of the commercially
available solutions do not suffice for our requirements. SAGEL has evolved through
3 cycles of solution prototypes and pilot experiments. We describe the learnings from
each of these phases and how we incorporated them to get to the first production-ready
version. We describe how we store and index map data on a SolrCloud cluster of Apache
Solr, an open-source search platform, and the core algorithm for geocoding which works
post-retrieval in order to determine the best matches among a set of candidate results.
We give a brief description of the system architecture and provide accuracy results
of our geocoding engine by measuring deviations of geocoded customer addresses across
India, from verified latitude, longitude coordinates of those addresses, for a sizeable
address set. We also measure and report our system's ability to geocode up to different
region levels, like city, locality or building. We compare our results with those
of the geocoding service provided by Google against a set of addresses for which we
have verified latitude-longitude coordinates and show that our geocoding engine is
almost as accurate as Google's, while having a higher coverage.},
booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {42},
numpages = {10},
keywords = {spatio-textual searching, storage and indexing, spatial data mining and knowledge discovery, geographic information retrieval},
location = {Burlingame, California},
series = {SIGSPACIAL '16}
}

@inproceedings{10.1145/2996890.2996903,
author = {Sukhoroslov, Oleg and Volkov, Sergey and Afanasiev, Alexander},
title = {Program Autotuning as a Service: Opportunities and Challenges},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.2996903},
doi = {10.1145/2996890.2996903},
abstract = {Program autotuning is becoming an increasingly valuable tool for improving performance
portability across diverse target architectures, exploring trade-offs between several
criteria, or meeting quality of service requirements. Recent work on general autotuning
frameworks enabled rapid development of domain-specific autotuners reusing common
libraries of parameter types and search techniques. In this work we explore the use
of such frameworks to develop general-purpose online services for program autotuning
using the Software as a Service model. Beyond the common benefits of this model, the
proposed approach opens up a number of unique opportunities, such as collecting performance
data and utilizing it to improve further runs, or enabling remote online autotuning.
However, the proposed autotuning as a service approach also brings in several challenges,
such as accessing target systems, dealing with measurement latency, and supporting
execution of user-provided code. This paper presents the first step towards implementing
the proposed approach and addressing these challenges. We describe an implementation
of generic autotuning service that can be used for tuning arbitrary programs on user-provided
computing systems. The service is based on OpenTuner autotuning framework and runs
on Everest platform that enables rapid development of computational web services.
In contrast to OpenTuner, the service doesn't require installation of the framework,
allows users to avoid writing code and supports efficient parallel execution of measurement
tasks across multiple machines. The performance of the service is evaluated by using
it for tuning synthetic and real programs.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {148–155},
numpages = {8},
keywords = {distributed computing, program autotuning, software as a service, web services},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/2816839.2816875,
author = {Ilyas, Bambrik and Fedoua, Didi},
title = {A Load Management Algorithm For Wireless Mesh Networks},
year = {2015},
isbn = {9781450334587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2816839.2816875},
doi = {10.1145/2816839.2816875},
abstract = {The WMN (Wireless Mesh Network) is a new emerging technology that can render the field
of industrial network more efficient and profitable. Due to its versatility that allows
a flexible configuration, this kind of network is commonly considered as a very suitable
architecture for mobile clients. The difference between the WMNs and other dynamic
networks, such as the MANET (Mobile Ad-hoc Network), is that the Mesh network contains
static wireless nodes called MR (Mesh Routers). Consequently, the presence of this
infrastructure makes the WMN more suitable to provide QoS (Quality of Service). However,
the guarantee of QoS in a dynamic topology is a difficult task by comparison with
static networks. These difficulties are caused by the random movement of the clients,
the shared nature of the wireless channel, the complexity of multi-hop communications
and most importantly the management of the traffic load forwarded through the MRs.
In this paper, we propose a new algorithm for load balancing in WMN that can search
for alternative paths in order to deviate from the loaded MRs. The proposed algorithm
can operate with different metrics at the same time and applies the Genetic Algorithm
in case there is a large population of possible solutions.},
booktitle = {Proceedings of the International Conference on Intelligent Information Processing, Security and Advanced Communication},
articleno = {46},
numpages = {6},
keywords = {traffic load, WMN, mobile clients, Mesh Routers, Genetic Algorithm, QoS},
location = {Batna, Algeria},
series = {IPAC '15}
}

@inproceedings{10.1145/3298689.3346961,
author = {Panteli, Maria},
title = {Recommendation Systems Compliant with Legal and Editorial Policies: The BBC+ App Journey},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346961},
doi = {10.1145/3298689.3346961},
abstract = {The BBC produces thousands of pieces of content every day and numerous BBC products
deliver this content to millions of users. For many years the content has been manually
curated (this is evident in the selection of stories on the front page of the BBC
News website and app for example). To support content creation and curation, a set
of editorial guidelines have been developed to build quality and trust in the BBC.
As personalisation becomes more important for audience engagement, we have been exploring
how algorithmically-driven recommendations could be integrated in our products. In
this talk we describe how we developed recommendation systems for the BBC+ app that
comply with legal and editorial policies and promote the values of the organisation.
We also discuss the challenges we face moving forward, extending the use of recommendation
systems for a public service media organisation like the BBC.The BBC+ app is the first
product to host in-house recommendations in a fully algorithmically-driven application.
The app surfaces short video clips and is targeted at younger audiences. The first
challenge we dealt with was content metadata. Content metadata are created for different
purposes and managed by different teams across the organisation making it difficult
to have reliable and consistent information. Metadata enrichment strategies have been
applied to identify content that is considered to be editorially sensitive, such as
political content, current legal cases, archived news, commercial content, and content
unsuitable for an under 16 audience. Metadata enrichment is also applied to identify
content that due care has not been taken such as poor titles, and spelling and grammar
mistakes. The first versions of recommendation algorithms exclude all editorially
risky content from the recommendations, the most serious of which is avoiding contempt
of court. In other cases we exclude content that could undermine our quality and trustworthiness.The
General Data Protection Regulation (GDPR) that recently came into effect had strong
implications on the design of our system architecture, the choice of the recommendation
models, and the implementation of specific product features. For example, the user
should be able to delete their data or switch off personalisation at any time. Our
system architecture should allow us to trace down and delete all data from that user
and switch to non-personalised content. The recommendations should also be explainable
and this led us to sometimes choosing a simpler model so that it is possible to more
easily explain why a user was recommended a particular type of content. Specific product
features were also added to enhance transparency and explainability. For example,
the user could view their history of watched items, delete any item, and get an explanation
of why a piece of content was recommended to them.At the BBC we aim to not only entertain
our audiences but also to inform and educate. These BBC values are also reflected
in our evaluation strategies and metrics. While we aim to increase audience engagement
we are also responsible for providing recent and diverse content that meets the needs
of all our audiences. Accuracy metrics such as Hit Rate and Normalized Discounted
Cumulative Gain (NDCG) can give a good estimate of the predictive performance of the
model. However, recency and diversity metrics have sometimes more weight in our products,
especially in applications delivering news content. What is more, qualitative evaluation
is very important before releasing any new model into production. We work closely
with editorial teams who provide feedback on the quality of the recommendations and
flag content not adhering to the BBC's values or the legal and editorial policies.The
development of the BBC+ app has been a great journey. We learned a lot about our content
metadata, the implications of GDPR in our system, and our evaluation strategies. We
created a minimum viable product that is compliant with legal and editorial policies.
However, a lot needs to be done to ensure the recommendations meet the quality standards
of the BBC. While excluding editorially sensitive content has limited the risk of
contempt of court, algorithmic fairness and impartiality still need to be addressed.
We encourage the community to look more into these topics and help us create the way
forward towards applications with responsible machine learning.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {529},
numpages = {1},
keywords = {recommendations, public service, technology policy},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{10.1145/3204949.3204976,
author = {Mekuria, Rufael and McGrath, Michael J. and Riccobene, Vincenzo and Bayon-Molino, Victor and Tselios, Christos and Thomson, John and Dobrodub, Artem},
title = {Automated Profiling of Virtualized Media Processing Functions Using Telemetry and Machine Learning},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204976},
doi = {10.1145/3204949.3204976},
abstract = {Most media streaming services are composed by different virtualized processing functions
such as encoding, packaging, encryption, content stitching etc. Deployment of these
functions in the cloud is attractive as it enables flexibility in deployment options
and resource allocation for the different functions. Yet, most of the time overprovisioning
of cloud resources is necessary in order to meet demand variability. This can be costly,
especially for large scale deployments. Prior art proposes resource allocation based
on analytical models that minimize the costs of cloud deployments under a quality
of service (QoS) constraint. However, these models do not sufficiently capture the
underlying complexity of services composed of multiple processing functions. Instead,
we introduce a novel methodology based on full-stack telemetry and machine learning
to profile virtualized or cloud native media processing functions individually. The
basis of the approach consists of investigating 4 categories of performance metrics:
throughput, anomaly, latency and entropy (TALE) in offline (stress tests) and online
setups using cloud telemetry. Machine learning is then used to profile the media processing
function in the targeted cloud/NFV environment and to extract the most relevant cloud
level Key Performance Indicators (KPIs) that relate to the final perceived quality
and known client side performance indicators. The results enable more efficient monitoring,
as only KPI related metrics need to be collected, stored and analyzed, reducing the
storage and communication footprints by over 85%. In addition a detailed overview
of the functions behavior was obtained, enabling optimized initial configuration and
deployment, and more fine-grained dynamic online resource allocation reducing overprovisioning
and avoiding function collapse. We further highlight the next steps towards cloud
native carrier grade virtualized processing functions relevant for future network
architectures such as in emerging 5G architectures.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {150–161},
numpages = {12},
keywords = {characterization, video streaming, experimentation, cloud computing, telemetry, performance},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/2749469.2750422,
author = {Zhang, Tianwei and Lee, Ruby B.},
title = {CloudMonatt: An Architecture for Security Health Monitoring and Attestation of Virtual Machines in Cloud Computing},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750422},
doi = {10.1145/2749469.2750422},
abstract = {Cloud customers need guarantees regarding the security of their virtual machines (VMs),
operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated
by the customer not knowing where his VM is executing, and on the semantic gap between
what the customer wants to know versus what can be measured in the cloud. We present
an architecture for monitoring a VM's security health, with the ability to attest
this to the customer in an unforgeable manner. We show a concrete implementation of
property-based attestation and a full prototype based on the OpenStack open source
cloud software.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {362–374},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@article{10.1145/2872887.2750422,
author = {Zhang, Tianwei and Lee, Ruby B.},
title = {CloudMonatt: An Architecture for Security Health Monitoring and Attestation of Virtual Machines in Cloud Computing},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3S},
issn = {0163-5964},
url = {https://doi.org/10.1145/2872887.2750422},
doi = {10.1145/2872887.2750422},
abstract = {Cloud customers need guarantees regarding the security of their virtual machines (VMs),
operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated
by the customer not knowing where his VM is executing, and on the semantic gap between
what the customer wants to know versus what can be measured in the cloud. We present
an architecture for monitoring a VM's security health, with the ability to attest
this to the customer in an unforgeable manner. We show a concrete implementation of
property-based attestation and a full prototype based on the OpenStack open source
cloud software.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {362–374},
numpages = {13}
}

@inproceedings{10.1145/3281411.3281426,
author = {Xhonneux, Mathieu and Duchene, Fabien and Bonaventure, Olivier},
title = {Leveraging EBPF for Programmable Network Functions with IPv6 Segment Routing},
year = {2018},
isbn = {9781450360807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281411.3281426},
doi = {10.1145/3281411.3281426},
abstract = {With the advent of Software Defined Networks (SDN), Network Function Virtualisation
(NFV) or Service Function Chaining (SFC), operators expect networks to support flexible
services beyond the mere forwarding of packets. The network programmability framework
which is being developed within the IETF by leveraging IPv6 Segment Routing enables
the realisation of in-network functions.In this paper, we demonstrate that this vision
of in-network programmability can be realised. By leveraging the eBPF support in the
Linux kernel, we implement a flexible framework that allows network operators to encode
their own network functions as eBPF code that is automatically executed while processing
specific packets. Our lab measurements indicate that the overhead of calling such
eBPF functions remains acceptable. Thanks to eBPF, operators can implement a variety
of network functions. We describe the architecture of our implementation in the Linux
kernel. This extension has been released with Linux 4.18. We illustrate the flexibility
of our approach with three different use cases: delay measurements, hybrid networks
and network discovery. Our lab measurements also indicate that the performance penalty
of running eBPF network functions on Linux routers does not incur a significant overhead.},
booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
pages = {67–72},
numpages = {6},
location = {Heraklion, Greece},
series = {CoNEXT '18}
}

@inproceedings{10.1145/3288599.3295582,
author = {Shah, Ryan and Nagaraja, Shishir},
title = {Do We Have the Time for IRM? Service Denial Attacks and SDN-Based Defences},
year = {2019},
isbn = {9781450360944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288599.3295582},
doi = {10.1145/3288599.3295582},
abstract = {Distributed sensor networks such as IoT deployments generate large quantities of measurement
data. Often, the analytics that runs on this data is available as a web service which
can be purchased for a fee. A major concern in the analytics ecosystem is ensuring
the security of the data. Often, companies offer Information Rights Management (IRM)
as a solution to the problem of managing usage and access rights of the data that
transits administrative boundaries. IRM enables individuals and corporations to create
restricted IoT data, which can have its flow from organisation to individual control
- disabling copying, forwarding, and allowing timed expiry. We describe our investigations
into this functionality and uncover a weak-spot in the architecture - its dependence
upon the accurate global availability of time. We present an amplified denial-of-service
attack which attacks time synchronisation and could prevent all the users in an organisation
from reading any sort of restricted data until their software has been re-installed
and re-configured. We argue that IRM systems built on current technology will be too
fragile for businesses to risk widespread use. We also present defences that leverage
the capabilities of Software-Defined Networks to apply a simple filter-based approach
to detect and isolate attack traffic.},
booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
pages = {496–501},
numpages = {6},
location = {Bangalore, India},
series = {ICDCN '19}
}

@article{10.1109/TNET.2014.2354262,
author = {Adhikari, Vijay K. and Guo, Yang and Hao, Fang and Hilt, Volker and Zhang, Zhi-Li and Varvello, Matteo and Steiner, Moritz},
title = {Measurement Study of Netflix, Hulu, and a Tale of Three CDNs},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2014.2354262},
doi = {10.1109/TNET.2014.2354262},
abstract = {Netflix and Hulu are leading Over-the-Top (OTT) content service providers in the US
and Canada. Netflix alone accounts for 29.7% of the peak downstream traffic in the
US in 2011. Understanding the system architectures and performance of Netflix and
Hulu can shed light on the design of such large-scale video streaming platforms, and
help improving the design of future systems. In this paper, we perform extensive measurement
study to uncover their architectures and service strategies. Netflix and Hulu bear
many similarities. Both Netflix and Hulu video streaming platforms rely heavily on
the third-party infrastructures, with Netflix migrating that majority of its functions
to the Amazon cloud, while Hulu hosts its services out of Akamai. Both service providers
employ the same set of three content distribution networks (CDNs) in delivering the
video contents. Using active measurement study, we dissect several key aspects of
OTT streaming platforms of Netflix and Hulu, e.g., employed streaming protocols, CDN
selection strategy, user experience reporting, etc. We discover that both platforms
assign the CDN to a video request without considering the network conditions and optimizing
the user-perceived video quality. We further conduct the performance measurement studies
of the three CDNs employed by Netflix and Hulu. We show that the available bandwidths
on all three CDNs vary significantly over the time and over the geographic locations.
We propose a measurement-based adaptive CDN selection strategy and a multiple-CDN-based
video delivery strategy that can significantly increase users' average available bandwidth.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {1984–1997},
numpages = {14},
keywords = {content distribution networks (CDN), Hulu, over-the-top (OTT) content service, video streaming, Netflix, CDN selection strategy}
}

@inproceedings{10.1109/CCGrid.2014.50,
author = {Tolosana-Calasanz, Rafael and Ba\~{n}ares, Jos\'{e} \'{A}ngel and Rana, Omer and Pham, Congduc and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
title = {Enforcing Quality of Service on OpenNebula-Based Shared Clouds},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.50},
doi = {10.1109/CCGrid.2014.50},
abstract = {With an increase in the number of monitoring sensors deployed on physical infrastructures,
there is a corresponding increase in data volumes that need to be processed. Data
measured or collected by sensors is typically processed at destination or "in-transit"
(i.e. from data capture to delivery to a user). When such data are processed in-transit
over a shared distributed computing infrastructure, it is useful to provide elastic
computational capability which can be adapted based on processing requirements and
demand. Where Service Level Agreements (SLAs) have been pre-agreed, such available
computational capacity needs to be shared in such a way that any Quality of Service
related constraints in such SLAs are not violated. This is particularly challenging
for time critical applications and with highly variable and unpredictable rates of
data generation (e.g. in Smart Grid applications where energy usage patterns may change
unpredictably). Previously, we proposed a Reference net based architectural model
for supporting QoS for multiple concurrent data streams being processed (prior to
delivery to a user) over a shared infrastructure. In this paper, we describe a practical
realisation of this architecture using the OpenNebula Cloud platform. We consider
our infrastructure to be composed of a number of nodes, each of which has multiple
processing units and data buffers. We utilize the "token bucket" model for regulating,
on a per stream basis, the data injection rate into each node. We subsequently demonstrate
how a streaming pipeline can be supported and managed using a dynamic control strategy
at each node.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {651–659},
numpages = {9},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3154273.3154316,
author = {Talasila, Prasad and Kakrambe, Mihir and Rai, Anurag and Santy, Sebastin and Goveas, Neena and Deshpande, Bharat M.},
title = {BITS Darshini: A Modular, Concurrent Protocol Analyzer Workbench},
year = {2018},
isbn = {9781450363723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3154273.3154316},
doi = {10.1145/3154273.3154316},
abstract = {Network measurements are essential for troubleshooting and active management of networks.
Protocol analysis of captured network packet traffic is an important passive network
measurement technique used by researchers and network operations engineers. In this
work, we present a measurement workbench tool named BITS Darshini (Darshini in short)
to enable scientific network measurements.We have created Darshini as a modular, concurrent
web application that stores experimental meta-data and allows users to specify protocol
parse graphs. Darshini performs protocol analysis on a concurrent pipeline architecture,
persists the analysis to a database and provides the analysis results via a REST API
service. We formulate the problem of mapping protocol parse graph to a concurrent
pipeline as a graph embedding problem. Our tool, Darshini, performs protocol analysis
up to transport layer and is suitable for the study of small and medium-sized networks.
Darshini enables secure collaboration and consultations with experts.},
booktitle = {Proceedings of the 19th International Conference on Distributed Computing and Networking},
articleno = {54},
numpages = {10},
keywords = {collaborative analysis, packet analyzer, measurement workbench, concurrent packet analysis, protocol parse graph, graph embedding, Network measurements},
location = {Varanasi, India},
series = {ICDCN '18}
}

@inproceedings{10.1145/2984393.2984398,
author = {Tomtsis, Dimitrios and Kontogiannis, Sotirios and Kokkonis, George and Zinas, Nicholas},
title = {IoT Architecture for Monitoring Wine Fermentation Process of Debina Variety Semi-Sparkling Wine},
year = {2016},
isbn = {9781450348102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984393.2984398},
doi = {10.1145/2984393.2984398},
abstract = {This paper proposes a new system architecture and HTTP communication mechanism called
Smart Barrel System (Wine-SBS) for the process of monitoring Debina varietal sparkling
wine fermenting conditions, produced at the area of Zitsa Epirus, Greece. The system
includes microcontroller equipment with sensors that monitor wine attributes and storage
conditions, called CBS-sensor transceivers, which are distributed among the debina
fermentation vessels. The transmission of measurements, which occur periodically,
are sent to a central cloud system application service. The CBS-sensor data are collected
by a CBS-sensor collector and then follows an HTTP/2 request of multiplexed HTTP flows
to a remote application server.},
booktitle = {Proceedings of the SouthEast European Design Automation, Computer Engineering, Computer Networks and Social Media Conference},
pages = {42–47},
numpages = {6},
keywords = {wireless sensor network, Precision enology, wine fermentation monitoring system},
location = {Kastoria, Greece},
series = {SEEDA-CECNSM '16}
}

@article{10.1109/TNET.2017.2746011,
author = {Sapountzis, Nikolaos and Spyropoulos, Thrasyvoulos and Nikaein, Navid and Salim, Umer},
title = {User Association in HetNets: Impact of Traffic Differentiation and Backhaul Limitations},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2017.2746011},
doi = {10.1109/TNET.2017.2746011},
abstract = {Operators, struggling to continuously add capacity and upgrade their architecture
to keep up with data traffic increase, are turning their attention to denser deployments
that improve spectral efficiency. Denser deployments make the problem of user association
challenging, and much work has been devoted to finding algorithms that strike a tradeoff
between user quality of service, and network-wide performance load-balancing. Nevertheless,
the majority of these algorithms typically consider simple setups with a single type
of traffic, usually elastic non-guaranteed bit rate GBR. They also focus on the radio
access part, ignoring the backhaul topology and potential capacity limitations. Backhaul
constraints are emerging as a key performance bottleneck in future networks, partly
due to the continuous improvement of the radio interface, and partly due to the need
for inexpensive backhaul links to reduce capital and operational expenditures. To
this end, we propose an analytical framework for user association that jointly considers
radio access and backhaul network performance. Specifically, we derive an algorithm
that takes into account spectral efficiency, base station load, backhaul link capacities
and topology, and two traffic classes GBR and non-GBR in both the uplink and downlink
directions. We prove analytically an optimal user association rule that ends up maximizing
either an arithmetic or a weighted harmonic mean of the achieved performance along
different dimensions e.g., uplink and downlink performances or GBR and non-GBR performances.
We then use extensive simulations to study the impact of: 1 traffic differentiation;
and 2 backhaul capacity limitations and topology on key performance metrics.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {3396–3410},
numpages = {15}
}

@inproceedings{10.1145/2684822.2697043,
author = {Lattanzi, Silvio and Mirrokni, Vahab},
title = {Distributed Graph Algorithmics: Theory and Practice},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2697043},
doi = {10.1145/2684822.2697043},
abstract = {As a fundamental tool in modeling and analyzing social, and information networks,
large-scale graph mining is an important component of any tool set for big data analysis.
Processing graphs with hundreds of billions of edges is only possible via developing
distributed algorithms under distributed graph mining frameworks such as MapReduce,
Pregel, Gigraph, and alike. For these distributed algorithms to work well in practice,
we need to take into account several metrics such as the number of rounds of computation
and the communication complexity of each round. For example, given the popularity
and ease-of-use of MapReduce framework, developing practical algorithms with good
theoretical guarantees for basic graph algorithms is a problem of great importance.In
this tutorial, we first discuss how to design and implement algorithms based on traditional
MapReduce architecture. In this regard, we discuss various basic graph theoretic problems
such as computing connected components, maximum matching, MST, counting triangle and
overlapping or balanced clustering. We discuss a computation model for MapReduce and
describe the sampling, filtering, local random walk, and core-set techniques to develop
efficient algorithms in this framework. At the end, we explore the possibility of
employing other distributed graph processing frameworks. In particular, we study the
effect of augmenting MapReduce with a distributed hash table (DHT) service and also
discuss the use of a new graph processing framework called ASYMP based on asynchronous
message-passing method. In particular, we will show that using ASyMP, one can improve
the CPU usage, and achieve significantly improved running time.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {419–420},
numpages = {2},
keywords = {parallel computing, mapreduce algorithms, large scale data-mining},
location = {Shanghai, China},
series = {WSDM '15}
}

@inproceedings{10.1145/2578153.2583037,
author = {Chrobot, Nina},
title = {The Role of Processing Fluency in Online Consumer Behavior: Evaluating Fluency by Tracking Eye Movements},
year = {2014},
isbn = {9781450327510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578153.2583037},
doi = {10.1145/2578153.2583037},
abstract = {The Internet enables people to extensively research products or services, and also
easily compare prices between offers [e.g. Baker et al. 2001]. Taking into account
the amount of information available on the Internet, acquisition of new information
can face some difficulties, especially when one wants to make a purchase decision.
Therefore, the ability to process relevant information fluently enables a user to
create a better experience and to become more efficient in gathering information related
to the purpose of the visit. This ability might be connected to the cognitive task
that can either be effortless or effortful, and may lead to a metacognitive experience
of either fluency or disfluency [Alter and Oppenheimer 2009]. Nevertheless, some e-commerce
websites are preferred over others and this preference varies between individuals.
This variation can be influenced by user's prior experience, cognitive sources but
also graphics or information architecture on the web page. Presented project aims
at applying the fluency concept to consumer behavior in online environment by studying
eye movements and promoting eye tracking as an objective measure.},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
pages = {387–388},
numpages = {2},
location = {Safety Harbor, Florida},
series = {ETRA '14}
}

@inproceedings{10.1145/3372224.3419195,
author = {Zhang, Chaoyun and Fiore, Marco and Ziemlicki, Cezary and Patras, Paul},
title = {Microscope: Mobile Service Traffic Decomposition for Network Slicing as a Service},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3419195},
doi = {10.1145/3372224.3419195},
abstract = {The growing diversification of mobile services imposes requirements on network performance
that are ever more stringent and heterogeneous. Network slicing aligns mobile network
operation to this context, by enabling operators to isolate and customize network
resources on a per-service basis. A key input for provisioning resources to slices
is real-time information about the traffic demands generated by individual services.
Acquiring such knowledge is however challenging, as legacy approaches based on in-depth
inspection of traffic streams have high computational costs, which inflate with the
widening adoption of encryption over data and control traffic. In this paper, we present
a new approach to service-level demand estimation for slicing, which hinges on decomposition,
i.e., the inference of per-service demands from traffic aggregates. By operating on
total traffic volumes only, our approach overcomes the complexity and limitations
of legacy traffic classification techniques, and provides a suitable input to recent 'Network Slice as a Service' (NSaaS) models. We implement decomposition through Microscope,
a novel framework that uses deep learning to infer individual service demands from
complex spatiotemporal features hidden in traffic aggregates. Microscope (i) transforms
traffic data collected in irregular radio access deployments in a format suitable
for convolutional learning, and (ii) can accommodate a variety of neural network architectures,
including original 3D Deformable Convolutional Neural Networks (3D-DefCNNs) that we
explicitly design for decomposition. Experiments with measurement data collected in
an operational network demonstrate that Microscope accurately estimates per-service
traffic demands with relative errors below 1.2%. Further, tests in practical NSaaS
management use cases show that resource allocations informed by decomposition yield
affordable costs for the mobile network operator.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {38},
numpages = {14},
keywords = {neural networks, traffic decomposition, service demand estimation, network slicing, mobile network data traffic, deep learning},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@inproceedings{10.1145/3120895.3120916,
author = {Arndt, Oliver Jakob and Spindeldreier, Christian and Wohnrade, Kevin and Pfefferkorn, Daniel and Neuenhahn, Martin and Blume, Holger},
title = {FPGA Accelerated NoC-Simulation: A Case Study on the Intel Xeon Phi Ringbus Topology},
year = {2017},
isbn = {9781450353168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3120895.3120916},
doi = {10.1145/3120895.3120916},
abstract = {Complex signal processing algorithms targeted on architectures with increasingly high
numbers of parallel processing units require high performance core-interconnections
(i.e., low latencies, high throughput, no pinch-offs or bottlenecks). Therefore, assisting
techniques, exploring characteristics of diverse topologies of common as well as innovative
Network-on-Chips (NoCs), are necessary for the development of chips with massive parallel
processing cores. In contrast to analytic NoC models, event driven NoC simulations
can handle even complex task graphs, but however feature long simulation times. Enabling
the simulation of even complex task graphs, in this work, we propose to use FPGA accelerated
simulation. While we extend such a simulator in order to imitate cache coherence communication-behavior,
we also present a translation of real measured profiles to task graphs for in-depth
simulation of the communication behavior of an existing NoC-based manycore. Therefore,
this approach is able to not only deal with synthetic scenarios, but analyse the communication
behavior of real world applications. Additionally, a simulation of the Histograms
of Oriented Gradients algorithm, running on the Intel Xeon Phi manycore, exhibiting
a 70-stop ring-bus, exemplifies this approach.},
booktitle = {Proceedings of the 8th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
articleno = {21},
numpages = {6},
location = {Bochum, Germany},
series = {HEART2017}
}

@inproceedings{10.1145/2766498.2774989,
author = {Podiyan, Pradeep and Butakov, Sergey and Zavarsky, Pavol},
title = {Study of Compliance of Android Location APIs with Geopriv},
year = {2015},
isbn = {9781450336239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766498.2774989},
doi = {10.1145/2766498.2774989},
abstract = {This paper carefully examines the location APIs of Android OS as well as the Geopriv
standard architecture to study measures that are being taken by Android OS to protect
the location privacy of a user. Android offers various location APIs in its architecture
for the app developers to work on location based services (LBS). The results of this
evaluation will be compared with Geopriv standard architecture and its ways to enhance
location information privacy on mobile platforms. The review of functionality of location
APIs shows that Android has limited features such as Geofencing to have some extent
of location privacy for a typical user. Only few of the recommendation in distribution
segment of Geopriv with slightly different approach are similar to the protection
mechanisms offered by location APIs in Android. The paper proposes general steps that
can be taken to address location privacy issues on mobile devices.},
booktitle = {Proceedings of the 8th ACM Conference on Security &amp; Privacy in Wireless and Mobile Networks},
articleno = {30},
numpages = {2},
keywords = {Geopriv, Android, location privacy},
location = {New York, New York},
series = {WiSec '15}
}

@inproceedings{10.1145/3139531.3139537,
author = {Liu, Ling},
title = {Keynote: Privacy and Trust: Friend or Foe},
year = {2017},
isbn = {9781450353939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139531.3139537},
doi = {10.1145/3139531.3139537},
abstract = {Internet of Things (IoT) and Big Data have fueled the development of fully distributed
computational architectures for future cyber systems from data analytics, machine
learning (ML) to artificial intelligence (AI). Trust and Privacy become two vital
and necessary measures for distributed management of IoT powered big data learning
systems and services. However, these two measures have been studied independently
in computer science, social science and law.Trust is widely considered as a critical
measure for the correctness, predictability, and resiliency (with respect to reliability
and security) of software systems, be it big data systems, IoT systems, machine learning
systems, or Artificial Intelligence systems. Privacy on the other hand is commonly
recognized as a personalization measure for imposing control on the ways of how data
is captured, accessed and analyzed, and the ways of how data analytic results from
ML models and AI systems should be released and shared.Broadly speaking, in human
society, we rely on three types of trust in our everyday work and life to achieve
a peaceful mind: (1) verifiable belief-driven trust, (2) statistical evidence based
trust, and (3) complex systemwide cognitive trust. Interestingly, privacy has been
a more controversial subject. On one hand, privacy is an important built-in dimension
of trust, which is deep rooted in human society, and a highly valued virtue in Western
civilization. Even though different human beings may have diverse levels of privacy
sensitivity, we all trust that our privacy is respected in our social and professional
environments, including at home, at work and in social commons. Thus, Privacy is a
perfect example of three-fold trust: belief-driven, statistical evident, and complex
cognitive trust. On the other hand, many view privacy (and privacy protection) as
an antagonistic measure of trust and one is often asked to show trust at the cost
of giving up on privacy.Are Privacy and Trust friend or foe? This keynote will share
my view to this question from multiple perspectives. I conjecture that the answer
to this question can fundamentally change the ways we conduct research in privacy
and trust in the next generation of big data enhanced cyber learning systems from
data mining, machine learning to artificial intelligence.},
booktitle = {Proceedings of the 2017 Workshop on Women in Cyber Security},
pages = {11},
numpages = {1},
keywords = {internet of things, deep learning, privacy, big data, trust},
location = {Dallas, Texas, USA},
series = {CyberW '17}
}

@inproceedings{10.1145/2619239.2631461,
author = {Fiadino, Pierdomenico and Schiavone, Mirko and Casas, Pedro},
title = {Vivisecting Whatsapp through Large-Scale Measurements in Mobile Networks},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2631461},
doi = {10.1145/2619239.2631461},
abstract = {WhatsApp, the new giant in instant multimedia messaging in mobile networks is rapidly
increasing its popularity, taking over the traditional SMS/MMS messaging. In this
paper we present the first large-scale characterization of WhatsApp, useful among
others to ISPs willing to understand the impacts of this and similar applications
on their networks. Through the combined analysis of passive measurements at the core
of a national mobile network, worldwide geo-distributed active measurements, and traffic
analysis at end devices, we show that: (i) the WhatsApp hosting architecture is highly
centralized and exclusively located in the US; (ii) video sharing covers almost 40%
of the total WhatsApp traffic volume; (iii) flow characteristics depend on the OS
of the end device; (iv) despite the big latencies to US servers, download throughputs
are as high as 1.5 Mbps; (v) users react immediately and negatively to service outages
through social networks feedbacks.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {133–134},
numpages = {2},
keywords = {instant multimedia messaging, service outages, whatsapp, mobile networks, large-scale measurements},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@article{10.1145/2740070.2631461,
author = {Fiadino, Pierdomenico and Schiavone, Mirko and Casas, Pedro},
title = {Vivisecting Whatsapp through Large-Scale Measurements in Mobile Networks},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2631461},
doi = {10.1145/2740070.2631461},
abstract = {WhatsApp, the new giant in instant multimedia messaging in mobile networks is rapidly
increasing its popularity, taking over the traditional SMS/MMS messaging. In this
paper we present the first large-scale characterization of WhatsApp, useful among
others to ISPs willing to understand the impacts of this and similar applications
on their networks. Through the combined analysis of passive measurements at the core
of a national mobile network, worldwide geo-distributed active measurements, and traffic
analysis at end devices, we show that: (i) the WhatsApp hosting architecture is highly
centralized and exclusively located in the US; (ii) video sharing covers almost 40%
of the total WhatsApp traffic volume; (iii) flow characteristics depend on the OS
of the end device; (iv) despite the big latencies to US servers, download throughputs
are as high as 1.5 Mbps; (v) users react immediately and negatively to service outages
through social networks feedbacks.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {133–134},
numpages = {2},
keywords = {service outages, large-scale measurements, mobile networks, whatsapp, instant multimedia messaging}
}

@inproceedings{10.1145/3018981.3018986,
author = {Mell, Peter and Shook, James and Harang, Richard},
title = {Measuring and Improving the Effectiveness of Defense-in-Depth Postures},
year = {2016},
isbn = {9781450347884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018981.3018986},
doi = {10.1145/3018981.3018986},
abstract = {Defense-in-depth is an important security architecture principle that has significant
application to industrial control systems (ICS), cloud services, storehouses of sensitive
data, and many other areas. We claim that an ideal defense-in-depth posture is 'deep',
containing many layers of security, and 'narrow', the number of node independent attack
paths is minimized. Unfortunately, accurately calculating both depth and width is
difficult using standard graph algorithms because of a lack of independence between
multiple vulnerability instances (i.e., if an attacker can penetrate a particular
vulnerability on one host then they can likely penetrate the same vulnerability on
another host). To address this, we represent known weaknesses and vulnerabilities
as a type of colored attack graph. We measure depth and width through solving the
shortest color path and minimum color cut problems. We prove both of these to be NP-Hard
and thus for our solution we provide a suite of greedy heuristics. We then empirically
apply our approach to large randomly generated networks as well as to ICS networks
generated from a published ICS attack template. Lastly, we discuss how to use these
results to help guide improvements to defense-in-depth postures.},
booktitle = {Proceedings of the 2nd Annual Industrial Control System Security Workshop},
pages = {15–22},
numpages = {8},
keywords = {security, measurement, attack graph, defense in depth},
location = {Los Angeles, CA, USA},
series = {ICSS '16}
}

@inproceedings{10.1145/3018896.3018934,
author = {Kokkonis, George and Kontogiannis, Sotirios and Tomtsis, Dimitrios},
title = {FITRA: A Neuro-Fuzzy Computational Algorithm Approach Based on an Embedded Water Planting System},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018934},
doi = {10.1145/3018896.3018934},
abstract = {This paper proposes a novel neuro-fuzzy computational algorithm for embedded irrigation
systems called FITRA. It presents a new system architecture for the process of continuously
monitoring environmental conditions and efficient irrigation of arable areas. The
system includes microcontroller equipment with multiple sensors interspersed all over
the field. Transmissions of measurements, which occur periodically, send to a central
cloud system Application Service (AS) assisted by a 3G network. The decision for irrigation
or not is made by a neuro-fuzzy algorithm. As an input for that algorithm are the
values taken from the interspersed sensors. As an output, this algorithm controls
the central solenoid water valve of the water planting system. The irrigation system
automatically adjusts to changing environmental conditions.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {39},
numpages = {8},
keywords = {neuro-fuzzy algorithms, soil sensor, agriculture, smart irrigation, smart farming, water planting systems, IoT},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3054977.3057290,
author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
title = {Simulating Conflict Detection in Heterogeneous Services of a Smart City: Demo Abstract},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3057290},
doi = {10.1145/3054977.3057290},
abstract = {Despite the increasing intelligence of smart services and sophistication of IoT platforms,
the safety issues in smart cities are not addressed adequately, especially the safety
issues arising from the integration of smart services. Therefore, in this demo abstract,
we present CityGuard, a safety-aware watchdog architecture to detect conflicts among
actions of heterogeneous services considering both safety and performance requirements.
This demo simulates parts of New York City to depict how CityGuard identifies unsafe
actions and thus helps to prevent the city from safety hazards, detects two major
types of conflicts, i.e., device and environmental conflicts, and improves the overall
city performance in terms of multiple performance metrics. This demo complements the
full paper on CityGuard that appears in this conference [2].},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {275–276},
numpages = {2},
keywords = {Smart City, City Simulation, Conflict Detection, City Safety},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@inproceedings{10.1145/3466933.3466945,
author = {Oliveira, Breno Silva and Ara\'{u}jo, \'{I}talo L. and Paiva, Joseane O. V. and Junior, Evilasio C. and Andrade, Rossana M. C.},
title = {Refactoring Decision Based on Measurements for IoHT Apps},
year = {2021},
isbn = {9781450384919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466933.3466945},
doi = {10.1145/3466933.3466945},
abstract = {Internet of Things (IoT) provides smart objects with the ability to connect to the
Internet, allowing the exchange of information among them to provide a certain service
and the development of innovative applications in several domains, including e-Health,
in which it is called Internet of Health Things (IoHT). This domain can be critical
specially when the application deals with the monitoring of the user health in real-time,
what demands software quality assurance, even more than in other applications. Measures
can be used to support that, for example, measures can suggest which components need
refactoring to improve the software code, thus improving the application. In this
work, we report how to do that with two existing measures that guide the refactoring
process of an IoHT application for fall detection, called WatchAlert. These measures
indicate that changes in both the architecture and the algorithms for fall detection
should occur. After the refactoring, the app accuracy was improved from 73.3% to 92.7%.
We believe that this work can contribute to other studies focusing on developing applications
on the IoHT domain using a methodology, a set of refactoring techniques, and lessons
learned that could be replicated to improve the quality of this type of application.},
booktitle = {XVII Brazilian Symposium on Information Systems},
articleno = {12},
numpages = {9},
keywords = {Refactoring, Fall detection, Measures, Internet of Things, e-Health},
location = {Uberl\^{a}ndia, Brazil},
series = {SBSI 2021}
}

@inproceedings{10.1145/3297280.3297648,
author = {Koupaee, Mahnaz},
title = {Mortality Prediction Using Medical Notes: Student Research Abstract},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297648},
doi = {10.1145/3297280.3297648},
abstract = {Mortality prediction is a critical task for assessing patients' conditions in Intensive
Care Units (ICU) of hospitals to improve decision-making and quality of care. Measurements
taken and recorded at different time points are the main source of information to
be used for tasks related to healthcare. However, the notes written by medical service
providers during patients' stay in hospital as a rich source of detailed information
is not sufficiently exploited. In this work, we propose a Convolutional Neural Network
(CNN) architecture to utilize the unstructured texts to predict the pre-discharge
and post-discharge mortality of ICU patients. Evaluations show high performance of
the proposed method in terms of precision and recall. Moreover, our method outperforms
the state of the art method by achieving a higher AUC.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {778–781},
numpages = {4},
keywords = {convolutional neural network, medical notes, MIMIC, mortality prediction},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3449301.3449322,
author = {Wen, Yana and Wei, Tingyue and Cui, Kewei and Ling, Bai and Zhang, Yahao and Huang, Meng},
title = {Research on Belt and Road Big Data Visualization Based on Text Clustering Algorithm},
year = {2020},
isbn = {9781450388597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449301.3449322},
doi = {10.1145/3449301.3449322},
abstract = {In the era of big data, people's visual needs for data expression are increasing.
In order to achieve better big data display effects, this article introduced the use
of text clustering algorithms to achieve data crawling and Echarts technology to realize
big data visualization. This system used mvvm's architecture and vue framework development
platform, ThinkPHP was used as the background framework, and ES6 related technologies
and specifications were used for application development. This system used Echarts,
IView, GIS technology and JavaScript development methods to demonstrate economic big
data module functions on the web side; Applied CSS3, HTML5, GIS technology to implement
project achievement module and university alliance module; Applied Echarts, HTML5,
JS function library technology to achieve national information module. This system
used stored procedure, database index optimization technology to achieve rapid screening
of massive data, and dynamically update and displayed related data through two-way
data binding. This system combined real-time location technology with GIS technology
to measure the distance between the user and the destination, and automatically plan
the tour route to provide related services. This system can provide feasibility suggestions
for strategic researchers or experts in related areas of the “Belt and Road”, and
provide theoretical basis and technical support.},
booktitle = {2020 6th International Conference on Robotics and Artificial Intelligence},
pages = {121–125},
numpages = {5},
keywords = {big data visualization, Text clustering algorithm, One Belt One Road, Keywords-component},
location = {Singapore, Singapore},
series = {ICRAI 2020}
}

@inproceedings{10.1145/2815675.2815677,
author = {Gracia-Tinedo, Ra\'{u}l and Tian, Yongchao and Samp\'{e}, Josep and Harkous, Hamza and Lenton, John and Garc\'{\i}a-L\'{o}pez, Pedro and S\'{a}nchez-Artigas, Marc and Vukolic, Marko},
title = {Dissecting UbuntuOne: Autopsy of a Global-Scale Personal Cloud Back-End},
year = {2015},
isbn = {9781450338486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815675.2815677},
doi = {10.1145/2815675.2815677},
abstract = {Personal Cloud services, such as Dropbox or Box, have been widely adopted by users.
Unfortunately, very little is known about the internal operation and general characteristics
of Personal Clouds since they are proprietary services.In this paper, we focus on
understanding the nature of Personal Clouds by presenting the internal structure and
a measurement study of UbuntuOne (U1). We first detail the U$1$ architecture, core
components involved in the U1 metadata service hosted in the datacenter of Canonical,
as well as the interactions of U$1$ with Amazon S3 to outsource data storage. To our
knowledge, this is the first research work to describe the internals of a large-scale
Personal Cloud.Second, by means of tracing the U$1$ servers, we provide an extensive
analysis of its back-end activity for one month. Our analysis includes the study of
the storage workload, the user behavior and the performance of the U1 metadata store.
Moreover, based on our analysis, we suggest improvements to U1 that can also benefit
similar Personal Cloud systems.Finally, we contribute our dataset to the community,
which is the first to contain the back-end activity of a large-scale Personal Cloud.
We believe that our dataset provides unique opportunities for extending research in
the field.},
booktitle = {Proceedings of the 2015 Internet Measurement Conference},
pages = {155–168},
numpages = {14},
keywords = {personal cloud, performance analysis, measurement},
location = {Tokyo, Japan},
series = {IMC '15}
}

@article{10.1145/3399742,
author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
title = {Spectre Attacks: Exploiting Speculative Execution},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3399742},
doi = {10.1145/3399742},
abstract = {Modern processors use branch prediction and speculative execution to maximize performance.
For example, if the destination of a branch depends on a memory value that is in the
process of being read, CPUs will try to guess the destination and attempt to execute
ahead. When the memory value finally arrives, the CPU either discards or commits the
speculative computation. Speculative logic is unfaithful in how it executes, can access
the victim's memory and registers, and can perform operations with measurable side
effects.Spectre attacks involve inducing a victim to speculatively perform operations
that would not occur during correct program execution and which leak the victim's
confidential information via a side channel to the adversary. This paper describes
practical attacks that combine methodology from side-channel attacks, fault attacks,
and return-oriented programming that can read arbitrary memory from the victim's process.
More broadly, the paper shows that speculative execution implementations violate the
security assumptions underpinning numerous software security mechanisms, such as operating
system process separation, containerization, just-in-time (JIT) compilation, and countermeasures
to cache timing and side-channel attacks. These attacks represent a serious threat
to actual systems because vulnerable speculative execution capabilities are found
in microprocessors from Intel, AMD, and ARM that are used in billions of devices.Although
makeshift processor-specific countermeasures are possible in some cases, sound solutions
will require fixes to processor designs as well as updates to instruction set architectures
(ISAs) to give hardware architects and software developers a common understanding
as to what computation state CPU implementations are (and are not) permitted to leak.},
journal = {Commun. ACM},
month = jun,
pages = {93–101},
numpages = {9}
}

@inproceedings{10.1145/2642687.2642690,
author = {Migault, Daniel and Palomares, Daniel and Hendrik, Hendrik and Laurent, Maryline},
title = {Secure IPsec Based Offload Architectures for Mobile Data},
year = {2014},
isbn = {9781450330275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642687.2642690},
doi = {10.1145/2642687.2642690},
abstract = {Radio Access Network (RAN) are likely to be overloaded, and some places will not be
able to provide the necessary requested bandwidth. In order to respond to the demand
of bandwidth, overloaded RAN are currently offloading their traffic on WLAN. WLAN
Access Points like (ISP provided xDSL boxes) are untrusted, unreliable and do not
handle mobility. As a result, mobility, multihoming, and security cannot be handled
by the network anymore, and must be handled by the terminal. This paper positions
offload architectures based on IPsec and shows that IPsec can provide end-to-end security,
as well as seamless connectivity across IP networks. Then, the remaining of the paper
evaluates how mobility on these IPsec based architectures impacts the Quality of Service
(QoS) for real time applications such as an audio streaming service. QoS is measured
using network interruption time and POLQA. Measurements compare TCP/HLS and UDP/RTSP
over various IPsec configurations.},
booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {95–104},
numpages = {10},
keywords = {wlan offload architecture, terminal mobility, quality of service, IPsec multiple interfaces, IPsec mobility},
location = {Montreal, QC, Canada},
series = {Q2SWinet '14}
}

@inproceedings{10.1145/3329391,
author = {Esposito, Christian and Pop, Florin and Choi, Chang},
title = {Session Details: Theme: Information Systems: SFECS - Sustainability of Fog/Edge Computing Systems Track},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329391},
doi = {10.1145/3329391},
abstract = {Fog/Edge Computing paradigms are widely used in enterprises to address the emerging
challenges of big data analysis, because of their underlying scalable, flexible and
distributed data management schemes. The data centers in the Clouds are facing great
challenges on the burden of the consequent increasing the amount of data to be man-
aged and the additional requirements of location awareness and low latency at the
edge of network necessary by smart cites and factories. These are the reasons why
a centralized model cannot be an efficient solution for generated or required data
by the IoT devices in those applications and there is the progressive shift towards
fog nodes and smarted edge nodes mediating between the cloud and the IoT devices.
The Fog/Edge computing paradigm is a decentralized model that transfers a part of
low computing data analysis from the cloud to the intermediate (fog) nodes or the
edges, performing only high computing tasks in the cloud. This new approach tries
to minimize the three factors that negatively compromise the effective and efficient
application of the Cloud computing to smart cities and factories, or similar application
domains: the network bandwidth usage, decentralization of the data processing tasks
and reduced response latency for clients (IoT devices). Fog/Edge computing is a hierarchical
approach where the overall infrastructure is structured in multiple layers, each responsible
of offering a good coordination and data management to the nodes at the lower layer.
The lowest layer is usually composed of sensors and/or actuators that measure and/or
control the environment or a given business process, implemented as mobile devices
that are running a sensing/controlling application. In this case, combining Sustainable
computing with Fog and Edge computing represents a new approach for increasing quality-of-
service and efficiency of the system, creating the capability to present temporal
and geo-coded information, and increasing innovation, and co-designing sustainable
future large scale distributed systems. This new paradigm appears to offer a good
approach in handling the scale factor of the data size, reducing the network bandwidth
usage and the response latency of the system. In order to support specifically the
Fog/Edge architectures, there is a need, for instance, of location-awareness and computation
placement, replication and recovery. In many cases Edge resources would be required
for both computation and data storage to address the time and locality constraints.
There are multiple kinds of orchestration management solutions for virtualization
in this type of architecture with different characteristics and drawbacks. This results
in different restrictions for application definition, scalability, availability, load
balancing and so on. Also, virtualization may be needed at multiple levels in a Fog/Edge
architecture as it consists of the following levels of abstraction: at the sensing
level we have the IoT devices/smart things, at the Edge level there are the gateways
to a first collection and the data from the IoT devices and their preliminary processing,
at the Fog level we have an additional data management layer, and at the Cloud level
there is the compute/storage infrastructure with applications on top. Last, but not
least, the energy efficiency is particularly important at the IoT and edge level since
the devices may be equipped with a limited battery, possible difficult or impossible
to be charged. So, optimizing the energy consumption is a must. To address several
open research is- sues regarding sustainability of future Fog/Edge systems, this track
aims at solicit contributions highlighting challenges, state-of-the-art, and solutions
to a set of currently unresolved key questions including - but not limited to - performance,
modeling, optimization, energy-efficiency, reliability, security, privacy and techno-economic
aspects of Fog/Edge systems. Through addressing these concerns while understanding
their impacts and limitations, technological advancements will be channeled toward
more sustainable/efficient platforms for tomorrow's ever-connected systems.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3316615.3316622,
author = {Ming, Fan Xiu and Habeeb, Riyaz Ahamed Ariyaluran and Md Nasaruddin, Fariza Hanum Binti and Gani, Abdullah Bin},
title = {Real-Time Carbon Dioxide Monitoring Based on IoT &amp; Cloud Technologies},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316622},
doi = {10.1145/3316615.3316622},
abstract = {In recent years, environment monitoring are of greater importance towards the area
of climate monitoring, analysis, agricultural productivity management, quality assurance
of water, air, alongside with other potential factors that are closely connected to
industrial development and convenience of living. This research is motivated by creating
awareness of smart home residents on indoor air quality, as well as providing insight
of carbon dioxide emissions for industries and environmental organizations.This paper
proposes an efficient solution towards environment monitoring of carbon dioxide integrated
with Internet of Things capability and cloud computing technology. Aforementioned
techniques will deliver highly accessible and real-time data visualization which would
be greatly beneficial for Smart Homes efficiency of analysis actualization and counter-measures
deployment. A monitoring architecture was developed to generate, accumulate, store
and visualize carbon dioxide concentration using MQ135 carbon dioxide sensor, ESP8266
Wi-Fi module, Firebase Cloud Storage Service and Android mobile application Carbon
Insight for data visualization. 2880 data points in the time frame of 10 days with
a 30-second interval was collected, stored and visualized with the application of
this system.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {517–521},
numpages = {5},
keywords = {Internet of things, cloud, environment monitoring},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@inproceedings{10.1145/2851613.2851727,
author = {Megyesi, P\'{e}ter and Botta, Alessio and Aceto, Giuseppe and Pescap\`{e}, Antonio and Moln\'{a}r, S\'{a}ndor},
title = {Available Bandwidth Measurement in Software Defined Networks},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851727},
doi = {10.1145/2851613.2851727},
abstract = {Software Defined Networking (SDN) is an emerging paradigm that is expected to revolutionize
computer networks. With the decoupling of data and control plane and the introduction
of open communication interfaces between layers, SDN enables programmability over
the entire network, promising rapid innovation in this area. The SDN concept was already
proven to work successfully in cloud and data center environments thus the proper
monitoring of such networks is already in the focus of the research community. Methods
for measuring Quality of Service (QoS) parameters such as bandwidth utilization, packet
loss, and delay have been recently introduced in literature, but they lack a solution
for tackling down the question of available bandwidth. In this paper, we attempt to
fill this gap and introduce a novel mechanism for measuring available bandwidth in
SDN networks. We take advantage of the SDN architecture and build an application over
the Network Operating System (NOS). Our application can track the topology of the
network and the bandwidth utilization over the network links, and thus it is able
to calculate the available bandwidth between any two points in the network. We validate
our method using the popular Mininet network emulation environment and the widely
used NOS called Floodlight. We present results providing insights into the measurement
accuracy and showing its relationship with the delay in the control network and the
polling frequency.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {651–657},
numpages = {7},
keywords = {network operating system, floodlight, OpenFlow, software defined networks, available bandwidth, mininet},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2699343.2699348,
author = {Achtzehn, Andreas and Riihihj\"{a}rvi, Janne and Barri\'{\i}a Castillo, Irving Antonio and Petrova, Marina and M\"{a}h\"{o}nen, Petri},
title = {<i>CrowdREM</i>: Harnessing the Power of the Mobile Crowd for Flexible Wireless Network Monitoring},
year = {2015},
isbn = {9781450333917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2699343.2699348},
doi = {10.1145/2699343.2699348},
abstract = {High-speed mobile broadband connections have opened exciting new opportunities to
collect sensor data from thousands or even millions of distributed mobile devices
for the purpose of crowdsourced decision making. In this paper, we propose CrowdREM
(crowdsourced radio environment mapping), a framework with the specific aim of monitoring
and modelling wireless cellular networks. CrowdREM enables operator-independent and
highly efficient collection of network performance data along all layers of the communications
protocol stack. Such extensive information on network load, spectrum usage, or local
coverage can help operators to optimize their networks and service quality and enable
improved consumer decision making. In this paper, we introduce the mbox{CrowdREM}
mobile architecture and show first results from a prototype implementation on open-source
mobile phones. We demonstrate the versatility of using commodity devices for network
and spectrum monitoring, and present the challenges originating from the use of uncalibrated
and low-precision measurement equipment. We have acquired an extensive data set from
using our prototype implementation in a 21-day measurement campaign covering more
than 1,000 hours of measurement data. From this we present and discuss the potential
derivation of tangible and relevant network performance and signal quality indicators,
which could, e.g., be conducted by independent parties.},
booktitle = {Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications},
pages = {63–68},
numpages = {6},
keywords = {drive testing, crowdsourcing, cellular networks, mobile},
location = {Santa Fe, New Mexico, USA},
series = {HotMobile '15}
}

@proceedings{10.1145/2656434,
title = {RIIT '14: Proceedings of the 3rd Annual Conference on Research in Information Technology},
year = {2014},
isbn = {9781450327114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great pleasure that we welcome you to the 15th Annual Conference on Information
Technology Education (SIGITE 2014) and the 3rd Annual Conference on Research in Information
Technology (RIIT 2014). The theme this year is "Riding the Wave of Change in Information
Technology" and the many quality submissions we received allowed us to assemble one
of the strongest programs in the history of the conferences. As in past years, the
synergies between research and education in information technology are prevalent,
and several themes emerged from the accepted submissions. Networking, security, and
development remain popular with researchers, and interest in mobile computing, resource
measurement and management, capstone courses, and personalization has grown.The call
for participation attracted 111 submissions, 72 of which were submitted to SIGITE
and 39 to RIIT. Both numbers represent a larger pool than in recent years, demonstrating
that the conferences are of great interest in the community. Ninety-five of the submissions
were papers, with 59 papers submitted to SIGITE and 36 papers submitted to RIIT. SIGITE
has 27 papers in its program for an acceptance rate of 46% and RIIT has 14 papers
for an acceptance rate of 39%. All of the authors presenting should be congratulated
on their excellent work.A conference cannot happen without the help of its reviewers,
and this year was no exception. Fiftyfive reviewers worked diligently to ensure that
every paper had at least three independent reviews. It was a significant effort to
produce the 317 reviews that ended up in the system, and we thank the reviewers from
the bottom of our heart. New to the conferences this year was a meta review process,
in which 13 diligent meta reviewers together examined all reviews for each submission
and reconciled those reviews into a coherent message for each author. We hope the
meta review process enabled authors to have more substantive feedback on their work,
whether it appears in the final program or not.The conference runs from Thursday to
Saturday and each day offers something of interest to attendees. On Thursday our keynote
speaker is Dr. Flavio Villanustre, Vice-President of Technology Architecture &amp; Product
for LexisNexis and HPCC Systems. The day continues with a workshop on end-user development
activities and paper sessions for both SIGITE and RIIT. Thursday concludes with a
reception, which we know will be useful for networking with colleagues old and new.
Friday introduces a new presentation format, lightning talks on research in progress,
at the conferences. There are also paper sessions for SIGITE and RIIT, a poster session
in the afternoon and, of course, more opportunities for networking during lunch and
the breaks. Saturday offers a three-hour workshop on process-oriented guided inquiry
learning (POGIL) as well as a panel on mobile computing courses and some excellent
SIGITE papers. We also hope that you stay for the closing session where we will share
our plans for SIGITE/RIIT 2015 in Chicago.We hope you find the conference presentations
interesting and thought-provoking, you reconnect with colleagues you know, you find
new collaborators, and you submit the work that results to SIGITE or RIIT next year.
The excellence you see at SIGITE/RIIT 2014 depends on your energy and effort, and
we thank you for letting us be a part of it.},
location = {Atlanta, Georgia, USA}
}

@inproceedings{10.1145/3452918.3467815,
author = {Dijkstra-Soudarissanane, Sylvie and Klunder, Tessa and Brandt, Aschwin and Niamut, Omar},
title = {Towards XR Communication for Visiting Elderly at Nursing Homes},
year = {2021},
isbn = {9781450383899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452918.3467815},
doi = {10.1145/3452918.3467815},
abstract = {Due to the current pandemic, the elderly in care homes are greatly affected by the
lack of contact with their families, resulting in various mental conditions (e.g.,
depression, feelings of loneliness) and deterioration of mental health for dementia
patients. In response, residents and family members increasingly resorted to mediated
communication to maintain social contact. To facilitate high-quality mediated social
contact between residents in nursing homes and remote family members, we developed
an Augmented Reality (AR)-based communication tool. The proposed demonstrator improved
this situation by providing a working communication tool that enables the elderly
to feel being together with their family by means of AR techniques. A complete end-to-end-chain
architecture is defined, where the aspects of capture, transmission, and rendering
are thoroughly investigated to fit the purpose of the use case. Based on an extensive
user study comprising user experience (UX) and quality of service (QoS) measurements,
each module is presented with the improvements made and the resulting higher quality
AR communication platform.},
booktitle = {ACM International Conference on Interactive Media Experiences},
pages = {319–321},
numpages = {3},
keywords = {Augmented Reality, Social XR, Immersive Media, AR, Volumetric video, WebRTC, Conferencing, Communication},
location = {Virtual Event, USA},
series = {IMX '21}
}

@proceedings{10.1145/2656450,
title = {SIGITE '14: Proceedings of the 15th Annual Conference on Information Technology Education},
year = {2014},
isbn = {9781450326865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great pleasure that we welcome you to the 15th Annual Conference on Information
Technology Education (SIGITE 2014) and the 3rd Annual Conference on Research in Information
Technology (RIIT 2014). The theme this year is "Riding the Wave of Change in Information
Technology" and the many quality submissions we received allowed us to assemble one
of the strongest programs in the history of the conferences. As in past years, the
synergies between research and education in information technology are prevalent,
and several themes emerged from the accepted submissions. Networking, security, and
development remain popular with researchers, and interest in mobile computing, resource
measurement and management, capstone courses, and personalization has grown.The call
for participation attracted 111 submissions, 72 of which were submitted to SIGITE
and 39 to RIIT. Both numbers represent a larger pool than in recent years, demonstrating
that the conferences are of great interest in the community. Ninety-five of the submissions
were papers, with 59 papers submitted to SIGITE and 36 papers submitted to RIIT. SIGITE
has 27 papers in its program for an acceptance rate of 46% and RIIT has 14 papers
for an acceptance rate of 39%. All of the authors presenting should be congratulated
on their excellent work.A conference cannot happen without the help of its reviewers,
and this year was no exception. Fiftyfive reviewers worked diligently to ensure that
every paper had at least three independent reviews. It was a significant effort to
produce the 317 reviews that ended up in the system, and we thank the reviewers from
the bottom of our heart. New to the conferences this year was a meta review process,
in which 13 diligent meta reviewers together examined all reviews for each submission
and reconciled those reviews into a coherent message for each author. We hope the
meta review process enabled authors to have more substantive feedback on their work,
whether it appears in the final program or not.The conference runs from Thursday to
Saturday and each day offers something of interest to attendees. On Thursday our keynote
speaker is Dr. Flavio Villanustre, Vice-President of Technology Architecture &amp; Product
for LexisNexis and HPCC Systems. The day continues with a workshop on end-user development
activities and paper sessions for both SIGITE and RIIT. Thursday concludes with a
reception, which we know will be useful for networking with colleagues old and new.
Friday introduces a new presentation format, lightning talks on research in progress,
at the conferences. There are also paper sessions for SIGITE and RIIT, a poster session
in the afternoon and, of course, more opportunities for networking during lunch and
the breaks. Saturday offers a three-hour workshop on process-oriented guided inquiry
learning (POGIL) as well as a panel on mobile computing courses and some excellent
SIGITE papers. We also hope that you stay for the closing session where we will share
our plans for SIGITE/RIIT 2015 in Chicago.We hope you find the conference presentations
interesting and thought-provoking, you reconnect with colleagues you know, you find
new collaborators, and you submit the work that results to SIGITE or RIIT next year.
The excellence you see at SIGITE/RIIT 2014 depends on your energy and effort, and
we thank you for letting us be a part of it.},
location = {Atlanta, Georgia, USA}
}

@inproceedings{10.1145/3286719.3286727,
author = {Coroller, Stevan and Chabridon, Sophie and Laurent, Maryline and Conan, Denis and Leneutre, Jean},
title = {Position Paper: Towards End-to-End Privacy for Publish/Subscribe Architectures in the Internet of Things},
year = {2018},
isbn = {9781450361187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286719.3286727},
doi = {10.1145/3286719.3286727},
abstract = {The Internet of Things paradigm lacks end-to-end privacy solutions to consider its
full adoption in real life scenarios in the near future. The recent enactment of the
EU General Data Protection Regulation (GDPR) indeed emphasises the need for stronger
security and privacy measures for personal data processing and free movement, including
consent management and accountability by the data controller and processor. In this
paper, we suggest an architecture to enforce end-to-end data usage control in Distributed
Event-Based Systems (DEBS), from data producers to consumer services, taking into
account some of the GDPR requirements concerning consent management and data processing
transparency. Our architecture proposal is based on UCONABC usage control models,
which we overlap with a distributed hash table overlay for scalability and fault-tolerance
concerns, and across and within systems data usage control. Our proposal highlights
the benefits of combining both DEBS and end-user usage control architectures. To complete
our approach, we quickly survey existing encryption models that ensure data confidentiality
in topic-based Publish/Subscribe systems and highlight the remaining obstacles to
transpose them to content-based DEBS with an overlay of brokers.},
booktitle = {Proceedings of the 5th Workshop on Middleware and Applications for the Internet of Things},
pages = {35–40},
numpages = {6},
keywords = {IoT, Privacy, Usage Control, Content-based Distributed Event-Based Systems},
location = {Rennes, France},
series = {M4IoT'18}
}

@inproceedings{10.1145/3005745.3005762,
author = {Tilmans, Olivier and B\"{u}hler, Tobias and Vissicchio, Stefano and Vanbever, Laurent},
title = {Mille-Feuille: Putting ISP Traffic under the Scalpel},
year = {2016},
isbn = {9781450346610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3005745.3005762},
doi = {10.1145/3005745.3005762},
abstract = {For Internet Service Provider (ISP) operators, getting an accurate picture of how
their network behaves is challenging. Given the traffic volumes that their networks
carry and the impossibility to control end-hosts, ISP operators are typically forced
to randomly sample traffic, and rely on aggregated statistics. This provides coarse-grained
visibility, at a time resolution that is far from ideal (seconds or minutes). In this
paper, we present Mille-Feuille, a novel monitoring architecture that provides fine-grained
visibility over ISP traffic. Mille-Feuille schedules activation and deactivation of
traffic-mirroring rules, that are then provisioned network-wide from a central location,
within milliseconds. By doing so, Mille-Feuille combines the scalability of sampling
with the visibility and controllability of traffic mirroring. As a result, it supports
a set of monitoring primitives, ranging from checking key performance indicators (e.g.,
one-way delay) for single destinations to estimating traffic matrices in sub-seconds.
Our preliminary measurements on existing routers confirm that Mille-Feuille is viable
in practice.},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
pages = {113–119},
numpages = {7},
location = {Atlanta, GA, USA},
series = {HotNets '16}
}

@inproceedings{10.1145/3152434.3152450,
author = {Singhvi, Arjun and Banerjee, Sujata and Harchol, Yotam and Akella, Aditya and Peek, Mark and Rydin, Pontus},
title = {Granular Computing and Network Intensive Applications: Friends or Foes?},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152450},
doi = {10.1145/3152434.3152450},
abstract = {Computing/infrastructure as a service continues to evolve with bare metal, virtual
machines, containers and now serverless granular computing service offerings. Granular
computing enables developers to decompose their applications into smaller logical
units or functions, and run them on small, low cost and short lived computation containers
without having to worry about setting up servers - hence the term serverless computing.
While serverless environments can be used very cost effectively for large scale parallel
processing data analytics applications, it is less clear if network intensive packet
processing applications can also benefit from these new computing services as they
do not share the same characteristics. This paper examines the architectural constraints
as well as current serverless implementations to develop a position on this topic
and influence the next generation of computing services. We support our position through
measurement and experimentation on Amazon's AWS Lambda service with a few popular
network functions.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {157–163},
numpages = {7},
location = {Palo Alto, CA, USA},
series = {HotNets-XVI}
}

@inproceedings{10.1145/2883851.2883876,
author = {Renz, Jan and Hoffmann, Daniel and Staubitz, Thomas and Meinel, Christoph},
title = {Using A/B Testing in MOOC Environments},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883876},
doi = {10.1145/2883851.2883876},
abstract = {In recent years, Massive Open Online Courses (MOOCs) have become a phenomenon offering
the possibility to teach thousands of participants simultaneously. In the same time
the platforms used to deliver these courses are still in their fledgling stages. While
course content and didactics of those massive courses are the primary key factors
for the success of courses, still a smart platform may increase or decrease the learners
experience and his learning outcome. The paper at hand proposes the usage of an A/B
testing framework that is able to be used within an micro-service architecture to
validate hypotheses about how learners use the platform and to enable data-driven
decisions about new features and settings. To evaluate this framework three new features
(Onboarding Tour, Reminder Mails and a Pinboard Digest) have been identified based
on a user survey. They have been implemented and introduced on two large MOOC platforms
and their influence on the learners behavior have been measured. Finally this paper
proposes a data driven decision workflow for the introduction of new features and
settings on e-learning platforms.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {304–313},
numpages = {10},
keywords = {MOOC, A/B testing, e-learning, controlled online tests, microservice},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1109/UCC.2014.49,
author = {Keller, Matthias and Robbert, Christoph and Karl, Holger},
title = {Template Embedding: Using Application Architecture to Allocate Resources in Distributed Clouds},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.49},
doi = {10.1109/UCC.2014.49},
abstract = {In distributed cloud computing, application deployment across multiple sites can improve
quality of service. Recent research developed algorithms to find optimal locations
for virtual machines. However, those algorithms assume to have either single-tier
applications or a fixed number of virtual machines--a strong simplification of reality.
This paper investigates the placement and scaling of complex application architectures.
An application is dynamically scaled to fit both the current demand situation and
the currently available infrastructure resources. We compare two approaches: The first
one is based on virtual network embedding. The second approach is a novel method called
Template Embedding. It is based on a hierarchical 1-allocation hub flow problem and
combines application scaling and embedding in one step. Extensive experiments on 43200
network configurations showed that Template Embedding outperforms virtual network
embedding in all cases in three metrics: success rate, solution quality, and runtime.
This positive result shows that template embedding is a promising approach for distributed
cloud resource allocation.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {387–395},
numpages = {9},
keywords = {Flow Problem, Distributed Cloud Computing, Application Architecture, Hub Problem, Cloud Resource Allocation},
series = {UCC '14}
}

@inproceedings{10.1145/2642668.2642673,
author = {Hatoum, Rima and Hatoum, Abbas and Ghaith, Alaa and Pujolle, Guy},
title = {Qos-Based Joint Resource Allocation with Link Adaptation for SC-FDMA Uplink in Heterogeneous Networks},
year = {2014},
isbn = {9781450330268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642668.2642673},
doi = {10.1145/2642668.2642673},
abstract = {The LTE-based femtocell network is a promising solution adopted today to cope with
the huge cellular traffic requirements. In particular, the Uplink communication becomes
an attractive issue especially with the emerging of the interactive services and large
uploaded data volume. Intelligent allocation of the resources and interference management
are the main challenges in such context. In this paper, we propose a linear optimization
model for the SC-FDMA Uplink transmission aiming to adaptively allocate resources
with respect to the link quality. Both power and modulation and coding schemes are
independently assigned to each user over each allocated sub-channel. The cluster architecture
is adopted as a hybrid centralized/distributed network. The user differentiation strategy
ensures the QoS guarantee with respect to a priority level of each user. Taking into
account the specifications of the uplink communication, we confirm through comparative
simulations the outperformance of our proposal considering several metrics such as
throughput satisfaction rate, transmitted power, outage probability, special spectrum
reuse and others.},
booktitle = {Proceedings of the 12th ACM International Symposium on Mobility Management and Wireless Access},
pages = {59–66},
numpages = {8},
keywords = {interference mitigation, uplink, QoS, resource allocation, link adaptation, SC-FDMA-femtocell},
location = {Montreal, QC, Canada},
series = {MobiWac '14}
}

@proceedings{10.1145/2898375,
title = {HotSos '16: Proceedings of the Symposium and Bootcamp on the Science of Security},
year = {2016},
isbn = {9781450342773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Science of Security (SoS) emphasizes the advancement of research methods as well as
the development of new research results. This dual focus is intended to improve both
the confidence we gain from scientific results and also the capacity and efficiency
through which we address increasingly challenging technical problems.The HotSoS conferences
have focused on work related to one or more of the five Hard Problems identified by
the Science of Security community:•Scalability and composability in the construction
of secure systems•Policy-governed collaboration in handling data across different
domains of authority for security and privacy protection•Predictive security metrics
to guide choice-making in security engineering and response•Resilient architectures
that can deliver service despite compromised components•Human behavior, modeling users,
operators, and adversaries to support improved design and analysisA second and equally
major focus of the conferences is on the advancement of scientific methods, including
data gathering and analysis, experimental methods, and mathematical models for modeling
and reasoning. This includes the exploration of interactions among these methods to
enhance validity.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/2676723.2691890,
author = {Hoffman, Mark E.},
title = {Student Board-Writing to Integrate Communication Skills and Content to Enhance Student Learning (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2691890},
doi = {10.1145/2676723.2691890},
abstract = {Students frequently use a whiteboard to individually demonstrate understanding or
interactively develop understanding in groups. The practice is employed to develop
content knowledge; however, an opportunity to intentionally develop communication
skills is overlooked. On the other hand, instructors carefully integrate instructional
organization and communication to maximize student content learning. Taken together,
this presents an opportunity for students to intentionally improve their communication
skills in the service of content learning. This poster details a "work in progress"
project where students follow organizational guidelines for written homework and board-writing
to facilitate in-class, problem solution presentation. Problem solution presentations
occur during one class period each week. Students are given colored pencils for written
homework and colored markers for board-writing. Student work including written homework
and board-writing was gathered from the 2013 and 2014 iterations of a sophomore-level
computer architecture course. Preliminary analysis of student work shows that students
either adopt the guidelines from the start or learn to use them through feedback and
practice. On the semester-end survey, students report that adopting guidelines for
written homework, board-writing, and color scheme improve presentation, and board-writing
improves student learning. Future work includes gathering data from more students
including recorded student presentations, developing quantitative scores to analyze
student work, and developing measures of student learning.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {684},
numpages = {1},
keywords = {student board-writing, content learning, communication skills},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.5555/2893711.2893715,
author = {Rauter, Tobias and H\"{o}ller, Andrea and Iber, Johannes and Kreiner, Christian},
title = {Thingtegrity: A Scalable Trusted Computing Architecture for the Internet of Things},
year = {2016},
isbn = {9780994988607},
publisher = {Junction Publishing},
address = {USA},
abstract = {Remote attestation is used to prove the integrity of one system (prover) to another
(challenger). The prover measures its configuration and transmits the result to the
challenger for verification. Common attestation methods lead to complex configuration
measurements (e.g., hash of all executables), which are updated every time one of
the software modules changes. The updated configuration has to be distributed to all
possible challengers since they need a reference to enable the verification. Recently,
an idea of reducing the complexity of the configuration measurement by taking into
account privileges of software modules has been presented. However, this approach
has not been exhaustively analyzed since, as yet, no implementation exists. Especially
in the Internet of Things (IoT) domain, where resources are constrained strictly while
devices are potentially physically exposed to adversaries, attestation methodologies
with reduced overhead are desireable. In this work we combine binary-, property- and
privilege-based remote attestation to integrate a trusted computing architecture transparently
into iotivity, an existing IoT middleware. As a first step, we aim to enable to attestation
of the integrity of complex devices with different services to constrained devices.
With the help of an illustrative simulated environment, we show that our architecture
reduces the effort of bootstrapping trusted relations, as well as updating single
modules in the whole system, even if software and devices from different vendors are
combined.},
booktitle = {Proceedings of the 2016 International Conference on Embedded Wireless Systems and Networks},
pages = {23–34},
numpages = {12},
location = {Graz, Austria},
series = {EWSN '16}
}

@article{10.1145/3372136,
author = {Lao, Laphou and Li, Zecheng and Hou, Songlin and Xiao, Bin and Guo, Songtao and Yang, Yuanyuan},
title = {A Survey of IoT Applications in Blockchain Systems: Architecture, Consensus, and Traffic Modeling},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3372136},
doi = {10.1145/3372136},
abstract = {Blockchain technology can be extensively applied in diverse services, including online
micro-payments, supply chain tracking, digital forensics, health-care record sharing,
and insurance payments. Extending the technology to the Internet of things (IoT),
we can obtain a verifiable and traceable IoT network. Emerging research in IoT applications
exploits blockchain technology to record transaction data, optimize current system
performance, or construct next-generation systems, which can provide additional security,
automatic transaction management, decentralized platforms, offline-to-online data
verification, and so on. In this article, we conduct a systematic survey of the key
components of IoT blockchain and examine a number of popular blockchain applications.In
particular, we first give an architecture overview of popular IoT-blockchain systems
by analyzing their network structures and protocols. Then, we discuss variant consensus
protocols for IoT blockchains, and make comparisons among different consensus algorithms.
Finally, we analyze the traffic model for P2P and blockchain systems and provide several
metrics. We also provide a suitable traffic model for IoT-blockchain systems to illustrate
network traffic distribution.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {18},
numpages = {32},
keywords = {Blockchain, architecture, traffic modeling, IoT, consensus}
}

@inproceedings{10.1145/2568088.2576760,
author = {Ghaith, Shadi and Wang, Miao and Perry, Philip and Murphy, Liam},
title = {Software Contention Aware Queueing Network Model of Three-Tier Web Systems},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2576760},
doi = {10.1145/2568088.2576760},
abstract = {Using modelling to predict the performance characteristics of software applications
typically uses Queueing Network Models representing the various system hardware resources.
Leaving out the software resources, such as the limited number of threads, in such
models leads to a reduced prediction accuracy. Accounting for Software Contention
is a challenging task as existing techniques to model software components are complex
and require deep knowledge of the software architecture. Furthermore, they also require
complex measurement processes to obtain the model's service demands. In addition,
solving the resultant model usually require simulation solvers which are often time
consuming.In this work, we aim to provide a simpler model for three-tier web software
systems which accounts for Software Contention that can be solved by time efficient
analytical solvers. We achieve this by expanding the existing "Two-Level Iterative
Queuing Modelling of Software Contention" method to handle the number of threads at
the Application Server tier and the number of Data Sources at the Database Server
tier. This is done in a generic manner to allow for extending the solution to other
software components like memory and critical sections. Initial results show that our
technique clearly outperforms existing techniques.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {273–276},
numpages = {4},
keywords = {performance prediction, performance models, web applications, software contention},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.5555/2821327.2821330,
author = {Zimmermann, Olaf},
title = {Metrics for Architectural Synthesis and Evaluation: Requirements and Compilation by Viewpoint: An Industrial Experience Report},
year = {2015},
publisher = {IEEE Press},
abstract = {During architectural analysis and synthesis, architectural metrics are established
tacitly or explicitly. In architectural evaluation, these metrics are then consulted
to assess whether architectures are fit for purpose and in line with recommended practices
and published architectural knowledge. This experience report presents a personal
retrospective of the author's use of architectural metrics during 20 years in IT architect
roles in professional services as well as research and development. This reflection
drives the identification of use cases, critical success factors and elements of risk
for architectural metrics management. An initial catalog of architectural metrics
is compiled next, which is organized by viewpoints and domains. The report concludes
with a discussion of practical impact of architectural metrics and potential research
topics in this area.},
booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
pages = {8–14},
numpages = {7},
keywords = {patterns, viewpoints, architectural metrics, architectural metrics management, enterprise information systems, architectural reviews, integration},
location = {Florence, Italy},
series = {SAM '15}
}

@inproceedings{10.1145/3437120.3437284,
author = {Psilias, Dimitrios and Milidonis, Athanasios and Voyiatzis, Ioannis},
title = {Architecture for Secure UAV Systems},
year = {2020},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437284},
doi = {10.1145/3437120.3437284},
abstract = {UAV applications are providing an extended range of services in society's needs. These
applications require high execution speed and security to all transmitted data. In
this paper an architecture is proposed for secure UAV applications. The architecture
consists of a microcontroller to execute the flight controller tasks and a FPGA for
implementing the security related tasks. The microcontroller is an Arduino which is
widely used in UAVs. Arduino communicates with all sensors and generates outputs needed
for controlling the UAV's motors. The circuit inside the FPGA encrypts/decrypts data
related to transmission. Measurements taken concerning the execution time and power
consumption, reveal the benefits of the extra hardware added for encryption/decryption
in comparison with those of a single microcontroller.},
booktitle = {24th Pan-Hellenic Conference on Informatics},
pages = {99–102},
numpages = {4},
location = {Athens, Greece},
series = {PCI 2020}
}

@inproceedings{10.1145/2973750.2973766,
author = {Yu, Der-Yeuan and Ranganathan, Aanjhan and Masti, Ramya Jayaram and Soriente, Claudio and Capkun, Srdjan},
title = {SALVE: Server Authentication with Location Verification},
year = {2016},
isbn = {9781450342261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973750.2973766},
doi = {10.1145/2973750.2973766},
abstract = {The Location Service (LCS) proposed by the telecommunication industry is an architecture
that allows the location of mobile devices to be accessed in various applications.
We explore the use of LCS in location-enhanced server authentication, which traditionally
relies on certificates. Given recent incidents involving certificate authorities,
various techniques to strengthen server authentication were proposed. They focus on
improving the certificate validation process, such as pinning, revocation, or multi-path
probing. In this paper, we propose using the server's geographic location as a second
factor of its authenticity. Our solution, SALVE, achieves location-based server authentication
by using secure DNS resolution and by leveraging LCS for location measurements. We
develop a TLS extension that enables the client to verify the server's location in
addition to its certificate. Successful server authentication therefore requires a
valid certificate and the server's presence at a legitimate geographic location, e.g.,
on the premises of a data center. SALVE prevents server impersonation by remote adversaries
with mis-issued certificates or stolen private keys of the legitimate server. We develop
a prototype implementation and our evaluation in real-world settings shows that it
incurs minimal impact to the average server throughput. Our solution is backward compatible
and can be integrated with existing approaches for improving server authentication
in TLS.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking},
pages = {401–414},
numpages = {14},
keywords = {location service, TLS, server authentication, location-based authentication},
location = {New York City, New York},
series = {MobiCom '16}
}

@inproceedings{10.4108/icst.pervasivehealth.2014.255331,
author = {Weiss, Patrick and Heldmann, Marcus and Gabrecht, Alexander and Schweikard, Achim and M\"{u}nte, Thomas M. and Maehle, Erik},
title = {A Low Cost Tele-Rehabilitation Device for Training of Wrist and Finger Functions after Stroke},
year = {2014},
isbn = {9781631900112},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.pervasivehealth.2014.255331},
doi = {10.4108/icst.pervasivehealth.2014.255331},
abstract = {There is a need for robotic rehabilitation devices that improve the outcome while
reducing the cost of therapy. This paper presents a device for training of supination/pronation,
dorsal wrist extension, and finger manipulation after stroke. The system exhibits
modularity in terms of the communication architecture and different optional components.
User interfaces (UI) can be implemented on different kinds of devices including a
Rasperry Pi single-board computer on which a Qt-based graphical UI was run in this
instance. Tele-rehabilitation functionality is included using SSL-encrypted RESTful
web services on a three-tier architecture. Expensive sensors were omitted in order
to have a cost-effective system which is a requirement for home-based rehabilitation.
The current-based torque sensing is evaluated by comparing current measurements to
force-torque sensor values. After canceling out the static friction, the low error
justified the omission of an additional sensor.},
booktitle = {Proceedings of the 8th International Conference on Pervasive Computing Technologies for Healthcare},
pages = {422–425},
numpages = {4},
keywords = {robotic rehabilitation, stroke, tele-rehabilitation, wrist and finger functions, home health care},
location = {Oldenburg, Germany},
series = {PervasiveHealth '14}
}

@inproceedings{10.1145/3357141.3357149,
author = {Seabra, Matheus and Naz\'{a}rio, Marcos Felipe and Pinto, Gustavo},
title = {REST or GraphQL? A Performance Comparative Study},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357149},
doi = {10.1145/3357141.3357149},
abstract = {Given the variety of architectural models that can be used, a frequent questioning
among software development practitioners is: which architectural model to use? To
respond this question regarding performance issues, three target applications have
been studied, each written using two models web services architectures: REST and GraphQL.
Through research of performance metrics of response time and the average transfer
rate between the requests, it was possible to deduce the particularities of each architectural
model in terms of performance metrics. It was observed that migrating to GraphQL.
resulted in an increase in performance in two-thirds of the tested applications, with
respect to average number of requests per second and transfer rate of data. However,
it was noticed that services after migration for GraphQL performed below its REST
counterpart for workloads above 3000 requests, ranging from 98 to 2159 Kbytes per
second after the migration study. On the other hand, for more trivial workloads, services
on both REST and GraphQL architectures presented similar performances, where values
between REST and GraphQL services ranged from 6.34 to 7.68 requests per second for
workloads of 100 requests.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {123–132},
numpages = {10},
keywords = {Modelo arquitetural, REST, Teste de desempenho, GraphQL},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/2736084.2736091,
author = {Zhang, Cong and Liu, Jiangchuan},
title = {On Crowdsourced Interactive Live Streaming: A Twitch.Tv-Based Measurement Study},
year = {2015},
isbn = {9781450333528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2736084.2736091},
doi = {10.1145/2736084.2736091},
abstract = {Empowered by today's rich tools for media generation and collaborative production,
the multimedia service paradigm is shifting from the conventional single source, to
multi-source, to many sources, and now toward crowdsource. Such crowdsourced live
streaming platforms as Twitch.tv allow general users to broadcast their content to
massive viewers, thereby greatly expanding the content and user bases. The resources
available for these non-professional broadcasters however are limited and unstable,
which potentially impair the streaming quality and viewers' experience. The diverse
live interactions among the broadcasters and viewers can further aggravate the problem.In
this paper, we present an initial investigation on the modern crowdsourced live streaming
systems. Taking Twitch as a representative, we outline their inside architecture using
both crawled data and captured traffic of local broadcasters/viewers. Closely examining
the access data collected in a two-month period, we reveal that the view patterns
are determined by both events and broadcasters' sources. Our measurements explore
the unique source- and event-driven views, showing that the current delay strategy
on the viewer's side substantially impacts the viewers' interactive experience, and
there is significant disparity between the long broadcast latency and the short live
messaging latency. On the broadcaster's side, the dynamic uploading capacity is a
critical challenge, which noticeably affects the smoothness of live streaming for
viewers.},
booktitle = {Proceedings of the 25th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {55–60},
numpages = {6},
keywords = {view statistics, crowdsourced live streaming, interactive latency, Twitch.tv},
location = {Portland, Oregon},
series = {NOSSDAV '15}
}

@inproceedings{10.1145/2775292.2775312,
author = {Scully, Timothy and Dobo\v{s}, Jozef and Sturm, Timo and Jung, Yvonne},
title = {3drepo.Io: Building the next Generation Web3D Repository with AngularJS and X3DOM},
year = {2015},
isbn = {9781450336475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2775292.2775312},
doi = {10.1145/2775292.2775312},
abstract = {This paper presents a novel open source web-based 3D version control system positioned
directly within the context of the recent strategic plan for digitising the construction
sector in the United Kingdom. The aim is to achieve reduction of cost and carbon emissions
in the built environment by up to 20% simply by properly managing digital information
and 3D models. Even though previous works in the field concentrated mainly on defining
novel WebGL frameworks and later on the efficiency of 3D data delivery over the Internet,
there is still the emerging need for a practical solution that would provide ubiquitous
access to 3D assets, whether it is for large international enterprises or individual
members of the general public. We have, therefore, developed a novel platform leveraging
the latest open web-based technologies such as AngularJS and X3DOM in order to define
an industrial-strength collaborative cloud hosting service 3drepo.io. Firstly, we
introduce the work and outline the high-level system architecture as well as improvements
in relation to previous work. Next, we describe database and front-end considerations
with emphasis on scalability and enhanced security. Finally, we present several performance
measurement experiments and a selection of real-life industrial use cases. We conclude
that jQuery provides performance benefits over AngularJS when manipulating large scene
graphs in web browsers.},
booktitle = {Proceedings of the 20th International Conference on 3D Web Technology},
pages = {235–243},
numpages = {9},
keywords = {version control, X3DOM, 3D repo, AngularJS, BIM},
location = {Heraklion, Crete, Greece},
series = {Web3D '15}
}

@inproceedings{10.1145/3405837.3411375,
author = {Khooi, Xin Zhe and Csikor, Levente and Kang, Min Suk and Divakaran, Dinil Mon},
title = {In-Network Defense against AR-DDoS Attacks},
year = {2020},
isbn = {9781450380485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405837.3411375},
doi = {10.1145/3405837.3411375},
abstract = {The prevalence of the disruptive amplified reflection DDoS (AR-DDoS) attacks is one
of the biggest concerns of all network operators today. The increasing magnitude of
new attacks are rendering existing measures (e.g., scrubbing services) inefficient.
This work demonstrates DIDA, an efficient, topology independent, in-line AR-DDoS detection
and mitigation architecture that operates entirely in the data plane.},
booktitle = {Proceedings of the SIGCOMM '20 Poster and Demo Sessions},
pages = {18–20},
numpages = {3},
keywords = {detection and mitigation, programmable switches, denial-of-service attacks, amplification attacks, in-network, reflection attacks},
location = {Virtual event},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3030207.3044531,
author = {Jun, Tae Joon and Yoo, Myong Hwan and Kim, Daeyoung and Cho, Kyu Tae and Lee, Seung Young and Yeun, Kyuoke},
title = {HPC Supported Mission-Critical Cloud Architecture},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3044531},
doi = {10.1145/3030207.3044531},
abstract = {Tactical Operations Center (TOC) system in military field is an advanced computer
system composed of multiple servers and desktops to interlock internal/external weapon
systems processing mission-critical applications in combat situation. However, the
current TOC system has several limitations such as difficulty of integrating tactical
weapon systems including missile launch system and radar system into the single TOC
system due to the heterogeneity of HW and SW between systems, and an inefficient computing
resource management for the weapon systems.In this paper, we proposed a novel HPC
supported mission-critical Cloud architecture as TOC for Surface-to-Air-Missile (SAM)
system with OpenStack Cloud OS, Data Distribution Service (DDS), and GPU virtualization
techniques. With this approach, our system provides elastic resource management over
the weapon systems with virtual machines, integration of heterogeneous systems with
different kinds of guest OS, real-time, reliable, and high-speed communication between
the virtual machines and virtualized GPU resource over the virtual machines. Evaluation
of our TOC system includes DDS performance measurement over 10Gbps Ethernet and QDR
InfiniBand networks on the virtualized environment with OpenStack Cloud OS, and GPU
virtualization performance evaluation with two different methods, PCI pass-through
and remote-API. With the evaluation results, we conclude that our system provides
reasonable performance in the combat situation compared to the previous TOC system
while additionally supports scalable and elastic use of computing resource through
the virtual machines.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {223–232},
numpages = {10},
keywords = {cloud computing, tactical operations center, data distribution service, gpgpu},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@article{10.1145/3442187,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet},
title = {VidCloud: Joint Stall and Quality Optimization for Video Streaming over Cloud},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3442187},
doi = {10.1145/3442187},
abstract = {As video-streaming services have expanded and improved, cloud-based video has evolved
into a necessary feature of any successful business for reaching internal and external
audiences. In this article, video streaming over distributed storage is considered
where the video segments are encoded using an erasure code for better reliability.
We consider a representative system architecture for a realistic (typical) content
delivery network (CDN). Given multiple parallel streams/link between each server and
the edge router, we need to determine, for each client request, the subset of servers
to stream the video, as well as one of the parallel streams from each chosen server.
To have this scheduling, this article proposes a two-stage probabilistic scheduling.
The selection of video quality is also chosen with a certain probability distribution
that is optimized in our algorithm. With these parameters, the playback time of video
segments is determined by characterizing the download time of each coded chunk for
each video segment. Using the playback times, a bound on the moment generating function
of the stall duration is used to bound the mean stall duration. Based on this, we
formulate an optimization problem to jointly optimize the convex combination of mean
stall duration and average video quality for all requests, where the two-stage probabilistic
scheduling, video quality selection, bandwidth split among parallel streams, and auxiliary
bound parameters can be chosen. This non-convex problem is solved using an efficient
iterative algorithm. Based on the offline version of our proposed algorithm, an online
policy is developed where servers selection, quality, bandwidth split, and parallel
streams are selected in an online manner. Experimental results show significant improvement
in QoE metrics for cloud-based video as compared to the considered baselines.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = jan,
articleno = {17},
numpages = {32},
keywords = {erasure codes, Video streaming over cloud, video quality, two-stage probabilistic scheduling, mean stall duration}
}

@inproceedings{10.1145/2666620.2666630,
author = {Vidas, Timothy and Tan, Jiaqi and Nahata, Jay and Tan, Chaur Lih and Christin, Nicolas and Tague, Patrick},
title = {A5: Automated Analysis of Adversarial Android Applications},
year = {2014},
isbn = {9781450331555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666620.2666630},
doi = {10.1145/2666620.2666630},
abstract = {Mobile malware is growing - both in overall volume and in number of existing variants
- at a pace rapid enough that systematic manual, human analysis is becoming increasingly
difficult. As a result, there is a pressing need for techniques and tools that provide
automated analysis of mobile malware samples. We present A5, an open source automated
system to process Android malware. A5 is a hybrid system combining static and dynamic
malware analysis techniques. Android's architecture permits many different paths for
malware to react to system events, any of which may result in malicious behavior.
Key innovations in A5 consist of novel methods of interacting with mobile malware
to better coerce malicious behavior, and in combining both virtual and physical pools
of Android platforms to capture behavior that could otherwise be missed. The primary
output of A5 is a set of network threat indicators and intrusion detection system
signatures that can be used to detect and prevent malicious network activity. We detail
A5's distributed design and demonstrate applicability of our interaction techniques
using examples from real malware. Additionally, we compare A5 with other automated
systems and provide performance measurements of an implementation, using a published
dataset of 1,260 unique malware samples, showing that A5 can quickly process large
amounts of malware. We provide a public web interface to our implementation of A5
that allows third parties to use A5 as a web service.},
booktitle = {Proceedings of the 4th ACM Workshop on Security and Privacy in Smartphones &amp; Mobile Devices},
pages = {39–50},
numpages = {12},
keywords = {dynamic analysis, mobile malware, sandbox, virtualization, static analysis, malicious behavior},
location = {Scottsdale, Arizona, USA},
series = {SPSM '14}
}

@inproceedings{10.1145/3052973.3053028,
author = {Inci, Mehmet Sinan and Eisenbarth, Thomas and Sunar, Berk},
title = {Hit by the Bus: QoS Degradation Attack on Android},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3053028},
doi = {10.1145/3052973.3053028},
abstract = {Mobile apps need optimal performance and responsiveness to rise amongst numerous rivals
on the market. Further, some apps like media streaming or gaming apps cannot even
function properly with a performance below a certain threshold. In this work, we present
the first performance degradation attack on Android OS that can target rival apps
using a combination of logical channel leakages and low-level architectural bottlenecks
in the underlying hardware. To show the viability of the attack, we design a proof-of-concept
app and test it on various mobile platforms. The attack runs covertly and brings the
target to the level of unresponsiveness. With less than 10% CPU time in the worst
case, it requires minimal computational effort to run as a background service, and
requires only the UsageStats permission from the user. We quantify the impact of our
attack using 11 popular benchmark apps, running 44 different tests.} The measured
QoS degradation varies across platforms and applications, reaching a maximum of 90%
in some cases. The attack combines the leakage from logical channels with low-level
architectural bottlenecks to design a malicious app that can covertly degrade Quality
of Service (QoS) of any targeted app. Furthermore, our attack code has a small footprint
and is not detected by the Android system as malicious. Finally, our app can pass
the Google Play Store malware scanner, Google Bouncer, as well as the top malware
scanners in the Play Store.},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {716–727},
numpages = {12},
keywords = {mobile security, mobile malware, performance degradation, QoS attack},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@inproceedings{10.1145/2684746.2689095,
author = {Ben Fakih, Hichem and Elhossini, Ahmed and Juurlink, Ben},
title = {An Efficient and Flexible FPGA Implementation of a Face Detection System (Abstract Only)},
year = {2015},
isbn = {9781450333153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684746.2689095},
doi = {10.1145/2684746.2689095},
abstract = {Robust and rapid face detection systems are constantly gaining more interest, since
they represent the first stone for many challenging tasks in the field of computer
vision. In this paper a software-hardware co-design approach is presented, that enables
the detection of frontal faces in real time. A complete hardware implementation of
all components taking part of the face detection is introduced. This work is based
on the object detection framework of Viola and Jones, which makes use of a cascade
of classifiers to reduce the computation time. The proposed architecture is flexible,
as it allows the use of multiple instances of the face detector. This makes developers
free to choose the speed range and reserved resources for this task. The current implementation
runs on the Zynq SoC and receives images over IP network, which allows exposing the
face detection task as a remote service that can be consumed from any device connected
to the network. We performed several measurements for the final detector and the software
equivalent. Using three Evaluator cores, the ZedBoard system achieves a maximal average
frame rate of 13.4 FPS when analysing an image containing 640x480 pixels. This stands
for an improvement of 5.25 times compared to the software solution and represents
acceptable results for most real-time systems. On the ZC706 system, a higher frame
rate of 16.58 FPS is achieved. The proposed hardware solution achieved 92% accuracy,
which is low compared to the software solution (97%) due to different scaling algorithm.
The proposed solution achieved higher frame rate compared to other solutions found
in the literature.},
booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {261},
numpages = {1},
keywords = {zynq, copmuter visioin, fpga, face detection, viola and jones},
location = {Monterey, California, USA},
series = {FPGA '15}
}

@inproceedings{10.1145/3131365.3131373,
author = {Chung, Taejoong and van Rijswijk-Deij, Roland and Choffnes, David and Levin, Dave and Maggs, Bruce M. and Mislove, Alan and Wilson, Christo},
title = {Understanding the Role of Registrars in DNSSEC Deployment},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131373},
doi = {10.1145/3131365.3131373},
abstract = {The Domain Name System (DNS) provides a scalable, flexible name resolution service.
Unfortunately, its unauthenticated architecture has become the basis for many security
attacks. To address this, DNS Security Extensions (DNSSEC) were introduced in 1997.
DNSSEC's deployment requires support from the top-level domain (TLD) registries and
registrars, as well as participation by the organization that serves as the DNS operator.
Unfortunately, DNSSEC has seen poor deployment thus far: despite being proposed nearly
two decades ago, only 1% of .com, .net, and .org domains are properly signed.In this
paper, we investigate the underlying reasons why DNSSEC adoption has been remarkably
slow. We focus on registrars, as most TLD registries already support DNSSEC and registrars
often serve as DNS operators for their customers. Our study uses large-scale, longitudinal
DNS measurements to study DNSSEC adoption, coupled with experiences collected by trying
to deploy DNSSEC on domains we purchased from leading domain name registrars and resellers.
Overall, we find that a select few registrars are responsible for the (small) DNSSEC
deployment today, and that many leading registrars do not support DNSSEC at all, or
require customers to take cumbersome steps to deploy DNSSEC. Further frustrating deployment,
many of the mechanisms for conveying DNSSEC information to registrars are error-prone
or present security vulnerabilities. Finally, we find that using DNSSEC with third-party
DNS operators such as Cloudflare requires the domain owner to take a number of steps
that 40% of domain owners do not complete. Having identified several operational challenges
for full DNSSEC deployment, we make recommendations to improve adoption.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {369–383},
numpages = {15},
keywords = {registrar, DNS, DNS security extension, public key infrastructure, PKI, DNS operator, DNSSEC},
location = {London, United Kingdom},
series = {IMC '17}
}

@inproceedings{10.1145/3003733.3003756,
author = {Efthymiopoulos, Nikolaos and Efthymiopoulou, Maria and Christakidis, Athanasios},
title = {Experimentation on Low Delay and Stable Congestion Control for P2P Video Streaming},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003756},
doi = {10.1145/3003733.3003756},
abstract = {In recent years, a number of research efforts have focused on using peer-to-peer (P2P)
systems in order to provide live streaming (LS) and Video-on-Demand (VoD) services.
Most of them focused on the development of distributed P2P block schedulers for content
exchange among the participating peers and on the architecture of the overlay graph
(P2P overlay) that interconnects the set of these peers. Currently, the effort has
shifted towards the combination of P2P systems with cloud infrastructures. By deploying
monitoring and control architectures they use resources from the cloud in order to
enhance the QoS, thus achieving an attractive trade-off between stability and low
cost operation. However, there is a lack of research effort on the congestion control
layer of these systems while the existing congestion control architectures in use
are not suited for P2P traffic. This paper proposes a P2P congestion control protocol
suitable for LS and VoD that: i) is capable to manage sequential traffic to multiple
network destinations, ii) efficiently exploits the available bandwidth, iii) accurately
measures the idle peers' resources, iv) it avoids network congestion, and v) is friendly
to other TCP generated traffic. Our proposed algorithms and protocol have been implemented,
tested and evaluated through a series of real experiments in the context of STEER
[20].},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {48},
numpages = {6},
keywords = {video streaming, P2P, congestion control},
location = {Patras, Greece},
series = {PCI '16}
}

@inproceedings{10.1145/3375555.3384938,
author = {Zibitsker, Boris and Lupersolsky, Alex},
title = {How to Apply Modeling to Compare Options and Select the Appropriate Cloud Platform},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3384938},
doi = {10.1145/3375555.3384938},
abstract = {Organizations want to take advantage of the flexibility and scalability of Cloud platforms.
By migrating to the Cloud, they hope to develop and implement new applications faster
with lower cost. Amazon AWS, Microsoft Azure, Google, IBM, Oracle and others Cloud
providers support different DBMS like Snowflake, Redshift, Teradata Vantage, and others.
These platforms have different architectures, mechanisms of allocation and management
of resources, and levels of sophistication of DBMS optimizers which affect performance,
scalability and cost. As a result, the response time, CPU Service Time and the number
of I/Os for the same query, accessing the similar table in the Cloud could be significantly
different than On Prem. In order to select the appropriate Cloud platform as a first
step we perform a Workload Characterization for On Prem Data Warehouse. Each Data
Warehouse workload represents a specific line of business and includes activity of
many users generating concurrently simple and complex queries accessing data from
different tables. Each workload has different demands for resources and different
Response Time and Throughput Service Level Goals. In this presentation we will review
results of the workload characterization for an On Prem Data Warehouse environment.
During the second step we collected measurement data for standard TPC-DS benchmark
tests performed in AWS Vantage, Redshift and Snowflake Cloud platform for different
sizes of the data sets and different number of concurrent users. During the third
step we used the results of the workload characterization and measurement data collected
during the benchmark to modify BEZNext On Prem Closed Queueing model to model individual
Clouds. And finally, during the fourth step we used our Model to take into consideration
differences in concurrency, priorities and resource allocation to different workloads.
BEZNext optimization algorithms incorporating Graduate search mechanism are used to
find the AWS instance type and minimum number of instances which will be required
to meet SLGs for each of the workloads. Publicly available information about the cost
of the different AWS instances is used to predict the cost of supporting workloads
in the Cloud month by month during next 12 months.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {16},
numpages = {1},
keywords = {seasonality determination, service level goals, workload forecasting, benchmarking, workload characterization, cloud platform, optimization., modeling},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/2959100.2959122,
author = {Celma, Oscar},
title = {The Exploit-Explore Dilemma in Music Recommendation},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959100.2959122},
doi = {10.1145/2959100.2959122},
abstract = {Were The Rolling Stones right when they said, "You can't always get what you want;
but if you try sometime you get what you need"? Recommendation systems are the crystal
ball of the Internet: predicting user intentions, making sense of big data, and delivering
what people are looking for before they even know they want it. Pandora radio is best
known for the Music Genome Project; the most unique and richly labeled music catalog
of 1.5 million+ tracks. While this content-based approach to music recommendation
is extremely effective and still used today as the foundation to the leading online
radio service, Pandora has also collected more than a decade of contextual listener
feedback in the form of more than 65 billion thumbs from 79M+ monthly active users
who have created more than 9 billion stations. This session will look at how the interdisciplinary
team at Pandora goes about making sense of these massive data sets to successfully
make large scale music recommendations to our listeners.As opposed to more traditional
recommender systems which need only to recommend a single item or set of items, Pandora's
recommenders must provide an evolving set of sequential items, which constantly keep
the experience new and exciting. In this talk I will present a dynamic ensemble learning
system that combines musicological data and machine learning models to provide a truly
personalized experience. This approach allows us to switch from a lean back experience
(exploitation) to a more exploration mode to discover new music tailored specifically
to users individual tastes. To exemplify this, I will present a recently launched
product led by the research team, Thumbprint Radio.Following this session the audience
will have an in-depth understanding of how Pandora uses science to determine the perfect
balance of familiarity, discovery, repetition and relevance for each individual listener,
measures and evaluates user satisfaction, and how our online and offline architecture
stack plays a critical role in our success.},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
pages = {377},
numpages = {1},
keywords = {machine listening, ensemble learning, content-based recommendation, thumbprint radio, A/B online testing, offline evaluation, exploit-explore dilemma},
location = {Boston, Massachusetts, USA},
series = {RecSys '16}
}

@inproceedings{10.1145/3241539.3241567,
author = {Marquez, Cristina and Gramaglia, Marco and Fiore, Marco and Banchs, Albert and Costa-Perez, Xavier},
title = {How Should I Slice My Network? A Multi-Service Empirical Evaluation of Resource Sharing Efficiency},
year = {2018},
isbn = {9781450359030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241539.3241567},
doi = {10.1145/3241539.3241567},
abstract = {By providing especially tailored instances of a virtual network,network slicing allows
for a strong specialization of the offered services on the same shared infrastructure.
Network slicing has profound implications on resource management, as it entails an
inherent trade-off between: (i) the need for fully dedicated resources to support
service customization, and (ii) the dynamic resource sharing among services to increase
resource efficiency and cost-effectiveness of the system. In this paper, we provide
a first investigation of this trade-off via an empirical study of resource management
efficiency in network slicing. Building on substantial measurement data collected
in an operational mobile network (i) we quantify the efficiency gap introduced by
non-reconfigurable allocation strategies of different kinds of resources, from radio
access to the core of the network, and (ii) we quantify the advantages of their dynamic
orchestration at different timescales. Our results provide insights on the achievable
efficiency of network slicing architectures, their dimensioning, and their interplay
with resource management algorithms.},
booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
pages = {191–206},
numpages = {16},
keywords = {resource management, network efficiency, network slicing},
location = {New Delhi, India},
series = {MobiCom '18}
}

@inproceedings{10.5555/2667510.2667516,
author = {Seneviratne, Janaka and Parampalli, Udaya and Kulik, Lars},
title = {An Authorised Pseudonym System for Privacy Preserving Location Proof Architectures},
year = {2014},
isbn = {9781921770326},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {An emerging class of Location Based Services (LBSs) needs verified mobile device locations
for service provision. For example, an automated car park billing system requires
verified locations of cars to confirm the place and the duration of parked cars. Location
Proof Architectures (LPAs) allow a user (or a device on behalf of its user) to obtain
a proof of its presence at a location from a trusted third party. A major concern
in LPAs is to preserve user location privacy. To achieve this a user's identity and
location data should be maintained separately with additional measures that prevent
leaking sensitive identity and location data. In this paper, we present a privacy
preserving LPA in which users appear under pseudonyms. Our main contribution is a
third party free pseudonym registering protocol based on blind signature schemes.
We show that our protocol allows to build a pseudonym system with a guaranteed degree
of privacy agreed at the time of pseudonym registration. We also demonstrate that
a pseudonym can be authenticated across different organizations in an LPA. Our system
ensures that (i) only authenticated users can register their pseudonyms, (ii) the
pseudonyms have a consistent degree of privacy at the point of registration and (iii)
a user cannot take another user's pseudonym.},
booktitle = {Proceedings of the Twelfth Australasian Information Security Conference - Volume 149},
pages = {47–56},
numpages = {10},
keywords = {pseudonym, location proof architecture, privacy},
location = {Auckland, New Zealand},
series = {AISC '14}
}

@inproceedings{10.1145/2656075.2656086,
author = {Hsieh, Chih-Ming and Samie, Farzad and Srouji, M. Sammer and Wang, Manyi and Wang, Zhonglei and Henkel, J\"{o}rg},
title = {Hardware/Software Co-Design for a Wireless Sensor Network Platform},
year = {2014},
isbn = {9781450330510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656075.2656086},
doi = {10.1145/2656075.2656086},
abstract = {Wireless sensor networks have become shared resources providing sensing services to
monitor ambient environment. The tasks performed by the sensor nodes and the network
structure are becoming more and more complex so that they cannot be handled efficiently
by traditional sensor nodes any more. The traditional sensor node architecture, which
has software implementation running on a fixed hardware design, is no longer fit to
the changing requirements when new applications with complex computation are added
to this shared infrastructure due to several reasons. First, the operation behavior
changes because of the application requirements and the environmental conditions which
makes a fixed architecture not efficient all the time. Second, to collaborate with
other already deployed sensor networks and to maintain an efficient network structure,
the sensor nodes require flexible communication capabilities. Furthermore, the information
required to determine an efficient hardware/software co-design under the system constraints
cannot be known a priori. Therefore a platform which can adapt to run-time situations
will play an important role in wireless sensor networks. In this paper, we present
a hardware/software co-design framework for a wireless sensor platform, which can
adaptively change its hardware/software configuration to accelerate complex operations
and provides a flexible communication mechanism to deal with complex network structures.
We perform real-world measurements on our prototype to analyze its capabilities. In
addition, our case studies with prototype implementation and network simulations show
the energy savings of the sensor network application by using the proposed design
with run-time adaptivity.},
booktitle = {Proceedings of the 2014 International Conference on Hardware/Software Codesign and System Synthesis},
articleno = {1},
numpages = {10},
keywords = {FPGA, sensor networks, multi-radio, reconfiguration, hardware accelerator, low power},
location = {New Delhi, India},
series = {CODES '14}
}

@inproceedings{10.1145/2789168.2790089,
author = {Kurose, James F.},
title = {Research Challenges and Opportunities in a Mobility-Centric World},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2790089},
doi = {10.1145/2789168.2790089},
abstract = {The Internet recently passed an historic inflection point, with the number of broadband
mobile devices surpassing the number of wired PCs and servers connected to the Internet.
Mobility now profoundly affects the architecture, services and applications in both
the wireless and wired domains. In this "bottom up" talk, we begin by discussing several
specific mobility-related challenges and recent results in areas including mobility
measurement (including privacy considerations) and modeling, and context-sensitive
services. We then take a broader look at current and future challenges, and conclude
by discussing several NSF investments in programs and projects in area of mobile networking.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {290},
numpages = {1},
keywords = {mobility, computer networks, architecture, measurement modeling},
location = {Paris, France},
series = {MobiCom '15}
}

@inproceedings{10.1145/3240765.3243485,
author = {Wei, Tianshu and Chen, Xiaoming and Li, Xin and Zhu, Qi},
title = {Model-Based and Data-Driven Approaches for Building Automation and Control},
year = {2018},
isbn = {9781450359504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240765.3243485},
doi = {10.1145/3240765.3243485},
abstract = {Smart buildings in the future are complex cyber-physical-human systems that involve
close interactions among embedded platform (for sensing, computation, communication
and control), mechanical components, physical environment, building architecture,
and occupant activities. The design and operation of such buildings require a new
set of methodologies and tools that can address these heterogeneous domains in a holistic,
quantitative and automated fashion. In this paper, we will present our design automation
methods for improving building energy efficiency and offering comfortable services
to occupants at low cost. In particular, we will highlight our work in developing
both model-based and data-driven approaches for building automation and control, including
methods for co-scheduling heterogeneous energy demands and supplies, for integrating
intelligent building energy management with grid optimization through a proactive
demand response framework, for optimizing HVAC control with deep reinforcement learning,
and for accurately measuring in-building temperature by combining prior modeling information
with few sensor measurements based upon Bayesian inference.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
articleno = {26},
numpages = {8},
keywords = {smart buildings, deep reinforcement learning, model-based design, Bayesian inference, data-driven, model predictive control},
location = {San Diego, California},
series = {ICCAD '18}
}

@inproceedings{10.1145/2627566.2627575,
author = {Antonescu, Alexandru-Florian and Braun, Torsten},
title = {Modeling and Simulation of Concurrent Workload Processing in Cloud-Distributed Enterprise Information Systems},
year = {2014},
isbn = {9781450329927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627566.2627575},
doi = {10.1145/2627566.2627575},
abstract = {Cloud Computing enables provisioning and distribution of highly scalable services
in a reliable, on-demand and sustainable manner. Distributed Enterprise Information
Systems (dEIS) are a class of applications with important economic value and with
strong requirements in terms of performance and reliability. In order to validate
dEIS architectures, stability, scaling and SLA compliance, large testing deployments
are necessary, adding complexity to the design and testing of such systems. To fill
this gap, we present and validate a methodology for modeling and simulating such complex
distributed systems using the CloudSim cloud computing simulator, based on measurement
data from an actual distributed system. We present an approach for creating a performance-based
model of a distributed cloud application using recorded service performance traces.
We then show how to integrate the created model into CloudSim. We validate the CloudSim
simulation model by comparing performance traces gathered during distributed concurrent
experiments with simulation results using different VM configurations. We demonstrate
the usefulness of using a cloud simulator for modeling properties of real cloud-distributed
applications.},
booktitle = {Proceedings of the 2014 ACM SIGCOMM Workshop on Distributed Cloud Computing},
pages = {11–16},
numpages = {6},
keywords = {cloud computing, performance profiling, distributed applications, modelling and simulation},
location = {Chicago, Illinois, USA},
series = {DCC '14}
}

@inproceedings{10.1145/2789168.2790094,
author = {Cui, Yong and Lai, Zeqi and Wang, Xin and Dai, Ningwei and Miao, Congcong},
title = {QuickSync: Improving Synchronization Efficiency for Mobile Cloud Storage Services},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2790094},
doi = {10.1145/2789168.2790094},
abstract = {Mobile cloud storage services have gained phenomenal success in recent few years.
In this paper, we identify, analyze and address the synchronization (sync) inefficiency
problem of modern mobile cloud storage services. Our measurement results demonstrate
that existing commercial sync services fail to make full use of available bandwidth,
and generate a large amount of unnecessary sync traffic in certain circumstance even
though the incremental sync is implemented. These issues are caused by the inherent
limitations of the sync protocol and the distributed architecture. Based on our findings,
we propose QuickSync, a system with three novel techniques to improve the sync efficiency
for mobile cloud storage services, and build the system on two commercial sync services.
Our experimental results using representative workloads show that QuickSync is able
to reduce up to 52.9% sync time in our experiment settings.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {592–603},
numpages = {12},
keywords = {performance, mobile cloud storage, measurement},
location = {Paris, France},
series = {MobiCom '15}
}

@inproceedings{10.1145/2987443.2987482,
author = {Orsini, Chiara and King, Alistair and Giordano, Danilo and Giotsas, Vasileios and Dainotti, Alberto},
title = {BGPStream: A Software Framework for Live and Historical BGP Data Analysis},
year = {2016},
isbn = {9781450345262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987443.2987482},
doi = {10.1145/2987443.2987482},
abstract = {We present BGPStream, an open-source software framework for the analysis of both historical
and real-time Border Gateway Protocol (BGP) measurement data. Although BGP is a crucial
operational component of the Internet infrastructure, and is the subject of research
in the areas of Internet performance, security, topology, protocols, economics, etc.,
there is no efficient way of processing large amounts of distributed and/or live BGP
measurement data. BGPStream fills this gap, enabling efficient investigation of events,
rapid prototyping, and building complex tools and large-scale monitoring applications
(e.g., detection of connectivity disruptions or BGP hijacking attacks). We discuss
the goals and architecture of BGPStream. We apply the components of the framework
to different scenarios, and we describe the development and deployment of complex
services for global Internet monitoring that we built on top of it.},
booktitle = {Proceedings of the 2016 Internet Measurement Conference},
pages = {429–444},
numpages = {16},
keywords = {bgp measurement, network monitoring, internet measurement, bgp monitoring, internet routing, network measurement, real-time monitoring},
location = {Santa Monica, California, USA},
series = {IMC '16}
}

@inproceedings{10.1145/3307334.3326070,
author = {Xiao, Ao and Liu, Yunhao and Li, Yang and Qian, Feng and Li, Zhenhua and Bai, Sen and Liu, Yao and Xu, Tianyin and Xin, Xianlong},
title = {An In-Depth Study of Commercial MVNO: Measurement and Optimization},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326070},
doi = {10.1145/3307334.3326070},
abstract = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs),
which operate on top of the existing cellular infrastructures of base carriers while
offering cheaper or more flexible data plans compared to those of the base carriers.
In this paper, we present a nearly two-year measurement study towards understanding
various key aspects of today's MVNO ecosystem, including its architecture, performance,
economics, customers, and the complex interplay with the base carrier. Our study focuses
on a large commercial MVNO with reviseabout 1 million customers, operating atop a
nation-wide base carrier. Our measurements clarify several key concerns raised by
MVNO customers, such as inaccurate billing and potential performance discrimination
with the base carrier. We also leverage big data analytics and machine learning to
optimize an MVNO's key businesses such as data plan reselling and customer churn mitigation.
Our proposed techniques can help achieve %will lead to higher revenues and improved
services for commercial MVNOs.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {457–468},
numpages = {12},
keywords = {mvno, network performance, machine learning, churn mitigation, data prediction},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@article{10.1109/TNET.2020.2981514,
author = {Li, Yang and Zheng, Jianwei and Li, Zhenhua and Liu, Yunhao and Qian, Feng and Bai, Sen and Liu, Yao and Xin, Xianlong},
title = {Understanding the Ecosystem and Addressing the Fundamental Concerns of Commercial MVNO},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2981514},
doi = {10.1109/TNET.2020.2981514},
abstract = {Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs),
which operate on top of existing cellular infrastructures of base carriers, while
offering cheaper or more flexible data plans compared to those of the base carriers.
In this paper, we present a two-year measurement study towards understanding various
fundamental aspects of today's MVNO ecosystem, including its architecture, customers,
performance, economics, and the complex interplay with the base carrier. Our study
focuses on a large commercial MVNO with one million customers, operating atop a nation-wide
base carrier. Our measurements clarify several key concerns raised by MVNO customers,
such as inaccurate billing and potential performance discrimination with the base
carrier. We also leverage big data analytics, statistical modeling, and machine learning
to address the MVNO's key concerns with regard to data usage prediction, data plan
reselling, customer churn mitigation, and billing delay reduction. Our proposed techniques
can help achieve higher revenues and improved services for commercial MVNOs.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {1364–1377},
numpages = {14}
}

@inproceedings{10.1145/3331076.3331108,
author = {Buccafurri, Francesco and Musarella, Lorenzo and Nardone, Roberto},
title = {Enabling Propagation in Web of Trust by Ethereum},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331108},
doi = {10.1145/3331076.3331108},
abstract = {Web of Trust offers a way to bind identities with the corresponding public keys. It
relies on a distributed architecture, where each user could play the role of certificate
signer. With the widespread diffusion of social networks, the trust propagation is
a matter of growing interest. This paper proposes an approach enabling the propagation
in Web of Trust by means of Ethereum. The usage of Ethereum eliminates the necessity
of single-organization trusted services, which is, in general, not realistic. Although
the information stored on Ethereum is public, the privacy of users is protected because
trust chains involve only Ethereum addresses and strong measures are implemented to
contrast their malicious de-anonymization. The approach relies on the usage of a smart
contract for storing the status of certificate signatures and to manage revocations.
When a user u wants to trust another user v, the smart contract checks the presence
of trust chains originating from root nodes of u.},
booktitle = {Proceedings of the 23rd International Database Applications &amp; Engineering Symposium},
articleno = {9},
numpages = {6},
keywords = {trust propagation, blockchain, smart contract, social network, pretty good privacy, Ethereum},
location = {Athens, Greece},
series = {IDEAS '19}
}

@inproceedings{10.1145/3267809.3275470,
author = {Nadgowda, Shripad and Isci, Canturk},
title = {Drishti: Disaggregated and Interoperable Security Analytics Framework for Cloud},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3275470},
doi = {10.1145/3267809.3275470},
abstract = {Application and platform security has always been critical for the success of any
business. Traditionally, applications were deployed directly on physical servers.
As a result, there are myriad traditional security solutions that were developed around
this model to run as local agents on the systems they monitor and protect. These solutions
are then refined and standardized with decades of experience. With the emergence of
virtualization, cloud and particularly containerization, use of these solutions is
becoming challenging with consolidation and scale. As we begin to deploy hundreds
of cloud instances on a single node, traditional solutions, designed for local execution
do not scale out. At the same time, the clean separation of a virtual machine (VM)
or a container from the platform itself, and maturing introspection and inspection
APIs provide a simple, practical way to decouple monitored from the monitors [3].
Furthermore, as the scope of cloud security expands from simple monitoring and auditing
to more complex learning based analytics, analytics components are further offloaded
to separate data services, where they can burn extensive cycles, and in some cases
use specialized hardware for security analytics, out of the critical path of the monitored
applications [5]. As a result, traditional agent-based tightly-coupled model is being
replaced by a more dis-aggregated {system, observation, analytics, actions} architecture.To
implement such dis-aggregated model in practice, first system state needs to be transferred
from cloud platform to analytic platform. File system more generally is representative
of the system state that persists features of interest for security analytics like
processes, metrics, configurations, packages across various files. Remote replication
or snapshotting [1] of whole file system is very in-efficient, since only small set
of files are accessed during the analytics. As a result, a new family of cloud-native
security solutions have recently emerged in the field that uses various specialized
data collection techniques[2, 4]. These techniques perform out-of band introspection
of systems to interpret and extract required system features from the file system
to essentially serialize system state into data. This data is then transferred to
an analytic platform for analysis. Unlike the traditional security solutions that
work locally against the system's standard POSIXy file system interfaces, these emerging
security analytics "work from data" on the analytic platform. However since the target
system is now available as "data", existing agent-based security solutions become
incompatible to work against the system. One mitigating solution is to rewrite all
existing solutions, which requires huge amount of resources and effort.In Drishti,
we address this challenge from a fundamentally different perspective. Instead of rewriting
security solutions to work from data, we make the data work for traditional security
applications. We achieve this by developing a pseudo-system interface over systems
data collected from cloud instances. With this approach, existing solutions run unmodified,
as "black box" software over this system interface, as if they were running on the
actual cloud instance. Drishti framework is our realization of this approach. It is
logically the inverse of the first step of cloud-native security analytics that convert
system state into data. With Drishti we transform data back to system on the analytic
platform by orchestrating two file system components. First, a standard native system
interface is re-calibrated over the system data through our new FUSE file system,
confuse or ClOud Native Filesystem in UserSpacE. Second, we mimic the "effect" of
an agent installation via an overlay file system based on the the agent image. Within
the Drishti framework the underlying data looks like a standard POSIX system to each
on-boarded security solution. This allows us to run existing agent-based security
solutions as is, but still decoupled from the actual system. Drishti also provides
a standard and interoperable platform for designing new security analytic solutions.Overall,
Drishti demonstrates a novel, pragmatic and highly-practical approach for bringing
security analytics into the cloud. It enables us to leverage existing solutions built
based on decades of experience by eliminating the need for reinventing the wheel for
cloud.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {528},
numpages = {1},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@article{10.1109/TNET.2020.2976129,
author = {Xie, Kun and Chen, Yuxiang and Wang, Xin and Xie, Gaogang and Cao, Jiannong and Wen, Jigang},
title = {Accurate and Fast Recovery of Network Monitoring Data: A GPU Accelerated Matrix Completion},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2976129},
doi = {10.1109/TNET.2020.2976129},
abstract = {Gaining a full knowledge of end-to-end network performance is important for some advanced
network management and services. Although it becomes increasingly critical, end-to-end
network monitoring usually needs active probing of the path and the overhead will
increase quadratically with the number of network nodes. To reduce the measurement
overhead, matrix completion is proposed recently to predict the end-to-end network
performance among all node pairs by only measuring a small set of paths. Despite its
potential, applying matrix completion to recover the missing data suffers from low
recovery accuracy and long recovery time. To address the issues, we propose MC-GPU
to exploit Graphics Processing Units (GPUs) to enable parallel matrix factorization
for high-speed and highly accurate Matrix Completion. To well exploit the special
architecture features of GPUs for both task independent and data-independent parallel
task execution, we propose several novel techniques: similar OD (origin and destination)
pairs reordering taking advantage of the locality-sensitive hash (LSH) functions,
balanced matrix partition, and parallel matrix completion. We implement the proposed
MC-GPU on the GPU platform and evaluate the performance using real trace data. We
compare the proposed MC-GPU with the state of the art matrix completion algorithms,
and our results demonstrate that MC-GPU can achieve significantly faster speed with
high data recovery accuracy.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {958–971},
numpages = {14}
}

@inproceedings{10.1145/3197768.3201560,
author = {Tzallas, Alexandros T. and Katertsidis, Nikolaos and Glykos, Konstantinos and Segkouli, Sofia and Votis, Konstantinos and Tzovaras, Dimitrios and Barru\'{e}, Cristian and Paliokas, Ioannis and Cort\'{e}s, Ulises},
title = {Designing a Gamified Social Platform for People Living with Dementia and Their Live-in Family Caregivers},
year = {2018},
isbn = {9781450363907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197768.3201560},
doi = {10.1145/3197768.3201560},
abstract = {In the current paper, a social gamified platform for people living with dementia and
their live-in family caregivers, integrating a broader diagnostic approach and interactive
interventions is presented. The CAREGIVERSPRO-MMD (C-MMD) platform constitutes a support
tool for the patient and the informal caregiver - also referred to as the dyad - that
strengthens self-care, and builds community capacity and engagement at the point of
care. The platform is implemented to improve social collaboration, adherence to treatment
guidelines through gamification, recognition of progress indicators and measures to
guide management of patients with dementia, and strategies and tools to improve treatment
interventions and medication adherence. Moreover, particular attention was provided
on guidelines, considerations and user requirements for the design of a User-Centered
Design (UCD) platform. The design of the platform has been based on a deep understanding
of users, tasks and contexts in order to improve platform usability, and provide adaptive
and intuitive User Interfaces with high accessibility. In this paper, the architecture
and services of the C-MMD platform are presented, and specifically the gamification
aspects.},
booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
pages = {476–481},
numpages = {6},
keywords = {Dementia, Cloud Platform, Interventions, Social Networking, Caregivers, Gamification, Self-management},
location = {Corfu, Greece},
series = {PETRA '18}
}

@article{10.1145/3057857,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Visual Simple Transformations: Empowering End-Users to Wire Internet of Things Objects},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3057857},
doi = {10.1145/3057857},
abstract = {Empowering end-users to wire Internet of Things (IoT) objects (things and services)
together would allow them to more easily conceive and realize interesting IoT solutions.
A challenge lies in devising a simple end-user development approach to support the
specification of transformations, which can bridge the mismatch in the data being
exchanged among IoT objects. To tackle this challenge, we present Visual Simple Transformations
(ViSiT) as an approach that allows end-users to use a jigsaw puzzle metaphor for specifying
transformations that are automatically converted into underlying executable workflows.
ViSiT is explained by presenting meta-models and an architecture for implementing
a system of connected IoT objects. A tool is provided for supporting end-users in
visually developing and testing transformations. Another tool is also provided for
allowing software developers to modify, if they wish, a transformation's underlying
implementation. This work was evaluated from a technical perspective by developing
transformations and measuring ViSiT's efficiency and scalability and by constructing
an example application to show ViSiT's practicality. A study was conducted to evaluate
this work from an end-user perspective, and its results showed positive indications
of perceived usability, learnability, and the ability to conceive real-life scenarios
for ViSiT.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {10},
numpages = {43},
keywords = {End-user development, transformations, internet of things}
}

@inproceedings{10.1145/3053600.3053634,
author = {Walter, J\"{u}rgen and Stier, Christian and Koziolek, Heiko and Kounev, Samuel},
title = {An Expandable Extraction Framework for Architectural Performance Models},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053634},
doi = {10.1145/3053600.3053634},
abstract = {Providing users with Quality of Service (QoS) guarantees and the prevention of performance
problems are challenging tasks for software systems. Architectural performance models
can be applied to explore performance properties of a software system at design time
and run time. At design time, architectural performance models support reasoning on
effects of design decisions. At run time, they enable automatic reconfigurations by
reasoning on the effects of changing user behavior. In this paper, we present a framework
for the extraction of architectural performance models based on monitoring log files
generalizing over the targeted architectural modeling language. Using the presented
framework, the creation of a performance model extraction tool for a specific modeling
formalism requires only the implementation of a key set of object creation routines
specific to the formalism. Our framework integrates them with extraction techniques
that apply to many architectural performance models, e.g., resource demand estimation
techniques. This lowers the effort to implement performance model extraction tools
tremendously through a high level of reuse. We evaluate our framework presenting builders
for the Descartes Modeling Language (DML) and the Palladio Component Model(PCM). For
the extracted models we compare simulation results with measurements receiving accurate
results.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {165–170},
numpages = {6},
keywords = {descartes modeling language, palladio component model, automated performance model extraction, builder pattern},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1145/2968216,
author = {Maqsood, Tahir and Khalid, Osman and Irfan, Rizwana and Madani, Sajjad A. and Khan, Samee U.},
title = {Scalability Issues in Online Social Networks},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/2968216},
doi = {10.1145/2968216},
abstract = {The last decade witnessed a tremendous increase in popularity and usage of social
network services, such as Facebook, Twitter, and YouTube. Moreover, advances in Web
technologies coupled with social networks has enabled users to not only access, but
also generate, content in many forms. The overwhelming amount of produced content
and resulting network traffic gives rise to precarious scalability issues for social
networks, such as handling a large number of users, infrastructure management, internal
network traffic, content dissemination, and data storage. There are few surveys conducted
to explore the different dimensions of social networks, such as security, privacy,
and data acquisition. Most of the surveys focus on privacy or security-related issues
and do not specifically address scalability challenges faced by social networks. In
this survey, we provide a comprehensive study of social networks along with their
significant characteristics and categorize social network architectures into three
broad categories: (a) centralized, (b) decentralized, and (c) hybrid. We also highlight
various scalability issues faced by social network architectures. Finally, a qualitative
comparison of presented architectures is provided, which is based on various scalability
metrics, such as availability, latency, interserver communication, cost of resources,
and energy consumption, just to name a few.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {40},
numpages = {42},
keywords = {social network, Scalability, hybrid social networks, decentralized social networks, centralized social networks}
}

@article{10.1109/TNET.2013.2253797,
author = {De Cicco, Luca and Mascolo, Saverio},
title = {An Adaptive Video Streaming Control System: Modeling, Validation, and Performance Evaluation},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2013.2253797},
doi = {10.1109/TNET.2013.2253797},
abstract = {Adaptive video streaming is a relevant advancement with respect to classic progressive
download streaming a la YouTube. Among the different approaches, the video stream-switching
technique is getting wide acceptance, being adopted by Microsoft, Apple, and popular
video streaming services such as Akamai, Netflix, Hulu, Vudu, and Livestream. In this
paper, we present a model of the automatic video stream-switching employed by one
of these leading video streaming services along with a description of the client-side
communication and control protocol. From the control architecture point of view, the
automatic adaptation is achieved by means of two interacting control loops having
the controllers at the client and the actuators at the server: One loop is the buffer
controller, which aims at steering the client playout buffer to a target length by
regulating the server sending rate; the other one implements the stream-switching
controller and aims at selecting the video level. A detailed validation of the proposed
model has been carried out through experimental measurements in an emulated scenario.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {526–539},
numpages = {14},
keywords = {adaptive video streaming, modeling, stream-switching, performance evaluation}
}

@article{10.1145/3457143,
author = {Burny, Nicolas and Vanderdonckt, Jean},
title = {UiLab, a Workbench for Conducting and Reproducing Experiments in GUI Visual Design},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3457143},
doi = {10.1145/3457143},
abstract = {With the continuously increasing number and variety of devices, the study of visual
design of their Graphical User Interfaces grows in importance and scope, particularly
for new devices, including smartphones, tablets, and large screens. Conducting a visual
design experiment typically requires defining and building a GUI dataset with different
resolutions for different devices, computing visual design measures for the various
configurations, and analyzing their results. This workflow is very time- and resource-consuming,
therefore limiting its reproducibility. To address this problem, we present UiLab,
a cloud-based workbench that parameterizes the settings for conducting an experiment
on visual design of Graphical User Interfaces, for facilitating the design of such
experiments by automating some workflow stages, and for fostering their reproduction
by automating their deployment. Based on requirements elicited for UiLab, we define
its conceptual model to delineate the borders of services of the software architecture
to support the new workflow. We exemplify it by demonstrating a system walkthrough
and we assess its impact on experiment reproducibility in terms of design and development
time saved with respect to a classical workflow. Finally, we discuss potential benefits
brought by this workbench with respect to reproducing experiments in GUI visual design
and existing shortcomings to initiate future avenues. We publicly release UiLab source
code on a GitHub repository.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {196},
numpages = {31},
keywords = {usability evaluation, visual design, user interface evaluation, aesthetics}
}

@inproceedings{10.1145/2976767.2987689,
author = {Falkner, Katrina and Szabo, Claudia and Chiprianov, Vanea},
title = {Model-Driven Performance Prediction of Systems of Systems},
year = {2016},
isbn = {9781450343213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976767.2987689},
doi = {10.1145/2976767.2987689},
abstract = {Systems of Systems exhibit characteristics that pose difficulty in modelling and predicting
their overall performance capabilities, including the presence of operational independence,
emergent behaviour, and evolutionary development. When considering Systems of Systems
within the autonomous defence systems context, these aspects become increasingly critical,
as performance constraints are typically driven by hard constraints on space, weight
and power.System execution modelling languages and tools permit early prediction of
the performance of model-driven systems, however the focus to date has been on understanding
the performance of a model rather than determining if it meets performance requirements,
and only subsequently carrying out analysis to reveal the causes of any requirement
violations. Such an analysis is even more difficult when applied to several systems
cooperating to achieve a common goal - a System of Systems (SoS).The successful integration
of systems within a SoS context has been identified as one of the most substantial
challenges facing military systems development [2]. Accordingly, there is a critical
need to understand the non-functional aspects of the SoS (such as quality of service,
power, size, cost and scalable management of communications), and to explore how these
non-functional aspects evolve under new conditions and deployment scenarios. It is
crucial that we develop methodologies for modelling and understanding non-functional
properties early in the development and integration cycle to better inform our understanding
of the impact of emergent behaviour and evolution within the SoS.We propose an integrated
approach to performance prediction of model-driven real time embedded defence systems
and systems of systems [1]. Our architectural prototyping system supports a scenario-driven
experimental platform for evaluating model suitability within a set of deployment
and real-time performance constraints. We present an overview of our performance prediction
system, demonstrating the integration of modelling, execution and performance analysis,
and discuss a case study to illustrate our approach. Our work employs state-of-the-art
model-driven engineering techniques to facilitate SoS performance prediction and analysis
at design time, either before the SoS is built and deployed, or during its lifetime
when required to evolve.Our model-driven performance prediction platform supports
a scenario-driven experimental environment for evaluating a SoS within the context
of a specific deployment (modelling geographical distribution) and integration constraints.
The main contributions of our work are: (a) a modeling methodology that captures diverse
perspectives of the performance modeling of Systems of Systems; (b) a performance
analysis engine that captures metrics associated with these perspectives and (c) a
case study showing the performance evaluaton of a system of systems and its evolution
as a result of the performance analysis. We discuss how our approach to modelling
supports the specific characteristics of an SoS, and illustrate this through a case
study, based on a "Blue Ocean" scenario, demonstrating how we may obtain performance
predictions within a SoS with emergent and evolutionary properties. Within the context
of our environment, we define models for the individual systems within our System
of Systems, defined for representative workload to predict execution costs, i.e. CPU,
memory usage and network usage, within a generic situation. Our modelling environment
supports the generation of executable forms of these models, which may then be executed
above realistic deployment scenarios in order to obtain predictions of System of System
performance.},
booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
pages = {44},
numpages = {1},
location = {Saint-malo, France},
series = {MODELS '16}
}

@inproceedings{10.1145/3277868.3277880,
author = {Klugman, Noah and Dutta, Prabal},
title = {Set and Forget Sensing with Applets on IFTTT},
year = {2018},
isbn = {9781450360494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277868.3277880},
doi = {10.1145/3277868.3277880},
abstract = {Rich data sets can be collected trivially by bootstrapping off mobile phones and cloud
services. We describe an end-to-end system built with IFTTT that requires no code
to collect arrival and departure times from a geographic area on the campus of the
University of California, Berkeley. This system was configured and deployed in less
than one half hour, cost nothing to deploy or run, and functioned without interruption
for seven months, taking 463 measurements of a single participant. Along with providing
the data set, which provides some insight into the working life of a graduate student,
we describe each part of the system architecture and discuss how a model of sensing-as-an-applet
enables data streams with de-facto standardized, high reliability, and close-to-no-barrier
of entry.},
booktitle = {Proceedings of the First Workshop on Data Acquisition To Analysis},
pages = {23–24},
numpages = {2},
keywords = {sensing at scale, trigger-action programming, IFTTT},
location = {Shenzhen, China},
series = {DATA '18}
}

@inproceedings{10.1145/3301551.3301582,
author = {Ruangvanich, Supparang and Nilsook, Prachyanun},
title = {Personality Learning Analytics System in Intelligent Virtual Learning Environment},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301582},
doi = {10.1145/3301551.3301582},
abstract = {In this paper, the researchers propose a conceptual for system architecture of learning
analytics process in the intelligent learning environment. Within this concept, today's
competitive business environment need for businesses in order to implement the monitor
and analyze the user-generated data on their own and their competitors. The achievement
of competitive advantage is often necessary to listen to and understand what customers
are saying about competitors' products and services. Not only personality analytics
but also the conceptual description can capture an intelligent learning environment,
and it is the analytic tools that are used to improve learning and education. The
researchers also discuss how learning analytics is developed in different fields.
It closely tied to, a series of other fields of study including business intelligence,
web analytics, academic analytics, educational data mining, and action analytics.
The researchers believe that conceptual of personality analytics in the intelligent
learning environment can play an essential role in managing and analyzing personality
and contribute to the concept of personality analytics in the intelligent learning
environment. The results of this research could be summarized as follows: learning
analytics process should be used as measuring and collecting data about learners and
learning with the aim of improving teaching and learning practice through analysis
of the data. By achieving this process, it should collect data to report or analyze
the happening about the learner. Then, instructors monitor learning what is happening
now, while as learning analytics should get what is going to happen in the future
for learners. Finally, instructors take action to feedback learners.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {245–250},
numpages = {6},
keywords = {Virtual Learning Environment, Learning Analytics, Intelligent Environment, Personal Analytics, System Architecture},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@inproceedings{10.1145/2645884.2645890,
author = {Irish, Andrew T. and Iland, Daniel and Isaacs, Jason T. and Hespanha, Jo\~{a}o P. and Belding, Elizabeth M. and Madhow, Upamanyu},
title = {Using Crowdsourced Satellite SNR Measurements for 3D Mapping and Real-Time GNSS Positioning Improvement},
year = {2014},
isbn = {9781450330732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645884.2645890},
doi = {10.1145/2645884.2645890},
abstract = {Geopositioning using Global Navigation Satellite Systems (GNSS), such as the Global
Positioning System (GPS), is inaccurate in urban environments due to frequent non-line-of-sight
(NLOS) signal reception. This poses a major problem for mobile services that benefit
from accurate urban localization, such as navigation, hyperlocal advertising, and
geofencing applications. However, urban NLOS signal reception can be exploited in
two ways. First, one can use satellite signal-to-noise ratio (SNR) measurements crowdsourced
from mobile devices to create 3D environment maps. This is possible because, for example,
the SNR of signals obstructed by buildings is lower on average than that of line-of-sight
(LOS) signals. Second, in a sort of reverse process called Shadow Matching, SNR readings
from a particular device at an instant in time can be compared to 3D maps to provide
real-time localization improvement. In this paper we give a brief overview of how
such a system works and describe a scalable, low-cost, software-only architecture
that implements it.},
booktitle = {Proceedings of the 6th Annual Workshop on Wireless of the Students, by the Students, for the Students},
pages = {5–8},
numpages = {4},
keywords = {localization improvement, shadow matching, 3d mapping, crowdsourcing, gnss, gps},
location = {Maui, Hawaii, USA},
series = {S3 '14}
}

@inproceedings{10.1145/3229556.3229563,
author = {Kastanakis, Savvas and Sermpezis, Pavlos and Kotronis, Vasileios and Dimitropoulos, Xenofontas},
title = {CABaRet: Leveraging Recommendation Systems for Mobile Edge Caching},
year = {2018},
isbn = {9781450359061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229556.3229563},
doi = {10.1145/3229556.3229563},
abstract = {Joint caching and recommendation has been recently proposed for increasing the efficiency
of mobile edge caching. While previous works assume collaboration between mobile network
operators and content providers (who control the recommendation systems), this might
be challenging in today's economic ecosystem, with existing protocols and architectures.
In this paper, we propose an approach that enables cache-aware recommendations without
requiring a network and content provider collaboration. We leverage information provided
publicly by the recommendation system, and build a system that provides cache-friendly
and high-quality recommendations. We apply our approach to the YouTube service, and
conduct measurements on YouTube video recommendations and experiments with video requests,
to evaluate the potential gains in the cache hit ratio. Finally, we analytically study
the problem of caching optimization under our approach. Our results show that significant
caching gains can be achieved in practice; 8 to 10 times increase in the cache hit
ratio from cache-aware recommendations, and an extra 2 times increase from caching
optimization.},
booktitle = {Proceedings of the 2018 Workshop on Mobile Edge Communications},
pages = {19–24},
numpages = {6},
keywords = {Recommendation Systems, Joint Caching and Recommendation, Mobile Edge Networks},
location = {Budapest, Hungary},
series = {MECOMM'18}
}

@inproceedings{10.1145/3368235.3368838,
author = {Harsh, Piyush and Ribera Laszkowski, Juan Francisco and Edmonds, Andy and Quang Thanh, Tran and Pauls, Michael and Vlaskovski, Radoslav and Avila-Garc\'{\i}a, Orlando and Pages, Enric and Gort\'{a}zar Bellas, Francisco and Gallego Carrillo, Micael},
title = {Cloud Enablers For Testing Large-Scale Distributed Applications},
year = {2019},
isbn = {9781450370448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368235.3368838},
doi = {10.1145/3368235.3368838},
abstract = {Testing large-scale distributed systems (also known as testing in the large) is a
challenge that spreads across different technical domains and areas of expertise.
Current methods and tools provide some minimal guarantees in relation to the correctness
of their functional properties and have serious limitations when evaluating their
extra-functional properties in realistic conditions, such as scalability, availability
and performance efficiency. Cloud Testing and more specifically "testing in the cloud''
has arisen to tackle those challenges. In this new paradigm, cloud-based environment
and infrastructure are used to run realistic end-to-end and/or system-level tests,
collect test data and analyse them. In this paper we present a set of cloud-native
services to take from the tester the responsibility of managing the resources and
complementary services required to simulate realistic operational conditions and production
environments. Specifically, they provide cloud testing capabilities such as logs and
measurements collection from both testing jobs and system under test; test data analytics
and visualization; provisioning and operation of additional services and processes
to replicate realistic production ecosystems; support to scalability and diversity
of underlying testing infrastructure; and replication of the operational conditions
of the software under test through its instrumentation. We present the architecture
of the cloud testing solution and the detailed design of each of the services; we
also evaluate their relative contribution to satisfy different needs in the context
of test execution.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
pages = {35–42},
numpages = {8},
keywords = {reliability, cloud testing, continuous testing, testing, continuous integration, large-scale distributed systems, scalability},
location = {Auckland, New Zealand},
series = {UCC '19 Companion}
}

@inproceedings{10.5555/3395101.3395117,
author = {Patan\'{e}, Giancarlo M. M. and Valastro, Gianluca C. and Sambo, Yusuf A. and Ozturk, Metin and Hussain, Sajjad and Imran, Muhammad A. and Panno, Daniela},
title = {Flexible SDN/NFV-Based SON Testbed for 5G Mobile Networks},
year = {2019},
isbn = {9781728129235},
publisher = {IEEE Press},
abstract = {In the next few years, a considerable innovation concerning the design of the future
5G mobile networks will be a concrete step towards enabling effective high throughput
and low latency services. Software Defined Networking (SDN), Network Function Virtualization
(NFV) and Self Organizing Network (SON) are considered the enabling technologies to
achieve these goals. In this paper, assuming a Control-Data Separation Architecture
(CDSA), we propose a flexible SDN/NFV-based SON testbed, for future 5G mobile networks.
The main contribution of our work is to cover the need for a CDSA based testbed, enabling
the investigation of the NG-SON capabilities for practical implementations. We implement
two different testbed setups, a real one and a virtualized one, both based on the
FlexRAN and OpenAirInterface software tools. First, we implement a specific case study,
i.e., the RAN entities activation/deactivation procedures. Next, we carry out time
measurements, concerning the aforementioned procedures, in order to prove proper testbed
functioning. Finally, we validate the C-SON and D-SON capabilities of our testbed,
considering the features of the results.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Symposium on Distributed Simulation and Real Time Applications},
pages = {79–86},
numpages = {8},
keywords = {OpenAirInterface, NFV, Cloud-RAN, FlexRAN, 5G, SDN},
location = {Cosenza, Italy},
series = {DS-RT '19}
}

@proceedings{10.1145/2619239,
title = {SIGCOMM '14: Proceedings of the 2014 ACM Conference on SIGCOMM},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SIGCOMM 2014!This year's conference continues the SIGCOMM tradition
of being the premier forum for the presentation of research on networking and communications.
The technical program this year features a set of outstanding papers that cover a
wide variety of areas including network architecture, software defined networks, data
center networks, wireless networks, network services, congestion management, security,
privacy, measurement and analysis.This year's call for papers attracted 242 submissions
from all over the world. The 54 member Technical Program Committee along with a selected
group of external experts carefully considered all of the submissions over two rounds
of reviewing including an author feedback period - with a total of 968 detailed reviews
completed. The TPC meeting to select the final program was held at ICSI, Berkeley,
in late April 2014. At the conclusion of the meeting, the committee had assembled
a wonderful program composed of 45 papers, to be presented over three days at the
conference. The quality of submissions was extremely high as reflected in the final
technical program.},
location = {Chicago, Illinois, USA}
}

@article{10.1145/3428151,
author = {Bibi, Iram and Akhunzada, Adnan and Malik, Jahanzaib and Khan, Muhammad Khurram and Dawood, Muhammad},
title = {Secure Distributed Mobile Volunteer Computing with Android},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3428151},
doi = {10.1145/3428151},
abstract = {Volunteer Computing provision of seamless connectivity that enables convenient and
rapid deployment of greener and cheaper computing infrastructure is extremely promising
to complement next-generation distributed computing systems. Undoubtedly, without
tactile Internet and secure VC ecosystems, harnessing its full potentials and making
it an alternative viable and reliable computing infrastructure is next to impossible.
Android-enabled smart devices, applications, and services are inevitable for Volunteer
computing. Contrarily, the progressive developments of sophisticated Android malware
may reduce its exponential growth. Besides, Android malwares are considered the most
potential and persistent cyber threat to mobile VC systems. To secure Android-based
mobile volunteer computing, the authors proposed MulDroid, an efficient and self-learning
autonomous hybrid (Long-Short-Term Memory, Convolutional Neural Network, Deep Neural
Network) multi-vector Android malware threat detection framework. The proposed mechanism
is highly scalable with well-coordinated infrastructure and self-optimizing capabilities
to proficiently tackle fast-growing dynamic variants of sophisticated malware threats
and attacks with 99.01% detection accuracy. For a comprehensive evaluation, the authors
employed current state-of-the-art malware datasets (Android Malware Dataset, Androzoo)
with standard performance evaluation metrics. Moreover, MulDroid is compared with
our constructed contemporary hybrid DL-driven architectures and benchmark algorithms.
Our proposed mechanism outperforms in terms of detection accuracy with a trivial tradeoff
speed efficiency. Additionally, a 10-fold cross-validation is performed to explicitly
show unbiased results.},
journal = {ACM Trans. Internet Technol.},
month = sep,
articleno = {2},
numpages = {21},
keywords = {deep learning (DL), Volunteer computing (VC), android malware, tactile internet}
}

@inproceedings{10.1145/3143434.3143443,
author = {Bogner, Justus and Wagner, Stefan and Zimmermann, Alfred},
title = {Automatically Measuring the Maintainability of Service- and Microservice-Based Systems: A Literature Review},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143443},
doi = {10.1145/3143434.3143443},
abstract = {In a time of digital transformation, the ability to quickly and efficiently adapt
software systems to changed business requirements becomes more important than ever.
Measuring the maintainability of software is therefore crucial for the long-term management
of such products. With Service-based Systems (SBSs) being a very important form of
enterprise software, we present a holistic overview of such metrics specifically designed
for this type of system, since traditional metrics - e.g. object-oriented ones - are
not fully applicable in this case. The selected metric candidates from the literature
review were mapped to 4 dominant design properties: size, complexity, coupling, and
cohesion. Microservice-based Systems (μSBSs) emerge as an agile and fine-grained variant
of SBSs. While the majority of identified metrics are also applicable to this specialization
(with some limitations), the large number of services in combination with technological
heterogeneity and decentralization of control significantly impacts automatic metric
collection in such a system. Our research therefore suggest that specialized tool
support is required to guarantee the practical applicability of the presented metrics
to μSBSs.},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {107–115},
numpages = {9},
keywords = {maintainability, metrics, service-based systems, SOA, microservices},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@article{10.1007/s00779-012-0618-y,
author = {Belsis, Petros and Pantziou, Grammati},
title = {A K-Anonymity Privacy-Preserving Approach in Wireless Medical Monitoring Environments},
year = {2014},
issue_date = {January   2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-012-0618-y},
doi = {10.1007/s00779-012-0618-y},
abstract = {With the proliferation of wireless sensor networks and mobile technologies in general,
it is possible to provide improved medical services and also to reduce costs as well
as to manage the shortage of specialized personnel. Monitoring a person's health condition
using sensors provides a lot of benefits but also exposes personal sensitive information
to a number of privacy threats. By recording user-related data, it is often feasible
for a malicious or negligent data provider to expose these data to an unauthorized
user. One solution is to protect the patient's privacy by making difficult a linkage
between specific measurements with a patient's identity. In this paper we present
a privacy-preserving architecture which builds upon the concept of k-anonymity; we
present a clustering-based anonymity scheme for effective network management and data
aggregation, which also protects user's privacy by making an entity indistinguishable
from other k similar entities. The presented algorithm is resource aware, as it minimizes
energy consumption with respect to other more costly, cryptography-based approaches.
The system is evaluated from an energy-consuming and network performance perspective,
under different simulation scenarios.},
journal = {Personal Ubiquitous Comput.},
month = jan,
pages = {61–74},
numpages = {14},
keywords = {Clustering, Remote medical monitoring, Sensors, Anonymity}
}

@article{10.1145/3412821.3412823,
author = {Cerina, L. and Santambrogio, M. D.},
title = {SAGe: A Configurable Code Generator for Efficient Symbolic Analysis of Time-Series},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
url = {https://doi.org/10.1145/3412821.3412823},
doi = {10.1145/3412821.3412823},
abstract = {Some of the most recent applications and services revolve around the analysis of time-series,
which generally exhibits chaotic characteristics. This behavior brought back the necessity
to simplify their representation to discover meaningful patterns and extract information
efficiently. Furthermore, recent trends show how computation is moving back from the
Cloud to the Edge of network, meaning that algorithms should be compatible with low-power
embedded devices. A family of methods called Symbolic Analysis (SA) tries to solve
this issue, reducing the dimensionality of the original data in a set of symbolic
words and providing distance metrics for the obtained symbols. However, SA is usually
implemented using application-specific tools, which are not easily adaptable, or mathematical
environments (e.g. R, Julia) that do not ensure portability, or that require additional
work to maximize computing performance. We propose here SAGe: a code generation tool
that helps the user to prototype efficient and portable code, starting from a high-level
representation of SA requirements. Other than exploiting similarities between SA pipelines,
SAGe employs general code templates to build and deploy the code on different architectures,
such as embedded devices, microcontrollers, and FPGAs. Preliminary results show a
speedup up to 223x against Python implementations running on an x86 desktop machine
and a notable increase in computational efficiency on a reconfigurable device.},
journal = {SIGBED Rev.},
month = jul,
pages = {12–17},
numpages = {6}
}

@inproceedings{10.1145/3090354.3090463,
author = {Rafii, Fadoua and Hassani, Badr Dine Rossi and Kbir, M'hamed A\"{\i}t},
title = {New Approach for Microarray Data Decision Making with Respect to Multiple Sources},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090463},
doi = {10.1145/3090354.3090463},
abstract = {Microarray technology is an innovative technology, which has brought changes to the
biological fields. It is considered as an interesting advent for worthwhile researches.
It has permitted simultaneous measurements of the hundreds of activities of genes.
However, most of users and specifically researchers and biologists find difficulties
while extracting and interpreting this kind of data, also the results of Microarray
experiments are stored in multiple and different databases. The present paper focuses
on providing a global architecture for making decisions on Microarray data, by taking
advantages from the semantic web technologies and the data mining techniques. The
major goal consists on getting decisions about a given disease from many experiment
data distributed on many sources over the net. The input dataset, real elements array
form, is retrieved from the integrated experiments designed for cancer studies. This
work is interested to two huge Microarray databases: GEO and ArrayExpress. The integration
was based on semantic web technologies used to integrate data from several Web sites
and Microarray data sources. This can be done by a user to combine several experiments
that treat the same disease or phenomenon in order to have more significant results.
Also a user can upload a specific dataset, via Web services provided by a laboratory,
that can be combined with other data, containing the same genes and treating the same
disease, and receive results of data mining techniques proposed by this laboratory.
We suppose that each laboratory has its own Web services that can receive data which
respects a predefined format.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {106},
numpages = {5},
keywords = {Data mining, Microarray, Semantic web},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3199902.3199904,
author = {\v{S}ljivo, Amina and Kerkhove, Dwight and Moerman, Ingrid and De Poorter, Eli and Hoebeke, Jeroen},
title = {Interactive Web Visualizer for IEEE 802.11ah Ns-3 Module},
year = {2018},
isbn = {9781450364133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199902.3199904},
doi = {10.1145/3199902.3199904},
abstract = {The main purpose of running ns-3 simulations is to generate relevant data sets for
further study. There are two strategies to generate output from ns-3, either using
generic predefined bulk output mechanisms or using the ns-3's Tracing system. Both
require parsing the raw output data to extract and process the data of interest to
obtain meaningful information. However, parsing such output is in most cases time
consuming and prone to mistakes. Post-processing is even harder when a large number
of simulations needs to be analyzed and even the tracing system cannot simplify this
task. Moreover, results obtained this way are only available once the simulation is
finished.Therefore, we developed a user-friendly interactive visualization and post-processing
tool for IEEE 802.11ah called ahVisualizer. Beside the topology and MAC configuration,
ahVisualizer also plots our traces for each node over time during the simulation,
as well as averages and standard deviations for each traced parameter. It can compare
all the measured values across different simulations. Users can easily download figures
and data in various formats. Moreover, it includes a post-processing tool which plots
desired series, with desired fixed parameters, from a large set of simulations. This
paper presents the ahVisualizer, its services and its architecture and shows how this
tool enables much faster and easier data analysis and monitoring of ns-3 simulations
with 802.11ah.},
booktitle = {Proceedings of the 10th Workshop on Ns-3},
pages = {23–29},
numpages = {7},
keywords = {visualization, Wi-Fi HaLow, ns-3, post-processing, analysis, distributed simulations, IEEE 802.11ah},
location = {Surathkal, India},
series = {WNS3 '18}
}

@article{10.1145/3059149,
author = {Ying, Xuhang and Zhang, Jincheng and Yan, Lichao and Chen, Yu and Zhang, Guanglin and Chen, Minghua and Chandra, Ranveer},
title = {Exploring Indoor White Spaces in Metropolises},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3059149},
doi = {10.1145/3059149},
abstract = {It is a promising vision to exploit white spaces, that is, vacant VHF and UHF TV channels,
to meet the rapidly growing demand for wireless data services in both outdoor and
indoor scenarios. While most prior works have focused on outdoor white space, the
indoor story is largely open for investigation. Motivated by this observation and
discovering that 70% of the spectrum demand comes from indoor environment, we carry
out a comprehensive study to explore indoor white spaces. We first conduct a large-scale
measurement study and compare outdoor and indoor TV spectrum occupancy at 30+ diverse
locations in a typical metropolis—Hong Kong. Our results show that abundant white
spaces are available in different areas in Hong Kong, which account for more than
50% and 70% of the entire TV spectrum in outdoor and indoor scenarios, respectively.
Although there are substantially more white spaces indoors than outdoors, there have
been very few solutions for identifying indoor white space. To fill in this gap, we
develop the first data-driven, low-cost indoor white space identification system for
White-space Indoor Spectrum EnhanceR (WISER), to allow secondary users to identify
white spaces for communication without sensing the spectrum themselves. We design
the architecture and algorithms to address the inherent challenges. We build a WISER
prototype and carry out real-world experiments to evaluate its performance. Our results
show that WISER can identify 30%--40% more indoor white spaces with negligible false
alarms, as compared to alternative baseline approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {9},
numpages = {25},
keywords = {TV white spaces, sensor placement, clustering algorithms}
}

@article{10.1145/3448738,
author = {Shi, Cong and Liu, Jian and Liu, Hongbo and Chen, Yingying},
title = {WiFi-Enabled User Authentication through Deep Learning in Daily Activities},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1914},
url = {https://doi.org/10.1145/3448738},
doi = {10.1145/3448738},
abstract = {User authentication is a critical process in both corporate and home environments
due to the ever-growing security and privacy concerns. With the advancement of smart
cities and home environments, the concept of user authentication is evolved with a
broader implication by not only preventing unauthorized users from accessing confidential
information but also providing the opportunities for customized services corresponding
to a specific user. Traditional approaches of user authentication either require specialized
device installation or inconvenient wearable sensor attachment. This article supports
the extended concept of user authentication with a device-free approach by leveraging
the prevalent WiFi signals made available by IoT devices, such as smart refrigerator,
smart TV, and smart thermostat, and so on. The proposed system utilizes the WiFi signals
to capture unique human physiological and behavioral characteristics inherited from
their daily activities, including both walking and stationary ones. Particularly,
we extract representative features from channel state information (CSI) measurements
of WiFi signals, and develop a deep-learning-based user authentication scheme to accurately
identify each individual user. To mitigate the signal distortion caused by surrounding
people’s movements, our deep learning model exploits a CNN-based architecture that
constructively combines features from multiple receiving antennas and derives more
reliable feature abstractions. Furthermore, a transfer-learning-based mechanism is
developed to reduce the training cost for new users and environments. Extensive experiments
in various indoor environments are conducted to demonstrate the effectiveness of the
proposed authentication system. In particular, our system can achieve over 94% authentication
accuracy with 11 subjects through different activities.},
journal = {ACM Trans. Internet Things},
month = may,
articleno = {13},
numpages = {25},
keywords = {User authentication, IoT, WiFi signals}
}

@inproceedings{10.1145/3132340.3132358,
author = {Olariu, Stephan and Florin, Ryan},
title = {Vehicular Clouds Research: What is Missing?},
year = {2017},
isbn = {9781450351645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132340.3132358},
doi = {10.1145/3132340.3132358},
abstract = {Vehicular Clouds (VCs) have become an active research topic. However, even a cursory
look reveals that the VC literature of recent years is full of papers discussing fanciful
VC architectures and services that often seem too good to be true. And many of them
are. It seems to us that promoting VC models without any regard to their practical
feasibility is apt to discredit the VC concept altogether. Part of the problem stems
from the fact that some authors do not seem to be concerned with the obvious fact
that moving vehicles' residency times in the VC may, indeed, be very short and, therefore,
so is their contribution to the amount of useful work performed. Should a vehicle
running a user job leave the VC prematurely, the amount of work performed by that
vehicle may be lost, unless special precautions are taken. Such precautionary measures
involve either some flavor of checkpointing or some form of redundant job assignment.
Both approaches have consequences in terms of overhead and impact job completion time.
The success of conventional cloud computing (CC) is attributable to the ability to
provide quantifiable functional characteristics such as scalability, reliability and
availability. By the same token, if the VCs are to see a widespread adoption, the
same quantitative aspects have to be addressed here, too. Feasibility issues in terms
of sufficient compute power, communication bandwidth, reliability, availability, and
job duration time are all fundamental quantitative aspects of VCs that need to be
studied and understood before one can claim with any degree of certainty that they
can support the workload for which they are intended. The first contribution of this
paper is to make a case for the stringent need to address quantitatively the performance
characteristics of VC architectures and proposed services. Our second contribution
is to point out directions and challenges facing the VC community.},
booktitle = {Proceedings of the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {77–84},
numpages = {8},
keywords = {availability, acm proceedings, vehicular clouds, reliability, redundancy, cloud computing},
location = {Miami, Florida, USA},
series = {DIVANet '17}
}

@inproceedings{10.1145/3394885.3431616,
author = {Chakaravarthy, Ravikumar V. and Kwon, Hyun and Jiang, Hua},
title = {Vision Control Unit in Fully Self Driving Vehicles Using Xilinx MPSoC and Opensource Stack},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431616},
doi = {10.1145/3394885.3431616},
abstract = {Fully self-driving (FSD) vehicles are becoming increasing popular over the last few
years and companies are investing significantly into its research and development.
In the recent years, FSD technology innovators like Tesla, Google etc. have been working
on proprietary autonomous driving stacks and have been able to successfully bring
the vehicle to the roads. On the other end, organizations like Autoware Foundation
and Baidu are fueling the growth of self-driving mobility using open source stacks.
These organizations firmly believe in enabling autonomous driving technology for everyone
and support developing software stacks through the open source community that is SoC
vendor agnostic. In this proposed solution we describe a vision control unit for a
fully self-driving vehicle developed on Xilinx MPSoC platform using open source software
components.The vision control unit of an FSD vehicle is responsible for camera video
capture, image processing and rendering, AI algorithm processing, data and meta-data
transfer to next stage of the FSD pipeline. In this proposed solution we have used
many open source stacks and frameworks for video and AI processing. The processing
of the video pipeline and algorithms take full advantage of the pipelining and parallelism
using all the heterogenous cores of the Xilinx MPSoC. In addition, we have developed
an extensible, scalable, adaptable and configurable AI backend framework, XTA, for
acceleration purposes that is derived from a popular, open source AI backend framework,
TVM-VTA. XTA uses all the MPSoC cores for its computation in a parallel and pipelined
fashion. XTA also adapts to the compute and memory parameters of the system and can
scale to achieve optimal performance for any given AI problem. The FSD system design
is based on a distributed system architecture and uses open source components like
Autoware for autonomous driving algorithms, ROS and Distributed Data Services as a
messaging middleware between the functional nodes and a real-time kernel to coordinate
the actions. The details of image capture, rendering and AI processing of the vision
perception pipeline will be presented along with the performance measurements of the
vision pipeline.In this proposed solution we will demonstrate some of the key use
cases of vision perception unit like surround vision and object detection. In addition,
we will also show the capability of Xilinx MPSoC technology to handle multiple channels
of real time camera and the integration with the Lidar/Radar point cloud data to feed
into the decision-making unit of the overall system. The system is also designed with
the capability to update the vision control unit through Over the Air Update (OTA).
It is also envisioned that the core AI engine will require regular updates with the
latest training values; hence a built-in platform level mechanism supporting such
capability is essential for real world deployment.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {311–317},
numpages = {7},
keywords = {MPSoC, XTA, Vision Pipeline, Autoware, AI, Heterogenous Processing, FSD, ROS},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@inproceedings{10.1109/ICCPS.2014.6843740,
author = {Zhang, Jiaxing and Qiu, Hanjiao and Shamsabadi, Salar Shahini and Birken, Ralf and Schirner, Gunar},
title = {WiP Abstract: System-Level Integration of Mobile Multi-Modal Multi-Sensor Systems},
year = {2014},
isbn = {9781479949304},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCPS.2014.6843740},
doi = {10.1109/ICCPS.2014.6843740},
abstract = {Heterogeneous roaming sensor systems have gained significant importance in many domains
of civil infrastructure performance inspection as they accelerate data collection
and analysis. However, designing such systems is challenging due to the immense complexity
in the heterogeneity and processing demands of the involved sensors. Unifying frameworks
are needed to simplify development, deployment and operation of roaming sensors and
computing units.To address the sensing needs, we propose SIROM3, a Scalable Intelligent
Roaming Multi-Modal Multi-Sensor framework. SIROM3 incorporates a CPS approach for
infrastructure performance monitoring to address the following challenges: 1. Scalability
and expandability. It offers a scalable and expandable solution enabling diversity
in sensing and the growth in processing platforms from sensors to control centers.
2. Fusion foundations. SIROM3 enables fusion of data collected by logically and geo-spatially
distributed sensors. 3. Big data handling. Automatic collection, categorization, storage
and manipulation of heterogeneous large volume of data streams. 4. Automation. SIROM3
minimizes human interaction through full automation from data acquisition to visualization
of the fused results.Illustrated in Fig. 1, SIROM3 realizes scalability and expandability
in a system-level design approach encapsulating common functionality across hierarchical
components in a Run-Time Environment (RTE). The RTE deploys a layered design paradigm
defining services in both software and hardware architectures. Equipped with multiple
RTE-enabled Multi-Sensor Aggregators (MSA), an array of Roaming Sensor Systems (RSS)
operate as mobile agents attached to vehicles to provide distributed computing services
regulated by Fleet Control and Management (FCM) center via communication network.
A series of foundational services including the Precise Timing Protocol (PTP), GPS
timing systems, Distance Measurement Instruments (DMI) through middleware services
(CORBA) embedded in the RTE build the fusion foundations for data correlation and
analysis. A Heterogeneous Stream File-system Overlay (HSFO) alleviates the big data
challenge. It facilitates storing, processing, categorizing and fusing large heterogeneous
data stream collected by versatile sensors. A GIS visualization module is integrated
for visual analysis and monitoring.SIROM3 enables coordination and collaboration across
sensors, MSAs and RSSes, which produce high volume of heterogeneous data stored in
HSFO. To fuse the data efficiently, SIROM3 contains an expandable plugin system (part
of RTE) for rapid algorithm prototyping using data streams in the architectural hierarchy
(i.e. from MSAs to FCM) via the HSFO API. This makes an ideal test-bed to develop
new algorithms and methodologies expanding CPS principles to civil infrastructure
performance monitoring. In result, SIROM3 simplifies the development, construction
and operation of roaming multi-modal multi-sensor systems.We demonstrate the efficiency
of SIROM3 by automating the assessment of road surface conditions at the city scale.
We realized an RSS with 6 MSAs and 30 heterogeneous sensors, including radars, microphones,
GPS and cameras, all deployed onto a van sponsored by the VOTERS (Versatile Onboard
Traffic Embedded Roaming Sensors) project. Over 20 terabytes of data have been collected,
aggregated, fused, analyzed and geo-spatially visualized using SIROM3 for studying
the pavement conditions of the city of Brockton, MA covering 300 miles. The expandability
of SIROM3 is shown by adding a millimeter-wave radar needing less than 50 lines of
C++ code for system integration. SIROM3 offers a unified solution for comprehensive
roadway assessment and evaluation. The integrated management of big data (from collection
to automated processing) is an ideal research platform for automated assessment of
civil infrastructure performance.},
booktitle = {ICCPS '14: ACM/IEEE 5th International Conference on Cyber-Physical Systems (with CPS Week 2014)},
pages = {227},
numpages = {1},
location = {Berlin, Germany},
series = {ICCPS '14}
}

@article{10.1145/2935634.2935640,
author = {Orwat, Carsten and Bless, Roland},
title = {Values and Networks: Steps Toward Exploring Their Relationships},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2935634.2935640},
doi = {10.1145/2935634.2935640},
abstract = {Many technical systems of the Information and Communication Technology (ICT) sector
enable, structure and/or constrain social interactions. Thereby, they influence or
implement certain values, including human rights, and affect or raise conflicts among
values. The ongoing developments toward an "Internet of everything'' is likely to
lead to further value conflicts. This trend illustrates that a better understanding
of the relationships between social values and networks is urgently needed because
it is largely unknown what values lie behind protocols, design principles, or technical
and organizational options of the Internet. This paper focuses on the complex steps
of realizing human rights in Internet architectures and protocols as well as in Internet-based
products and services. Besides direct implementation of values in Internet protocols,
there are several other options that can indirectly contribute to realizing human
rights via political processes and market choices. Eventually, a better understanding
of what values can be realized by networks in general, what technical measures may
affect certain values, and where complementary institutional developments are needed
may lead toward a methodology for considering technical and institutional systems
together.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = may,
pages = {25–31},
numpages = {7},
keywords = {network design, rules, human rights, values, communication protocols, institutions, governance}
}

@inproceedings{10.1145/3447545.3451195,
author = {Calzarossa, Maria Carla and Massari, Luisa and Tessera, Daniele},
title = {Performance Monitoring Guidelines},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451195},
doi = {10.1145/3447545.3451195},
abstract = {Monitoring, that is, the process of collecting measurements on infrastructures and
services, is an important subject of performance engineering. Although monitoring
is not a new education topic, nowadays its relevance is rapidly increasing and its
application is particularly demanding due to the complex distributed architectures
of new and emerging technologies. As a consequence, monitoring has become a "must
have" skill for students majoring in computer science and in computing-related fields.
In this paper, we present a set of guidelines and recommendations to plan, design
and setup sound monitoring projects. Moreover, we investigate and discuss the main
challenges to be faced to build confidence in the entire monitoring process and ensure
measurement quality. Finally, we describe practical applications of these concepts
in teaching activities.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {109–114},
numpages = {6},
keywords = {performance monitoring, active measurements, measurement platforms, passive measurements, performance engineering, measurement quality},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1145/3390605,
author = {Ismail, Leila and Materwala, Huned},
title = {Computing Server Power Modeling in a Data Center: Survey, Taxonomy, and Performance Evaluation},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3390605},
doi = {10.1145/3390605},
abstract = {Data centers are large-scale, energy-hungry infrastructure serving the increasing
computational demands as the world is becoming more connected in smart cities. The
emergence of advanced technologies such as cloud-based services, internet of things
(IoT), and big data analytics has augmented the growth of global data centers, leading
to high energy consumption. This upsurge in energy consumption of the data centers
not only incurs the issue of surging high cost (operational and maintenance) but also
has an adverse effect on the environment. Dynamic power management in a data center
environment requires the cognizance of the correlation between the system and hardware-level
performance counters and the power consumption. Power consumption modeling exhibits
this correlation and is crucial in designing energy-efficient optimization strategies
based on resource utilization. Several works in power modeling are proposed and used
in the literature. However, these power models have been evaluated using different
benchmarking applications, power-measurement techniques, and error-calculation formulas
on different machines. In this work, we present a taxonomy and evaluation of 24 software-based
power models using a unified environment, benchmarking applications, power-measurement
techniques, and error formulas, with the aim of achieving an objective comparison.
We use different server architectures to assess the impact of heterogeneity on the
models’ comparison. The performance analysis of these models is elaborated in the
article.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {58},
numpages = {34},
keywords = {Data center, machine learning, server power consumption modeling, green computing, energy-efficiency, resource utilization}
}

@inproceedings{10.1145/3267955.3267972,
author = {Sardara, Mauro and Muscariello, Luca and Compagno, Alberto},
title = {A Transport Layer and Socket API for (h)ICN: Design, Implementation and Performance Analysis},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3267972},
doi = {10.1145/3267955.3267972},
abstract = {In this paper we present the design of a transport layer and socket API that can be
used in several ICN architectures such as NDN, CCN and hICN. The current design makes
it possible to expose an API that is simple to insert in current applications and
easy to use to develop novel ones. The proliferation of connected applications for
very different use cases and services with wide spectrum of requirements suggests
that several transport services will coexist in the Internet. This is just about to
happen with QUIC, MPTCP, LEDBAT as the most notable ones but is expected to grow and
diversify with the advent of applications for 5G, IoT, MEC with heterogeneous connectivity.
The advantages of ICN have to be measurable from the application, end-services and
in the network, with relevant key performance indicators. We have implemented an high
speed transport stack with most of the designed features that we present in this paper
with extensive experiments and benchmarks to show the scalability of the current systems
in different use cases.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {137–147},
numpages = {11},
keywords = {socket API, ICN, transport services},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@inproceedings{10.1145/3079856.3080210,
author = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
title = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080210},
doi = {10.1145/3079856.3080210},
abstract = {With increasing deployment of virtual machines for cloud services and server applications,
memory address translation overheads in virtualized environments have received great
attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss
necessitates up to 24 memory references for one guest to host translation. While dedicated
page walk caches and such recent enhancements eliminate many of these memory references,
our measurements on the Intel Skylake processors indicate that many programs in virtualized
mode of execution still spend hundreds of cycles for translations that do not hit
in the TLBs.This paper presents an innovative scheme to reduce the cost of address
translations by using a very large Translation Lookaside Buffer that is part of memory,
the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses
required in commonly used 2D walks with radix-4 type of page tables. Even if many
of the 24 accesses may hit in the page walk caches, the aggregated cost of the many
hits plus the overhead of occasional misses from page walk caches still exceeds the
cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space,
TLB entries (as opposed to multiple page table entries) can be cached in large L2
and L3 data caches, yielding significant benefits. Through detailed evaluation running
SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves
performance by approximately 10% on average. The improvement is more than 16% for
5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate
nearly all TLB misses in 8-core systems.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {469–480},
numpages = {12},
keywords = {Die-Stacked DRAM, Virtualization, Very Large TLB, Address Translation},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1145/3140659.3080210,
author = {Ryoo, Jee Ho and Gulur, Nagendra and Song, Shuang and John, Lizy K.},
title = {Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080210},
doi = {10.1145/3140659.3080210},
abstract = {With increasing deployment of virtual machines for cloud services and server applications,
memory address translation overheads in virtualized environments have received great
attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss
necessitates up to 24 memory references for one guest to host translation. While dedicated
page walk caches and such recent enhancements eliminate many of these memory references,
our measurements on the Intel Skylake processors indicate that many programs in virtualized
mode of execution still spend hundreds of cycles for translations that do not hit
in the TLBs.This paper presents an innovative scheme to reduce the cost of address
translations by using a very large Translation Lookaside Buffer that is part of memory,
the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses
required in commonly used 2D walks with radix-4 type of page tables. Even if many
of the 24 accesses may hit in the page walk caches, the aggregated cost of the many
hits plus the overhead of occasional misses from page walk caches still exceeds the
cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space,
TLB entries (as opposed to multiple page table entries) can be cached in large L2
and L3 data caches, yielding significant benefits. Through detailed evaluation running
SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves
performance by approximately 10% on average. The improvement is more than 16% for
5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate
nearly all TLB misses in 8-core systems.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {469–480},
numpages = {12},
keywords = {Die-Stacked DRAM, Address Translation, Very Large TLB, Virtualization}
}

@inproceedings{10.1145/2876019.2876023,
author = {Pan, Xiang and Yegneswaran, Vinod and Chen, Yan and Porras, Phillip and Shin, Seungwon},
title = {HogMap: Using SDNs to Incentivize Collaborative Security Monitoring},
year = {2016},
isbn = {9781450340786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876019.2876023},
doi = {10.1145/2876019.2876023},
abstract = {Cyber Threat Intelligence (CTI) sharing facilitates a comprehensive understanding
of adversary activity and enables enterprise networks to prioritize their cyber defense
technologies. To that end, we introduce HogMap, a novel software-defined infrastructure
that simplifies and incentivizes collaborative measurement and monitoring of cyber-threat
activity. HogMap proposes to transform the cyber-threat monitoring landscape by integrating
several novel SDN-enabled capabilities: (i) intelligent in-place filtering of malicious
traffic, (ii) dynamic migration of interesting and extraordinary traffic and (iii)
a software-defined marketplace where various parties can opportunistically subscribe
to and publish cyber-threat intelligence services in a flexible manner.We present
the architectural vision and summarize our preliminary experience in developing and
operating an SDN-based HoneyGrid, which spans three enterprises and implements several
of the enabling capabilities (e.g., traffic filtering, traffic forwarding and connection
migration). We find that SDN technologies greatly simplify the design and deployment
of such globally distributed and elastic HoneyGrids.},
booktitle = {Proceedings of the 2016 ACM International Workshop on Security in Software Defined Networks &amp; Network Function Virtualization},
pages = {7–12},
numpages = {6},
keywords = {cyber threat intelligence, honeygrid, marketplace, sdn, honeynet},
location = {New Orleans, Louisiana, USA},
series = {SDN-NFV Security '16}
}

@inproceedings{10.1145/3204949.3204976,
author = {Mekuria, Rufael and McGrath, Michael J. and Riccobene, Vincenzo and Bayon-Molino, Victor and Tselios, Christos and Thomson, John and Dobrodub, Artem},
title = {Automated Profiling of Virtualized Media Processing Functions Using Telemetry and Machine Learning},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204976},
doi = {10.1145/3204949.3204976},
abstract = {Most media streaming services are composed by different virtualized processing functions
such as encoding, packaging, encryption, content stitching etc. Deployment of these
functions in the cloud is attractive as it enables flexibility in deployment options
and resource allocation for the different functions. Yet, most of the time overprovisioning
of cloud resources is necessary in order to meet demand variability. This can be costly,
especially for large scale deployments. Prior art proposes resource allocation based
on analytical models that minimize the costs of cloud deployments under a quality
of service (QoS) constraint. However, these models do not sufficiently capture the
underlying complexity of services composed of multiple processing functions. Instead,
we introduce a novel methodology based on full-stack telemetry and machine learning
to profile virtualized or cloud native media processing functions individually. The
basis of the approach consists of investigating 4 categories of performance metrics:
throughput, anomaly, latency and entropy (TALE) in offline (stress tests) and online
setups using cloud telemetry. Machine learning is then used to profile the media processing
function in the targeted cloud/NFV environment and to extract the most relevant cloud
level Key Performance Indicators (KPIs) that relate to the final perceived quality
and known client side performance indicators. The results enable more efficient monitoring,
as only KPI related metrics need to be collected, stored and analyzed, reducing the
storage and communication footprints by over 85%. In addition a detailed overview
of the functions behavior was obtained, enabling optimized initial configuration and
deployment, and more fine-grained dynamic online resource allocation reducing overprovisioning
and avoiding function collapse. We further highlight the next steps towards cloud
native carrier grade virtualized processing functions relevant for future network
architectures such as in emerging 5G architectures.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {150–161},
numpages = {12},
keywords = {characterization, performance, cloud computing, telemetry, experimentation, video streaming},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/2701126.2701226,
author = {Rizvi, Syed and Ryoo, Jungwoo and Kissell, John and Aiken, Bill},
title = {A Stakeholder-Oriented Assessment Index for Cloud Security Auditing},
year = {2015},
isbn = {9781450333771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701126.2701226},
doi = {10.1145/2701126.2701226},
abstract = {Cloud computing is an emerging computing model that provides numerous advantages to
organizations (both service providers and customers) in terms of massive scalability,
lower cost, and flexibility, to name a few. Despite these technical and economical
advantages of cloud computing, many potential cloud consumers are still hesitant to
adopt cloud computing due to security and privacy concerns. This paper describes some
of the unique cloud computing security factors and subfactors that play a critical
role in addressing cloud security and privacy concerns. To mitigate these concerns,
we develop a security metric tool to provide information to cloud users about the
security status of a given cloud vendor. The primary objective of the proposed metric
is to produce a security index that describes the security level accomplished by an
evaluated cloud computing vendor. The resultant security index will give confidence
to different cloud stakeholders and is likely to help them in decision making, increase
the predictability of the quality of service, and allow appropriate proactive planning
if needed before migrating to the cloud. To show the practicality of the proposed
metric, we provide two case studies based on the available security information about
two well-known cloud service providers (CSP). The results of these case studies demonstrated
the effectiveness of the security index in determining the overall security level
of a CSP with respect to the security preferences of cloud users.},
booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication},
articleno = {55},
numpages = {7},
keywords = {data privacy, cloud security, security metrics, cloud auditing},
location = {Bali, Indonesia},
series = {IMCOM '15}
}

@inproceedings{10.1145/2980258.2982046,
author = {VasanthaAzhagu, A. Kannaki and Gnanasekar, J. M.},
title = {Cloud Computing Overview, Security Threats and Solutions-A Survey},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2982046},
doi = {10.1145/2980258.2982046},
abstract = {Cloud Computing aims to provide computing everywhere. It delivers computing resources
on demand over internet in terms of anything anywhere anytime concept. It provides
everything as a service to its users like infrastructure platform software hardware
workplace data and security. Cloud computing has made revolutionary transformations
in the government and business. Cloud Computing transforms the databases and application
software to the huge data centers, where the management of the services and data may
not be trustworthy. To verify the correctness, integrity, confidentially and availability
of data in the cloud, in this paper, we focus on various cloud computing security
threats and solution that have been used since security is an important measure for
quality of service.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {109},
numpages = {6},
keywords = {Availability, Cloud Computing, Deployment Security threats, Integrity, Quality of Service (QoS)},
location = {Pondicherry, India},
series = {ICIA-16}
}

@inproceedings{10.1145/2668930.2688043,
author = {Becker, Matthias and Lehrig, Sebastian and Becker, Steffen},
title = {Systematically Deriving Quality Metrics for Cloud Computing Systems},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688043},
doi = {10.1145/2668930.2688043},
abstract = {In cloud computing, software architects develop systems for virtually unlimited resources
that cloud providers account on a pay-per-use basis. Elasticity management systems
provision these resources autonomously to deal with changing workload. Such changing
workloads call for new objective metrics allowing architects to quantify quality properties
like scalability, elasticity, and efficiency, e.g., for requirements/SLO engineering
and software design analysis. In literature, initial metrics for these properties
have been proposed. However, current metrics lack a systematic derivation and assume
knowledge of implementation details like resource handling. Therefore, these metrics
are inapplicable where such knowledge is unavailable.To cope with these lacks, this
short paper derives metrics for scalability, elasticity, and efficiency properties
of cloud computing systems using the goal question metric (GQM) method. Our derivation
uses a running example that outlines characteristics of cloud computing systems. Eventually,
this example allows us to set up a systematic GQM plan and to derive an initial set
of six new metrics. We particularly show that our GQM plan allows to classify existing
metrics.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {169–174},
numpages = {6},
keywords = {metric, elasticity, cloud computing, gqm, efficiency, slo, analysis, scalability},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@article{10.1145/2557833.2557854,
author = {Yadav, Nikita and Khatri, Sujata and Singh, V. B.},
title = {Developing an Intelligent Cloud for Higher Education},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2557854},
doi = {10.1145/2557833.2557854},
abstract = {With rapid development in the IT world, technologies are becoming more dynamic and
advanced. Today, technologies are changing with customer requirements. In the IT world,
research is carried out to make technology better to meet the requirements that change
with time. With the advancement in the IT world, online services have proliferated.
Now a days, cloud computing is the hottest buzzword in the IT world. Cloud computing
is not limited to the E-Governance and business worlds, but is also making a great
impact in the education world. With growing demand for education, technologies and
research, all universities and education institutions have their eyes on cloud computing.
The main pillars of educational institutions are students, faculties, administrations
and libraries. Faculty and students do research and need quality data while students
of a particular field need a subject-oriented knowledge. Manually getting these kinds
of data is time consuming as students depend on literature, books, different kind
of software and hardware. With cloud computing in higher education, cost-effective
measures can be taken to minimize the dependency on books, hardware and software.
In this paper, we discuss how Artifical Intelligence based cloud computing in higher
education will improve quality and ease the process of getting e-resources (software/hardware
platform, storage etc.). This study will help in understanding effective cost-cutting
measures. We also discuss how cloud computing in the library and administration will
brighten the education prospects.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–5},
numpages = {5},
keywords = {higher education, E-administration, E-library, cloud computing, E-learning}
}

@inproceedings{10.1145/2737182.2737185,
author = {Lehrig, Sebastian and Eikerling, Hendrik and Becker, Steffen},
title = {Scalability, Elasticity, and Efficiency in Cloud Computing: A Systematic Literature Review of Definitions and Metrics},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737185},
doi = {10.1145/2737182.2737185},
abstract = {Context: In cloud computing, there is a multitude of definitions and metrics for scalability,
elasticity, and efficiency. However, stakeholders have little guidance for choosing
fitting definitions and metrics for these quality properties, thus leading to potential
misunderstandings. For example, cloud consumers and providers cannot negotiate reliable
and quantitative service level objectives directly understood by each stakeholder.
Objectives: Therefore, we examine existing definitions and metrics for these quality
properties from the viewpoint of cloud consumers, cloud providers, and software architects
with regard to commonly used concepts. Methods: We execute a systematic literature
review (SLR), reproducibly collecting common concepts in definitions and metrics for
scalability, elasticity, and efficiency. As quality selection criteria, we assess
whether existing literature differentiates the three properties, exemplifies metrics,
and considers typical cloud characteristics and cloud roles. Results: Our SLR yields
418 initial results from which we select 20 for in-depth evaluation based on our quality
selection criteria. In our evaluation, we recommend concepts, definitions, and metrics
for each property. Conclusions: Software architects can use our recommendations to
analyze the quality of cloud computing applications. Cloud providers and cloud consumers
can specify service level objectives based on our metric suggestions.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {83–92},
numpages = {10},
keywords = {systematic literature review, cloud computing, cloud, scalability, metrics, elasticity, efficiency, definitions},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@inproceedings{10.1145/2896387.2896403,
author = {Al-Ghuwairi, Abdel-Rahman and Eid, Hazem and Aloran, Mohammad and Salah, Zaher and Baarah, Aladdin Hussein and Al-oqaily, Ahmad A.},
title = {A Mutation-Based Model to Rank Testing as a Service (TaaS) Providers in Cloud Computing},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2896403},
doi = {10.1145/2896387.2896403},
abstract = {With the increase of cloud computing service models, the need to measure and evaluate
them are increased as well. In this paper, we proposed a novel measurement approach
for the purpose of evaluating the quality of Testing as a Service (TaaS), which is
considered as one of the most recent outstanding model within cloud computing environment.
(TaaS) as outstanding model include the provision of multi-sub services, such as enabling
cloud customer to verify his own code through the use of cloud provider resources.
Its goes without questioning that testing over web environment requires high level
of resources, time, and effort. Therefore, it should take high attention toward the
quality of the used testing technique. Where, the quality of testing technique associated
with set of attributes that has the ability to determine testing effectiveness. Thus,
in this paper we propose a measurement approach to evaluate the effectiveness of TaaS,
over cloud computing environment which relies on the use of mutation score. The main
contribution of the proposed model represent in the use of mutation score to evaluate
cloud providers ability to perform TaaS, and rank them according to the percentage
of TaaS effectiveness.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {18},
numpages = {5},
keywords = {Mutation, Effectiveness, Cloud services, Testing as a services, Cloud computing, Measurement},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/3234781.3234787,
author = {Tse, Daniel and Yuen, Hok Hin and He, Qiran and Wang, Chaoya and Yu, Jiheng},
title = {The Security Vulnerabilities of On-Demand and Sharing Economy},
year = {2018},
isbn = {9781450364904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234781.3234787},
doi = {10.1145/3234781.3234787},
abstract = {The cloud computing has been widely in on-demand-and-sharing service economy and has
become a hotspot in recent years especially in the IT industry which really lead to
some changes in human's daily life. However, many users and researchers believed that
the information security is the most significant challenge in cloud computing. Therefore,
this paper aims to discover the threats and vulnerabilities of the cloud storage which
is the most common application originating from the cloud computing. This research
utilized a quantitative approach and all qualified respondents were asked to complete
an online questionnaire. The result shows that (1) Data loss and leakage is the biggest
threat in using cloud storage application (2) Abuse use of cloud computational resources
is the most severe impact in cloud storage application (3) Respondents with different
backgrounds have the different perspectives towards the cloud service (4) The countermeasures
to minimize the security vulnerability are flexibility in choosing the protective
measures, strengthen the infrastructure, improve the password authentication and strengthen
the authorization.},
booktitle = {Proceedings of the 2nd International Conference on E-Commerce, E-Business and E-Government},
pages = {47–53},
numpages = {7},
keywords = {cloud storage application, threats, vulnerabilities, on-demand-and-sharing economy, cloud computing},
location = {Hong Kong, Hong Kong},
series = {ICEEG '18}
}

@inproceedings{10.5555/3233397.3233523,
author = {Kirsal, Yonal and Ever, Yoney Kirsal and Mostarda, Leonardo and Gemikonakli, Orhan},
title = {Analytical Modelling and Performability Analysis for Cloud Computing Using Queuing System},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {In recent years, cloud computing becomes a new computing model emerged from the rapid
development of the internet. Users can reach their resources with high flexibility
using the cloud computing systems all over the world. However, such systems are prone
to failures. In order to obtain realistic quality of service (QoS) measurements, failure
and recovery behaviours of the system should be considered. System's failures and
repairs are associated with availability context in QoS measurements. In this paper,
performance issues are considered with the availability of the system. Markov Reward
Model (MRM) method is used to get QoS measurements. The mean queue length (MQL) results
are calculated using the MRM. The results explicitly show that failures and repairs
affect the system performance significantly.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {643–647},
numpages = {5},
keywords = {quality of service, cloud computing, queuing system, analytical modelling, performability analysis},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@proceedings{10.1145/2755979,
title = {VTDC '15: Proceedings of the 8th International Workshop on Virtualization Technologies in Distributed Computing},
year = {2015},
isbn = {9781450335737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {During the past few years, we have begun to see a convergence of cloud computing and
high performance computing (HPC) infrastructures, technologies, and applications.
In HPC, applications have since long predominantly been parallel batch jobs with execution
times measured in hours or even days, managed by mature batch and scheduling systems
developed and refined over decades. With the introduction of clouds, providing elastic
capacity and new programming models for internet-type applications, also traditional
HPC users have begun to explore new methods to solve their problems. Cloud applications
often come with large numbers of shorter tasks, frequently pipelined and sometimes
combined with long-running service-type application components, not too different
from what has been seen in HPC since long. Building on previous successful and highly
attended VTDC workshops, this 8th edition is a forum to dwell from these synergies
and exchange ideas among researchers in the broad area of virtualization technologies
in distributed computing in order to further the forefronts of both HPC and cloud
computing.The technical program is what defines the workshop. For the establishment
of the program, we are grateful to all authors and to the program committee that has
provided each paper with an average of over 6 high quality reviews, hopefully contributing
both to the paper quality and to each author's future research. We thank the invited
speakers, Dr. John Russell Lange (University of Pittsburgh, USA), Dr. Abhishek Gupta
(Intel Corp, USA), and Prof. Guillaume Pierre (IRISA / Rennes 1 University, France),
for their presentations, each highlighting a different aspect on the convergence of
HPC and cloud computing.},
location = {Portland, Oregon, USA}
}

@inproceedings{10.1145/2797143.2797145,
author = {Stephanakis, Ioannis M. and Chochliouros, Ioannis P. and Sfakianakis, Evangelos and Shirazi, Noorulhassan},
title = {Anomaly Detection In Secure Cloud Environments Using a Self-Organizing Feature Map (SOFM) Model For Clustering Sets of R-Ordered Vector-Structured Features},
year = {2015},
isbn = {9781450335805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797143.2797145},
doi = {10.1145/2797143.2797145},
abstract = {Cloud computing delivers services over virtualized networks to many end-users. Cloud
services are characterized by such attributes as on-demand self-service, broad network
access, resource pooling, rapid and elastic resource provisioning and metered services
of various qualities. Cloud networks provide data as well as multimedia and video
services. Cloud computing for critical structure IT is a relative new area of potential
applications. Cloud networks are classified into private cloud networks, public cloud
networks and hybrid cloud networks. Anomaly detection systems are defined as a branch
of intrusion detection systems that deal with identifying anomalous events with respect
to normal system behavior. A novel application of a Self-Organizing-Feature Map (SOFM)
of reduced/aggregate sets of ordered vector structured features that are used for
detecting anomalies in the context of secure cloud environments is herein proposed.
Multivalue inputs consist of reduced/aggregate ordered sets of vector and binary features.
The nodes of the SOFM - after training - are indicative of local distributions of
feature measurements during normal cloud operation. Anomalies are detected as outliers
of the trained SOFM. Each structured vector consists of binary as well as histogram
data. The aggregated Canberra distance is used to order histogram data whereas the
Jaccard distance is used for multivalue binary data. The so-called Cross-Order Distance
Matrix is defined for both cases. The distance depends upon the selection of a similarity/distance
measure and a method for operating upon the elements of the Cross-Order Distance Matrix.
Several methods of estimating the distance between two ordered sets of features are
investigated in the course of this paper.},
booktitle = {Proceedings of the 16th International Conference on Engineering Applications of Neural Networks (INNS)},
articleno = {27},
numpages = {9},
keywords = {Reduced/aggregate-ordering, Secure cloud networks, Self-Organizing Feature Maps (SOFMs), clustering, Canberra distance, intrusion detection, Jaccard distance},
location = {Rhodes, Island, Greece},
series = {EANN '15}
}

@inproceedings{10.1145/3328020.3353936,
author = {Gao, Zhijun and Gao, Yuxin and Xu, Jingjing},
title = {Designing Metrics to Evaluate the Help Center of Baidu Cloud},
year = {2019},
isbn = {9781450367905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328020.3353936},
doi = {10.1145/3328020.3353936},
abstract = {Help centers are mainly designed to assist users with their product uses. The question
as to how we measure the quality of a help center remains unanswered. As the first
step of a joint research initiated by Peking University and Baidu Cloud that aims
to develop a set of computable metrics to evaluate the quality of help centers, this
experience report shares the results of data analysis on correlation between user
behavioral data and technical documentation quality. The documents and data we use
are a suite of cloud computing services provided by Baidu Cloud. The report begins
with an introduction of the research goal; following reviews on the related work,
it then lays out the design of the experiments with user data collected from Baidu
Cloud. In our experiments, we categorize all documents into three groups and try to
identify which metrics would affect documentation quality most. The result shows that
the key index that contributes most to the model is PV/UV. At last, the report concludes
with our current experimental efforts and future work in our plan.},
booktitle = {Proceedings of the 37th ACM International Conference on the Design of Communication},
articleno = {28},
numpages = {7},
keywords = {web metrics, help center evaluation, technical information, quality evaluation},
location = {Portland, Oregon},
series = {SIGDOC '19}
}

@inproceedings{10.1145/3267357.3267362,
author = {Arias-Cabarcos, Patricia and Almen\'{a}rez, Florina and D\'{\i}az-S\'{a}nchez, Daniel and Mar\'{\i}n, Andr\'{e}s},
title = {FRiCS: A Framework for Risk-Driven Cloud Selection},
year = {2018},
isbn = {9781450359887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267357.3267362},
doi = {10.1145/3267357.3267362},
abstract = {Our devices and interactions in a world where physical and digital realities are more
and more blended, generate a continuum of multimedia data that needs to be stored,
shared and processed to provide services that enrich our daily lives. Cloud computing
plays a key role in these tasks, dissolving resource allocation and computational
boundaries, but it also requires advanced security mechanisms to protect the data
and provide privacy guarantees. Therefore, security assurance must be evaluated before
offloading tasks to a cloud provider, a process which is currently manual, complex
and inadequate for dynamic scenarios. However, though there are many tools for evaluating
cloud providers according to quality of service criteria, automated categorization
and selection based on risk metrics is still challenging. To address this gap, we
present FRiCS, a Framework for Risk-driven Cloud Selection, which contributes with:
1) a set of cloud security metrics and risk-based weighting policies, 2) distributed
components for metric extraction and aggregation, and 3) decision-making plugins for
ranking and selection. We have implemented the whole system and conducted a case-study
validation based on public cloud providers' security data, showing the benefits of
the proposed approach.},
booktitle = {Proceedings of the 2nd International Workshop on Multimedia Privacy and Security},
pages = {18–26},
numpages = {9},
keywords = {risk-driven security, cloud computing, decision making, security metrics, cloud-based multimedia systems},
location = {Toronto, Canada},
series = {MPS '18}
}

@inproceedings{10.1109/CCGrid.2016.83,
author = {Farias, Victor A. E. and Sousa, Fl\'{a}vio R. C. and Maia, Jos\'{e} G. R. and Gomes, Jo\~{a}o P. P. and Machado, Javam C.},
title = {Machine Learning Approach for Cloud NoSQL Databases Performance Modeling},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.83},
doi = {10.1109/CCGrid.2016.83},
abstract = {Cloud computing is a successful, emerging paradigm that supports on-demand services
with pay-as-you-go model. With the exponential growth of data, NoSQL databases have
been used to manage data in the cloud. In these newly emerging settings, mechanisms
to guarantee Quality of Service heavily relies on performance predictability, i.e.,
the ability to estimate the impact of concurrent query execution on the performance
of individual queries in a continuously evolving workload. This paper presents a performance
modeling approach for NoSQL databases in terms of performance metrics which is capable
of capturing the non-linear effects caused by concurrency and distribution aspects.
Experimental results confirm that our performance modeling can accurately predict
mean response time measurements under a wide range of workload configurations.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {617–620},
numpages = {4},
keywords = {cloud computing, NoSQL, performance modeling},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/3147213.3149214,
author = {Aske, Austin and Zhao, Xinghui},
title = {An Actor-Based Framework for Edge Computing},
year = {2017},
isbn = {9781450351492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147213.3149214},
doi = {10.1145/3147213.3149214},
abstract = {The Actor model provides inherent parallelism, along with other convenient features
to build large-scale distributed systems. In this paper, we present ActorEdge, an
Actor based distributed framework for edge computing. ActorEdge provides straitforward
integration with existing technologies, while enabling application developers to dynamically
utilize computational resources on the edge of the clouds. ActorEdge has proven to
outperform cloud computing options by providing superior quality of service, measuring
a 10x lower latency, 30% less jitter, and greater bandwidth. Using this framework,
programmers can easily develop and deploy their applications on a heterogeneous system,
including cloud servers/data centers, edge servers, and mobile devices.},
booktitle = {Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {199–200},
numpages = {2},
keywords = {mobile clouds, cloud computing, edge computing, actors},
location = {Austin, Texas, USA},
series = {UCC '17}
}

@inproceedings{10.1145/3132847.3133045,
author = {Fang, Zhou and Yu, Tong and Mengshoel, Ole J. and Gupta, Rajesh K.},
title = {QoS-Aware Scheduling of Heterogeneous Servers for Inference in Deep Neural Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133045},
doi = {10.1145/3132847.3133045},
abstract = {Deep neural networks (DNNs) are popular in diverse fields such as computer vision
and natural language processing. DNN inference tasks are emerging as a service provided
by cloud computing environments. However, cloud-hosted DNN inference faces new challenges
in workload scheduling for the best Quality of Service (QoS), due to dependence on
batch size, model complexity and resource allocation. This paper represents the QoS
metric as a utility function of response delay and inference accuracy. We first propose
a simple and effective heuristic approach that keeps low response delay and satisfies
the requirement on processing throughput. Then we describe an advanced deep reinforcement
learning (RL) approach that learns to schedule from experience. The RL scheduler is
trained to maximize QoS, using a set of system statuses as the input to the RL policy
model. Our approach performs scheduling actions only when there are free GPUs, thus
reduces scheduling overhead over common RL schedulers that run at every continuous
time step. We evaluate the schedulers on a simulation platform and demonstrate the
advantages of RL over heuristics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2067–2070},
numpages = {4},
keywords = {web service, deep neural networks inference, deep reinforcement learning, reinforcement learning, qos aware scheduling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.5555/2872550.2872554,
author = {Yu, Ning and Gu, Feng and Guo, Xuan and He, Zaobo},
title = {A Fine-Grained Flow Control Model for Cloud-Assisted Data Broadcasting},
year = {2015},
isbn = {9781510801004},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Cloud-assisted data broadcasting is an emerging application where cloud computing
assists data broadcasting to extend the capacity of system computing and improve the
interactivity of the conventional media. However, with the increase in scale, it brings
the difficulty on the complexity to provide the sufficient quality of service for
diverse receivers. In order to obtain a fine-grained flow rate as well as the system
stability, we propose a model based on parallel scheduling, fair queue and Proportional-Integral-Derivative
(PID) controller to cope with these challenges. PID controller takes advantage of
the feedback of the statistical output stream and automatically adjusts the transmission
flow so that the system can achieve the fine-grained multiplexing performance. Meanwhile,
we adopt a set of novel metrics to monitor and measure the quality of flow control
in order to weaken the negative impact of coarse-grained flow to user-end devices
to the minimum level. Extensive simulations and evaluations have illustrated the superiority
of the proposed model in the performance and the quality of service in terms of proposed
measurement metrics.},
booktitle = {Proceedings of the 18th Symposium on Communications &amp; Networking},
pages = {24–31},
numpages = {8},
keywords = {impact energy, quality of service, fair queue, cloud-assisted data broadcasting, fine-grained flow control, energy metric, time division multiplexing, user-end devices, impact power, proportional-integral-derivative (PID) controller, heterogeneous network},
location = {Alexandria, Virginia},
series = {CNS '15}
}

@proceedings{10.1145/2737182,
title = {QoSA '15: Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 11th International ACM Sigsoft Conference on the Quality of Software
Architectures -- QoSA 2015. For more than a decade, QoSA has strived to advance the
state of the art of quality aspects of software architecture, focusing broadly on
its quality characteristics and how these relate to the design of software architectures.
Specific issues of interest are defining and modeling quality measures, evaluating
and managing architecture quality, linking architecture to requirements and implementation,
and preserving architecture quality throughout the system lifetime. Past themes for
QoSA include Architecting for Adaptivity (2014), The System View (2013), Evolving
Architectures (2012), Quality throughout the Software Lifecycle (2011), and Research
into Practice -- Reality and Gaps (2010).QoSA 2015's theme is "Software Architecture
for the 4th Industrial Revolution". After mechanization, mass production, and electronics,
the Internet is about to enable a new level of productivity in manufacturing. This
shall be enabled by smart cyber-physical systems connected to cloud computing services
and communicating using standardized semantics. In the near future, industrial big
data analytics on monitored sensor data shall improve the efficiency and individualization
of production facilities. This year's QoSA conference solicited contributions that
explore the various implications of this upcoming industrial revolution on software
architecture. This included reference architectures, software architectures adapting
at run time, architecture styles and patterns for cyber-physical and distributed systems.The
call for papers attracted 42 initial submissions from Asia, North America, Africa,
and Europe and 28 final submissions were considered during the review process. The
program committee accepted 11 full papers and 2 short papers that cover topics, such
as new architecture modeling approaches, architectural tactics for mobile computing,
cloud computing architectures, and cyberphysical systems. QoSA's 2015 proceedings
also include 2 papers from the WCOP 2015, the 20th International Doctoral Symposium
on Components and Architecture.QoSA 2015 is part of the federated events on component-based
software engineering and software architecture (CompArch 2015), which include WICSA
2015 (12th Working IEEE / IFIP Conference on Software Architecture) and CBSE 2015
(18th International ACM SIGSOFT Symposium on Component-Based Software Engineering).},
location = {Montr\'{e}al, QC, Canada}
}

@inproceedings{10.1145/3425269.3425272,
author = {Silva, Jorge Luiz Machado da and de Fran\c{c}a, Breno B. Nicolau and Rubira, Cec\'{\i}lia Mary Fischer},
title = {Generating Trustworthiness Adaptation Plans Based on Quality Models for Cloud Platforms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425272},
doi = {10.1145/3425269.3425272},
abstract = {Cloud computing platforms can offer many benefits related to the provision of service
processing and storage for hosting client applications. Trustworthiness can be defined
as the trust of a customer in a cloud service and its provider; however, the assurance
of this property is not trivial. First, trustworthiness in general is not composed
by a single quality attribute, but by the combination of multiple attributes, such
as data privacy, performance, reliability, etc. Second, during runtime clients can
experience a change of the trustworthiness level required by their application due
to the degradation of the cloud service. This article presents a solution that monitors
during runtime the set of quality attributes of a specific application and generates
adaptation plans in order to certify that an adequate resource amount be provided
by the cloud in order to keep its trustworthiness level. Our solution is based on
quality models to compute the metric associated to each non-functional requirement
and their combination them into different types of trustworthiness levels. The main
contribution of the solution is to provide an approach which deals with multiple requirements
at the same time (or simultaneously) during runtime in order to adapt the cloud resources
to keep the trustworthiness level required by the application. The solution was evaluated
by an experiment considering a scenario where the application trustworthiness level
was composed by three quality attributes: data privacy, performance and reliability.
Initial results have shown that the approach is feasible in terms of the execution
of the adaptation plans during runtime to certify the trustworthiness level required
by the application.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {141–150},
numpages = {10},
keywords = {Cloud Computing, Trustworthiness, Adaptation Planning, Self-adaptive Systems},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3447545.3451190,
author = {Henning, S\"{o}ren and Hasselbring, Wilhelm},
title = {How to Measure Scalability of Distributed Stream Processing Engines?},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451190},
doi = {10.1145/3447545.3451190},
abstract = {Scalability is promoted as a key quality feature of modern big data stream processing
engines. However, even though research made huge efforts to provide precise definitions
and corresponding metrics for the term scalability, experimental scalability evaluations
or benchmarks of stream processing engines apply different and inconsistent metrics.
With this paper, we aim to establish general metrics for scalability of stream processing
engines. Derived from common definitions of scalability in cloud computing, we propose
two metrics: a load capacity function and a resource demand function. Both metrics
relate provisioned resources and load intensities, while requiring specific service
level objectives to be fulfilled. We show how these metrics can be employed for scalability
benchmarking and discuss their advantages in comparison to other metrics, used for
stream processing engines and other software systems.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {85–88},
numpages = {4},
keywords = {cloud computing, stream processing, metrics, scalability},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3128128.3128161,
author = {Marwan, M. and Kartit, A. and Ouahmane, H.},
title = {Protecting Medical Data in Cloud Storage Using Fault-Tolerance Mechanism},
year = {2017},
isbn = {9781450352819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128128.3128161},
doi = {10.1145/3128128.3128161},
abstract = {Given the fact that cloud computing offers cost-efficient storage systems, medical
organizations are more interested in using this alternative solution to safeguard
their patients' data. Equally interestingly, users are charged based typically on
the amount of occupied storage space. Basically, this concept is meant to cut costs
and improve the quality of healthcare services. Consequently, implementing cloud storage
would help clients to manage their data efficiently. Besides, it allows users to outsource
the storage process by using virtual storage systems instead of local ones. Despite
its significant impact in healthcare domain, adopting this paradigm to save medical
data on remote servers poses serious challenges, especially security risks. Currently,
various cryptographic techniques have been used to ensure data confidentiality and
to avoid data disclosure. Globally, this model uses traditional cryptosystems such
as AES, RSA to address security issues in cloud storage. As far as we know, there
are only a few works in literature that deal with availability and data recovery in
cloud computing. In general, the classical approach which is based on backup or replication
is not suitable for cloud environment due to the highly dynamic nature of this model.
The intent of this work is to enhance the reliability of cloud storage in order to
meet security requirements. In this study, we propose a novel method based on Shamir's
Secret Share Scheme and multi-cloud concept to avoid data loss and unauthorized access.
More precisely, this technique seeks to divide consumers' data into several portions
using Shamir's Secret Share to prevent privacy disclosure. Based on these considerations,
we store these created portions in different nodes to minimize security risks, particularly
internal attacks. To sum up, this method is designed to ensure fault-tolerance, which
is the main subject of this study. In fact, we need just certain shares to reconstruct
the secret data rather than using all parts. The experimental results are in accordance
with the theoretical assumptions behind this model, and hence, confirm that the proposed
framework provides necessary measures for preventing data loss in cloud storage.},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
pages = {214–219},
numpages = {6},
keywords = {cloud computing, medical image, fault tolerance, security},
location = {Rabat, Morocco},
series = {ICSDE '17}
}

@inproceedings{10.1109/UCC.2014.49,
author = {Keller, Matthias and Robbert, Christoph and Karl, Holger},
title = {Template Embedding: Using Application Architecture to Allocate Resources in Distributed Clouds},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.49},
doi = {10.1109/UCC.2014.49},
abstract = {In distributed cloud computing, application deployment across multiple sites can improve
quality of service. Recent research developed algorithms to find optimal locations
for virtual machines. However, those algorithms assume to have either single-tier
applications or a fixed number of virtual machines--a strong simplification of reality.
This paper investigates the placement and scaling of complex application architectures.
An application is dynamically scaled to fit both the current demand situation and
the currently available infrastructure resources. We compare two approaches: The first
one is based on virtual network embedding. The second approach is a novel method called
Template Embedding. It is based on a hierarchical 1-allocation hub flow problem and
combines application scaling and embedding in one step. Extensive experiments on 43200
network configurations showed that Template Embedding outperforms virtual network
embedding in all cases in three metrics: success rate, solution quality, and runtime.
This positive result shows that template embedding is a promising approach for distributed
cloud resource allocation.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {387–395},
numpages = {9},
keywords = {Flow Problem, Hub Problem, Cloud Resource Allocation, Distributed Cloud Computing, Application Architecture},
series = {UCC '14}
}

@inproceedings{10.1145/2668930.2688818,
author = {Lehrig, Sebastian and Becker, Steffen},
title = {The CloudScale Method for Software Scalability, Elasticity, and Efficiency Engineering: A Tutorial},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688818},
doi = {10.1145/2668930.2688818},
abstract = {In cloud computing, software engineers design systems for virtually unlimited resources
that cloud providers account on a pay-per-use basis. Elasticity management systems
provision these resource autonomously to deal with changing workloads. Such workloads
call for new objective metrics allowing engineers to quantify quality properties like
scalability, elasticity, and efficiency. However, software engineers currently lack
engineering methods that aid them in engineering their software regarding such properties.
Therefore, the CloudScale project developed tools for such engineering tasks. These
tools cover reverse engineering of architectural models from source code, editors
for manual design/adaption of such models, as well as tools for the analysis of modeled
and operating software regarding scalability, elasticity, and efficiency. All tools
are interconnected via ScaleDL, a common architectural language, and the CloudScale
Method that leads through the engineering process. In this tutorial, we execute our
method step-by-step such that every tool and ScaleDL are briefly introduced.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {329–331},
numpages = {3},
keywords = {engineering, cloudscale, tutorial, metrics, efficiency, scalability, elasticity, cloud computing, software analysis, method},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/2996890.3007870,
author = {Uhlir, Vojtech and Tomanek, Ondrej and Kencl, Lukas},
title = {Latency-Based Benchmarking of Cloud Service Providers},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.3007870},
doi = {10.1145/2996890.3007870},
abstract = {With the ever-increasing trend of migration of applications to the Cloud environment,
there is a growing need to thoroughly evaluate quality of the Cloud service itself,
before deciding upon a hosting provider. Benchmarking the Cloud services is difficult
though, due to the complex nature of the Cloud Computing setup and the diversity of
locations, of applications and of their specific service requirements. However, such
comparison may be crucial for decision making and for troubleshooting of services
offered by the intermediate businesses - the so-called Cloud tenants. Existing cross-sectional
studies and benchmarking methodologies provide only a shallow comparison of Cloud
services, whereas state-of-the-art tooling for specific comparisons of application-performance
parameters, such as for example latency, is insufficient. In this work, we propose
a novel methodology for benchmarking of Cloud-service providers, which is based on
latency measurements collected via active probing, and can be tailored to specific
application needs. Furthermore, we demonstrate its applicability on a practical longitudinal
study of real measurements of two major Cloud-service providers - Amazon and Microsoft.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {263–268},
numpages = {6},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/3386367.3431670,
author = {Sacco, Alessio and Esposito, Flavio and Marchetto, Guido},
title = {A Distributed Reinforcement Learning Approach for Energy and Congestion-Aware Edge Networks},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431670},
doi = {10.1145/3386367.3431670},
abstract = {The abiding attempt of automation has also pervaded computer networks, with the ability
to measure, analyze, and control themselves in an automated manner, by reacting to
changes in the environment (e.g., demand) while exploiting existing flexibilities.
When provided with these features, networks are often referred to as "self-driving".
Network virtualization and machine learning are the drivers. In this regard, the provision
and orchestration of physical or virtual resources are crucial for both Quality of
Service guarantees and cost management in the edge/cloud computing ecosystem. Auto-scaling
mechanisms are hence essential to effectively manage the lifecycle of network resources.
In this poster, we propose Relevant, a distributed reinforcement learning approach
to enable distributed automation for network orchestrators. Our solution aims at solving
the congestion control problem within Software-Defined Network infrastructures, while
being mindful of the energy consumption, helping resources to scale up and down as
traffic demands fluctuate and energy optimization opportunities arise.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {546–547},
numpages = {2},
keywords = {self-driving networks, reinforcement learning, auto-scaling},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@inproceedings{10.1145/3316615.3316622,
author = {Ming, Fan Xiu and Habeeb, Riyaz Ahamed Ariyaluran and Md Nasaruddin, Fariza Hanum Binti and Gani, Abdullah Bin},
title = {Real-Time Carbon Dioxide Monitoring Based on IoT &amp; Cloud Technologies},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316622},
doi = {10.1145/3316615.3316622},
abstract = {In recent years, environment monitoring are of greater importance towards the area
of climate monitoring, analysis, agricultural productivity management, quality assurance
of water, air, alongside with other potential factors that are closely connected to
industrial development and convenience of living. This research is motivated by creating
awareness of smart home residents on indoor air quality, as well as providing insight
of carbon dioxide emissions for industries and environmental organizations.This paper
proposes an efficient solution towards environment monitoring of carbon dioxide integrated
with Internet of Things capability and cloud computing technology. Aforementioned
techniques will deliver highly accessible and real-time data visualization which would
be greatly beneficial for Smart Homes efficiency of analysis actualization and counter-measures
deployment. A monitoring architecture was developed to generate, accumulate, store
and visualize carbon dioxide concentration using MQ135 carbon dioxide sensor, ESP8266
Wi-Fi module, Firebase Cloud Storage Service and Android mobile application Carbon
Insight for data visualization. 2880 data points in the time frame of 10 days with
a 30-second interval was collected, stored and visualized with the application of
this system.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {517–521},
numpages = {5},
keywords = {cloud, Internet of things, environment monitoring},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@inproceedings{10.1109/CCGrid.2015.152,
author = {Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
title = {Modeling Cross-Architecture Co-Tenancy Performance Interference},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.152},
doi = {10.1109/CCGrid.2015.152},
abstract = {Cloud computing has become a dominant computing paradigm to provide elastic, affordable
computing resources to end users. Due to the increased computing power of modern machines
powered by multi/many-core computing, data centers often co-locate multiple virtual
machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing
and competition. Applications or VMs co-locating in one physical machine can interfere
with each other despite of the promise of performance isolation through virtualization.
Modeling and predicting co-run interference therefore becomes critical for data center
job scheduling and QoS (Quality of Service) assurance. Co-run interference can be
categorized into two metrics, sensitivity and pressure, where the former denotes how
an application's performance is affected by its co-run applications, and the latter
measures how it impacts the performance of its co-run applications. This paper shows
that sensitivity and pressure are both application- and architecture-dependent. Further,
we propose a regression model that predicts an application's sensitivity and pressure
across architectures with high accuracy. This regression model enables a data center
scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate
with another VMs/applications.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {231–240},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@article{10.1145/3284553,
author = {Avgeris, Marios and Dechouniotis, Dimitrios and Athanasopoulos, Nikolaos and Papavassiliou, Symeon},
title = {Adaptive Resource Allocation for Computation Offloading: A Control-Theoretic Approach},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3284553},
doi = {10.1145/3284553},
abstract = {Although mobile devices today have powerful hardware and networking capabilities,
they fall short when it comes to executing compute-intensive applications. Computation
offloading (i.e., delegating resource-consuming tasks to servers located at the edge
of the network) contributes toward moving to a mobile cloud computing paradigm. In
this work, a two-level resource allocation and admission control mechanism for a cluster
of edge servers offers an alternative choice to mobile users for executing their tasks.
At the lower level, the behavior of edge servers is modeled by a set of linear systems,
and linear controllers are designed to meet the system’s constraints and quality of
service metrics, whereas at the upper level, an optimizer tackles the problems of
load balancing and application placement toward the maximization of the number the
offloaded requests. The evaluation illustrates the effectiveness of the proposed offloading
mechanism regarding the performance indicators, such as application average response
time, and the optimal utilization of the computational resources of edge servers.},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {23},
numpages = {20},
keywords = {feedback control, Edge computing, linear modeling}
}

@inproceedings{10.1145/3030207.3030214,
author = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Papadopoulos, Alessandro V. and Ghit, Bogdan and Epema, Dick and Iosup, Alexandru},
title = {An Experimental Performance Evaluation of Autoscaling Policies for Complex Workflows},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030214},
doi = {10.1145/3030207.3030214},
abstract = {Simplifying the task of resource management and scheduling for customers, while still
delivering complex Quality-of-Service (QoS), is key to cloud computing. Many autoscaling
policies have been proposed in the past decade to decide on behalf of cloud customers
when and how to provision resources to a cloud application utilizing cloud elasticity
features. However, in prior work, when a new policy is proposed, it is seldom compared
to the state-of-the-art, and is often compared only to static provisioning using a
predefined QoS target. This reduces the ability of cloud customers and of cloud operators
to choose and deploy an autoscaling policy. In our work, we conduct an experimental
performance evaluation of autoscaling policies, using as application model workflows,
a commonly used formalism for automating resource management for applications with
well-defined yet complex structure. We present a detailed comparative study of general
state-of-the-art autoscaling policies, along with two new workflow-specific policies.
To understand the performance differences between the 7 policies, we conduct various
forms of pairwise and group comparisons. We report both individual and aggregated
metrics. Our results highlight the trade-offs between the suggested policies, and
thus enable a better understanding of the current state-of-the-art.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {75–86},
numpages = {12},
keywords = {dag, cloud computing, metrics, supply, workflows, auto-scaling, scheduling, elasticity, clouds, opennebula, spec, demand, autoscaling, level of parallelism, performance, directed acyclic graph, workloads},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/2910017.2910602,
author = {Slivar, Ivan and Skorin-Kapov, Lea and Suznjevic, Mirko},
title = {Cloud Gaming QoE Models for Deriving Video Encoding Adaptation Strategies},
year = {2016},
isbn = {9781450342971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910017.2910602},
doi = {10.1145/2910017.2910602},
abstract = {Cloud gaming has been recognized as a promising shift in the online game industry,
with the aim being to deliver high-quality graphics games to any type of end user
device. The concepts of cloud computing are leveraged to render the game scene as
a video stream which is then delivered to players in real-time. Given high bandwidth
and strict latency requirements, a key challenge faced by cloud game providers lies
in configuring the video encoding parameters so as to maximize player Quality of Experience
(QoE) while meeting bandwidth availability constraints. In this paper we address this
challenge by conducting a subjective laboratory study involving 52 players and two
different games aimed at identifying QoE-driven video encoding adaptation strategies.
Empirical results are used to derive analytical QoE estimation models as functions
of bitrate and framerate, while also taking into account game type and player skill.
Results have shown that under certain identified bandwidth conditions, reductions
of framerate lead to QoE improvements due to improved graphics quality. Given that
results indicate that different QoE-driven video adaptation policies should likely
be applied for different types of games, we further report on objective video metrics
that may be used to classify games for the purpose of choosing an appropriate and
QoE-driven video codec configuration strategy.},
booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
articleno = {18},
numpages = {12},
keywords = {cloud gaming QoE, cloud gaming, QoE modeling, QoE},
location = {Klagenfurt, Austria},
series = {MMSys '16}
}

@inproceedings{10.1145/2898445.2898446,
author = {Sun, Degang and Zhang, Jie and Fan, Wei and Wang, Tingting and Liu, Chao and Huang, Weiqing},
title = {SPLM: Security Protection of Live Virtual Machine Migration in Cloud Computing},
year = {2016},
isbn = {9781450342858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2898445.2898446},
doi = {10.1145/2898445.2898446},
abstract = {Virtual machine live migration technology, as an important support for cloud computing,
has become a central issue in recent years. The virtual machines' runtime environment
is migrated from the original physical server to another physical server, maintaining
the virtual machines running at the same time. Therefore, it can make load balancing
among servers and ensure the quality of service. However, virtual machine migration
security issue cannot be ignored due to the immature development of it. This paper
we analyze the security threats of the virtual machine migration, and compare the
current proposed protection measures. While, these methods either rely on hardware,
or lack adequate security and expansibility. In the end, we propose a security model
of live virtual machine migration based on security policy transfer and encryption,
named as SPLM (Security Protection of Live Migration) and analyze its security and
reliability, which proves that SPLM is better than others. This paper can be useful
for the researchers to work on this field. The security study of live virtual machine
migration in this paper provides a certain reference for the research of virtualization
security, and is of great significance.},
booktitle = {Proceedings of the 4th ACM International Workshop on Security in Cloud Computing},
pages = {2–9},
numpages = {8},
keywords = {live migration, virtual machine, virtualization, cloud computing, security},
location = {Xi'an, China},
series = {SCC '16}
}

@article{10.1145/3132041,
author = {Slivar, Ivan and Suznjevic, Mirko and Skorin-Kapov, Lea},
title = {Game Categorization for Deriving QoE-Driven Video Encoding Configuration Strategies for Cloud Gaming},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3132041},
doi = {10.1145/3132041},
abstract = {Cloud gaming has been recognized as a promising shift in the online game industry,
with the aim of implementing the “on demand” service concept that has achieved market
success in other areas of digital entertainment such as movies and TV shows. The concepts
of cloud computing are leveraged to render the game scene as a video stream that is
then delivered to players in real-time. The main advantage of this approach is the
capability of delivering high-quality graphics games to any type of end user device;
however, at the cost of high bandwidth consumption and strict latency requirements.
A key challenge faced by cloud game providers lies in configuring the video encoding
parameters so as to maximize player Quality of Experience (QoE) while meeting bandwidth
availability constraints. In this article, we tackle one aspect of this problem by
addressing the following research question: Is it possible to improve service adaptation
based on information about the characteristics of the game being streamed? To answer
this question, two main challenges need to be addressed: the need for different QoE-driven
video encoding (re-)configuration strategies for different categories of games, and
how to determine a relevant game categorization to be used for assigning appropriate
configuration strategies. We investigate these problems by conducting two subjective
laboratory studies with a total of 80 players and three different games. Results indicate
that different strategies should likely be applied for different types of games, and
show that existing game classifications are not necessarily suitable for differentiating
game types in this context. We thus further analyze objective video metrics of collected
game play video traces as well as player actions per minute and use this as input
data for clustering of games into two clusters. Subjective results verify that different
video encoding configuration strategies may be applied to games belonging to different
clusters.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jun,
articleno = {56},
numpages = {24},
keywords = {Cloud gaming, video codec configuration strategies, game categorization, Quality of Experience}
}

@article{10.1145/3164537,
author = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Bauer, Andr\'{e} and Papadopoulos, Alessandro V. and Epema, Dick and Iosup, Alexandru},
title = {An Experimental Performance Evaluation of Autoscalers for Complex Workflows},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3164537},
doi = {10.1145/3164537},
abstract = {Elasticity is one of the main features of cloud computing allowing customers to scale
their resources based on the workload. Many autoscalers have been proposed in the
past decade to decide on behalf of cloud customers when and how to provision resources
to a cloud application based on the workload utilizing cloud elasticity features.
However, in prior work, when a new policy is proposed, it is seldom compared to the
state-of-the-art, and is often compared only to static provisioning using a predefined
quality of service target. This reduces the ability of cloud customers and of cloud
operators to choose and deploy an autoscaling policy, as there is seldom enough analysis
on the performance of the autoscalers in different operating conditions and with different
applications. In our work, we conduct an experimental performance evaluation of autoscaling
policies, using as application model workflows, a popular formalism for automating
resource management for applications with well-defined yet complex structures. We
present a detailed comparative study of general state-of-the-art autoscaling policies,
along with two new workflow-specific policies. To understand the performance differences
between the seven policies, we conduct various experiments and compare their performance
in both pairwise and group comparisons. We report both individual and aggregated metrics.
As many workflows have deadline requirements on the tasks, we study the effect of
autoscaling on workflow deadlines. Additionally, we look into the effect of autoscaling
on the accounted and hourly based charged costs, and we evaluate performance variability
caused by the autoscaler selection for each group of workflow sizes. Our results highlight
the trade-offs between the suggested policies, how they can impact meeting the deadlines,
and how they perform in different operating conditions, thus enabling a better understanding
of the current state-of-the-art.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = apr,
articleno = {8},
numpages = {32},
keywords = {elasticity, benchmarking, metrics, Autoscaling, scientific workflows}
}

@inproceedings{10.1145/3030207.3044530,
author = {Michael, Nicolas and Ramannavar, Nitin and Shen, Yixiao and Patil, Sheetal and Sung, Jan-Lung},
title = {CloudPerf: A Performance Test Framework for Distributed and Dynamic Multi-Tenant Environments},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3044530},
doi = {10.1145/3030207.3044530},
abstract = {The evolution of cloud-computing imposes many challenges on performance testing and
requires not only a different approach and methodology of performance evaluation and
analysis, but also specialized tools and frameworks to support such work. In traditional
performance testing, typically a single workload was run against a static test configuration.
The main metrics derived from such experiments included throughput, response times,
and system utilization at steady-state. While this may have been sufficient in the
past, where in many cases a single application was run on dedicated hardware, this
approach is no longer suitable for cloud-based deployments. Whether private or public
cloud, such environments typically host a variety of applications on distributed shared
hardware resources, simultaneously accessed by a large number of tenants running heterogeneous
workloads. The number of tenants as well as their activity and resource needs dynamically
change over time, and the cloud infrastructure reacts to this by reallocating existing
or provisioning new resources. Besides metrics such as the number of tenants and overall
resource utilization, performance testing in the cloud must be able to answer many
more questions: How is the quality of service of a tenant impacted by the constantly
changing activity of other tenants? How long does it take the cloud infrastructure
to react to changes in demand, and what is the effect on tenants while it does so?
How well are service level agreements met? What is the resource consumption of individual
tenants? How can global performance metrics on application- and system-level in a
distributed system be correlated to an individual tenant's perceived performance?In
this paper we present CloudPerf, a performance test framework specifically designed
for distributed and dynamic multi-tenant environments, capable of answering all of
the above questions, and more. CloudPerf consists of a distributed harness, a protocol-independent
load generator and workload modeling framework, an extensible statistics framework
with live-monitoring and post-analysis tools, interfaces for cloud deployment operations,
and a rich set of both low-level as well as high-level workloads from different domains.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {189–200},
numpages = {12},
keywords = {cloud, multi-tenancy, statistics collection, load generation, performance testing, workload modeling},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1109/MICRO.2018.00056,
author = {Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
title = {CounterMiner: Mining Big Performance Data from Hardware Counters},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00056},
doi = {10.1109/MICRO.2018.00056},
abstract = {Modern processors typically provide a small number of hardware performance counters
to capture a large number of microarchitecture events1. These counters can easily
generate a huge amount (e.g., GB or TB per day) of data, which we call big performance
data in cloud computing platforms with more than thousands of servers and millions
of complex workloads running ina"24/7/365" manner. The big performance data provides
a precious foundation for root cause analysis of performance bottlenecks, architecture
and compiler optimization, and many more. However, it is challenging to extract value
from the big performance data due to: 1) the many unperceivable errors (e.g., outliers
and missing values); and 2) the difficulty of obtaining insights, e.g., relating events
to performance.In this paper, we propose CounterMiner, a rigorous methodology that
enables the measurement and understanding of big performance data by using data mining
and machine learning techniques. It includes three novel components: 1) using data
cleaning to improve data quality by replacing outliers and filling in missing values;
2) iteratively quantifying, ranking, and pruning events based on their importance
with respect to performance; 3) quantifying interaction intensity between two events
by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from
the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results
show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing
10 events on 4 hardware counters. We also conduct a real-world case study, showing
that identifying important configuration parameters of Spark programs by event importance
is much faster than directly ranking the importance of these parameters.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {613–626},
numpages = {14},
keywords = {performance counters, computer architecture, big data, data mining},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@proceedings{10.1145/2859889,
title = {ICPE '16 Companion: Companion Publication for ACM/SPEC on International Conference on Performance Engineering},
year = {2016},
isbn = {9781450341479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 7th ACM/SPEC International Conference on Performance Engineering (ICPE 2016) takes
place in Delft in The Netherlands in March 2016. The conference grew out of the ACM
Workshop on Software Performance (WOSP since 1998) and the SPEC International Performance
Engineering Workshop (SIPEW since 2008), with the goal of integrating theory and practice
in the field of performance engineering. It is a great pleasure for us to offer an
outstanding technical program this year, which we believe will allow researchers and
practitioners to present their visions and latest innovation, and to exchange ideas
within the community.Overall, we received 89 high quality submissions across all three
tracks. The main Research Track attracted 57 submissions with 19 accepted (33% acceptance
rate) for presentation at the conference. Among them were 16 full papers and three
short papers. Each paper received at least three reviews from experienced program
committee members. In the Work-In-Progress and Vision Track, six out of 15 contributions
were selected. The Industry and Experience Track received 17 submissions, of which
seven were selected for inclusion in the program. The accepted papers were organized
into five research track sessions, two industry track sessions, and one WiP and vision
track session. Three best paper candidates were also selected: two research papers
and one industry paper.We are proud to have three excellent keynote speakers as part
of our technical program: Bianca Schroeder from University of Toronto, Canada, presenting
"Case studies from the real world: The importance of measurement and analysis in building
better systems"Wilhelm Hasselbring from Kiel University, Germany, discussing "Microservices
for Scalability"Angelo Corsaro, Chief Technology Officer at PrismTech, talking about
"Cloudy, Foggy and Misty Internet of Things"In addition, the program includes four
tutorials, a doctoral symposium, a poster and demo track, the SPEC Distinguished Dissertation
Award, and three interesting workshops, including the International Workshop on Large-Scale
Testing (LT), the 2nd International Workshop on Performance Analysis of Big data Systems
(PABS), and the 2nd Workshop on Challenges in Performance Methods for Software Development
(WOSPC).The program covers traditional ICPE topics such as software and systems performance
modeling and prediction, analysis and optimization, characterization and profiling,
as well as application of performance engineering theory and techniques to several
practical fields, including distributed systems, cloud computing, storage, energy,
big data, virtualized systems and containers.},
location = {Delft, The Netherlands}
}

@proceedings{10.1145/2851553,
title = {ICPE '16: Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 7th ACM/SPEC International Conference on Performance Engineering (ICPE 2016) takes
place in Delft in The Netherlands in March 2016. The conference grew out of the ACM
Workshop on Software Performance (WOSP since 1998) and the SPEC International Performance
Engineering Workshop (SIPEW since 2008), with the goal of integrating theory and practice
in the field of performance engineering. It is a great pleasure for us to offer an
outstanding technical program this year, which we believe will allow researchers and
practitioners to present their visions and latest innovation, and to exchange ideas
within the community.Overall, we received 89 high quality submissions across all three
tracks. The main Research Track attracted 57 submissions with 19 accepted (33% acceptance
rate) for presentation at the conference. Among them were 16 full papers and three
short papers. Each paper received at least three reviews from experienced program
committee members. In the Work-In-Progress and Vision Track, six out of 15 contributions
were selected. The Industry and Experience Track received 17 submissions, of which
seven were selected for inclusion in the program. The accepted papers were organized
into five research track sessions, two industry track sessions, and one WiP and vision
track session. Three best paper candidates were also selected: two research papers
and one industry paper.We are proud to have three excellent keynote speakers as part
of our technical program: Bianca Schroeder from University of Toronto, Canada, presenting
"Case studies from the real world: The importance of measurement and analysis in building
better systems"Wilhelm Hasselbring from Kiel University, Germany, discussing "Microservices
for Scalability"Angelo Corsaro, Chief Technology Officer at PrismTech, talking about
"Cloudy, Foggy and Misty Internet of Things"In addition, the program includes four
tutorials, a doctoral symposium, a poster and demo track, the SPEC Distinguished Dissertation
Award, and three interesting workshops, including the International Workshop on Large-Scale
Testing (LT), the 2nd International Workshop on Performance Analysis of Big data Systems
(PABS), and the 2nd Workshop on Challenges in Performance Methods for Software Development
(WOSPC).The program covers traditional ICPE topics such as software and systems performance
modeling and prediction, analysis and optimization, characterization and profiling,
as well as application of performance engineering theory and techniques to several
practical fields, including distributed systems, cloud computing, storage, energy,
big data, virtualized systems and containers.},
location = {Delft, The Netherlands}
}

@inproceedings{10.5555/2722129.2722142,
author = {Buchet, Micka\"{e}l and Chazal, Fr\'{e}d\'{e}ric and Oudot, Steve Y. and Sheehy, Donald R.},
title = {Efficient and Robust Persistent Homology for Measures},
year = {2015},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {A new paradigm for point cloud data analysis has emerged recently, where point clouds
are no longer treated as mere compact sets but rather as empirical measures. A notion
of distance to such measures has been defined and shown to be stable with respect
to perturbations of the measure. This distance can easily be computed pointwise in
the case of a point cloud, but its sublevel-sets, which carry the geometric information
about the measure, remain hard to compute or approximate. This makes it challenging
to adapt many powerful techniques based on the Euclidean distance to a point cloud
to the more general setting of the distance to a measure on a metric space.We propose
an efficient and reliable scheme to approximate the topological structure of the family
of sublevel-sets of the distance to a measure. We obtain an algorithm for approximating
the persistent homology of the distance to an empirical measure that works in arbitrary
metric spaces. Precise quality and complexity guarantees are given with a discussion
on the behavior of our approach in practice.},
booktitle = {Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {168–180},
numpages = {13},
location = {San Diego, California},
series = {SODA '15}
}

@inproceedings{10.1145/3151848.3151850,
author = {Karadimce, Aleksandar and Davcev, Danco},
title = {Bayesian Network Model for Estimating User Satisfaction of Multimedia Cloud Services},
year = {2017},
isbn = {9781450353007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151848.3151850},
doi = {10.1145/3151848.3151850},
abstract = {The focus of this research is given to find a metric and determine the quality of
the offered multimedia cloud services from an end users perception. The Quality of
Experience (QoE) has been introduced to measure the quality features, which is used
to determine the end-to-end user perceived quality of the used multimedia service.
In this study, we have used students satisfaction survey, which provides direct subjective
data on need, habits, and frequency of using different multimedia services. This data
has been used for validation of the proposed Bayesian Network model for interactive
estimation of the acceptability of multimedia cloud services based on the user preferences.},
booktitle = {Proceedings of the 15th International Conference on Advances in Mobile Computing &amp; Multimedia},
pages = {3–12},
numpages = {10},
keywords = {Quality of Experience, mobile cloud services, Bayesian Network, survey evaluation},
location = {Salzburg, Austria},
series = {MoMM2017}
}

@proceedings{10.1145/3183713,
title = {SIGMOD '18: Proceedings of the 2018 International Conference on Management of Data},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to share with you the proceedings of SIGMOD 2018, the 44th ACM
SIGMOD International Conference on Management of Data, in Houston, Texas. For many
people, the words 'Houston, Texas' conjure up images of cowboy hats and oil rigs.
This is not without reason. More than 20 Fortune 500 oil and gas companies are headquartered
in Houston, and Texas beef is legendary. But less appreciated is that Houston is a
vibrant and diverse city. By the usual metrics it is the most racially and ethnically
diverse city in the United States. That diversity helps to make Houston a foodie's
paradise, with wonderful Mexican, Tex-Mex, Vietnamese, and Chinese restaurants, and
great Southern options, such as soul food and Cajun. Not to mention the best Texas-style
barbecue!The SIGMOD conference is being held at the Marriott Marquis Houston, overlooking
downtown Houston's Discovery Green park. Adjacent to Discovery Green are Minute Maid
Park and the Toyota Center, home of baseball's Houston Astros and basketball's Houston
Rockets, respectively. The conference banquet is at Minute Maid Park. Downtown Houston
is a short car or train ride from great Houston museum district attractions such as
the Menil Collection and the world-class shopping of the Houston Galleria area. And
to repeat, everywhere you go in Houston, you'll find great food!This year's technical
program features 90 research papers selected from 461 submissions, 15 industrial papers
selected from 40 submissions, two invited industrial papers, 35 demonstration papers
selected from 108 submissions, and 5 tutorials selected from 14 submissions (two of
which were merged into a 2-session tutorial). There are 15 research sessions, 4 industry
sessions, an invited special session, and two demonstration sessions. The two invited
keynotes were chosen to broaden the SIGMOD community's understanding of areas having
a major effect on data management: Eric Brewer, VP of Infrastructure at Google and
faculty member at UC Berkeley, talking about the effect of container technology on
cloud computing; and Pedro Domingos, Professor at University of Washington, talking
about machine learning-what works, what doesn't, and where the field is headed. Like
last year, the keynotes are followed by a plenary session of teaser talks, where each
presenter gives a one-minute summary of their paper, to give attendees a high-level
view of the conference and help them decide which sessions to attend.There are two
changes in the session organization from recent years, whose goal is to make the program
more compact and interesting for attendees. First, tutorials are presented during
the main conference on Tuesday through Thursday, rather than on Friday after the main
conference is over. Second, to ensure there are at most four parallel sessions in
each time slot, each research paper presentation is allocated either 20 minutes or
10 minutes. The decision of long vs. short presentations had several phases. During
the reviewing process, PC members were asked to recommend whether each paper, if accepted,
should be a long or short presentation. Then research PC group leaders made a recommendation
for each of the accepted papers they supervised -- definitely 20 minutes, 20 minutes
if there's time available, borderline, or definitely 10 minutes -- based on reviews,
reviewer discussions, and their own judgment, without knowing the identity of authors.
Their recommendation is not necessarily a quality metric. They recommended 'definitely
10' for some papers highly-rated by reviewers, because the topic was narrow, could
be explained in 10 minutes, or couldn't be explained in 20 minutes so extra time wouldn't
help. For borderline papers, the final decision was based on many factors, such as
topic diversity, institutional diversity, and the time available in the relevant session.The
Research Program Committee consisted of a Program Chair, two Program Vice Chairs,
15 group leaders, and 173 Program Committee Members. There were two rounds of submissions,
with deadlines in July and November, respectively. Initially, each paper received
three reviews. Additional reviews were solicited in cases where the reviewers did
not have enough confidence, or where there was a significant score discrepancy in
the first three reviews. Papers were extensively discussed online. Of the 458 submissions,
20 were desk rejected (i.e., without reviews), 9 were accepted based on the first
round of reviews, and 327 were rejected. Authors of the remaining 102 papers were
asked to revise their papers to address reviewers' criticisms; 81 of those revisions
were ultimately accepted. While the entire program committee worked hard to select
an excellent program, the chairs and area leaders are especially grateful to the following
program committee members for their very high quality work on the committee: Ashraf
Aboulnaga, Manos Athanassoulis, Sebastian Breβ, Graham Cormode, Sudipto Das, Khuzaima
Daudjee, Aaron Elmore, Ada Fu, Michael Hay, Yuxiong He, Yannis Katsis, Alexandra Meliou,
Dan Olteanu, Andrew Pavlo, Peter Pietzuch, Lucian Popa, Semih Salihoglu, Ryan Stutsman,
Yufei Tao, and Alexander Thomson.The program also includes industry papers, demonstrations,
tutorials, workshops, a Student Research Competition, and a New Researcher Symposium.
We thank the organizers of all the technical events, including research PC vice-chairs
Xin Luna Dong and Mohamed Mokbel, industrial PC chairs Samuel Madden and Neoklis Polyzotis,
demonstration chairs Georgia Koutrika and Feifei Li, tutorial chairs Ihab Ilyas and
Stratis Viglas, workshop chairs Ihab Ilyas and Benny Kimelfeld, Student Research Competition
chairs Alvin Cheung and Jana Giceva, and New Research Symposium chairs Katja Hose
and Eugene Wu. We are also grateful to the CMT team, who modified their reviewing
system to accommodate new aspects of this year's PC process.},
location = {Houston, TX, USA}
}

@inproceedings{10.1145/3329391,
author = {Esposito, Christian and Pop, Florin and Choi, Chang},
title = {Session Details: Theme: Information Systems: SFECS - Sustainability of Fog/Edge Computing Systems Track},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329391},
doi = {10.1145/3329391},
abstract = {Fog/Edge Computing paradigms are widely used in enterprises to address the emerging
challenges of big data analysis, because of their underlying scalable, flexible and
distributed data management schemes. The data centers in the Clouds are facing great
challenges on the burden of the consequent increasing the amount of data to be man-
aged and the additional requirements of location awareness and low latency at the
edge of network necessary by smart cites and factories. These are the reasons why
a centralized model cannot be an efficient solution for generated or required data
by the IoT devices in those applications and there is the progressive shift towards
fog nodes and smarted edge nodes mediating between the cloud and the IoT devices.
The Fog/Edge computing paradigm is a decentralized model that transfers a part of
low computing data analysis from the cloud to the intermediate (fog) nodes or the
edges, performing only high computing tasks in the cloud. This new approach tries
to minimize the three factors that negatively compromise the effective and efficient
application of the Cloud computing to smart cities and factories, or similar application
domains: the network bandwidth usage, decentralization of the data processing tasks
and reduced response latency for clients (IoT devices). Fog/Edge computing is a hierarchical
approach where the overall infrastructure is structured in multiple layers, each responsible
of offering a good coordination and data management to the nodes at the lower layer.
The lowest layer is usually composed of sensors and/or actuators that measure and/or
control the environment or a given business process, implemented as mobile devices
that are running a sensing/controlling application. In this case, combining Sustainable
computing with Fog and Edge computing represents a new approach for increasing quality-of-
service and efficiency of the system, creating the capability to present temporal
and geo-coded information, and increasing innovation, and co-designing sustainable
future large scale distributed systems. This new paradigm appears to offer a good
approach in handling the scale factor of the data size, reducing the network bandwidth
usage and the response latency of the system. In order to support specifically the
Fog/Edge architectures, there is a need, for instance, of location-awareness and computation
placement, replication and recovery. In many cases Edge resources would be required
for both computation and data storage to address the time and locality constraints.
There are multiple kinds of orchestration management solutions for virtualization
in this type of architecture with different characteristics and drawbacks. This results
in different restrictions for application definition, scalability, availability, load
balancing and so on. Also, virtualization may be needed at multiple levels in a Fog/Edge
architecture as it consists of the following levels of abstraction: at the sensing
level we have the IoT devices/smart things, at the Edge level there are the gateways
to a first collection and the data from the IoT devices and their preliminary processing,
at the Fog level we have an additional data management layer, and at the Cloud level
there is the compute/storage infrastructure with applications on top. Last, but not
least, the energy efficiency is particularly important at the IoT and edge level since
the devices may be equipped with a limited battery, possible difficult or impossible
to be charged. So, optimizing the energy consumption is a must. To address several
open research is- sues regarding sustainability of future Fog/Edge systems, this track
aims at solicit contributions highlighting challenges, state-of-the-art, and solutions
to a set of currently unresolved key questions including - but not limited to - performance,
modeling, optimization, energy-efficiency, reliability, security, privacy and techno-economic
aspects of Fog/Edge systems. Through addressing these concerns while understanding
their impacts and limitations, technological advancements will be channeled toward
more sustainable/efficient platforms for tomorrow's ever-connected systems.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3183767.3183776,
author = {Nobre, Ricardo and Reis, Lu\'{\i}s and Bispo, Jo\~{a}o and Carvalho, Tiago and Cardoso, Jo\~{a}o M.P. and Cherubin, Stefano and Agosta, Giovanni},
title = {Aspect-Driven Mixed-Precision Tuning Targeting GPUs},
year = {2018},
isbn = {9781450364447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183767.3183776},
doi = {10.1145/3183767.3183776},
abstract = {Writing mixed-precision kernels allows to achieve higher throughput together with
outputs whose precision remain within given limits. The recent introduction of native
half-precision arithmetic capabilities in several GPUs, such as NVIDIA P100 and AMD
Vega 10, contributes to make precision-tuning even more relevant as of late. However,
it is not trivial to manually find which variables are to be represented as half-precision
instead of single- or double-precision. Although the use of half-precision arithmetic
can speed up kernel execution considerably, it can also result in providing non-usable
kernel outputs, whenever the wrong variables are declared using the half-precision
data-type. In this paper we present an automatic approach for precision tuning. Given
an OpenCL kernel with a set of inputs declared by a user (i.e., the person responsible
for programming and/or tuning the kernel), our approach is capable of deriving the
mixed-precision versions of the kernel that are better improve upon the original with
respect to a given metric (e.g., time-to-solution, energy-to-solution). We allow the
user to declare and/or select a metric to measure and to filter solutions based on
the quality of the output. We implement a proof-of-concept of our approach using an
aspect-oriented programming language called LARA. It is capable of generating mixed-precision
kernels that result in considerably higher performance when compared with the original
single-precision floating-point versions, while generating outputs that can be acceptable
in some scenarios.},
booktitle = {Proceedings of the 9th Workshop and 7th Workshop on Parallel Programming and RunTime Management Techniques for Manycore Architectures and Design Tools and Architectures for Multicore Embedded Computing Platforms},
pages = {26–31},
numpages = {6},
keywords = {mixed-precision, aspect-driven, GPGPU},
location = {Manchester, United Kingdom},
series = {PARMA-DITAM '18}
}

@article{10.1145/3264284,
author = {Squillante, Mark S.},
title = {Session Details: Special Issue on the Workshop on MAthematical Performance Modeling and Analysis (MAMA 2014)},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3264284},
doi = {10.1145/3264284},
abstract = {The complexity of computer systems, networks and applications, as well as the advancements
in computer technology, continue to grow at a rapid pace. Mathematical analysis, modeling
and optimization have been playing, and continue to play, an important role in research
studies to investigate fundamental issues and trade-offs at the core of performance
problems in the design and implementation of complex computer systems, networks and
applications.On June 20, 2014, the 16th Workshop on MAthematical performance Modeling
and Analysis (MAMA 2014) was held in Austin TX, USA, sponsored by ACM SIGMETRICS,
and held in conjunction with SIGMETRICS 2014. This workshop seeks to bring together
researchers working on the mathematical, methodological and theoretical aspects of
performance analysis, modeling and optimization. It is intended to provide a forum
at SIGMETRICS conferences for talks on early research in the more mathematical areas
of computer performance analysis. These talks tend to be based on very recent research
results (including work in progress) or on new research results that will be otherwise
submitted only to a journal (or recently have been submitted to a journal). Thus,
part of the goal is to complement and supplement the SIGMETRICS Conference program
with such talks without removing any theoretical contributions from the main technical
program. Furthermore, we continue to experience the desired result of having abstracts
from previous MAMA workshops appear as full papers in the main program of subsequent
SIGMETRICS and related conferences.All submissions were reviewed by at least 4 members
of the program committee, from which a total of 13 were selected for presentation
at the MAMA 2014 workshop. This special issue of Performance Evaluation Review includes
extended abstracts relating to these presentations (arranged in the order of their
presentation), which cover a wide range of topics in the area of mathematical performance
analysis, modeling and optimization.The study of Gelenbe examines the backlog of energy
and of data packets in a sensor node that harvests energy, computing the properties
of energy and data backlogs and discussing system stability. Meyfroyt derives asymptotic
results for the coverage ratio under a specific class of spatial stochastic models
(Cooperative Sequential Adsorption) and investigates the scalability of the Trickle
communication protocol algorithm. The study of Tune and Roughan applies the principle
of maximum entropy to develop fast traffic matrix synthesis models, with the future
goal of developing realistic spatio-temporal traffic matrices. Bradonji\'{c} et al. compare
and contrast the capacity, congestion and reliability requirements for alternative
connectivity models of large-scale data centers relative to fat trees. The study of
Rochman et al. considers the problem of resource placement in network applications,
based on a largescale service faced with regionally distributed demands for various
resources in cloud computing. Xie and Lui investigate the design and analysis of a
rating system and a mechanism to encourage users to participate in crowdsourcing and
to incentivize workers to develop high-quality solutions. The study of Asadi et al.
formulates a general problem for the joint per-user mode selection, connection activation
and resource scheduling of connections using both LTE and WiFi resources within the
context of device-todevice communications. Zheng and Tan consider a nonconvex joint
rate and power control optimization to achieve egalitarian fairness (max-min weighted
fairness) in wireless networks, exploiting the nonlinear Perron-Frobenius theory and
nonnegative matrix theory. The study of Goldberg et al. derives an asymptotically
optimal control policy for a stochastic capacity problem of dynamically matching supply
resources and uncertain demand, based on connections with lost-sales inventory models.
Ghaderi et al. investigate a dynamic stochastic bin packing problem, analyzing the
fluid limits of the system under an asymptotic best-fit algorithm and showing it asymptotically
minimizes the number of servers used in steady state. The study of Tizghadam and Leon-Garcia
examines the impact of overlaying or removing a subgraph on the Moore-Penrose inverse
of the Laplacian matrix of an existing network topology and proposes an iterative
method to find key performance measures. Miyazawa considers a two-node generalized
Jackson network in a phase-type setting as a special case of a Markov-modulated twodimensional
reflecting random walk and analyzes the tail asymptotics for this reflecting process.
The study of Squillante et al. investigates improvement in scalability of search in
networks through the use of multiple random walks, deriving bounds on the hitting
time to a set of nodes and on various performance metrics.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = sep,
numpages = {1}
}

@inproceedings{10.1145/2713168.2723146,
author = {Pegus, Patrick and Cecchet, Emmanuel and Shenoy, Prashant},
title = {Video BenchLab Demo: An Open Platform for Video Realistic Streaming Benchmarking},
year = {2015},
isbn = {9781450333511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2713168.2723146},
doi = {10.1145/2713168.2723146},
abstract = {In this demonstration, we present an open, flexible and realistic benchmarking platform
named Video BenchLab to measure the performance of streaming media workloads. While
Video BenchLab can be used with any existing media server, we provide a set of tools
for researchers to experiment with their own platform and protocols. The components
include a MediaDrop video server, a suite of tools to bulk insert videos and generate
streaming media workloads, a dataset of freely available video and a client runtime
to replay videos in the native video players of real Web browsers such as Firefox,
Chrome and Internet Explorer. Various metrics are collected to capture the quality
of video playback and identify issues that can happen during video replay. Finally,
we provide a Dashboard to manage experiments, collect results and perform analytics
to compare performance between experiments.The demonstration showcases all the BenchLab
video components including a MediaDrop server accessed by real web browsers running
locally and in the cloud. We demo the whole experiment lifecycle from creation to
deployment as well as result collection and analysis.},
booktitle = {Proceedings of the 6th ACM Multimedia Systems Conference},
pages = {101–104},
numpages = {4},
keywords = {benchmarking, web browsers, video, streaming},
location = {Portland, Oregon},
series = {MMSys '15}
}

@article{10.1145/2998454,
author = {Li, Ning and Jiang, Hong and Feng, Dan and Shi, Zhan},
title = {Customizable SLO and Its Near-Precise Enforcement for Storage Bandwidth},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1553-3077},
url = {https://doi.org/10.1145/2998454},
doi = {10.1145/2998454},
abstract = {Cloud service is being adopted as a utility for large numbers of tenants by renting
Virtual Machines (VMs). But for cloud storage, unpredictable IO characteristics make
accurate Service-Level-Objective (SLO) enforcement challenging. As a result, it has
been very difficult to support simple-to-use and technology-agnostic SLO specifying
a particular value for a specific metric (e.g., storage bandwidth). This is because
the quality of SLO enforcement depends on performance error and fluctuation that measure
the precision of SLO enforcement. High precision of SLO enforcement is critical for
user-oriented performance customization and user experiences. To address this challenge,
this article presents V-Cup, a framework for VM-oriented customizable SLO and its
near-precise enforcement. It consists of multiple auto-tuners, each of which exports
an interface for a tenant to customize the desired storage bandwidth for a VM and
enable the storage bandwidth of the VM to converge on the target value with a predictable
precision. We design and implement V-Cup in the Xen hypervisor based on the fair sharing
scheduler for VM-level resource management. Our V-Cup prototype evaluation shows that
it achieves satisfying performance guarantees through near-precise SLO enforcement.},
journal = {ACM Trans. Storage},
month = feb,
articleno = {6},
numpages = {25},
keywords = {storage management, end-to-end control, Cloud storage, service-level objective}
}

@inproceedings{10.1145/3383812.3383838,
author = {Pacot, Mark Phil B. and Marcos, Nelson},
title = {Cloud Removal from Aerial Images Using Generative Adversarial Network with Simple Image Enhancement},
year = {2020},
isbn = {9781450377201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383812.3383838},
doi = {10.1145/3383812.3383838},
abstract = {The atmospheric condition of the presence of clouds is one of the biggest problems
in most aerial imaging systems. It degrades the visual quality of images leading to
the loss of information for ground scenes. Hence, an effective cloud removal algorithm
is a significant factor for this kind of problem and other related applications. The
proposed cloud removal technique using the generative adversarial network with simple
image enhancement (SIE-GAN) is a useful tool in removing cloud formations, most notably
in images acquired using Unmanned Aerial Vehicle System (UAVs). This technique showed
flexibility in performing the given task with satisfactory results, which is a gauge
based on No-Reference Image Quality Metric, specifically the Perception-based Image
Quality Evaluator (PIQE). Also, the proposed algorithm outperformed some of existing
cloud removal algorithms by producing a better quality output when tested on the too-cloudy
satellite images. Overall, the authors introduced a new frontier in generating cloud-free
aerial images and added a valuable contribution to the array of cloud removal algorithms.},
booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
pages = {77–81},
numpages = {5},
keywords = {cloud removal, generative adversarial network, unmanned aerial vehicle system, no-reference image quality metric, simple image enhancement},
location = {Singapore, Singapore},
series = {ICIGP 2020}
}

@article{10.1145/3093893,
author = {Garc\'{\i}a-Dorado, Jos\'{e} Luis},
title = {Bandwidth Measurements within the Cloud: Characterizing Regular Behaviors and Correlating Downtimes},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3093893},
doi = {10.1145/3093893},
abstract = {The search for availability, reliability, and quality of service has led cloud infrastructure
customers to disseminate their services, contents, and data over multiple cloud data
centers, often involving several Cloud service providers (CSPs). The consequence of
this is that a large amount of data must be transmitted across the public Cloud. However,
little is known about the bandwidth dynamics involved. To address this, we have conducted
a measurement campaign for bandwidth between 18 data centers of four major CSPs. This
extensive campaign allowed us to characterize the resulting time series of bandwidth
as the addition of a stationary component and some infrequent excursions (typically
downtimes). While the former provides a description of the bandwidth users can expect
in the Cloud, the latter is closely related to the robustness of the Cloud (i.e.,
the occurrence of downtimes is correlated). Both components have been studied further
by applying factor analysis, specifically analysis of variance, as a mechanism to
formally compare data centers’ behaviors and extract generalities. The results show
that the stationary process is closely related to the data center locations and CSPs
involved in transfers that, fortunately, make the Cloud more predictable and allow
the set of reported measurements to be extrapolated. On the other hand, although correlation
in the Cloud is low, that is, only 10% of the measured pair of paths showed some correlation,
we found evidence that such correlation depends on the particular relationships between
pairs of data centers with little connection to more general factors. Positively,
this implies that data centers either in the same area or within the same CSP do not
show qualitatively more correlation than other data centers, which eases the deployment
of robust infrastructures. On the downside, this metric is scarcely generalizable
and, consequently, calls for exhaustive monitoring.},
journal = {ACM Trans. Internet Technol.},
month = aug,
articleno = {39},
numpages = {25},
keywords = {traffic correlation, inter-cloud, TCP bandwidth, ANOVA, Public cloud}
}

@article{10.1109/TNET.2016.2614129,
author = {Zhao, Zhiwei and Dong, Wei and Bu, Jiajun and Gu, Tao and Min, Geyong},
title = {Accurate and Generic Sender Selection for Bulk Data Dissemination in Low-Power Wireless Networks},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2614129},
doi = {10.1109/TNET.2016.2614129},
abstract = {Data dissemination is a fundamental service offered by low-power wireless networks.
Sender selection is the key to the dissemination performance and has been extensively
studied. Sender impact metric plays a significant role in sender selection, since
it determines which senders are selected for transmission. Recent studies have shown
that spatial link diversity has a significant impact on the efficiency of broadcast.
However, the existing metrics overlook such impact. Besides, they consider only gains
but ignore the costs of sender candidates. As a result, existing works cannot achieve
accurate estimation of the sender impact. Moreover, they cannot well support data
dissemination with network coding, which is commonly used for lossy environments.
In this paper, we first propose a novel sender impact metric, namely,  $gamma $ ,
which jointly exploits link quality and spatial link diversity to calculate the gain/cost
ratio of the sender candidates. Then, we develop a generic sender selection scheme
based on the  $gamma $  metric called  $gamma $ -component that can generally support
both types of dissemination using native packets and network coding. Extensive evaluations
are conducted through real testbed experiments and large-scale simulations. The performance
results and analysis show that  $gamma $  achieves far more accurate impact estimation
than the existing works. In addition, the dissemination protocols based on  $gamma
$ -component outperform the existing protocols in terms of completion time and transmissions
by 20.5% and 23.1%, respectively.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {948–959},
numpages = {12}
}

@inproceedings{10.5555/2755535.2755542,
author = {Huang, Chun-Ying and Chen, Po-Han and Huang, Yu-Ling and Chen, Kuan-Ta and Hsu, Cheng-Hsin},
title = {Measuring the Client Performance and Energy Consumption in Mobile Cloud Gaming},
year = {2014},
publisher = {IEEE Press},
abstract = {Mobile cloud gaming allows gamers to play games on resource-constrained mobile devices,
and a measurement study to quality the client performance and energy consumption is
crucial to attract and retain the gamers. In this paper, we adopt an open source cloud
gaming platform to conduct extensive experiments on real mobile clients. Our experiment
results show two major findings that are of interests to researchers, developers,
and gamers. First, compared to mobile native games, mobile cloud games save energy
by up to 30%. Second, the frame rate, bit rate, and resolution all affect the decoders'
resource consumption, while frame rate imposes the highest impact. These findings
shed some light on the further enhancements of the emerging mobile cloud gaming platforms.},
booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
articleno = {5},
numpages = {3},
location = {Nagoya, Japan},
series = {NetGames '14}
}

@inproceedings{10.1145/2882903.2882943,
author = {Kalyvianaki, Evangelia and Fiscato, Marco and Salonidis, Theodoros and Pietzuch, Peter},
title = {THEMIS: Fairness in Federated Stream Processing under Overload},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882943},
doi = {10.1145/2882903.2882943},
abstract = {Federated stream processing systems, which utilise nodes from multiple independent
domains, can be found increasingly in multi-provider cloud deployments, internet-of-things
systems, collaborative sensing applications and large-scale grid systems. To pool
resources from several sites and take advantage of local processing, submitted queries
are split into query fragments, which are executed collaboratively by different sites.
When supporting many concurrent users, however, queries may exhaust available processing
resources, thus requiring constant load shedding. Given that individual sites have
autonomy over how they allocate query fragments on their nodes, it is an open challenge
how to ensure global fairness on processing quality experienced by queries in a federated
scenario.We describe THEMIS, a federated stream processing system for resource-starved,
multi-site deployments. It executes queries in a globally fair fashion and provides
users with constant feedback on the experienced processing quality for their queries.
THEMIS associates stream data with its source information content (SIC), a metric
that quantifies the contribution of that data towards the query result, based on the
amount of source data used to generate it. We provide the BALANCE-SIC distributed
load shedding algorithm that balances the SIC values of result data. Our evaluation
shows that the BALANCE-SIC algorithm yields balanced SIC values across queries, as
measured by Jain's Fairness Index. Our approach also incurs a low execution time overhead.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {541–553},
numpages = {13},
keywords = {federated data stream processing, fairness, tuple shedding, approximate data processing},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/2882903.2882943,
author = {Kalyvianaki, Evangelia and Fiscato, Marco and Salonidis, Theodoros and Pietzuch, Peter},
title = {THEMIS: Fairness in Federated Stream Processing under Overload},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882943},
doi = {10.1145/2882903.2882943},
abstract = {Federated stream processing systems, which utilise nodes from multiple independent
domains, can be found increasingly in multi-provider cloud deployments, internet-of-things
systems, collaborative sensing applications and large-scale grid systems. To pool
resources from several sites and take advantage of local processing, submitted queries
are split into query fragments, which are executed collaboratively by different sites.
When supporting many concurrent users, however, queries may exhaust available processing
resources, thus requiring constant load shedding. Given that individual sites have
autonomy over how they allocate query fragments on their nodes, it is an open challenge
how to ensure global fairness on processing quality experienced by queries in a federated
scenario.We describe THEMIS, a federated stream processing system for resource-starved,
multi-site deployments. It executes queries in a globally fair fashion and provides
users with constant feedback on the experienced processing quality for their queries.
THEMIS associates stream data with its source information content (SIC), a metric
that quantifies the contribution of that data towards the query result, based on the
amount of source data used to generate it. We provide the BALANCE-SIC distributed
load shedding algorithm that balances the SIC values of result data. Our evaluation
shows that the BALANCE-SIC algorithm yields balanced SIC values across queries, as
measured by Jain's Fairness Index. Our approach also incurs a low execution time overhead.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {541–553},
numpages = {13},
keywords = {federated data stream processing, fairness, tuple shedding, approximate data processing},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1109/IPSN.2018.00046,
author = {Islam, Bashima and Islam, Md Tamzeed and Nirjon, Shahriar},
title = {Glimpse.3D: A Motion-Triggered Stereo Body Camera for 3D Experience Capture and Preview},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00046},
doi = {10.1109/IPSN.2018.00046},
abstract = {The Glimpse.3D is a body-worn camera that captures, processes, stores, and transmits
3D visual information of a real-world environment using a low cost camera-based sensor
system that is constrained by its limited processing capability, storage, and battery
life. The 3D content is viewed on a mobile device such as a smartphone or a virtual
reality headset. This system can be used in applications such as capturing and sharing
3D content in the social media, training people in different professions, and post-facto
analysis of an event. Glimpse.3D uses off-the-shelf hardware and standard computer
vision algorithms. Its novelty lies in the ability to optimally control camera data
acquisition and processing stages to guarantee the desired quality of captured information
and battery life. The design of the controller is based on extensive measurements
and modeling of the relationships between the linear and angular motion of a body-worn
camera and the quality of generated 3D point clouds as well as the battery life of
the system. To achieve this, we 1) devise a new metric to quantify the quality of
generated 3D point clouds, 2) formulate an optimization problem to find an optimal
trigger point for the camera system that prolongs its battery life while maximizing
the quality of captured 3D environment, and 3) make the model adaptive so that the
system evolves and its performance improves over time.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {176–187},
numpages = {12},
keywords = {body camera, 3D-reconstruction},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1109/IPSN.2018.00030,
author = {Islam, Bashima and Islam, Md Tamzeed and Nirjon, Shahriar},
title = {A Motion-Triggered Stereo Camera for 3D Experience Capture: Demo Abstract},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00030},
doi = {10.1109/IPSN.2018.00030},
abstract = {This demo is an implementation of our motion-triggered camera system that captures,
processes, stores, and transmits 3D visual information of a real-world environment
using a low-cost camera-based sensor system that is constrained by its limited processing
capability, storage, and battery life. This system can be used in applications such
as capturing and sharing 3D content in the social media, training people in different
professions, and post-facto analysis of an event. This system uses off-the-shelf hardware
and standard computer vision algorithms. Its novelty lies in the ability to optimally
control camera data acquisition and processing stages to guarantee the desired quality
of captured information and battery life. The design of the controller is based on
extensive measurements and modeling of the relationships between the linear and angular
motion of a camera and the quality of generated 3D point clouds as well as the battery
life of the system. To achieve this, we 1) devise a new metric to quantify the quality
of generated 3D point clouds, 2) formulate an optimization problem to find an optimal
trigger point for the camera system and prolongs its battery life while maximizing
the quality of captured 3D environment, and 3) make the model adaptive so that the
system evolves and its performance improves over time.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {134–135},
numpages = {2},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1145/3466772.3467048,
author = {Kassir, Saadallah and de Veciana, Gustavo and Wang, Nannan and Wang, Xi and Palacharla, Paparao},
title = {Joint Update Rate Adaptation in Multiplayer Cloud-Edge Gaming Services: Spatial Geometry and Performance Tradeoffs},
year = {2021},
isbn = {9781450385589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466772.3467048},
doi = {10.1145/3466772.3467048},
abstract = {In this paper, we analyze the performance of Multiplayer Cloud Gaming (MCG) systems.
To that end, we introduce a model and new MCG-Quality of Service (QoS) metric that
captures the freshness of the players' updates and fairness in their gaming experience.
We introduce an efficient measurement-based Joint Multiplayer Rate Adaptation (JMRA)
algorithm that optimizes the MCG-QoS by overcoming large (possibly varying) network
transport delays by increasing the associated players' update rates. The resulting
MCG-QoS is shown to be Schur-concave in the network delays, leading to natural characterizations
and performance comparisons associated with the players' spatial geometry and network
congestion. In particular, joint rate adaptation enables service providers to combat
variability in network delays and players' geographic spread to achieve high service
coverage. This, in turn, allows us to explore the spatial density and capacity of
compute resources that need to be provisioned. Finally, we leverage tools from majorization
theory, to show how service placement decisions can be made to improve the robustness
of the MCG-QoS to stochastic network delays.},
booktitle = {Proceedings of the Twenty-Second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {191–200},
numpages = {10},
keywords = {Rate Adaptation, Multiplayer Cloud Gaming, Network Resource Provisioning, Service Placement, Edge Computing},
location = {Shanghai, China},
series = {MobiHoc '21}
}

@article{10.1109/TNET.2019.2900434,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet and Ra, Moo-Ryong},
title = {Multi-Tier Caching Analysis in CDN-Based Over-the-Top Video Streaming Systems},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2900434},
doi = {10.1109/TNET.2019.2900434},
abstract = {Internet video traffic has been rapidly increasing and is further expected to increase
with the emerging 5G applications, such as higher definition videos, the IoT, and
augmented/virtual reality applications. As end users consume video in massive amounts
and in an increasing number of ways, the content distribution network CDN should be
efficiently managed to improve the system efficiency. The streaming service can include
multiple caching tiers, at the distributed servers and the edge routers, and efficient
content management at these locations affects the quality of experience QoE of the
end users. In this paper, we propose a model for video streaming systems, typically
composed of a centralized origin server, several CDN sites, and edge-caches located
closer to the end user. We comprehensively consider different systems design factors,
including the limited caching space at the CDN sites, allocation of CDN for a video
request, choice of different ports or paths from the CDN and the central storage,
bandwidth allocation, the edge-cache capacity, and the caching policy. We focus on
minimizing a performance metric, stall duration tail probability SDTP, and present
a novel and efficient algorithm accounting for the multiple design flexibilities.
The theoretical bounds with respect to the SDTP metric are also analyzed and presented.
The implementation of a virtualized cloud system managed by Openstack demonstrates
that the proposed algorithms can significantly improve the SDTP metric compared with
the baseline strategies.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {835–847},
numpages = {13}
}

@article{10.1145/3284360,
author = {Dey, Tamal K. and Shi, Dayu and Wang, Yusu},
title = {SimBa: An Efficient Tool for Approximating Rips-Filtration Persistence via <u class="uu">Sim</u>plicial <u class="uu">Ba</u>tch Collapse},
year = {2019},
issue_date = {2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
issn = {1084-6654},
url = {https://doi.org/10.1145/3284360},
doi = {10.1145/3284360},
abstract = {In topological data analysis, a point cloud data P extracted from a metric space is
often analyzed by computing the persistence diagram or barcodes of a sequence of Rips
complexes built on P indexed by a scale parameter. Unfortunately, even for input of
moderate size, the size of the Rips complex may become prohibitively large as the
scale parameter increases. Starting with the Sparse Rips filtration introduced by
Sheehy, some existing methods aim to reduce the size of the complex to improve time
efficiency as well. However, as we demonstrate, existing approaches still fall short
of scaling well, especially for high-dimensional data. In this article, we investigate
the advantages and limitations of existing approaches. Based on insights gained from
the experiments, we propose an efficient new algorithm, called SimBa, for approximating
the persistent homology of Rips filtrations with quality guarantees. Our new algorithm
leverages a batch-collapse strategy as well as a new Sparse Rips-like filtration.
We experiment on a variety of low- and high-dimensional datasets. We show that our
strategy presents a significant size reduction and that our algorithm for approximating
Rips filtration persistence is an order of magnitude faster than existing methods
in practice.},
journal = {ACM J. Exp. Algorithmics},
month = jan,
articleno = {1.5},
numpages = {16},
keywords = {persistent homology, Topological data analysis, simplicial maps, approximation, rips filtration}
}

@inproceedings{10.1109/CCGrid.2016.56,
author = {Ibrahim, Abdallah Ali Zainelabden A. and Kliazovich, Dzmitry and Bouvry, Pascal},
title = {Service Level Agreement Assurance between Cloud Services Providers and Cloud Customers},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.56},
doi = {10.1109/CCGrid.2016.56},
abstract = {Cloud services providers deliver cloud services to cloud customers on pay-per-use
model while the quality of the provided services are defined using service level agreements
also known as SLAs. Unfortunately, there is no standard mechanism which exists to
verify and assure that delivered services satisfy the signed SLA agreement in an automatic
way. There is no guarantee in terms of quality. Those applications have many performance
metrics. In this doctoral thesis, we propose a framework for SLA assurance, which
can be used by both cloud providers and cloud users. Inside the proposed framework,
we will define the performance metrics for the different applications. We will assess
the applications performance in different testing environment to assure good services
quality as mentioned in SLA. The proposed framework will be evaluated through simulations
and using testbed experiments. After testing the applications performance by measuring
the performance metrics, we will review the time correlations between those metrics.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {588–591},
numpages = {4},
keywords = {simulation, data centers, performance, metrics, quality of experience, applications, quality of services, cloud computing, service level agreement},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/2872427.2883053,
author = {Zhou, Ke and Redi, Miriam and Haines, Andrew and Lalmas, Mounia},
title = {Predicting Pre-Click Quality for Native Advertisements},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883053},
doi = {10.1145/2872427.2883053},
abstract = {Native advertising is a specific form of online advertising where ads replicate the
look-and-feel of their serving platform. In such context, providing a good user experience
with the served ads is crucial to ensure long-term user engagement. In this work,
we explore the notion of ad quality, namely the effectiveness of advertising from
a user experience perspective. We design a learning framework to predict the pre-click
quality of native ads. More specifically, we look at detecting offensive native ads,
showing that, to quantify ad quality, ad offensive user feedback rates are more reliable
than the commonly used click-through rate metrics. We then conduct a crowd-sourcing
study to identify which criteria drive user preferences in native advertising. We
translate these criteria into a set of ad quality features that we extract from the
ad text, image and advertiser, and then use them to train a model able to identify
offensive ads. We show that our model is very effective in detecting offensive ads,
and provide in-depth insights on how different features affect ad quality. Finally,
we deploy a preliminary version of such model and show its effectiveness in the reduction
of the offensive ad feedback rate.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {299–310},
numpages = {12},
keywords = {native advertising, image and text, features, ad quality, ad feedback, offensive rate, pre-click experience},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{10.1145/2713168.2723145,
author = {Pegus, Patrick and Cecchet, Emmanuel and Shenoy, Prashant},
title = {Video BenchLab: An Open Platform for Realistic Benchmarking of Streaming Media Workloads},
year = {2015},
isbn = {9781450333511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2713168.2723145},
doi = {10.1145/2713168.2723145},
abstract = {In this paper, we present an open, flexible and realistic benchmarking platform named
Video BenchLab to measure the performance of streaming media workloads. While Video
BenchLab can be used with any existing media server, we provide a set of tools for
researchers to experiment with their own platform and protocols. The components include
a MediaDrop video server, a suite of tools to bulk insert videos and generate streaming
media workloads, a dataset of freely available video and a client runtime to replay
videos in the native video players of real Web browsers such as Firefox, Chrome and
Internet Explorer. We define simple metrics that are able to capture the quality of
video playback and identify issues that can happen during video replay. Finally, we
provide a Dashboard to manage experiments, collect results and perform analytics to
compare performance between experiments.We present a series of experiments with Video
BenchLab to illustrate how the video specific metrics can be used to measure the user
perceived experience in real browsers when streaming videos. We also show Internet
scale experiments by deploying clients in data centers distributed all over the globe.
All the software, datasets, workloads and results used in this paper are made freely
available on SourceForge for anyone to reuse and expand.},
booktitle = {Proceedings of the 6th ACM Multimedia Systems Conference},
pages = {165–176},
numpages = {12},
keywords = {web browsers, streaming, video, benchmarking},
location = {Portland, Oregon},
series = {MMSys '15}
}

@article{10.1145/2816795.2818061,
author = {Liao, Jing and Finch, Mark and Hoppe, Hugues},
title = {Fast Computation of Seamless Video Loops},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2816795.2818061},
doi = {10.1145/2816795.2818061},
abstract = {Short looping videos concisely capture the dynamism of natural scenes. Creating seamless
loops usually involves maximizing spatiotemporal consistency and applying Poisson
blending. We take an end-to-end view of the problem and present new techniques that
jointly improve loop quality while also significantly reducing processing time. A
key idea is to relax the consistency constraints to anticipate the subsequent blending,
thereby enabling looping of low-frequency content like moving clouds and changing
illumination. We also analyze the input video to remove an undesired bias toward short
loops. The quality gains are demonstrated visually and confirmed quantitatively using
a new gradient-domain consistency metric. We improve system performance by classifying
potentially loopable pixels, masking the 2D graph cut, pruning graph-cut labels based
on dominant periods, and optimizing on a coarse grid while retaining finer detail.
Together these techniques reduce computation times from tens of minutes to nearly
real-time.},
journal = {ACM Trans. Graph.},
month = oct,
articleno = {197},
numpages = {10},
keywords = {blend-aware consistency, video textures, cinemagraphs}
}

@inproceedings{10.1145/2649387.2660839,
author = {Lindsey, Aaron and Yeh, Hsin-Yi (Cindy) and Wu, Chih-Peng and Thomas, Shawna and Amato, Nancy M.},
title = {Improving Decoy Databases for Protein Folding Algorithms},
year = {2014},
isbn = {9781450328944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2649387.2660839},
doi = {10.1145/2649387.2660839},
abstract = {Predicting protein structures and simulating protein folding are two of the most important
problems in computational biology today. Simulation methods rely on a scoring function
to distinguish the native structure (the most energetically stable) from non-native
structures. Decoy databases are collections of non-native structures used to test
and verify these functions.We present a method to evaluate and improve the quality
of decoy databases by adding novel structures and removing redundant structures. We
test our approach on 17 different decoy databases of varying size and type and show
significant improvement across a variety of metrics. We also test our improved databases
on a popular modern scoring function and show that they contain a greater number of
native-like structures than the original databases, thereby producing a more rigorous
database for testing scoring functions.},
booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {717–724},
numpages = {8},
keywords = {protein folding, decoy databases, sampling methods},
location = {Newport Beach, California},
series = {BCB '14}
}

@inproceedings{10.1145/3319647.3325849,
author = {Nagin, Kenneth and Kassis, Andre and Lorenz, Dean and Barabash, Katherine and Raichstein, Eran},
title = {Estimating Client QoE from Measured Network QoS},
year = {2019},
isbn = {9781450367493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319647.3325849},
doi = {10.1145/3319647.3325849},
abstract = {This research is done in the context of the SliceNet project [4] that aims to extend
5G infrastructure with cognitive management of cross-domain, cross-layer network slices
[1], with emphasis on Quality of Experience (QoE) for vertical industries. The provisioning
of network slices with proper QoE guarantees is seen as one of the key enablers of
future 5G-enabled networks. The challenge is to assess the QoE experienced by the
vertical application and its users without requiring the applications or the users
to measure and report QoE related metrics back to the provider. To address this challenge,
we propose a method for deriving application-level QoE from network-level Quality
of Service (QoS) measurements, easily accessible by the provider. In particular, we
describe a PoC where QoE, perceived by application users, is estimated from low level
network monitoring data, by applying cognitive methods. Our main goal is enabling
the cloud provider to support the desired E2E QoE-based Service Level Agreements (SLAs),
e.g. by monitoring QoS metrics within the provider's domain to optimize resource allocation
through provider's actuators. Additional benefit can be achieved by applying the same
technique to troubleshoot issues in the provider's infrastructure. In this work, we
employed classical statistical methods to assess the relationship between the application-level
QoE and the network-level QoS.},
booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage},
pages = {188},
numpages = {1},
location = {Haifa, Israel},
series = {SYSTOR '19}
}

@inproceedings{10.1145/3038912.3052560,
author = {Haq, Osama and Raja, Mamoon and Dogar, Fahad R.},
title = {Measuring and Improving the Reliability of Wide-Area Cloud Paths},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052560},
doi = {10.1145/3038912.3052560},
abstract = {Many popular cloud applications use inter-data center paths; yet, little is known
about the characteristics of these ``cloud paths''. Over an eighteen month period,
we measure the inter-continental cloud paths of three providers (Amazon, Google, and
Microsoft) using client side (VM-to-VM) measurements. We find that cloud paths are
more predictable compared to public Internet paths, with an order of magnitude lower
loss rate and jitter at the tail (95th percentile and beyond) compared to public Internet
paths. We also investigate the nature of packet losses on these paths (e.g., random
vs. bursty) and potential reasons why these paths may be better in quality. Based
on our insights, we consider how we can further improve the quality of these paths
with the help of existing loss mitigation techniques. We demonstrate that using the
cloud path in conjunction with a detour path can mask most of the cloud losses, resulting
in up to five 9's of network availability for applications.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {253–262},
numpages = {10},
keywords = {inter-data center networks, loss rate, detour routing, cloud paths reliability, cloud availability, latency, bandwidth},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1109/AST.2019.000-2,
author = {Martinez-Ortiz, Andres-Leonardo and Lizcano, David and Ortega, Miguel},
title = {Software Metrics Artifacts Making Web Quality Measurable: AST 2019 Invited Paper},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2019.000-2},
doi = {10.1109/AST.2019.000-2},
abstract = {Mining open source repositories introduces an effective approach to put in practice
empirical software engineering in a variety of technologies. Kernel development (Linux)
first and then Internet (Chromium) and more recently cloud orchestration (Kubernetes)
and machine learning (TensorFlow) are fundamental pieces not just for open source
ecosystem but also for the industry leading software innovation. Empirical software
engineering sustains a better understanding of these projects, reducing even more
the barriers for adoption. In this work we focus on empirical quality assessment developing
software metrics artifacts to make web components quality measurable. After reviewing
the state of the art and main frameworks for software measurement, we will present
our proposal for the empirical evaluation of quality metrics for web components, data
collection, measurement and prediction, discussing main benefits and some drawback
of the selected approach, which will be aimed at future works.},
booktitle = {Proceedings of the 14th International Workshop on Automation of Software Test},
pages = {1–6},
numpages = {6},
keywords = {web technologies, open source, quality metrics, software engineering},
location = {Montreal, Quebec, Canada},
series = {AST '19}
}

@inproceedings{10.5555/3291291.3291304,
author = {Silva, Gabriel Costa and R\'{e}, Reginaldo and Silva, Marco Aur\'{e}lio Graciotto},
title = {Evaluating Efficiency, Effectiveness and Satisfaction of AWS and Azure from the Perspective of Cloud Beginners},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Quality has long been regarded as an important driver of cloud adoption. In particular,
quality in use (QiU) of cloud platforms may drive cloud beginners to the cloud platform
that offers the best cloud experience. Cloud beginners are critical to the cloud market
because they currently represent nearly a third of cloud users. We carried out three
experiments to measure the QiU (dependent variable) of public cloud platforms (independent
variable) regarding efficiency, effectiveness and satisfaction. AWS EC2 and Azure
Virtual Machines are the two cloud services used as representative proxies to evaluate
cloud platforms (treatments). Eleven undergraduate students with limited cloud knowledge
(participants) manually created 152 VMs (task) using the web interface of cloud platforms
(instrument) following seven different configurations (trials) for each cloud platform.
Whereas AWS performed significantly better than Azure for efficiency (p-value not
exceeding 0.001, A-statistic = 0.68), we could not find a significant difference between
platforms for effectiveness (p-value exceeding 0.05) - although the effect size was
found relevant (odds ratio = 0.41). Regarding satisfaction, most of our participants
perceived the AWS as (i) having the best GUI to benefiting user interaction, (ii)
the easiest platform to use, and (iii) the preferred cloud platform for creating VMs.
Once confirmed by independent replications, our results suggest that AWS outperforms
Azure regarding QiU. Therefore, cloud beginners might have a better cloud experience
starting off their cloud projects by using AWS rather than Azure. In addition, our
results may help to explain the AWS's cloud leadership.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {114–125},
numpages = {12},
keywords = {experimentation, cloud platforms, quality in use},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/2790060.2790071,
author = {Legrand, H\'{e}l\`{e}ne and Boubekeur, Tamy},
title = {Morton Integrals for High Speed Geometry Simplification},
year = {2015},
isbn = {9781450337076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790060.2790071},
doi = {10.1145/2790060.2790071},
abstract = {Real time geometry processing has progressively reached a performance level that makes
a number of signal-inspired primitives practical for on-line applications scenarios.
This often comes through the joint design of operators, data structure and even dedicated
hardware. Among the major classes of geometric operators, filtering and super-sampling
(via tessellation) have been successfully expressed under high-performance constraints.
The subsampling operator i.e., adaptive simplification, remains however a challenging
case for non-trivial input models. In this paper, we build a fast geometry simplification
algorithm over a new concept: Morton Integrals. By summing up quadric error metric
matrices along Morton-ordered surface samples, we can extract concurrently the nodes
of an adaptive cut in the so-defined implicit hierarchy, and optimize all simplified
vertices in parallel. This approach is inspired by integral images and exploits recent
advances in high performance spatial hierarchy construction and traversal. As a result,
our GPU implementation can downsample a mesh made of several millions of polygons
at interactive rates, while providing better quality than uniform simplification and
preserving important salient features. We present results for surface meshes, polygon
soups and point clouds, and discuss variations of our approach to account for per-sample
attributes and alternatives error metrics.},
booktitle = {Proceedings of the 7th Conference on High-Performance Graphics},
pages = {105–112},
numpages = {8},
keywords = {Morton code, GPU algorithms, mesh simplification, adaptive clustering},
location = {Los Angeles, California},
series = {HPG '15}
}

@article{10.1109/TNET.2018.2851379,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet},
title = {Video Streaming in Distributed Erasure-Coded Storage Systems: Stall Duration Analysis},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2851379},
doi = {10.1109/TNET.2018.2851379},
abstract = {The demand for global video has been burgeoning across industries. With the expansion
and improvement of video-streaming services, cloud-based video is evolving into a
necessary feature of any successful business for reaching internal and external audiences.
This paper considers video streaming over distributed systems where the video segments
are encoded using an erasure code for better reliability, thus being the first work
to our best knowledge that considers video streaming over erasure-coded distributed
cloud systems. The download time of each coded chunk of each video segment is characterized,
and the ordered statistics over the choice of the erasure-coded chunks is used to
obtain the playback time of different video segments. Using the playback times, bounds
on the moment generating function on the stall duration are used to bound the mean
stall duration. Moment generating function-based bounds on the ordered statistics
are also used to bound the stall duration tail probability, which determines the probability
that the stall time is greater than a pre-defined number. These two metrics, mean
stall duration and the stall duration tail probability, are important quality of experience
QoE measures for the end users. Based on these metrics, we formulate an optimization
problem to jointly minimize the convex combination of both the QoE metrics averaged
over all requests over the placement and access of the video content. The non-convex
problem is solved using an efficient iterative algorithm. Numerical results show a
significant improvement in QoE metrics for cloud-based video compared to the considered
baselines.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {1921–1932},
numpages = {12}
}

@inproceedings{10.1145/3240765.3240798,
author = {Yen, Chih-Hsuan and Chen, Wei-Ming and Hsiu, Pi-Cheng and Kuo, Tei-Wei},
title = {Differentiated Handling of Physical Scenes and Virtual Objects for Mobile Augmented Reality},
year = {2018},
isbn = {9781450359504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240765.3240798},
doi = {10.1145/3240765.3240798},
abstract = {Mobile devices running augmented reality applications consume considerable energy
for graphics-intensive workloads. This paper presents a scheme for the differentiated
handling of camera-captured physical scenes and computer-generated virtual objects
according to different perceptual quality metrics. We propose online algorithms and
their realtime implementations to reduce energy consumption through dynamic frame
rate adaptation while maintaining the visual quality required for augmented reality
applications. To evaluate system efficacy, we integrate our scheme into Android and
conduct extensive experiments on a commercial smartphone with various application
scenarios. The results show that the proposed scheme can achieve energy savings of
up to 39.1% in comparison to the native graphics system in Android while maintaining
satisfactory visual quality.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
articleno = {36},
numpages = {8},
keywords = {energy savings, mobile systems, augmented reality, visual quality, frame frame adaptation},
location = {San Diego, California},
series = {ICCAD '18}
}

@inproceedings{10.1145/3307630.3342385,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {HADAS: Analysing Quality Attributes of Software Configurations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342385},
doi = {10.1145/3307630.3342385},
abstract = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses
of SPLs rely on solvers to navigate complex dependencies among features and find legal
solutions. Variability analysis tools are complex due to the diversity of products
and domain-specific knowledge. On that, while there are experimental studies that
analyse quality attributes, the knowledge is not easily accessible for developers,
and its appliance is not trivial. Aiming to allow the industry to quality-explore
SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects
quality attributes metrics in a cloud repository, and (2) reasons about it helping
developers with quality attributes requirements.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {variability, numerical, attribute, software product line, model, NFQA},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2649563.2649571,
author = {Weber, Andreas and Herbst, Nikolas and Groenda, Henning and Kounev, Samuel},
title = {Towards a Resource Elasticity Benchmark for Cloud Environments},
year = {2014},
isbn = {9781450330596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2649563.2649571},
doi = {10.1145/2649563.2649571},
abstract = {Auto-scaling features offered by today's cloud infrastructures provide increased flexibility
especially for customers that experience high variations in the load intensity over
time. However, auto-scaling features introduce new system quality attributes when
considering their accuracy, timing, and boundaries. Therefore, distinguishing between
different offerings has become a complex task, as it is not yet supported by reliable
metrics and measurement approaches. In this paper, we discuss shortcomings of existing
approaches for measuring and evaluating elastic behavior and propose a novel benchmark
methodology specifically designed for evaluating the elasticity aspects of modern
cloud platforms. The benchmark is based on open workloads with realistic load variation
profiles that are calibrated to induce identical resource demand variations independent
of the underlying hardware performance. Furthermore, we propose new metrics that capture
the accuracy of resource allocations and de-allocations, as well as the timing aspects
of an auto-scaling mechanism explicitly.},
booktitle = {Proceedings of the 2nd International Workshop on Hot Topics in Cloud Service Scalability},
articleno = {5},
numpages = {8},
keywords = {Resource, Elasticity, Supply, Demand, Load Profile},
location = {Dublin, Ireland},
series = {HotTopiCS '14}
}

@inproceedings{10.1145/3290688.3290705,
author = {Tesfamicael, Aklilu Daniel and Liu, Vicky and Foo, Ernest and Caelli, Bill},
title = {QoE Estimation Model for a Secure Real-Time Voice Communication System in the Cloud},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290705},
doi = {10.1145/3290688.3290705},
abstract = {As moving towards cloud-based real-time services, we are witnessing the shift from
a technology-driven services to service provisioning paradigms, that is, from Quality
of Service (QoS) to Quality of Experience (QoE). User experience and satisfaction
are placed at the epicenter of the system design. QoE is a measurement of user experience
on the provided service by a system. Often QoE is measured by subjective mechanisms,
such as user experience surveys and mean opinion scores (MOS) methods, which can be
a costly and time-consuming process. Using an adequate QoE model to measure user experience
of perceived quality is cost-effective, compared to using time-consuming subjective
surveys. Applying an adequate QoE model to assess user experience is advantageous
for cloud-based real-time services such as voice and video. This study uses a formula-based
QoE estimation model to estimate and predict QoE prior to the deployment or during
the planning stage of the system service. This study investigates a real-world scenario
of a company that recently moved to its premises-based real-time trading communication
system (TCS) to a public cloud. A simulation system using OPNET is also implemented
to illustrate the usefulness of the model. Our result shows that the effect of delay
on the users experience of the service provided by the cloud-based TCS is minimum
comparing to packet loss rate (PLR) and Jitter. However, it has been observed that
the overhead of the different security settings of the TCS system had no major negative
impact to the user experience. The proposed model can be used as a QoE control mechanism
and network optimization for cloud-based TCS services.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {10},
numpages = {9},
keywords = {QoS, TCS, Real-time, VoIP, QoE, E-Model},
location = {Sydney, NSW, Australia},
series = {ACSW 2019}
}

@inproceedings{10.1109/UCC.2014.87,
author = {Ali-Eldin, Ahmed and Seleznjev, Oleg and Sj\"{o}stedt-de Luna, Sara and Tordsson, Johan and Elmroth, Erik},
title = {Measuring Cloud Workload Burstiness},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.87},
doi = {10.1109/UCC.2014.87},
abstract = {Workload burstiness and spikes are among the main reasons for service disruptions
and decrease in the Quality-of-Service (QoS) of online services. They are hurdles
that complicate autonomic resource management of data enters. In this paper, we review
the state-of-the-art in online identification of workload spikes and quantifying burstiness.
The applicability of some of the proposed techniques is examined for Cloud systems
where various workloads are co-hosted on the same platform. We discuss Sample Entropy
(Samp En), a measure used in biomedical signal analysis, as a potential measure for
burstiness. A modification to the original measure is introduced to make it more suitable
for Cloud workloads.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {566–572},
numpages = {7},
series = {UCC '14}
}

@inproceedings{10.1145/2884781.2884814,
author = {Su, Guoxin and Rosenblum, David S. and Tamburrelli, Giordano},
title = {Reliability of Run-Time Quality-of-Service Evaluation Using Parametric Model Checking},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884814},
doi = {10.1145/2884781.2884814},
abstract = {Run-time Quality-of-Service (QoS) assurance is crucial for business-critical systems.
Complex behavioral performance metrics (PMs) are useful but often difficult to monitor
or measure. Probabilistic model checking, especially parametric model checking, can
support the computation of aggregate functions for a broad range of those PMs. In
practice, those PMs may be defined with parameters determined by run-time data. In
this paper, we address the reliability of QoS evaluation using parametric model checking.
Due to the imprecision with the instantiation of parameters, an evaluation outcome
may mislead the judgment about requirement violations. Based on a general assumption
of run-time data distribution, we present a novel framework that contains light-weight
statistical inference methods to analyze the reliability of a parametric model checking
output with respect to an intuitive criterion. We also present case studies in which
we test the stability and accuracy of our inference methods and describe an application
of our framework to a cloud server management problem.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {73–84},
numpages = {12},
keywords = {run-time evaluation, data distribution, probabilistic model checking, reliability, Quality-of-Service},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/2602339.2602375,
author = {Zheng, Yixin and Li, Linglong and Zhang, Lin},
title = {Poster Abstract: PiMi Air Community: Getting Fresher Indoor Air by Sharing Data and Know-Hows},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {PiMiair.org is a participatory indoor air quality data sharing project we launched
in January 2014. Over 200 PiMi air boxes, a low-cost indoor air quality monitor, were
given out to volunteer users across China. The PiMi air boxes measure the approximate
indoor particulate matter concentration, and the ambient temperate and humidity. When
a user accesses the PiMi air box for his personal air quality data on his smartphone,
the data is relayed to the backend PiMi cloud server for analysis. Accumulating large
amount of indoor air quality data under different circumstances, the PiMi cloud server
is able to use statistical learning methodologies to detect point of interests (POIs)
in the data series, and asks users to label their activities or events at the POIs.
Together with the user-reported physicality information on the indoor environments,
PiMiair.org is able to quantitatively evaluate the impacts of the environment physicality
and human behaviors on the indoor air quality, and mine the knowledges on how to alleviate
indoor air pollution. We believe that by sharing these knowledge among the community,
healthier breathing environments could be nurtured for the well-being of the public.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {283–284},
numpages = {2},
keywords = {human factors, indoor air quality, participatory sensing},
location = {Berlin, Germany},
series = {IPSN '14}
}

@article{10.1145/2853073.2853085,
author = {Baliyan, Niyati and Kumar, Sandeep},
title = {A Hierarchical Fuzzy System for Quality Assessment of Semantic Web Application as a Service},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853085},
doi = {10.1145/2853073.2853085},
abstract = {Semantic Web enabled applications are becoming popular due to the presence of their
machine comprehensible description, which makes them easily sharable across machines.
If such applications are deployed as services to the user through the Cloud, they
can facilitate transparency and reusability. There exist no attributes, metrics, or
models for monitoring the quality of such applications. In the current work, a hierarchical
fuzzy system for quality assessment of Semantic Web based applications delivered as
services on the Cloud, is proposed. The quality attributes proposed herein have been
validated through the standard IEEE-1061 validation framework. Experimental results
reveal that the proposed hierarchical fuzzy system handles the multiplicity of quality
attributes, and can be used for the relative ranking of Semantic Web applications
available as services},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–7},
numpages = {7},
keywords = {Quality Metrics, Fuzzy Logic, Cloud, Semantic Web}
}

@inproceedings{10.5555/2755535.2755538,
author = {Claypool, Mark and Finkel, David},
title = {The Effects of Latency on Player Performance in Cloud-Based Games},
year = {2014},
publisher = {IEEE Press},
abstract = {Cloud-based games are an increasingly popular method to distribute and play computer
games on the Internet. While there has been some work studying network aspects of
cloud-based games and examining the effects of latency on traditional games, there
has not been sufficient research on the impact of latency on cloud-based games nor
a comparison of the impact of latency on cloud-based games versus traditional games.
This paper presents the results of two user studies that measure the objective and
subjective effects of latency on cloud-based games, one study using the commercial
cloud game system OnLive and the other study using the academic cloud game system
GamingAnywhere. Analysis of the results shows both quality of experience and user
performance degrade linearly with an increase in latency. More significantly, latency
affects cloud-based games in a manner most similar to that of traditional first-person
avatar games, the most sensitive class of games, despite the fact that the cloud-based
games may have a different user perspective. These results have implications for cloud-based
game designers and cloud system developers.},
booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
articleno = {2},
numpages = {6},
location = {Nagoya, Japan},
series = {NetGames '14}
}

@inproceedings{10.1145/3036290.3036321,
author = {L\'{o}pez, Cindy and Heinsen, Rene and Huh, Eui-Nam},
title = {Improving Availability Applying Intelligent Replication in Federated Cloud Storage Based on Log Analysis},
year = {2017},
isbn = {9781450348287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3036290.3036321},
doi = {10.1145/3036290.3036321},
abstract = {This study is focusing on improving the availability of federated storage services
in order to provide better quality-of-service (QoS) to the customer with the minimum
use of resources. One of the most efficient solutions to get the best experience in
the cloud is to combine the services offered. In order for this to happen, there exist
different approaches for selecting the best subset of services to reach the optimal
performance. However, those works focus on one time selection processes, despite of
customer's requirements are continuously changing and demanding adaptable storage
service. In this research, I propose a method to improve storage availability through
log sentiment analysis and intelligent replication. This methodology is based on the
merging of two types of log analysis and the measurement of availability and performance
metrics in order to select the best subset of services in cloud storage service federation.},
booktitle = {Proceedings of the 2017 International Conference on Machine Learning and Soft Computing},
pages = {148–153},
numpages = {6},
keywords = {availability, replication, subset selection, log analysis, Federated Cloud Storage, performance, cloud computing, sentiment analysis},
location = {Ho Chi Minh City, Vietnam},
series = {ICMLSC '17}
}

@inproceedings{10.1145/3097895.3097900,
author = {Zhang, Wenxiao and Han, Bo and Hui, Pan},
title = {On the Networking Challenges of Mobile Augmented Reality},
year = {2017},
isbn = {9781450350556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097895.3097900},
doi = {10.1145/3097895.3097900},
abstract = {In this paper, we conduct a reality check for Augmented Reality (AR) on mobile devices.
We dissect and measure the cloud-offloading feature for computation-intensive visual
tasks of two popular commercial AR systems. Our key finding is that their cloud-based
recognition is still not mature and not optimized for latency, data usage and energy
consumption. In order to identify the opportunities for further improving the Quality
of Experience (QoE) for mobile AR, we break down the end-to-end latency of the pipeline
for typical cloud-based mobile AR and pinpoint the dominating components in the critical
path.},
booktitle = {Proceedings of the Workshop on Virtual Reality and Augmented Reality Network},
pages = {24–29},
numpages = {6},
keywords = {networking challenges, end-to-end latency, Augmented reality, cloud offloading, reality check},
location = {Los Angeles, CA, USA},
series = {VR/AR Network '17}
}

@article{10.1145/3047646,
author = {Chen, Qi and Liu, Ye and Liu, Guangchi and Yang, Qing and Shi, Xianming and Gao, Hongwei and Su, Lu and Li, Quanlong},
title = {Harvest Energy from the Water: A Self-Sustained Wireless Water Quality Sensing System},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3047646},
doi = {10.1145/3047646},
abstract = {Water quality data is incredibly important and valuable, but its acquisition is not
always trivial. A promising solution is to distribute a wireless sensor network in
water to measure and collect the data; however, a drawback exists in that the batteries
of the system must be replaced or recharged after being exhausted. To mitigate this
issue, we designed a self-sustained water quality sensing system that is powered by
renewable bioenergy generated from microbial fuel cells (MFCs). MFCs collect the energy
released from native magnesium oxidizing microorganisms (MOMs) that are abundant in
natural waters. The proposed energy-harvesting technology is environmentally friendly
and can provide maintenance-free power to sensors for several years. Despite these
benefits, an MFC can only provide microwatt-level power that is not sufficient to
continuously power a sensor. To address this issue, we designed a power management
module to accumulate energy when the input voltage is as low as 0.33V. We also proposed
a radio-frequency (RF) activation technique to remotely activate sensors that otherwise
are switched off in default. With this innovative technique, a sensor’s energy consumption
in sleep mode can be completely avoided. Additionally, this design can enable on-demand
data acquisitions from sensors. We implement the proposed system and evaluate its
performance in a stream. In 3-month field experiments, we find the system is able
to reliably collect water quality data and is robust to environment changes.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {3},
numpages = {24},
keywords = {Energy harvesting, power management, microbial fuel cell, water quality monitoring, radio-frequency (RF) activation}
}

@inproceedings{10.1145/3373724.3373726,
author = {Chen, Wenyu and Xiong, Wei and Cheng, Jierong and Li, Yusha},
title = {Automatic Dimensional Measurement Using Datums Generated from Point Clouds},
year = {2019},
isbn = {9781450372350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373724.3373726},
doi = {10.1145/3373724.3373726},
abstract = {Dimensional measurement is critical for quality control. Manual dimensional measurement
using standard gauges can only be applied on a few datums. To measure a huge number
of datums, a component needs to be scanned into a point cloud and measured digitally.
For precision components, datum generation on the scanned point cloud is labor-intensive.
Given a raw point cloud from scanner, this paper proposes an automatic dimensional
measurement solution with an adaptive local registration algorithm and an adaptive
datum generation algorithm. Using datums on the CAD model as reference, the adaptive
local registration algorithm selects local regions on the scanned model to compensate
the local deviation between the CAD model and the scanned model. After that, with
outliers and noises in the raw data, the adaptive datum generation algorithm creates
the correct datums on the scanned model adaptive to the actual geometry. Dimensional
measurement based on the generated datums can be achieved automatically. Moreover,
the solution does not require users to manually preprocess the point cloud, such as
outlier and noise removal. As such, it improves the productivity in dimensional inspection.},
booktitle = {Proceedings of the 2019 5th International Conference on Robotics and Artificial Intelligence},
pages = {59–63},
numpages = {5},
keywords = {Inspection, Datum generation, Dimensional measurement},
location = {Singapore, Singapore},
series = {ICRAI '19}
}

@inproceedings{10.5555/2755535.2755555,
author = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Xiao, Yu and Yl\"{a}-J\"{a}\"{a}ski, Antti},
title = {Towards Pervasive and Mobile Gaming with Distributed Cloud Infrastructure},
year = {2014},
publisher = {IEEE Press},
abstract = {Cloud gaming, where the game is rendered in the cloud and is streamed to an end-user
device through a thin client, is rapidly gaining ground. Latency is still a key challenge
to cloud gaming: highly interactive games can become unplayable even with response
delays below 100 ms. To overcome this issue, we propose to deploy gaming services
on a more distributed cloud infrastructure, and to instantiate gaming servers in close
proximity of the user when necessary in order to shorten the response delay. Our prototype
distributed cloud gaming platform also allows flexible configuration of gaming controls
and video streams, enabling the use of public displays in mobile cloud gaming. We
test our prototype with two games in different deployment scenarios, and measure the
response delay and power consumption of the mobile devices. Our experiment results
confirm that it is feasible to improve the quality of gaming experience through the
deployment strategies provided by the proposed system.},
booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
articleno = {16},
numpages = {6},
location = {Nagoya, Japan},
series = {NetGames '14}
}

@inproceedings{10.1145/2996890.2996906,
author = {Chhetri, Mohan Baruwal and Vo, Quoc Bao and Kowalczyk, Ryszard},
title = {CL-SLAM: Cross-Layer SLA Monitoring Framework for Cloud Service-Based Applications},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.2996906},
doi = {10.1145/2996890.2996906},
abstract = {Modern applications are increasingly being composed from multiple components that
require and consume services at different layers of the cloud stack. The diverse,
dynamic and unpredictable nature of both cloud services and application workloads
makes quality-assured provision of such cloud service-based applications (CSBAs) a
major challenge. While elasticity and autoscaling gives CSBA providers the ability
to scale cloud resources on-demand, they require a comprehensive, system-wide view
of the application performance in order to make timely, cost-effective and performance-efficient
scaling decisions. In this paper, we propose, develop and validate CL-SLAM - a Cross-Layer
SLA Monitoring Framework for CSBAs. Its main features include (a) realtime, fine-grained
visibility into CSBA performance, (b) visual descriptive analytics to identify correlations
and inter-dependencies between cross-layer performance metrics, (c) temporal profiling
of CSBA performance, (d) proactive monitoring, detection and root-cause analysis of
SLA violation, and (e) support for both reactive and proactive adaptation in support
of quality-assured CSBA provision. We validate our approach through a prototype implementation.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {30–36},
numpages = {7},
keywords = {cross-layer SLA monitoring, cloud service-based application},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/2739482.2764720,
author = {Oprescu, Ana-Maria and (Vintila) Filip, Alexandra and Kielmann, Thilo},
title = {Fast Pareto Front Approximation for Cloud Instance Pool Optimization},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764720},
doi = {10.1145/2739482.2764720},
abstract = {Computing the Pareto Set (PS) of optimal cloud schedules in terms of cost and makespan
for a given application and set of cloud instance types is NP-complete. Moreover,
cloud instances' volatility requires fast PS recomputations. While genetic algorithms
(GA) are a promising approach, little knowledge of an approximated PS's quality leads
to GAs running for overly many generations, contradicting the goal of quickly computing
an approximate solution. We address this with MOO-GA, our GA enhanced with a domain-tailored
termination criteria delivering fast, well-approximated Pareto sets. We compare to
NSGAIII using PS convergence and diversity, and computational effort metrics. Results
show MOO-GA consistently computing better quality Pareto sets within one second on
average (df=98, p-value&lt;10-3).},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1443–1444},
numpages = {2},
keywords = {pareto frontier, genetic algorithms, infrastructure-as-a-service},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3423328.3423497,
author = {Li, Yen-Chun and Hsu, Chia-Hsin and Lin, Yu-Chun and Hsu, Cheng-Hsin},
title = {Performance Measurements on a Cloud VR Gaming Platform},
year = {2020},
isbn = {9781450381581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423328.3423497},
doi = {10.1145/3423328.3423497},
abstract = {As cloud gaming and Virtual Reality (VR) games become popular in the game industry,
game developers engage in these fields to boost their sales. Because cloud gaming
possesses the merit of lifting computation loads from client devices to servers, it
solves the high resource consumption issue of VR games on regular clients. However,
it is important to know where is the bottleneck of the cloud VR gaming platform and
how can it be improved in the future. In this paper, we conduct extensive experiments
on the state-of-the-art cloud VR gaming platform--Air Light VR (ALVR). In particular,
we analyze the performance of ALVR using both Quality-of-Service and Quality-of-Experience
metrics. Our experiments reveal that latency (up to 90 ms RTT) has less influence
on user experience compared to bandwidth limitation (as small as 35 Mbps) and packet
loss rate (as high as 8%) . Moreover, we find that VR gamers can hardly notice the
difference between the gaming experience with different latency values (between 0
and 90 ms RTT). Such findings shed some lights on how to further improve the cloud
VR gaming platform, e.g., a budget of up to 90 ms RTT may be used to absorb network
dynamics when bandwidth is insufficient.},
booktitle = {Proceedings of the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications},
pages = {37–45},
numpages = {9},
keywords = {measurement, prototype, computer games, cloud computing, virtual reality},
location = {Seattle, WA, USA},
series = {QoEVMA'20}
}

@inproceedings{10.1109/CCGrid.2014.103,
author = {Wu, Jie and Jansen, Christoph and Beier, Maximilian and Witt, Michael and Krefting, Dagmar},
title = {Extending XNAT towards a Cloud-Based Quality Assessment Platform for Retinal Optical Coherence Tomographies},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.103},
doi = {10.1109/CCGrid.2014.103},
abstract = {Neurosciencific research is increasingly based on image analysis methods. Large sets
of imaging data are processed using complex image analysis tools. While today magnetic
resonance imaging (MRI) is widely used for both functional and anatomical analysis
of the human brain, new imaging modalities are beginning to prove their capabilities
for neurological research. Among them, optical coherence tomography (OCT) allows for
noninvasive visualization of anatomical structures on a micrometer scale. Becoming
a standard diagnostic tool in ophthalmology, it is of rising interest for neurological
research. Crucial to all data analysis methods is the quality of the input data. The
platform presented in this paper is designed for automatic quality assessment of retinal
OCTs. It extends the image management platform XNAT by services to calculate and store
quality measures. It is also extensible regarding new quality measure algorithms,
allowing the developer to upload Matlab code, compile it for the infrastructure's
hardware architecture and test it in the system. The image processing tools to calculate
the quality measures are provided as a cloud-based service employing OpenStack as
underlying IT infrastructure. The prototype implementation encompassing security and
performance aspects are presented.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {764–773},
numpages = {10},
keywords = {medical imaging, cloud, neuroimaging, XNAT, SaaS, OCT, IaaS},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3328886.3328892,
author = {Alcivar, Nayeth I. Solorzano and Gallego, Diego Carrera and Quijije, Lissenia Sornoza and Quelal, Marco Mendoza},
title = {Developing a Dashboard for Monitoring Usability of Educational Games Apps for Children},
year = {2019},
isbn = {9781450361682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328886.3328892},
doi = {10.1145/3328886.3328892},
abstract = {Nowadays digital game applications or interactive children's educational games implemented
in mobile devices (to be identified as Apps), are beginning to be widely used to complement
children's education, particularly during early childhood education. However, digital
game Apps do not generate a timely collection of data that could be obtained, so that
with a proper interpretation they can serve as a guide in making decisions about the
content, types, and level of games that should be created as digital tools to support
children's education. In this article, is indicated how through the development of
a dashboard, linked to a database in the cloud, it is possible to obtain and present
information that allows measuring the use and playability and usability factors for
these types of Apps, in an orderly and precise manner. For the development of the
dashboard and its link in real time with the Apps to monitor, JavaScript was used
through the framework Sails.js and the database implemented in PostgreSQL. In parallel,
for the data transmission tests, two mobile applications were implemented in Android,
one programmed in Unity and the second using Adobe Animate. Both Apps were designed
by recording internal data in JSON file format. To analyze and obtain results, we
used PQM metrics 2014 (Playability Quality Model), and we applied an adapted theory
which helps to facilitate the identification of factors affecting the use and adoption
of information systems and technologies in Latin American local contexts. The Pilot
tests were carried out with children from 4 to 8 years attending schools of marginal
areas in the city of Guayaquil, Ecuador. These children with little knowledge of technology
use, facilitate better evaluation of different scenarios to measure the behavioral
use of the Apps and their contents without significant influence of previous knowledge
about digital educational games. This article presents the first results of an extensive
and longitudinal multidisciplinary research, relevant to organizations and people
involved in early childhood education.},
booktitle = {Proceedings of the 2019 2nd International Conference on Computers in Management and Business},
pages = {70–75},
numpages = {6},
keywords = {Apps, Dashboard, Latin America, Usability, Technology Adoption and Education, Ecuador, Children, Digital Games, MIDI},
location = {Cambridge, United Kingdom},
series = {ICCMB 2019}
}

@inproceedings{10.1145/2964284.2973806,
author = {Mekuria, Rufael and Cesar, Pablo},
title = {MP3DG-PCC, Open Source Software Framework for Implementation and Evaluation of Point Cloud Compression},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973806},
doi = {10.1145/2964284.2973806},
abstract = {We present MP3DG-PCC, an open source framework for design, implementation and evaluation
of point cloud compression algorithms. The framework includes objective quality metrics,
lossy and lossless anchor codecs, and a test bench for consistent comparative evaluation.
The framework and proposed methodology is in use for the development of an international
point cloud compression standard in MPEG. In addition, the library is integrated with
the popular point cloud library, making a large number of point cloud processing available
and aligning the work with the broader open source community.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1222–1226},
numpages = {5},
keywords = {point cloud compression, evaluation, compression, 3d virtual reality},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.1145/3388440.3414205,
author = {Vijayan, Vipin and Gu, Shawn and Krebs, Eric T. and Meng, Lei and Milenkovi\'{c}, Tijana},
title = {Pairwise Versus Multiple Global Network Alignment},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3414205},
doi = {10.1145/3388440.3414205},
abstract = {This abstract is based on the following paper: Vijayan, Vipin, Shawn Gu, Eric T. Krebs,
Lei Meng, and Tijana Milenkovi\'{c}. "Pairwise Versus Multiple Global Network Alignment."
IEEE Access 8 (2020): 41961--41974.Proteins, the major macromolecules of life, interact
with each other to carry out cellular functioning. Thus, analyses of protein-protein
interaction (PPI) networks can yield important insights into biological function,
disease, and evolution. While biotechnological advancements have made PPI network
data available for many species, functions of many proteins in many of these species
remain unknown. One way to uncover these functions is to transfer biological knowledge
from a well-studied species to a poorly-studied one. Genomic sequence alignment, which
has revolutionized our biomedical understanding, can be used for this purpose. However,
sequence alignment has a major drawback: it does not consider interactions between
proteins (which are ultimately what carry out function). So, biological network alignment
(NA) can be used in a complementary fashion to predict protein functional knowledge
that sequence alignment alone cannot predict. Specifically, NA compares PPI networks
of different species to find regions of their similarity (or conservation), thus allowing
for the transfer of functional knowledge across conserved network (rather than just
sequence) regions.Like genomic sequence alignment, NA can be local or global. Just
as the recent trend in the NA field, we also focus on global NA, which can be pairwise
(PNA) and multiple (MNA). While PNA aligns two networks, MNA can align more than two
networks at once. Since MNA can capture conserved network regions between more networks
than PNA, it is hypothesized that MNA leads to deeper biological insights compared
to PNA. However, due to different outputs of PNA and MNA, a PNA method is only compared
to other PNA methods, and an MNA method is only compared to other MNA methods. Comparison
of PNA against MNA must be done to evaluate whether MNA indeed yields more biologically
meaningful alignments than PNA, as only this would justify MNA's higher computational
complexity.We introduce a framework that allows for this. We evaluate eight prominent
PNA and MNA methods, on synthetic and real-world biological networks, using topological
and functional alignment quality measures. We compare PNA against MNA in both a pairwise
(native to PNA) and multiple (native to MNA) manner. PNA is expected to lead to higher-quality
alignments than MNA under the pairwise evaluation framework. Indeed, this is what
we find. MNA is expected to lead to higher-quality alignments than PNA under the multiple
evaluation framework. Shockingly, we find this not always to hold; PNA is often better
than MNA in this framework, depending on the choice of evaluation test. Thus, we believe
that any new MNA methods should be compared not just to existing MNA methods, but
also to existing PNA methods using our evaluation framework, to properly judge the
quality of alignments that they produce. Also, we confirm empirically that PNA is
faster than MNA in both evaluation frameworks. These results indicate that currently,
MNA offers little advantage over PNA; in order for MNA to gain an advantage, a drastic
redesign of MNA's current algorithmic principles might be needed.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {4},
numpages = {1},
keywords = {multi-network comparison, protein function prediction, biological network alignment},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/2940136.2940143,
author = {Wamser, Florian and Seufert, Michael and H\"{o}fner, Steffen and Tran-Gia, Phuoc},
title = {Concept for Client-Initiated Selection of Cloud Instances for Improving QoE of Distributed Cloud Services},
year = {2016},
isbn = {9781450344258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940136.2940143},
doi = {10.1145/2940136.2940143},
abstract = {We introduce a concept for client-initiated selection of service location and service
quality for improving the Quality of Experience (QoE) of general cloud services. It
is loosely based on the HTTP adaptive streaming approach (e.g., MPEG DASH). A manifest
file compiled by the cloud service provider specifies the available service locations
and qualities, from which the user selects the optimal service instance based on contextual
information obtained from client measurements and user preferences. The proposed concept
is defined and is implemented in two client-based decision algorithms for improving
the QoE of a simple picture gallery cloud service. These decision algorithms are evaluated
and their impact on the service delivery is discussed. The evaluation shows that it
is possible to improve the service location and quality selection by light-weight
client-based algorithms.},
booktitle = {Proceedings of the 2016 Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {49–54},
numpages = {6},
keywords = {Quality of Experience, Cloud Services, Client-based Access for Cloud Services},
location = {Florianopolis, Brazil},
series = {Internet-QoE '16}
}

@inproceedings{10.1145/2578260.2578273,
author = {Wang, Cong and Zink, Michael},
title = {On the Feasibility of DASH Streaming in the Cloud},
year = {2014},
isbn = {9781450327060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578260.2578273},
doi = {10.1145/2578260.2578273},
abstract = {As shown in recent studies, video streaming is by far the biggest category of backbone
Internet traffic in the US. As a measure to reduce the cost of highly over-provisioned
physical infrastructures while remaining the quality of video services, many streaming
service providers started to use cloud services where physical resources can be dynamically
allocated based on current demand. This paper characterizes the performance of Dynamic
Adaptive Streaming over HTTP (DASH), a new MPEG standard on adaptive streaming, in
the cloud. We seek to answer the following questions that are critical to content
providers that are hosting video in clouds: Which data center is the best to host
videos? Does geographical distance matter? What type of instance is best suitable
depending on different needs? How to efficiently solve the trade-off between performance
and cost? The measurement methods and results presented in this paper can be easily
expanded into other VoD services, and they allow us to i) characterize DASH behavior
when streaming from the cloud; ii) identify the key factors that influence the DASH
performance; and iii) suggest improvements for related services.},
booktitle = {Proceedings of Network and Operating System Support on Digital Audio and Video Workshop},
pages = {49–54},
numpages = {6},
keywords = {HTTP adaptive streaming, quality of experience, video-on-demand, Cloud computing},
location = {Singapore, Singapore},
series = {NOSSDAV '14}
}

@inproceedings{10.1145/2822332.2822339,
author = {Evans, Kieran and Jones, Andrew and Preece, Alun and Quevedo, Francisco and Rogers, David and Spasi\'{c}, Irena and Taylor, Ian and Stankovski, Vlado and Taherizadeh, Salman and Trnkoczy, Jernej and Suciu, George and Suciu, Victor and Martin, Paul and Wang, Junchao and Zhao, Zhiming},
title = {Dynamically Reconfigurable Workflows for Time-Critical Applications},
year = {2015},
isbn = {9781450339896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2822332.2822339},
doi = {10.1145/2822332.2822339},
abstract = {Cloud-based applications that depend on time-critical data processing or network throughput
require the capability of reconfiguring their infrastructure on demand as and when
conditions change. Although the ability to apply quality of service constraints on
the current Cloud offering is limited, there are ongoing efforts to change this. One
such effort is the European funded SWITCH project that aims to provide a programming
model and toolkit to help programmers specify quality of service and quality of experience
metrics of their distributed application and to provide the means to specify the reconfiguration
actions which can be taken to maintain these requirements. In this paper, we present
an approach to application reconfiguration by applying a workflow methodology to implement
a prototype involving multiple reconfiguration scenarios of a distributed real-time
social media analysis application, called Sentinel. We show that by using a lightweight
RPC-based workflow approach, we can monitor a live application in real time and spawn
dependency-based workflows to reconfigure the underlying Docker containers that implement
the distributed components of the application. We propose to use this prototype as
the basis for part of the SWITCH workbench, which will support more advanced programmable
infrastructures.},
booktitle = {Proceedings of the 10th Workshop on Workflows in Support of Large-Scale Science},
articleno = {7},
numpages = {10},
keywords = {quality of service, workflows, time-critical applications, quality of experience, dynamic data driven systems},
location = {Austin, Texas},
series = {WORKS '15}
}

@inproceedings{10.1145/3308560.3317075,
author = {Debattista, Jeremy and Attard, Judie and Brennan, Rob and O'Sullivan, Declan},
title = {Is the LOD Cloud at Risk of Becoming a Museum for Datasets? Looking Ahead towards a Fully Collaborative and Sustainable LOD Cloud},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317075},
doi = {10.1145/3308560.3317075},
abstract = {The Linked Open Data (LOD) cloud has been around since 2007. Throughout the years,
this prominent depiction served as the epitome for Linked Data and acted as a starting
point for many. In this article we perform a number of experiments on the dataset
metadata provided by the LOD cloud, in order to understand better whether the current
visualised datasets are accessible and with an open license. Furthermore, we perform
quality assessment of 17 metrics over accessible datasets that are part of the LOD
cloud. These experiments were compared with previous experiments performed on older
versions of the LOD cloud. The results showed that there was no improvement on previously
identified problems. Based on our findings, we therefore propose a strategy and architecture
for a potential collaborative and sustainable LOD cloud.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {850–858},
numpages = {9},
keywords = {LOD cloud, metadata quality, sustainable services, Linked Data, data quality},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/2597176.2578273,
author = {Wang, Cong and Zink, Michael},
title = {On the Feasibility of DASH Streaming in the Cloud},
year = {2014},
isbn = {9781450327060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597176.2578273},
doi = {10.1145/2597176.2578273},
abstract = {As shown in recent studies, video streaming is by far the biggest category of backbone
Internet traffic in the US. As a measure to reduce the cost of highly over-provisioned
physical infrastructures while remaining the quality of video services, many streaming
service providers started to use cloud services where physical resources can be dynamically
allocated based on current demand. This paper characterizes the performance of Dynamic
Adaptive Streaming over HTTP (DASH), a new MPEG standard on adaptive streaming, in
the cloud. We seek to answer the following questions that are critical to content
providers that are hosting video in clouds: Which data center is the best to host
videos? Does geographical distance matter? What type of instance is best suitable
depending on different needs? How to efficiently solve the trade-off between performance
and cost? The measurement methods and results presented in this paper can be easily
expanded into other VoD services, and they allow us to i) characterize DASH behavior
when streaming from the cloud; ii) identify the key factors that influence the DASH
performance; and iii) suggest improvements for related services.},
booktitle = {Proceedings of Network and Operating System Support on Digital Audio and Video Workshop},
pages = {49–54},
numpages = {6},
keywords = {quality of experience, HTTP adaptive streaming, Cloud computing, video-on-demand},
location = {Singapore, Singapore},
series = {NOSSDAV '14}
}

@inproceedings{10.1145/3194124.3194130,
author = {Shatnawi, Anas and Orr\`{u}, Matteo and Mobilio, Marco and Riganelli, Oliviero and Mariani, Leonardo},
title = {Cloudhealth: A Model-Driven Approach to Watch the Health of Cloud Services},
year = {2018},
isbn = {9781450357302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194124.3194130},
doi = {10.1145/3194124.3194130},
abstract = {Cloud systems are complex and large systems where services provided by different operators
must coexist and eventually cooperate. In such a complex environment, controlling
the health of both the whole environment and the individual services is extremely
important to timely and effectively react to misbehaviours, unexpected events, and
failures. Although there are solutions to monitor cloud systems at different granularity
levels, how to relate the many KPIs that can be collected about the health of the
system and how health information can be properly reported to operators are open questions.This
paper reports the early results we achieved in the challenge of monitoring the health
of cloud systems. In particular we present CloudHealth, a model-based health monitoring
approach that can be used by operators to watch specific quality attributes. The Cloud-Health
Monitoring Model describes how to operationalize high level monitoring goals by dividing
them into subgoals, deriving metrics for the subgoals, and using probes to collect
the metrics. We use the CloudHealth Monitoring Model to control the probes that must
be deployed on the target system, the KPIs that are dynamically collected, and the
visualization of the data in dashboards.},
booktitle = {Proceedings of the 1st International Workshop on Software Health},
pages = {40–47},
numpages = {8},
keywords = {quality model, software health, monitoring, cloud service, monitoring model, metrics, KPI},
location = {Gothenburg, Sweden},
series = {SoHeal '18}
}

@inproceedings{10.1145/2565585.2565607,
author = {Klugman, Noah and Rosa, Javier and Pannuto, Pat and Podolsky, Matthew and Huang, William and Dutta, Prabal},
title = {Grid Watch: Mapping Blackouts with Smart Phones},
year = {2014},
isbn = {9781450327428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2565585.2565607},
doi = {10.1145/2565585.2565607},
abstract = {The power grid is one of humanity's most significant engineering undertakings and
it is essential in developed and developing nations alike. Currently, transparency
into the power grid relies on utility companies and more fine-grained insight is provided
by costly smart meter deployments. We claim that greater visibility into power grid
conditions can be provided in an inexpensive and crowd-sourced manner independent
of utility companies by leveraging existing smartphones. Our key insight is that an
unmodified smartphone can detect power outages by monitoring changes to its own power
state, locally verifying these outages using a variety of sensors that reduce the
likelihood of false power outage reports, and corroborating actual reports with other
phones through data aggregation in the cloud. The proposed approach enables a decentralized
system that can scale, potentially providing researchers and concerned citizens with
a powerful new tool to analyze the power grid and hold utility companies accountable
for poor power quality. This paper demonstrates the viability of the basic idea, identifies
a number of challenges that are specific to this application as well as ones that
are common to many crowd-sourced applications, and highlights some improvements to
smartphone operating systems that could better support such applications in the future.},
booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
articleno = {1},
numpages = {6},
keywords = {side channel information, power monitoring, smartphone applications, smart grid, crowdsourcing},
location = {Santa Barbara, California},
series = {HotMobile '14}
}

@article{10.1145/3442187,
author = {Al-Abbasi, Abubakr O. and Aggarwal, Vaneet},
title = {VidCloud: Joint Stall and Quality Optimization for Video Streaming over Cloud},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3442187},
doi = {10.1145/3442187},
abstract = {As video-streaming services have expanded and improved, cloud-based video has evolved
into a necessary feature of any successful business for reaching internal and external
audiences. In this article, video streaming over distributed storage is considered
where the video segments are encoded using an erasure code for better reliability.
We consider a representative system architecture for a realistic (typical) content
delivery network (CDN). Given multiple parallel streams/link between each server and
the edge router, we need to determine, for each client request, the subset of servers
to stream the video, as well as one of the parallel streams from each chosen server.
To have this scheduling, this article proposes a two-stage probabilistic scheduling.
The selection of video quality is also chosen with a certain probability distribution
that is optimized in our algorithm. With these parameters, the playback time of video
segments is determined by characterizing the download time of each coded chunk for
each video segment. Using the playback times, a bound on the moment generating function
of the stall duration is used to bound the mean stall duration. Based on this, we
formulate an optimization problem to jointly optimize the convex combination of mean
stall duration and average video quality for all requests, where the two-stage probabilistic
scheduling, video quality selection, bandwidth split among parallel streams, and auxiliary
bound parameters can be chosen. This non-convex problem is solved using an efficient
iterative algorithm. Based on the offline version of our proposed algorithm, an online
policy is developed where servers selection, quality, bandwidth split, and parallel
streams are selected in an online manner. Experimental results show significant improvement
in QoE metrics for cloud-based video as compared to the considered baselines.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = jan,
articleno = {17},
numpages = {32},
keywords = {mean stall duration, two-stage probabilistic scheduling, video quality, Video streaming over cloud, erasure codes}
}

@inproceedings{10.1145/3361525.3361543,
author = {Grohmann, Johannes and Nicholson, Patrick K. and Iglesias, Jesus Omana and Kounev, Samuel and Lugones, Diego},
title = {Monitorless: Predicting Performance Degradation in Cloud Applications with Machine Learning},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361543},
doi = {10.1145/3361525.3361543},
abstract = {Today, software operation engineers rely on application key performance indicators
(KPIs) for sizing and orchestrating cloud resources dynamically. KPIs are monitored
to assess the achievable performance and to configure various cloud-specific parameters
such as flavors of instances and autoscaling rules, among others. Usually, keeping
KPIs within acceptable levels requires application expertise which is expensive and
can slow down the continuous delivery of software. Expertise is required because KPIs
are normally based on application-specific quality-of-service metrics, like service
response time and processing rate, instead of generic platform metrics, like those
typical across various environments (e.g., CPU and memory utilization, I/O rate, etc.)In
this paper, we investigate the feasibility of outsourcing the management of application
performance from developers to cloud operators. In the same way that the serverless
paradigm allows the execution environment to be fully managed by a third party, we
discuss a monitorless model to streamline application deployment by delegating performance
management. We show that training a machine learning model with platform-level data,
collected from the execution of representative containerized services, allows inferring
application KPI degradation. This is an opportunity to simplify operations as engineers
can rely solely on platform metrics -- while still fulfilling application KPIs --
to configure portable and application agnostic rules and other cloud-specific parameters
to automatically trigger actions such as autoscaling, instance migration, network
slicing, etc.Results show that monitorless infers KPI degradation with an accuracy
of 97% and, notably, it performs similarly to typical autoscaling solutions, even
when autoscaling rules are optimally tuned with knowledge of the expected workload.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {149–162},
numpages = {14},
keywords = {Cloud computing, DevOps, Machine learning, Monitoring},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/2835075.2835078,
author = {Adegboyega, Abiola},
title = {An Adaptive Resource Provisioning Scheme for Effective QoS Maintenance in the IaaS Cloud},
year = {2015},
isbn = {9781450337328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835075.2835078},
doi = {10.1145/2835075.2835078},
abstract = {Effective bandwidth provisioning is of vital importance in the virtualized cloud where
tenants with unique SLAs share a finite network. Different tenants collocated on the
same physical server deployed with increasing VM density necessitates Quality of Service
(QoS) provisioning beginning at the hypervisor. Recent efforts at provisioning the
cloud network through various reservation methodologies have achieved some measure
of success. However most of them do not account for the entire path over which application
components communicate and cannot provide the necessary Service Level Agreement (SLA).
Cloud applications components often communicate across multiple network devices aggregated
into layers connected over finite bandwidth links that affect application response.
Furthermore, traffic to and from tenant applications display volatility. In view of
this, we design a virtual network reservation framework that is mindful of application
performance across multiple network devices &amp; traffic volatility. Our network reservation
framework is based on a forecasting engine motivated by the volatility existent in
traffic to and from virtualized cloud environments. This forecasting engine is able
to maintain SLAs by employing dynamic time-series models to develop novel bandwidth
provisioning thresholds that adapt to the time-variation in tenant workloads. We test
the effectiveness of our methods in the OpenStack cloud environment focusing on traffic
directionality in the datacenter network, VM density and QoS across multiple flows
competing for finite bandwidth. Our forecasting method offers a 25% improvement in
prediction accuracy over existing methods while the reservation framework maintains
SLAs at 95%.},
booktitle = {Proceedings of the International Workshop on Virtualization Technologies},
articleno = {2},
numpages = {6},
keywords = {Virtualization, QoS, Forecasting, SDN, Volatility},
location = {Vancouver, BC, Canada},
series = {VT15}
}

@inproceedings{10.1145/3094405.3094406,
author = {Zhao, Yang and Xia, Nai and Tian, Chen and Li, Bo and Tang, Yizhou and Wang, Yi and Zhang, Gong and Li, Rui and Liu, Alex X.},
title = {Performance of Container Networking Technologies},
year = {2017},
isbn = {9781450350587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094405.3094406},
doi = {10.1145/3094405.3094406},
abstract = {Container networking is now an important part of cloud virtualization architectures.
It provides network access for containers by connecting both virtual and physical
network interfaces. The performance of container networking has multiple dependencies,
and each factor may significantly affect the performance. In this paper, we perform
systematic experiments to study the performance of container networking technologies.
For every measurement result, we try our best to qualify influencing factors.},
booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
pages = {1–6},
numpages = {6},
keywords = {Container, Measurement, Networking},
location = {Los Angeles, CA, USA},
series = {HotConNet '17}
}

@inproceedings{10.5555/2821357.2821366,
author = {Herbst, Nikolas Roman and Kounev, Samuel and Weber, Andreas and Groenda, Henning},
title = {BUNGEE: An Elasticity Benchmark for Self-Adaptive IaaS Cloud Environments},
year = {2015},
publisher = {IEEE Press},
abstract = {Today's infrastructure clouds provide resource elasticity (i.e. auto-scaling) mechanisms
enabling self-adaptive resource provisioning to reflect variations in the load intensity
over time. These mechanisms impact on the application performance, however, their
effect in specific situations is hard to quantify and compare. To evaluate the quality
of elasticity mechanisms provided by different platforms and configurations, respective
metrics and benchmarks are required. Existing metrics for elasticity only consider
the time required to provision and deprovision resources or the costs impact of adaptations.
Existing benchmarks lack the capability to handle open workloads with realistic load
intensity profiles and do not explicitly distinguish between the performance exhibited
by the provisioned underlying resources, on the one hand, and the quality of the elasticity
mechanisms themselves, on the other hand.In this paper, we propose reliable metrics
for quantifying the timing aspects and accuracy of elasticity. Based on these metrics,
we propose a novel approach for benchmarking the elasticity of Infrastructure-as-a-Service
(IaaS) cloud platforms independent of the performance exhibited by the provisioned
underlying resources. We show that the proposed metrics provide consistent ranking
of elastic platforms on an ordinal scale. Finally, we present an extensive case study
of real-world complexity demonstrating that the proposed approach is applicable in
realistic scenarios and can cope with different levels of resource efficiency.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {46–56},
numpages = {11},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/3349611.3355546,
author = {Schwind, Anika and Haberzettl, Lorenz and Wamser, Florian and Ho\ss{}feld, Tobias},
title = {QoE Analysis of Spotify Audio Streaming and App Browsing},
year = {2019},
isbn = {9781450369275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349611.3355546},
doi = {10.1145/3349611.3355546},
abstract = {Spotify is the most-listened audio streaming provider in 2019 with 217 million active
users per month. Providers are therefore interested in the quality and functionality
of Spotify in order to provide their users with the best possible streaming quality.
While video streaming services such as Netflix and their streaming approach have been
extensively explored in previous research, audio streaming services like Spotify and
their corresponding behavior at certain network conditions have not been considered
in detail yet. In this paper, we perform a QoE analysis under various network conditions
and examine the app browsing performance of the audio streaming platform Spotify using
its native Android mobile application. We have developed a measurement tool that emulates
a user listening to audio through Spotify. While streaming, application and network
layer parameters are captured that have a high correlation to the user's QoE. The
paper shows a baseline scenario including the streaming of a single song as well as
playlist streaming behavior. Next, the effect of interruptions on the streaming behavior
is evaluated and finally, the influence of network impairments on QoE key performance
indicators such as initial delay is shown.},
booktitle = {Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {25–30},
numpages = {6},
keywords = {mobile application, audio streaming, spotify, qoe, browsing},
location = {Los Cabos, Mexico},
series = {Internet-QoE'19}
}

@inproceedings{10.1145/3229591.3229592,
author = {R\"{u}th, Jan and Glebke, Ren\'{e} and Wehrle, Klaus and Causevic, Vedad and Hirche, Sandra},
title = {Towards In-Network Industrial Feedback Control},
year = {2018},
isbn = {9781450359085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229591.3229592},
doi = {10.1145/3229591.3229592},
abstract = {Controlling physical machinery and processes is at the core of production automation.
However, challenged by inflexibility, automation and control is evaluating to outsource
this control to resourceful cloud environments. While this enables to derive better
control through a plethora of measurements, it challenges the control quality through
delay introduced through networks.In this paper, we show how to unify control and
communication by offloading delay sensitive control tasks from the cloud to local
network elements --- a previously unexplored area for in-network processing --- enabling
both, ultra-high quality-of-control and scalable orchestration through cloud environments.
Our implementation demonstrates how we combine state of the art control with communication.
We achieve this by expressing the control and the datapath in P4 which we synthesize
to BPF programs that we execute in XDP environments on Netronome SmartNICs. Further,
we highlight the demands of control towards communication to build more involved and
complex in-network controllers.},
booktitle = {Proceedings of the 2018 Morning Workshop on In-Network Computing},
pages = {14–19},
numpages = {6},
location = {Budapest, Hungary},
series = {NetCompute '18}
}

@inproceedings{10.1145/3070607.3070608,
author = {Hutchison, Dylan and Howe, Bill and Suciu, Dan},
title = {LaraDB: A Minimalist Kernel for Linear and Relational Algebra Computation},
year = {2017},
isbn = {9781450350198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3070607.3070608},
doi = {10.1145/3070607.3070608},
abstract = {Analytics tasks manipulate structured data with variants of relational algebra (RA)
and quantitative data with variants of linear algebra (LA). The two computational
models have overlapping expressiveness, motivating a common programming model that
affords unified reasoning and algorithm design. At the logical level we propose LARA,
a lean algebra of three operators, that expresses RA and LA as well as relevant optimization
rules. We show a series of proofs that position LARA at just the right level of expressiveness
for a middleware algebra: more explicit than MapReduce but more general than RA or
LA. At the physical level we find that the LARA operators afford efficient implementations
using a single primitive that is available in a variety of backend engines: range
scans over partitioned sorted maps.To evaluate these ideas, we implemented the LARA
operators as range iterators in Apache Accumulo, a popular implementation of Google's
BigTable. First we show how LARA expresses a sensor quality control task, and we measure
the performance impact of optimizations LARA admits on this task. Second we show that
the LARADB implementation outperforms Accumulo's native MapReduce integration on a
core task involving join and aggregation in the form of matrix multiply, especially
at smaller scales that are typically a poor fit for scale-out approaches. We find
that LARADB offers a conceptually lean framework for optimizing mixed-abstraction
analytics tasks, without giving up fast record-level updates and scans.},
booktitle = {Proceedings of the 4th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond},
articleno = {2},
numpages = {10},
location = {Chicago, IL, USA},
series = {BeyondMR'17}
}

@inproceedings{10.1145/2737095.2742919,
author = {Nasser, Soliman and Barry, Andew and Doniec, Marek and Peled, Guy and Rosman, Guy and Rus, Daniela and Volkov, Mikhail and Feldman, Dan},
title = {Fleye on the Car: Big Data Meets the Internet of Things},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2742919},
doi = {10.1145/2737095.2742919},
abstract = {Vehicle-based vision algorithms, such as the collision alert systems [4], are able
to interpret a scene in real-time and provide drivers with immediate feedback. However,
such technologies are based on cameras on the car, limited to the vicinity of the
car, severely limiting their potential. They cannot find empty parking slots, bypass
traffic jams, or warn about dangers outside the car's immediate surrounding. An intelligent
driving system augmented with additional sensors and network inputs may significantly
reduce the number of accidents, improve traffic congestion, and care for the safety
and quality of people's lives.We propose an open-code system, called Fleye, that consists
of an autonomous drone (nano quadrotor) that carries a radio camera and flies few
meters in front and above the car. The streaming video is transmitted in real time
from the quadcopter to Amazon's EC2 cloud together with information about the driver,
the drone, and the car's state. The output is then transmitted to the "smart glasses"
of the driver. The control of the drone, as well as the sensor data collection from
the driver, is done by low cost (&lt;30$) minicomputer. Most computation is done in the
cloud, allowing straightforward integration of multiple vehicle behaviour and additional
sensors, as well as greater computational capability.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {382–383},
numpages = {2},
keywords = {video streaming, internet of things, quadrotors, collision alert system},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/2666310.2666376,
author = {Qamar, Ahmad M. and Afyouni, Imad and Rahman, Md. Abdur and Rehman, Faizan Ur and Hussain, Delwar and Basalamah, Saleh and Lbath, Ahmed},
title = {A GIS-Based Serious Game Interface for Therapy Monitoring},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666376},
doi = {10.1145/2666310.2666376},
abstract = {In this paper, we present a novel idea of a map-based therapy environment for people
with Hemiplegia. The therapy environment is designed according to the suggestions
of therapists, which consists of a spatial map browsing serious game augmented with
our novel multi-sensory natural user interface (NUI). The NUI is based on 3D motion
sensors that can recognize different hand and body gestures used for browsing a 3D
or 2D map. The 3D motion sensors work in a non-invasive way; hence, they do not require
any wearable body attachments and can be used at home without assistance from the
therapists. The map-browsing environment provides an immersive experience to the disabled
users, which helps in performing therapy in an interesting and entertaining manner.
We have developed analytics for measuring certain quality of health improvement metrics
from each type of spatial map browsing movements. The 3D motion sensors have been
tested with Nokia, Google, ESRI, and a number of other maps that allow a subject to
visualize and browse the 3D and 2D maps of the world. The map browsing session data
shows the nature of big data; hence, the session data is stored in a cloud environment.
Our developed serious game environment is web-based; thus anyone having the appropriate
low cost sensor hardware can plug it in and start experiencing a natural way of hands
free map browsing. We have deployed our framework in a hospital that treats Hemiplegic
patients. Based on the feedback obtained, the developed platform shows a huge potential
for use in hospitals that provide physiotherapy services as well as at patients' home
as an assistive therapeutic service.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {589–592},
numpages = {4},
keywords = {therapy, kinect, GIS, leap, e-health, serious games},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/2774993.2775063,
author = {Sun, Peng and Vanbever, Laurent and Rexford, Jennifer},
title = {Scalable Programmable Inbound Traffic Engineering},
year = {2015},
isbn = {9781450334518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2774993.2775063},
doi = {10.1145/2774993.2775063},
abstract = {With the rise of video streaming and cloud services, enterprise and access networks
receive much more traffic than they send, and must rely on the Internet to offer good
end-to-end performance. These edge networks often connect to multiple ISPs for better
performance and reliability, but have only limited ways to influence which of their
ISPs carries the traffic for each service. In this paper, we present Sprite, a software-defined
solution for flexible inbound traffic engineering (TE). Sprite offers direct, fine-grained
control over inbound traffic, by announcing different public IP prefixes to each ISP,
and performing source network address translation (SNAT) on outbound request traffic.
Our design achieves scalability in both the data plane (by performing SNAT on edge
switches close to the clients) and the control plane (by having local agents install
the SNAT rules). The controller translates high-level TE objectives, based on client
and server names, as well as performance metrics, to a dynamic network policy based
on real-time traffic and performance measurements. We evaluate Sprite with live data
from "in the wild" experiments on an EC2-based testbed, and demonstrate how Sprite
dynamically adapts the network policy to achieve high-level TE objectives, such as
balancing YouTube traffic among ISPs to improve video quality.},
booktitle = {Proceedings of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research},
articleno = {12},
numpages = {7},
keywords = {software-defined networking, scalability, traffic engineering},
location = {Santa Clara, California},
series = {SOSR '15}
}

@inproceedings{10.1145/3349611.3355543,
author = {Loh, Frank and Vomhoff, Viktoria and Wamser, Florian and Metzger, Florian and Ho\ss{}feld, Tobias},
title = {Traffic Measurement Study on Video Streaming with the Amazon Echo Show},
year = {2019},
isbn = {9781450369275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349611.3355543},
doi = {10.1145/3349611.3355543},
abstract = {The Amazon Echo Show is one of the most widely used smart speakers with the ability
to stream video. Due to its popularity, the traffic profiles of such devices are of
interest to network operators and providers. This work presents a measurement study
of the Amazon Echo Show in terms of network traffic and streaming behavior. More than
470,hours of streaming data are collected and analyzed at network layer. Based on
this, streaming quality is derived at application layer. The study quantifies the
traffic and shows that streaming with the Amazon Echo Show is comparable to streaming
with a native web browser, but in a more conservative way.},
booktitle = {Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {31–36},
numpages = {6},
keywords = {alexa, traffic analysis, qoe, amazon echo, streaming},
location = {Los Cabos, Mexico},
series = {Internet-QoE'19}
}

@inproceedings{10.1145/3458306.3458873,
author = {Huang, Tianchi and Zhang, Rui-Xiao and Sun, Lifeng},
title = {Deep Reinforced Bitrate Ladders for Adaptive Video Streaming},
year = {2021},
isbn = {9781450384353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458306.3458873},
doi = {10.1145/3458306.3458873},
abstract = {In the typical transcoding pipeline for adaptive video streaming, raw videos are pre-chunked
and pre-encoded according to a set of resolution-bitrate or resolution-quality pairs
on the server-side, where the pair is often named as bitrate ladder. Different from
existing heuristics, we argue that a good bitrate ladder should be optimized by considering
video content features, network capacity, and storage costs on the cloud. We propose
DeepLadder, a per-chunk optimization scheme which adopts state-of-the-art deep reinforcement
learning (DRL) method to optimize the bitrate ladder w.r.t the above concerns. Technically,
DeepLadder selects the proper setting for each video resolution autoregressively.
We use over 8,000 video chunks, measure over 1,000,000 perceptual video qualities,
collect real-world network traces for more than 50 hours, and invent faithful virtual
environments to help train DeepLadder efficiently. Across a series of comprehensive
experiments on both Constant Bitrate (CBR) and Variable Bitrate (VBR)-encoded videos,
we demonstrate significant improvements in average video quality bandwidth utilization,
and storage overhead in comparison to prior work as well as the ability to be deployed
in the real-world transcoding framework.},
booktitle = {Proceedings of the 31st ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {66–73},
numpages = {8},
keywords = {bitrate ladder, adaptive video streaming},
location = {Istanbul, Turkey},
series = {NOSSDAV '21}
}

@inproceedings{10.1145/2740908.2742827,
author = {Assaf, Ahmad and Senart, Aline and Troncy, Rapha\"{e}l},
title = {Roomba: Automatic Validation, Correction and Generation of Dataset Metadata},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742827},
doi = {10.1145/2740908.2742827},
abstract = {Data is being published by both the public and private sectors and covers a diverse
set of domains ranging from life sciences to media or government data. An example
is the Linked Open Data (LOD) cloud which is potentially a gold mine for organizations
and individuals who are trying to leverage external data sources in order to produce
more informed business decisions. Considering the significant variation in size, the
languages used and the freshness of the data, one realizes that spotting spam datasets
or simply finding useful datasets without prior knowledge is increasingly complicated.
In this paper, we propose Roomba, a scalable automatic approach for extracting, validating,
correcting and generating descriptive linked dataset profiles. While Roomba is generic,
we target CKAN-based data portals and we validate our approach against a set of open
data portals including the Linked Open Data (LOD) cloud as viewed on the DataHub.
The results demonstrate that the general state of various datasets and groups, including
the LOD cloud group, needs more attention as most of the datasets suffer from bad
quality metadata and lack some informative metrics that are required to facilitate
dataset search.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {159–162},
numpages = {4},
keywords = {dataset profile, data quality, linked data, metadata},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3001913.3006645,
author = {Gibson, Marsalis T. and Rosa, Javier and Brewer, Eric A.},
title = {MDB: A Metadata Tracking Microcontroller Micro-Database},
year = {2016},
isbn = {9781450346498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001913.3006645},
doi = {10.1145/3001913.3006645},
abstract = {This work in progress explores a database designed to enable data sharing on custom
hardware data collection devices and prototypes. Projects and systems are frequently
based on the Arduino framework, examples include ODK's FoneAstra [3], the Open Energy
Monitor [7], and the Grove system of sensors [5]. The Arduino platform is targeted
because of its ease of use, community support, and low cost as a data collecting device
compared to other off-the-shelf sensors. However, there is a need for a framework
suitable for microcontrollers that enable ease of integration into other data collection
systems. This includes the ability to synchronize data with collection and aggregation
devices designed to work offline as well as the ability to track sensors and describe
data sources for other machines and users. To address the issue, we propose a solution
based on an existing small database usable on the Arduino platform that would integrate
into the Mezuri [6] data collection system. The database is designed to fit within
the running memory constraints on a microcontroller to store sensor data with relatively
few fields per reading on flash media. This framework, with explicit support for metadata,
enables users in emerging regions to directly measure physical quantities as well
as indirectly measure human behavior in future development projects involving direct
sensing. The database can be used by a non-expert. In particular, we investigate the
qualities that a technically inclined social scientist would look for when storing
such data on microcontrollers. To enable Mezuri integration we will support metadata
as a first class object accessible with additional utility functions and native synchronization
support.},
booktitle = {Proceedings of the 7th Annual Symposium on Computing for Development},
articleno = {36},
numpages = {4},
keywords = {Metadata, Arduino, Embedded Databases, Emerging Regions, Sensors, Data Collection, Microcontroller},
location = {Nairobi, Kenya},
series = {ACM DEV '16}
}

@article{10.1145/3337956,
author = {Moghaddam, Sara Kardani and Buyya, Rajkumar and Ramamohanarao, Kotagiri},
title = {Performance-Aware Management of Cloud Resources: A Taxonomy and Future Directions},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3337956},
doi = {10.1145/3337956},
abstract = {The dynamic nature of the cloud environment has made the distributed resource management
process a challenge for cloud service providers. The importance of maintaining quality
of service in accordance with customer expectations and the highly dynamic nature
of cloud-hosted applications add new levels of complexity to the process. Advances
in big-data learning approaches have shifted conventional static capacity planning
solutions to complex performance-aware resource management methods. It is shown that
the process of decision-making for resource adjustment is closely related to the behavior
of the system, including the utilization of resources and application components.
Therefore, a continuous monitoring of system attributes and performance metrics provides
the raw data for the analysis of problems affecting the performance of the application.
Data analytic methods, such as statistical and machine-learning approaches, offer
the required concepts, models, and tools to dig into the data and find general rules,
patterns, and characteristics that define the functionality of the system. Obtained
knowledge from the data analysis process helps to determine the changes in the workloads,
faulty components, or problems that can cause system performance to degrade. A timely
reaction to performance degradation can avoid violations of service level agreements,
including performing proper corrective actions such as auto-scaling or other resource
adjustment solutions. In this article, we investigate the main requirements and limitations
of cloud resource management, including a study of the approaches to workload and
anomaly analysis in the context of performance management in the cloud. A taxonomy
of the works on this problem is presented that identifies main approaches in existing
research from the data analysis side to resource adjustment techniques. Finally, considering
the observed gaps in the general direction of the reviewed works, a list of these
gaps is proposed for future researchers to pursue.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {84},
numpages = {37},
keywords = {Anomaly detection, resource management, big-data analytics, performance management}
}

@inproceedings{10.1145/3417113.3422184,
author = {Malavolta, Ivano and Grua, Eoin Martino and Lam, Cheng-Yu and de Vries, Randy and Tan, Franky and Zielinski, Eric and Peters, Michael and Kaandorp, Luuk},
title = {A Framework for the Automatic Execution of Measurement-Based Experiments on Android Devices},
year = {2020},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3422184},
doi = {10.1145/3417113.3422184},
abstract = {Conducting measurement-based experiments is fundamental for assessing the quality
of Android apps in terms of, e.g., energy consumption, CPU, and memory usage. However,
orchestrating such experiments is not trivial as it requires large boilerplate code,
careful setup of measurement tools, and the adoption of various empirical best practices
scattered across the literature. All together, those factors are slowing down the
scientific advancement and harming experiments' replicability in the mobile software
engineering area.In this paper we present Android Runner (AR), a framework for automatically
executing measurement-based experiments on native and web apps running on Android
devices. In AR, an experiment is defined once in a descriptive fashion, and then its
execution is fully automatic, customizable, and replicable. AR is implemented in Python
and it can be extended with third-party profilers.AR has been used in more than 25
scientific studies primarily targeting performance and energy efficiency.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering Workshops},
pages = {61–66},
numpages = {6},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3277593.3277619,
author = {Belkaroui, Rami and Bertaux, Aur\'{e}lie and Labbani, Ouassila and Hugol-Gential, Cl\'{e}mentine and Nicolle, Christophe},
title = {Towards Events Ontology Based on Data Sensors Network for Viticulture Domain},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277619},
doi = {10.1145/3277593.3277619},
abstract = {Wine Cloud project is the first "Big Data" platform on the french viticulture value
chain. The aim of this platform is to provide a complete traceability of the life
cycle of the wine, from the wine-grower to the consumer. In particular, Wine Cloud
may qualify as an agricultural decision platform that will be used for vine life cycle
management in order to predict the occurrence of major risks (vine diseases, grape
vine pests, physiological risks, fermentation stoppage, oxidation of vine, etc...).
Also to make wine production more rational by offering winegrower a set of recommendation
regarding their strategy's of production development.The proposed platform "Wine Cloud"
is based on heterogeneous sensors network (agricultural machines, plant sensors and
measuring stations) deployed throughout a vineyard. These sensors allow for capturing
data from the agricultural process and remote monitoring vineyards in the Internet
of Things (IoT) era. However, the sensors data from different source is hard to work
together for lack of semantic. Therefore, the task of coherently combining heterogeneous
sensors data becomes very challenging. The integration of heterogeneous data from
sensors can be achieved by data mining algorithms able to build correlations. Nevertheless,
the meaning and the value of these correlations is difficult to perceive without highlighting
the meaning of the data and the semantic description of the measured environment.In
order to bridge this gap and build causality relationships form heterogeneous sensor
data, we propose an ontology-based approach, that consists in exploring heterogeneous
sensor data (light, temperature, atmospheric pressure, etc) in terms of ontologies
enriched with semantic meta-data describing the life cycle of the monitored environment.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {44},
numpages = {7},
keywords = {semantic sensor data, smart viticulture, ontologies, event ontology, IoT, big data},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/3412841.3441886,
author = {Chikhaoui, Amina and Lemarchand, Laurent and Boukhalfa, Kamel and Boukhobza, Jalil},
title = {<i>StorNIR</i>, a Multi-Objective Replica Placement Strategy for Cloud Federations},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441886},
doi = {10.1145/3412841.3441886},
abstract = {Federation of clouds makes it possible to transparently extend the resources of Cloud
Service Providers (CSPs). For storage services several metrics need to be considered
to satisfy customers QoS, that is storage performance, network latency and data availability.
Data replication is a key strategy to optimize such metrics. For a CSP, member of
a Federation, an effective placement of customers data object replicas is crucial
to satisfy QoS demands. In this paper, we modeled the replica placement problem as
a multi-objective optimization problem (MOOP) taking into account the local storage
classes, other federation CSPs (external) storage services, and customers requirements.
To solve this problem, we propose StorNIR a cost-efficient data object Storing scheme
based on NSGAII upgraded with Injection and Reparation operators. StorNIR is a matheuristic
that consists in hybridizing an exact method with NSGAII meta-heuristic. A repair
operator was designed to make the solutions feasible with regards to the system constraints
(storage volume, IOPs, etc). StorNIR performed better than both NSGAII meta-heuristic
and the exact method in terms of quality of solutions and scalability. The repair
function improves the NSGAII meta-heuristic up to 7 times with 7.4% more extra time
execution. On average, StorNIR enhances by 17 times the quality of the initial solutions
calculated by CPLEX in terms of Hypervolume. In addition, the designed matheuristic
approach can be generalized to other meta-heuristics than NSGAII such as MOPSO meta-heuristic.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {50–59},
numpages = {10},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3447526.3472057,
author = {Chaudhary, Akash and Belani, Manshul and Maheshwari, Naman and Parnami, Aman},
title = {Verbose : Designing a Context-Based Educational System for Improving Communicative Expressions},
year = {2021},
isbn = {9781450383288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447526.3472057},
doi = {10.1145/3447526.3472057},
abstract = { ESL (English as a second language) speakers tend to follow the tone structure of
their first language, making their speech difficult to understand for native speakers,
thereby limiting their opportunities for education and employment. To address this
problem, we build an interactive smartphone-based educational mobile application using
the user-centered design process. This application teaches English intonations based
on globally consistent pitch patterns through conversations with a trained chat assistant,
which inculcates expert linguists’ teaching principles. After co-designing the application’s
parameters with primary stakeholders and expert visual designers, we assess its effectiveness
by measuring the pre and post-performance of the users after the system usage, using
various quantitative measures, like intonation scores, SEQ, and SUS. Feedback from
users suggests that ESL speakers find significant improvement in the perception of
their vocal expressions, thereby highlighting the necessity of such a system in improving
the quality of conversations that people have in general.},
booktitle = {Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction},
articleno = {41},
numpages = {13},
keywords = {Stress-timed language, Learning application, Context-based learning, Intonations, Communicative expressions},
location = {Toulouse &amp; Virtual, France},
series = {MobileHCI '21}
}

@inproceedings{10.1145/3018896.3025135,
author = {Saha, Debanshee and Shinde, Manasi and Thadeshwar, Shail},
title = {IoT Based Air Quality Monitoring System Using Wireless Sensors Deployed in Public Bus Services},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3025135},
doi = {10.1145/3018896.3025135},
abstract = {The ambient air quality monitoring network involves the measurement of a number of
air pollutants at various locations in the city so as to maintain a sustainable air
quality. It is the need of hour to monitor air quality in order to reduce air pollution.
Exposure to air pollution can lead to respiratory and cardiovascular diseases, which
is estimated to be the cause for 620,000 early deaths in 2010, and the impact on health
due to air pollution in India has been calculated at 3 percent of its GDP. In recent
years, air pollution has acquired critical dimensions and the air quality in most
cities that monitor outdoor air pollution fail to meet WHO guidelines for safe levels.
Air pollution is a major environmental change that causes many hazardous effects on
human beings which need to be controlled. With the advancements in technology, several
innovations have been made in the field of communications that are transitioning to
the Internet of Things (IoT). In this domain, Wireless Sensor Networks (WSN) are one
of those independent sensing devices to monitor physical and environmental conditions
along with thousands of applications in other fields. In this paper, we are proposing
the deployment of WSN sensor nodes in public transport buses for the constant monitoring
of air pollution. The data regarding the air pollution particles such as emissions,
smoke, and other pollutants will be collected via sensors on the public transport
bus and the data will be aggregated and transmitted to the nearest sink node. Using
the concept of the Internet of Things (IoT) the collected data will be uploaded on
the cloud server also called as the IoT cloud where a large amount of the data is
stored. This data can then be accessed at any point to analyze and accurate measures
can be taken to map the air pollution.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {87},
numpages = {6},
keywords = {smart city, air pollution, wireless sensor networks, internet of things (IoT)},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3090354.3090366,
author = {Zertal, Soumia and Batouche, Mohamed Chawki},
title = {A Hybrid Approach for Optimized Composition of Cloud Services},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090366},
doi = {10.1145/3090354.3090366},
abstract = {The increasing use of Cloud services as well as the increasing demands of complex
cloud services creates the need for a dynamic and adaptive composition of services,
in a decentralized and large scale environment, where the quality of services may
increase or decrease. Early attempts for dynamic composition of services have been
proposed. But they are limited by their ability to adapt when deploying in highly
dynamic and open environments. For better performance measurements, we use, in this
paper, the Particle Swarm Optimization (PSO) algorithm to find and provide the services
that meets the user's query. To assess the utility of each service, we take into consideration
its values of service quality provided in the past. The latter is represented by the
mechanism of stigmergy which uses the pheromone as a means of communication between
services.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {12},
numpages = {7},
keywords = {Stigmegy, Optimization, Particle Swarm Optimization, Service Composition, Cloud Computing},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3098603.3098608,
author = {Tasiopoulos, Argyrios G. and Atarashi, Ray and Psaras, Ioannis and Pavlou, George},
title = {On the Bitrate Adaptation of Shared Media Experience Services},
year = {2017},
isbn = {9781450350563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098603.3098608},
doi = {10.1145/3098603.3098608},
abstract = {In Shared Media Experience Services (SMESs), a group of people is interested in streaming
consumption in a synchronised way, like in the case of cloud gaming, live streaming,
and interactive social applications. However, group synchronisation comes at the expense
of other Quality of Experience (QoE) factors due to both the dynamic and diverse network
conditions that each group member experiences. Someone might wonder if there is a
way to keep a group synchronised while maintaining the highest possible QoE for each
one of its members. In this work, at first we create a Quality Assessment Framework
capable of evaluating different SMESs improvement approaches with respect to traditional
metrics like media bitrate quality, playback disruption, and end user desynchronisation.
Secondly, we focus on the bitrate adaptation for improving the QoE of SMESs, as an
incrementally deployable end user triggered approach, and we formulate the problem
in the context of Adaptive Real Time Dynamic Programming (ARTDP). Finally, we develop
and apply a simple QoE aware bitrate adaptation mechanism that we compare against
youtube live-streaming traces to find that it improves the youtube performance by
more than 30%.},
booktitle = {Proceedings of the Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {25–30},
numpages = {6},
keywords = {Bitrate Adaptation, QoE Assessment Framework, Shared Media Experience Services (SMESs)},
location = {Los Angeles, CA, USA},
series = {Internet QoE '17}
}

@article{10.1145/3383464,
author = {Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv},
title = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3383464},
doi = {10.1145/3383464},
abstract = {Recent years have witnessed the booming of big data analytical applications (BDAAs).
This trend provides unrivaled opportunities to reveal the latent patterns and correlations
embedded in the data, and thus productive decisions may be made. This was previously
a grand challenge due to the notoriously high dimensionality and scale of big data,
whereas the quality of service offered by providers is the first priority. As BDAAs
are routinely deployed on Clouds with great complexities and uncertainties, it is
a critical task to manage the service level agreements (SLAs) so that a high quality
of service can then be guaranteed. This study performs a systematic literature review
of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review
surveys the challenges and contemporary approaches along this direction centering
on SLA. A research taxonomy is proposed to formulate the results of the systematic
literature review. A new conceptual SLA model is defined and a multi-dimensional categorization
scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding
of managing SLAs and the motivation of trends for future research.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {46},
numpages = {40},
keywords = {SLA metrics, Big data, big data analytics application, service level agreement, SLA, service layer}
}

@inproceedings{10.1145/3428502.3428618,
author = {Symeonidis, Panagiotis and Mitropoulos, Pantelis and Taskaris, Simeon and Vakkas, Theodoros and Adamopoulou, Eleni and Karakirios, Dimitrios and Salamalikis, Vasileios and Kosmopoulos, Georgios and Kazantzidis, Andreas},
title = {ThermiAir: An Innovative Air Quality Monitoring System for Airborne Particulate Matter in Thermi, Greece},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428618},
doi = {10.1145/3428502.3428618},
abstract = {This paper presents the development of an innovative air quality monitoring platform
for the Municipality of Thermi in Thessaloniki. The monitoring network consists of
25 low cost but very accurate IoT sensors measuring the concentration of Particulate
Matter (PM 10, PM 2.5, PM 1.0). Using these new generation of sensors, it is feasible
to monitor air quality at city block level, revealing the spatial pattern of air pollution,
and thus allowing local and regional agencies to design and apply the most suitable
policies and measures to tackle the air pollution problem. The real time measurements
are stored in the Cloud and are disseminated to the citizens and the local authorities'
stakeholders through a web and a mobile app. The web application provides an air quality
dashboard which presents the overall air quality in the Municipality. Both the Air
Quality Index (AQI) and raw concentration data are used. Various types of presentations
are available including maps and charts. The web application provides also a three-day
air quality forecast using the Copernicus forecast data. The mobile app provides easy
access to the real time data in a simple to understand way, suitable for the public
users.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {775–778},
numpages = {4},
keywords = {data analytics, Air pollution, geographic information systems, Air quality, IoT sensors},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/3339825.3393581,
author = {Taraghi, Babak and Zabrovskiy, Anatoliy and Timmerer, Christian and Hellwagner, Hermann},
title = {CAdViSE: Cloud-Based Adaptive Video Streaming Evaluation Framework for the Automated Testing of Media Players},
year = {2020},
isbn = {9781450368452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339825.3393581},
doi = {10.1145/3339825.3393581},
abstract = {Attempting to cope with fluctuations of network conditions in terms of available bandwidth,
latency and packet loss, and to deliver the highest quality of video (and audio) content
to users, research on adaptive video streaming has attracted intense efforts from
the research community and huge investments from technology giants. How successful
these efforts and investments are, is a question that needs precise measurements of
the results of those technological advancements. HTTP-based Adaptive Streaming (HAS)
algorithms, which seek to improve video streaming over the Internet, introduce video
bitrate adaptivity in a way that is scalable and efficient. However, how each HAS
implementation takes into account the wide spectrum of variables and configuration
options, brings a high complexity to the task of measuring the results and visualizing
the statistics of the performance and quality of experience. In this paper, we introduce
CAdViSE, our Cloud-based Adaptive Video Streaming Evaluation framework for the automated
testing of adaptive media players. The paper aims to demonstrate a test environment
which can be instantiated in a cloud infrastructure, examines multiple media players
with different network attributes at defined points of the experiment time, and finally
concludes the evaluation with visualized statistics and insights into the results.},
booktitle = {Proceedings of the 11th ACM Multimedia Systems Conference},
pages = {349–352},
numpages = {4},
keywords = {HTTP adaptive streaming, MPEG-DASH, automated testing, network emulation, quality of experience, media players},
location = {Istanbul, Turkey},
series = {MMSys '20}
}

@inproceedings{10.1145/3349614.3356028,
author = {Tomei, Matthew and Schwing, Alexander and Narayanasamy, Satish and Kumar, Rakesh},
title = {Sensor Training Data Reduction for Autonomous Vehicles},
year = {2019},
isbn = {9781450369282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349614.3356028},
doi = {10.1145/3349614.3356028},
abstract = {Ensuring safety and reliability of autonomous vehicles requires good learning models
which, in turn, require a large amount of real-world training data. Data produced
by in-vehicle sensors (e.g., cameras, LIDARs, IMUs, etc.) can be used for training;
however, both local storage and transmission of this sensor data to the cloud for
subsequent use in training can be prohibitively expensive due to the staggering volume
of data produced by these sensors, especially the cameras. In this paper, we perform
the first exploration of techniques for reducing video frames in a way that the quality
of training for autonomous vehicles is minimally affected. We particularly focus on
utility aware data reduction schemes where the potential contribution of a video frame
to enhancing the quality of learning (or utility) is explicitly considered during
data reduction. Since actual utility of a video frame cannot be computed online, we
use surrogate utility metrics to decide what video frames to keep for training and
which ones to discard. Our results show that utility-aware data reduction schemes
can reduce the amount of camera data required for training by as much as $16times$
compared to random sampling for the same quality of learning (in terms of IoU).},
booktitle = {Proceedings of the 2019 Workshop on Hot Topics in Video Analytics and Intelligent Edges},
pages = {45–50},
numpages = {6},
keywords = {sensor, semantic segmentation, compression, machine learning, self driving car, autonomous vehicle, data reduction, active learning},
location = {Los Cabos, Mexico},
series = {HotEdgeVideo'19}
}

@inproceedings{10.1109/CCGRID.2017.120,
author = {Shekhar, Shashank and Gokhale, Aniruddha},
title = {Dynamic Resource Management Across Cloud-Edge Resources for Performance-Sensitive Applications},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.120},
doi = {10.1109/CCGRID.2017.120},
abstract = {A large number of modern applications and systems are cloud-hosted, however, limitations
in performance assurances from the cloud, and the longer and often unpredictable end-to-end
network latencies between the end user and the cloud can be detrimental to the response
time requirements of the applications, specifically those that have stringent Quality
of Service (QoS) requirements. Although edge resources, such as cloudlets, may alleviate
some of the latency concerns, there is a general lack of mechanisms that can dynamically
manage resources across the cloud-edge spectrum. To address these gaps, this research
proposes Dynamic Data Driven Cloud and Edge Systems (D3CES). It uses measurement data
collected from adaptively instrumenting the cloud and edge resources to learn and
enhance models of the distributed resource pool. In turn, the framework uses the learned
models in a feedback loop to make effective resource management decisions to host
applications and deliver their QoS properties. D3CES is being evaluated in the context
of a variety of cyber physical systems, such as smart city, online games, and augmented
reality applications.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {707–710},
numpages = {4},
keywords = {CPS, Edge Computing, IoT, DDDAS, Cloud Computing, Fog Computing, Resource Management},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3173162.3173207,
author = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A. and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark},
title = {Vbench: Benchmarking Video Transcoding in the Cloud},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173207},
doi = {10.1145/3173162.3173207},
abstract = {This paper presents vbench, a publicly available benchmark for cloud video services.
We are the first study, to the best of our knowledge, to characterize the emerging
video-as-a-service workload. Unlike prior video processing benchmarks, vbench's videos
are algorithmically selected to represent a large commercial corpus of millions of
videos. Reflecting the complex infrastructure that processes and hosts these videos,
vbench includes carefully constructed metrics and baselines. The combination of validated
corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and
compression. We demonstrate the importance of video selection with a microarchitectural
study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial
corpus that are not visible in other video corpuses. Our experiments with GPUs under
vbench's scoring scenarios reveal that context is critical: GPUs are well suited for
live-streaming, while for video-on-demand shift costs from compute to storage and
network. Counterintuitively, they are not viable for popular videos, for which highly
compressed, high quality copies are required. We instead find that popular videos
are currently well-served by the current trajectory of software encoders.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {797–809},
numpages = {13},
keywords = {video transcoding, accelerator, benchmark},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}

@article{10.1145/3296957.3173207,
author = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A. and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark},
title = {Vbench: Benchmarking Video Transcoding in the Cloud},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3173207},
doi = {10.1145/3296957.3173207},
abstract = {This paper presents vbench, a publicly available benchmark for cloud video services.
We are the first study, to the best of our knowledge, to characterize the emerging
video-as-a-service workload. Unlike prior video processing benchmarks, vbench's videos
are algorithmically selected to represent a large commercial corpus of millions of
videos. Reflecting the complex infrastructure that processes and hosts these videos,
vbench includes carefully constructed metrics and baselines. The combination of validated
corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and
compression. We demonstrate the importance of video selection with a microarchitectural
study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial
corpus that are not visible in other video corpuses. Our experiments with GPUs under
vbench's scoring scenarios reveal that context is critical: GPUs are well suited for
live-streaming, while for video-on-demand shift costs from compute to storage and
network. Counterintuitively, they are not viable for popular videos, for which highly
compressed, high quality copies are required. We instead find that popular videos
are currently well-served by the current trajectory of software encoders.},
journal = {SIGPLAN Not.},
month = mar,
pages = {797–809},
numpages = {13},
keywords = {video transcoding, accelerator, benchmark}
}

@inproceedings{10.1109/CCGRID.2018.00021,
author = {Imai, Shigeru and Patterson, Stacy and Varela, Carlos A.},
title = {Uncertainty-Aware Elastic Virtual Machine Scheduling for Stream Processing Systems},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00021},
doi = {10.1109/CCGRID.2018.00021},
abstract = {Stream processing systems deployed on the cloud need to be elastic to effectively
accommodate workload variations over time. Performance models can predict maximum
sustainable throughput (MST) as a function of the number of VMs allocated. We present
a scheduling framework that incorporates three statistical techniques to improve Quality
of Service (QoS) of cloud stream processing systems: (i) uncertainty quantification
to consider variance in the MST model; (ii) online learning to update MST model as
new performance metrics are gathered; and (iii) workload models to predict input data
stream rates assuming regular patterns occur over time. Our framework can be parameterized
by a QoS satisfaction target that statistically finds the best performance/cost tradeoff.
Our results illustrate that each of the three techniques alone significantly improves
QoS, from 52% to 73-81% QoS satisfaction rates on average for eight benchmark applications.
Furthermore, applying all three techniques allows us to reach 98.62% QoS satisfaction
rate with a cost less than twice the cost of the optimal (in hindsight) VM allocations,
and half of the cost of allocating VMs for the peak demand in the workload.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {62–71},
numpages = {10},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/3344341.3368796,
author = {Kuhlenkamp, J\"{o}rn and Werner, Sebastian and Borges, Maria C. and El Tal, Karim and Tai, Stefan},
title = {An Evaluation of FaaS Platforms as a Foundation for Serverless Big Data Processing},
year = {2019},
isbn = {9781450368940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344341.3368796},
doi = {10.1145/3344341.3368796},
abstract = {Function-as-a-Service (FaaS), offers a new alternative to operate cloud-based applications.
FaaS platforms enable developers to define their application only through a set of
service functions, relieving them of infrastructure management tasks, which are executed
automatically by the platform. Since its introduction, FaaS has grown to support workloads
beyond the lightweight use-cases it was originally intended for, and now serves as
a viable paradigm for big data processing. However, several questions regarding FaaS
platform quality are still unanswered. Specifically, the impact of automatic infrastructure
management on serverless big data applications remains unexplored.In this paper, we
propose a novel evaluation method (SIEM) to understand the impact of these tasks.
For this purpose, we introduce new metrics to quantify quality in different big data
application scenarios. We show an application of SIEM by evaluating the four major
FaaS providers, and contribute results and new insights for FaaS-based big data processing.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
pages = {1–9},
numpages = {9},
keywords = {serverless, benchmarking, big data processing, cloud computing},
location = {Auckland, New Zealand},
series = {UCC'19}
}

@inproceedings{10.1145/3395027.3419595,
author = {Ughetta, William and Kernighan, Brian W.},
title = {The Old Bailey and OCR: Benchmarking AWS, Azure, and GCP with 180,000 Page Images},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419595},
doi = {10.1145/3395027.3419595},
abstract = {The Proceedings of the Old Bailey is a corpus of over 180,000 page images of court
records printed from April 1674 to April 1913 and presents a comprehensive challenge
for Optical Character Recognition (OCR) services. The Old Bailey is an ideal benchmark
for historical document OCR, representing more than two centuries of variations in
documents, including spellings, formats, and printing and preservation qualities.
In addition to its historical and sociological significance, the Old Bailey is filled
with imperfections that reflect the reality of coping with large-scale historical
data. Most importantly, the Old Bailey contains human transcriptions for each page,
which can be used to help measure OCR accuracy. Since humans do make mistakes in transcriptions,
the relative performance of OCR services will be more informative than their absolute
performance. This paper compares three leading commercial OCR cloud services: Amazon
Web Services's Textract (AWS); Microsoft Azure's Cognitive Services (Azure); and Google
Cloud Platform's Vision (GCP). Benchmarking involved downloading over 180,000 images,
executing the OCR, and measuring the error rate of the OCR text against the human
transcriptions. Our results found that AWS had the lowest median error rate, Azure
had the lowest median round trip time, and GCP had the best combination of a low error
rate and a low duration.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {19},
numpages = {4},
keywords = {Old Bailey, Historical Documents, Amazon Web Services, Optical Character Recognition, Microsoft Azure, Google Cloud Platform},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@inproceedings{10.1109/CCGrid.2014.22,
author = {Byholm, Benjamin and Porres, Iv\'{a}n},
title = {Cost-Efficient, Reliable, Utility-Based Session Management in the Cloud},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.22},
doi = {10.1109/CCGrid.2014.22},
abstract = {We present a model and system for cost-efficient and reliable management of sessions
in a Cloud, based on the von Neumann-Morgenstern utility theorem. Our model enables
a web application provider to maximize profit while maintaining a desired quality
of service. The objective is to determine whether, when, where, and how long to store
a session, given multiple storage options with various properties, e.g. cost, capacity,
and reliability. Reliability is affected by three factors: how often session state
is stored, how many stores are used, and how reliable those stores are. To account
for these factors, we use a Markovian reliability model and treat the valid storage
options for each session as a von Neumann-Morgenstern lottery. We proceed by representing
the resulting problem as a knapsack problem, which can be heuristically solved for
a good compromise between efficiency and effectiveness. We analyze the results from
a discrete-event simulation involving multiple session management policies, including
two utility-based policies: a greedy heuristic policy intended to give real-time performance
and a reference policy based on solving the linear programming relaxation of the knapsack
problem, giving a theoretical upper bound on achievable utility. As the focus of this
work is exploratory, rather than performance-based, we do not directly measure the
time required for solving the model. Instead, we give the computational complexity
of the algorithms. Our results indicate that otherwise unprofitable services become
profitable through utility-based session management in a cloud setting. However, if
the costs are much lower than the expected revenues, all policies manage to turn a
profit. Different policies performed the best under different circumstances.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {102–111},
numpages = {10},
keywords = {analytical models, and serviceability, availability, utility theory, simulation, web-based services, distributed systems, markov processes, reliability},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3380851.3416742,
author = {Berger, Arthur},
title = {Designing an Analytics Approach for Technical Content},
year = {2020},
isbn = {9781450375252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380851.3416742},
doi = {10.1145/3380851.3416742},
abstract = {Working on an enterprise cloud product, my documentation team rethought our approach
to content analytics. Despite a variety of tools and awareness of industry best practices,
my team felt stuck using analytics only in annual or on-demand reports to management,
instead of to produce value for our end users. We employed Design Thinking practices
to guide a multifaceted user research project that led to changes in the way that
we created documentation and automated quality content checks. Key takeaways include
to involve the technical documentation team in identifying not only what metrics to
collect, but also how to collect, report, and use the metrics in order to increase
buy-in and the likelihood that data analytics about content leads to meaningful change
within the content itself.},
booktitle = {Proceedings of the 38th ACM International Conference on Design of Communication},
articleno = {7},
numpages = {5},
keywords = {Data analytics, design thinking,, content strategy},
location = {Denton, TX, USA},
series = {SIGDOC '20}
}

@inproceedings{10.1145/3428502.3428511,
author = {Branco, Te\'{o}filo T. and Kawashita, Ilka M. and de S\'{a}-Soares, Filipe and Monteiro, Cl\'{a}udio N.},
title = {An IoT Application Case Study to Optimize Electricity Consumption in the Government Sector},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428511},
doi = {10.1145/3428502.3428511},
abstract = {This paper presents a case study where sensor modules supported by Internet of Things
(IoT) technology were used to monitor and control electricity consumption of air conditioning
units in an innovation center of a public government institution. This study evaluates
alternatives to improve the management of electricity consumption in Salvador City
Hall's facilities. To contribute to the economy and sustainability of the Administration,
we aim to increase the efficiency of the processes currently adopted. Our focus is
on minimizing electricity waste and reducing costs. Installed sensor modules measure
electricity consumption and control the operation of air conditioning equipment, allowing
the administrator to manage the operation of these devices. The installation of smart
sensor modules connected to an IoT platform allows energy consumption data to be sent
to a computing Cloud and to be monitored remotely through dashboards generated by
specialized software. A quantitative analysis was conducted to measure the efficiency
of the air conditioning control system and identify opportunities for applying the
IoT solution to control natural resources in the public sector. The monitoring of
these signals subsidized the analyzes required for informed decision making of interventions
to improve the system's stability and promote the reduction of consumption. Also,
the system has demonstrated its ability to protect air conditioners, monitor the quality
of the power supplied, proactively control consumption, and establish appropriate
user behaviors for reducing consumption. Results demonstrated the feasibility of implementing
automated systems to improve the consumption of natural resources in the public sector.
We also identified some managerial behaviors required to enable this type of technological
solution.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {70–81},
numpages = {12},
keywords = {Internet of Thinks (IoT), Sustainability, Innovation, Smart Technologies, E-government},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/2987550.2987584,
author = {Cano, Ignacio and Aiyar, Srinivas and Krishnamurthy, Arvind},
title = {Characterizing Private Clouds: A Large-Scale Empirical Analysis of Enterprise Clusters},
year = {2016},
isbn = {9781450345255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987550.2987584},
doi = {10.1145/2987550.2987584},
abstract = {There is an increasing trend in the use of on-premise clusters within companies. Security,
regulatory constraints, and enhanced service quality push organizations to work in
these so called private cloud environments. On the other hand, the deployment of private
enterprise clusters requires careful consideration of what will be necessary or may
happen in the future, both in terms of compute demands and failures, as they lack
the public cloud's flexibility to immediately provision new nodes in case of demand
spikes or node failures.In order to better understand the challenges and tradeoffs
of operating in private settings, we perform, to the best of our knowledge, the first
extensive characterization of on-premise clusters. Specifically, we analyze data ranging
from hardware failures to typical compute/storage requirements and workload profiles,
from a large number of Nutanix clusters deployed at various companies.We show that
private cloud hardware failure rates are lower, and that load/demand needs are more
predictable than in other settings. Finally, we demonstrate the value of the measurements
by using them to provide an analytical model for computing durability in private clouds,
as well as a machine learning-driven approach for characterizing private clouds' growth.},
booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
pages = {29–41},
numpages = {13},
keywords = {Private clouds, Measurements, Reliability, Performance},
location = {Santa Clara, CA, USA},
series = {SoCC '16}
}

@inproceedings{10.1109/ISLPED52811.2021.9502472,
author = {Marculescu, Diana},
title = {When Climate Meets Machine Learning: Edge to Cloud ML Energy Efficiency},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISLPED52811.2021.9502472},
doi = {10.1109/ISLPED52811.2021.9502472},
abstract = {A large portion of current cloud and edge workloads feature Machine Learning (ML)
tasks, thereby requiring a deep understanding of their energy efficiency. While the
holy grail for judging the quality of a ML model has largely been testing accuracy,
and only recently its resource usage, neither of these metrics translate directly
to energy efficiency, runtime, or mobile device battery lifetime. This work uncovers
the need for building accurate, platform-specific power and latency models for ML
and efficient hardware-aware ML design methodologies, thus allowing machine learners
and hardware designers to identify not just the best accuracy ML model configuration,
but also those that satisfy given hardware constraints.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
articleno = {37},
numpages = {1},
keywords = {hardware-aware ML, quantization, model compression, neural architecture search},
location = {Boston, Massachusetts},
series = {ISLPED '21}
}

@inproceedings{10.1145/2950290.2994157,
author = {Rossi, Chuck and Shibley, Elisa and Su, Shi and Beck, Kent and Savor, Tony and Stumm, Michael},
title = {Continuous Deployment of Mobile Software at Facebook (Showcase)},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2994157},
doi = {10.1145/2950290.2994157},
abstract = { Continuous deployment is the practice of releasing software updates to production
as soon as it is ready, which is receiving increased adoption in industry. The frequency
of updates of mobile software has traditionally lagged the state of practice for cloud-based
services for a number of reasons. Mobile versions can only be released periodically.
Users can choose when and if to upgrade, which means that several different releases
coexist in production. There are hundreds of Android hardware variants, which increases
the risk of having errors in the software being deployed.  Facebook has made significant
progress in increasing the frequency of its mobile deployments. Over a period of 4
years, the Android release has gone from a deployment every 8 weeks to a deployment
every week. In this paper, we describe in detail the mobile deployment process at
FB. We present our findings from an extensive analysis of software engineering metrics
based on data collected over a period of 7 years. A key finding is that the frequency
of deployment does not directly affect developer productivity or software quality.
We argue that this finding is due to the fact that increasing the frequency of continuous
deployment forces improved release and deployment automation, which in turn reduces
developer workload. Additionally, the data we present shows that dog-fooding and obtaining
feedback from alpha and beta customers is critical to maintaining release quality.
},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {12–23},
numpages = {12},
keywords = {Software release, Mobile code testing, Continuous deployment, Continuous delivery, Agile development},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3240508.3240642,
author = {Pang, Haitian and Zhang, Cong and Wang, Fangxin and Hu, Han and Wang, Zhi and Liu, Jiangchuan and Sun, Lifeng},
title = {Optimizing Personalized Interaction Experience in Crowd-Interactive Livecast: A Cloud-Edge Approach},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240642},
doi = {10.1145/3240508.3240642},
abstract = {Enabling users to interact with broadcasters and audience, the crowd-interactive livecast
greatly improves viewer's quality of experience (QoE) and attracts millions of daily
active users recently. In addition to striking the balance between resource utilization
and viewers' QoE met in the traditional video streaming service, this novel service
needs to take supererogatory efforts to improve the interaction QoE, which reflects
the viewer interaction experience. To tackle this issue, we conduct measurement studies
over a large-scale dataset crawled from a representative livecast service provider.
We observe that the individual's interaction pattern is quite heterogeneous: only
10% viewers proactively participate in the interaction, and the rest viewers usually
watch passively. Incorporating the insight into the emerging cloud-edge architecture,
we propose a framework PIECE, which optimizes the Personalized Interaction Experience
with Cloud-Edge architecture (PIECE) for intelligent user access control and livecast
distribution. In particular, we first devise a novel deep neural network based algorithm
to predict users' interaction intensity using the historical viewer pattern. We then
design an algorithm to maximize the individual's QoE, by strategically matching viewer
sessions and transcoding-delivery paths over cloud-edge infrastructure. Finally, we
use trace-driven experiments to verify the effectiveness of PIECE. Our results show
that our prediction algorithm outperforms the state-of-the-art algorithms with a much
smaller mean absolute error (40% reduction). Furthermore, in comparison with the cloud-based
video delivery strategy, the proposed framework can simultaneously improve the average
viewers QoE (26% improvement) and interaction QoE (21% improvement), while maintaining
a high streaming bitrate.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1217–1225},
numpages = {9},
keywords = {cloud-edge, interactive live streaming, viewer interaction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3458305.3463384,
author = {Sethuraman, Manasvini and Sarma, Anirudh and Dhekne, Ashutosh and Ramachandran, Umakishore},
title = {Foresight: Planning for Spatial and Temporal Variations in Bandwidth for Streaming Services on Mobile Devices},
year = {2021},
isbn = {9781450384346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458305.3463384},
doi = {10.1145/3458305.3463384},
abstract = {Spatiotemporal variation in cellular bandwidth availability is well-known and could
affect a mobile user's quality of experience (QoE), especially while using bandwidth
intensive streaming applications such as movies, podcasts, and music videos during
commute. If such variations are made available to a streaming service in advance it
could perhaps plan better to avoid sub-optimal performance while the user travels
through regions of low bandwidth availability. The intuition is that such future knowledge
could be used to buffer additional content in regions of higher bandwidth availability
to tide over the deficits in regions of low bandwidth availability. Foresight is a
service designed to provide this future knowledge for client apps running on a mobile
device. It comprises three components: (a) a crowd-sourced bandwidth estimate reporting
facility, (b) an on-cloud bandwidth service that records the spatiotemporal variations
in bandwidth and serves queries for bandwidth availability from mobile users, and
(c) an on-device bandwidth manager that caters to the bandwidth requirements from
client apps by providing them with bandwidth allocation schedules. Foresight is implemented
in the Android framework. As a proof of concept for using this service, we have modified
an open-source video player---Exoplayer---to use the results of Foresight in its video
buffer management. Our performance evaluation shows Foresight's scalability. We also
showcase the opportunity that Foresight offers to ExoPlayer to enhance video quality
of experience (QoE) despite spatiotemporal bandwidth variations for metrics such as
overall higher bitrate of playback, reduction in number of bitrate switches, and reduction
in the number of stalls during video playback.},
booktitle = {Proceedings of the 12th ACM Multimedia Systems Conference},
pages = {227–240},
numpages = {14},
keywords = {bandwidth management, spatiotemporal bandwidth information},
location = {Istanbul, Turkey},
series = {MMSys '21}
}

@inproceedings{10.1145/2801694.2801710,
author = {Ganesan, Deepak},
title = {Towards Ultra-Low Power Wearable Health Sensing with Sparse Sampling and Asymmetric Communication},
year = {2015},
isbn = {9781450337014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801694.2801710},
doi = {10.1145/2801694.2801710},
abstract = {Wearable sensors offer tremendous opportunities for accelerating biomedical discovery,
and improving population-scale health and wellness. There is a growing appetite for
health analytics -- we are no longer content with wearables that count steps and calories,
we want to measure physiology, behavior, activities, cognition, affect, and other
parameters with the expectation that such data will lead to deep insights that can
improve quality of life.But a chasm separates expectations and reality. How do we
extract such insights from sensor platforms with tiny energy budgets? How do we communicate
high-rate sensor data to the cloud for enabling deep analytics while operating within
these energy budgets? How do we deal with noise, confounders, and artifacts that make
insights hard to extract from signals collected in real-world settings?In this talk,
I will discuss a few strategies to tackle these problems. I will discuss how we can
design an low-power computational eyeglass that continually tracks eye and visual
context by leveraging sparsity, how we can transfer data at Megabits/second from wearables
while operating at tens of micro-watts of power, and how we can leverage these techniques
in the context of mobile health.},
booktitle = {Proceedings of the 2015 Workshop on Wireless of the Students, by the Students, &amp; for the Students},
pages = {34},
numpages = {1},
keywords = {mobile health, backscatter communication, eye tracking},
location = {Paris, France},
series = {S3 '15}
}

@inproceedings{10.5555/3400397.3400622,
author = {Anagnostou, Anastasia and Taylor, Simon J. E. and Abubakar, Nura Tijjani and Kiss, Tamas and DesLauriers, James and Gesmier, Gregoire and Terstyanszky, Gabor and Kacsuk, Peter and Kovacs, Jozsef},
title = {Towards a Deadline-Based Simulation Experimentation Framework Using Micro-Services Auto-Scaling Approach},
year = {2019},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {There is growing number of research efforts in developing auto-scaling algorithms
and tools for cloud resources. Traditional performance metrics such as CPU, memory
and bandwidth usage for scaling up or down resources are not sufficient for all applications.
For example, modeling and simulation experimentation is usually expected to yield
results within a specific timeframe. In order to achieve this often the quality of
experiments is compromised either by restricting the parameter space to be explored
or by limiting the number of replications required to give statistical confidence.
In this paper, we present early stages of a deadline-based simulation experimentation
framework using a micro-services auto-scaling approach. A case study of an agent-based
simulation of a population physical activity behavior is used to demonstrate our framework.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2749–2758},
numpages = {10},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3152881.3152887,
author = {Rahman, Mahmudur and Hong, Hua-Jun and Rahman, Amatur and Tsai, Pei-Hsuan and Afrin, Afia and Uddin, Md Yusuf Sarwar and Venkatasubramanian, Nalini and Hsu, Cheng-Hsin},
title = {Adaptive Sensing Using Internet-of-Things with Constrained Communications},
year = {2017},
isbn = {9781450351683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152881.3152887},
doi = {10.1145/3152881.3152887},
abstract = {In this paper, we design and implement an Internet-of-Things (IoT) based platform
for developing cities using environmental sensing as driving application with a set
of air quality sensors that periodically upload sensor data to the cloud. Ubiquitous
and free WiFi access is unavailable in most developing cities; IoT deployments must
leverage 3G cellular connections that are expensive and metered. In order to best
utilize the limited 3G data plan, we envision two adaptation strategies to drive sensing
and sensemaking. The first technique is an infrastructure-level adaptation approach
where we adjust sensing intervals of periodic sensors so that the data volume remains
bounded within the plan. The second approach is at the information-level where application-specific
analytics are deployed on board devices (or the edge) through container technologies
(Docker and Kubernetes); the use case focuses on multimedia sensors that process captured
raw information to lower volume semantic data that is communicated. This approach
is implemented through the EnviroSCALE (Environmental Sensing and Community Alert
Network) platform, an inexpensive Raspberry Pi based environmental sensing system
that periodically publishes sensor data over a 3G connection with a limited data plan.
We outline our deployment experience of EnviroSCALE in Dhaka city, the capital of
Bangladesh. For information-level adaptation, we enhanced EnviroSCALE with Docker
containers with rich media analytics, along Kubernetes for provisioning IoT devices
and deploying the Docker images. To limit data communication overhead, the Docker
images are preloaded in the board but a small footprint of analytic code is transferred
whenever required. Our experiment results demonstrate the practicality of adaptive
sensing and triggering rich sensing analytics via user-specified criteria, even over
constrained data connections.},
booktitle = {Proceedings of the 16th Workshop on Adaptive and Reflective Middleware},
articleno = {6},
numpages = {6},
location = {Las Vegas, Nevada},
series = {ARM '17}
}

@inproceedings{10.1145/3462203.3475873,
author = {Agossou, B. Emmanuel and Toshiro, Takahara},
title = {IoT &amp; AI Based System for Fish Farming: Case Study of Benin},
year = {2021},
isbn = {9781450384780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462203.3475873},
doi = {10.1145/3462203.3475873},
abstract = {Agriculture including aquaculture has been changing through multiple technological
transformations in recent years. The Internet of Things (IoT) and Artificial Intelligence
(AI) are providing remarkable technological innovations on fish farming. In this research,
we present an automated IoT and AI-based system to improve fish farming. The proposed
system uses multiple sensors to measure in real-time water quality chemical parameters
such as: temperature, pH, turbidity, electrical conductivity, total dissolved solids,
etc., from the fish pond and send them on a cloud database to allow fish farmers to
access them in realtime with their devices (mobile phone, PC, tablets). The system
contains three web applications which fish farmers can use. The first web application
enables farmers with realtime visualizations of sensors data, issues alerts and remote
pumps controls. Fish farmers can use the second web application for fish disease detection
and to receive suggestions for diseases' care. This would help to classify two fish
diseases which are: Epizootic Ulcerative Syndrome(EUS), and Ichthyophthirus(Ich).
The third web application is a digital community platform for knowledge sharing, capacity
building, market opportunities and collaboration among fish farmers. Our system can
help reduce human efforts, reinforce capacity building, increase fish production and
market opportunities for fish farmers.},
booktitle = {Proceedings of the Conference on Information Technology for Social Good},
pages = {259–264},
numpages = {6},
keywords = {MQTT, IoT, ESP32, eFish Farm, Convolutional Neural Network, Arduino, AI, Smart Fish Farming},
location = {Roma, Italy},
series = {GoodIT '21}
}

@article{10.1145/3089262.3089268,
author = {Hossfeld, Tobias},
title = {2016 International Teletraffic Congress (ITC 28) Report},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/3089262.3089268},
doi = {10.1145/3089262.3089268},
abstract = {The 28th International Teletraffic Congress (ITC 28) was held on 12--16 September
2016 at the University of W"urzburg, Germany. The conference was technically cosponsored
by the IEEE Communications Society and the Information Technology Society within VDE,
and in cooperation with ACM SIGCOMM. ITC 28 provided a forum for leading researchers
from academia and industry to present and discuss the latest advances and developments
in design, modelling, measurement, and performance evaluation of communication systems,
networks, and services. The main theme of ITC 28, emph{Digital Connected World},
reflects the evolution of communications and networking, which is continually changing
the world we are living in. The technical program was composed of 37 contributed full
papers, 6 short demo papers and three keynote addresses. Three workshops dedicated
to timely topics were sponsored: Programmability for Cloud Networks and Applications,
Quality of Experience Centric Management, Quality Engineering for a Reliable Internet
of Services.See ITC 28 Homepage: url{https://itc28.org/}},
journal = {SIGCOMM Comput. Commun. Rev.},
month = may,
pages = {30–35},
numpages = {6},
keywords = {Performance Analysis and Modeling, Virtualization, Measurements, Video Streaming, Caching, Traffic and Network Management, Softwarization, Wireless and Cellular, Information Centric Networks, Clouds and Data Center}
}

@inproceedings{10.1145/3341105.3373915,
author = {Santos, Guilherme and Paulino, Herv\'{e} and Vardasca, Tom\'{e}},
title = {QoE-Aware Auto-Scaling of Heterogeneous Containerized Services (and Its Application to Health Services)},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373915},
doi = {10.1145/3341105.3373915},
abstract = {Containerized service is currently a widely adopted solution to deploy services in
the cloud. However, many companies offer a very diverse set of Web accessible services
that are subjected to very distinctive workloads. Consequently, to correctly provision
the right amount of resources for each of these services is a challenge. In this paper
we propose the Autonomic ConTainerized Service Scaler (ACTS), an autonomic system
able to horizontally and vertically scale a set of heterogeneous containerized services
subjected to different workloads. The adaptation decisions depended on a set of high-level
Quality of Experience (QoE) metrics centered on the services' end-user. We have applied
ACTS to some of the digital services of the Shared Services of the Ministry of Health
(SPMS) public company. The experimental results show that our solution is able to
adequately adapt the configuration of each service, as a direct response to alterations
on its workload.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {242–249},
numpages = {8},
keywords = {quality of experience, health care, auto-scaling, containers},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.1145/3408293,
author = {He, Xin and Liu, Qiong and Yang, You},
title = {Make Full Use of Priors: Cross-View Optimized Filter for Multi-View Depth Enhancement},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3408293},
doi = {10.1145/3408293},
abstract = {Multi-view video plus depth (MVD) is the promising and widely adopted data representation
for future 3D visual applications and interactive media. However, compression distortions
on depth videos impede the development of such applications, and filters are crucially
needed for the quality enhancement at the terminal side. Cross-view priors can intuitively
be involved in filter design, but these priors are also distorted in compression and
thus the contribution of them can hardly be considered in previous research. In this
article, we propose a cross-view optimized filter for depth map quality enhancement
by making full use of inner- and cross-view priors. We dedicate to evaluate the contributions
of distorted cross-view priors in filtering the current view of depth, and then both
inner- and cross-view priors can be involved in the filter design. Thus, distortions
of cross-view priors are not barriers again as before. For the purpose of that, mutual
information guided cross-view consistency is designed to evaluate the contributions
of cross-view priors from compression distortions of MVD. After that, under the framework
of global optimization, both inner- and cross-view priors are modeled and taken to
minimize the designed energy function where both data accuracy and spatial smoothness
are modeled. The experimental results show that the proposed model outperforms state-of-the-art
methods, where 3.289 dB and 0.0407 average gains on peak signal-to-noise ratio and
structural similarity metrics can be obtained, respectively. For the subjective evaluations,
object details and structure information are recovered in the compressed depth video.
We also verify our method via several practical applications, including virtual view
synthesis for smooth interaction and point cloud for 3D modeling for accuracy evaluation.
In these verifications, the ringing and malposition artifacts on object contours are
properly handled for interactive video, and discontinuous object surfaces are restored
for 3D modeling. All of these results suggest that compression distortions in MVD
can be properly filtered by the proposed model, which provides a promising solution
for future bandwidth constrained 3D and interactive visual applications.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {127},
numpages = {19},
keywords = {global optimization, Multi-view video plus depth, view consistency}
}

@inproceedings{10.1145/3277453.3277484,
author = {Shanthasheela, A. and Shanmugavadivu, P.},
title = {An Exploratory Analysis of Speckle Noise Removal Methods for Satellite Images},
year = {2018},
isbn = {9781450365413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277453.3277484},
doi = {10.1145/3277453.3277484},
abstract = {Satellite images captured in a variety of modalities serve as the primary source for
many applications. Satellite image processing extracts the image /spectral information
represented in the form of pixels, classifies those pixels based on the similarity
measures and further analyzes the inherent data, as per the requirements. The foremost
objective of satellite processing is to automatically categorize the pixels in an
image into the respective land cover class labels or themes. These pixels are classified
by its spectral information and it is determined by the relative reflectance in various
bands of wavelength. The accuracy and outcomes of any satellite image processing procedure,
irrespective of the application domain, directly depends on its quality. Satellite
images are invariably degraded by speckle noise. Hence, preprocessing the images for
speckle noise suppression and/or cloud removal is deemed an inevitable component in
satellite image processing. Researchers have proposed a spectrum of methods for speckle
noise/cloud removal. A detailed review on the significant research publications on
speckle noise removal are summarized in this article. The consolidation of methodology
merits and demerits of the select research articles are presented in this paper. This
review article on speckle noise removal is designed as a ready-reference for those
researchers working in satellite image processing.},
booktitle = {Proceedings of the 2018 International Conference on Electronics and Electrical Engineering Technology},
pages = {217–222},
numpages = {6},
keywords = {Satellite images, SAR, RADAR, Review, Speckle Noise, Noise filters, Literature Survey},
location = {Tianjin, China},
series = {EEET '18}
}

@inproceedings{10.1145/3468264.3473915,
author = {Kalia, Anup K. and Xiao, Jin and Krishna, Rahul and Sinha, Saurabh and Vukovic, Maja and Banerjee, Debasish},
title = {Mono2Micro: A Practical and Effective Tool for Decomposing Monolithic Java Applications to Microservices},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473915},
doi = {10.1145/3468264.3473915},
abstract = {In migrating production workloads to cloud, enterprises often face the daunting task
of evolving monolithic applications toward a microservice architecture. At IBM, we
developed a tool called Mono2Micro to assist with this challenging task. Mono2Micro
performs spatio-temporal decomposition, leveraging well-defined business use cases
and runtime call relations to create functionally cohesive partitioning of application
classes. Our preliminary evaluation of Mono2Micro showed promising results.  How well
does Mono2Micro perform against other decomposition techniques, and how do practitioners
perceive the tool? This paper describes the technical foundations of Mono2Micro and
presents results to answer these two questions. To answer the first question, we evaluated
Mono2Micro against four existing techniques on a set of open-source and proprietary
Java applications and using different metrics to assess the quality of decomposition
and tool’s efficiency. Our results show that Mono2Micro significantly outperforms
state-of-the-art baselines in specific metrics well-defined for the problem domain.
To answer the second question, we conducted a survey of twenty-one practitioners in
various industry roles who have used Mono2Micro. This study highlights several benefits
of the tool, interesting practitioner perceptions, and scope for further improvements.
Overall, these results show that Mono2Micro can provide a valuable aid to practitioners
in creating functionally cohesive and explainable microservice decompositions.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1214–1224},
numpages = {11},
keywords = {microservices, clustering, dynamic analysis},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3318396.3318427,
author = {Ho, P. C. W. and Fok, W. W. T. and Chan, C. K. K. and Yeung, H. H. Au and Ng, H. W. and Wong, S. L. and Ngai, S. Y. and Kwok, P. H. and Ho, Y. S. and Chan, K. H.},
title = {Flipping the Learning and Teaching of Reading Strategies and Comprehension through a Cloud-Based Interactive Big Data Reading Platform},
year = {2019},
isbn = {9781450362672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318396.3318427},
doi = {10.1145/3318396.3318427},
abstract = {This study investigates the learning approach of the designed Flipped Reading Platform
(FRP) and its effects on primary school students' general Chinese reading and comprehension
capabilities. This study was undertaken as part of the Quality Education Fund project
in Hong Kong, titled "Flipped Reading: Enhancing the Learning and Teaching of Reading
Strategies and Comprehension in Chinese via an Interactive Cloud Platform."This paper
presents the design of the Interactive Cloud Platform FRP, which incorporates elements
of both reading strategies and learning activities, and investigates the changes in
students' reading performance, applied strategies, and active learning level with
the application of FRP. The results show the experimental students using the FRP in
the pilot scheme generally gained more in three stages of reading comprehension, and
that low-achieving students learned reading strategies better. Analysis of FRP log
activities shows students' active engagement in reading and perceived competence.
Different learning outcomes were also found within the experimental group, categorized
by BYOD and non-BYOD classes. Implications of the study show the effectiveness of
FRP, and the design demonstrates how the reading measures integrated the assessment
indicators of both international and local standards in the domain of Chinese Language
reading. Further research can be developed to examine individual online reading performance
and learning behaviour on FRP.},
booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
pages = {185–191},
numpages = {7},
keywords = {e-Learning, Big data, reading strategy, Chinese Language, Cloud Platform, Flipped reading},
location = {Cambridge, United Kingdom},
series = {ICEIT 2019}
}

@inproceedings{10.1145/2642687.2642704,
author = {Palomares, Daniel and Migault, Daniel and Hendrik, Hendrik and Laurent, Maryline and Pujolle, Guy},
title = {Elastic Virtual Private Cloud},
year = {2014},
isbn = {9781450330275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642687.2642704},
doi = {10.1145/2642687.2642704},
abstract = {Several Virtual Private Networks are based on IPsec. However, IPsec has not been designed
with elasticity in mind, which makes clusters of IPsec security gateways hard to manage
for providing high Service Level Agreement (SLA). Thus, these SG clusters need management
techniques to maintain their Quality of Service. For example, ISPs use VPNs to secure
millions of communications when offloading End-Users from Radio Access Networks towards
alternative access networks such as WLANs. Additionally, Virtual Private Cloud (VPC)
providers also handle thousands of VPN connections when remote EUs access private
clouds services. This paper describes how to provide Traffic Management (TM) and High
Availability (HA) for VPN infrastructures by sharing or transferring an IPsec session.
TM and HA have been implemented and evaluated over a 2-nodes cluster. We measured
their impact on a real time audio streaming simulating a phone conversation. We found
out that over a 2 minutes conversation, the impact on QoS measured with POLQA while
applying TM or HA, is less than 3%.},
booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {127–131},
numpages = {5},
keywords = {high availability, VPN management, IPSEC, virtual private cloud, IKEV2, QoS, POLQA, context transfer},
location = {Montreal, QC, Canada},
series = {Q2SWinet '14}
}

@article{10.14778/2994509.2994527,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {On Measuring the Lattice of Commonalities among Several Linked Datasets},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994527},
doi = {10.14778/2994509.2994527},
abstract = {A big number of datasets has been published according to the principles of Linked
Data and this number keeps increasing. Although the ultimate objective is linking
and integration, it is not currently evident how connected the current LOD cloud is.
Measurements (and indexes) that involve more than two datasets are not available although
they are important: (a) for obtaining complete information about one particular URI
(or set of URIs) with provenance (b) for aiding dataset discovery and selection, (c)
for assessing the connectivity between any set of datasets for quality checking and
for monitoring their evolution over time, (d) for constructing visualizations that
provide more informative overviews. Since it would be prohibitively expensive to perform
all these measurements in a na\"{\i}ve way, in this paper we introduce indexes (and their
construction algorithms) that can speedup such tasks. In brief, we introduce (i) a
namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and
transitive closure of the owl:sameAs relationships encountered in the datasets, (iii)
a semantics-aware element index (that exploits the aforementioned indexes), and finally
(iv) two lattice-based incremental algorithms for speeding up the computation of the
intersection of URIs of any set of datasets. We discuss the speedup obtained by the
introduced indexes and algorithms through comparative results and finally we report
measurements about connectivity of the LOD cloud that have never been carried out
so far.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1101–1112},
numpages = {12}
}

@inproceedings{10.1145/3185768.3186297,
author = {Versluis, Laurens and van Eyk, Erwin and Iosup, Alexandru},
title = {An Analysis of Workflow Formalisms for Workflows with Complex Non-Functional Requirements},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186297},
doi = {10.1145/3185768.3186297},
abstract = {Cloud and datacenter operators offer progressively more sophisticated service level
agreements to customers. The Quality-of-Service guarantees by these operators have
started to entail non-functional requirements customers have regarding their applications.
At the same time, expressing applications as workflows in datacenters is increasingly
more common. Currently, non-functional requirements (NFRs) can only be defined on
entire workflows and cannot be changed at runtime, possibly wasting valuable resources.
To move towards modifiable NFRs at the task level, there is a need for a formalism
capable of expressing this. Existing formalisms do not support this level of granularity
or are restricted to a subset of NFRs. In this work, we investigate the current support
for NFRs in existing formalisms. Using a library containing workflows with and without
NFRs, we inspect the capability of existing formalisms to express these requirements.
Additionally, we create and evaluate five metrics to qualitatively and quantitatively
compare each formalism. Our main findings are that although current formalisms do
not support arbitrary NFRs per-task, the Directed Acyclic Graphs (DAGs) formalism
is the most suitable to extend.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {107–112},
numpages = {6},
keywords = {cloud, workflow, non-functional requirement, formalism, datacenter},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1109/SEAMS.2017.2,
author = {Moreno, Gabriel A. and Papadopoulos, Alessandro V. and Angelopoulos, Konstantinos and C\'{a}mara, Javier and Schmerl, Bradley},
title = {Comparing Model-Based Predictive Approaches to Self-Adaptation: CobRA and PLA},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.2},
doi = {10.1109/SEAMS.2017.2},
abstract = {Modern software-intensive systems must often guarantee certain quality requirements
under changing run-time conditions and high levels of uncertainty. Self-adaptation
has proven to be an effective way to engineer systems that can address such challenges,
but many of these approaches are purely reactive and adapt only after a failure has
taken place. To overcome some of the limitations of reactive approaches (e.g., lagging
behind environment changes and favoring short-term improvements), recent proactive
self-adaptation mechanisms apply ideas from control theory, such as model predictive
control (MPC), to improve adaptation. When selecting which MPC approach to apply,
the improvement that can be obtained with each approach is scenario-dependent, and
so guidance is needed to better understand how to choose an approach for a given situation.
In this paper, we compare CobRA and PLA, two approaches that are inspired by MPC.
CobRA is a requirements-based approach that applies control theory, whereas PLA is
architecture-based and applies stochastic analysis. We compare the two approaches
applied to RUBiS, a benchmark system for web and cloud application performance, discussing
the required expertise needed to use both approaches and comparing their run-time
performance with respect to different metrics.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {42–53},
numpages = {12},
keywords = {self-adaptation, adaptive system, model predictive control, CobRA, latency, PLA},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@inproceedings{10.1145/3196398.3196422,
author = {Widder, David Gray and Hilton, Michael and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {I'm Leaving You, Travis: A Continuous Integration Breakup Story},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196422},
doi = {10.1145/3196398.3196422},
abstract = {Continuous Integration (CI) services, which can automatically build, test, and deploy
software projects, are an invaluable asset in distributed teams, increasing productivity
and helping to maintain code quality. Prior work has shown that CI pipelines can be
sophisticated, and choosing and configuring a CI system involves tradeoffs. As CI
technology matures, new CI tool offerings arise to meet the distinct wants and needs
of software teams, as they negotiate a path through these tradeoffs, depending on
their context. In this paper, we begin to uncover these nuances, and tell the story
of open-source projects falling out of love with Travis, the earliest and most popular
cloud-based CI system. Using logistic regression, we quantify the effects that open-source
community factors and project technical factors have on the rate of Travis abandonment.
We find that increased build complexity reduces the chances of abandonment, that larger
projects abandon at higher rates, and that a project's dominant language has significant
but varying effects. Finally, we find the surprising result that metrics of configuration
attempts and knowledge dispersion in the project do not affect the rate of abandonment.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {165–169},
numpages = {5},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/2980258.2980451,
author = {Bhattacharya, Adrija and Choudhury, Sankhayan},
title = {An Efficient Service Selection Approach through a Goodness Measure of the Participating QoS},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2980451},
doi = {10.1145/2980258.2980451},
abstract = {The service repository in cloud consists of atomic services those need to be composed
as per the requirement of consumers. In general, various providers offer different
atomic services with same functionalities. These are called similar services and the
service selection is the process to choose the best one among them based on the associated
Quality of Services. Thus a service selection problem for satisfying the requirement
of a consumer with given constraints is conceptualized as a multi-objective optimization
problem. Sometime it involves the objectives that have conflict among them and as
a result the complexity of the problem increases. In such cases users are requested
to provide the feedback on the required QoS and accordingly the solution is offered.
This demands sufficient domain knowledge from a user that may not be feasible in real
cases. As a result the offered solution may deviate from the intended one. In this
work we have proposed a method to calculate an overall measure of a service considering
all QoS. It converts the multi-objective problem to single objective. This reduces
the exponential complexity of NP-Hard problem into a problem solvable in polynomial
time. The proposed Service Selection algorithm does not require any feedback from
the users. The algorithm is capable to offer a moderate solution to users considering
all requested QoS. The experiment shows that almost in every case the proposed algorithm
is able to deliver a solution satisfying all QoS as referred by a user.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {94},
numpages = {6},
keywords = {Service Selection, Goodness, QoS},
location = {Pondicherry, India},
series = {ICIA-16}
}

@article{10.1145/3038919,
author = {Olson, Judith S. and Wang, Dakuo and Olson, Gary M. and Zhang, Jingwen},
title = {How People Write Together Now: Beginning the Investigation with Advanced Undergraduates in a Project Course},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3038919},
doi = {10.1145/3038919},
abstract = {Today's commercially available word processors allow people to write collaboratively
in the cloud, both in the familiar asynchronous mode and now in synchronous mode as
well. This opens up new ways of working together. We examined the data traces of collaborative
writing behavior in student teams’ use of Google Docs to discover how they are writing
together now. We found that student teams write both synchronously and asynchronously,
take fluid roles in the writing and editing of the documents, and show a variety of
styles of collaborative writing, including writing from scratch, beginning with an
outline, pasting in a related example as a template to organize their own writing,
and three more. We also found that the document serves as a place where they share
a number of things not included in the final document, including links or references
to related materials, the assignment requirements from the instructor, and informal
discussions to coordinate the collaboration or to structure the document. We computed
a number of measures to depict a group's collaboration behavior and asked external
graders to score these documents for quality. We found that the documents that included
balanced participation and/or exhibited leadership were judged higher in quality,
as were those that were longer. We then suggested system design implications and behavioral
guidelines to support people writing together better, and concluded the paper with
future research directions.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = mar,
articleno = {4},
numpages = {40},
keywords = {Google docs, collaboration, co-authoring, writing style}
}

@inproceedings{10.1145/3448891.3448918,
author = {Du, Yifan and Sailhan, Fran\c{c}oise and Issarny, Val\'{e}rie},
title = {IAM&nbsp;– Interpolation and Aggregation on the Move: Collaborative Crowdsensing for Spatio-Temporal Phenomena},
year = {2020},
isbn = {9781450388405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448891.3448918},
doi = {10.1145/3448891.3448918},
abstract = { Crowdsensing allows citizens to contribute to the monitoring of their living environment
using the sensors embedded in their mobile devices, e.g., smartphones. However, crowdsensing
at scale involves significant communication, computation, and financial costs due
to the dependence on cloud infrastructures for the analysis (e.g., interpolation and
aggregation) of spatio-temporal data. This limits the adoption of crowdsensing by
activists although sorely needed to inform our knowledge of the environment. As an
alternative to the centralized analysis of crowdsensed observations, this paper introduces
a fully distributed interpolation-mediated aggregation approach running on smartphones.
To achieve so efficiently, we model the interpolation as a distributed tensor completion
problem, and we introduce a lightweight aggregation strategy that anticipates the
likelihood of future encounters according to the quality of the interpolation. Our
approach thus shifts the centralized post-processing of crowdsensed data to distributed
pre-processing on the move, based on opportunistic encounters of crowdsensors through
state-of-the-art D2D networking. The evaluation using a dataset of quantitative environmental
measurements collected from 550 crowdsensors over 1 year shows that our solution significantly
reduces –and may even eliminate– the dependence on the cloud infrastructure, while
it incurs a limited resource cost on end devices. Meanwhile, the overall data accuracy
remains comparable to that of the centralized approach.},
booktitle = {MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {337–346},
numpages = {10},
keywords = {Aggregation, Interpolation, Opportunistic Relay, Crowdsensing, Pervasive Computing, Ubiquitous Sensing},
location = {Darmstadt, Germany},
series = {MobiQuitous '20}
}

@article{10.1145/3165713,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3165713},
doi = {10.1145/3165713},
abstract = {Although the ultimate objective of Linked Data is linking and integration, it is not
currently evident how connected the current Linked Open Data (LOD) cloud is. In this
article, we focus on methods, supported by special indexes and algorithms, for performing
measurements related to the connectivity of more than two datasets that are useful
in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference,
i.e., for obtaining complete information about a set of entities, including provenance
information; (c) Data Quality Assessment and Improvement, i.e., for assessing the
connectivity between any set of datasets and monitoring their evolution over time,
as well as for estimating data veracity; (d) Dataset Visualizations; and various other
tasks. Since it would be prohibitively expensive to perform all these measurements
in a na\"{\i}ve way, in this article, we introduce indexes (and their construction algorithms)
that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix
index, (ii) a sameAs catalog for computing the symmetric and transitive closure of
the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware
element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based
incremental algorithms for speeding up the computation of the intersection of URIs
of any set of datasets. For enhancing scalability, we propose parallel index construction
algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved
speedup using either a single machine or a cluster of machines, and we provide insights
regarding the factors that affect efficiency. Finally, we report measurements about
the connectivity of the (billion triples-sized) LOD cloud that have never been carried
out so far.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {15},
numpages = {49},
keywords = {spark, Data quality, connectivity, lattice of measurements, big data, dataset selection, mapreduce, dataset discovery, linked data}
}

@inproceedings{10.5555/3233397.3233412,
author = {Perez-Palacin, Diego and Mirandola, Raffaela and Monterisi, Federico and Montoli, Andrea},
title = {QoS-Driven Probabilistic Runtime Evaluations of Virtual Machine Placement on Hosts},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {We tackle the cloud providers challenge of virtual machine placement when the client
experienced Quality of Service (QoS) is of paramount importance and resource demand
of virtual machines varies over time. To this end, this work investigates approaches
that leverage measured dynamic data for placement decisions. Relying on dynamic data
to guide decisions has, on the one hand, the potential to optimize hardware utilization,
while, on the other hand, increases the risk on the provided QoS. In this context,
we present three probabilistic methods for evaluation of host suitability to allocate
new virtual machines. We also present experiments results that illustrate the differences
in the outcomes of presented approaches.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {90–94},
numpages = {5},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1145/3233547.3233666,
author = {Kotlar, Alex V. and Wingo, Thomas S.},
title = {Tutorial: Rapidly Identifying Disease-Associated Rare Variants Using Annotation and Machine Learning at Whole-Genome Scale Online},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233666},
doi = {10.1145/3233547.3233666},
abstract = {Accurately identifying disease-associated alleles from large sequencing experiments
remains challenging. During this tutorial, participants will learn how to use a new
variant annotation and filtering web app called Bystro (https://bystro.io/) to analyze
sequencing experiments. Bystro is the first online, cloud-based application that makes
variant annotation and filtering accessible to all researchers for even the largest,
terabyte-sized whole-genome experiments containing thousands of samples. Using its
general-purpose, natural-language filtering engine, attendees will be shown how to
perform quality control measures and identify alleles of interest. They will then
be guided in exporting those variants, and using them in both a regression context
by performing rare-variant association tests in R, as well as classification context
by training new machine learning models in Python's scikit-learn library.},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {558},
numpages = {1},
keywords = {rare-variant association tests, variant classification, bioinformatics, machine learning},
location = {Washington, DC, USA},
series = {BCB '18}
}

@inproceedings{10.1145/3365245.3365247,
author = {Liu, Xiaofeng and Zou, Hui and Niu, Wanyu and Song, Yuqing and He, Wenzhang},
title = {An Approach of Traffic Accident Scene Reconstruction Using Unmanned Aerial Vehicle Photogrammetry},
year = {2019},
isbn = {9781450372435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365245.3365247},
doi = {10.1145/3365245.3365247},
abstract = {Accurate and detailed information of traffic accident scene is important for accident
investigation, and the current investigation methods (tape measuring and total station
survey) always need highway closure, and their working process is time-consuming.
From different angles or at different altitudes, unmanned aerial vehicle (UAV) can
monitor accident site without interrupting the traffic flow, therefore, UAV is introduced
for accident scene reconstruction. Firstly, the method framework of accident scene
reconstruction was proposed, in which UAV was used to take pictures of accident site,
and imaging system was adopted to reconstruct the 2D and 3D accident scene. Then,
3D reconstruction, point cloud generation, and model optimization were presented.
Next, a UAV flight experiment was conducted for traffic accident scene reconstruction,
and two evaluation indexes, signal-to-noise ratio and structural similarity, were
introduced to assess the image quality of accident scene reconstruction. The case
study demonstrates that compared with current methods, the proposed method is efficient;
moreover, the effect of accident scene reconstruction is satisfactory.},
booktitle = {Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing},
pages = {31–34},
numpages = {4},
keywords = {unmanned aerial vehicle, traffic investigation, Scene reconstruction, photogrammetry},
location = {Prague, Czech Republic},
series = {SSIP 2019}
}

@inproceedings{10.1145/3291064.3291070,
author = {Thirunavukkarasu, Gokul Sidarth and Champion, Benjamin and Horan, Ben and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
title = {IoT-Based System Health Management Infrastructure as a Service},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291070},
doi = {10.1145/3291064.3291070},
abstract = {Customization, enhanced quality of streamlined maintenance services and uplifted productivity
are some of the key highlights from the rapidly evolving concept of Industry 4.0.
IoT (Internet of things) based service infrastructure models designed for delivering
enterprise services with capabilities of pro-actively sensing malfunctions and responding
with preventive measures to streamline the automated service offered is one of the
prime application of this concept. Continuous maintenance services increase the optimum
through-life cost and in-service life cycle of the product providing the customer
with the feel of full ownership. In-service feedbacks also help the manufactures to
identify issues with respect to the designs and improve it in the future versions.
In this paper, as a proof of concept a cloud-based IoT service infrastructure for
providing real-time prognostic and supervised vehicle maintenance system is proposed.
This proposed system aims at providing an enterprise service infrastructure to the
registered vehicle service centers to keep track of the real-time vehicle diagnostic
information of their client's vehicle over cloud and use prognostic algorithms to
identify any malfunctions or abnormal behavior of the vehicles for automatically scheduling
a service appointment and automating the maintenance cycle of the vehicle. In addition
to this, the system provides features like remote supervision and diagnostics maintenance
enabling technicians to fix issues remotely, ensuring streamlined and reliable service.
Initially, before building the proposed prototype system, a few experimental trails
where conducted for analyzing the use of different IoT models used in the development
to identify the best-suited approach. The results indicated that the publisher-subscriber
(NodeJS) based model outperforms the request-response (PHP) based model in terms of
the hits per second and mean request time for an increased number of active users.
The results of the initial tests justify the reason for the using the publisher-subscriber
based IOT architecture. The conceptualized enterprise infrastructure illustrated in
the manuscript aims at providing a streamlined maintenance service.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {55–61},
numpages = {7},
keywords = {streamlined remote supervision, prognostic maintenance, internet of things, System health management infrastructure as a service, vehicle diagnosis},
location = {Singapore, Singapore},
series = {CCIOT 2018}
}

@inproceedings{10.1145/3410992.3410996,
author = {Noura, Mahda and Heil, Sebastian and Gaedke, Martin},
title = {Natural Language Goal Understanding for Smart Home Environments},
year = {2020},
isbn = {9781450387583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410992.3410996},
doi = {10.1145/3410992.3410996},
abstract = {One of the main challenges of the Internet of Things (IoT) is to enable end-users
without technical experience to use, control or monitor smart devices. However, enabling
end-users to interact with these smart devices in an intuitive and natural way becomes
increasingly important as they become more pervasive in our homes, workplaces and
public environments. Voice-based interfaces are the emerging trend to provide a more
natural human-device interaction in smart environments. Such interfaces require Natural
Language Understanding (NLU) approaches to identify the meaning of end-users' voice
inputs. Designing voice interfaces that are not limited to a small, fixed set of pre-defined
commands is far from trivial. Existing voice-based solutions in the smart home domain
either restrict the end-users to follow a strict language pattern, do not support
indirect goals, require a large training dataset, or need a voice assistant located
in the cloud. In this paper, we propose an approach for understanding end-users goals
from voice inputs in smart homes. Our approach alleviates the need for end-users to
learn or remember concrete operations of the devices and specific words/pattern structures
rather it enables them to control their smart homes based on the desired goals (effects).
We evaluate the approach through application to a collection of 253 goals from real
end-users and report on quality metrics. The results demonstrate that our solution
provides a good accuracy, high precision and acceptable recall for understanding end-users
goals in the smart home domain.},
booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
articleno = {1},
numpages = {8},
keywords = {goal recognition, natural language understanding, voice interface, internet of things, smart home},
location = {Malm\"{o}, Sweden},
series = {IoT '20}
}

@article{10.1145/2930659,
author = {Papadopoulos, Alessandro Vittorio and Ali-Eldin, Ahmed and \r{A}rz\'{e}n, Karl-Erik and Tordsson, Johan and Elmroth, Erik},
title = {PEAS: A Performance Evaluation Framework for Auto-Scaling Strategies in Cloud Applications},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/2930659},
doi = {10.1145/2930659},
abstract = {Numerous auto-scaling strategies have been proposed in the past few years for improving
various Quality of Service (QoS) indicators of cloud applications, for example, response
time and throughput, by adapting the amount of resources assigned to the application
to meet the workload demand. However, the evaluation of a proposed auto-scaler is
usually achieved through experiments under specific conditions and seldom includes
extensive testing to account for uncertainties in the workloads and unexpected behaviors
of the system. These tests by no means can provide guarantees about the behavior of
the system in general conditions. In this article, we present a Performance Evaluation
framework for Auto-Scaling (PEAS) strategies in the presence of uncertainties. The
evaluation is formulated as a chance constrained optimization problem, which is solved
using scenario theory. The adoption of such a technique allows one to give probabilistic
guarantees of the obtainable performance. Six different auto-scaling strategies have
been selected from the literature for extensive test evaluation and compared using
the proposed framework. We build a discrete event simulator and parameterize it based
on real experiments. Using the simulator, each auto-scaler’s performance is evaluated
using 796 distinct real workload traces from projects hosted on the Wikimedia foundations’
servers, and their performance is compared using PEAS. The evaluation is carried out
using different performance metrics, highlighting the flexibility of the framework,
while providing probabilistic bounds on the evaluation and the performance of the
algorithms. Our results highlight the problem of generalizing the conclusions of the
original published studies and show that based on the evaluation criteria, a controller
can be shown to be better than other controllers.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = aug,
articleno = {15},
numpages = {31},
keywords = {Performance evaluation, elasticity, cloud computing, randomized optimization, auto-scaling}
}

@article{10.1109/TNET.2020.2971587,
author = {Cheng, Yingying and Jia, Xiaohua},
title = {NAMP: Network-Aware Multipathing in Software-Defined Data Center Networks},
year = {2020},
issue_date = {April 2020},
publisher = {IEEE Press},
volume = {28},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2971587},
doi = {10.1109/TNET.2020.2971587},
abstract = {Data center networks employ parallel paths to perform load balancing. Existing traffic
splitting schemes propose weighted traffic distribution across multiple paths via
a centralized view. An SDN controller computes the traffic splitting ratio of a flow
group among all the paths, and implements the ratio by creating multiple rules in
the flow table of OpenFlow switches. However, since the number of rules in TCAM-based
flow table is limited, it is not scalable to implement the ideal splitting ratio for
every flow group. Existing solutions, WCMP and Niagara, aim at reducing the maximum
oversubscription of all egress ports and reducing traffic imbalance, respectively.
However, the transmission time of flow groups, which measures the quality of cloud
services, is sub-optimal in existing solutions that ignore heterogeneous network bandwidth.
We propose and implement NAMP, a multipathing scheme considering the network heterogeneity,
to efficiently optimize the transmission time of flow groups. Experimental results
show that NAMP reduces the transmission time by up to 45.4% than Niagara, up
to 50% than WCMP, and up to 60% than ECMP.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {846–859},
numpages = {14}
}

@inproceedings{10.1145/2693561.2693563,
author = {Klein, John and Gorton, Ian},
title = {Runtime Performance Challenges in Big Data Systems},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693563},
doi = {10.1145/2693561.2693563},
abstract = {Big data systems are becoming pervasive. They are distributed systems that include
redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient
to assure runtime performance in production. This is due to the scale of the deployed
system, the continually evolving workloads, and the unpredictable quality of service
of the shared infrastructure. Consequently, a solution for addressing performance
requirements needs sophisticated runtime observability and measurement. Observability
gives real-time insights into a system's health and status, both at the system and
application level, and provides historical data repositories for forensic analysis,
capacity planning, and predictive analytics. Due to the scale and heterogeneity of
big data systems, significant challenges exist in the design, customization and operations
of observability capabilities. These challenges include economical creation and insertion
of monitors into hundreds or thousands of computation and data nodes, efficient, low
overhead collection and storage of measurements (which is itself a big data problem),
and application-aware aggregation and visualization. In this paper we propose a reference
architecture to address these challenges, which uses a model-driven engineering toolkit
to generate architecture-aware monitors and application-specific visualizations.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {17–22},
numpages = {6},
keywords = {observability, big data, model-driven engineering},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@inproceedings{10.1109/IPSN.2018.00025,
author = {Adkins, Joshua and Ghena, Branden and Jackson, Neal and Pannuto, Pat and Rohrer, Samuel and Campbell, Bradford and Dutta, Prabal},
title = {Applications on the Signpost Platform for City-Scale Sensing: Demo Abstract},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00025},
doi = {10.1109/IPSN.2018.00025},
abstract = {City-scale sensing holds the promise of enabling deeper insight into how our urban
environments function. Applications such as observing air quality and measuring traffic
flows can have powerful impacts, allowing city planners and citizen scientists alike
to understand and improve their world. However, the path from conceiving applications
to implementing them is fraught with difficulty. A successful city-scale deployment
requires physical installation, power management, and communications---all challenging
tasks standing between a good idea and a realized one.The Signpost platform, presented
at IPSN 2018, has been created to address these challenges. Signpost enables easy
deployment by relying on harvested, solar energy and wireless networking rather than
their wired counterparts. To further lower the bar to deploying applications, the
platform provides the key resources necessary to support its pluggable sensor modules
in their distributed sensing tasks. In this demo, we present the Signpost hardware
and several applications running on a deployment of Signposts on UC Berkeley's campus,
including distributed, energy-adaptive traffic monitoring and fine grained weather
reporting. Additionally we show the cloud infrastructure supporting the Signpost deployment,
specifically the ability to push new applications and parameters down to existing
sensors, with the goal of demonstrating that the existing deployment can serve as
a future testbed.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {124–125},
numpages = {2},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1145/2851613.2851727,
author = {Megyesi, P\'{e}ter and Botta, Alessio and Aceto, Giuseppe and Pescap\`{e}, Antonio and Moln\'{a}r, S\'{a}ndor},
title = {Available Bandwidth Measurement in Software Defined Networks},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851727},
doi = {10.1145/2851613.2851727},
abstract = {Software Defined Networking (SDN) is an emerging paradigm that is expected to revolutionize
computer networks. With the decoupling of data and control plane and the introduction
of open communication interfaces between layers, SDN enables programmability over
the entire network, promising rapid innovation in this area. The SDN concept was already
proven to work successfully in cloud and data center environments thus the proper
monitoring of such networks is already in the focus of the research community. Methods
for measuring Quality of Service (QoS) parameters such as bandwidth utilization, packet
loss, and delay have been recently introduced in literature, but they lack a solution
for tackling down the question of available bandwidth. In this paper, we attempt to
fill this gap and introduce a novel mechanism for measuring available bandwidth in
SDN networks. We take advantage of the SDN architecture and build an application over
the Network Operating System (NOS). Our application can track the topology of the
network and the bandwidth utilization over the network links, and thus it is able
to calculate the available bandwidth between any two points in the network. We validate
our method using the popular Mininet network emulation environment and the widely
used NOS called Floodlight. We present results providing insights into the measurement
accuracy and showing its relationship with the delay in the control network and the
polling frequency.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {651–657},
numpages = {7},
keywords = {floodlight, network operating system, available bandwidth, OpenFlow, software defined networks, mininet},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3307334.3326087,
author = {Shi, Shu and Gupta, Varun and Jana, Rittwik},
title = {Freedom: Fast Recovery Enhanced VR Delivery Over Mobile Networks},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326087},
doi = {10.1145/3307334.3326087},
abstract = {In this paper we design and implement Freedom, a mobile VR system that deliver high
quality VR content on today's mobile devices using 4G/LTE cellular networks. Compared
to existing state-of-the-art, Freedom does not rely on any video frame pre- rendering
or viewpoint prediction. We send a latency-adaptive VAM frame that contains pixels
around the FoV. This allows the clients to render locally at a high refresh rate of
60 Hz to accommodate and compensate for the user's head movements before the next
server update arrives. We demonstrate that Freedom is the first system in the world
that can support dynamic and live 8K resolution VR content, while adapting to the
real-world latency variations experienced in cellular networks. Compared to streaming
the whole 360° panoramic VR content, we show that Freedom achieves up to 80% bandwidth
savings. Finally, we provide detailed end to end latency measurements of actual VR
systems by running extensive experiments in a private LTE testbed using a Mobile Edge
Cloud (MEC).},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {130–141},
numpages = {12},
keywords = {remote rendering, 360 video, mobile vr, motion-to-update latency, mobile edge cloud},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@inproceedings{10.1145/2964284.2964327,
author = {Wu, Chao and Jia, Jia and Zhu, Wenwu and Chen, Xu and Yang, Bowen and Zhang, Yaoxue},
title = {Affective Contextual Mobile Recommender System},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2964327},
doi = {10.1145/2964284.2964327},
abstract = {Exponential growth of media consumption in online social networks demands effective
recommendation to improve the quality of experience especially for on-the-go mobile
users. By means of large-scale trace-driven measurements over mobile Twitter traces
from users, we reveal the significance of affective features in shaping users' social
media behaviors. Existing recommender systems however, rarely support this psychological
effect in real-life. To capture this effect, in this paper we propose Kaleido, a real
mobile system to achieve an affect-aware learning-based social media recommendation.Specifically,
we design a machine learning mechanism to infer the affective feature within media
contents. Furthermore, a cluster-based latent bias model is provided for jointly training
the affect, behavior and social contexts. Our comprehensive experiments on Android
prototype expose a superior prediction accuracy of 82%, with more than 20% accuracy
improvement over existing mobile recommender systems. Moreover, by enabling users
to offload their machine learning procedures to the deployed edge-cloud testbed, our
system achieves speed-up of a factor of 1,000 against the local data training execution
on smartphones.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1375–1384},
numpages = {10},
keywords = {social networks, affective computing, recommender system, mobile application},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.5555/2602339.2602357,
author = {Misra, Prasant Kumar and Hu, Wen and Jin, Yuzhe and Liu, Jie and Souza de Paula, Amanda and Wirstrom, Niklas and Voigt, Thiemo},
title = {Energy Efficient GPS Acquisition with Sparse-Gps},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {Following rising demands in positioning with GPS, low-cost receivers are becoming
widely available; but their energy demands are still too high. For energy efficient
GPS sensing in delay-tolerant applications, the possibility of offloading a few milliseconds
of raw signal samples and leveraging the greater processing power of the cloud for
obtaining a position fix is being actively investigated. In an attempt to reduce the
energy cost of this data offloading operation, we propose Sparse-GPS1: a new computing
framework for GPS acquisition via sparse approximation. Within the framework, GPS
signals can be efficiently compressed by random ensembles. The sparse acquisition
information, pertaining to the visible satellites that are embedded within these limited
measurements, can subsequently be recovered by our proposed representation dictionary.
By extensive empirical evaluations, we demonstrate the acquisition quality and energy
gains of Sparse-GPS. We show that it is twice as energy efficient than offloading
uncompressed data, and has 5-10 times lower energy costs than standalone GPS; with
a median positioning accuracy of 40 m.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {155–166},
numpages = {12},
keywords = {sparse approximation, energy efficiency, synchronization, location sensing, compressed sensing, gps},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3240508.3240620,
author = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Eerik\"{a}inen, Jukka and Yl\"{a}-J\"{a}\"{a}ski, Antti},
title = {CloudVR: Cloud Accelerated Interactive Mobile Virtual Reality},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240620},
doi = {10.1145/3240508.3240620},
abstract = {High quality immersive Virtual Reality experience currently requires a PC setup with
cable connected head mounted display, which is expensive and restricts user mobility.
This paper presents CloudVR which is a system for cloud accelerated interactive mobile
VR. It is designed to provide short rotation and interaction latencies through panoramic
rendering and dynamic object placement. CloudVR also includes rendering optimizations
to reduce server-side computational load and bandwidth requirements between the server
and client. Performance measurements with a CloudVR prototype suggest that the optimizations
make it possible to double the server's framerate and halve the amount of bandwidth
required and that small objects can be quickly moved at run time to client device
for rendering to provide shorter interaction latency. A small-scale user study indicates
that CloudVR users do not notice small network latencies (20ms) and even much longer
ones (100-200ms) become non-trivial to detect when they do not affect the interaction
with objects. Finally, we present a design of CloudVR extension to multi-user scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1181–1189},
numpages = {9},
keywords = {rendering, unity, virtual reality, edge computing, cloud, optimization},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3293320.3293326,
author = {Kaliszan, Damian and F\"{u}rst, Steffen and Gienger, Michael and Gogolenko, Sergiy and Meyer, Norbert and Petruczynik, Sebastian},
title = {Comparative Benchmarking of HPC Systems for GSS Applications: GSS Applications in the HPC Ecosystem},
year = {2019},
isbn = {9781450366328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293320.3293326},
doi = {10.1145/3293320.3293326},
abstract = {The work undertaken in this paper was done in the Centre of Excellence for Global
Systems Science (CoeGSS), an interdisciplinary project, funded by the European Commission.The
project provides decision-support in the face of global challenges. It brings together
HPC and global systems science. This paper presents a proposition of GSS benchmark
with the aim to find the most suitable HPC architecture and the best HPC system which
allows to run GSS applications effectively. The GSS provides evidence about global
systems challenges, e.g. the network structure of the world economy, energy, water
and food supply systems, the global financial system or the global city system, and
the scientific community.The outcome of the analysis is defining a benchmark which
represents the GSS environment in the best way. Three exemplary challenges were defined
as pilot applications: Health Habits, Green Growth and Global Urbanisation extended
with additional applications from GSS ecosystem: Iterative proportional fitting (IPF),
Data rastering - a preprocessing process converting all vectorial representations
of georeferenced data into raster files to be later used as simulation input, Weather
Research and Forecasting (WRF) model, CMAQ/CCTM (Community Air Multiscale Quality
Modelling System/The CMAQ Chemistry-Transport Mode), CM1 (Cloud Modelling), ABMS (Agent-based
Modelling and Simulation), OpenSWPC (An Open-source Seismic Wave Propagation Code).
The above list seems to be quite rich and reflects the real GSS world as much as possible,
having in mind, for example the real-world applications availability.Additionally,
the authors tested new HPC platforms based on Intel® Xeon® Gold 6140, AMD EpycTM,
ARM Hi1616 and IBM Power8+. Due to the hardware availability, the testbed consisted
of a limited number of nodes. This restricted the ability to provide full tests of
scalability for given applications. However, this small number of available computational
units (cores) can provide valuable outcome including architecture comparison for different
applications based on execution times, TDPs1 and TCO2. These are the basic metrics
used for providing a ranking of HPC architectures. Finally, this document is thought
to be valuable information for the GSS community for future purposes and analysis
to determine their specific demands as well as - in general - to help develop a mature
final benchmark set reflecting the GSS environment requirements and specialty. As
none of the existing benchmarks is dedicated to the GSS community, the authors decided
to create one by calling it a GSS benchmark to serve and help GSS users in their future
work.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {43–52},
numpages = {10},
keywords = {Global Systems Science, HPC benchmarks, parallel applications, e-Infrastructure evaluation},
location = {Guangzhou, China},
series = {HPC Asia 2019}
}

@inproceedings{10.1145/3383812.3383823,
author = {Vani, K. Suvarna and M., Arul Raj and M., Padmaja and Kumar, K. Praveen and A., Jitendra and A., Ravi Raja},
title = {Detection and Extraction of Roads Using Cartosat-2 High Resolution Satellite Imagery},
year = {2020},
isbn = {9781450377201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383812.3383823},
doi = {10.1145/3383812.3383823},
abstract = {The extraction of roads from panchromatic images has many insightful applications
in the fields of urban planning, setting up of transportation, disaster management
and cartography in geographical information systems (GIS). The process of extracting
roads from high-resolution images is composite, due to the presence of different noises
(i.e., buildings, shadows, clouds etc.). Various image processing techniques and various
quality measures are applied on the high-resolution remote sensing satellite images
to improve the quality of the image and interactively extract the information of roads.
Cartosat-2 images available in Bhuvan website of ISRO are taken for testing the validity
of proposed method. The proposed method enhances the images using Contrast Limited
Adaptive Histogram Equalization (CLAHE) and Line Detector for detecting road segments.
Connected component analysis (CCA) is performed on segmented image for connection
of disconnected objects in the segmented image. Morphological operations fill the
holes caused by the presence of shadows, buildings and trees on the road surface.},
booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
pages = {7–11},
numpages = {5},
keywords = {cartosat-2 dataset, line segment detector, morphological operations, GIS, image processing},
location = {Singapore, Singapore},
series = {ICIGP 2020}
}

@article{10.1109/TNET.2014.2354262,
author = {Adhikari, Vijay K. and Guo, Yang and Hao, Fang and Hilt, Volker and Zhang, Zhi-Li and Varvello, Matteo and Steiner, Moritz},
title = {Measurement Study of Netflix, Hulu, and a Tale of Three CDNs},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2014.2354262},
doi = {10.1109/TNET.2014.2354262},
abstract = {Netflix and Hulu are leading Over-the-Top (OTT) content service providers in the US
and Canada. Netflix alone accounts for 29.7% of the peak downstream traffic in the
US in 2011. Understanding the system architectures and performance of Netflix and
Hulu can shed light on the design of such large-scale video streaming platforms, and
help improving the design of future systems. In this paper, we perform extensive measurement
study to uncover their architectures and service strategies. Netflix and Hulu bear
many similarities. Both Netflix and Hulu video streaming platforms rely heavily on
the third-party infrastructures, with Netflix migrating that majority of its functions
to the Amazon cloud, while Hulu hosts its services out of Akamai. Both service providers
employ the same set of three content distribution networks (CDNs) in delivering the
video contents. Using active measurement study, we dissect several key aspects of
OTT streaming platforms of Netflix and Hulu, e.g., employed streaming protocols, CDN
selection strategy, user experience reporting, etc. We discover that both platforms
assign the CDN to a video request without considering the network conditions and optimizing
the user-perceived video quality. We further conduct the performance measurement studies
of the three CDNs employed by Netflix and Hulu. We show that the available bandwidths
on all three CDNs vary significantly over the time and over the geographic locations.
We propose a measurement-based adaptive CDN selection strategy and a multiple-CDN-based
video delivery strategy that can significantly increase users' average available bandwidth.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {1984–1997},
numpages = {14},
keywords = {Hulu, Netflix, over-the-top (OTT) content service, CDN selection strategy, content distribution networks (CDN), video streaming}
}

@inproceedings{10.1145/2683405.2683409,
author = {Nguyen, Hoang Minh and W\"{u}nsche, Burkhard and Delmas, Patrice and Lutteroth, Christof},
title = {Identifying Low Confidence Mesh Regions: Uncertainty Measures and Segmentation},
year = {2014},
isbn = {9781450331845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2683405.2683409},
doi = {10.1145/2683405.2683409},
abstract = {3D digital models have become an important part of diverse applications ranging from
computer games, virtual reality, architectural design to visual impact studies. One
common method to create 3D models is to create a point cloud using laser scanners,
structured lighting sensors, or image-based modelling techniques, and then construct
a 3D mesh, and texture-map it using photographs of the observed scene. Attributed
to the inherent properties of general 3D scenes such as occluded or inaccessible parts,
reflective surfaces, lighting conditions or poor-quality inputs, 3D models produced
by these approaches often exhibit unsatisfactory and erroneous mesh regions. In many
cases, it is desirable to identify and extract such regions so that they can be constructed
or corrected through other means. While much effort has been invested into the problem
of 3D reconstructions, the task of evaluating existing models and preparing them for
subsequent enhancement processes has been largely neglected. In this paper, we present
a novel method for automatically detecting and segmenting mesh regions with low confidence
in their correctness. The confidence estimation is achieved by exploiting and integrating
various uncertainty measures such as geometric distances, normal variations and texture
discrepancies. Low-confidence mesh regions are isolated and removed in such a way
that the extracted region's boundary is as simple as possible in order to facilitate
subsequent automatic or manual improvement of these regions. Segmentation is achieved
by minimising an energy function that takes the genus and boundary length and smoothness
of the extracted regions into account.},
booktitle = {Proceedings of the 29th International Conference on Image and Vision Computing New Zealand},
pages = {48–53},
numpages = {6},
keywords = {Uncertainty Measure, Mesh Classification, 3D Reconstruction},
location = {Hamilton, New Zealand},
series = {IVCNZ '14}
}

@inproceedings{10.1145/3323503.3349545,
author = {de Amorim, Irandir O. and de Melo, Jose F. V. and Balieiro, Andson M. and Santos, Bruno B. dos},
title = {An Evolutionary Approach for Video Application Energy Consumption Estimation in Mobile Devices},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3349545},
doi = {10.1145/3323503.3349545},
abstract = {In the last years, the multimedia traffic has increased significantly and the mobile
devices (e.g. smart phones and tablets) have been widely used to consume this content
type. Video applications demand high energy consumption of the device because they
perform complex operations and deal with a large data amount. Although hardware improvements
in the mobile devices have been achieved, the advances in battery technology have
not kept the same pace. In this respect, the combination of video applications with
the limited battery capacity of the mobile devices has challenged the academia and
industry in the development of techniques for energy management and provision of quality
of experience (QoE) to the user. Energy consumption estimation models may assist these
techniques, as well as, the decision made process when the computational offloading
from the mobile device to the cloud is considered. This paper presents an evolutionary
approach based on Genetic Algorithms (GAs) and Swarm Particle Optimization (PSO) for
energy consumption estimation in mobile devices running video applications. The proposal
is directly applicable to different model types (linear and non-linear ones), without
the linearization cost, and it is evaluated in terms of mean squared error (MSE),
using energy consumption measurement data of videos with different configurations.
Results show the superiority of our proposal in comparison to the literature that
adopts the Ordinary Least Squares method.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {169–175},
numpages = {7},
keywords = {energy consumption model for mobile devices, genetic algorithms, particle swarm optimization, video application},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/2639108.2639118,
author = {Li, Liqun and Shen, Guobin and Zhao, Chunshui and Moscibroda, Thomas and Lin, Jyh-Han and Zhao, Feng},
title = {Experiencing and Handling the Diversity in Data Density and Environmental Locality in an Indoor Positioning Service},
year = {2014},
isbn = {9781450327831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639108.2639118},
doi = {10.1145/2639108.2639118},
abstract = {Diversity in training data density and environment locality is intrinsic in the real-world
deployment of indoor localization systems and has a major impact on the performance
of existing localization approaches. In this paper, through micro-benchmarks, we find
that fingerprint-based approaches are preferable in scenarios where a dense database
is available; while model-based approaches are the method of choice in the case of
sparse data. It should be noted, however, that practical situations are complex. A
single deployment often features both sparse and dense sampled areas. Furthermore,
the internal layout affects the propagation of radio signals and exhibits environmental
impacts. A certain number of measurement samples may be sufficient for one part of
the building, but entirely insufficient for another. Thus, finding the right indoor
localization algorithm for a given large-scale deployment is challenging, if not impossible;
there is no one-size-fits-all indoor localization approach.Realizing the fundamental
fact that the quality of the location database capturing the actual radio map dictates
localization accuracy, in this paper, we propose Modellet, an algorithmic approach
that optimally approximates the actual radio map by unifying model-based and fingerprint-based
approaches. Modellet represents the radio map using a fingerprint-cloud that incorporates
both measured real fingerprints and virtual fingerprints, which are computed from
models with a local support, based on the key concept of the supporting set. We evaluate
Modellet with data collected from an office building as well as 13 large-scale deployment
venues (shopping malls and airports), located across China, U.S., and Germany. Comparing
Modellet with two representative baseline approaches, RADAR and EZPerfect, demonstrates
that Modellet effectively adapts to different data densities and environmental conditions,
substantially outperforming existing approaches.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Computing and Networking},
pages = {459–470},
numpages = {12},
keywords = {fingerprint, indoor localization, model},
location = {Maui, Hawaii, USA},
series = {MobiCom '14}
}

@inproceedings{10.1109/CCGrid.2014.50,
author = {Tolosana-Calasanz, Rafael and Ba\~{n}ares, Jos\'{e} \'{A}ngel and Rana, Omer and Pham, Congduc and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
title = {Enforcing Quality of Service on OpenNebula-Based Shared Clouds},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.50},
doi = {10.1109/CCGrid.2014.50},
abstract = {With an increase in the number of monitoring sensors deployed on physical infrastructures,
there is a corresponding increase in data volumes that need to be processed. Data
measured or collected by sensors is typically processed at destination or "in-transit"
(i.e. from data capture to delivery to a user). When such data are processed in-transit
over a shared distributed computing infrastructure, it is useful to provide elastic
computational capability which can be adapted based on processing requirements and
demand. Where Service Level Agreements (SLAs) have been pre-agreed, such available
computational capacity needs to be shared in such a way that any Quality of Service
related constraints in such SLAs are not violated. This is particularly challenging
for time critical applications and with highly variable and unpredictable rates of
data generation (e.g. in Smart Grid applications where energy usage patterns may change
unpredictably). Previously, we proposed a Reference net based architectural model
for supporting QoS for multiple concurrent data streams being processed (prior to
delivery to a user) over a shared infrastructure. In this paper, we describe a practical
realisation of this architecture using the OpenNebula Cloud platform. We consider
our infrastructure to be composed of a number of nodes, each of which has multiple
processing units and data buffers. We utilize the "token bucket" model for regulating,
on a per stream basis, the data injection rate into each node. We subsequently demonstrate
how a streaming pipeline can be supported and managed using a dynamic control strategy
at each node.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {651–659},
numpages = {9},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/2716281.2836120,
author = {Kateja, Rajat and Baranasuriya, Nimantha and Navda, Vishnu and Padmanabhan, Venkata N.},
title = {DiversiFi: Robust Multi-Link Interactive Streaming},
year = {2015},
isbn = {9781450334129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2716281.2836120},
doi = {10.1145/2716281.2836120},
abstract = {Real-time, interactive streaming for applications such as audio-video conferencing
(e.g., Skype) and cloud-based gaming depends critically on the network providing low
latency, jitter, and packet loss, much more so than on-demand streaming (e.g., YouTube)
does. However, WiFi networks pose a challenge; our analysis of data from a large VoIP
provider and from our own measurements shows that the WiFi access link is a significant
cause of poor streaming experience.To improve streaming quality over WiFi, we present
DiversiFi, which takes advantage of the diversity of WiFi links available in the vicinity,
even when the individual links are poor. Leveraging such cross-link spatial and channel
diversity outperforms both traditional link selection and the temporal diversity arising
from retransmissions on the same link. It also provides significant gains over and
above the PHY-layer spatial diversity provided by MIMO. Our experimental evaluation
shows that, for a client with two NICs, enabling replication across two WiFi links
helps cut down the poor call rate (PCR) for VoIP by 2.24x.Finally, we present the
design and implementation of DiversiFi, which enables it to operate with single-NIC
clients, and with either minimally modified APs or unmodified APs augmented with a
middlebox. Over 61 runs, where the baseline average PCR is 4.9%, DiversiFi running
with a single NIC, switching between two links, helps cut the PCR down to 0%, while
duplicating wastefully only 0.62% of the packets and impacting competing TCP throughput
by only 2.5%. Thus, DiversiFi provides the benefit of multi-link diversity for real-time
interactive streaming in a manner that is deployable and imposes little overhead,
thereby ensuring coexistence with other applications.},
booktitle = {Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies},
articleno = {35},
numpages = {13},
keywords = {multi-path, wi-fi, VoIP, real-time streaming},
location = {Heidelberg, Germany},
series = {CoNEXT '15}
}

@proceedings{10.1145/2789168,
title = {MobiCom '15: Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM MobiCom 2015, the 21st Annual International Conference on Mobile Computing
and Networking. MobiCom is the premier forum for publishing and presenting cutting-edge
research in mobile systems and wireless networks. The technical program this year
features 38 outstanding papers that cover a wide variety of topics including energy,
sensing, security, wireless access, applications, localization, Internet of things,
mobile cloud, measurement and analysis. We created a new Experience track this year
to encourage authors to present extensive experiences with implementation, deployment,
and operations of mobile ncomputing and wireless networks. One of the accepted papers
is an Experience paper on cellular networks.This year's call for papers attracted
207 qualified submissions from across the globe that were carefully reviewed by 46
Technical Program Committee (TPC) members (+2 TPC chairs) along with a selected group
of external experts. The TPC was formed with the goal of covering diverse research
expertise as well as diverse perspectives and approaches. The TPC included researchers
from 12 countries including China, France, Germany, India, Italy, Singapore, South
Korea, Spain, Sweden, Switzerland, UK, and USA. 25% of the members were female, the
highest ever in the history of MobiCom. We also had broad industry participation with
TPC members from Alcatel-Lucent, Google, HP, IBM, Microsoft, NEC, and Telefonica.The
paper review process was double-blinded and carried out in three phases. In the first
phase, each paper was reviewed by at least three TPC members, and the top 112 papers
were selected for the second phase. In addition to reviewer scores, reviewer confidence
and normalization with respect to other papers in a reviewer's pile, were also considered
in selecting papers. In the second phase, each paper was reviewed by at least two
more reviewers followed by an online, often intense, discussion, producing 68 papers
for the final phase. The final TPC meeting was held on May 28th and 29th in Salt Lake
City, Utah. These 68 papers were organized by their topic areas, and discussed at
length at the meeting. Eventually, 38 papers were shortlisted for inclusion in the
program and a shepherd from the TPC was assigned to each of these papers. As the last
step, each of the shortlisted papers was shepherded through a "blind" process where
the authors interacted with all the reviewers and the shepherd to address the review
comments without knowing the reviewers' or the shepherds' identities. The end result
is an exciting technical program composed of 38 very high quality papers.During the
review process, Prof. Robin Kravets, the TPC co-chair of MobiCom 2013, handled the
papers that were co-authored by TPC chairs, and those that had conflict-of-interest
with both TPC chairs. To ensure fairness and preserve the anonymity of all authors
and reviewers, the assignment of reviewers, the reviews and discussions of these papers
were done out of band without any exposure to the TPC chairs.},
location = {Paris, France}
}

@article{10.1145/3450626.3459679,
author = {Ma, Xiaohe and Kang, Kaizhang and Zhu, Ruisheng and Wu, Hongzhi and Zhou, Kun},
title = {Free-Form Scanning of Non-Planar Appearance with Neural Trace Photography},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459679},
doi = {10.1145/3450626.3459679},
abstract = {We propose neural trace photography, a novel framework to automatically learn high-quality
scanning of non-planar, complex anisotropic appearance. Our key insight is that free-form
appearance scanning can be cast as a geometry learning problem on unstructured point
clouds, each of which represents an image measurement and the corresponding acquisition
condition. Based on this connection, we carefully design a neural network, to jointly
optimize the lighting conditions to be used in acquisition, as well as the spatially
independent reconstruction of reflectance from corresponding measurements. Our framework
is not tied to a specific setup, and can adapt to various factors in a data-driven
manner. We demonstrate the effectiveness of our framework on a number of physical
objects with a wide variation in appearance. The objects are captured with a light-weight
mobile device, consisting of a single camera and an RGB LED array. We also generalize
the framework to other common types of light sources, including a point, a linear
and an area light.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {124},
numpages = {13},
keywords = {illumination multiplexing, SVBRDF, optimal lighting pattern}
}

