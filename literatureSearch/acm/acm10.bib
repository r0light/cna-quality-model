@inproceedings{10.1145/2980258.2980451,
author = {Bhattacharya, Adrija and Choudhury, Sankhayan},
title = {An Efficient Service Selection Approach through a Goodness Measure of the Participating QoS},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2980451},
doi = {10.1145/2980258.2980451},
abstract = {The service repository in cloud consists of atomic services those need to be composed
as per the requirement of consumers. In general, various providers offer different
atomic services with same functionalities. These are called similar services and the
service selection is the process to choose the best one among them based on the associated
Quality of Services. Thus a service selection problem for satisfying the requirement
of a consumer with given constraints is conceptualized as a multi-objective optimization
problem. Sometime it involves the objectives that have conflict among them and as
a result the complexity of the problem increases. In such cases users are requested
to provide the feedback on the required QoS and accordingly the solution is offered.
This demands sufficient domain knowledge from a user that may not be feasible in real
cases. As a result the offered solution may deviate from the intended one. In this
work we have proposed a method to calculate an overall measure of a service considering
all QoS. It converts the multi-objective problem to single objective. This reduces
the exponential complexity of NP-Hard problem into a problem solvable in polynomial
time. The proposed Service Selection algorithm does not require any feedback from
the users. The algorithm is capable to offer a moderate solution to users considering
all requested QoS. The experiment shows that almost in every case the proposed algorithm
is able to deliver a solution satisfying all QoS as referred by a user.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {94},
numpages = {6},
keywords = {Service Selection, Goodness, QoS},
location = {Pondicherry, India},
series = {ICIA-16}
}

@article{10.1145/3038919,
author = {Olson, Judith S. and Wang, Dakuo and Olson, Gary M. and Zhang, Jingwen},
title = {How People Write Together Now: Beginning the Investigation with Advanced Undergraduates in a Project Course},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3038919},
doi = {10.1145/3038919},
abstract = {Today's commercially available word processors allow people to write collaboratively
in the cloud, both in the familiar asynchronous mode and now in synchronous mode as
well. This opens up new ways of working together. We examined the data traces of collaborative
writing behavior in student teams’ use of Google Docs to discover how they are writing
together now. We found that student teams write both synchronously and asynchronously,
take fluid roles in the writing and editing of the documents, and show a variety of
styles of collaborative writing, including writing from scratch, beginning with an
outline, pasting in a related example as a template to organize their own writing,
and three more. We also found that the document serves as a place where they share
a number of things not included in the final document, including links or references
to related materials, the assignment requirements from the instructor, and informal
discussions to coordinate the collaboration or to structure the document. We computed
a number of measures to depict a group's collaboration behavior and asked external
graders to score these documents for quality. We found that the documents that included
balanced participation and/or exhibited leadership were judged higher in quality,
as were those that were longer. We then suggested system design implications and behavioral
guidelines to support people writing together better, and concluded the paper with
future research directions.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = mar,
articleno = {4},
numpages = {40},
keywords = {Google docs, collaboration, co-authoring, writing style}
}

@inproceedings{10.1145/3448891.3448918,
author = {Du, Yifan and Sailhan, Fran\c{c}oise and Issarny, Val\'{e}rie},
title = {IAM&nbsp;– Interpolation and Aggregation on the Move: Collaborative Crowdsensing for Spatio-Temporal Phenomena},
year = {2020},
isbn = {9781450388405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448891.3448918},
doi = {10.1145/3448891.3448918},
abstract = { Crowdsensing allows citizens to contribute to the monitoring of their living environment
using the sensors embedded in their mobile devices, e.g., smartphones. However, crowdsensing
at scale involves significant communication, computation, and financial costs due
to the dependence on cloud infrastructures for the analysis (e.g., interpolation and
aggregation) of spatio-temporal data. This limits the adoption of crowdsensing by
activists although sorely needed to inform our knowledge of the environment. As an
alternative to the centralized analysis of crowdsensed observations, this paper introduces
a fully distributed interpolation-mediated aggregation approach running on smartphones.
To achieve so efficiently, we model the interpolation as a distributed tensor completion
problem, and we introduce a lightweight aggregation strategy that anticipates the
likelihood of future encounters according to the quality of the interpolation. Our
approach thus shifts the centralized post-processing of crowdsensed data to distributed
pre-processing on the move, based on opportunistic encounters of crowdsensors through
state-of-the-art D2D networking. The evaluation using a dataset of quantitative environmental
measurements collected from 550 crowdsensors over 1 year shows that our solution significantly
reduces –and may even eliminate– the dependence on the cloud infrastructure, while
it incurs a limited resource cost on end devices. Meanwhile, the overall data accuracy
remains comparable to that of the centralized approach.},
booktitle = {MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {337–346},
numpages = {10},
keywords = {Aggregation, Interpolation, Opportunistic Relay, Crowdsensing, Pervasive Computing, Ubiquitous Sensing},
location = {Darmstadt, Germany},
series = {MobiQuitous '20}
}

@article{10.1145/3165713,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3165713},
doi = {10.1145/3165713},
abstract = {Although the ultimate objective of Linked Data is linking and integration, it is not
currently evident how connected the current Linked Open Data (LOD) cloud is. In this
article, we focus on methods, supported by special indexes and algorithms, for performing
measurements related to the connectivity of more than two datasets that are useful
in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference,
i.e., for obtaining complete information about a set of entities, including provenance
information; (c) Data Quality Assessment and Improvement, i.e., for assessing the
connectivity between any set of datasets and monitoring their evolution over time,
as well as for estimating data veracity; (d) Dataset Visualizations; and various other
tasks. Since it would be prohibitively expensive to perform all these measurements
in a na\"{\i}ve way, in this article, we introduce indexes (and their construction algorithms)
that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix
index, (ii) a sameAs catalog for computing the symmetric and transitive closure of
the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware
element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based
incremental algorithms for speeding up the computation of the intersection of URIs
of any set of datasets. For enhancing scalability, we propose parallel index construction
algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved
speedup using either a single machine or a cluster of machines, and we provide insights
regarding the factors that affect efficiency. Finally, we report measurements about
the connectivity of the (billion triples-sized) LOD cloud that have never been carried
out so far.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {15},
numpages = {49},
keywords = {spark, Data quality, connectivity, lattice of measurements, big data, dataset selection, mapreduce, dataset discovery, linked data}
}

@inproceedings{10.5555/3233397.3233412,
author = {Perez-Palacin, Diego and Mirandola, Raffaela and Monterisi, Federico and Montoli, Andrea},
title = {QoS-Driven Probabilistic Runtime Evaluations of Virtual Machine Placement on Hosts},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {We tackle the cloud providers challenge of virtual machine placement when the client
experienced Quality of Service (QoS) is of paramount importance and resource demand
of virtual machines varies over time. To this end, this work investigates approaches
that leverage measured dynamic data for placement decisions. Relying on dynamic data
to guide decisions has, on the one hand, the potential to optimize hardware utilization,
while, on the other hand, increases the risk on the provided QoS. In this context,
we present three probabilistic methods for evaluation of host suitability to allocate
new virtual machines. We also present experiments results that illustrate the differences
in the outcomes of presented approaches.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {90–94},
numpages = {5},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1145/3233547.3233666,
author = {Kotlar, Alex V. and Wingo, Thomas S.},
title = {Tutorial: Rapidly Identifying Disease-Associated Rare Variants Using Annotation and Machine Learning at Whole-Genome Scale Online},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233666},
doi = {10.1145/3233547.3233666},
abstract = {Accurately identifying disease-associated alleles from large sequencing experiments
remains challenging. During this tutorial, participants will learn how to use a new
variant annotation and filtering web app called Bystro (https://bystro.io/) to analyze
sequencing experiments. Bystro is the first online, cloud-based application that makes
variant annotation and filtering accessible to all researchers for even the largest,
terabyte-sized whole-genome experiments containing thousands of samples. Using its
general-purpose, natural-language filtering engine, attendees will be shown how to
perform quality control measures and identify alleles of interest. They will then
be guided in exporting those variants, and using them in both a regression context
by performing rare-variant association tests in R, as well as classification context
by training new machine learning models in Python's scikit-learn library.},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {558},
numpages = {1},
keywords = {rare-variant association tests, variant classification, bioinformatics, machine learning},
location = {Washington, DC, USA},
series = {BCB '18}
}

@inproceedings{10.1145/3365245.3365247,
author = {Liu, Xiaofeng and Zou, Hui and Niu, Wanyu and Song, Yuqing and He, Wenzhang},
title = {An Approach of Traffic Accident Scene Reconstruction Using Unmanned Aerial Vehicle Photogrammetry},
year = {2019},
isbn = {9781450372435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365245.3365247},
doi = {10.1145/3365245.3365247},
abstract = {Accurate and detailed information of traffic accident scene is important for accident
investigation, and the current investigation methods (tape measuring and total station
survey) always need highway closure, and their working process is time-consuming.
From different angles or at different altitudes, unmanned aerial vehicle (UAV) can
monitor accident site without interrupting the traffic flow, therefore, UAV is introduced
for accident scene reconstruction. Firstly, the method framework of accident scene
reconstruction was proposed, in which UAV was used to take pictures of accident site,
and imaging system was adopted to reconstruct the 2D and 3D accident scene. Then,
3D reconstruction, point cloud generation, and model optimization were presented.
Next, a UAV flight experiment was conducted for traffic accident scene reconstruction,
and two evaluation indexes, signal-to-noise ratio and structural similarity, were
introduced to assess the image quality of accident scene reconstruction. The case
study demonstrates that compared with current methods, the proposed method is efficient;
moreover, the effect of accident scene reconstruction is satisfactory.},
booktitle = {Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing},
pages = {31–34},
numpages = {4},
keywords = {unmanned aerial vehicle, traffic investigation, Scene reconstruction, photogrammetry},
location = {Prague, Czech Republic},
series = {SSIP 2019}
}

@inproceedings{10.1145/3291064.3291070,
author = {Thirunavukkarasu, Gokul Sidarth and Champion, Benjamin and Horan, Ben and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
title = {IoT-Based System Health Management Infrastructure as a Service},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291070},
doi = {10.1145/3291064.3291070},
abstract = {Customization, enhanced quality of streamlined maintenance services and uplifted productivity
are some of the key highlights from the rapidly evolving concept of Industry 4.0.
IoT (Internet of things) based service infrastructure models designed for delivering
enterprise services with capabilities of pro-actively sensing malfunctions and responding
with preventive measures to streamline the automated service offered is one of the
prime application of this concept. Continuous maintenance services increase the optimum
through-life cost and in-service life cycle of the product providing the customer
with the feel of full ownership. In-service feedbacks also help the manufactures to
identify issues with respect to the designs and improve it in the future versions.
In this paper, as a proof of concept a cloud-based IoT service infrastructure for
providing real-time prognostic and supervised vehicle maintenance system is proposed.
This proposed system aims at providing an enterprise service infrastructure to the
registered vehicle service centers to keep track of the real-time vehicle diagnostic
information of their client's vehicle over cloud and use prognostic algorithms to
identify any malfunctions or abnormal behavior of the vehicles for automatically scheduling
a service appointment and automating the maintenance cycle of the vehicle. In addition
to this, the system provides features like remote supervision and diagnostics maintenance
enabling technicians to fix issues remotely, ensuring streamlined and reliable service.
Initially, before building the proposed prototype system, a few experimental trails
where conducted for analyzing the use of different IoT models used in the development
to identify the best-suited approach. The results indicated that the publisher-subscriber
(NodeJS) based model outperforms the request-response (PHP) based model in terms of
the hits per second and mean request time for an increased number of active users.
The results of the initial tests justify the reason for the using the publisher-subscriber
based IOT architecture. The conceptualized enterprise infrastructure illustrated in
the manuscript aims at providing a streamlined maintenance service.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {55–61},
numpages = {7},
keywords = {streamlined remote supervision, prognostic maintenance, internet of things, System health management infrastructure as a service, vehicle diagnosis},
location = {Singapore, Singapore},
series = {CCIOT 2018}
}

@inproceedings{10.1145/3410992.3410996,
author = {Noura, Mahda and Heil, Sebastian and Gaedke, Martin},
title = {Natural Language Goal Understanding for Smart Home Environments},
year = {2020},
isbn = {9781450387583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410992.3410996},
doi = {10.1145/3410992.3410996},
abstract = {One of the main challenges of the Internet of Things (IoT) is to enable end-users
without technical experience to use, control or monitor smart devices. However, enabling
end-users to interact with these smart devices in an intuitive and natural way becomes
increasingly important as they become more pervasive in our homes, workplaces and
public environments. Voice-based interfaces are the emerging trend to provide a more
natural human-device interaction in smart environments. Such interfaces require Natural
Language Understanding (NLU) approaches to identify the meaning of end-users' voice
inputs. Designing voice interfaces that are not limited to a small, fixed set of pre-defined
commands is far from trivial. Existing voice-based solutions in the smart home domain
either restrict the end-users to follow a strict language pattern, do not support
indirect goals, require a large training dataset, or need a voice assistant located
in the cloud. In this paper, we propose an approach for understanding end-users goals
from voice inputs in smart homes. Our approach alleviates the need for end-users to
learn or remember concrete operations of the devices and specific words/pattern structures
rather it enables them to control their smart homes based on the desired goals (effects).
We evaluate the approach through application to a collection of 253 goals from real
end-users and report on quality metrics. The results demonstrate that our solution
provides a good accuracy, high precision and acceptable recall for understanding end-users
goals in the smart home domain.},
booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
articleno = {1},
numpages = {8},
keywords = {goal recognition, natural language understanding, voice interface, internet of things, smart home},
location = {Malm\"{o}, Sweden},
series = {IoT '20}
}

@article{10.1145/2930659,
author = {Papadopoulos, Alessandro Vittorio and Ali-Eldin, Ahmed and \r{A}rz\'{e}n, Karl-Erik and Tordsson, Johan and Elmroth, Erik},
title = {PEAS: A Performance Evaluation Framework for Auto-Scaling Strategies in Cloud Applications},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/2930659},
doi = {10.1145/2930659},
abstract = {Numerous auto-scaling strategies have been proposed in the past few years for improving
various Quality of Service (QoS) indicators of cloud applications, for example, response
time and throughput, by adapting the amount of resources assigned to the application
to meet the workload demand. However, the evaluation of a proposed auto-scaler is
usually achieved through experiments under specific conditions and seldom includes
extensive testing to account for uncertainties in the workloads and unexpected behaviors
of the system. These tests by no means can provide guarantees about the behavior of
the system in general conditions. In this article, we present a Performance Evaluation
framework for Auto-Scaling (PEAS) strategies in the presence of uncertainties. The
evaluation is formulated as a chance constrained optimization problem, which is solved
using scenario theory. The adoption of such a technique allows one to give probabilistic
guarantees of the obtainable performance. Six different auto-scaling strategies have
been selected from the literature for extensive test evaluation and compared using
the proposed framework. We build a discrete event simulator and parameterize it based
on real experiments. Using the simulator, each auto-scaler’s performance is evaluated
using 796 distinct real workload traces from projects hosted on the Wikimedia foundations’
servers, and their performance is compared using PEAS. The evaluation is carried out
using different performance metrics, highlighting the flexibility of the framework,
while providing probabilistic bounds on the evaluation and the performance of the
algorithms. Our results highlight the problem of generalizing the conclusions of the
original published studies and show that based on the evaluation criteria, a controller
can be shown to be better than other controllers.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = aug,
articleno = {15},
numpages = {31},
keywords = {Performance evaluation, elasticity, cloud computing, randomized optimization, auto-scaling}
}

@article{10.1109/TNET.2020.2971587,
author = {Cheng, Yingying and Jia, Xiaohua},
title = {NAMP: Network-Aware Multipathing in Software-Defined Data Center Networks},
year = {2020},
issue_date = {April 2020},
publisher = {IEEE Press},
volume = {28},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2971587},
doi = {10.1109/TNET.2020.2971587},
abstract = {Data center networks employ parallel paths to perform load balancing. Existing traffic
splitting schemes propose weighted traffic distribution across multiple paths via
a centralized view. An SDN controller computes the traffic splitting ratio of a flow
group among all the paths, and implements the ratio by creating multiple rules in
the flow table of OpenFlow switches. However, since the number of rules in TCAM-based
flow table is limited, it is not scalable to implement the ideal splitting ratio for
every flow group. Existing solutions, WCMP and Niagara, aim at reducing the maximum
oversubscription of all egress ports and reducing traffic imbalance, respectively.
However, the transmission time of flow groups, which measures the quality of cloud
services, is sub-optimal in existing solutions that ignore heterogeneous network bandwidth.
We propose and implement NAMP, a multipathing scheme considering the network heterogeneity,
to efficiently optimize the transmission time of flow groups. Experimental results
show that NAMP reduces the transmission time by up to 45.4% than Niagara, up
to 50% than WCMP, and up to 60% than ECMP.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {846–859},
numpages = {14}
}

@inproceedings{10.1145/2693561.2693563,
author = {Klein, John and Gorton, Ian},
title = {Runtime Performance Challenges in Big Data Systems},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693563},
doi = {10.1145/2693561.2693563},
abstract = {Big data systems are becoming pervasive. They are distributed systems that include
redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient
to assure runtime performance in production. This is due to the scale of the deployed
system, the continually evolving workloads, and the unpredictable quality of service
of the shared infrastructure. Consequently, a solution for addressing performance
requirements needs sophisticated runtime observability and measurement. Observability
gives real-time insights into a system's health and status, both at the system and
application level, and provides historical data repositories for forensic analysis,
capacity planning, and predictive analytics. Due to the scale and heterogeneity of
big data systems, significant challenges exist in the design, customization and operations
of observability capabilities. These challenges include economical creation and insertion
of monitors into hundreds or thousands of computation and data nodes, efficient, low
overhead collection and storage of measurements (which is itself a big data problem),
and application-aware aggregation and visualization. In this paper we propose a reference
architecture to address these challenges, which uses a model-driven engineering toolkit
to generate architecture-aware monitors and application-specific visualizations.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {17–22},
numpages = {6},
keywords = {observability, big data, model-driven engineering},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@inproceedings{10.1109/IPSN.2018.00025,
author = {Adkins, Joshua and Ghena, Branden and Jackson, Neal and Pannuto, Pat and Rohrer, Samuel and Campbell, Bradford and Dutta, Prabal},
title = {Applications on the Signpost Platform for City-Scale Sensing: Demo Abstract},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00025},
doi = {10.1109/IPSN.2018.00025},
abstract = {City-scale sensing holds the promise of enabling deeper insight into how our urban
environments function. Applications such as observing air quality and measuring traffic
flows can have powerful impacts, allowing city planners and citizen scientists alike
to understand and improve their world. However, the path from conceiving applications
to implementing them is fraught with difficulty. A successful city-scale deployment
requires physical installation, power management, and communications---all challenging
tasks standing between a good idea and a realized one.The Signpost platform, presented
at IPSN 2018, has been created to address these challenges. Signpost enables easy
deployment by relying on harvested, solar energy and wireless networking rather than
their wired counterparts. To further lower the bar to deploying applications, the
platform provides the key resources necessary to support its pluggable sensor modules
in their distributed sensing tasks. In this demo, we present the Signpost hardware
and several applications running on a deployment of Signposts on UC Berkeley's campus,
including distributed, energy-adaptive traffic monitoring and fine grained weather
reporting. Additionally we show the cloud infrastructure supporting the Signpost deployment,
specifically the ability to push new applications and parameters down to existing
sensors, with the goal of demonstrating that the existing deployment can serve as
a future testbed.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {124–125},
numpages = {2},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1145/2851613.2851727,
author = {Megyesi, P\'{e}ter and Botta, Alessio and Aceto, Giuseppe and Pescap\`{e}, Antonio and Moln\'{a}r, S\'{a}ndor},
title = {Available Bandwidth Measurement in Software Defined Networks},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851727},
doi = {10.1145/2851613.2851727},
abstract = {Software Defined Networking (SDN) is an emerging paradigm that is expected to revolutionize
computer networks. With the decoupling of data and control plane and the introduction
of open communication interfaces between layers, SDN enables programmability over
the entire network, promising rapid innovation in this area. The SDN concept was already
proven to work successfully in cloud and data center environments thus the proper
monitoring of such networks is already in the focus of the research community. Methods
for measuring Quality of Service (QoS) parameters such as bandwidth utilization, packet
loss, and delay have been recently introduced in literature, but they lack a solution
for tackling down the question of available bandwidth. In this paper, we attempt to
fill this gap and introduce a novel mechanism for measuring available bandwidth in
SDN networks. We take advantage of the SDN architecture and build an application over
the Network Operating System (NOS). Our application can track the topology of the
network and the bandwidth utilization over the network links, and thus it is able
to calculate the available bandwidth between any two points in the network. We validate
our method using the popular Mininet network emulation environment and the widely
used NOS called Floodlight. We present results providing insights into the measurement
accuracy and showing its relationship with the delay in the control network and the
polling frequency.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {651–657},
numpages = {7},
keywords = {floodlight, network operating system, available bandwidth, OpenFlow, software defined networks, mininet},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3307334.3326087,
author = {Shi, Shu and Gupta, Varun and Jana, Rittwik},
title = {Freedom: Fast Recovery Enhanced VR Delivery Over Mobile Networks},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326087},
doi = {10.1145/3307334.3326087},
abstract = {In this paper we design and implement Freedom, a mobile VR system that deliver high
quality VR content on today's mobile devices using 4G/LTE cellular networks. Compared
to existing state-of-the-art, Freedom does not rely on any video frame pre- rendering
or viewpoint prediction. We send a latency-adaptive VAM frame that contains pixels
around the FoV. This allows the clients to render locally at a high refresh rate of
60 Hz to accommodate and compensate for the user's head movements before the next
server update arrives. We demonstrate that Freedom is the first system in the world
that can support dynamic and live 8K resolution VR content, while adapting to the
real-world latency variations experienced in cellular networks. Compared to streaming
the whole 360° panoramic VR content, we show that Freedom achieves up to 80% bandwidth
savings. Finally, we provide detailed end to end latency measurements of actual VR
systems by running extensive experiments in a private LTE testbed using a Mobile Edge
Cloud (MEC).},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {130–141},
numpages = {12},
keywords = {remote rendering, 360 video, mobile vr, motion-to-update latency, mobile edge cloud},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@inproceedings{10.1145/2964284.2964327,
author = {Wu, Chao and Jia, Jia and Zhu, Wenwu and Chen, Xu and Yang, Bowen and Zhang, Yaoxue},
title = {Affective Contextual Mobile Recommender System},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2964327},
doi = {10.1145/2964284.2964327},
abstract = {Exponential growth of media consumption in online social networks demands effective
recommendation to improve the quality of experience especially for on-the-go mobile
users. By means of large-scale trace-driven measurements over mobile Twitter traces
from users, we reveal the significance of affective features in shaping users' social
media behaviors. Existing recommender systems however, rarely support this psychological
effect in real-life. To capture this effect, in this paper we propose Kaleido, a real
mobile system to achieve an affect-aware learning-based social media recommendation.Specifically,
we design a machine learning mechanism to infer the affective feature within media
contents. Furthermore, a cluster-based latent bias model is provided for jointly training
the affect, behavior and social contexts. Our comprehensive experiments on Android
prototype expose a superior prediction accuracy of 82%, with more than 20% accuracy
improvement over existing mobile recommender systems. Moreover, by enabling users
to offload their machine learning procedures to the deployed edge-cloud testbed, our
system achieves speed-up of a factor of 1,000 against the local data training execution
on smartphones.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1375–1384},
numpages = {10},
keywords = {social networks, affective computing, recommender system, mobile application},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.5555/2602339.2602357,
author = {Misra, Prasant Kumar and Hu, Wen and Jin, Yuzhe and Liu, Jie and Souza de Paula, Amanda and Wirstrom, Niklas and Voigt, Thiemo},
title = {Energy Efficient GPS Acquisition with Sparse-Gps},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {Following rising demands in positioning with GPS, low-cost receivers are becoming
widely available; but their energy demands are still too high. For energy efficient
GPS sensing in delay-tolerant applications, the possibility of offloading a few milliseconds
of raw signal samples and leveraging the greater processing power of the cloud for
obtaining a position fix is being actively investigated. In an attempt to reduce the
energy cost of this data offloading operation, we propose Sparse-GPS1: a new computing
framework for GPS acquisition via sparse approximation. Within the framework, GPS
signals can be efficiently compressed by random ensembles. The sparse acquisition
information, pertaining to the visible satellites that are embedded within these limited
measurements, can subsequently be recovered by our proposed representation dictionary.
By extensive empirical evaluations, we demonstrate the acquisition quality and energy
gains of Sparse-GPS. We show that it is twice as energy efficient than offloading
uncompressed data, and has 5-10 times lower energy costs than standalone GPS; with
a median positioning accuracy of 40 m.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {155–166},
numpages = {12},
keywords = {sparse approximation, energy efficiency, synchronization, location sensing, compressed sensing, gps},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3240508.3240620,
author = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Eerik\"{a}inen, Jukka and Yl\"{a}-J\"{a}\"{a}ski, Antti},
title = {CloudVR: Cloud Accelerated Interactive Mobile Virtual Reality},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240620},
doi = {10.1145/3240508.3240620},
abstract = {High quality immersive Virtual Reality experience currently requires a PC setup with
cable connected head mounted display, which is expensive and restricts user mobility.
This paper presents CloudVR which is a system for cloud accelerated interactive mobile
VR. It is designed to provide short rotation and interaction latencies through panoramic
rendering and dynamic object placement. CloudVR also includes rendering optimizations
to reduce server-side computational load and bandwidth requirements between the server
and client. Performance measurements with a CloudVR prototype suggest that the optimizations
make it possible to double the server's framerate and halve the amount of bandwidth
required and that small objects can be quickly moved at run time to client device
for rendering to provide shorter interaction latency. A small-scale user study indicates
that CloudVR users do not notice small network latencies (20ms) and even much longer
ones (100-200ms) become non-trivial to detect when they do not affect the interaction
with objects. Finally, we present a design of CloudVR extension to multi-user scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1181–1189},
numpages = {9},
keywords = {rendering, unity, virtual reality, edge computing, cloud, optimization},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3293320.3293326,
author = {Kaliszan, Damian and F\"{u}rst, Steffen and Gienger, Michael and Gogolenko, Sergiy and Meyer, Norbert and Petruczynik, Sebastian},
title = {Comparative Benchmarking of HPC Systems for GSS Applications: GSS Applications in the HPC Ecosystem},
year = {2019},
isbn = {9781450366328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293320.3293326},
doi = {10.1145/3293320.3293326},
abstract = {The work undertaken in this paper was done in the Centre of Excellence for Global
Systems Science (CoeGSS), an interdisciplinary project, funded by the European Commission.The
project provides decision-support in the face of global challenges. It brings together
HPC and global systems science. This paper presents a proposition of GSS benchmark
with the aim to find the most suitable HPC architecture and the best HPC system which
allows to run GSS applications effectively. The GSS provides evidence about global
systems challenges, e.g. the network structure of the world economy, energy, water
and food supply systems, the global financial system or the global city system, and
the scientific community.The outcome of the analysis is defining a benchmark which
represents the GSS environment in the best way. Three exemplary challenges were defined
as pilot applications: Health Habits, Green Growth and Global Urbanisation extended
with additional applications from GSS ecosystem: Iterative proportional fitting (IPF),
Data rastering - a preprocessing process converting all vectorial representations
of georeferenced data into raster files to be later used as simulation input, Weather
Research and Forecasting (WRF) model, CMAQ/CCTM (Community Air Multiscale Quality
Modelling System/The CMAQ Chemistry-Transport Mode), CM1 (Cloud Modelling), ABMS (Agent-based
Modelling and Simulation), OpenSWPC (An Open-source Seismic Wave Propagation Code).
The above list seems to be quite rich and reflects the real GSS world as much as possible,
having in mind, for example the real-world applications availability.Additionally,
the authors tested new HPC platforms based on Intel® Xeon® Gold 6140, AMD EpycTM,
ARM Hi1616 and IBM Power8+. Due to the hardware availability, the testbed consisted
of a limited number of nodes. This restricted the ability to provide full tests of
scalability for given applications. However, this small number of available computational
units (cores) can provide valuable outcome including architecture comparison for different
applications based on execution times, TDPs1 and TCO2. These are the basic metrics
used for providing a ranking of HPC architectures. Finally, this document is thought
to be valuable information for the GSS community for future purposes and analysis
to determine their specific demands as well as - in general - to help develop a mature
final benchmark set reflecting the GSS environment requirements and specialty. As
none of the existing benchmarks is dedicated to the GSS community, the authors decided
to create one by calling it a GSS benchmark to serve and help GSS users in their future
work.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {43–52},
numpages = {10},
keywords = {Global Systems Science, HPC benchmarks, parallel applications, e-Infrastructure evaluation},
location = {Guangzhou, China},
series = {HPC Asia 2019}
}

@inproceedings{10.1145/3383812.3383823,
author = {Vani, K. Suvarna and M., Arul Raj and M., Padmaja and Kumar, K. Praveen and A., Jitendra and A., Ravi Raja},
title = {Detection and Extraction of Roads Using Cartosat-2 High Resolution Satellite Imagery},
year = {2020},
isbn = {9781450377201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383812.3383823},
doi = {10.1145/3383812.3383823},
abstract = {The extraction of roads from panchromatic images has many insightful applications
in the fields of urban planning, setting up of transportation, disaster management
and cartography in geographical information systems (GIS). The process of extracting
roads from high-resolution images is composite, due to the presence of different noises
(i.e., buildings, shadows, clouds etc.). Various image processing techniques and various
quality measures are applied on the high-resolution remote sensing satellite images
to improve the quality of the image and interactively extract the information of roads.
Cartosat-2 images available in Bhuvan website of ISRO are taken for testing the validity
of proposed method. The proposed method enhances the images using Contrast Limited
Adaptive Histogram Equalization (CLAHE) and Line Detector for detecting road segments.
Connected component analysis (CCA) is performed on segmented image for connection
of disconnected objects in the segmented image. Morphological operations fill the
holes caused by the presence of shadows, buildings and trees on the road surface.},
booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
pages = {7–11},
numpages = {5},
keywords = {cartosat-2 dataset, line segment detector, morphological operations, GIS, image processing},
location = {Singapore, Singapore},
series = {ICIGP 2020}
}

@article{10.1109/TNET.2014.2354262,
author = {Adhikari, Vijay K. and Guo, Yang and Hao, Fang and Hilt, Volker and Zhang, Zhi-Li and Varvello, Matteo and Steiner, Moritz},
title = {Measurement Study of Netflix, Hulu, and a Tale of Three CDNs},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2014.2354262},
doi = {10.1109/TNET.2014.2354262},
abstract = {Netflix and Hulu are leading Over-the-Top (OTT) content service providers in the US
and Canada. Netflix alone accounts for 29.7% of the peak downstream traffic in the
US in 2011. Understanding the system architectures and performance of Netflix and
Hulu can shed light on the design of such large-scale video streaming platforms, and
help improving the design of future systems. In this paper, we perform extensive measurement
study to uncover their architectures and service strategies. Netflix and Hulu bear
many similarities. Both Netflix and Hulu video streaming platforms rely heavily on
the third-party infrastructures, with Netflix migrating that majority of its functions
to the Amazon cloud, while Hulu hosts its services out of Akamai. Both service providers
employ the same set of three content distribution networks (CDNs) in delivering the
video contents. Using active measurement study, we dissect several key aspects of
OTT streaming platforms of Netflix and Hulu, e.g., employed streaming protocols, CDN
selection strategy, user experience reporting, etc. We discover that both platforms
assign the CDN to a video request without considering the network conditions and optimizing
the user-perceived video quality. We further conduct the performance measurement studies
of the three CDNs employed by Netflix and Hulu. We show that the available bandwidths
on all three CDNs vary significantly over the time and over the geographic locations.
We propose a measurement-based adaptive CDN selection strategy and a multiple-CDN-based
video delivery strategy that can significantly increase users' average available bandwidth.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {1984–1997},
numpages = {14},
keywords = {Hulu, Netflix, over-the-top (OTT) content service, CDN selection strategy, content distribution networks (CDN), video streaming}
}

@inproceedings{10.1145/2683405.2683409,
author = {Nguyen, Hoang Minh and W\"{u}nsche, Burkhard and Delmas, Patrice and Lutteroth, Christof},
title = {Identifying Low Confidence Mesh Regions: Uncertainty Measures and Segmentation},
year = {2014},
isbn = {9781450331845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2683405.2683409},
doi = {10.1145/2683405.2683409},
abstract = {3D digital models have become an important part of diverse applications ranging from
computer games, virtual reality, architectural design to visual impact studies. One
common method to create 3D models is to create a point cloud using laser scanners,
structured lighting sensors, or image-based modelling techniques, and then construct
a 3D mesh, and texture-map it using photographs of the observed scene. Attributed
to the inherent properties of general 3D scenes such as occluded or inaccessible parts,
reflective surfaces, lighting conditions or poor-quality inputs, 3D models produced
by these approaches often exhibit unsatisfactory and erroneous mesh regions. In many
cases, it is desirable to identify and extract such regions so that they can be constructed
or corrected through other means. While much effort has been invested into the problem
of 3D reconstructions, the task of evaluating existing models and preparing them for
subsequent enhancement processes has been largely neglected. In this paper, we present
a novel method for automatically detecting and segmenting mesh regions with low confidence
in their correctness. The confidence estimation is achieved by exploiting and integrating
various uncertainty measures such as geometric distances, normal variations and texture
discrepancies. Low-confidence mesh regions are isolated and removed in such a way
that the extracted region's boundary is as simple as possible in order to facilitate
subsequent automatic or manual improvement of these regions. Segmentation is achieved
by minimising an energy function that takes the genus and boundary length and smoothness
of the extracted regions into account.},
booktitle = {Proceedings of the 29th International Conference on Image and Vision Computing New Zealand},
pages = {48–53},
numpages = {6},
keywords = {Uncertainty Measure, Mesh Classification, 3D Reconstruction},
location = {Hamilton, New Zealand},
series = {IVCNZ '14}
}

@inproceedings{10.1145/3323503.3349545,
author = {de Amorim, Irandir O. and de Melo, Jose F. V. and Balieiro, Andson M. and Santos, Bruno B. dos},
title = {An Evolutionary Approach for Video Application Energy Consumption Estimation in Mobile Devices},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3349545},
doi = {10.1145/3323503.3349545},
abstract = {In the last years, the multimedia traffic has increased significantly and the mobile
devices (e.g. smart phones and tablets) have been widely used to consume this content
type. Video applications demand high energy consumption of the device because they
perform complex operations and deal with a large data amount. Although hardware improvements
in the mobile devices have been achieved, the advances in battery technology have
not kept the same pace. In this respect, the combination of video applications with
the limited battery capacity of the mobile devices has challenged the academia and
industry in the development of techniques for energy management and provision of quality
of experience (QoE) to the user. Energy consumption estimation models may assist these
techniques, as well as, the decision made process when the computational offloading
from the mobile device to the cloud is considered. This paper presents an evolutionary
approach based on Genetic Algorithms (GAs) and Swarm Particle Optimization (PSO) for
energy consumption estimation in mobile devices running video applications. The proposal
is directly applicable to different model types (linear and non-linear ones), without
the linearization cost, and it is evaluated in terms of mean squared error (MSE),
using energy consumption measurement data of videos with different configurations.
Results show the superiority of our proposal in comparison to the literature that
adopts the Ordinary Least Squares method.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {169–175},
numpages = {7},
keywords = {energy consumption model for mobile devices, genetic algorithms, particle swarm optimization, video application},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/2639108.2639118,
author = {Li, Liqun and Shen, Guobin and Zhao, Chunshui and Moscibroda, Thomas and Lin, Jyh-Han and Zhao, Feng},
title = {Experiencing and Handling the Diversity in Data Density and Environmental Locality in an Indoor Positioning Service},
year = {2014},
isbn = {9781450327831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639108.2639118},
doi = {10.1145/2639108.2639118},
abstract = {Diversity in training data density and environment locality is intrinsic in the real-world
deployment of indoor localization systems and has a major impact on the performance
of existing localization approaches. In this paper, through micro-benchmarks, we find
that fingerprint-based approaches are preferable in scenarios where a dense database
is available; while model-based approaches are the method of choice in the case of
sparse data. It should be noted, however, that practical situations are complex. A
single deployment often features both sparse and dense sampled areas. Furthermore,
the internal layout affects the propagation of radio signals and exhibits environmental
impacts. A certain number of measurement samples may be sufficient for one part of
the building, but entirely insufficient for another. Thus, finding the right indoor
localization algorithm for a given large-scale deployment is challenging, if not impossible;
there is no one-size-fits-all indoor localization approach.Realizing the fundamental
fact that the quality of the location database capturing the actual radio map dictates
localization accuracy, in this paper, we propose Modellet, an algorithmic approach
that optimally approximates the actual radio map by unifying model-based and fingerprint-based
approaches. Modellet represents the radio map using a fingerprint-cloud that incorporates
both measured real fingerprints and virtual fingerprints, which are computed from
models with a local support, based on the key concept of the supporting set. We evaluate
Modellet with data collected from an office building as well as 13 large-scale deployment
venues (shopping malls and airports), located across China, U.S., and Germany. Comparing
Modellet with two representative baseline approaches, RADAR and EZPerfect, demonstrates
that Modellet effectively adapts to different data densities and environmental conditions,
substantially outperforming existing approaches.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Computing and Networking},
pages = {459–470},
numpages = {12},
keywords = {fingerprint, indoor localization, model},
location = {Maui, Hawaii, USA},
series = {MobiCom '14}
}

@inproceedings{10.1109/CCGrid.2014.50,
author = {Tolosana-Calasanz, Rafael and Ba\~{n}ares, Jos\'{e} \'{A}ngel and Rana, Omer and Pham, Congduc and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
title = {Enforcing Quality of Service on OpenNebula-Based Shared Clouds},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.50},
doi = {10.1109/CCGrid.2014.50},
abstract = {With an increase in the number of monitoring sensors deployed on physical infrastructures,
there is a corresponding increase in data volumes that need to be processed. Data
measured or collected by sensors is typically processed at destination or "in-transit"
(i.e. from data capture to delivery to a user). When such data are processed in-transit
over a shared distributed computing infrastructure, it is useful to provide elastic
computational capability which can be adapted based on processing requirements and
demand. Where Service Level Agreements (SLAs) have been pre-agreed, such available
computational capacity needs to be shared in such a way that any Quality of Service
related constraints in such SLAs are not violated. This is particularly challenging
for time critical applications and with highly variable and unpredictable rates of
data generation (e.g. in Smart Grid applications where energy usage patterns may change
unpredictably). Previously, we proposed a Reference net based architectural model
for supporting QoS for multiple concurrent data streams being processed (prior to
delivery to a user) over a shared infrastructure. In this paper, we describe a practical
realisation of this architecture using the OpenNebula Cloud platform. We consider
our infrastructure to be composed of a number of nodes, each of which has multiple
processing units and data buffers. We utilize the "token bucket" model for regulating,
on a per stream basis, the data injection rate into each node. We subsequently demonstrate
how a streaming pipeline can be supported and managed using a dynamic control strategy
at each node.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {651–659},
numpages = {9},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/2716281.2836120,
author = {Kateja, Rajat and Baranasuriya, Nimantha and Navda, Vishnu and Padmanabhan, Venkata N.},
title = {DiversiFi: Robust Multi-Link Interactive Streaming},
year = {2015},
isbn = {9781450334129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2716281.2836120},
doi = {10.1145/2716281.2836120},
abstract = {Real-time, interactive streaming for applications such as audio-video conferencing
(e.g., Skype) and cloud-based gaming depends critically on the network providing low
latency, jitter, and packet loss, much more so than on-demand streaming (e.g., YouTube)
does. However, WiFi networks pose a challenge; our analysis of data from a large VoIP
provider and from our own measurements shows that the WiFi access link is a significant
cause of poor streaming experience.To improve streaming quality over WiFi, we present
DiversiFi, which takes advantage of the diversity of WiFi links available in the vicinity,
even when the individual links are poor. Leveraging such cross-link spatial and channel
diversity outperforms both traditional link selection and the temporal diversity arising
from retransmissions on the same link. It also provides significant gains over and
above the PHY-layer spatial diversity provided by MIMO. Our experimental evaluation
shows that, for a client with two NICs, enabling replication across two WiFi links
helps cut down the poor call rate (PCR) for VoIP by 2.24x.Finally, we present the
design and implementation of DiversiFi, which enables it to operate with single-NIC
clients, and with either minimally modified APs or unmodified APs augmented with a
middlebox. Over 61 runs, where the baseline average PCR is 4.9%, DiversiFi running
with a single NIC, switching between two links, helps cut the PCR down to 0%, while
duplicating wastefully only 0.62% of the packets and impacting competing TCP throughput
by only 2.5%. Thus, DiversiFi provides the benefit of multi-link diversity for real-time
interactive streaming in a manner that is deployable and imposes little overhead,
thereby ensuring coexistence with other applications.},
booktitle = {Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies},
articleno = {35},
numpages = {13},
keywords = {multi-path, wi-fi, VoIP, real-time streaming},
location = {Heidelberg, Germany},
series = {CoNEXT '15}
}

@proceedings{10.1145/2789168,
title = {MobiCom '15: Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM MobiCom 2015, the 21st Annual International Conference on Mobile Computing
and Networking. MobiCom is the premier forum for publishing and presenting cutting-edge
research in mobile systems and wireless networks. The technical program this year
features 38 outstanding papers that cover a wide variety of topics including energy,
sensing, security, wireless access, applications, localization, Internet of things,
mobile cloud, measurement and analysis. We created a new Experience track this year
to encourage authors to present extensive experiences with implementation, deployment,
and operations of mobile ncomputing and wireless networks. One of the accepted papers
is an Experience paper on cellular networks.This year's call for papers attracted
207 qualified submissions from across the globe that were carefully reviewed by 46
Technical Program Committee (TPC) members (+2 TPC chairs) along with a selected group
of external experts. The TPC was formed with the goal of covering diverse research
expertise as well as diverse perspectives and approaches. The TPC included researchers
from 12 countries including China, France, Germany, India, Italy, Singapore, South
Korea, Spain, Sweden, Switzerland, UK, and USA. 25% of the members were female, the
highest ever in the history of MobiCom. We also had broad industry participation with
TPC members from Alcatel-Lucent, Google, HP, IBM, Microsoft, NEC, and Telefonica.The
paper review process was double-blinded and carried out in three phases. In the first
phase, each paper was reviewed by at least three TPC members, and the top 112 papers
were selected for the second phase. In addition to reviewer scores, reviewer confidence
and normalization with respect to other papers in a reviewer's pile, were also considered
in selecting papers. In the second phase, each paper was reviewed by at least two
more reviewers followed by an online, often intense, discussion, producing 68 papers
for the final phase. The final TPC meeting was held on May 28th and 29th in Salt Lake
City, Utah. These 68 papers were organized by their topic areas, and discussed at
length at the meeting. Eventually, 38 papers were shortlisted for inclusion in the
program and a shepherd from the TPC was assigned to each of these papers. As the last
step, each of the shortlisted papers was shepherded through a "blind" process where
the authors interacted with all the reviewers and the shepherd to address the review
comments without knowing the reviewers' or the shepherds' identities. The end result
is an exciting technical program composed of 38 very high quality papers.During the
review process, Prof. Robin Kravets, the TPC co-chair of MobiCom 2013, handled the
papers that were co-authored by TPC chairs, and those that had conflict-of-interest
with both TPC chairs. To ensure fairness and preserve the anonymity of all authors
and reviewers, the assignment of reviewers, the reviews and discussions of these papers
were done out of band without any exposure to the TPC chairs.},
location = {Paris, France}
}

@article{10.1145/3450626.3459679,
author = {Ma, Xiaohe and Kang, Kaizhang and Zhu, Ruisheng and Wu, Hongzhi and Zhou, Kun},
title = {Free-Form Scanning of Non-Planar Appearance with Neural Trace Photography},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459679},
doi = {10.1145/3450626.3459679},
abstract = {We propose neural trace photography, a novel framework to automatically learn high-quality
scanning of non-planar, complex anisotropic appearance. Our key insight is that free-form
appearance scanning can be cast as a geometry learning problem on unstructured point
clouds, each of which represents an image measurement and the corresponding acquisition
condition. Based on this connection, we carefully design a neural network, to jointly
optimize the lighting conditions to be used in acquisition, as well as the spatially
independent reconstruction of reflectance from corresponding measurements. Our framework
is not tied to a specific setup, and can adapt to various factors in a data-driven
manner. We demonstrate the effectiveness of our framework on a number of physical
objects with a wide variation in appearance. The objects are captured with a light-weight
mobile device, consisting of a single camera and an RGB LED array. We also generalize
the framework to other common types of light sources, including a point, a linear
and an area light.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {124},
numpages = {13},
keywords = {illumination multiplexing, SVBRDF, optimal lighting pattern}
}

