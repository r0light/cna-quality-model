@inproceedings{10.5555/2821327.2821338,
author = {L\"{u}bke, Daniel},
title = {Using Metric Time-Lines for Identifying Architecture Shortcomings in Process Execution Architectures},
year = {2015},
publisher = {IEEE Press},
abstract = {Process Execution with Service Orchestrations is an emerging architectural style for
developing business software systems. However, few special metrics for guiding software
architecture decisions have been proposed and no existing business process metrics
have been evaluated for their suitability. By following static code metrics over time,
architects can gain a better understanding, how processes and the whole system evolve
and whether the metrics evolve as expected. This allows architects to recogniize when
to intervene in the development and make architecture adjustments or refactorings.
This paper presents an explatory study that uses time-lines of static process size
metrics for constant feedback to software architects that deal with process-oriented
architectures.},
booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
pages = {55–58},
numpages = {4},
location = {Florence, Italy},
series = {SAM '15}
}

@inproceedings{10.1145/3357384.3357956,
author = {Boiarov, Andrei and Tyantov, Eduard},
title = {Large Scale Landmark Recognition via Deep Metric Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357956},
doi = {10.1145/3357384.3357956},
abstract = {This paper presents a novel approach for landmark recognition in images that we've
successfully deployed at Mail.ru. This method enables us to recognize famous places,
buildings, monuments, and other landmarks in user photos. The main challenge lies
in the fact that it's very complicated to give a precise definition of what is and
what is not a landmark. Some buildings, statues and natural objects are landmarks;
others are not. There's also no database with a fairly large number of landmarks to
train a recognition model. A key feature of using landmark recognition in a production
environment is that the number of photos containing landmarks is extremely small.
This is why the model should have a very low false positive rate as well as high recognition
accuracy. We propose a metric learning-based approach that successfully deals with
existing challenges and efficiently handles a large number of landmarks. Our method
uses a deep neural network and requires a single pass inference that makes it fast
to use in production. We also describe an algorithm for cleaning landmarks database
which is essential for training a metric learning model. We provide an in-depth description
of basic components of our method like neural network architecture, the learning strategy,
and the features of our metric learning approach. We show the results of proposed
solutions in tests that emulate the distribution of photos with and without landmarks
from a user collection. We compare our method with others during these tests. The
described system has been deployed as a part of a photo recognition solution at Cloud
Mail.ru, which is the photo sharing and storage service at Mail.ru Group.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {169–178},
numpages = {10},
keywords = {metric learning, landmark recognition, deep learning},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3195546.3195549,
author = {Schmidts, Oliver and Kraft, Bodo and Schreiber, Marc and Z\"{u}ndorf, Albert},
title = {Continuously Evaluated Research Projects in Collaborative Decoupled Environments},
year = {2018},
isbn = {9781450357449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195546.3195549},
doi = {10.1145/3195546.3195549},
abstract = {Often, research results from collaboration projects are not transferred into productive
environments even though approaches are proven to work in demonstration prototypes.
These demonstration prototypes are usually too fragile and error-prone to be transferred
easily into productive environments. A lot of additional work is required.Inspired
by the idea of an incremental delivery process, we introduce an architecture pattern,
which combines the approach of Metrics Driven Research Collaboration with microservices
for the ease of integration. It enables keeping track of project goals over the course
of the collaboration while every party may focus on their expert skills: researchers
may focus on complex algorithms, practitioners may focus on their business goals.Through
the simplified integration (intermediate) research results can be introduced into
a productive environment which enables getting an early user feedback and allows for
the early evaluation of different approaches. The practitioners' business model benefits
throughout the full project duration.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice},
pages = {2–9},
numpages = {8},
keywords = {lean software development, software architecture, metrics, research best practices, research collaboration management},
location = {Gothenburg, Sweden},
series = {SER&amp;IP '18}
}

@inproceedings{10.1145/3131151.3131153,
author = {Durelli, Rafael S. and Viana, Matheus C. and de S. Landi, Andr\'{e} and Durelli, Vinicius H. S. and Delamaro, Marcio E. and de Camargo, Valter V.},
title = {Improving the Structure of KDM Instances via Refactorings: An Experimental Study Using KDM-RE},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131153},
doi = {10.1145/3131151.3131153},
abstract = {Architecture-Driven Modernization (ADM) is an initiative of the Object Management
Group (OMG) whose main purpose is to provide standard metamodels for software modernization
activities. The most important metamodel is the Knowledge Discovery Metamodel (KDM),
which represents software artifacts in a language-agnostic fashion. A fundamental
step in software modernization is refactoring. However, there is a lack of tools that
address how refactoring can be applied in conjunction with ADM. We developed a tool,
called KDM-RE, that supports refactorings in KDM instances through: (i) a set of wizards
that aid the software modernization engineer during refactoring activities; (ii) a
change propagation module that keeps the internal metamodels synchronized; and (iii)
the selection and application of refactorings available in its repository. This paper
evaluates the application of refactorings to KDM instances in an experiment involving
seven systems implemented in Java. We compared the pre-refactoring versions of these
systems with the refactored ones using the Quality Model for Object-Oriented Design
(QMOOD) metric set. The results from this evaluation suggest that KDM-RE provides
advantages to software modernization engineers refactoring systems represented as
KDMs.},
booktitle = {Proceedings of the 31st Brazilian Symposium on Software Engineering},
pages = {174–183},
numpages = {10},
keywords = {Knowledge-Discovery Metamodel, Model-Driven Development, Refactoring, Architecture-Driven Modernization},
location = {Fortaleza, CE, Brazil},
series = {SBES'17}
}

@article{10.1145/3293455,
author = {Arcuri, Andrea},
title = {RESTful API Automated Test Case Generation with EvoMaster},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3293455},
doi = {10.1145/3293455},
abstract = {RESTful APIs are widespread in industry, especially in enterprise applications developed
with a microservice architecture. A RESTful web service will provide data via an API
over the network using HTTP, possibly interacting with databases and other web services.
Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP
requests/responses to a remote server. Many approaches in the literature do black-box
testing, because often the tested API is a remote service whose code is not available.
In this article, we consider testing from the point of view of the developers, who
have full access to the code that they are writing. Therefore, we propose a fully
automated white-box testing approach, where test cases are automatically generated
using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding
metrics. However, REST is not a protocol but rather a set of guidelines on how to
design resources accessed over HTTP endpoints. For example, there are guidelines on
how related resources should be structured with hierarchical URIs and how the different
HTTP verbs should be used to represent well-defined actions on those resources. Test-case
generation for RESTful APIs that only rely on white-box information of the source
code might not be able to identify how to create prerequisite resources needed before
being able to test some of the REST endpoints. Smart sampling techniques that exploit
the knowledge of best practices in RESTful API design are needed to generate tests
with predefined structures to speed up the search. We implemented our technique in
a tool called EvoMaster, which is open source. Experiments on five open-source, yet
non-trivial, RESTful services show that our novel technique automatically found 80
real bugs in those applications. However, obtained code coverage is lower than the
one achieved by the manually written test suites already existing in those services.
Research directions on how to further improve such an approach are therefore discussed,
such as the handling of SQL databases.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {3},
numpages = {37},
keywords = {Software engineering, testing, REST, web service}
}

@article{10.1145/3351278,
author = {Yu, Tuo and Nahrstedt, Klara},
title = {ShoesHacker: Indoor Corridor Map and User Location Leakage through Force Sensors in Smart Shoes},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351278},
doi = {10.1145/3351278},
abstract = {The past few years have witnessed the rise of smart shoes, the wearable devices that
measure foot force or track foot motion. However, people are not aware of the possible
privacy leakage from in-shoe force sensors. In this paper, we explore the possibility
of locating an indoor victim based on the force signals leaked from smart shoes. We
present ShoesHacker, an attack scheme that reconstructs the corridor map of the building
that the victim walks in based on force data only. The corridor map enables the attacker
to recognize the building, and thus locate the victim on a global map. To handle the
lack of training data, we design the stair landing detection algorithm, based on which
we extract training data when victims are walking in stairwells. We estimate the trajectory
of each walk, and propose the path merging algorithm to merge the trajectories. Moreover,
we design a metric to quantify the similarity between corridor maps, which makes building
recognition possible. Our experimental results show that, the building recognition
accuracy reaches 77.5% in a 40-building dataset, and the victim can be located with
an average error lower than 6 m, which reveals the danger of privacy leakage through
smart shoes. CCS Concepts: • Information systems~Mobile information processing systems;
Location based services; • Human-centered computing~Mobile devices; Ubiquitous and
mobile computing systems and tools; • Security and privacy~Domain-specific security
and privacy architectures.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {120},
numpages = {29},
keywords = {smart shoes, force sensors, Corridor map reconstruction}
}

@inproceedings{10.1145/3219819.3219834,
author = {Staar, Peter W J and Dolfi, Michele and Auer, Christoph and Bekas, Costas},
title = {Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219834},
doi = {10.1145/3219819.3219834},
abstract = {Over the past few decades, the amount of scientific articles and technical literature
has increased exponentially in size. Consequently, there is a great need for systems
that can ingest these documents at scale and make the contained knowledge discoverable.
Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images)
as well as the presentation of the data (e.g. complex tables) make the extraction
of qualitative and quantitive data extremely challenging. In this paper, we present
a modular, cloud-based platform to ingest documents at scale. This platform, called
the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse
and annotate documents (i.e. collect ground-truth), train machine-learning classification
algorithms and ultimately convert any type of PDF or bitmap-documents to a structured
content representation format. We will show that each of the modules is scalable due
to an asynchronous microservice architecture and can therefore handle massive amounts
of documents. Furthermore, we will show that our capability to gather groundtruth
is accelerated by machine-learning algorithms by at least one order of magnitude.
This allows us to both gather large amounts of ground-truth in very little time and
obtain very good precision/recall metrics in the range of 99% with regard to content
conversion to structured output. The CCS platform is currently deployed on IBM internal
infrastructure and serving more than 250 active users for knowledge-engineering project
engagements.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {774–782},
numpages = {9},
keywords = {ibm research, machine learning, deep learning, artificial intelligence, ibm, cloud architecture, table processing, knowledge ingestion, cloud computing, ai, asynchronous architecture, pdf, document conversion},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3102304.3102334,
author = {Calcina-Ccori, Pablo and Costa, Laisa and Fedrecheski, Geovane and Esquiagola, John and Zuffo, Marcelo and da Silva, Fl\'{a}vio Corr\^{e}a},
title = {Agile Servient Integration with the Swarm: Automatic Code Generation for Nodes in the Internet of Things},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102334},
doi = {10.1145/3102304.3102334},
abstract = {Swarm vision, consists in an organic ecosystem of heterogeneous devices that communicate
and collaborate to achieve complex results. In previous work, we have proposed an
architecture to implement this vision based on web technologies. In this paper, we
have proposed a framework that makes the creation of Swarm-ready servients (devices
that acts both as server and client) easier, by generating a ready-to-run project
from a high-level description of the service. The project generated contains all dependencies
and libraries needed to integrate an IoT device into the Swarm, thus saving development
and configuration time. We compared the development effort of creating a servient
by hand and by using our framework, having the number of lines of code as a metric.
Our results show a reduction of 500% in the development effort to connect a device
to the Swarm. The next steps include a semantic high-level description for participating
services and support for resource-constrained devices.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {30},
numpages = {6},
keywords = {Swarm, automatic code generation, Servient, Internet of Things},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/3292425.3292428,
author = {Liu, Jiaming and Xian, Cuiling},
title = {Extracting Information of Urban Land Surface with High Resolution Remote Sensing Data},
year = {2018},
isbn = {9781450365468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292425.3292428},
doi = {10.1145/3292425.3292428},
abstract = {With the advantages of high spatial resolution and definition, and rich information,
land classification and land utilization of urban surface can be reached by using
the high resolution remote sensing data. Based on the high resolution remote sensing
image data Worldview-1 and Worldview-2 as the main data source, using the methods
of stereo images, object-oriented land use classification technique and architecture
shadow, this study (1) extracts 5-meter resolution DEM data of Shilipu district in
Wuhan and the average relative error compared with the measured DEM data of ground
points is 3.71% with relatively high precision, (2) obtains the land use information
of this area with an accuracy rate of 90%, and (3) achieve the data of building height
in this area with the relative error of less than 20%. The results of this paper show
that the high speed and precision can meet the 3d modeling elevation precision and
plane precision of digital urban buildings when using high-resolution remote sensing
images to extract basic geographic information in small urban areas, which will play
an important role in the research of urban surface information extraction in the future.},
booktitle = {Proceedings of the 2018 International Conference on Information Hiding and Image Processing},
pages = {62–67},
numpages = {6},
keywords = {High resolution, urban surface, remote sensing},
location = {Manchester, United Kingdom},
series = {IHIP 2018}
}

@article{10.1145/3394956,
author = {Sahoo, Kshira Sagar and Puthal, Deepak},
title = {SDN-Assisted DDoS Defense Framework for the Internet of Multimedia Things},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3394956},
doi = {10.1145/3394956},
abstract = {The Internet of Things is visualized as a fundamental networking model that bridges
the gap between the cyber and real-world entity. Uniting the real-world object with
virtualization technology is opening further opportunities for innovation in nearly
every individual’s life. Moreover, the usage of smart heterogeneous multimedia devices
is growing extensively. These multimedia devices that communicate among each other
through the Internet form a unique paradigm called the Internet of Multimedia Things
(IoMT). As the volume of the collected data in multimedia application increases, the
security, reliability of communications, and overall quality of service need to be
maintained. Primarily, distributed denial of service attacks unveil the pervasiveness
of vulnerabilities in IoMT systems. However, the Software Defined Network (SDN) is
a new network architecture that has the central visibility of the entire network,
which helps to detect any attack effectively. In this regard, the combination of SDN
and IoMT, termed SD-IoMT, has the immense ability to improve the network management
and security capabilities of the IoT system. This article proposes an SDN-assisted
two-phase detection framework, namely SD-IoMT-Protector, in which the first phase
utilizes the entropy technique as the detection metric to verify and alert about the
malicious traffic. The second phase has trained with an optimized machine learning
technique for classifying different attacks. The outcomes of the experimental results
signify the usefulness and effectiveness of the proposed framework for addressing
distributed denial of service issues of the SD-IoMT system.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {98},
numpages = {18},
keywords = {machine learning, IoMT, entropy, Control plane, security, SDN}
}

@inproceedings{10.1145/3401025.3401740,
author = {Scrocca, Mario and Tommasini, Riccardo and Margara, Alessandro and Valle, Emanuele Della and Sakr, Sherif},
title = {The Kaiju Project: Enabling Event-Driven Observability},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3401740},
doi = {10.1145/3401025.3401740},
abstract = {Microservices architectures are getting momentum. Even small and medium-size companies
are migrating towards cloud-based distributed solutions supported by lightweight virtualization
techniques, containers, and orchestration systems. In this context, understanding
the system behavior at runtime is critical to promptly react to errors. Unfortunately,
traditional monitoring techniques are not adequate for such complex and dynamic environments.
Therefore, a new challenge, namely observability, emerged from precise industrial
needs: expose and make sense of the system behavior at runtime. In this paper, we
investigate observability as a research problem. We discuss the benefits of events
as a unified abstraction for metrics, logs, and trace data, and the advantages of
employing event stream processing techniques and tools in this context. We show that
an event-based approach enables understanding the system behavior in near real-time
more effectively than state-of-the-art solutions in the field. We implement our model
in the Kaiju system and we validate it against a realistic deployment supported by
a software company.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {85–96},
numpages = {12},
keywords = {event stream processing, orchestration systems, observability, event-based systems},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.5555/2602339.2602341,
author = {Buevich, Maxim and Schnitzer, Dan and Escalada, Tristan and Jacquiau-Chamski, Arthur and Rowe, Anthony},
title = {Fine-Grained Remote Monitoring, Control and Pre-Paid Electrical Service in Rural Microgrids},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {In this paper, we present the architecture, design and experiences from a wirelessly
managed microgrid deployment in rural Les Anglais, Haiti. The system consists of a
three-tiered architecture with a cloud-based monitoring and control service, a local
embedded gateway infrastructure and a mesh network of wireless smart meters deployed
at 52 buildings. Each smart meter device has an 802.15.4 radio that enables remote
monitoring and control of electrical service. The meters communicate over a scalable
multi-hop TDMA network back to a central gateway that manages load within the system.
The gateway also provides an 802.11 interface for an on-site operator and a cellular
modem connection to a cloud-backend that manages and stores billing and usage data.
The cloud backend allows occupants in each home to pre-pay for electricity at a particular
peak power limit using a text messaging service. The system activates each meter within
seconds and locally enforces power limits with provisioning for theft detection. We
believe that this fine-grained micro-payment model can enable sustainable power in
otherwise unfeasible areas.This paper provides a chronology of our deployment and
installation strategy that involved GPS-based site mapping along with various network
conditioning actions required as the network evolved. Finally, we summarize key lessons
learned and hypothesis about additional hardware that could be used to ease the tracing
of faults like short circuits and downed lines within microgrids.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {1–12},
numpages = {12},
keywords = {wireless local area networks, sensor networks, microgrid},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3359789.3359843,
author = {Nagendra, Vasudevan and Yegneswaran, Vinod and Porras, Phillip and Das, Samir R},
title = {Coordinated Dataflow Protection for Ultra-High Bandwidth Science Networks},
year = {2019},
isbn = {9781450376280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359789.3359843},
doi = {10.1145/3359789.3359843},
abstract = {The Science DMZ (SDMZ) is a special purpose network architecture proposed by ESnet
(Energy Sciences Network) to facilitate distributed science experimentation on terabyte-
(or petabyte-) scale data, exchanged over ultra-high bandwidth WAN links. Critical
security challenges faced by these networks include: (i) network monitoring at high
bandwidths, (ii) reconciling site-specific policies with project-level policies for
conflict-free policy enforcement, (iii) dealing with geographically-distributed datasets
with varying levels of sensitivity, and (iv) dynamically enforcing appropriate security
rules. To address these challenges, we develop a fine-grained dataflow-based security
enforcement system, called CoordiNetZ (CNZ), that provides coordinated situational
awareness, i.e., the use of context-aware tagging for policy enforcement using the
dynamic contextual information derived from hosts and network elements. We also developed
tag and IP-based security microservices that incur minimal overheads in enforcing
security to data flows exchanged across geographically-distributed SDMZ sites. We
evaluate our prototype implementation across two geographically distributed SDMZ sites
with SDN-based case studies, and present performance measurements that respectively
highlight the utility of our framework and demonstrate efficient implementation of
security policies across distributed SDMZ networks.},
booktitle = {Proceedings of the 35th Annual Computer Security Applications Conference},
pages = {568–583},
numpages = {16},
keywords = {usability and human-centric aspects of security, distributed systems security, software-defined programmable security, NFV, big data security, network security, SDN},
location = {San Juan, Puerto Rico, USA},
series = {ACSAC '19}
}

@article{10.1145/2975161,
author = {Bouraoui, Hasna and Jerad, Chadlia and Chattopadhyay, Anupam and Hadj-Alouane, Nejib Ben},
title = {Hardware Architectures for Embedded Speaker Recognition Applications: A Survey},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/2975161},
doi = {10.1145/2975161},
abstract = {Authentication technologies based on biometrics, such as speaker recognition, are
attracting more and more interest thanks to the elevated level of security offered
by these technologies. Despite offering many advantages, such as remote use and low
vulnerability, speaker recognition applications are constrained by the heavy computational
effort and the hard real-time constraints. When such applications are run on an embedded
platform, the problem becomes more challenging, as additional constraints inherent
to this specific domain are added. In the literature, different hardware architectures
were used/designed for implementing a process with a focus on a given particular metric.
In this article, we give a survey of the state-of-the-art works on implementations
of embedded speaker recognition applications. Our aim is to provide an overview of
the different approaches dealing with acceleration techniques oriented towards speaker
and speech recognition applications and attempt to identify the past, current, and
future research trends in the area. Indeed, on the one hand, many flexible solutions
were implemented, using either General Purpose Processors or Digital Signal Processors.
In general, these types of solutions suffer from low area and energy efficiency. On
the other hand, high-performance solutions were implemented on Application Specific
Integrated Circuits or Field Programmable Gate Arrays but at the expense of flexibility.
Based on the available results, we compare the application requirements vis-\`{a}-vis
the performance achieved by the systems. This leads to the projection of new research
trends that can be undertaken in the future.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = apr,
articleno = {78},
numpages = {28},
keywords = {Embedded hardware, acceleration, classification algorithms and implementations, speaker recognition}
}

@inproceedings{10.1145/3412841.3441899,
author = {Torquato, Matheus and Maciel, Paulo and Vieira, Marco},
title = {Analysis of VM Migration Scheduling as Moving Target Defense against Insider Attacks},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441899},
doi = {10.1145/3412841.3441899},
abstract = {As cybersecurity threats evolve, cloud computing defenses must adapt to face new challenges.
Unfortunately, due to resource sharing, cloud computing platforms open the door for
insider attacks, which consist of malicious actions from cloud authorized users (e.g.,
clients of an Infrastructure-as-a-Service (IaaS) cloud) targeting the co-hosted users
or the underlying provider environment. Virtual machine (VM) migration is a Moving
Target Defense (MTD) technique to mitigate insider attacks effects, as it provides
VMs positioning manageability. However, there is a clear demand for studies quantifying
the security benefits of VM migration-based MTD considering different system architecture
configurations. This paper tries to fill such a gap by presenting a Stochastic Reward
Net model for the security evaluation of a VM migration-based MTD. The security metric
of interest is the probability of attack success. We consider multiple architectures,
ranging from one physical machine pool (without MTD) up to four physical machine pools.
The evaluation also considers the unavailability due to VM migration. The key contributions
are i) a set of results highlighting the probability of insider attacks success over
time in different architectures and VM migration schedules, and ii) suggestions for
selecting VMs as candidates for MTD deployment based on the tolerance levels of the
attack success probability. The results are validated against simulation results to
confirm the accuracy of the model.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {194–202},
numpages = {9},
keywords = {VM migration, moving target defense, availability, stochastic petri nets, migration-based dynamic platform},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3164541.3164576,
author = {Kim, Heejin and Jeon, Seil and Raza, Syed M. and Lee, Joohyun and Choo, Hyunseung},
title = {Service-Aware Split Point Selection for User-Centric Mobility Enhancement in SDN},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164576},
doi = {10.1145/3164541.3164576},
abstract = {IP mobility anchor works as the redirection/split point of the packet destined to
the mobile terminal (MT), as well as IP address/prefix assignment and mobility binding
management in the legacy mobility management protocols. In software-defined networking
(SDN), the split point can be managed by the SDN controller or controller application,
as the control of the network is separated from the forwarding entities. The demand
of user QoE is ever increasing and they always want to get the best service continuity
served in mobility management. Differentiated split point selection per service type
could be one of the effective measures to enhance user QoE in a mobility management
environment. In this paper, we propose a service-aware split point selection mechanism
for user-centric mobility management enhancement in SDN. Specifically, we propose
the mobility control architecture, which can classify service flow type and determine
advantageous split point depending on a service flow type. We analyze the performance
of the proposed split point selection mechanism compared to target mechanisms. We
also measure the performance metrics on an ONOS-based SDN testbed to identify the
superiority of the proposed mechanism.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {95},
numpages = {8},
keywords = {mobility management, split point selection, software-defined networking},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@article{10.1145/3319498,
author = {Izadpanah, Ramin and Allan, Benjamin A. and Dechev, Damian and Brandt, Jim},
title = {Production Application Performance Data Streaming for System Monitoring},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3319498},
doi = {10.1145/3319498},
abstract = {In this article, we present an approach to streaming collection of application performance
data. Practical application performance tuning and troubleshooting in production high-performance
computing (HPC) environments requires an understanding of how applications interact
with the platform, including (but not limited to) parallel programming libraries such
as Message Passing Interface (MPI). Several profiling and tracing tools exist that
collect heavy runtime data traces either in memory (released only at application exit)
or on a file system (imposing an I/O load that may interfere with the performance
being measured). Although these approaches are beneficial in development stages and
post-run analysis, a systemwide and low-overhead method is required to monitor deployed
applications continuously. This method must be able to collect information at both
the application and system levels to yield a complete performance picture.In our approach,
an application profiler collects application event counters. A sampler uses an efficient
inter-process communication method to periodically extract the application counters
and stream them into an infrastructure for performance data collection. We implement
a tool-set based on our approach and integrate it with the Lightweight Distributed
Metric Service (LDMS) system, a monitoring system used on large-scale computational
platforms. LDMS provides the infrastructure to create and gather streams of performance
data in a low overhead manner. We demonstrate our approach using applications implemented
with MPI, as it is one of the most common standards for the development of large-scale
scientific applications.We utilize our tool-set to study the impact of our approach
on an open source HPC application, Nalu. Our tool-set enables us to efficiently identify
patterns in the behavior of the application without source-level knowledge. We leverage
LDMS to collect system-level performance data and explore the correlation between
the system and application events. Also, we demonstrate how our tool-set can help
detect anomalies with a low latency. We run tests on two different architectures:
a system enabled with Intel Xeon Phi and another system equipped with Intel Xeon processor.
Our overhead study shows our method imposes at most 0.5% CPU usage overhead on the
application in realistic deployment scenarios.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = apr,
articleno = {8},
numpages = {25},
keywords = {application profiling, Application and system monitoring, performance data streaming}
}

@inproceedings{10.1145/3452296.3472922,
author = {Fayed, Marwan and Bauer, Lorenz and Giotsas, Vasileios and Kerola, Sami and Majkowski, Marek and Odintsov, Pavel and Sitnicki, Jakub and Chung, Taejoong and Levin, Dave and Mislove, Alan and Wood, Christopher A. and Sullivan, Nick},
title = {The Ties That Un-Bind: Decoupling IP from Web Services and Sockets for Robust Addressing Agility at CDN-Scale},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472922},
doi = {10.1145/3452296.3472922},
abstract = {The couplings between IP addresses, names of content or services, and socket interfaces,
are too tight. This impedes system manageability, growth, and overall provisioning.
In turn, large-scale content providers are forced to use staggering numbers of addresses,
ultimately leading to address exhaustion (IPv4) and inefficiency (IPv6).In this paper,
we revisit IP bindings, entirely. We attempt to evolve addressing conventions by decoupling
IP in DNS and from network sockets. Alongside technologies such as SNI and ECMP, a
new architecture emerges that ``unbinds'' IP from services and servers, thereby returning
IP's role to merely that of reachability. The architecture is under evaluation at
a major CDN in multiple datacenters. We show that addresses can be generated randomly
emph{per-query}, for 20M+ domains and services, from as few as ~4K addresses, 256
addresses, and even emph{one} IP address. We explain why this approach is transparent
to routing, L4/L7 load-balancers, distributed caching, and all surrounding systems
-- and is emph{highly desirable}. Our experience suggests that many network-oriented
systems and services (e.g., route leak mitigation, denial of service, measurement)
could be improved, and new ones designed, if built with addressing agility.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {433–446},
numpages = {14},
keywords = {programmable sockets, provisioning, addressing, content distribution},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@inproceedings{10.5555/2821357.2821365,
author = {Nikravesh, Ali Yadavar and Ajila, Samuel A. and Lung, Chung-Horng},
title = {Towards an Autonomic Auto-Scaling Prediction System for Cloud Resource Provisioning},
year = {2015},
publisher = {IEEE Press},
abstract = {This paper investigates the accuracy of predictive auto-scaling systems in the Infrastructure
as a Service (IaaS) layer of cloud computing. The hypothesis in this research is that
prediction accuracy of auto-scaling systems can be increased by choosing an appropriate
time-series prediction algorithm based on the performance pattern over time. To prove
this hypothesis, an experiment has been conducted to compare the accuracy of time-series
prediction algorithms for different performance patterns. In the experiment, workload
was considered as the performance metric, and Support Vector Machine (SVM) and Neural
Networks (NN) were utilized as time-series prediction techniques. In addition, we
used Amazon EC2 as the experimental infrastructure and TPC-W as the benchmark to generate
different workload patterns. The results of the experiment show that prediction accuracy
of SVM and NN depends on the incoming workload pattern of the system under study.
Specifically, the results show that SVM has better prediction accuracy in the environments
with periodic and growing workload patterns, while NN outperforms SVM in forecasting
unpredicted workload pattern. Based on these experimental results, this paper proposes
an architecture for a self-adaptive prediction suite using an autonomic system approach.
This suite can choose the most suitable prediction technique based on the performance
pattern, which leads to more accurate prediction results.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {35–45},
numpages = {11},
keywords = {workload pattern, neural networks, resource provisioning, support vector machine, cloud computing, auto-scaling, autonomic},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/2747470.2747471,
author = {Dudouet, Florian and Edmonds, Andrew and Erne, Michael},
title = {Reliable Cloud-Applications: An Implementation through Service Orchestration},
year = {2015},
isbn = {9781450334761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2747470.2747471},
doi = {10.1145/2747470.2747471},
abstract = {As cloud-deployed applications became more and more mainstream, continuously more
complex services started to be deployed; indeed where initially monolithic applications
were simply ported to the cloud, applications are now more and more often composed
of micro-services. This improves the flexibility of an application but also makes
it more complex due to the sheer number of services comprising it.As deployment and
runtime management becomes more complex, orchestration software are becoming necessary
to completely manage the lifecycle of cloud applications. One crucial problem remaining
is how these applications can be made reliable in the cloud, a naturally unreliable
environment.In this paper we propose concepts and architectures which were implemented
in our orchestration software to guarantee reliability. Our initial implementation
also relies on Monasca, a well-known monitoring software for Open-Stack, to gather
proper metric and execute threshold-based actions. This allows us to show how service
reliability can be ensured using orchestration and how a proper incident-management
software feeding decisions to the orchestration engine ensures high-availability of
all components of managed applications.},
booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
pages = {1–6},
numpages = {6},
keywords = {incident management, orchestration, cloud computing, reliability},
location = {Bordeaux, France},
series = {AIMC '15}
}

@inproceedings{10.1145/3368691.3368704,
author = {Albataineh, Abdallah and Al-Qassas, Raad S. and Qasaimeh, Malik},
title = {A New Architecture for Voice Interconnection Using Packet Switched Network},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368704},
doi = {10.1145/3368691.3368704},
abstract = {Interconnecting voice service providers require a mutual trust between communicating
entities, which are built either using bilateral agreements or intermediary service
provider. To achieve such relationship between Anonymous Service Providers we should
have an automated mechanism. In this paper, we propose a conceptual architecture that
can build such relationship between communicating Anonymous Service Providers. By
applying this architecture, we argue that we can increase efficiency, security, and
performance of service provider's networks. The impact of internet speed on the interconnection
network is measured using key metrics including ACD, ASR, PDD, NER, and MOS.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {13},
numpages = {7},
keywords = {voice network architecture, SS7, ACD, PDD, ASR, SIP},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.5555/3191835.3191902,
author = {Lin, Chih-Lu and Chen, Ying-Liang and Kao, Hung-Yu},
title = {Question Difficulty Evaluation by Knowledge Gap Analysis in Question Answer Communities},
year = {2014},
isbn = {9781479958764},
publisher = {IEEE Press},
abstract = {The Community Question Answer (CQA) service is a typical forum of Web 2.0 that shares
knowledge among people. There are thousands of questions that are posted and solved
every day. Because of the various users of the CQA service, question search and ranking
are the most important topics of research in the CQA portal. In this study, we addressed
the problem of identifying questions as being hard or easy by means of a probability
model. In addition, we observed the phenomenon called knowledge gap that is related
to the habit of users and used a knowledge gap diagram to illustrate how much of a
knowledge gap exists in different categories. To this end, we proposed an approach
called the knowledge-gap-based difficulty rank (KG-DRank) algorithm, which combines
the user-user network and the architecture of the CQA service to find hard questions.
We used f-measure, AUC, MAP, NDCG, precision@Top5 and concordance analysis to evaluate
the experimental results. Our results show that our approach leads to better performance
than other baseline approaches across all evaluation metrics.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {336–339},
numpages = {4},
keywords = {social network, CQA portal, link analysis, knowledge gap, expert finding, difficulty},
location = {Beijing, China},
series = {ASONAM '14}
}

@inproceedings{10.1145/3343031.3350592,
author = {Galteri, Leonardo and Seidenari, Lorenzo and Bertini, Marco and Uricchio, Tiberio and Del Bimbo, Alberto},
title = {Fast Video Quality Enhancement Using GANs},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350592},
doi = {10.1145/3343031.3350592},
abstract = {Video compression algorithms result in a reduction of image quality, because of their
lossy approach to reduce the required bandwidth. This affects commercial streaming
services such as Netflix, or Amazon Prime Video, but affects also video conferencing
and video surveillance systems. In all these cases it is possible to improve the video
quality, both for human view and for automatic video analysis, without changing the
compression pipeline, through a post-processing that eliminates the visual artifacts
created by the compression algorithms. Generative Adversarial Networks have obtained
extremely high quality results in image enhancement tasks; however, to obtain such
results large generators are usually employed, resulting in high computational costs
and processing time. In this work we present an architecture that can be used to reduce
the computational cost and that has been implemented on mobile devices. A possible
application is to improve video conferencing, or live streaming. In these cases there
is no original uncompressed video stream available. Therefore, we report results using
no-reference video quality metric showing high naturalness and quality even for efficient
networks.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1065–1067},
numpages = {3},
keywords = {gans, video quality enhancement, real-time enhancement, video streaming, video compression},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3258045,
author = {Savola, Reijo and Abie, Habtamu and Kanstr\'{e}n, Teemu},
title = {Session Details: Fourth International Workshop on Measurability of Security in Software Architectures (MeSSa 2017)},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3258045},
doi = {10.1145/3258045},
abstract = {Cybersecurity incidents are increasing, and at the same time, our society depends
more and more on cyber-physical systems. Systematic approaches to measure cybersecurity
are needed in order to support efficient construction and maintenance of secure software
systems. Security measurement of software architectures is needed to produce sufficient
evidence of security level as early as in the design phase. Design-time security measuring
should support "security by design" approach. Moreover, software architectures have
to support runtime security measurement to obtain up-to-date security information
from an online software system, service or product. Security metrics and measurements
are exploited in situational awareness monitoring and self-adaptive security solutions.
The area of security metrics and security assurance metrics research is evolving,
but still lacks widely accepted metrics definitions and applicable measuring techniques.
Strong collaboration between security experts, software architects and system developers
is needed to address this. MeSSa2017 workshop addresses these and other related topics
to increase the importance of the overall picture, requiring sets of design patterns,
measurements, metrics, best practices, and means to integrate this cost-effectively
in the overall design and operational profiles.The outcome of the workshop will be
an increased shared understanding of challenges and opportunities in systematic approaches
to measure cybersecurity, which are needed in order to support efficient construction
and maintenance of secure software systems.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/3238147.3240463,
author = {Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin},
title = {Achieving Test Automation with Testers without Coding Skills: An Industrial Report},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240463},
doi = {10.1145/3238147.3240463},
abstract = {We present a process driven test automation solution which enables delegating (part
of) automation tasks from test automation engineer (expensive resource) to test analyst
(non-developer, less expensive). In our approach, a test automation engineer implements
test steps (or actions) which are executed automatically. Such automated test steps
represent user actions in the system under test and specified by a natural language
which is understandable by a non-technical person. Then, a test analyst with a domain
knowledge organizes automated steps combined with test input to create an automated
test case. It should be emphasized that the test analyst does not need to possess
programming skills to create, modify or execute automated test cases. We refine benchmark
test automation architecture to be better suitable for an effective separation and
sharing of responsibilities between the test automation engineer (with coding skills)
and test analyst (with a domain knowledge). In addition, we propose a metric to empirically
estimate cooperation between test automation engineer and test analyst's works. The
proposed automation solution has been defined based on our experience in the development
and maintenance of Helsenorg, the national electronic health services in Norway which
has had over one million of visits per month past year, and we still use it to automate
the execution of regression tests.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {749–756},
numpages = {8},
keywords = {Test automation, keyword-driven test automation, process-driven test automation, Helsenorge, DSL for test automation},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/2601248.2601264,
author = {Stevanetic, Srdjan and Zdun, Uwe},
title = {Exploring the Relationships between the Understandability of Components in Architectural Component Models and Component Level Metrics},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601264},
doi = {10.1145/2601248.2601264},
abstract = {Architectural component models represent high level designs and are frequently used
as a central view of architectural descriptions of software systems. The components
in those models represent important high level organization units that group other
components and classes in object-oriented design views. Hence, understandability of
components and their interactions plays a key role in supporting the architectural
understanding of a software system. In this paper we present a study we carried out
to examine the relationships between the effort required to understand a component,
measured through the time that participants spent on studying a component, and component
level metrics that describe component's size, complexity and coupling in terms of
the number of classes in a component and the classes' relationships. The participants
were 49 master students, and they had to fully understand the components' functionalities
in order to answer 4 true/false questions for each of the 7 components in the architecture
of the Soomla Android store system. Correlation, collinearity and multivariate regression
analysis were performed. The results of the analysis show a statistically significant
correlation between three of the metrics, number of classes, number of incoming dependencies,
and number of internal dependencies, on one side, and the effort required to understand
a component, on the other side. In a multivariate regression analysis we obtained
3 reasonably well-fitting models that can be used to estimate the effort required
to understand a component. In our future work we plan to study more components and
investigate more metrics and their relationships to the understandability of components
and architectural component models.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {32},
numpages = {10},
keywords = {software metrics, understandability, architectural component models, empirical evaluation},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1145/3468264.3473915,
author = {Kalia, Anup K. and Xiao, Jin and Krishna, Rahul and Sinha, Saurabh and Vukovic, Maja and Banerjee, Debasish},
title = {Mono2Micro: A Practical and Effective Tool for Decomposing Monolithic Java Applications to Microservices},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473915},
doi = {10.1145/3468264.3473915},
abstract = {In migrating production workloads to cloud, enterprises often face the daunting task
of evolving monolithic applications toward a microservice architecture. At IBM, we
developed a tool called Mono2Micro to assist with this challenging task. Mono2Micro
performs spatio-temporal decomposition, leveraging well-defined business use cases
and runtime call relations to create functionally cohesive partitioning of application
classes. Our preliminary evaluation of Mono2Micro showed promising results.  How well
does Mono2Micro perform against other decomposition techniques, and how do practitioners
perceive the tool? This paper describes the technical foundations of Mono2Micro and
presents results to answer these two questions. To answer the first question, we evaluated
Mono2Micro against four existing techniques on a set of open-source and proprietary
Java applications and using different metrics to assess the quality of decomposition
and tool’s efficiency. Our results show that Mono2Micro significantly outperforms
state-of-the-art baselines in specific metrics well-defined for the problem domain.
To answer the second question, we conducted a survey of twenty-one practitioners in
various industry roles who have used Mono2Micro. This study highlights several benefits
of the tool, interesting practitioner perceptions, and scope for further improvements.
Overall, these results show that Mono2Micro can provide a valuable aid to practitioners
in creating functionally cohesive and explainable microservice decompositions.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1214–1224},
numpages = {11},
keywords = {clustering, microservices, dynamic analysis},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Software Development and Research with the HADAS Toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is
a non-trivial task. Developers require energy metrics that support sustainable software
architectures reflecting quality attributes such as security, reliability, performance,
etc., identifying what are the concerns that impact more in the energy consumption.
A variability model of different designs and implementations of an energy model should
exist for this task, as well as a service that stores and compares the experimentation
results of energy and time consumption of each concern, finding out what is the most
eco-efficient solution. The experimental measurements are performed by energy experts
and researchers that share the energy model and metrics in a collaborative repository.
HADAS confronts these tasks modelling and reasoning with the variability of energy
consuming concerns for different energy contexts, connecting HADAS variability model
with its energy efficiency collaborative repository, establishing a Software Product
Line (SPL) service. Our main goal is to help developers to perform sustainability
analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit
prototype is implemented based on a Clafer model and Choco solver, and it has been
tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {software product line, optimisation, variability, clafer, CVL, repository, energy efficiency, metrics},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.5555/2821481.2821486,
author = {Van Landuyt, Dimitri and Joosen, Wouter},
title = {On the Role of Early Architectural Assumptions in Quality Attribute Scenarios: A Qualitative and Quantitative Study},
year = {2015},
publisher = {IEEE Press},
abstract = {Architectural assumptions are fundamentally different from architectural decisions
because they can not be traced directly to requirements, nor to domain, technical
or environmental constraints; they represent conditions under which the designed solution
is expected to be valid. Early architectural assumptions are similar in nature, with
the key difference that they are not made during architectural design but during requirement
elicitation, not by the software architect (a solution-oriented stakeholder), but
by the requirements engineer (a problem-oriented stakeholder). They represent initial
assumptions about the system's architecture, and allow the requirements engineer to
be more precise in documenting the requirements of the system.The role of early architectural
assumptions in the current practice of quality attribute scenario elicitation and
related development activities in the transition to architecture is unknown and under-investigated.
In this paper, we present the results of an exploratory study that focuses on the
role and nature of these assumptions in the early development stages. We studied a
reasonably large set of quality attribute scenarios for a realistic industrial case,
a smart metering system. Our study (i) confirms that quality attribute scenario elicitation
in practice does rely heavily on early architectural assumptions, and (ii) shows that
they do influence the perceived quality of the requirements body as a whole, in some
cases positively, in other cases negatively.These findings provide empirical arguments
in favor of making such assumptions explicit already during the requirements elicitation
activities. Especially in the context of iterative software development methodologies
such as the Twin Peaks model, a well-defined and -documented set of assumptions could
smoothen the transition between successive development iterations.},
booktitle = {Proceedings of the Fifth International Workshop on Twin Peaks of Requirements and Architecture},
pages = {9–15},
numpages = {7},
location = {Florence, Italy},
series = {TwinPeaks '15}
}

@inproceedings{10.1145/2745802.2745822,
author = {Stevanetic, Srdjan and Zdun, Uwe},
title = {Software Metrics for Measuring the Understandability of Architectural Structures: A Systematic Mapping Study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745822},
doi = {10.1145/2745802.2745822},
abstract = {The main idea of software architecture is to concentrate on the "big picture" of a
software system. In the context of object-oriented software systems higher-level architectural
structures or views above the level of classes are frequently used to capture the
"big picture" of the system. One of the critical aspects of these higher-level views
is understandability, as one of their main purposes is to enable designers to abstract
away fine-grained details. In this article we present a systematic mapping study on
software metrics related to the understandability concepts of such higher-level software
structures with regard to their relations to the system implementation. In our systematic
mapping study, we started from 3951 studies obtained using an electronic search in
the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our
inclusion/exclusion criteria as well as the snowballing technique we selected 268
studies for in-depth study. From those, we selected 25 studies that contain relevant
metrics. We classify the identified studies and metrics with regard to the measured
artefacts, attributes, quality characteristics, and representation model used for
the metrics definitions. Additionally, we present the assessment of the maturity level
of the identified studies. Overall, there is a lack of maturity in the studies. We
discuss possible techniques how to mitigate the identified problems. From the academic
point of view we believe that our study is a good starting point for future studies
aiming at improving the existing works. From a practitioner's point of view, the results
of our study can be used as a catalogue and an indication of the maturity of the existing
research results.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {21},
numpages = {14},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1145/2949550.2949652,
author = {Hu, Hao and Hong, Xingchen and Terstriep, Jeff and Liu, Yan Y. and Finn, Michael P. and Rush, Johnathan and Wendel, Jeffrey and Wang, Shaowen},
title = {TopoLens: Building a CyberGIS Community Data Service for Enhancing the Usability of High-Resolution National Topographic Datasets},
year = {2016},
isbn = {9781450347556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2949550.2949652},
doi = {10.1145/2949550.2949652},
abstract = {Geospatial data, often embedded with geographic references, are important to many
application and science domains, and represent a major type of big data. The increased
volume and diversity of geospatial data have caused serious usability issues for researchers
in various scientific domains, which call for innovative cyberGIS solutions. To address
these issues, this paper describes a cyberGIS community data service framework to
facilitate geospatial big data access, processing, and sharing based on a hybrid supercomputer
architecture. Through the collaboration between the CyberGIS Center at the University
of Illinois at Urbana-Champaign (UIUC) and the U.S. Geological Survey (USGS), a community
data service for accessing, customizing, and sharing digital elevation model (DEM)
and its derived datasets from the 10-meter national elevation dataset, namely TopoLens,
is created to demonstrate the workflow integration of geospatial big data sources,
computation, analysis needed for customizing the original dataset for end user needs,
and a friendly online user environment. TopoLens provides online access to precomputed
and on-demand computed high-resolution elevation data by exploiting the ROGER supercomputer.
The usability of this prototype service has been acknowledged in community evaluation.},
booktitle = {Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},
articleno = {39},
numpages = {8},
keywords = {web-based gateway environment, CyberGIS, microservices, geospatial big data, elevation data, data sharing},
location = {Miami, USA},
series = {XSEDE16}
}

@article{10.1145/3377138,
author = {Wu, Hao and Liu, Weizhi and Lin, Huanxin and Wang, Cho-Li},
title = {A Model-Based Software Solution for Simultaneous Multiple Kernels on GPUs},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3377138},
doi = {10.1145/3377138},
abstract = {As a critical computing resource in multiuser systems such as supercomputers, data
centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking
is an intuitive solution to underutilization in GPGPU computing. Recently proposed
solutions of multitasking GPUs can be classified into two categories: (1) spatially
partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of
compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels
simultaneously within a CU. Compared to SPS, SMK can improve resource utilization
even further due to the interleaving of instructions from kernels with low dynamic
resource contentions.However, it is hard to implement SMK on current GPU architecture,
because (1) techniques for applying SMK on top of GPU hardware scheduling policy are
scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences
of concurrently executed kernels. In this article, we propose a lightweight and effective
performance model to evaluate the complex interferences of SMK. Based on the probability
of independent events, our performance model is built from a totally new angle and
contains limited parameters. Then, we propose a metric, symbiotic factor, which can
evaluate an SMK scheme so that kernels with complementary resource utilization can
corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing
and kernel stretching techniques and integrate them to apply SMK on GPUs instead of
simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based
concurrent kernel execution whose kernel launching order brings fast execution time,
the results of corunning kernel pairs show 11%, 18%, and 12% speedup on AMD R9 290X,
RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the
results show 29%, 18%, and 51% speedup on AMD R9 290X, RX 480, and Vega 64, respectively,
on average.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {7},
numpages = {26},
keywords = {concurrent kernel execution, GPGPU}
}

@inproceedings{10.1145/3209978.3210005,
author = {Mohammad, Hafeezul Rahman and Xu, Keyang and Callan, Jamie and Culpepper, J. Shane},
title = {Dynamic Shard Cutoff Prediction for Selective Search},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210005},
doi = {10.1145/3209978.3210005},
abstract = {Selective search architectures use resource selection algorithms such as Rank-S or
Taily to rank index shards and determine how many to search for a given query. Most
prior research evaluated solutions by their ability to improve efficiency without
significantly reducing early-precision metrics such as P@5 and NDCG@10. This paper
recasts selective search as an early stage of a multi-stage retrieval architecture,
which makes recall-oriented metrics more appropriate. A new algorithm is presented
that predicts the number of shards that must be searched for a given query in order
to meet recall-oriented goals. Decoupling shard ranking from deciding how many shards
to search clarifies efficiency vs. effectiveness trade-offs, and enables them to be
optimized independently. Experiments on two corpora demonstrate the value of this
approach.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {85–94},
numpages = {10},
keywords = {resource selection, distributed search, selective search},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3204949.3204956,
author = {Mangla, Tarun and Zegura, Ellen and Ammar, Mostafa and Halepovic, Emir and Hwang, Kyung-Wook and Jana, Rittwik and Platania, Marco},
title = {VideoNOC: Assessing Video QoE for Network Operators Using Passive Measurements},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3204956},
doi = {10.1145/3204949.3204956},
abstract = {Video streaming traffic is rapidly growing in mobile networks. Mobile Network Operators
(MNOs) are expected to keep up with this growing demand, while maintaining a high
video Quality of Experience (QoE). This makes it critical for MNOs to have a solid
understanding of users' video QoE with a goal to help with network planning, provisioning
and traffic management. However, designing a system to measure video QoE has several
challenges: i) large scale of video traffic data and diversity of video streaming
services, ii) cross-layer constraints due to complex cellular network architecture,
and iii) extracting QoE metrics from network traffic. In this paper, we present VideoNOC,
a prototype of a flexible and scalable platform to infer objective video QoE metrics
(e.g., bitrate, rebuffering) for MNOs. We describe the design and architecture of
VideoNOC, and outline the methodology to generate a novel data source for fine-grained
video QoE monitoring. We then demonstrate some of the use cases of such a monitoring
system. VideoNOC reveals video demand across the entire network, provides valuable
insights on a number of design choices by content providers (e.g., OS-dependent performance,
video player parameters like buffer size, range of encoding bitrates, etc.) and helps
analyze the impact of network conditions on video QoE (e.g., mobility and high demand).},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {101–112},
numpages = {12},
keywords = {video streaming, passive measurement, cellular network, QoE},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/3167486.3167534,
author = {Seraoui, Youssef and Belmekki, Mostafa and Bellafkih, Mostafa and Raouyane, Brahim},
title = {ETOM Mapping onto NFV Framework: IMS Use Case},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167534},
doi = {10.1145/3167486.3167534},
abstract = {Telecom professionals have a strong interest in the proposition and adaptation of
innovate network management models and frameworks to help mobile network operators
(MNOs) to improve their business processes and get more agile in the telecoms industry
that evolves with great speed. The model being established by the TeleManagement Forum
(TM Forum) is the Enhanced Telecom Operations MAP (eTOM) business process framework
on which we rely in this work to propose a mapping of the eTOM model onto the network
functions virtualization (NFV) framework with the projection of this function mapping
onto the IP Multimedia Subsystem (IMS) use case. This mapping covers essentially four
main components playing important rules in the MNO's business processes, including
customers, services, infrastructure resources, and also service providers. The main
goal, thereby, is to design a combined architecture in a virtualized environment for
dynamic delivery of services with quality of service (QoS) and improved resource performance
so as to meet the purposes of the 5G network in terms of a proposed, virtual telecom
environment managed and orchestrated by the conjunction of the aforementioned paradigms.
Indeed, we conducted simulations to evaluate part of this function mapping in an IMS
setting for static service chain provisioning. Thus, results showed possible provisioning
of services in this context in measuring SIP related key performance indicators and
performance metrics. Results showed the feasibility of our approach. In addition,
resource performance improved obviously in the NFV context in accordance with eTOM
business processes.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {45},
numpages = {8},
keywords = {New Generation Operations Systems and Software (NGOSS), service chain provisioning, network functions virtualization (NFV), IP Multimedia Subsystem (IMS), resource performance, Enhanced Telecom Operations Map (eTOM), five generation (5G), Business process, quality of service (QoS)},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@inproceedings{10.1145/3286685.3286686,
author = {Saurez, Enrique and Balasubramanian, Bharath and Schlichting, Richard and Tschaen, Brendan and Huang, Zhe and Narayanan, Shankaranarayanan Puzhavakath and Ramachandran, Umakishore},
title = {METRIC: A Middleware for Entry Transactional Database Clustering at the Edge},
year = {2018},
isbn = {9781450361170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286685.3286686},
doi = {10.1145/3286685.3286686},
abstract = {A geo-distributed database for edge architectures spanning thousands of sites needs
to assure efficient local updates while replicating sufficient state across sites
to enable global management and support mobility, failover etc. To address this requirement,
a new paradigm for database clustering that achieves a better balance than existing
solutions between performance and strength of semantics called entry transactionality
is introduced. Inspired by entry consistency in shared memory systems, entry transactionality
guarantees that only a client that owns a range of keys in the database has a sequentially
consistent value of the keys and can perform local and, hence, efficient transactions
across these keys. Important use cases enabled by entry transactionality such as federated
controllers and state management for edge applications are identified. The semantics
of entry transactionality incorporating the complex failure modes in geo-distributed
services are defined, and the difficult challenges in realizing these semantics are
outlined. Then, a novel Middleware for Entry Transactional Clustering (METRIC) that
combines existing SQL databases with an underlying geo-distributed entry consistent
store to realize entry transactionality is described. This paper provides initial
findings from an on-going effort.},
booktitle = {Proceedings of the 3rd Workshop on Middleware for Edge Clouds &amp; Cloudlets},
pages = {2–7},
numpages = {6},
location = {Rennes, France},
series = {MECC'18}
}

@inproceedings{10.1145/3130218.3130225,
author = {Wang, Zicong and Chen, Xiaowen and Li, Chen and Guo, Yang},
title = {Fairness-Oriented and Location-Aware NUCA for Many-Core SoC},
year = {2017},
isbn = {9781450349840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3130218.3130225},
doi = {10.1145/3130218.3130225},
abstract = {Non-uniform cache architecture (NUCA) is often employed to organize the last level
cache (LLC) by Networks-on-Chip (NoC). However, along with the scaling up for network
size of Systems-on-Chip (SoC), two trends gradually begin to emerge. First, the network
latency is becoming the major source of the cache access latency. Second, the communication
distance and latency gap between different cores is increasing. Such gap can seriously
cause the network latency imbalance problem, aggravate the degree of non-uniform for
cache access latencies, and then worsen the system performance.In this paper, we propose
a novel NUCA-based scheme, named fairness-oriented and location-aware NUCA (FL-NUCA),
to alleviate the network latency imbalance problem and achieve more uniform cache
access. We strive to equalize network latencies which are measured by three metrics:
average latency (AL), latency standard deviation (LSD), and maximum latency (ML).
In FL-NUCA, the memory-to-LLC mapping and links are both non-uniform distributed to
better fit the network topology and traffics, thereby equalizing network latencies
from two aspects, i.e., non-contention latencies and contention latencies, respectively.
The experimental results show that FL-NUCA can effectively improve the fairness of
network latencies. Compared with the traditional static NUCA (S-NUCA), in simulation
with synthetic traffics, the average improvements for AL, LSD, and ML are 20.9%, 36.3%,
and 35.0%, respectively. In simulation with PARSEC benchmarks, the average improvements
for AL, LSD, and ML are 6.3%, 3.6%, and 11.2%, respectively.},
booktitle = {Proceedings of the Eleventh IEEE/ACM International Symposium on Networks-on-Chip},
articleno = {13},
numpages = {8},
keywords = {Networks-on-chip, non-uniform cache architecture, memory mapping},
location = {Seoul, Republic of Korea},
series = {NOCS '17}
}

@inproceedings{10.1145/3323716.3323729,
author = {Berba, Elizalde M. and Palaoag, Thelma D.},
title = {Improving Customer Satisfaction on Internet Services in L-NU Using Virtualized AAA Network Architecture},
year = {2019},
isbn = {9781450361040},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323716.3323729},
doi = {10.1145/3323716.3323729},
abstract = {This study mainly aims to improve the satisfaction level on internet services in Lyceum-Northwestern
University (L-NU). From a traditional network architecture, the researchers made use
of a virtualized Authentication, Authorization and Accounting (AAA) network architecture
to improve the internet services provided to the students of L-NU. For the methodology
of this study, the researchers had to make use of a network lifecycle called Prepare,
Plan, Design, Implement, Operate, and Optimize (PPDIOO) and there was also a need
to combine both quantitative and qualitative research approach. To make this happen,
the researchers had to fulfill the following objectives: a) determine the current
network setup of L-NU, b) measure the current satisfaction level of the users, c)
design, develop and implement a virtualized AAA network architecture, d) measure the
satisfaction level of the users who have used the AAA network setup, and e) compare
the measured satisfaction level from the users who used the internet facilities using
the traditional network architecture and satisfaction level from users who have used
the internet facilities after the implementation of AAA network architecture. As a
result, it was found out that the implementation of the new network architecture has
significantly improved the internet service level of L-NU which is reflected by a
higher customer satisfaction rating. Therefore, the researchers conclude that it is
most essential that AAA network architecture be implemented to enterprise type of
network setup such as but not limited to education institutions in managing their
internet services. Consequently, this kind of network architecture lead to a more
effective and more efficient way of managing network resources of an institution or
an organization while further improving the satisfaction level. In order to optimize
the AAA network architecture and gain more implementation advantages, virtualization
technology was used to contain and run numerous operating system instances such as
four physical servers into one single physical server which favors to saving resources
such as energy, space, money and of which also leads to simplified administration.},
booktitle = {Proceedings of the 8th International Conference on Informatics, Environment, Energy and Applications},
pages = {178–183},
numpages = {6},
keywords = {authorization, AAA, authentication, hypervisor, accounting, customer satisfaction, PPDIOO, virtualization},
location = {Osaka, Japan},
series = {IEEA '19}
}

@inproceedings{10.1145/2619239.2631456,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
title = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2631456},
doi = {10.1145/2619239.2631456},
abstract = {After more than two decades of evolution, TCP and its end host based modifications
can still suffer from severely degraded performance under real-world challenging network
conditions. The reason, as we observe, is due to TCP family's fundamental architectural
deficiency, which hardwires packet-level events to control responses and ignores emprical
performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented
Congestion Control (PCC), a new congestion control architecture in which each sender
controls its sending strategy based on empirically observed performance metrics. We
show through preliminary experimental results that PCC achieves consistently high
performance under various challenging network conditions.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {365–366},
numpages = {2},
keywords = {congestion control},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@article{10.1145/2740070.2631456,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael},
title = {Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2631456},
doi = {10.1145/2740070.2631456},
abstract = {After more than two decades of evolution, TCP and its end host based modifications
can still suffer from severely degraded performance under real-world challenging network
conditions. The reason, as we observe, is due to TCP family's fundamental architectural
deficiency, which hardwires packet-level events to control responses and ignores emprical
performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented
Congestion Control (PCC), a new congestion control architecture in which each sender
controls its sending strategy based on empirically observed performance metrics. We
show through preliminary experimental results that PCC achieves consistently high
performance under various challenging network conditions.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {365–366},
numpages = {2},
keywords = {congestion control}
}

@inproceedings{10.1145/2568088.2568098,
author = {Ewing, John M. and Menasc\'{e}, Daniel A.},
title = {A Meta-Controller Method for Improving Run-Time Self-Architecting in SOA Systems},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2568098},
doi = {10.1145/2568088.2568098},
abstract = {This paper builds on SASSY, a system for automatically generating SOA software architectures
that optimize a given utility function of multiple QoS metrics. In SASSY, SOA software
systems are automatically re-architected when services fail or degrade. Optimizing
both architecture and service provider selection presents a pair of nested NP-hard
problems. Here we adapt hill-climbing, beam search, simulated annealing, and evolutionary
programming to both architecture optimization and service provider selection. Each
of these techniques has several parameters that influence their efficiency. We introduce
in this paper a meta-controller that automates the run-time selection of heuristic
search techniques and their parameters. We examine two different meta-controller implementations
that each use online learning. The first implementation identifies the best heuristic
search combination from various prepared combinations. The second implementation analyzes
the current self-architecting problem (e.g. changes in performance metrics, service
degradations/failures) and looks for similar, previously encountered re-architecting
problems to find an effective heuristic search combination for the current problem.
A large set of experiments demonstrates the effectiveness of the first meta-controller
implementation and indicates opportunities for improving the second meta-controller
implementation.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {173–184},
numpages = {12},
keywords = {heuristic search, meta-controlled qos optimization, autonomic computing, soa, combinatorial search techniques, metaheuristics, automated run-time software architecting},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@inproceedings{10.1109/CCGrid.2015.152,
author = {Kuang, Wei and Brown, Laura E. and Wang, Zhenlin},
title = {Modeling Cross-Architecture Co-Tenancy Performance Interference},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.152},
doi = {10.1109/CCGrid.2015.152},
abstract = {Cloud computing has become a dominant computing paradigm to provide elastic, affordable
computing resources to end users. Due to the increased computing power of modern machines
powered by multi/many-core computing, data centers often co-locate multiple virtual
machines (VMs) into one physical machine, resulting in co-tenancy, and resource sharing
and competition. Applications or VMs co-locating in one physical machine can interfere
with each other despite of the promise of performance isolation through virtualization.
Modeling and predicting co-run interference therefore becomes critical for data center
job scheduling and QoS (Quality of Service) assurance. Co-run interference can be
categorized into two metrics, sensitivity and pressure, where the former denotes how
an application's performance is affected by its co-run applications, and the latter
measures how it impacts the performance of its co-run applications. This paper shows
that sensitivity and pressure are both application- and architecture-dependent. Further,
we propose a regression model that predicts an application's sensitivity and pressure
across architectures with high accuracy. This regression model enables a data center
scheduler to guarantee the QoS of a VM/application when it is scheduled to co-locate
with another VMs/applications.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {231–240},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3286062.3286086,
author = {Sharma, Puneet and Raghuramu, Arun and Lee, David and Saxena, Vinay and Chuah, Chen-Nee},
title = {We Don't Need No Licensing Server},
year = {2018},
isbn = {9781450361200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286062.3286086},
doi = {10.1145/3286062.3286086},
abstract = {Cloudification of edge to core infrastructure has led to new and rich application
and service deployment and operational models. These ecosystems have complex relationships
between the application vendors, infrastructure operators and application users. Traditional
licensing and compliance enforcement methods such as those based on in person audits
and dynamic issuing of license keys inhibit the resource provisioning and consumption
flexibility offered by cloudified services due to scalability and management overheads.
In this work, we argue the need for a trusted framework for application usage rights
compliance. This new architecture named "Metered Boot" provides a way to realize trusted,
capacity/usage based rights compliance for service deployments that allows decoupling
of usage rights governed by application vendors from the resource provisioning by
the infrastructure provider. We have built a Metered Boot prototype for a particular
usecase of NFV usage rights compliance.},
booktitle = {Proceedings of the 17th ACM Workshop on Hot Topics in Networks},
pages = {162–168},
numpages = {7},
location = {Redmond, WA, USA},
series = {HotNets '18}
}

@inproceedings{10.1109/ICSE-NIER.2019.00037,
author = {Aniche, Maur\'{\i}cio and Yoder, Joseph W. and Kon, Fabio},
title = {Current Challenges in Practical Object-Oriented Software Design},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00037},
doi = {10.1109/ICSE-NIER.2019.00037},
abstract = {According to the extensive 50-year-old body of knowledge in object-oriented programming
and design, good software designs are, among other characteristics, lowly coupled,
highly cohesive, extensible, comprehensible, and not fragile. However, with the increased
complexity and heterogeneity of contemporary software, this might not be enough.This
paper discusses the practical challenges of object-oriented design in modern software
development. We focus on three main challenges: (1) how technologies, frameworks,
and architectures pressure developers to make design decisions that they would not
take in an ideal scenario, (2) the complexity of current real-world problems require
developers to devise not only a single, but several models for the same problem that
live and interact together, and (3) how existing quality assessment techniques for
object-oriented design should go beyond high-level metrics.Finally, we propose an
agenda for future research that should be tackled by both scientists and practitioners
soon. This paper is a call for arms for more reality-oriented research on the object-oriented
software design field.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {113–116},
numpages = {4},
keywords = {domain modeling, software architecture, class design, object-oriented programming, software engineering, object-oriented design, software design},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/3176258.3176328,
author = {Alshehri, Asma and Benson, James and Patwa, Farhan and Sandhu, Ravi},
title = {Access Control Model for Virtual Objects (Shadows) Communication for AWS Internet of Things},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176328},
doi = {10.1145/3176258.3176328},
abstract = {The concept of Internet of Things (IoT) has received considerable attention and development
in recent years. There have been significant studies on access control models for
IoT in academia, while companies have already deployed several cloud-enabled IoT platforms.
However, there is no consensus on a formal access control model for cloud-enabled
IoT. The access-control oriented (ACO) architecture was recently proposed for cloud-enabled
IoT, with virtual objects (VOs) and cloud services in the middle layers. Building
upon ACO, operational and administrative access control models have been published
for virtual object communication in cloud-enabled IoT illustrated by a use case of
sensing speeding cars as a running example.In this paper, we study AWS IoT as a major
commercial cloud-IoT platform and investigate its suitability for implementing the
afore-mentioned academic models of ACO and VO communication control. While AWS IoT
has a notion of digital shadows closely analogous to VOs, it lacks explicit capability
for VO communication and thereby for VO communication control. Thus there is a significant
mismatch between AWS IoT and these academic models. The principal contribution of
this paper is to reconcile this mismatch by showing how to use the mechanisms of AWS
IoT to effectively implement VO communication models. To this end, we develop an access
control model for virtual objects (shadows) communication in AWS IoT called AWS-IoT-ACMVO.
We develop a proof-of-concept implementation of the speeding cars use case in AWS
IoT under guidance of this model, and provide selected performance measurements. We
conclude with a discussion of possible alternate implementations of this use case
in AWS IoT.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {175–185},
numpages = {11},
keywords = {devices, abac, security, iot architecture, acl, aws iot, internet of things (iot), access control, virtual objects, rbac},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1145/3453688.3461512,
author = {Gao, Chengsi and Li, Bing and Wang, Ying and Chen, Weiwei and Zhang, Lei},
title = {Tenet: A Neural Network Model Extraction Attack in Multi-Core Architecture},
year = {2021},
isbn = {9781450383936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453688.3461512},
doi = {10.1145/3453688.3461512},
abstract = {As neural networks (NNs) are being widely deployed in many cloud-oriented systems
for safety-critical tasks, the privacy and security of NNs become significant concerns
to users in the cloud platform that shares the computation infrastructure such as
memory resource. In this work, we observed that the memory timing channel in the shared
memory of cloud multi-core architecture poses the risk of network model information
leakage. Based on the observation, we propose a learning-based method to steal the
model architecture of the NNs by exploiting the memory timing channel without any
high-level privilege or physical access. We first trained an end-to-end measurement
network offline to learn the relation between memory timing information and NNs model
architecture. Then, we performed an online attack and reconstructed the target model
using the prediction from the measurement network. We evaluated the proposed attack
method on a multi-core architecture simulator. The experimental results show that
our learning-based attack method can reconstruct the target model with high accuracy
and improve the adversarial attack success rate by 42.4%.},
booktitle = {Proceedings of the 2021 on Great Lakes Symposium on VLSI},
pages = {21–26},
numpages = {6},
keywords = {deep learning security, multi-core, machine learning, memory timing channel},
location = {Virtual Event, USA},
series = {GLSVLSI '21}
}

@inproceedings{10.1145/2661714.2661726,
author = {Stohr, Denny and Wilk, Stefan and Effelsberg, Wolfgang},
title = {Monitoring of User Generated Video Broadcasting Services},
year = {2014},
isbn = {9781450331579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661714.2661726},
doi = {10.1145/2661714.2661726},
abstract = {Mobile video broadcasting services offer users the opportunity to instantly share
content from their mobile handhelds to a large audience over the Internet. However,
existing data caps in cellular network contracts and limitations in their upload capabilities
restrict the adoption of mobile video broadcasting services. Additionally, the quality
of those video streams is often reduced by the lack of skills of recording users and
the technical limitations of the video capturing devices. Our research focuses on
large-scale events that attract dozens of users to record video in parallel. In many
cases, available network infrastructure is not capable to upload all video streams
in parallel. To make decisions on how to appropriately transmit those video streams,
a suitable monitoring of the video generation process is required. For this scenario,
a measurement framework is proposed that allows Internet-scale mobile broadcasting
services to deliver samples in an optimized way. Our framework architecture analyzes
three zones for effectively monitoring user-generated video. Besides classical Quality
of Service metrics on the network state, video quality indicators and additional auxiliary
sensor information is gathered. Aim of this framework is an efficient coordination
of devices and their uploads based on the currently observed system state.},
booktitle = {Proceedings of the First International Workshop on Internet-Scale Multimedia Management},
pages = {39–42},
numpages = {4},
keywords = {cellular networks, mobile, mix, video broadcast, network monitoring, video composition, measurement},
location = {Orlando, Florida, USA},
series = {WISMM '14}
}

@article{10.1145/2983637,
author = {Miao, Wang and Min, Geyong and Wu, Yulei and Wang, Haozhe and Hu, Jia},
title = {Performance Modelling and Analysis of Software-Defined Networking under Bursty Multimedia Traffic},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2983637},
doi = {10.1145/2983637},
abstract = {Software-Defined Networking (SDN) is an emerging architecture for the next-generation
Internet, providing unprecedented network programmability to handle the explosive
growth of big data driven by the popularisation of smart mobile devices and the pervasiveness
of content-rich multimedia applications. In order to quantitatively investigate the
performance characteristics of SDN networks, several research efforts from both simulation
experiments and analytical modelling have been reported in the current literature.
Among those studies, analytical modelling has demonstrated its superiority in terms
of cost-effectiveness in the evaluation of large-scale networks. However, for analytical
tractability and simplification, existing analytical models are derived based on the
unrealistic assumptions that the network traffic follows the Poisson process, which
is suitable to model nonbursty text data, and the data plane of SDN is modelled by
one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies
have shown that, due to the features of heavy volume and high velocity, the multimedia
big data generated by real-world multimedia applications reveals the bursty and correlated
nature in the network transmission. With the aim of capturing such features of realistic
traffic patterns and obtaining a comprehensive and deeper understanding of the performance
behaviour of SDN networks, this article presents a new analytical model to investigate
the performance of SDN in the presence of the bursty and correlated arrivals modelled
by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance
metrics in terms of the average latency and average network throughput of the SDN
networks are derived based on the developed analytical model. To consider a realistic
multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted
to model the SDN data plane. To address the challenging problem of obtaining the key
performance metrics, for example, queue-length distribution of a PQ system with a
given service capacity, a versatile methodology extending the Empty Buffer Approximation
(EBA) method is proposed to facilitate the decomposition of such a PQ system to two
SSSQ systems. The validity of the proposed model is demonstrated through extensive
simulation experiments. To illustrate its application, the developed model is then
utilised to study the strategy of the network configuration and resource allocation
in SDN networks.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = sep,
articleno = {77},
numpages = {19},
keywords = {performance modelling and analysis, Software-defined networking, multimedia big data, queueing decomposition, resource allocation}
}

@inproceedings{10.1145/3437120.3437292,
author = {Maikantis, Theodoros and Tsintzira, Angeliki-Agathi and Ampatzoglou, Apostolos and Arvanitou, Elvira-Maria and Chatzigeorgiou, Alexander and Stamelos, Ioannis and Bibi, Stamatia and Deligiannis, Ignatios},
title = {Software Architecture Reconstruction via a Genetic Algorithm: Applying the Move Class Refactoring},
year = {2020},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437292},
doi = {10.1145/3437120.3437292},
abstract = {Modularity is one of the four key principles of software design and architecture.
According to this principle, software should be organized into modules that are tightly
linked internally (high cohesion), whereas at the same time as independent from other
modules as possible (low coupling). However, in practice, this principle is violated
due to poor architecting design decisions, lack of time, or coding shortcuts, leading
to a phenomenon termed as architectural technical debt (ATD). To alleviate this problem
(lack of architectural modularity), the most common solution is the application of
a software refactoring, namely Move Class—i.e., moving classes (the core artifact
in object-oriented systems) from one module to another. To identify Move Class refactoring
opportunities, we employ a search-based optimization process, relying on optimization
metrics, through which optimal moves are derived. Given the extensive search space
required for applying a brute-force search strategy, in this paper, we propose the
use of a genetic algorithm that re-arranges existing software classes into existing
or new modules (software packages in Java, or folders in C++). To validate the usefulness
of the proposed refactorings, we performed an industrial case study on three projects
(from the Aviation, Healthcare, and Manufacturing application domains). The results
of the study indicate that the proposed architecture reconstruction is able to improve
modularity, improving both coupling and cohesion. The obtained results can be useful
to practitioners through an open source tool; whereas at the same point, they open
interesting future work directions.},
booktitle = {24th Pan-Hellenic Conference on Informatics},
pages = {135–139},
numpages = {5},
location = {Athens, Greece},
series = {PCI 2020}
}

@inproceedings{10.1145/3405656.3418718,
author = {G\"{u}ndo\u{g}an, Cenk and Ams\"{u}ss, Christian and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
title = {Toward a RESTful Information-Centric Web of Things: A Deeper Look at Data Orientation in CoAP},
year = {2020},
isbn = {9781450380409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405656.3418718},
doi = {10.1145/3405656.3418718},
abstract = {The information-centric networking (ICN) paradigm offers replication of autonomously
verifiable content throughout a network, in which content is bound to names instead
of hosts. This has proven beneficial in particular for the constrained IoT. Several
approaches, the most prominent of which being Named Data Networking, propose access
to named content directly on the network layer. Independently, the IETF CoAP protocol
group started to develop mechanisms that support autonomous content processing and
in-network storage.In this paper, we explore the emerging CoAP protocol building blocks
and how they contribute to an information-centric network architecture for a data-oriented
RESTful Web of Things. We discuss design options and measure characteristic performances
of different network configurations, which deploy CoAP proxies and OSCORE content
object security, and compare with NDN. Our findings indicate an almost continuous
design space ranging from plain CoAP at the one end to NDN on the other. On both ends---ICN
and CoAP---we identify protocol features and aspects whose mutual transfer potentially
improves design and operation of the other.},
booktitle = {Proceedings of the 7th ACM Conference on Information-Centric Networking},
pages = {77–88},
numpages = {12},
keywords = {Internet of Things, ICN, protocol evaluation, content object security, CoAP Proxy, OSCORE},
location = {Virtual Event, Canada},
series = {ICN '20}
}

@article{10.1145/3151123.3151125,
author = {Zeinalipour-Yazti, Demetrios and Laoudias, Christos},
title = {The Anatomy of the Anyplace Indoor Navigation Service},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3151123.3151125},
doi = {10.1145/3151123.3151125},
abstract = {The pervasiveness of smartphones is leading to the uptake of a new class of Internet-based
Indoor Navigation (IIN) services, which might soon diminish the need of Satellite-based
localization technologies in urban environments. These services rely on geo-location
databases that store spatial models along with wireless, light and magnetic signals
used to localize users and provide better power efficiency and wider coverage than
predominant approaches. In this article we overview Anyplace, an open, modular, extensible
and scalable navigation architecture that exploits crowdsourced Wi-Fi data to develop
a novel navigation service that won several international research awards for its
utility and accuracy (i.e., less than 2 meters). Our MIT-licenced open-source software
stack has to this date been used by thaousands of researchers and practitioners around
the globe, with the public Anyplace service reaching over 100,000 real user interactions.},
journal = {SIGSPATIAL Special},
month = oct,
pages = {3–10},
numpages = {8}
}

