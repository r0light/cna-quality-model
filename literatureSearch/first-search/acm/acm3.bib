@inproceedings{10.1145/3379310.3379320,
author = {Lumba, Ester and Waworuntu, Alexander},
title = {Application of Lecturer Performance Report in Indonesia with Model View Controller (MVC) Architecture},
year = {2020},
isbn = {9781450376853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379310.3379320},
doi = {10.1145/3379310.3379320},
abstract = {Lecturers in Indonesia have a fundamental obligation to conduct Tri Dharma activities
consisting of teaching, research and community service. Most higher education institutions
use Tri Dharma as a measure of lecturer's performance. In addition, lecturer activity
data related to Tri Dharma is needed by the head of study program and department related
to research, publication and community service to be stored which will be used as
a source of data during the accreditation process. This paper discusses the application
development of lecturer performance reports using the Model View Controller (MVC)
architecture with Java programming language. The result is a desktop-based application
that will be used by the head of the study program and the lecturers.},
booktitle = {Proceedings of the 2020 2nd Asia Pacific Information Technology Conference},
pages = {23–28},
numpages = {6},
keywords = {desktop-based application, application development, MVC architecture, Indonesia higher-education},
location = {Bali Island, Indonesia},
series = {APIT 2020}
}

@inproceedings{10.1145/3338466.3358916,
author = {Alder, Fritz and Asokan, N. and Kurnikov, Arseny and Paverd, Andrew and Steiner, Michael},
title = {S-FaaS: Trustworthy and Accountable Function-as-a-Service Using Intel SGX},
year = {2019},
isbn = {9781450368261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338466.3358916},
doi = {10.1145/3338466.3358916},
abstract = {Function-as-a-Service (FaaS) is a recent and popular cloud computing paradigm in which
the function provider specifies a function to be run and is billed only for the computational
resources used by that function. Compared to other cloud paradigms, FaaS requires
significantly more fine-grained measurement of functions' compute time and memory
usage. Since functions are short and stateless, small ephemeral entities (e.g. individuals
or underutilized data centers) can become FaaS service providers. However, this exacerbates
the already substantial challenges of 1) ensuring integrity of computation, 2) minimizing
information revealed to the service provider, and 3) accurately measuring computational
resource usage.To address these challenges, we introduce S-FaaS, the first architecture
and implementation of FaaS to provide strong security and accountability guarantees
using Intel SGX. To match the dynamic event-driven nature of FaaS, we introduce a
new key distribution enclave and a novel transitive attestation protocol. A core contribution
of S-FaaS is our set of reusable resource measurement mechanisms that securely measure
compute time and memory usage inside an enclave. We have integrated S-FaaS into the
OpenWhisk FaaS framework and provide this as open source software.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop},
pages = {185–199},
numpages = {15},
keywords = {intel sgx, function-as-a-service, resource measurement},
location = {London, United Kingdom},
series = {CCSW'19}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-Grained Test Case Prioritization for Integration Testing of Delta-Oriented Software Product Lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = { Software product line (SPL) testing is a challenging task, due to the huge number
of variants sharing common functionalities to be taken into account for efficient
testing. By adopting the concept of regression testing, incremental SPL testing strategies
cope with this challenge by exploiting the reuse potential of test artifacts between
subsequent variants under test. In previous work, we proposed delta-oriented test
case prioritization for incremental SPL integration testing, where differences between
architecture test model variants allow for reasoning about the order of reusable test
cases to be executed. However, the prioritization left two issues open, namely (1)
changes to component behavior are ignored, which may also influence component interactions
and, (2) the weighting and ordering of similar test cases result in an unintended
clustering of test cases. In this paper, we extend the test case prioritization technique
by (1) incorporating changes to component behavior allowing for a more fine-grained
analysis and (2) defining a dissimilarity measure to avoid clustered test case orders.
We prototyped our test case prioritization technique and evaluated its applicability
and effectiveness by means of a case study from the automotive domain showing positive
results. },
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3318265.3318280,
author = {Hang, Zijun and Shi, Yang and Wen, Mei and Quan, Wei and Zhang, Chunyuan},
title = {SWAP: A Sliding Window Algorithm for in-Network Packet Measurement},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318280},
doi = {10.1145/3318265.3318280},
abstract = {Network traffic measurement is a fundamental part of many network applications, such
as DDOS detection, capacity planning, and quality-of-service improvement. To achieve
this, we need to count the number of packets passed during a past time interval. Traditionally,
switches sample the packets and send them to the CPU for analysis. It is unavoidable
that the sampling will sacrifice the measuring accuracy. Nowadays, programmable switches
can keep the counters in the data plane. However, they still rely on the CPU to drain
and clear the records periodically, which brings in too much communication latency.
To overcome these disadvantages, we propose a metering mechanism under the RMT architectural
model called SWAP. SWAP is carefully designed to count the number of packets during
an interval accurately with little hardware resource usage. We prototype it using
P4 and simulation results show SWAP achieves high efficiency and moderate accuracy
at line speed.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {84–89},
numpages = {6},
keywords = {P4, programmable switches, network algorithm, software-defined networks},
location = {Xi'an, China},
series = {HP3C '19}
}

@inproceedings{10.1145/3320326.3320391,
author = {El Mrabet, Zakaria and Ezzari, Mehdi and Elghazi, Hassan and El Majd, Badr Abou},
title = {Deep Learning-Based Intrusion Detection System for Advanced Metering Infrastructure},
year = {2019},
isbn = {9781450366458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320326.3320391},
doi = {10.1145/3320326.3320391},
abstract = {Smart grid is an alternative solution of the conventional power grid which harnesses
the power of the information technology to save the energy and meet todays' environment
requirements. Due to the inherent vulnerabilities in the information technology, the
smart grid is exposed to wide variety of threats that could be translated into cyber-attacks.
In this paper, we develop a deep learning-based intrusion detection system to defend
against cyber-attacks in the advanced metering infrastructure network. The proposed
machine learning approach is trained and tested extensively on an empirical industrial
dataset which is composed of several attack' categories including the scanning, buffer
overflow, and denial of service attacks. Then, an experimental comparison in terms
of detection accuracy is conducted to evaluate the performance of the proposed approach
with Na\"{\i}ve Bayes, Support Vector Machine, and Random Forest. The obtained results
suggest that the proposed approaches produce optimal results comparing to the other
algorithms. Finally, we propose a network architecture to deploy the proposed anomaly-based
intrusion detection system across the Advanced metering infrastructure network. In
addition, we propose a network security architecture composed of two types of Intrusion
detection system types, Host and Network based, deployed across the Advanced Metering
Infrastructure network to inspect the traffic and detect the malicious one at all
the levels.},
booktitle = {Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security},
articleno = {58},
numpages = {7},
keywords = {Deep learning, Intrusion detection system, Advanced Metering Infrastructure, cross entropy loss, detection accuracy},
location = {Rabat, Morocco},
series = {NISS19}
}

@inproceedings{10.1145/2970276.2970338,
author = {Peldszus, Sven and Kulcs\'{a}r, G\'{e}za and Lochau, Malte and Schulze, Sandro},
title = {Continuous Detection of Design Flaws in Evolving Object-Oriented Programs Using Incremental Multi-Pattern Matching},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970338},
doi = {10.1145/2970276.2970338},
abstract = { Design flaws in object-oriented programs may seriously corrupt code quality thus
increasing the risk for introducing subtle errors during software maintenance and
evolution. Most recent approaches identify design flaws in an ad-hoc manner, either
focusing on software metrics, locally restricted code smells, or on coarse-grained
architectural anti-patterns. In this paper, we utilize an abstract program model capturing
high-level object-oriented code entities, further augmented with qualitative and quantitative
design-related information such as coupling/cohesion. Based on this model, we propose
a comprehensive methodology for specifying object-oriented design flaws by means of
compound rules integrating code metrics, code smells and anti-patterns in a modular
way. This approach allows for efficient, automated design-flaw detection through incremental
multi-pattern matching, by facilitating systematic information reuse among multiple
detection rules as well as between subsequent detection runs on continuously evolving
programs. Our tool implementation comprises well-known anti-patterns for Java programs.
The results of our experimental evaluation show high detection precision, scalability
to real-size programs, as well as a remarkable gain in efficiency due to information
reuse. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {578–589},
numpages = {12},
keywords = {object-oriented software architecture, design-flaw detection, continuous software evolution},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3459104.3459148,
author = {Pradhan, Ayush and Joy, Eldhose and Jawagal, Harsha and Prasad Jayaraman, Sundar},
title = {A Framework for Leveraging Contextual Information in Automated Domain Specific Comprehension},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459148},
doi = {10.1145/3459104.3459148},
abstract = {When it comes to information, Enterprises today are seen as a black hole, a mass of
it goes in but gets difficult to extract the practical knowledge out of it. An automated
system that has the ability to consume this large mass of information and provide
specific, knowledgeable, domain-oriented responses back, will go a long way in unlocking
the value of this large-scale unstructured information. In a bid to enrich the answering
system's accuracy in Machine Reading Comprehension (MRC), we propose a domain-specific
Question Answers (QuAns) framework that specifically aims to auto-generate questions
from a domain-based document using an improvised Sequence to Sequence (Seq2Seq) technique
equipped with Attention and Copy mechanism. The generated questions are conditioned
on a set of candidate answers, derived using a combination of heuristic-driven and
graph-based techniques. Further, it also leverages the contextual information by pooling
strategy to build an automated response system using a deep custom fine-tuned Bidirectional
Encoder Representations from Transformers (BERT) framework and retrieving the top-k
contexts for a user query. The evaluation of the QuAns architecture is performed in
combination with human supervision as at times, the automated metrics like BLEU, Exact
Match (EM), F1 score, etc. fail to gauge the diverse semantic and structural aspects
of a generated response. Primarily, the proffered ensemble technique has leveraged
the augmented domain knowledge to enrich the answering response efficacy and improving
the EM and F1 score by 14.86% and 12.76% respectively over Vanilla BERT architecture.
To enhance the user experience, the conversational system is equipped with Natural
Language Generation (NLG) to present a human-readable response. Our architectural
pipeline aims to provide a one-stop solution for the organizations in processing huge
volumes of multidisciplinary data by significantly reducing the human introspection
and the overhead cost.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {263–270},
numpages = {8},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@article{10.1145/3308897.3308943,
author = {Luong, Doanh Kim and Ali, Muhammad and Benamrane, Fouad and Ammar, Ibrahim and Hu, Yim-Fun},
title = {Seamless Handover for Video Streaming over an SDN-Based Aeronautical Communications Network},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3308897.3308943},
doi = {10.1145/3308897.3308943},
abstract = {There have been increasing interests in applying Software Defined Networking (SDN)
to aeronautical communications primarily for air traffic management purposes. From
the service passenger communications' point of view, a major goal is to improve passengers'
perception of quality of experience on the infotainment services being provided for
them. Due to the high speed of aircrafts and the use of multiple radio technologies
during different flight phases and across different areas, vertical handovers between
these different radio technologies are envisaged. This poses a challenge to maintain
the quality of service during such handovers, especially for high bandwidth applications
such as video streaming. This paper proposes an SDN-based aeronautical communications
architecture consisting of both satellite and terrestrial-based radio technology.
In addition, an experimental implementation of the Locator ID Separation Protocol
(LISP) protocol with built-in multi-homing capability over the SDN-based architecture
was proposed to handle vertical handovers between the satellite and other radio technologies
onboard the aircraft. By using both objective and subjective Quality of Experience
(QoE) metrics, the simulation experiments show the benefit of combining LISP with
SDN to improve the video streaming quality during the handover in the aeronautical
communication environment.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jan,
pages = {98–99},
numpages = {2},
keywords = {sdn, multi-homing, vertical handovers, aeronautical communications, lisp mobility}
}

@inproceedings{10.1145/2742580.2742810,
author = {Karedla, Rama},
title = {Programming for the Intel Xeon Processor},
year = {2015},
isbn = {9781450335270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742580.2742810},
doi = {10.1145/2742580.2742810},
abstract = {Software programmers tend to focus on the software layer leaving performance on the
table by not taking advantage of the underlying hardware. This talk will help the
programmer take advantage of the underlying Intel Xeon server architecture to write
more efficient programs. We broadly cover topics such as time measurement, memory
ordering, making efficient use of the multi level caches, NUMA aware programming and
the use of the many compute cores available in the Xeon architecture via multi-threading.We
hope to show the benefit to both, latency and throughput oriented applications. The
talk will also address using the new AVX vector registers to achieve higher performance,
and briefly touch upon the recently announced Transactional Synchronization Extensions
(TSX) features. Examples of application profiling will demonstrate the benefit of
optimizing for performance in parallel with code development.},
booktitle = {Applicative 2015},
location = {New York, NY, USA},
series = {Applicative 2015}
}

@inproceedings{10.1145/2609248.2609264,
author = {Lazarescu, Mihai T. and Cohen, Albert and Guatto, Adrien and L\^{e}, Nhat Minn and Lavagno, Luciano and Pop, Antoniu and Prieto, Manuel and Terechko, Andrei and Sutii, Alexandru},
title = {Energy-Aware Parallelization Flow and Toolset for C Code},
year = {2014},
isbn = {9781450329415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609248.2609264},
doi = {10.1145/2609248.2609264},
abstract = {Multicore architectures are increasingly used in embedded systems to achieve higher
throughput with lower energy consumption. This trend accentuates the need to convert
existing sequential code to effectively exploit the resources of these architectures.
We present a parallelization flow and toolset for legacy C code that includes a performance
estimation tool, a parallelization tool, and a streaming-oriented parallelization
framework. These are part of the work-in-progress EU FP7 PHARAON project that aims
to develop a complete set of techniques and tools to guide and assist software development
for heterogeneous parallel architectures. We demonstrate the effectiveness of the
use of the toolset in an experiment where we measure the parallelization quality and
time for inexperienced users, and the parallelization flow and performance results
for the parallelization of a practical example of a stereo vision application.},
booktitle = {Proceedings of the 17th International Workshop on Software and Compilers for Embedded Systems},
pages = {79–88},
numpages = {10},
keywords = {execution profiling, program parallelization, energy estimation, data dependency analysis},
location = {Sankt Goar, Germany},
series = {SCOPES '14}
}

@article{10.1145/2641361.2641374,
author = {Ohkawa, Takeshi and Uetake, Daichi and Yokota, Takashi and Ootsu, Kanemitsu and Baba, Takanobu},
title = {Reconfigurable and Hardwired ORB Engine on FPGA by Java-to-HDL Synthesizer for Realtime Application},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {5},
issn = {0163-5964},
url = {https://doi.org/10.1145/2641361.2641374},
doi = {10.1145/2641361.2641374},
abstract = {A platform for networked FPGA system design, which is named "ORB Engine", is proposed
to add more controllability and design productivity on FPGA-based systems composed
of software and hardwired IPs. A developer can define an object-oriented interface
for the circuit IP in FPGA, and implement the control sequence part using Java. The
circuit IP in FPGA can be handled through object-oriented interface from variety of
programing languages like C++, Java, Python, Ruby and so on. Application specific
and high-efficiency circuit for ORB (Object Request Broker) protocol processing is
synthesized from easy-handling Java code using JavaRock Java-to-HDL synthesizer within
the de-facto standard CORBA (Common Object Request Broker Architecture). The measurement
result shows a very low latency as low as 200us of UDP/IP packet in/out and exhibits
a fluctuation free delay performance, which is desirable for real-time applications.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {77–82},
numpages = {6}
}

@article{10.1145/2659118.2659135,
author = {Tiwari, Umesh and Kumar, Santosh},
title = {In-out Interaction Complexity Metrics for Component-Based Software},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2659118.2659135},
doi = {10.1145/2659118.2659135},
abstract = {In the current state of software engineering, component-based software development
is one of the most alluring paradigms for developing large and complex software products.
In this software engineering methodology pre-engineered, pre-tested, context-based,
adaptable, deployable software components are assembled according to a predefined
architecture. Rather than developing a system from scratch, component-based software
development emphasizes the integration of these components according to the user's
requirements and specifications. In component-based software, the components interact
to access and provide services and functionality to each other. Currently, the emphasis
of industry and researchers is on developing impressive and efficient metrics and
measurement tools to analyze the interaction complexity among these components. To
represent the request and the response of services among components, we have used
outgoing edges and incoming edges respectively. In this paper we have defined these
interactions as In-Interactions and Out-Interactions. The metrics proposed in this
paper are solely based on the interactions among the components. In this work some
simple methods and metrics for computing the complexity of composable components are
suggested. The metrics discussed in this paper include the computation of interaction
complexities as Total-Interactions of a component, Total- Interactions of component-based
software, Interaction-Ratio of a component, Interaction-Ratio of component-based software,
Average- Interaction among components and Interaction-Percentage of components.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–4},
numpages = {4},
keywords = {component-based software development, adaptable, in-interactions, out-interactions, context-based, metrics, pre-engineered}
}

@inproceedings{10.1145/3409390.3409402,
author = {Monfared, Saleh Khalaj and Hajihassani, Omid and Kiarostami, Mohammad Sina and Zanjani, Soroush Meghdadi and Rahmati, Dara and Gorgin, Saeid},
title = {BSRNG: A High Throughput Parallel BitSliced Approach for Random Number Generators},
year = {2020},
isbn = {9781450388689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409390.3409402},
doi = {10.1145/3409390.3409402},
abstract = { In this work, a high throughput method for generating high-quality Pseudo-Random
Numbers using the bitslicing technique is proposed. In such a technique, instead of
the conventional row-major data representation, column-major data representation is
employed, which allows the bitslicing implementation to take full advantage of all
the available datapath of the hardware platform. By employing this data representation
as building blocks of algorithms, we showcase the capability and scalability of our
proposed method in various PRNG methods in the category of block and stream ciphers.
The LFSR-based (Linear Feedback Shift Register) nature of the PRNG in our implementation
perfectly suits the GPU’s many-core structure due to its register oriented architecture.
In the proposed SIMD vectorized GPU implementation, each GPU thread can generate several
32 pseudo-random bits in each LFSR clock cycle. We then compare our implementation
with some of the most significant PRNGs that display a satisfactory performance throughput
and randomness criteria. The proposed implementation successfully passes the NIST
test for statistical randomness and bit-wise correlation criteria. For computer-based
PRNG and the optical solutions in terms of performance and performance per cost, this
technique is efficient while maintaining an acceptable randomness measure. Our highest
performance among all of the implemented CPRNGs with the proposed method is achieved
by the MICKEY 2.0 algorithm, which shows 40% improvement over state of the art NVIDIA’s
proprietary high-performance PRNG, cuRAND library, achieving 2.72 Tb/s of throughput
on the affordable NVIDIA GTX 2080 Ti.},
booktitle = {49th International Conference on Parallel Processing - ICPP : Workshops},
articleno = {12},
numpages = {10},
keywords = {Cryptography, Stream cipher, Bitslicing, cuRAND, PRNG, High-performance, CUDA},
location = {Edmonton, AB, Canada},
series = {ICPP Workshops '20}
}

@article{10.1145/3183517,
author = {Floris, Alessandro and Ahmad, Arslan and Atzori, Luigi},
title = {QoE-Aware OTT-ISP Collaboration in Service Management: Architecture and Approaches},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3183517},
doi = {10.1145/3183517},
abstract = {It is a matter of fact that quality of experience (QoE) has become one of the key
factors determining whether a new multimedia service will be successfully accepted
by the final users. Accordingly, several QoE models have been developed with the aim
of capturing the perception of the user by considering as many influencing factors
as possible. However, when it comes to adopting these models in the management of
the services and networks, it frequently happens that no single provider has access
to all of the tools to either measure all influencing factors parameters or control
over the delivered quality. In particular, it often happens to the over-the-top (OTT)
and Internet service providers (ISPs), which act with complementary roles in the service
delivery over the Internet. On the basis of this consideration, in this article we
first highlight the importance of a possible OTT-ISP collaboration for a joint service
management in terms of technical and economic aspects. Then we propose a general reference
architecture for a possible collaboration and information exchange among them. Finally,
we define three different approaches, namely joint venture, customer lifetime value
based, and QoE fairness based. The first aims to maximize the revenue by providing
better QoE to customers paying more. The second aims to maximize the profit by providing
better QoE to the most profitable customers (MPCs). The third aims to maximize QoE
fairness among all customers. Finally, we conduct simulations to compare the three
approaches in terms of QoE provided to the users, profit generated for the providers,
and QoE fairness.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {36},
numpages = {24},
keywords = {OTT, quality of experience, Internet service providers, ISP, Over The Top service providers, QoE management, OTT-ISP collaboration, QoE}
}

@inproceedings{10.1145/2695664.2695835,
author = {Rrushi, Julian L. and Farhangi, Hassan and Nikolic, Radina and Howey, Clay and Carmichael, Kelly and Palizban, Ali},
title = {By-Design Vulnerabilities in the ANSI C12.22 Protocol Specification},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695835},
doi = {10.1145/2695664.2695835},
abstract = {The ANSI C12.22 is a standard that specifies interfaces to data communication networks
in the smart grid. In this paper we discuss several vulnerabilities by design that
we discovered in the ANSI C12.22 protocol specification during an analysis of the
overall protocol architecture. The consequences of an exploitation of those vulnerabilities
consist of denial of service conditions and disruptions to ANSI C12.22 nodes and relays.
We developed attack code to experiment with exploitations of most of the vulnerabilities
that we discuss in this paper. Our research testbed consisted of meters that we emulated
via the Trilliant TstBench software. The emulated meters were running on virtual Windows
machines on a virtual network. In the paper, we provide details of the vulnerabilities
by design that we identified, and thus propose a series of revisions of the ANSI C12.22
protocol specification with the objective of mitigating those vulnerabilities.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {2231–2236},
numpages = {6},
keywords = {smartgrid security, vulnerabilities, ANSI C12.22 protocol},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2815317.2815321,
author = {da Silva, Madalena P. and Dantas, Mario A.R. and Gon\c{c}alves, Alexandre L. and Pinto, Alex R.},
title = {A Managing QoE Approach for Provisioning User Experience Aware Services Using SDN},
year = {2015},
isbn = {9781450337571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815317.2815321},
doi = {10.1145/2815317.2815321},
abstract = {Provision and delivery of services with quality is a classic research problem, however
the computational resources available in the network infrastructure of providers are,
usually, managed with conventional Quality of Service (QoS) parameters. This paper
presents an approach of Quality of Experience (QoE) management for providing services
aware of the user experience. QoE modeling and architecture are proposed, with a semantic
engine able to learn the user's experience during the use of a service, detecting
violations of QoS metrics and providing information, allowing the controller to perform
actions in the elements of the Software Defined Networking. The experimental results
demonstrate that the proposal is feasible and functional and that the time spent between
QoE detection and adaptation of policies in network resources do not influence the
quality perceived by the user.},
booktitle = {Proceedings of the 11th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {51–58},
numpages = {8},
keywords = {software-defined network, semantic engine, quality of service, quality of experience},
location = {Cancun, Mexico},
series = {Q2SWinet '15}
}

@inproceedings{10.1145/3220267.3220280,
author = {Odema, Mohanad and Adly, Ihab and El-Baz, Ahmed and Amin, Hani},
title = {A RESTful Architecture for Portable Remote Online Experimentation Services},
year = {2018},
isbn = {9781450364690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220267.3220280},
doi = {10.1145/3220267.3220280},
abstract = {In this paper, an architecture is proposed to deliver portable remote online experimentation
services. This can benefit the educational and academic sectors in terms of providing
remote online accessibility to real experiment setups. Thus, the users can be relieved
from geographical and time dependence for the experiment to be conducted. Nowadays,
almost all web services leverage the efficiency and prevalence of the REST (Representational
State Transfer) architecture. Hence, this proposed remote online service has been
implemented in compliance with the RESTful architectural style.Web-based experiments
require compatibility with any of the users' portable devices and accessibility at
any time. A RESTful architecture can fulfill these requirements. In addition, different
experiments can be made available online based on this architecture while sharing
the same infrastructure. A case study has been selected to obtain measurements of
different force components existing inside wind tunnels. The complete implementation
of this system is provided starting from the embedded controller retrieving sensor
measurements to the web server development and user interface design.},
booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
pages = {102–105},
numpages = {4},
keywords = {Online testing and experimentation, RESTful architecture, Remote testing facilities},
location = {Cairo, Egypt},
series = {ICSIE '18}
}

@inproceedings{10.1145/3364641.3364680,
author = {Couto, Christian Marlon Souza and Terra, Ricardo},
title = {A Quality-Oriented Approach to Recommend Move Method Refactorings},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364680},
doi = {10.1145/3364641.3364680},
abstract = {Refactoring processes are common in large software systems, especially when developers
neglect architectural erosion process for long periods. Even though there are many
refactoring approaches, very few consider the refactoring impact on the software quality.Given
this scenario, we propose a refactoring approach to software systems oriented to software
quality metrics. Based on the QMOOD (Quality Model for Object Oriented Design), the
main idea is to move methods between classes in order to maximize the values of the
quality metrics. Using a formal notation, we describe the problem as follows. Given
a software system S, our approach recommends a sequence of refactorings R1, R2,...,
Rn that result in system versions S1, S2,..., Sn, where quality(Si+1) &gt; quality(Si).We
performed three types of evaluation to verify the usefulness of our implemented tool,
called QMove. First, we applied our approach on 13 open-source systems that we modified
by randomly moving a subset of its methods to other classes, then checking if our
approach would recommend the moved methods to return to their original place, and
we achieve 84% recall, on average. Second, we compared QMove against two state-of-art
refactoring tools (JMove and JDeodorant) on the 13 previously evaluated systems, and
QMove showed better recall value (84%) than the other two (30% and 29%, respectively).
Third, we conducted the same comparison among QMove, JMove, and JDeodorant applied
in two proprietary systems where experts evaluated the quality of the recommendations.
QMove obtained eight positively evaluated recommendations from the experts, against
two and none of JMove and JDeodorant, respectively.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {315},
numpages = {1},
keywords = {refactoring, software architecture, quality metrics},
location = {Fortaleza, Brazil},
series = {SBQS'19}
}

@inproceedings{10.1145/3297663.3310307,
author = {van der Sar, Jerom and Donkervliet, Jesse and Iosup, Alexandru},
title = {Yardstick: A Benchmark for Minecraft-like Services},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3310307},
doi = {10.1145/3297663.3310307},
abstract = {Online gaming applications entertain hundreds of millions of daily active players
and often feature vastly complex architecture. Among online games, Minecraft-like
games simulate unique (e.g., modifiable) environments, are virally popular, and are
increasingly provided as a service. However, the performance of Minecraft-like services,
and in particular their scalability, is not well understood. Moreover, currently no
benchmark exists for Minecraft-like games. Addressing this knowledge gap, in this
work we design and use the Yardstick benchmark to analyze the performance of Minecraft-like
services. Yardstick is based on an operational model that captures salient characteristics
of Minecraft-like services. As input workload, Yardstick captures important features,
such as the most-popular maps used within the Minecraft community. Yardstick captures
system- and application-level metrics, and derives from them service-level metrics
such as frequency of game-updates under scalable workload. We implement Yardstick,
and, through real-world experiments in our clusters, we explore the performance and
scalability of popular Minecraft-like servers, including the official vanilla server,
and the community-developed servers Spigot and Glowstone. Our findings indicate the
scalability limits of these servers, that Minecraft-like services are poorly parallelized,
and that Glowstone is the least viable option among those tested.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {243–253},
numpages = {11},
keywords = {yardstick, distributed systems, as a service, online gaming, minecraft, benchmark},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/2856636.2876471,
author = {Zodik, Gabi},
title = {Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2876471},
doi = {10.1145/2856636.2876471},
abstract = {The second wave of change presented by the age of mobility, wearables, and IoT focuses
on how organizations and enterprises, from a wide variety of commercial areas and
industries, will use and leverage the new technologies available. Businesses and industries
that don't change with the times will simply cease to exist.Applications need to be
powered by cognitive and contextual technologies to support real-time proactive decisions.
These decisions will be based on the mobile context of a specific user or group of
users, incorporating location, time of day, current user task, and more. Driven by
the huge amounts of data produced by mobile and wearables devices, and influenced
by privacy concerns, the next wave in computing will need to exploit data and computing
at the edge of the network. Future mobile apps will have to be cognitive to 'understand'
user intentions based on all the available interactions and unstructured data.Mobile
applications are becoming increasingly ubiquitous, going beyond what end users can
easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business
(B2B) apps, only about 30% of the development efforts appear in the interface of the
mobile app. For example, areas such as the collaborative nature of the software or
the shortened development cycle and time-to-market are not apparent to end users.
The other 70% of the effort invested is dedicated to integrating the applications
with back-office systems and developing those aspects of the application that operate
behind the scenes.An important, yet often complex, part of the solution and mobile
app takes place far from the public eye-in the back-office environment. It is there
that various aspects of customer relationship management must be addressed: tracking
usage data, pushing out messaging as needed, distributing apps to employees within
the enterprise, and handling the wide variety of operational and management tasks-often
involving the collection and monitoring of data from sensors and wearable devices.
All this must be carried out while addressing security concerns that range from verifying
user identities, to data protection, to blocking attempted breaches of the organization,
and activation of malicious code. Of course, these tasks must be augmented by a systematic
approach and vigilant maintenance of user privacy.The first wave of the mobile revolution
focused on development platforms, run-time platforms, deployment, activation, and
management tools for multi-platform environments, including comprehensive mobile device
management (MDM). To realize the full potential of this revolution, we must capitalize
on information about the context within which mobile devices are used. With both employees
and customers, this context could be a simple piece of information such as the user
location or time of use, the hour of the day, or the day of the week. The context
could also be represented by more complex data, such as the amount of time used, type
of activity performed, or user preferences. Further insight could include the relationship
history with the user and the user's behavior as part of that relationship, as well
as a long list of variables to be considered in various scenarios. Today, with the
new wave of wearables, the definition of context is being further extended to include
environmental factors such as temperature, weather, or pollution, as well as personal
factors such as heart rate, movement, or even clothing worn.In both B2E and B2C situations,
a context-dependent approach, based on the appropriate context for each specific user,
offers a superior tool for working with both employees and clients alike. This mode
of operation does not start and end with the individual user. Rather, it takes into
account the people surrounding the user, the events taking place nearby, appliances
or equipment activated, the user's daily schedule, as well as other, more general
information, such as the environment and weather.Developing enterprise-wide, context-dependent,
mobile solutions is still a complex challenge. A system of real added-value services
must be developed, as well as a comprehensive architecture. These four-tier architectures
comprise end-user devices like wearables and smartphones, connected to systems of
engagement (SoEs), and systems of record (SoRs). All this is needed to enable data
analytics and collection in the context where it is created. The data collected will
allow further interaction with employees or customers, analytics, and follow-up actions
based on the results of that analysis. We also need to ensure end-to-end (E2E) security
across these four tiers, and to keep the data and application contexts in sync. These
are just some of the challenges being addressed by IBM Research.As an example, these
technologies could be deployed in the retail space, especially in brick-and-mortar
stores. Identifying a customer entering a store, detecting her location among the
aisles, and cross-referencing that data with the customer's transaction history, could
lead to special offers tailor-made for that specific customer or suggestions relevant
to her purchasing process. This technology enables real-world implementation of metrics,
analytics, and other tools familiar to us from the online realm. We can now measure
visits to physical stores in the same way we measure web page hits: analyze time spent
in the store, the areas visited by the customer, and the results of those visits.
In this way, we can also identify shoppers wandering around the store and understand
when they are having trouble finding the product they want to purchase. We can also
gain insight into the standard traffic patterns of shoppers and how they navigate
a store's floors and departments. We might even consider redesigning the store layout
to take advantage of this insight to enhance sales.In healthcare, the context can
refer to insight extracted from data received from sensors on the patient, from either
his mobile device or wearable technology, and information about the patient's environment
and location at that moment in time. This data can help determine if any assistance
is required. For example, if a patient is discharged from the hospital for continued
at-home care, doctors can continue to remotely monitor his condition via a system
of sensors and analytic tools that interpret the sensor readings.This approach can
also be applied to the area of safety. Scientists at IBM Research are developing a
platform that collects and analyzes data from wearable technology to protect the safety
of employees working in construction, heavy industry, manufacturing, or out in the
field. This solution can serve as a real-time warning system by analyzing information
gathered from wearable sensors embedded in personal protective equipment, such as
smart safety helmets and protective vests, and in the workers' individual smartphones.
These sensors can continuously monitor a worker's pulse rate, movements, body temperature,
and hydration level, as well as environmental factors such as noise level, and other
parameters. The system can provide immediate alerts to the worker about any dangers
in the work environment to prevent possible injury. It can also be used to prevent
accidents before they happen or detect accidents once they occur. For example, with
sophisticated algorithms, we can detect if a worker falls based on a sudden difference
in elevations detected by an accelerometer, and then send an alert to notify her peers
and supervisor or call for help. Monitoring can also help ensure safety in areas where
continuous exposure to heat or dangerous materials must be limited based on regulated
time periods.Mobile technologies can also help manage events with massive numbers
of participants, such as professional soccer games, music festivals, and even large-scale
public demonstrations, by sending alerts concerning long and growing lines or specific
high-traffic areas. These technologies can be used to detect accidents typical of
large-scale gatherings, send warnings about overcrowding, and alert the event organizers.
In the same way, they can alleviate parking problems or guide public transportation
operators- all via analysis and predictive analytics.IBM Research - Haifa is currently
involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers
have a special expertise in time- and location-based intelligent applications, including
visual maps that display activity contexts and predictive analytics systems for mobile
data and users. In another area, IBM researchers in Haifa are developing new cognitive
services driven from the unique data available on mobile and wearable devices. Looking
to the future, the IBM Research team is further advancing the integration of wearable
technology, augmented reality systems, and biometric tools for mobile user identity
validation.Managing contextual data and analyzing the interaction between the different
kinds of data presents fascinating challenges for the development of next-generation
programming. For example, we need to rethink when and where data processing and computations
should occur: Is it best to leave them at the user-device level, or perhaps they should
be moved to the back-office systems, servers, and/or the cloud infrastructures with
which the user device is connected? New-age applications are becoming more and more
distributed. They operate on a wide range of devices, such as wearable technologies,
use a variety of sensors, and depend on cloud-based systems.As a result, a new distributed
programming paradigm is emerging to meet the needs of these use-cases and real-time
scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and
data in business systems, and must be able to shift computation from the cloud to
the edge, based on context in close to real-time. By processing data at the edge of
the network, close to where the interactions and processing are happening, we can
help reduce latency and offer new opportunities for improved privacy and security.Despite
all these interactions, data collection, and the analytic insights based upon them-we
cannot forget the issues of privacy. Without a proper and reliable solution that offers
more control over what personal data is shared and how it is used, people will refrain
from sharing information. Such sharing is necessary for developing and understanding
the context in which people are carrying out various actions, and to offer them tools
and services to enhance their actions.In the not-so-distant future, we anticipate
the appearance of ad-hoc networks for wearable technology systems that will interact
with one another to further expand the scope and value of available context-dependent
data.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {11–12},
numpages = {2},
location = {Goa, India},
series = {ISEC '16}
}

@inproceedings{10.1109/CCGrid.2014.103,
author = {Wu, Jie and Jansen, Christoph and Beier, Maximilian and Witt, Michael and Krefting, Dagmar},
title = {Extending XNAT towards a Cloud-Based Quality Assessment Platform for Retinal Optical Coherence Tomographies},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.103},
doi = {10.1109/CCGrid.2014.103},
abstract = {Neurosciencific research is increasingly based on image analysis methods. Large sets
of imaging data are processed using complex image analysis tools. While today magnetic
resonance imaging (MRI) is widely used for both functional and anatomical analysis
of the human brain, new imaging modalities are beginning to prove their capabilities
for neurological research. Among them, optical coherence tomography (OCT) allows for
noninvasive visualization of anatomical structures on a micrometer scale. Becoming
a standard diagnostic tool in ophthalmology, it is of rising interest for neurological
research. Crucial to all data analysis methods is the quality of the input data. The
platform presented in this paper is designed for automatic quality assessment of retinal
OCTs. It extends the image management platform XNAT by services to calculate and store
quality measures. It is also extensible regarding new quality measure algorithms,
allowing the developer to upload Matlab code, compile it for the infrastructure's
hardware architecture and test it in the system. The image processing tools to calculate
the quality measures are provided as a cloud-based service employing OpenStack as
underlying IT infrastructure. The prototype implementation encompassing security and
performance aspects are presented.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {764–773},
numpages = {10},
keywords = {cloud, IaaS, neuroimaging, medical imaging, OCT, SaaS, XNAT},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3238147.3240467,
author = {Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin},
title = {Experiences Applying Automated Architecture Analysis Tool Suites},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240467},
doi = {10.1145/3238147.3240467},
abstract = {In this paper, we report our experiences of applying three complementary automated
software architecture analysis techniques, supported by a tool suite, called DV8,
to 8 industrial projects within a large company. DV8 includes two state-of-the-art
architecture-level maintainability metrics—Decoupling Level and Propagation Cost,
an architecture flaw detection tool, and an architecture root detection tool. We collected
development process data from the project teams as input to these tools, reported
the results back to the practitioners, and followed up with telephone conferences
and interviews. Our experiences revealed that the metrics scores, quantitative debt
analysis, and architecture flaw visualization can effectively bridge the gap between
management and development, help them decide if, when, and where to refactor. In particular,
the metrics scores, compared against industrial benchmarks, faithfully reflected the
practitioners’ intuitions about the maintainability of their projects, and enabled
them to better understand the maintainability relative to other projects internal
to their company, and to other industrial products. The automatically detected architecture
flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify
the “hotspots" within the systems that are responsible for high maintenance costs.
Except for the two smallest projects for which both architecture metrics indicated
high maintainability, all other projects are planning or have already begun refactorings
to address the problems detected by our analyses. We are working on further automating
the tool chain, and transforming the analysis suite into deployable services accessible
by all projects within the company.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {779–789},
numpages = {11},
keywords = {Software Quality, Software Maintenance, Software Architecture},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/3178461.3178462,
author = {Meryem, Amar and Samira, Douzi and Bouabid, El Ouahidi},
title = {Enhancing Cloud Security Using Advanced MapReduce K-Means on Log Files},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178462},
doi = {10.1145/3178461.3178462},
abstract = {Many customers ranked cloud security as a major challenge that threaten their work
and reduces their trust on cloud service's provider. Hence, a significant improvement
is required to establish better adaptations of security measures that suit recent
technologies and especially distributed architectures.Considering the meaningful recorded
data in cloud generated log files, making analysis on them, mines insightful value
about hacker's activities. It identifies malicious user behaviors and predicts new
suspected events. Not only that, but centralizing log files, prevents insiders from
causing damage to system. In this paper, we proposed to take away sensitive log files
into a single server provider and combining both MapReduce programming and k-means
on the same algorithm to cluster observed events into classes having similar features.
To label unknown user behaviors and predict new suspected activities this approach
considers cosine distances and deviation metrics.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {63–67},
numpages = {5},
keywords = {K-means, Deviation metric, MapReduce, Cloud Security, log files},
location = {Casablanca, Morocco},
series = {ICSIM2018}
}

@inproceedings{10.1145/2841113.2841114,
author = {Forget, Alain and Chiasson, Sonia and Biddle, Robert},
title = {Choose Your Own Authentication},
year = {2015},
isbn = {9781450337540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2841113.2841114},
doi = {10.1145/2841113.2841114},
abstract = {To solve the long-standing problems users have in creating and remembering text passwords,
a wide variety of alternative authentication schemes have been proposed. Some of these
schemes outperform others by various metrics in various contexts. However, none unilaterally
outperform all others, and so text passwords persist as the main scheme applications
depend upon. In this paper, we challenge the long-standing assumption that only one
authentication scheme can be offered by an application service. We propose Choose
Your Own Authentication (CYOA): a novel authentication architecture that enables users
to choose a scheme amongst several available alternatives. CYOA would enable users
to select whichever scheme best suits their preferences, abilities, and usage context.
Existing text password systems could easily be replaced. Furthermore, the three-party
architecture would enable delegating the management of authentication systems to trusted-third
parties. The architecture allows rapid deployment and testing of novel authentication
technologies. Our two-week usability study suggests that participants were willing
to leverage alternative schemes. Participants were confident that CYOA could keep
their financial information secure.},
booktitle = {Proceedings of the 2015 New Security Paradigms Workshop},
pages = {1–15},
numpages = {15},
keywords = {user study, usable security, survey, Authentication},
location = {Twente, Netherlands},
series = {NSPW '15}
}

@inproceedings{10.1145/3377813.3381349,
author = {Diamantopoulos, Nikos and Wong, Jeffrey and Mattos, David Issa and Gerostathopoulos, Ilias and Wardrop, Matthew and Mao, Tobias and McFarland, Colin},
title = {Engineering for a Science-Centric Experimentation Platform},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381349},
doi = {10.1145/3377813.3381349},
abstract = {Netflix is an internet entertainment service that routinely employs experimentation
to guide strategy around product innovations. As Netflix grew, it had the opportunity
to explore increasingly specialized improvements to its service, which generated demand
for deeper analyses supported by richer metrics and powered by more diverse statistical
methodologies. To facilitate this, and more fully harness the skill sets of both engineering
and data science, Netflix engineers created a science-centric experimentation platform
that leverages the expertise of scientists from a wide range of backgrounds working
on data science tasks by allowing them to make direct code contributions in the languages
used by them (Python and R). Moreover, the same code that runs in production is able
to be run locally, making it straightforward to explore and graduate both metrics
and causal inference methodologies directly into production services.In this paper,
we provide two main contributions. Firstly, we report on the architecture of this
platform, with a special emphasis on its novel aspects: how it supports science-centric
end-to-end workflows without compromising engineering requirements. Secondly, we describe
its approach to causal inference, which leverages the potential outcomes conceptual
framework to provide a unified abstraction layer for arbitrary statistical models
and methodologies.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {191–200},
numpages = {10},
keywords = {causal inference, A/B testing, experimentation, software architecture, science-centric},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.1145/3465481.3470091,
author = {Komisarek, Miko\l{}aj and Pawlicki, Marek and Kowalski, Miko\l{}aj and Marzecki, Adrian and Kozik, Rafa\l{} and Choraundefined, Micha\l{}},
title = {Network Intrusion Detection in the Wild - the Orange Use Case in the SIMARGL Project},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470091},
doi = {10.1145/3465481.3470091},
abstract = { There is a profuse abundance of network security incidents around the world every
day. Increasingly, services and data stored on servers fall victim to sophisticated
techniques that cause all sorts of damage. Hackers invent new ways to bypass security
measures and modify the existing viruses in order to deceive defense systems. Therefore,
in response to these illegal procedures, new ways to defend against them are being
developed. In this paper, a method for anomaly detection based on machine learning
technique is presented and a near real-time processing system architecture is proposed.
The main contribution is a test-run of ML algorithms on real-world data coming from
a world-class telecom operator. This work investigates the effectiveness of detecting
malicious behaviour in network packets using several machine learning techniques.
The results achieved are expressed with a set of metrics. For better clarity on the
classifier performance, 10-fold cross-validation was used.},
booktitle = {The 16th International Conference on Availability, Reliability and Security},
articleno = {65},
numpages = {7},
keywords = {network intrusion detection, machine learning},
location = {Vienna, Austria},
series = {ARES 2021}
}

@inproceedings{10.1145/3167132.3167178,
author = {Fernandes, Rodrigo and Sim\~{a}o, Jos\'{e} and Veiga, Lu\'{\i}s},
title = {EcoVMbroker: Energy-Aware Scheduling for Multi-Layer Datacenters},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167178},
doi = {10.1145/3167132.3167178},
abstract = {The cloud relies on efficient algorithms to find resources for jobs by fulfilling
the job's requirements and at the same time optimise an objective function. Utility
is a measure of the client satisfaction that can be seen as an objective function
maximised by schedulers based on the agreed service level agreement (SLA). We propose
EcoVM-Broker which can reduce energy consumption by using dynamic voltage frequency
scaling (DVFS) and applying reductions of utility, different for classes of users
and across ranges of resource allocations. Using efficient data structures and a hierarchical
architecture, we created a scalable solution for the fast growing heterogeneous cloud.
EcoVMBroker proved that we can delegate work in a hierarchical datacenter, make decisions
based on summaries of resource usage collected from several nodes and still be efficient.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {403–410},
numpages = {8},
keywords = {DVFS, virtual machice scheduling, partial utility, energy efficiency},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3210259.3210262,
author = {Erb, Benjamin and Mei\ss{}ner, Dominik and Kargl, Frank and Steer, Benjamin A. and Cuadrado, Felix and Margan, Domagoj and Pietzuch, Peter},
title = {Graphtides: A Framework for Evaluating Stream-Based Graph Processing Platforms},
year = {2018},
isbn = {9781450356954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210259.3210262},
doi = {10.1145/3210259.3210262},
abstract = {Stream-based graph systems continuously ingest graph-changing events via an established
input stream, performing the required computation on the corresponding graph. While
there are various benchmarking and evaluation approaches for traditional, batch-oriented
graph processing systems, there are no common procedures for evaluating stream-based
graph systems. We, therefore, present GraphTides, a generic framework which includes
the definition of an appropriate system model, an exploration of the parameter space,
suitable workloads, and computations required for evaluating such systems. Furthermore,
we propose a methodology and provide an architecture for running experimental evaluations.
With our framework, we hope to systematically support system development, performance
measurements, engineering, and comparisons of stream-based graph systems.},
booktitle = {Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {3},
numpages = {10},
keywords = {graph processing, graph analytics, evolving graphs, evaluation, temporal graphs, stream-based graphs, measurements},
location = {Houston, Texas},
series = {GRADES-NDA '18}
}

@inproceedings{10.1145/3459104.3459136,
author = {A. Panayiotou, Nikolaos and P. Stavrou, Vasileios and E. Stergiou, Konstantinos},
title = {Applying the Industry 4.0 in a Smart Gas Grid: The Greek Gas Distribution Network Case},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459136},
doi = {10.1145/3459104.3459136},
abstract = {The aim of this paper is to design and implement a series of actions regarding the
operation of DEDA S.A. (Natural Gas Distribution Networks), based on principles of
Industry 4.0. The Natural Gas Distribution sector is one of the most critical and
innovative areas where Industry 4.0 can be applied, being part of critical infrastructure
management. At first, company's business process architecture was developed, with
the aim to export DEDA's business process and functional specifications related to
the required information systems. Subsequently, company's communication network is
implemented alongside the company's gas network, in coordination with the company's
control room.In addition, modernization of metering system is taking place in order
to exchange information between smart meters and the control room. A number of Information
Systems, such as the pipeline surveillance system and the Business Intelligence system
will also be installed in order to ensure communication at different levels using
Cloud technologies. The implementation is expected to improve DEDA's organization,
increasing customers' service level. As a result, there will be an expected increase
in the operational efficiency of DEDA's network through the use of advanced technologies,
in cooperation with business process modelling techniques. The effort should be continued
in this direction in order to achieve even greater improvement in business processes,
information systems and pipeline automation.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {180–184},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up Architectural SW Health Builds in a New Product Line Generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start
from scratch but takes into consideration the architecture and assets of the former
product line generation. Being able to accommodate legacy and 3rd party code is one
of the major product line qualities to be met. On the other side, product line qualities
like reusability, maintainability and alterability, i.e. being able to cope up with
a large amount of variability, with configurability and fast integratability are major
drivers.While setting up a new product line generation and thus a new corresponding
architecture, we this time focused on architectural software (SW) health and tracking
of architectural metrics from the very beginning. Taking the definition of "architecture
being a set of design decisions" [18] literally, we attempt to implement an architectural
check for every design decision taken. Architectural design decisions in our understanding
do not only - and even not mainly - deal with the definition of components and their
interaction but with patterns and rules or anti-patterns. The rules and anti-patterns,
"what not to do" or more often also "what not to do <u>any more</u>", is even more
important in setting up a new product line generation because developers are not only
used to the old style of developing and the old architecture, but also still have
to develop assets for both generations.In this article we describe selected architectural
checks that we have implemented, the layered architecture check and the check for
usage of obsolete services. Additionally we discuss selected architectural metrics:
the coupling coefficient metrics and the instability metrics. In the summary and outlook
we describe our experiences and still open topics in setting up architectural SW health
checks for a large-scale product line.The real-world examples are taken from the domain
of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {architectural technical debt, software architecture, product line development, technical debt, software erosion, architectural checks, embedded software},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.1145/3358695.3361753,
author = {Mohammadi, Farnaz and Panou, Angeliki and Ntantogian, Christoforos and Karapistoli, Eirini and Panaousis, Emmanouil and Xenakis, Christos},
title = {CUREX: SeCUre and PRivate HEalth Data EXchange},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3361753},
doi = {10.1145/3358695.3361753},
abstract = {The Health sector's increasing dependence on digital information and communication
infrastructures renders it vulnerable to privacy and cybersecurity threats, especially
as the theft of health data has become lucrative for cyber criminals. CUREX comprehensively
addresses the protection of the confidentiality and integrity of health data by producing
a novel, flexible and scalable situational awareness-oriented platform. It allows
a healthcare provider to assess cybersecurity and privacy risks that are exposed to
and suggest optimal strategies for addressing these risks with safeguards tailored
to each business case and application. CUREX is fully GDPR compliant by design. At
its core, a decentralised architecture enhanced by a private blockchain infrastructure
ensures the integrity of the data and –most importantly- the patient safety. Crucially,
CUREX expands beyond technical measures and improves cyber hygiene through training
and awareness activities for healthcare personnel. Its validation focuses on highly
challenging cases of health data exchange, spanning patient cross-border mobility,
remote healthcare, and data exchange for research.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {263–268},
numpages = {6},
keywords = {Risk assessment, Blockchain, eHealth, Cybersecurity, Cyber hygiene},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.1145/3465481.3470018,
author = {Eckel, Michael and Riemann, Tim},
title = {Userspace Software Integrity Measurement},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470018},
doi = {10.1145/3465481.3470018},
abstract = {Todays computing systems are more interconnected and sophisticated than ever before.
Especially in healthcare 4.0, services and infrastructures rely on cyber-physical
systemss (CPSess) and Internet of Things (IoT) devices. This adds to the complexity
of these highly connected systems and their manageability. Even worse, the variety
of emerging cyber attacks is becoming more severe and sophisticated, making healthcare
one of the most important sectors with major security risks. The development of appropriate
countermeasures constitutes one of the most complex and difficult challenges in cyber
security research. Research areas include, among others, anomaly detection, network
security, multi-layer event detection, cyber resiliency, and integrity protection.
Securing the integrity of software running on a device is a desirable protection goal
in the context of systems security. With a Trusted Platform Module (TPM), measured
boot, and remote attestation there exist technologies to ensure that a system has
booted up correctly and runs only authentic software. The Linux Integrity Measurement
Architecture (IMA) extends these principles into the operating systems (OSes), measuring
native binaries before they are loaded. However, interpreted language files, such
as Java classes and Python scripts, are not considered executables and are not measured
as such. Contemporary OSess ship with many of these and it is vital to consider them
as security-critical as native binaries. In this paper, we introduce Userspace Software
Integrity Measurement (USIM) for the Linux OSes. Userspace Software Integrity Measurement
(USIM) enables interpreters to measure, log, and irrevocably anchor critical events
in the TPM. We develop a software library in C which provides TPM-based measurement
functionality as well as the USIM service, which provides concurrent access handling
to the TPM based event logging. Further, we develop and implement a concept to realize
highly frequent event logging on the slow TPM. We integrate this library into the
Java Virtual Machine (JVM) to measure Java classes and show that it can be easily
integrated into other interpreters. With performance measurements we demonstrate that
our contribution is feasible and that overhead is negligible. },
booktitle = {The 16th International Conference on Availability, Reliability and Security},
articleno = {138},
numpages = {11},
keywords = {integrity verification, Trusted Computing, Systems security},
location = {Vienna, Austria},
series = {ARES 2021}
}

@inproceedings{10.1145/3384217.3386393,
author = {Petz, Adam},
title = {An Infrastructure for Faithful Execution of Remote Attestation Protocols},
year = {2020},
isbn = {9781450375610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384217.3386393},
doi = {10.1145/3384217.3386393},
abstract = {Experience shows that even with a well-intentioned user at the keyboard, a motivated
attacker can compromise a computer system at a layer below or adjacent to the shallow
forms of authentication that are now accepted as commonplace[3]. Therefore, rather
than asking "Can we trust the person behind the keyboard", a still better question
might be: "Can we trust the computer system underneath?". An emerging technology for
gaining trust in a remote computing system is remote attestation. Remote attestation
is the activity of making a claim about properties of a target by supplying evidence
to an appraiser over a network[2]. Although many existing approaches to remote attestation
wisely adopt a layered architecture-where the bottom layers measure layers above-the
dependencies between components remain static and measurement orderings fixed. For
modern computing environments with diverse topologies, we can no longer fix a target
architecture any more than we can fix a protocol to measure that architecture.Copland
[1] is a domain-specific language and formal framework that provides a vocabulary
for specifying the goals of layered attestation protocols. It also provides a reference
semantics that characterizes system measurement events and evidence handling; a foundation
for comparing protocol alternatives. The aim of this work is to refine the Copland
semantics to a more fine-grained notion of attestation manager execution-a high-privilege
thread of control responsible for invoking attestation services and bundling evidence
results. This refinement consists of two cooperating components called the Copland
Compiler and the Attestation Virtual Machine (AVM). The Copland Compiler translates
a Copland protocol description into a sequence of primitive attestation instructions
to be executed in the AVM. When considered in combination with advances in virtualization,
trusted hardware, and high-assurance system software components-like compilers, file-systems,
and OS kernels-a formally verified remote attestation infrastructure creates exciting
opportunities for building system-level security arguments.},
booktitle = {Proceedings of the 7th Symposium on Hot Topics in the Science of Security},
articleno = {17},
numpages = {1},
location = {Lawrence, Kansas},
series = {HotSoS '20}
}

@article{10.1145/3233182,
author = {Ji, Kecheng and Ling, Ming and Shi, Longxing and Pan, Jianping},
title = {An Analytical Cache Performance Evaluation Framework for Embedded Out-of-Order Processors Using Software Characteristics},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3233182},
doi = {10.1145/3233182},
abstract = {Utilizing analytical models to evaluate proposals or provide guidance in high-level
architecture decisions is been becoming more and more attractive. A certain number
of methods have emerged regarding cache behaviors and quantified insights in the last
decade, such as the stack distance theory and the memory level parallelism (MLP) estimations.
However, prior research normally oversimplified the factors that need to be considered
in out-of-order processors, such as the effects triggered by reordered memory instructions,
and multiple dependences among memory instructions, along with the merged accesses
in the same MSHR entry. These ignored influences actually result in low and unstable
precisions of recent analytical models.By quantifying the aforementioned effects,
this article proposes a cache performance evaluation framework equipped with three
analytical models, which can more accurately predict cache misses, MLPs, and the average
cache miss service time, respectively. Similar to prior studies, these analytical
models are all fed with profiled software characteristics in which case the architecture
evaluation process can be accelerated significantly when compared with cycle-accurate
simulations.We evaluate the accuracy of proposed models compared with gem5 cycle-accurate
simulations with 16 benchmarks chosen from Mobybench Suite 2.0, Mibench 1.0, and Mediabench
II. The average root mean square errors for predicting cache misses, MLPs, and the
average cache miss service time are around 4%, 5%, and 8%, respectively. Meanwhile,
the average error of predicting the stall time due to cache misses by our framework
is as low as 8%. The whole cache performance estimation can be sped by about 15 times
versus gem5 cycle-accurate simulations and 4 times when compared with recent studies.
Furthermore, we have shown and studied the insights between different performance
metrics and the reorder buffer sizes by using our models. As an application case of
the framework, we also demonstrate how to use our framework combined with McPAT to
find out Pareto optimal configurations for cache design space explorations.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
articleno = {79},
numpages = {25},
keywords = {Analytical models, cache misses, cache miss service time, software characteristics, memory level parallelism}
}

@inproceedings{10.1145/3341105.3374026,
author = {Araldo, Andrea and Stefano, Alessandro Di and Stefano, Antonella Di},
title = {Resource Allocation for Edge Computing with Multiple Tenant Configurations},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374026},
doi = {10.1145/3341105.3374026},
abstract = {Edge Computing (EC) consists in deploying computational resources, e.g., memory, CPUs,
at the Edge of the network, e.g., base stations, access points, and run there a part
of the computation currently running on the Cloud. This approach promises to reduce
latency, inter-domain traffic and enhance user experience. Since resources at the
Edge are scarce, resource allocation is crucial for EC. While most of the studies
assume users interact directly with the Edge submitting a sequence of tasks, we instead
consider that users will interact with different Service Providers (SPs), as they
currently do in the Web. We therefore consider the case of a Network Operator (NO)
that owns the resources at the Edge and must decide how much resource to allocate
to the different tenants (SPs).We propose MORA, a polynomial time strategy which allows
the NO to maximize its utility, which can be inter-domain traffic savings, improved
users' QoE or other metrics of interest. The core of MORA is that (i) it exploits
service elasticity, i.e., the fact that services can adapt to the resources allocated
by the NO and rely on a remote Cloud for the excess of computation, (ii) it is suitable
for micro-services architecture, which decomposes a single service in a set of components,
which MORA places in the different computational nodes of the Edge and (iii) it copes
with multi-dimensional resources, e.g., memory and CPUs. After analyzing the properties
of the algorithm, we show numerically that it performs close to the optimum. To guarantee
reproducibility, the numerical evaluation is performed on publicly available traces
from Google and Alibaba clusters and in synthetic scenarios and our code is open source.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1190–1199},
numpages = {10},
keywords = {container systems, edge computing, network optimization, resource allocation, cloud computing},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.1109/TASLP.2019.2915922,
author = {Xu, Zhen and Sun, Chengjie and Long, Yinong and Liu, Bingquan and Wang, Baoxun and Wang, Mingjiang and Zhang, Min and Wang, Xiaolong},
title = {Dynamic Working Memory for Context-Aware Response Generation},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2915922},
doi = {10.1109/TASLP.2019.2915922},
abstract = {In human-to-human conversations, the context generally provides several backgrounds
and strategic points for the following response. Therefore, many response generation
approaches have explored the methodologies to incorporate the context into the encoder–decoder
architecture, to generate context-aware responses that are remarkably relevant and
cohesive to the given context. However, most approaches pay less attention to semantic
interactions implicitly existing within contextual utterances, which are of great
importance to capture semantic clues of the given dialog context, indeed. This paper
proposes a dynamic working memory mechanism to model long-term semantic hints in the
conversation context, by performing semantic interactions between utterances and updating
context representation dynamically. Then, the outputs of the dynamic working memory
are employed to provide helpful clues for the encoder–decoder architecture to generate
responses to the given dialog. We have evaluated the proposed approach on Twitter
Customer Service Corpus and OpenSubtitles Corpus, with several automatic evaluation
metrics and the human evaluation, and the empirical results show the effectiveness
of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1419–1431},
numpages = {13}
}

@inproceedings{10.1145/3386367.3431306,
author = {Marques, Jonatas and Levchenko, Kirill and Gaspary, Luciano},
title = {IntSight: Diagnosing SLO Violations with in-Band Network Telemetry},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431306},
doi = {10.1145/3386367.3431306},
abstract = {Performance requirements for many of today's high-perfor-mance networks are expressed
as service-level objectives (SLOs), i.e., precise guarantees, typically on latency
and bandwidth, that a user can expect from the network. For network operators, monitoring
their own SLO compliance, and quickly diagnosing any violations, is a critical element
for effective operations. Unfortunately, existing network architectures are not engineered
for this purpose; there is no mechanism, for example, for the operator to monitor
the 95th per-centile latency experienced by a customer. Data plane programmability
has made per-packet measurements possible but brings the challenge of keeping the
monitoring overhead low and practical. In this paper, we present IntSight, a system
for highly accurate and fine-grained detection and diagnosis of SLO violations. The
main contribution of IntSight is, building upon in-band telemetry, introducing path-wise
computation of network metrics and selective generation of reports. We show the effectiveness
of IntSight by way of two use cases. Our evaluation using real networks also shows
that IntSight generates up to two orders of magnitude less monitoring traffic than
state-of-the-art approaches. Furthermore, its processing and memory requirements are
low and therefore compatible with currently existing programmable platforms.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {421–434},
numpages = {14},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@inproceedings{10.5555/2820518.2820566,
author = {Mirakhorli, Mehdi and Cleland-Huang, Jane},
title = {Modifications, Tweaks, and Bug Fixes in Architectural Tactics},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Architectural qualities such as reliability, performance, and security, are often
realized in a software system through the adoption of tactical design decisions such
as the decision to use redundant processes, a heartbeat monitor, or a specific authentication
mechanism. Such decisions are critical for delivering a system that meets its quality
requirements. Despite the stability of high-level decisions, our analysis has shown
that tactic-related classes tend to be modified more frequently than other classes
and are therefore stronger predictors of change than traditional Object-Oriented coupling
and cohesion metrics. In this paper we present the results from this initial study,
including an analysis of why tactic-related classes are changed, and a discussion
of the implications of these findings for maintaining architectural quality over the
lifetime of a software system.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {377–380},
numpages = {4},
keywords = {modifications, tactics, bugs, architectural decisions, metrics},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1145/2699503,
author = {Patrignani, Marco and Agten, Pieter and Strackx, Raoul and Jacobs, Bart and Clarke, Dave and Piessens, Frank},
title = {Secure Compilation to Protected Module Architectures},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/2699503},
doi = {10.1145/2699503},
abstract = {A fully abstract compiler prevents security features of the source language from being
bypassed by an attacker operating at the target language level. Unfortunately, developing
fully abstract compilers is very complex, and it is even more so when the target language
is an untyped assembly language. To provide a fully abstract compiler that targets
untyped assembly, it has been suggested to extend the target language with a protected
module architecture—an assembly-level isolation mechanism which can be found in next-generation
processors. This article provides a fully abstract compilation scheme whose source
language is an object-oriented, high-level language and whose target language is such
an extended assembly language. The source language enjoys features such as dynamic
memory allocation and exceptions. Secure compilation of first-order method references,
cross-package inheritance, and inner classes is also presented. Moreover, this article
contains the formal proof of full abstraction of the compilation scheme. Measurements
of the overhead introduced by the compilation scheme indicate that it is negligible.},
journal = {ACM Trans. Program. Lang. Syst.},
month = apr,
articleno = {6},
numpages = {50},
keywords = {Fully abstract compilation, protected module architecture}
}

@inproceedings{10.5555/2665049.2665054,
author = {Kofler, Klaus and Davis, Gregory and Gesing, Sandra},
title = {SAMPO: An Agent-Based Mosquito Point Model in OpenCL},
year = {2014},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Agent-based modeling and simulations are applied for problems where population-level
patterns arise from the interaction of many autonomous individuals. These problems
are compute-intensive and excellent candidates for the use of parallel algorithms
and architectures. As a cross-platform software development framework for parallel
architectures, OpenCL appears as an ideal tool to implement such algorithms. However,
OpenCL does not natively support object-oriented development, which most of the toolkits
and frameworks used to build agent-based models require.The present work describes
an OpenCL implementation of an existing agent-based model, simulating populations
of the Anopheles gambiae mosquito, one of the most important vectors of malaria in
Africa. Discussed are the methods and techniques used to overcome the design challenges,
which arise when transitioning from an object-oriented program to an efficient OpenCL
implementation. In particular, the parallelism inside the program has been maximized,
dynamic divergent branching was reduced, and the number of data transfers between
the OpenCL host and device has been minimized as far as possible.Even though our implementation
was designed for this specific use case, the approach can be generalized to other
contexts, as most agent-based point models would benefit from the same basic design
decisions that we took for our implementation. Comparisons between the object-oriented
and the OpenCL implementation illustrate that using an OpenCL approach offers two
important performance benefits: an overall simulation time speedup of up to 576 with
no measurable loss of accuracy, and better scalability as the agent-population size
increases. The tradeoffs necessary to achieve these performance benefits and the implications
for future agent-based software development frameworks are discussed.},
booktitle = {Proceedings of the 2014 Symposium on Agent Directed Simulation},
articleno = {5},
numpages = {10},
keywords = {agent-based modelling, GPGPU, OpenCL},
location = {Tampa, Florida},
series = {ADS '14}
}

@inproceedings{10.1145/2641798.2641814,
author = {Xu, Yi and Helal, Sumi},
title = {Application Caching for Cloud-Sensor Systems},
year = {2014},
isbn = {9781450330305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641798.2641814},
doi = {10.1145/2641798.2641814},
abstract = {Driven by critical and pressing smart city applications, accessing massive numbers
of sensors by cloud-hosted services is becoming an emerging and inevitable situation.
Na\"{\i}vely connecting massive numbers of sensors to the cloud raises major scalability
and energy challenges. An architecture embodying distributed optimization is needed
to manage the scale and to allow limited energy sensors to last longer in such a dynamic
and high-velocity big data system. We developed a multi-tier architecture which we
call Cloud, Edge and Beneath (CEB). Based on CEB, we propose an Application Fragment
Caching Algorithm (AFCA) which selectively caches application fragments from the cloud
to lower layers of CEB to improve cloud scalability. Through experiments, we show
and measure the effect of AFCA on cloud scalability.},
booktitle = {Proceedings of the 17th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {303–306},
numpages = {4},
keywords = {cloud-sensor systems, cloud computing, application caching},
location = {Montreal, QC, Canada},
series = {MSWiM '14}
}

@inproceedings{10.1145/2985766.2985772,
author = {Edoh, Thierry Oscar C. and Atchome, Athanase and Alahassa, Bidossessi R.U. and Pawar, Pravin},
title = {Evaluation of a Multi-Tier Heterogeneous Sensor Network for Patient Monitoring: The Case of Benin},
year = {2016},
isbn = {9781450345187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2985766.2985772},
doi = {10.1145/2985766.2985772},
abstract = {In this paper we propose and evaluate a wireless sensor network (WSN) system in order
to improve an existing patient-monitoring and surveillance system at the cardiologic
intensive care unit (CICU) of a large university clinic (Centre Hospitalier Universitaire
Hubert Koutoukou Maga - CHU-HKM) in Cotonou city of Benin (a West-African country).
We have designed a multi-tier architecture and simulated a heterogeneous, autonomous,
and energy efficient wireless sensor network system to overcome issues faced by existing
patient monitoring system in CICU such as manual collection and processing of data.
The improvement of the patient monitoring system has the objectives of providing affordable
and better health care service provision as well as autonomous and automatic collection
and processing of patient's bio-signals and environmental data. The proposed Wireless
Sensor Network consists of wireless heterogeneous nodes which sense patient bio-signals,
measure environmental parameters in the hospitalization rooms such as ambient temperature,
quality of air and send collected data to a gateway (central node) for processing
and storage. The conducted simulation experiments show that the proposed sensor network
architecture which uses ZigBee wireless standard and protocol highly improves the
patience monitoring and surveillance experience at CICU. It promotes collection and
autonomous processing of patient physiological data and room ambient temperature data.
Incorporating such system in CICU will be highly beneficial for taking a correct decision
during treatment. Beyond the accuracy and quality of the collected medical data, proposed
WSN is also designed to reduce the energy consumption within the sensor network system.},
booktitle = {Proceedings of the 2016 ACM Workshop on Multimedia for Personal Health and Health Care},
pages = {23–29},
numpages = {7},
keywords = {wireless sensors network, cooperative sensors, patient monitorin, zigbee standards, intensive care unit, cardiology},
location = {Amsterdam, The Netherlands},
series = {MMHealth '16}
}

@inproceedings{10.1145/2851613.2851874,
author = {Abderrahim, Wiem and Choukair, Zied},
title = {PaaS Dependability Integration Architecture Based on Cloud Brokering},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851874},
doi = {10.1145/2851613.2851874},
abstract = {Cloud computing has revolutionized the way IT is provisioned nowadays since it exposes
computing capabilities as rental resources to consumers. The emergence of cloud computing
services hasn't though prevented outages in these environments even among high profile
ranked cloud providers. Tremendous efforts concentrated on fault management measures
have been applied for these environments. But they have been focused mainly on the
IaaS service model and have been operated on the cloud provider side alone. In this
context, this paper proposes an architecture for cloud brokering that implements dependability
properties in an end to end way involving different cloud actors and all over the
cloud service models SaaS, PaaS and IaaS.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {484–487},
numpages = {4},
keywords = {fault tolerance, cloud provider, PaaS, IaaS, SaaS, fault management, cloud broker, fault forecasting, dependability},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1109/TNET.2018.2793892,
author = {Araldo, Andrea and Dan, Gyorgy and Rossi, Dario},
title = {Caching Encrypted Content Via Stochastic Cache Partitioning},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2793892},
doi = {10.1109/TNET.2018.2793892},
abstract = {In-network caching is an appealing solution to cope with the increasing bandwidth
demand of video, audio, and data transfer over the Internet. Nonetheless, in order
to protect consumer privacy and their own business, content providers CPs increasingly
deliver encrypted content, thereby preventing Internet service providers ISPs from
employing traditional caching strategies, which require the knowledge of the objects
being transmitted. To overcome this emerging tussle between security and efficiency,
in this paper we propose an architecture in which the ISP partitions the cache space
into slices, assigns each slice to a different CP, and lets the CPs remotely manage
their slices. This architecture enables transparent caching of encrypted content and
can be deployed in the very edge of the ISP’s network i.e., base stations and femtocells,
while allowing CPs to maintain exclusive control over their content. We propose an
algorithm, called SDCP, for partitioning the cache storage into slices so as to maximize
the bandwidth savings provided by the cache. A distinctive feature of our algorithm
is that ISPs only need to measure the aggregated miss rates of each CP, but they need
not know the individual objects that are requested. We prove that the SDCP algorithm
converges to a partitioning that is close to the optimal, and we bound its optimality
gap. We use simulations to evaluate SDCP’s convergence rate under stationary and nonstationary
content popularity. Finally, we show that SDCP significantly outperforms traditional
reactive caching techniques, considering both CPs with perfect and with imperfect
knowledge of their content popularity.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {548–561},
numpages = {14}
}

@inproceedings{10.1145/3209914.3209918,
author = {Zou, Luyao and Rui, Xuhua and Nguyen, Tuan Anh and Min, Dugki and Choi, Eunmi and Thang, Tran Duc and Son, Nguyen Nhu},
title = {A Scalable Network Area Storage with Virtualization: Modelling and Evaluation Using Stochastic Reward Nets},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3209918},
doi = {10.1145/3209914.3209918},
abstract = {Modelling and analysis of storage system in data centers for availability prediction
is of paramount importance. Many studies in literature proposed different architectures
and techniques to enhance availability of the storage system. In this paper, we proposed
to incorporate virtualization techniques on a network area storage. We used stochastic
reward nets to model the system's architecture and operational scenarios. Furthermore,
we investigated various measures of interests including steady state availability,
downtime and downtime cost, and sensitivity of the system availability with respect
to impacting parameters. The analysis results show that the proposed storage system
with virtualization can obtain an acceptable level of service availability. Furthermore,
the sensitivity analysis also points out complicated dependences of service availability
upon system parameters. This paper presents a preliminary study to help guide the
development of a scalable network area storage with virtualization in practice.},
booktitle = {Proceedings of the 2018 International Conference on Information Science and System},
pages = {225–233},
numpages = {9},
keywords = {Stochastic Reward Nets, Availability, Reliability, Network Attached Storage},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@inproceedings{10.1145/3291064.3291070,
author = {Thirunavukkarasu, Gokul Sidarth and Champion, Benjamin and Horan, Ben and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
title = {IoT-Based System Health Management Infrastructure as a Service},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291070},
doi = {10.1145/3291064.3291070},
abstract = {Customization, enhanced quality of streamlined maintenance services and uplifted productivity
are some of the key highlights from the rapidly evolving concept of Industry 4.0.
IoT (Internet of things) based service infrastructure models designed for delivering
enterprise services with capabilities of pro-actively sensing malfunctions and responding
with preventive measures to streamline the automated service offered is one of the
prime application of this concept. Continuous maintenance services increase the optimum
through-life cost and in-service life cycle of the product providing the customer
with the feel of full ownership. In-service feedbacks also help the manufactures to
identify issues with respect to the designs and improve it in the future versions.
In this paper, as a proof of concept a cloud-based IoT service infrastructure for
providing real-time prognostic and supervised vehicle maintenance system is proposed.
This proposed system aims at providing an enterprise service infrastructure to the
registered vehicle service centers to keep track of the real-time vehicle diagnostic
information of their client's vehicle over cloud and use prognostic algorithms to
identify any malfunctions or abnormal behavior of the vehicles for automatically scheduling
a service appointment and automating the maintenance cycle of the vehicle. In addition
to this, the system provides features like remote supervision and diagnostics maintenance
enabling technicians to fix issues remotely, ensuring streamlined and reliable service.
Initially, before building the proposed prototype system, a few experimental trails
where conducted for analyzing the use of different IoT models used in the development
to identify the best-suited approach. The results indicated that the publisher-subscriber
(NodeJS) based model outperforms the request-response (PHP) based model in terms of
the hits per second and mean request time for an increased number of active users.
The results of the initial tests justify the reason for the using the publisher-subscriber
based IOT architecture. The conceptualized enterprise infrastructure illustrated in
the manuscript aims at providing a streamlined maintenance service.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {55–61},
numpages = {7},
keywords = {streamlined remote supervision, prognostic maintenance, vehicle diagnosis, internet of things, System health management infrastructure as a service},
location = {Singapore, Singapore},
series = {CCIOT 2018}
}

@inproceedings{10.1145/3319535.3363279,
author = {Mo, Fan and Shahin Shamsabadi, Ali and Katevas, Kleomenis and Cavallaro, Andrea and Haddadi, Hamed},
title = {Poster: Towards Characterizing and Limiting Information Exposure in DNN Layers},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363279},
doi = {10.1145/3319535.3363279},
abstract = {Pre-trained Deep Neural Network (DNN) models are increasingly used in smartphones
and other user devices to enable prediction services, leading to potential disclosures
of (sensitive) information from training data captured inside these models. Based
on the concept of generalization error, we propose a framework to measure the amount
of sensitive information memorized in each layer of a DNN. Our results show that,
when considered individually, the last layers encode a larger amount of information
from the training data compared to the first layers. We find that the same DNN architecture
trained with different datasets has similar exposure per layer. We evaluate an architecture
to protect the most sensitive layers within an on-device Trusted Execution Environment
(TEE) against potential white-box membership inference attacks without the significant
computational overhead.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2653–2655},
numpages = {3},
keywords = {trusted execution environment, privacy, deep learning, training data, sensitive information exposure},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1109/UCC.2014.167,
author = {Wagle, Shyam S.},
title = {SLA Assured Brokering (SAB) and CSP Certification in Cloud Computing},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.167},
doi = {10.1109/UCC.2014.167},
abstract = {Due to lack of information of the cloud service providers (CSPs), customers can not
easily choose services according to their requirement and due to vendor lock-in and
lack of interoperability standards among cloud service providers, customers cannot
switch the providers once services are subscribed from CSPs. Recently proposed third
party architecture which is called cloud broker can access inter-cloud and provides
services to the customers according to their requirement but providing SLA based cloud
services as per their requirement is still missing in current researches. In our work,
we propose the SLA assured brokering framework which matches the requirements of the
customer with SLA offered by CSPs using similarity matching algorithm and willingness
to pay capacity for the services. It also measures the services offered by CSPs for
certifying and ranking the CSPs.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {1016–1017},
numpages = {2},
keywords = {SLA, Cloud Brokering, Certifying, Similarity Matching},
series = {UCC '14}
}

@inproceedings{10.1145/2933349.2933359,
author = {Sirin, Utku and Appuswamy, Raja and Ailamaki, Anastasia},
title = {OLTP on a Server-Grade ARM: Power, Throughput and Latency Comparison},
year = {2016},
isbn = {9781450343190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933349.2933359},
doi = {10.1145/2933349.2933359},
abstract = {Although scaling out of low-power cores is an alternative to power-hungry Intel Xeon
processors for reducing the power overheads, they have proven inadequate for complex,
non-parallelizable workloads. On the other hand, by the introduction of the 64-bit
ARMv8 architecture, traditionally low power ARM processors have become powerful enough
to run computationally intensive server-class applications.In this study, we compare
a high-performance Intel x86 processor with a commercial implementation of the ARM
Cortex-A57. We measure the power used, throughput delivered and latency quantified
when running OLTP workloads. Our results show that the ARM processor consumes 3 to
15 times less power than the x86, while penalizing OLTP throughput by a much lower
factor (1.7 to 3). As a result, the significant power savings deliver up to 9 times
higher energy efficiency. The x86's heavily optimized power-hungry micro-architectural
structures contribute to throughput only marginally. As a result, the x86 wastes power
when utilization is low, while lightweight ARM processor consumes only as much power
as it is utilized, achieving energy proportionality. On the other hand, ARM's quantified
latency can be up to 11x higher than x86 towards to the tail of latency distribution,
making x86 more suitable for certain type of service-level agreements.},
booktitle = {Proceedings of the 12th International Workshop on Data Management on New Hardware},
articleno = {10},
numpages = {7},
location = {San Francisco, California},
series = {DaMoN '16}
}

@inproceedings{10.1145/3291533.3291540,
author = {Dalgkitsis, Anestis and Louta, Malamati and Karetsos, George T.},
title = {Traffic Forecasting in Cellular Networks Using the LSTM RNN},
year = {2018},
isbn = {9781450366106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291533.3291540},
doi = {10.1145/3291533.3291540},
abstract = {In this work we design and implement a Neural Network that can identify recurrent
patterns in various metrics which can be then used for cellular network traffic forecasting.
Based on a custom architecture and memory, this Neural Network can handle prediction
tasks faster and more accurately in real life scenarios. This approach offers a solution
for service providers to enhance cellular network performance, by utilizing effectively
the available resources. In order to provide a robust conclusion about the performance
and precision of the proposed Neural Network, multiple predictions were made using
the same data-set and the results were compared against other similar algorithms from
the literature.},
booktitle = {Proceedings of the 22nd Pan-Hellenic Conference on Informatics},
pages = {28–33},
numpages = {6},
keywords = {long-short term memory, cellular networks, traffic forecasting, recurrent neural networks},
location = {Athens, Greece},
series = {PCI '18}
}

