% Encoding: UTF-8

@InProceedings{Anchuri2014,
  author    = {Anchuri, Pranay and Sumbaly, Roshan and Shah, Sam},
  booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
  title     = {Hotspot Detection in a Service-Oriented Architecture},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {1749–1758},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '14},
  abstract  = {Large-scale websites are predominantly built as a service-oriented architecture. Here,
services are specialized for a certain task, run on multiple machines, and communicate
with each other to serve a user's request. Reducing latency and improving the cost
to serve is quite important, but optimizing this service call graph is particularly
challenging due to the volume of data and the graph's non-uniform and dynamic nature.In
this paper, we present a framework to detect hotspots in a service-oriented architecture.
The framework is general, in that it can handle arbitrary objective functions. We
show that finding the optimal set of hotspots for a metric, such as latency, is NP-complete
and propose a greedy algorithm by relaxing some constraints. We use a pattern mining
algorithm to rank hotspots based on the impact and consistency. Experiments on real
world service call graphs from LinkedIn, the largest online professional social network,
show that our algorithm consistently outperforms baseline methods.},
  doi       = {10.1145/2661829.2661991},
  isbn      = {9781450325981},
  keywords  = {monitoring, call graph, service-oriented architecture, hotspots},
  location  = {Shanghai, China},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2661829.2661991},
}

@Article{Castelluccia2014,
  author     = {Castelluccia, Daniela and Boffoli, Nicola},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Service-Oriented Product Lines: A Systematic Mapping Study},
  year       = {2014},
  issn       = {0163-5948},
  month      = mar,
  number     = {2},
  pages      = {1–6},
  volume     = {39},
  abstract   = {Software product line engineering and service-oriented architectures both enable organizations
to capitalize on reuse of existing software assets and capabilities and improve competitive
advantage in terms of development savings, product flexibility, time-to-market. Both
approaches accommodate variation of assets, including services, by changing the software
being reused or composing services according a new orchestration. Therefore, variability
management in Service-oriented Product Lines (SoPL) is one of the main challenges
today. In order to highlight the emerging evidence-based results from the research
community, we apply the well-defined method of systematic mapping in order to populate
a classification scheme for the SoPL field of interest. The analysis of results throws
light on the current open issues. Moreover, different facets of the scheme can be
combined to answer more specific research questions. The report reveals the need for
more empirical research able to provide new metrics measuring efficiency and efficacy
of the proposed models, new methods and tools supporting variability management in
SoPL, especially during maintenance and verification and validation. The mapping study
about SoPL opens further investigations by means of a complete systematic review to
select and validate the most efficient solutions to variability management in SoPL.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2579281.2579294},
  issue_date = {March 2014},
  keywords   = {empirical study, mapping study, service-oriented computing, product line development, service-oriented architecture, software product line, variability management},
  numpages   = {6},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2579281.2579294},
}

@InProceedings{Oliveira2016,
  author    = {Oliveira, Joyce Aline and Junior, Jose J.L.D.},
  booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
  title     = {A Three-Dimensional View of Reuse in Service Oriented Architecture},
  year      = {2016},
  address   = {Porto Alegre, BRA},
  pages     = {409–416},
  publisher = {Brazilian Computer Society},
  series    = {SBSI 2016},
  abstract  = {The reuse in Service Oriented Architecture (SOA) has been used strategically in organizations
to reduce development costs and increase the quality of applications. This article
reports a qualitative research realized with experts in order to identify goals, barriers,
facilitators, strategies, metrics and benefits associated with reuse in SOA. The results
were summarized in three dimensions (management, architecture, operation) and represented
by a conceptual model that can serve as a preliminary roadmap to manage the reuse
in SOA.},
  isbn      = {9788576693178},
  keywords  = {qualitative research, SOA reuse, Services Oriented Architecture},
  location  = {Florianopolis, Santa Catarina, Brazil},
  numpages  = {8},
}

@InProceedings{Lehmann2017,
  author    = {Lehmann, Martin and Sandnes, Frode Eika},
  booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
  title     = {A Framework for Evaluating Continuous Microservice Delivery Strategies},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICC '17},
  abstract  = {The emergence of service-oriented computing, and in particular microservice architecture,
has introduced a new layer of complexity to the already challenging task of continuously
delivering changes to the end users. Cloud computing has turned scalable hardware
into a commodity, but also imposes some requirements on the software development process.
Yet, the literature mainly focuses on quantifiable metrics such as number of manual
steps and lines of code required to make a change. The industry, on the other hand,
appears to focus more on qualitative metrics such as increasing the productivity of
their developers. These are common goals, but must be measured using different approaches.
Therefore, based on interviews of industry stakeholders a framework for evaluating
and comparing approaches to continuous microservice delivery is proposed. We show
that it is possible to efficiently evaluate and compare strategies for continuously
delivering microservices.},
  articleno = {64},
  doi       = {10.1145/3018896.3018961},
  isbn      = {9781450347747},
  keywords  = {microservices, microservice architectures, deployment strategy, cloud computing, evaluation framework, continuous deployment},
  location  = {Cambridge, United Kingdom},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3018896.3018961},
}

@InProceedings{Oliveira2018,
  author    = {Oliveira, Joyce Aline and Vargas, Matheus and Rodrigues, Roni},
  booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
  title     = {SOA Reuse: Systematic Literature Review Updating and Research Directions},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SBSI'18},
  abstract  = {Service Oriented Architecture (SOA) reuse has been used strategically in organizations
to reduce development costs and increase the quality of applications. This article
analyzes a systematic literature review in order to identify concepts, goals, strategies,
and metrics of SOA reuse. The results show that the main goal of SOA reuse is to decrease
development costs. The factor that most negatively influences SOA reuse is the existence
of legacy systems. The strategy used most to potentialize SOA reuse is business process
management. Metrics proposed by studies to measure SOA reuse are related to modularity
and adaptability indicators. The study is relevant because it increases the body of
knowledge of the area. Additionally, a set of gaps to be addressed by researchers
and reuse practitioners was identified.},
  articleno = {71},
  doi       = {10.1145/3229345.3229419},
  isbn      = {9781450365598},
  keywords  = {Service Oriented Architecture, systematic literature review, SOA reuse},
  location  = {Caxias do Sul, Brazil},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3229345.3229419},
}

@InProceedings{Ilin2017,
  author    = {Ilin, I. and Levina, A. and Abran, A. and Iliashenko, O.},
  booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
  title     = {Measurement of Enterprise Architecture (EA) from an IT Perspective: Research Gaps and Measurement Avenues},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {232–243},
  publisher = {Association for Computing Machinery},
  series    = {IWSM Mensura '17},
  abstract  = {Reorganizational projects in general and software-related projects in particular,
are often implemented with a focus only on the reorganized components within an organizational
management system, not taking into account relationships with the other components
of an enterprise architecture (EA). This paper first looks at the current state of
EA measurement to identify weaknesses and gaps in aligning and measuring EA components,
EA structures and EA interrelationships from an IT perspective. It then identifies
from related works available innovative measurement concepts that could contribute
for aligning, measuring and monitoring software-related projects within an EA strategy.
This includes measurement avenues within a Balanced Scorecard (BSC), contributions
of functional size measurement to the BSC, and measurement of software structures
and functionality within a service-oriented architecture (SOA).},
  doi       = {10.1145/3143434.3143457},
  isbn      = {9781450348539},
  keywords  = {balanced scorecard (BSC), function points (FP), enterprise architecture (EA), enterprise architecture measurement, service-oriented architecture (SOA), functional size measurement (FSM)},
  location  = {Gothenburg, Sweden},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3143434.3143457},
}

@InProceedings{Bogner2018,
  author    = {Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred},
  booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
  title     = {Limiting Technical Debt with Maintainability Assurance: An Industry Survey on Used Techniques and Differences with Service- and Microservice-Based Systems},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {125–133},
  publisher = {Association for Computing Machinery},
  series    = {TechDebt '18},
  abstract  = {Maintainability assurance techniques are used to control this quality attribute and
limit the accumulation of potentially unknown technical debt. Since the industry state
of practice and especially the handling of Service- and Microservice-Based Systems
in this regard are not well covered in scientific literature, we created a survey
to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related
treatment of systems based on service-orientation, and c) influences on developer
satisfaction w.r.t. maintainability. 60 software professionals responded to our online
questionnaire. The results indicate that using explicit and systematic techniques
has benefits for maintainability. The more sophisticated the applied methods the more
satisfied participants were with the maintainability of their software while no link
to a hindrance in productivity could be established. Other important findings were
the absence of architecture-level evolvability control mechanisms as well as a significant
neglect of service-oriented particularities for quality assurance. The results suggest
that industry has to improve its quality control in these regards to avoid problems
with long-living service-based software systems.},
  doi       = {10.1145/3194164.3194166},
  isbn      = {9781450357135},
  keywords  = {microservice-based systems, maintainability, industry, survey, software quality control, service-based systems},
  location  = {Gothenburg, Sweden},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3194164.3194166},
}

@Article{Wu2018,
  author     = {Wu, Yumei and Fang, Yuanyuan and Liu, Bin and Zhao, Zehui},
  journal    = {Personal Ubiquitous Comput.},
  title      = {A Novel Service Deployment Approach Based on Resilience Metrics for Service-Oriented System},
  year       = {2018},
  issn       = {1617-4909},
  month      = oct,
  number     = {5–6},
  pages      = {1099–1107},
  volume     = {22},
  abstract   = {Service-Oriented Architecture (SOA) has been widely used in IT areas and is expected
to bring a lot of benefits. However, the SOA system developers have to address new
challenging issues such as computational resource failure before such benefits can
be realized. This paper develops a graph-theoretic model for the SOA system and proposes
metrics that quantify the resilience of such system under resource failures. It explores
two service deployment strategies to optimize resilience by taking not only communication
costs among services but also the computation costs of services into consideration.
Among them, two types of undirected graphs are developed to model the relationships
between services, including Service Dependence Graph (SDG) and Service Concurrence
Graph (SCG). Then, these two graphs are integrated into Service Relationship Graph
(SRG) and adopt the k-cut optimization theory to complete the service deployment.
Finally, this paper verifies the effectiveness of the above methods in improving the
resilience of the system through a series of experiments, which indicate that our
methods perform better than the previous methods in improving resilience of the SOA
system.},
  address    = {Berlin, Heidelberg},
  issue_date = {October 2018},
  keywords   = {Resilience, SOA, Service relationship graph, Service deployment},
  numpages   = {9},
  publisher  = {Springer-Verlag},
}

@InProceedings{Mathas2018,
  author    = {Mathas, Christos M. and Segou, Olga E. and Xylouris, Georgios and Christinakis, Dimitris and Kourtis, Michail-Alexandros and Vassilakis, Costas and Kourtis, Anastasios},
  booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
  title     = {Evaluation of Apache Spot's Machine Learning Capabilities in an SDN/NFV Enabled Environment},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ARES 2018},
  abstract  = {Software Defined Networking (SDN) and Network Function Virtualisation (NFV) are transforming
modern networks towards a service-oriented architecture. At the same time, the cybersecurity
industry is rapidly adopting Machine Learning (ML) algorithms to improve detection
and mitigation of complex attacks. Traditional intrusion detection systems perform
signature-based detection, based on well-known malicious traffic patterns that signify
potential attacks. The main drawback of this method is that attack patterns need to
be known in advance and signatures must be preconfigured. Hence, typical systems fail
to detect a zero-day attack or an attack with unknown signature. This work considers
the use of machine learning for advanced anomaly detection, and specifically deploys
the Apache Spot ML framework on an SDN/NFV-enabled testbed running cybersecurity services
as Virtual Network Functions (VNFs). VNFs are used to capture traffic for ingestion
by the ML algorithm and apply mitigation measures in case of a detected anomaly. Apache
Spot utilises Latent Dirichlet Allocation to identify anomalous traffic patterns in
Netflow, DNS and proxy data. The overall performance of Apache Spot is evaluated by
deploying Denial of Service (Slowloris, BoNeSi) and a Data Exfiltration attack (iodine).},
  articleno = {52},
  doi       = {10.1145/3230833.3233278},
  isbn      = {9781450364485},
  keywords  = {Apache Spot, Software Defined Networking, Network Function Virtualisation, Latent Dirichlet Allocation, Penetration Testing, Machine Learning, SHIELD Project},
  location  = {Hamburg, Germany},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3230833.3233278},
}

@InProceedings{Tummalapalli2020,
  author    = {Tummalapalli, Sahithi and Kumar, Lov and Murthy, N. L. Bhanu},
  booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
  title     = {Prediction of Web Service Anti-Patterns Using Aggregate Software Metrics and Machine Learning Techniques},
  year      = {2020},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ISEC 2020},
  abstract  = {Service-Oriented Architecture(SOA) can be characterized as an approximately coupled
engineering intended to meet the business needs of an association/organization. Service-Based
Systems (SBSs) are inclined to continually change to enjoy new client necessities
and adjust the execution settings, similar to some other huge and complex frameworks.
These changes may lead to the evolution of designs/products with poor Quality of Service
(QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns.
Anti-patterns makes the evolution and maintenance of the software systems hard and
complex. Early identification of modules, classes, or source code regions where anti-patterns
are more likely to occur can help in amending and maneuvering testing efforts leading
to the improvement of software quality. In this work, we investigate the application
of three sampling techniques, three feature selection techniques, and sixteen different
classification techniques to develop the models for web service anti-pattern detection.
We report the results of an empirical study by evaluating the approach proposed, on
a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of
five types of web-service anti-patterns. Experimental results demonstrated that SMOTE
is the best performing data sampling techniques. The experimental results also reveal
that the model developed by considering Uncorrelated Significant Predictors(SUCP)
as the input obtained better performance compared to the model developed by other
metrics. Experimental results also show that the Least Square Support Vector Machine
with Linear(LSLIN) function has outperformed all other classifier techniques.},
  articleno = {8},
  doi       = {10.1145/3385032.3385042},
  isbn      = {9781450375948},
  keywords  = {Anti-pattern, Classifiers, Service-Based Systems(SBS), Aggregation measures, WSDL, Feature Selection, Class imbalance distribution, Machine Learning, Web-Services, Source Code Metrics},
  location  = {Jabalpur, India},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3385032.3385042},
}

@InProceedings{Camargo2016,
  author    = {de Camargo, Andr\'{e} and Salvadori, Ivan and Mello, Ronaldo dos Santos and Siqueira, Frank},
  booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
  title     = {An Architecture to Automate Performance Tests on Microservices},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {422–429},
  publisher = {Association for Computing Machinery},
  series    = {iiWAS '16},
  abstract  = {The microservices architecture provides a new approach to develop applications. As
opposed to monolithic applications, in which the application comprises a single software
artifact, an application based on the microservices architecture is composed by a
set of services, each one designed to perform a single and well-defined task. These
services allow the development team to decouple several parts of the application using
different frameworks, languages and hardware for each part of the system. One of the
drawbacks for adopting the microservices architecture to develop applications is testability.
In a single application test boundaries can be more easily established and tend to
be more stable as the application evolves, while with microservices we can have a
set of hundreds of services that operate together and are prone to change more rapidly.
Each one of these services needs to be tested and updated as the service changes.
In addition, the different characteristics of these services such as languages, frameworks
or the used infrastructure have to be considered in the testing phase. Performance
tests are applied to assure that a particular software complies with a set of non-functional
requirements such as throughput and response time. These metrics are important to
ensure that business constraints are respected and to help finding performance bottlenecks.
In this paper, we present a new approach to allow the performance tests to be executed
in an automated way, with each microservice providing a test specification that is
used to perform tests. Along with the architecture, we also provide a framework that
implements some key concepts of this architecture. This framework is available as
an open source project1.},
  doi       = {10.1145/3011141.3011179},
  isbn      = {9781450348072},
  keywords  = {test automation, performance test, microservices},
  location  = {Singapore, Singapore},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3011141.3011179},
}

@InProceedings{Brito2021,
  author    = {Brito, Miguel and Cunha, J\'{a}come and Saraiva, Jo\~{a}o},
  booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
  title     = {Identification of Microservices from Monolithic Applications through Topic Modelling},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {1409–1418},
  publisher = {Association for Computing Machinery},
  series    = {SAC '21},
  abstract  = {Microservices emerged as one of the most popular architectural patterns in the recent
years given the increased need to scale, grow and flexibilize software projects accompanied
by the growth in cloud computing and DevOps. Many software applications are being
submitted to a process of migration from its monolithic architecture to a more modular,
scalable and flexible architecture of microservices. This process is slow and, depending
on the project's complexity, it may take months or even years to complete.This paper
proposes a new approach on microservice identification by resorting to topic modelling
in order to identify services according to domain terms. This approach in combination
with clustering techniques produces a set of services based on the original software.
The proposed methodology is implemented as an open-source tool for exploration of
monolithic architectures and identification of microservices. A quantitative analysis
using the state of the art metrics on independence of functionality and modularity
of services was conducted on 200 open-source projects collected from GitHub. Cohesion
at message and domain level metrics' showed medians of roughly 0.6. Interfaces per
service exhibited a median of 1.5 with a compact interquartile range. Structural and
conceptual modularity revealed medians of 0.2 and 0.4 respectively.Our first results
are positive demonstrating beneficial identification of services due to overall metrics'
results.},
  doi       = {10.1145/3412841.3442016},
  isbn      = {9781450381048},
  location  = {Virtual Event, Republic of Korea},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3412841.3442016},
}

@InProceedings{Ma2020,
  author    = {Ma, Meng and Xu, Jingmin and Wang, Yuan and Chen, Pengfei and Zhang, Zonghua and Wang, Ping},
  booktitle = {Proceedings of The Web Conference 2020},
  title     = {AutoMAP: Diagnose Your Microservice-Based Web Applications Automatically},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {246–258},
  publisher = {Association for Computing Machinery},
  series    = {WWW '20},
  abstract  = {The high complexity and dynamics of the microservice architecture make its application
diagnosis extremely challenging. Static troubleshooting approaches may fail to obtain
reliable model applies for frequently changing situations. Even if we know the calling
dependency of services, we lack a more dynamic diagnosis mechanism due to the existence
of indirect fault propagation. Besides, algorithm based on single metric usually fail
to identify the root cause of anomaly, as single type of metric is not enough to characterize
the anomalies occur in diverse services. In view of this, we design a novel tool,
named AutoMAP, which enables dynamic generation of service correlations and automated
diagnosis leveraging multiple types of metrics. In AutoMAP, we propose the concept
of anomaly behavior graph to describe the correlations between services associated
with different types of metrics. Two binary operations, as well as a similarity function
on behavior graph are defined to help AutoMAP choose appropriate diagnosis metric
in any particular scenario. Following the behavior graph, we design a heuristic investigation
algorithm by using forward, self, and backward random walk, with an objective to identify
the root cause services. To demonstrate the strengths of AutoMAP, we develop a prototype
and evaluate it in both simulated environment and real-work enterprise cloud system.
Experimental results clearly indicate that AutoMAP achieves over 90% precision, which
significantly outperforms other selected baseline methods. AutoMAP can be quickly
deployed in a variety of microservice-based systems without any system knowledge.
It also supports introduction of various expert knowledge to improve accuracy.},
  doi       = {10.1145/3366423.3380111},
  isbn      = {9781450370233},
  keywords  = {anomaly diagnosis, web application, Microservice architecture, root cause, cloud computing},
  location  = {Taipei, Taiwan},
  numpages  = {13},
  url       = {https://doi.org/10.1145/3366423.3380111},
}

@InProceedings{Lopez2017,
  author    = {L\'{o}pez, Manuel Ram\'{\i}rez and Spillner, Josef},
  booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
  title     = {Towards Quantifiable Boundaries for Elastic Horizontal Scaling of Microservices},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {35–40},
  publisher = {Association for Computing Machinery},
  series    = {UCC '17 Companion},
  abstract  = {One of the most useful features of a microservices architecture is its versatility
to scale horizontally. However, not all services scale in or out uniformly. The performance
of an application composed of microservices depends largely on a suitable combination
of replica count and resource capacity. In practice, this implies limitations to the
efficiency of autoscalers which often overscale based on an isolated consideration
of single service metrics. Consequently, application providers pay more than necessary
despite zero gain in overall performance. Solving this issue requires an application-specific
determination of scaling limits due to the general infeasibility of an application-agnostic
solution. In this paper, we study microservices scalability, the auto-scaling of containers
as microservice implementations and the relation between the number of replicas and
the resulting application task performance. We contribute a replica count determination
solution with a mathematical approach. Furthermore, we offer a calibration software
tool which places scalability boundaries into declarative composition descriptions
of applications ready to be consumed by cloud platforms.},
  doi       = {10.1145/3147234.3148111},
  isbn      = {9781450351959},
  keywords  = {scalability, replication, optimization, microservices},
  location  = {Austin, Texas, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3147234.3148111},
}

@InProceedings{Anwar2015,
  author    = {Anwar, Ali and Sailer, Anca and Kochut, Andrzej and Butt, Ali R.},
  booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
  title     = {Anatomy of Cloud Monitoring and Metering: A Case Study and Open Problems},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {APSys '15},
  abstract  = {Microservices based architecture has recently gained traction among the cloud service
providers in quest for a more scalable and reliable modular architecture. In parallel
with this architectural choice, cloud providers are also facing the market demand
for fine grained usage based prices. Both the management of the microservices complex
dependencies, as well as the fine grained metering require the providers to track
and log detailed monitoring data from their deployed cloud setups. Hence, on one hand,
the providers need to record all such performance changes and events, while on the
other hand, they are concerned with the additional cost associated with the resources
required to store and process this ever increasing amount of collected data.In this
paper, we analyze the design of the monitoring subsystem provided by open source cloud
solutions, such as OpenStack. Specifically, we analyze how the monitoring data is
collected by OpenStack and assess the characteristics of the data it collects, aiming
to pinpoint the limitations of the current approach and suggest alternate solutions.
Our preliminary evaluation of the proposed solutions reveals that it is possible to
reduce the monitored data size by up to 80% and missed anomaly detection rate from
3% to as low as 0.05% to 0.1%.},
  articleno = {6},
  doi       = {10.1145/2797022.2797039},
  isbn      = {9781450335546},
  location  = {Tokyo, Japan},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2797022.2797039},
}

@InProceedings{FreitasApolinario2020,
  author    = {de Freitas Apolin\'{a}rio, Daniel Rodrigo and de Fran\c{c}a, Breno Bernard Nicolau},
  booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
  title     = {Towards a Method for Monitoring the Coupling Evolution of Microservice-Based Architectures},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {71–80},
  publisher = {Association for Computing Machinery},
  series    = {SBCARS '20},
  abstract  = {The microservice architecture is claimed to satisfy ongoing software development demands,
such as resilience, flexibility, and velocity. However, developing applications based
on microservices also brings some drawbacks, such as the increased software operational
complexity. Recent studies have also pointed out the lack of methods to prevent problems
related to the maintainability of these solutions. Disregarding established design
principles during the software evolution may lead to the so-called architectural erosion,
which can end up in a condition of unfeasible maintenance. As microservices can be
considered a new architecture style, there are few initiatives to monitoring the evolution
of software microservice-based architectures. In this paper, we introduce the SYMBIOTE
method for monitoring the coupling evolution of microservice-based systems. More specifically,
this method collects coupling metrics during runtime (staging or production environments)
and monitors them throughout software evolution. The longitudinal analysis of the
collected measures allows detecting an upward trend in coupling metrics that could
be signs of architectural erosion. To develop the proposed method, we performed an
experimental analysis of the coupling metrics behavior using artificially-generated
data.},
  doi       = {10.1145/3425269.3425273},
  isbn      = {9781450387545},
  keywords  = {maintainability, coupling metrics, software evolution, microservices},
  location  = {Natal, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3425269.3425273},
}

@InProceedings{Cardarelli2019,
  author    = {Cardarelli, Mario and Iovino, Ludovico and Di Francesco, Paolo and Di Salle, Amleto and Malavolta, Ivano and Lago, Patricia},
  booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
  title     = {An Extensible Data-Driven Approach for Evaluating the Quality of Microservice Architectures},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {1225–1234},
  publisher = {Association for Computing Machinery},
  series    = {SAC '19},
  abstract  = {Microservice architecture (MSA) is defined as an architectural style where the software
system is developed as a suite of small services, each running in its own process
and communicating with lightweight mechanisms. The benefits of MSA are many, ranging
from an increase in development productivity, to better business-IT alignment, agility,
scalability, and technology flexibility. The high degree of microservices distribution
and decoupling is, however, imposing a number of relevant challenges from an architectural
perspective. In this context, measuring, controlling, and keeping a satisfactory level
of quality of the system architecture is of paramount importance.In this paper we
propose an approach for the specification, aggregation, and evaluation of software
quality attributes for the architecture of microservice-based systems. The proposed
approach allows developers to (i) produce architecture models of the system, either
manually or automatically via recovering techniques, (ii) contribute to an ecosystem
of well-specified and automatically-computable software quality attributes for MSAs,
and (iii) continuously measure and evaluate the architecture of their systems by (re-)using
the software quality attributes defined in the ecosystem. The approach is implemented
by using Model-Driven Engineering techniques.The current implementation of the approach
has been validated by assessing the maintainability of a third-party, publicly available
benchmark system.},
  doi       = {10.1145/3297280.3297400},
  isbn      = {9781450359337},
  keywords  = {microservices, architecture recovery, software quality, model-driven},
  location  = {Limassol, Cyprus},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3297280.3297400},
}

@InProceedings{Brilhante2017,
  author    = {Brilhante, Jonathan and Costa, Rostand and Maritan, Tiago},
  booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
  title     = {Asynchronous Queue Based Approach for Building Reactive Microservices},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {373–380},
  publisher = {Association for Computing Machinery},
  series    = {WebMedia '17},
  abstract  = {To achieve scalability and flexibility in larger applications a new approach arises,
named by Microservices (MS). However MS architectures are at their inception and are
even more a concept than a fully mature design pattern. One of the hardest topics
in this approach is how to properly migrate or develop a single microservice, in terms
of scope, efficiency and dependability. In this sense, this work proposes a new architectural
model based on high-level architecture pattern of reactive programming to the internal
structure of a new microservice. The new model of microservices are internally coordinated
by asynchronous queues, which allowed to preserve compatibility with most monolithic
components and provide an encapsulation process to enable its continuity. A comparative
study between the standard approach and the proposed architecture was carried out
to measure the eventual performance improvement of the new strategy.},
  doi       = {10.1145/3126858.3126873},
  isbn      = {9781450350969},
  keywords  = {asynchronous queues, reactive approach, micro services, refactoring},
  location  = {Gramado, RS, Brazil},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3126858.3126873},
}

@InProceedings{Khazaei2017,
  author    = {Khazaei, Hamzeh and Ravichandiran, Rajsimman and Park, Byungchul and Bannazadeh, Hadi and Tizghadam, Ali and Leon-Garcia, Alberto},
  booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
  title     = {Elascale: Autoscaling and Monitoring as a Service},
  year      = {2017},
  address   = {USA},
  pages     = {234–240},
  publisher = {IBM Corp.},
  series    = {CASCON '17},
  abstract  = {Auto-scalability has become an evident feature for cloud software systems including
but not limited to big data and IoT applications. Cloud application providers now
are in full control over their applications' microservices and macroservices; virtual
machines and containers can be provisioned or deprovisioned on demand at run-time.
Elascale strives to adjust both micro/macro resources with respect to workload and
changes in the internal state of the whole application stack. Elascale leverages Elasticsearch
stack for collection, analysis and storage of performance metrics. Elascale then uses
its default scaling engine to elastically adapt the managed application. Extendibility
is guaranteed through provider, schema, plug-in and policy elements in the Elascale
by which flexible scalability algorithms, including both reactive and proactive techniques,
can be designed and implemented for various technologies, infrastructures and software
stacks. In this paper, we present the architecture and initial implementation of Elascale;
an instance will be leveraged to add auto-scalability to a generic IoT application.
Due to zero dependency to the target software system, Elascale can be leveraged to
provide auto-scalability and monitoring as-a-service for any type of cloud software
system.},
  keywords  = {macroservices, elasticsearch, monitoring, microservices, auto-scalability, cloud application, scalability as a service, containers, docker},
  location  = {Markham, Ontario, Canada},
  numpages  = {7},
}

@InProceedings{Toffetti2015,
  author    = {Toffetti, Giovanni and Brunner, Sandro and Bl\"{o}chlinger, Martin and Dudouet, Florian and Edmonds, Andrew},
  booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
  title     = {An Architecture for Self-Managing Microservices},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {19–24},
  publisher = {Association for Computing Machinery},
  series    = {AIMC '15},
  abstract  = {Running applications in the cloud efficiently requires much more than deploying software
in virtual machines. Cloud applications have to be continuously managed: 1) to adjust
their resources to the incoming load and 2) to face transient failures replicating
and restarting components to provide resiliency on unreliable infrastructure. Continuous
management monitors application and infrastructural metrics to provide automated and
responsive reactions to failures (health management) and changing environmental conditions
(auto-scaling) minimizing human intervention.In the current practice, management functionalities
are provided as infrastructural or third party services. In both cases they are external
to the application deployment. We claim that this approach has intrinsic limits, namely
that separating management functionalities from the application prevents them from
naturally scaling with the application and requires additional management code and
human intervention. Moreover, using infrastructure provider services for management
functionalities results in vendor lock-in effectively preventing cloud applications
to adapt and run on the most effective cloud for the job.In this position paper we
propose a novel architecture that enables scalable and resilient self-management of
microservices applications on cloud.},
  doi       = {10.1145/2747470.2747474},
  isbn      = {9781450334761},
  location  = {Bordeaux, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2747470.2747474},
}

@Article{Brondolin2020,
  author     = {Brondolin, Rolando and Santambrogio, Marco D.},
  journal    = {ACM Trans. Archit. Code Optim.},
  title      = {A Black-Box Monitoring Approach to Measure Microservices Runtime Performance},
  year       = {2020},
  issn       = {1544-3566},
  month      = nov,
  number     = {4},
  volume     = {17},
  abstract   = {Microservices changed cloud computing by moving the applications’ complexity from
one monolithic executable to thousands of network interactions between small components.
Given the increasing deployment sizes, the architectural exploitation challenges,
and the impact on data-centers’ power consumption, we need to efficiently track this
complexity. Within this article, we propose a black-box monitoring approach to track
microservices at scale, focusing on architectural metrics, power consumption, application
performance, and network performance. The proposed approach is transparent w.r.t.
the monitored applications, generates less overhead w.r.t. black-box approaches available
in the state-of-the-art, and provides fine-grain accurate metrics.},
  address    = {New York, NY, USA},
  articleno  = {34},
  doi        = {10.1145/3418899},
  issue_date = {December 2020},
  keywords   = {network performance monitoring, cloud computing, performance monitoring, docker, power attribution, kubernetes, Microservices},
  numpages   = {26},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3418899},
}

@InProceedings{Kogias2020,
  author    = {Kogias, Marios and Bugnion, Edouard},
  booktitle = {4th Asia-Pacific Workshop on Networking},
  title     = {Tail-Tolerance as a Systems Principle Not a Metric},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {16–22},
  publisher = {Association for Computing Machinery},
  series    = {APNet '20},
  abstract  = {Tail-latency tolerance (or just simply tail-tolerance) is the ability for a system
to deliver a response with low-latency nearly all the time. It it typically expressed
as a system metric (e.g., the 99th or 99.99th percentile latency) or as a service-level
objective (e.g., the maximum throughput so that the tail latency is below a desired
threshold). We advocate instead that modern datacenter systems should incorporate
tail-tolerance as a core systems design principle and not a metric to be observed,
and that tail-tolerant systems can be built out of large and complex applications
whose individual components may suffer from latency deviations. This is analogous
to fault-tolerance, where a fault-tolerant system can be built out of unreliable components.
The general solution is for the system to control the applied load and keep it under
the threshold that violates the latency SLO. We propose to augment RPC semantics with
an architectural layer that measures the observed tail latency and probabilistically
rejects RPC requests maintaining throughput under the threshold that violates the
SLO. Our design is application-independent, and does not make any assumptions about
the request service time distribution. We implemented a proof of concept for such
a tail-tolerant layer using programmable switches, called SVEN. We demonstrate that
the approach is suitable even for microsecond-scale RPCs with variable service times.
Moreover, our approach does not induce measurable overheads, and can maintain the
maximum achieved throughput very close to the load level that would violate the SLO
without SVEN.},
  doi       = {10.1145/3411029.3411032},
  isbn      = {9781450388764},
  location  = {Seoul, Republic of Korea},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3411029.3411032},
}

@InProceedings{Scrocca2020,
  author    = {Scrocca, Mario and Tommasini, Riccardo and Margara, Alessandro and Valle, Emanuele Della and Sakr, Sherif},
  booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
  title     = {The Kaiju Project: Enabling Event-Driven Observability},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {85–96},
  publisher = {Association for Computing Machinery},
  series    = {DEBS '20},
  abstract  = {Microservices architectures are getting momentum. Even small and medium-size companies
are migrating towards cloud-based distributed solutions supported by lightweight virtualization
techniques, containers, and orchestration systems. In this context, understanding
the system behavior at runtime is critical to promptly react to errors. Unfortunately,
traditional monitoring techniques are not adequate for such complex and dynamic environments.
Therefore, a new challenge, namely observability, emerged from precise industrial
needs: expose and make sense of the system behavior at runtime. In this paper, we
investigate observability as a research problem. We discuss the benefits of events
as a unified abstraction for metrics, logs, and trace data, and the advantages of
employing event stream processing techniques and tools in this context. We show that
an event-based approach enables understanding the system behavior in near real-time
more effectively than state-of-the-art solutions in the field. We implement our model
in the Kaiju system and we validate it against a realistic deployment supported by
a software company.},
  doi       = {10.1145/3401025.3401740},
  isbn      = {9781450380287},
  keywords  = {event stream processing, orchestration systems, observability, event-based systems},
  location  = {Montreal, Quebec, Canada},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3401025.3401740},
}

@InProceedings{Nikravesh2015,
  author    = {Nikravesh, Ali Yadavar and Ajila, Samuel A. and Lung, Chung-Horng},
  booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
  title     = {Towards an Autonomic Auto-Scaling Prediction System for Cloud Resource Provisioning},
  year      = {2015},
  pages     = {35–45},
  publisher = {IEEE Press},
  series    = {SEAMS '15},
  abstract  = {This paper investigates the accuracy of predictive auto-scaling systems in the Infrastructure
as a Service (IaaS) layer of cloud computing. The hypothesis in this research is that
prediction accuracy of auto-scaling systems can be increased by choosing an appropriate
time-series prediction algorithm based on the performance pattern over time. To prove
this hypothesis, an experiment has been conducted to compare the accuracy of time-series
prediction algorithms for different performance patterns. In the experiment, workload
was considered as the performance metric, and Support Vector Machine (SVM) and Neural
Networks (NN) were utilized as time-series prediction techniques. In addition, we
used Amazon EC2 as the experimental infrastructure and TPC-W as the benchmark to generate
different workload patterns. The results of the experiment show that prediction accuracy
of SVM and NN depends on the incoming workload pattern of the system under study.
Specifically, the results show that SVM has better prediction accuracy in the environments
with periodic and growing workload patterns, while NN outperforms SVM in forecasting
unpredicted workload pattern. Based on these experimental results, this paper proposes
an architecture for a self-adaptive prediction suite using an autonomic system approach.
This suite can choose the most suitable prediction technique based on the performance
pattern, which leads to more accurate prediction results.},
  keywords  = {workload pattern, neural networks, resource provisioning, support vector machine, cloud computing, auto-scaling, autonomic},
  location  = {Florence, Italy},
  numpages  = {11},
}

@InProceedings{Dudouet2015,
  author    = {Dudouet, Florian and Edmonds, Andrew and Erne, Michael},
  booktitle = {Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
  title     = {Reliable Cloud-Applications: An Implementation through Service Orchestration},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {1–6},
  publisher = {Association for Computing Machinery},
  series    = {AIMC '15},
  abstract  = {As cloud-deployed applications became more and more mainstream, continuously more
complex services started to be deployed; indeed where initially monolithic applications
were simply ported to the cloud, applications are now more and more often composed
of micro-services. This improves the flexibility of an application but also makes
it more complex due to the sheer number of services comprising it.As deployment and
runtime management becomes more complex, orchestration software are becoming necessary
to completely manage the lifecycle of cloud applications. One crucial problem remaining
is how these applications can be made reliable in the cloud, a naturally unreliable
environment.In this paper we propose concepts and architectures which were implemented
in our orchestration software to guarantee reliability. Our initial implementation
also relies on Monasca, a well-known monitoring software for Open-Stack, to gather
proper metric and execute threshold-based actions. This allows us to show how service
reliability can be ensured using orchestration and how a proper incident-management
software feeding decisions to the orchestration engine ensures high-availability of
all components of managed applications.},
  doi       = {10.1145/2747470.2747471},
  isbn      = {9781450334761},
  keywords  = {incident management, orchestration, cloud computing, reliability},
  location  = {Bordeaux, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2747470.2747471},
}

@InProceedings{Stevanetic2014,
  author    = {Stevanetic, Srdjan and Zdun, Uwe},
  booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
  title     = {Exploring the Relationships between the Understandability of Components in Architectural Component Models and Component Level Metrics},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {EASE '14},
  abstract  = {Architectural component models represent high level designs and are frequently used
as a central view of architectural descriptions of software systems. The components
in those models represent important high level organization units that group other
components and classes in object-oriented design views. Hence, understandability of
components and their interactions plays a key role in supporting the architectural
understanding of a software system. In this paper we present a study we carried out
to examine the relationships between the effort required to understand a component,
measured through the time that participants spent on studying a component, and component
level metrics that describe component's size, complexity and coupling in terms of
the number of classes in a component and the classes' relationships. The participants
were 49 master students, and they had to fully understand the components' functionalities
in order to answer 4 true/false questions for each of the 7 components in the architecture
of the Soomla Android store system. Correlation, collinearity and multivariate regression
analysis were performed. The results of the analysis show a statistically significant
correlation between three of the metrics, number of classes, number of incoming dependencies,
and number of internal dependencies, on one side, and the effort required to understand
a component, on the other side. In a multivariate regression analysis we obtained
3 reasonably well-fitting models that can be used to estimate the effort required
to understand a component. In our future work we plan to study more components and
investigate more metrics and their relationships to the understandability of components
and architectural component models.},
  articleno = {32},
  doi       = {10.1145/2601248.2601264},
  isbn      = {9781450324762},
  keywords  = {software metrics, understandability, architectural component models, empirical evaluation},
  location  = {London, England, United Kingdom},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2601248.2601264},
}

@InProceedings{VanLanduyt2015,
  author    = {Van Landuyt, Dimitri and Joosen, Wouter},
  booktitle = {Proceedings of the Fifth International Workshop on Twin Peaks of Requirements and Architecture},
  title     = {On the Role of Early Architectural Assumptions in Quality Attribute Scenarios: A Qualitative and Quantitative Study},
  year      = {2015},
  pages     = {9–15},
  publisher = {IEEE Press},
  series    = {TwinPeaks '15},
  abstract  = {Architectural assumptions are fundamentally different from architectural decisions
because they can not be traced directly to requirements, nor to domain, technical
or environmental constraints; they represent conditions under which the designed solution
is expected to be valid. Early architectural assumptions are similar in nature, with
the key difference that they are not made during architectural design but during requirement
elicitation, not by the software architect (a solution-oriented stakeholder), but
by the requirements engineer (a problem-oriented stakeholder). They represent initial
assumptions about the system's architecture, and allow the requirements engineer to
be more precise in documenting the requirements of the system.The role of early architectural
assumptions in the current practice of quality attribute scenario elicitation and
related development activities in the transition to architecture is unknown and under-investigated.
In this paper, we present the results of an exploratory study that focuses on the
role and nature of these assumptions in the early development stages. We studied a
reasonably large set of quality attribute scenarios for a realistic industrial case,
a smart metering system. Our study (i) confirms that quality attribute scenario elicitation
in practice does rely heavily on early architectural assumptions, and (ii) shows that
they do influence the perceived quality of the requirements body as a whole, in some
cases positively, in other cases negatively.These findings provide empirical arguments
in favor of making such assumptions explicit already during the requirements elicitation
activities. Especially in the context of iterative software development methodologies
such as the Twin Peaks model, a well-defined and -documented set of assumptions could
smoothen the transition between successive development iterations.},
  location  = {Florence, Italy},
  numpages  = {7},
}

@InProceedings{Stevanetic2015,
  author    = {Stevanetic, Srdjan and Zdun, Uwe},
  booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
  title     = {Software Metrics for Measuring the Understandability of Architectural Structures: A Systematic Mapping Study},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {EASE '15},
  abstract  = {The main idea of software architecture is to concentrate on the "big picture" of a
software system. In the context of object-oriented software systems higher-level architectural
structures or views above the level of classes are frequently used to capture the
"big picture" of the system. One of the critical aspects of these higher-level views
is understandability, as one of their main purposes is to enable designers to abstract
away fine-grained details. In this article we present a systematic mapping study on
software metrics related to the understandability concepts of such higher-level software
structures with regard to their relations to the system implementation. In our systematic
mapping study, we started from 3951 studies obtained using an electronic search in
the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our
inclusion/exclusion criteria as well as the snowballing technique we selected 268
studies for in-depth study. From those, we selected 25 studies that contain relevant
metrics. We classify the identified studies and metrics with regard to the measured
artefacts, attributes, quality characteristics, and representation model used for
the metrics definitions. Additionally, we present the assessment of the maturity level
of the identified studies. Overall, there is a lack of maturity in the studies. We
discuss possible techniques how to mitigate the identified problems. From the academic
point of view we believe that our study is a good starting point for future studies
aiming at improving the existing works. From a practitioner's point of view, the results
of our study can be used as a catalogue and an indication of the maturity of the existing
research results.},
  articleno = {21},
  doi       = {10.1145/2745802.2745822},
  isbn      = {9781450333504},
  location  = {Nanjing, China},
  numpages  = {14},
  url       = {https://doi.org/10.1145/2745802.2745822},
}

@InProceedings{Ewing2014,
  author    = {Ewing, John M. and Menasc\'{e}, Daniel A.},
  booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
  title     = {A Meta-Controller Method for Improving Run-Time Self-Architecting in SOA Systems},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {173–184},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '14},
  abstract  = {This paper builds on SASSY, a system for automatically generating SOA software architectures
that optimize a given utility function of multiple QoS metrics. In SASSY, SOA software
systems are automatically re-architected when services fail or degrade. Optimizing
both architecture and service provider selection presents a pair of nested NP-hard
problems. Here we adapt hill-climbing, beam search, simulated annealing, and evolutionary
programming to both architecture optimization and service provider selection. Each
of these techniques has several parameters that influence their efficiency. We introduce
in this paper a meta-controller that automates the run-time selection of heuristic
search techniques and their parameters. We examine two different meta-controller implementations
that each use online learning. The first implementation identifies the best heuristic
search combination from various prepared combinations. The second implementation analyzes
the current self-architecting problem (e.g. changes in performance metrics, service
degradations/failures) and looks for similar, previously encountered re-architecting
problems to find an effective heuristic search combination for the current problem.
A large set of experiments demonstrates the effectiveness of the first meta-controller
implementation and indicates opportunities for improving the second meta-controller
implementation.},
  doi       = {10.1145/2568088.2568098},
  isbn      = {9781450327336},
  keywords  = {heuristic search, meta-controlled qos optimization, autonomic computing, soa, combinatorial search techniques, metaheuristics, automated run-time software architecting},
  location  = {Dublin, Ireland},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2568088.2568098},
}

@Article{Miao2016,
  author     = {Miao, Wang and Min, Geyong and Wu, Yulei and Wang, Haozhe and Hu, Jia},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {Performance Modelling and Analysis of Software-Defined Networking under Bursty Multimedia Traffic},
  year       = {2016},
  issn       = {1551-6857},
  month      = sep,
  number     = {5s},
  volume     = {12},
  abstract   = {Software-Defined Networking (SDN) is an emerging architecture for the next-generation
Internet, providing unprecedented network programmability to handle the explosive
growth of big data driven by the popularisation of smart mobile devices and the pervasiveness
of content-rich multimedia applications. In order to quantitatively investigate the
performance characteristics of SDN networks, several research efforts from both simulation
experiments and analytical modelling have been reported in the current literature.
Among those studies, analytical modelling has demonstrated its superiority in terms
of cost-effectiveness in the evaluation of large-scale networks. However, for analytical
tractability and simplification, existing analytical models are derived based on the
unrealistic assumptions that the network traffic follows the Poisson process, which
is suitable to model nonbursty text data, and the data plane of SDN is modelled by
one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies
have shown that, due to the features of heavy volume and high velocity, the multimedia
big data generated by real-world multimedia applications reveals the bursty and correlated
nature in the network transmission. With the aim of capturing such features of realistic
traffic patterns and obtaining a comprehensive and deeper understanding of the performance
behaviour of SDN networks, this article presents a new analytical model to investigate
the performance of SDN in the presence of the bursty and correlated arrivals modelled
by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance
metrics in terms of the average latency and average network throughput of the SDN
networks are derived based on the developed analytical model. To consider a realistic
multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted
to model the SDN data plane. To address the challenging problem of obtaining the key
performance metrics, for example, queue-length distribution of a PQ system with a
given service capacity, a versatile methodology extending the Empty Buffer Approximation
(EBA) method is proposed to facilitate the decomposition of such a PQ system to two
SSSQ systems. The validity of the proposed model is demonstrated through extensive
simulation experiments. To illustrate its application, the developed model is then
utilised to study the strategy of the network configuration and resource allocation
in SDN networks.},
  address    = {New York, NY, USA},
  articleno  = {77},
  doi        = {10.1145/2983637},
  issue_date = {December 2016},
  keywords   = {performance modelling and analysis, Software-defined networking, multimedia big data, queueing decomposition, resource allocation},
  numpages   = {19},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2983637},
}

@Article{Tiwari2014,
  author     = {Tiwari, Umesh and Kumar, Santosh},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {In-out Interaction Complexity Metrics for Component-Based Software},
  year       = {2014},
  issn       = {0163-5948},
  month      = sep,
  number     = {5},
  pages      = {1–4},
  volume     = {39},
  abstract   = {In the current state of software engineering, component-based software development
is one of the most alluring paradigms for developing large and complex software products.
In this software engineering methodology pre-engineered, pre-tested, context-based,
adaptable, deployable software components are assembled according to a predefined
architecture. Rather than developing a system from scratch, component-based software
development emphasizes the integration of these components according to the user's
requirements and specifications. In component-based software, the components interact
to access and provide services and functionality to each other. Currently, the emphasis
of industry and researchers is on developing impressive and efficient metrics and
measurement tools to analyze the interaction complexity among these components. To
represent the request and the response of services among components, we have used
outgoing edges and incoming edges respectively. In this paper we have defined these
interactions as In-Interactions and Out-Interactions. The metrics proposed in this
paper are solely based on the interactions among the components. In this work some
simple methods and metrics for computing the complexity of composable components are
suggested. The metrics discussed in this paper include the computation of interaction
complexities as Total-Interactions of a component, Total- Interactions of component-based
software, Interaction-Ratio of a component, Interaction-Ratio of component-based software,
Average- Interaction among components and Interaction-Percentage of components.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2659118.2659135},
  issue_date = {September 2014},
  keywords   = {component-based software development, adaptable, in-interactions, out-interactions, context-based, metrics, pre-engineered},
  numpages   = {4},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2659118.2659135},
}

@InProceedings{Mo2018,
  author    = {Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  title     = {Experiences Applying Automated Architecture Analysis Tool Suites},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {779–789},
  publisher = {Association for Computing Machinery},
  series    = {ASE 2018},
  abstract  = {In this paper, we report our experiences of applying three complementary automated
software architecture analysis techniques, supported by a tool suite, called DV8,
to 8 industrial projects within a large company. DV8 includes two state-of-the-art
architecture-level maintainability metrics—Decoupling Level and Propagation Cost,
an architecture flaw detection tool, and an architecture root detection tool. We collected
development process data from the project teams as input to these tools, reported
the results back to the practitioners, and followed up with telephone conferences
and interviews. Our experiences revealed that the metrics scores, quantitative debt
analysis, and architecture flaw visualization can effectively bridge the gap between
management and development, help them decide if, when, and where to refactor. In particular,
the metrics scores, compared against industrial benchmarks, faithfully reflected the
practitioners’ intuitions about the maintainability of their projects, and enabled
them to better understand the maintainability relative to other projects internal
to their company, and to other industrial products. The automatically detected architecture
flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify
the “hotspots" within the systems that are responsible for high maintenance costs.
Except for the two smallest projects for which both architecture metrics indicated
high maintainability, all other projects are planning or have already begun refactorings
to address the problems detected by our analyses. We are working on further automating
the tool chain, and transforming the analysis suite into deployable services accessible
by all projects within the company.},
  doi       = {10.1145/3238147.3240467},
  isbn      = {9781450359375},
  keywords  = {Software Quality, Software Maintenance, Software Architecture},
  location  = {Montpellier, France},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3238147.3240467},
}

@InProceedings{Abderrahim2016,
  author    = {Abderrahim, Wiem and Choukair, Zied},
  booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
  title     = {PaaS Dependability Integration Architecture Based on Cloud Brokering},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {484–487},
  publisher = {Association for Computing Machinery},
  series    = {SAC '16},
  abstract  = {Cloud computing has revolutionized the way IT is provisioned nowadays since it exposes
computing capabilities as rental resources to consumers. The emergence of cloud computing
services hasn't though prevented outages in these environments even among high profile
ranked cloud providers. Tremendous efforts concentrated on fault management measures
have been applied for these environments. But they have been focused mainly on the
IaaS service model and have been operated on the cloud provider side alone. In this
context, this paper proposes an architecture for cloud brokering that implements dependability
properties in an end to end way involving different cloud actors and all over the
cloud service models SaaS, PaaS and IaaS.},
  doi       = {10.1145/2851613.2851874},
  isbn      = {9781450337397},
  keywords  = {fault tolerance, cloud provider, PaaS, IaaS, SaaS, fault management, cloud broker, fault forecasting, dependability},
  location  = {Pisa, Italy},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2851613.2851874},
}

@Article{Hu2017,
  author     = {Hu, Han and Wen, Yonggang and Chua, Tat-Seng and Li, Xuelong},
  journal    = {ACM Trans. Intell. Syst. Technol.},
  title      = {Cost-Optimized Microblog Distribution over Geo-Distributed Data Centers: Insights from Cross-Media Analysis},
  year       = {2017},
  issn       = {2157-6904},
  month      = apr,
  number     = {3},
  volume     = {8},
  abstract   = {The unprecedent growth of microblog services poses significant challenges on network
traffic and service latency to the underlay infrastructure (i.e., geo-distributed
data centers). Furthermore, the dynamic evolution in microblog status generates a
huge workload on data consistence maintenance. In this article, motivated by insights
of cross-media analysis-based propagation patterns, we propose a novel cache strategy
for microblog service systems to reduce the inter-data center traffic and consistence
maintenance cost, while achieving low service latency. Specifically, we first present
a microblog classification method, which utilizes the external knowledge from correlated
domains, to categorize microblogs. Then we conduct a large-scale measurement on a
representative online social network system to study the category-based propagation
diversity on region and time scales. These insights illustrate social common habits
on creating and consuming microblogs and further motivate our architecture design.
Finally, we formulate the content cache problem as a constrained optimization problem.
By jointly using the Lyapunov optimization framework and simplex gradient method,
we find the optimal online control strategy. Extensive trace-driven experiments further
demonstrate that our algorithm reduces the system cost by 24.5% against traditional
approaches with the same service latency.},
  address    = {New York, NY, USA},
  articleno  = {40},
  doi        = {10.1145/3014431},
  issue_date = {April 2017},
  keywords   = {cross-media analysis, performance optimization, Social media analytics, data center},
  numpages   = {18},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3014431},
}

@InProceedings{Scheuner2019,
  author    = {Scheuner, Joel and Leitner, Philipp},
  booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
  title     = {Performance Benchmarking of Infrastructure-as-a-Service (IaaS) Clouds with Cloud WorkBench},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {53–56},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '19},
  abstract  = {The continuing growth of the cloud computing market has led to an unprecedented diversity
of cloud services with different performance characteristics. To support service selection,
researchers and practitioners conduct cloud performance benchmarking by measuring
and objectively comparing the performance of different providers and configurations
(e.g., instance types in different data center regions). In this tutorial, we demonstrate
how to write performance tests for IaaS clouds using the Web-based benchmarking tool
Cloud WorkBench (CWB). We will motivate and introduce benchmarking of IaaS cloud in
general, demonstrate the execution of a simple benchmark in a public cloud environment,
summarize the CWB tool architecture, and interactively develop and deploy a more advanced
benchmark together with the participants.},
  doi       = {10.1145/3302541.3310294},
  isbn      = {9781450362863},
  keywords  = {performance, benchmarking, cloud computing},
  location  = {Mumbai, India},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3302541.3310294},
}

@InProceedings{Klaver2021,
  author    = {Klaver, Luuk and van der Knaap, Thijs and van der Geest, Johan and Harmsma, Edwin and van der Waaij, Bram and Pileggi, Paolo},
  booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
  title     = {Towards Independent Run-Time Cloud Monitoring},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {21–26},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '21},
  abstract  = {Cloud computing services are integral to the digital transformation. They deliver
greater connectivity, tremendous savings, and lower total cost of ownership. Despite
such benefits and benchmarking advances, costs are still quite unpredictable, performance
is unclear, security is inconsistent, and there is minimal control over aspects like
data and service locality. Estimating performance of cloud environments is very hard
for cloud consumers. They would like to make informed decisions about which provider
better suits their needs using specialized evaluation mechanisms. Providers have their
own tools reporting specific metrics, but they are potentially biased and often incomparable
across providers. Current benchmarking tools allow comparison but consumers need more
flexibility to evaluate environments under actual operating conditions for specialized
applications. Ours is early stage work and a step towards a monitoring solution that
enables independent evaluation of clouds for very specific application needs. In this
paper, we present our initial architecture of the Cloud Monitor that aims to integrate
existing and new benchmarks in a flexible and extensible way. By way of a simplistic
demonstrator, we illustrate the concept. We report some preliminary monitoring results
after a brief time of monitoring and are able to observe unexpected anomalies. The
results suggest an independent monitoring solution is a powerful enabler of next generation
cloud computing, not only for the consumer but potentially the whole ecosystem.},
  doi       = {10.1145/3447545.3451180},
  isbn      = {9781450383318},
  keywords  = {performance evaluation, run-time monitoring, benchmarking, cloud computing},
  location  = {Virtual Event, France},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3447545.3451180},
}

@InProceedings{Zhang2015,
  author    = {Zhang, Tianwei and Lee, Ruby B.},
  booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
  title     = {CloudMonatt: An Architecture for Security Health Monitoring and Attestation of Virtual Machines in Cloud Computing},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {362–374},
  publisher = {Association for Computing Machinery},
  series    = {ISCA '15},
  abstract  = {Cloud customers need guarantees regarding the security of their virtual machines (VMs),
operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated
by the customer not knowing where his VM is executing, and on the semantic gap between
what the customer wants to know versus what can be measured in the cloud. We present
an architecture for monitoring a VM's security health, with the ability to attest
this to the customer in an unforgeable manner. We show a concrete implementation of
property-based attestation and a full prototype based on the OpenStack open source
cloud software.},
  doi       = {10.1145/2749469.2750422},
  isbn      = {9781450334020},
  location  = {Portland, Oregon},
  numpages  = {13},
  url       = {https://doi.org/10.1145/2749469.2750422},
}

@InProceedings{TolosanaCalasanz2014,
  author    = {Tolosana-Calasanz, Rafael and Ba\~{n}ares, Jos\'{e} \'{A}ngel and Rana, Omer and Pham, Congduc and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
  booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Enforcing Quality of Service on OpenNebula-Based Shared Clouds},
  year      = {2014},
  pages     = {651–659},
  publisher = {IEEE Press},
  series    = {CCGRID '14},
  abstract  = {With an increase in the number of monitoring sensors deployed on physical infrastructures,
there is a corresponding increase in data volumes that need to be processed. Data
measured or collected by sensors is typically processed at destination or "in-transit"
(i.e. from data capture to delivery to a user). When such data are processed in-transit
over a shared distributed computing infrastructure, it is useful to provide elastic
computational capability which can be adapted based on processing requirements and
demand. Where Service Level Agreements (SLAs) have been pre-agreed, such available
computational capacity needs to be shared in such a way that any Quality of Service
related constraints in such SLAs are not violated. This is particularly challenging
for time critical applications and with highly variable and unpredictable rates of
data generation (e.g. in Smart Grid applications where energy usage patterns may change
unpredictably). Previously, we proposed a Reference net based architectural model
for supporting QoS for multiple concurrent data streams being processed (prior to
delivery to a user) over a shared infrastructure. In this paper, we describe a practical
realisation of this architecture using the OpenNebula Cloud platform. We consider
our infrastructure to be composed of a number of nodes, each of which has multiple
processing units and data buffers. We utilize the "token bucket" model for regulating,
on a per stream basis, the data injection rate into each node. We subsequently demonstrate
how a streaming pipeline can be supported and managed using a dynamic control strategy
at each node.},
  doi       = {10.1109/CCGrid.2014.50},
  isbn      = {9781479927838},
  location  = {Chicago, Illinois},
  numpages  = {9},
  url       = {https://doi.org/10.1109/CCGrid.2014.50},
}

@InProceedings{Lattanzi2015,
  author    = {Lattanzi, Silvio and Mirrokni, Vahab},
  booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
  title     = {Distributed Graph Algorithmics: Theory and Practice},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {419–420},
  publisher = {Association for Computing Machinery},
  series    = {WSDM '15},
  abstract  = {As a fundamental tool in modeling and analyzing social, and information networks,
large-scale graph mining is an important component of any tool set for big data analysis.
Processing graphs with hundreds of billions of edges is only possible via developing
distributed algorithms under distributed graph mining frameworks such as MapReduce,
Pregel, Gigraph, and alike. For these distributed algorithms to work well in practice,
we need to take into account several metrics such as the number of rounds of computation
and the communication complexity of each round. For example, given the popularity
and ease-of-use of MapReduce framework, developing practical algorithms with good
theoretical guarantees for basic graph algorithms is a problem of great importance.In
this tutorial, we first discuss how to design and implement algorithms based on traditional
MapReduce architecture. In this regard, we discuss various basic graph theoretic problems
such as computing connected components, maximum matching, MST, counting triangle and
overlapping or balanced clustering. We discuss a computation model for MapReduce and
describe the sampling, filtering, local random walk, and core-set techniques to develop
efficient algorithms in this framework. At the end, we explore the possibility of
employing other distributed graph processing frameworks. In particular, we study the
effect of augmenting MapReduce with a distributed hash table (DHT) service and also
discuss the use of a new graph processing framework called ASYMP based on asynchronous
message-passing method. In particular, we will show that using ASyMP, one can improve
the CPU usage, and achieve significantly improved running time.},
  doi       = {10.1145/2684822.2697043},
  isbn      = {9781450333177},
  keywords  = {parallel computing, mapreduce algorithms, large scale data-mining},
  location  = {Shanghai, China},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2684822.2697043},
}

@InProceedings{GraciaTinedo2015,
  author    = {Gracia-Tinedo, Ra\'{u}l and Tian, Yongchao and Samp\'{e}, Josep and Harkous, Hamza and Lenton, John and Garc\'{\i}a-L\'{o}pez, Pedro and S\'{a}nchez-Artigas, Marc and Vukolic, Marko},
  booktitle = {Proceedings of the 2015 Internet Measurement Conference},
  title     = {Dissecting UbuntuOne: Autopsy of a Global-Scale Personal Cloud Back-End},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {155–168},
  publisher = {Association for Computing Machinery},
  series    = {IMC '15},
  abstract  = {Personal Cloud services, such as Dropbox or Box, have been widely adopted by users.
Unfortunately, very little is known about the internal operation and general characteristics
of Personal Clouds since they are proprietary services.In this paper, we focus on
understanding the nature of Personal Clouds by presenting the internal structure and
a measurement study of UbuntuOne (U1). We first detail the U$1$ architecture, core
components involved in the U1 metadata service hosted in the datacenter of Canonical,
as well as the interactions of U$1$ with Amazon S3 to outsource data storage. To our
knowledge, this is the first research work to describe the internals of a large-scale
Personal Cloud.Second, by means of tracing the U$1$ servers, we provide an extensive
analysis of its back-end activity for one month. Our analysis includes the study of
the storage workload, the user behavior and the performance of the U1 metadata store.
Moreover, based on our analysis, we suggest improvements to U1 that can also benefit
similar Personal Cloud systems.Finally, we contribute our dataset to the community,
which is the first to contain the back-end activity of a large-scale Personal Cloud.
We believe that our dataset provides unique opportunities for extending research in
the field.},
  doi       = {10.1145/2815675.2815677},
  isbn      = {9781450338486},
  keywords  = {personal cloud, performance analysis, measurement},
  location  = {Tokyo, Japan},
  numpages  = {14},
  url       = {https://doi.org/10.1145/2815675.2815677},
}

@InProceedings{Singhvi2017,
  author    = {Singhvi, Arjun and Banerjee, Sujata and Harchol, Yotam and Akella, Aditya and Peek, Mark and Rydin, Pontus},
  booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
  title     = {Granular Computing and Network Intensive Applications: Friends or Foes?},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {157–163},
  publisher = {Association for Computing Machinery},
  series    = {HotNets-XVI},
  abstract  = {Computing/infrastructure as a service continues to evolve with bare metal, virtual
machines, containers and now serverless granular computing service offerings. Granular
computing enables developers to decompose their applications into smaller logical
units or functions, and run them on small, low cost and short lived computation containers
without having to worry about setting up servers - hence the term serverless computing.
While serverless environments can be used very cost effectively for large scale parallel
processing data analytics applications, it is less clear if network intensive packet
processing applications can also benefit from these new computing services as they
do not share the same characteristics. This paper examines the architectural constraints
as well as current serverless implementations to develop a position on this topic
and influence the next generation of computing services. We support our position through
measurement and experimentation on Amazon's AWS Lambda service with a few popular
network functions.},
  doi       = {10.1145/3152434.3152450},
  isbn      = {9781450355698},
  location  = {Palo Alto, CA, USA},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3152434.3152450},
}

@InProceedings{Zimmermann2015,
  author    = {Zimmermann, Olaf},
  booktitle = {Proceedings of the Second International Workshop on Software Architecture and Metrics},
  title     = {Metrics for Architectural Synthesis and Evaluation: Requirements and Compilation by Viewpoint: An Industrial Experience Report},
  year      = {2015},
  pages     = {8–14},
  publisher = {IEEE Press},
  series    = {SAM '15},
  abstract  = {During architectural analysis and synthesis, architectural metrics are established
tacitly or explicitly. In architectural evaluation, these metrics are then consulted
to assess whether architectures are fit for purpose and in line with recommended practices
and published architectural knowledge. This experience report presents a personal
retrospective of the author's use of architectural metrics during 20 years in IT architect
roles in professional services as well as research and development. This reflection
drives the identification of use cases, critical success factors and elements of risk
for architectural metrics management. An initial catalog of architectural metrics
is compiled next, which is organized by viewpoints and domains. The report concludes
with a discussion of practical impact of architectural metrics and potential research
topics in this area.},
  keywords  = {patterns, viewpoints, architectural metrics, architectural metrics management, enterprise information systems, architectural reviews, integration},
  location  = {Florence, Italy},
  numpages  = {7},
}

@InProceedings{Jun2017,
  author    = {Jun, Tae Joon and Yoo, Myong Hwan and Kim, Daeyoung and Cho, Kyu Tae and Lee, Seung Young and Yeun, Kyuoke},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
  title     = {HPC Supported Mission-Critical Cloud Architecture},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {223–232},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '17},
  abstract  = {Tactical Operations Center (TOC) system in military field is an advanced computer
system composed of multiple servers and desktops to interlock internal/external weapon
systems processing mission-critical applications in combat situation. However, the
current TOC system has several limitations such as difficulty of integrating tactical
weapon systems including missile launch system and radar system into the single TOC
system due to the heterogeneity of HW and SW between systems, and an inefficient computing
resource management for the weapon systems.In this paper, we proposed a novel HPC
supported mission-critical Cloud architecture as TOC for Surface-to-Air-Missile (SAM)
system with OpenStack Cloud OS, Data Distribution Service (DDS), and GPU virtualization
techniques. With this approach, our system provides elastic resource management over
the weapon systems with virtual machines, integration of heterogeneous systems with
different kinds of guest OS, real-time, reliable, and high-speed communication between
the virtual machines and virtualized GPU resource over the virtual machines. Evaluation
of our TOC system includes DDS performance measurement over 10Gbps Ethernet and QDR
InfiniBand networks on the virtualized environment with OpenStack Cloud OS, and GPU
virtualization performance evaluation with two different methods, PCI pass-through
and remote-API. With the evaluation results, we conclude that our system provides
reasonable performance in the combat situation compared to the previous TOC system
while additionally supports scalable and elastic use of computing resource through
the virtual machines.},
  doi       = {10.1145/3030207.3044531},
  isbn      = {9781450344043},
  keywords  = {cloud computing, tactical operations center, data distribution service, gpgpu},
  location  = {L'Aquila, Italy},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3030207.3044531},
}

@InProceedings{Zibitsker2020,
  author    = {Zibitsker, Boris and Lupersolsky, Alex},
  booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
  title     = {How to Apply Modeling to Compare Options and Select the Appropriate Cloud Platform},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {16},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '20},
  abstract  = {Organizations want to take advantage of the flexibility and scalability of Cloud platforms.
By migrating to the Cloud, they hope to develop and implement new applications faster
with lower cost. Amazon AWS, Microsoft Azure, Google, IBM, Oracle and others Cloud
providers support different DBMS like Snowflake, Redshift, Teradata Vantage, and others.
These platforms have different architectures, mechanisms of allocation and management
of resources, and levels of sophistication of DBMS optimizers which affect performance,
scalability and cost. As a result, the response time, CPU Service Time and the number
of I/Os for the same query, accessing the similar table in the Cloud could be significantly
different than On Prem. In order to select the appropriate Cloud platform as a first
step we perform a Workload Characterization for On Prem Data Warehouse. Each Data
Warehouse workload represents a specific line of business and includes activity of
many users generating concurrently simple and complex queries accessing data from
different tables. Each workload has different demands for resources and different
Response Time and Throughput Service Level Goals. In this presentation we will review
results of the workload characterization for an On Prem Data Warehouse environment.
During the second step we collected measurement data for standard TPC-DS benchmark
tests performed in AWS Vantage, Redshift and Snowflake Cloud platform for different
sizes of the data sets and different number of concurrent users. During the third
step we used the results of the workload characterization and measurement data collected
during the benchmark to modify BEZNext On Prem Closed Queueing model to model individual
Clouds. And finally, during the fourth step we used our Model to take into consideration
differences in concurrency, priorities and resource allocation to different workloads.
BEZNext optimization algorithms incorporating Graduate search mechanism are used to
find the AWS instance type and minimum number of instances which will be required
to meet SLGs for each of the workloads. Publicly available information about the cost
of the different AWS instances is used to predict the cost of supporting workloads
in the Cloud month by month during next 12 months.},
  doi       = {10.1145/3375555.3384938},
  isbn      = {9781450371094},
  keywords  = {seasonality determination, service level goals, workload forecasting, benchmarking, workload characterization, cloud platform, optimization., modeling},
  location  = {Edmonton AB, Canada},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3375555.3384938},
}

@InProceedings{Marquez2018,
  author    = {Marquez, Cristina and Gramaglia, Marco and Fiore, Marco and Banchs, Albert and Costa-Perez, Xavier},
  booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
  title     = {How Should I Slice My Network? A Multi-Service Empirical Evaluation of Resource Sharing Efficiency},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {191–206},
  publisher = {Association for Computing Machinery},
  series    = {MobiCom '18},
  abstract  = {By providing especially tailored instances of a virtual network,network slicing allows
for a strong specialization of the offered services on the same shared infrastructure.
Network slicing has profound implications on resource management, as it entails an
inherent trade-off between: (i) the need for fully dedicated resources to support
service customization, and (ii) the dynamic resource sharing among services to increase
resource efficiency and cost-effectiveness of the system. In this paper, we provide
a first investigation of this trade-off via an empirical study of resource management
efficiency in network slicing. Building on substantial measurement data collected
in an operational mobile network (i) we quantify the efficiency gap introduced by
non-reconfigurable allocation strategies of different kinds of resources, from radio
access to the core of the network, and (ii) we quantify the advantages of their dynamic
orchestration at different timescales. Our results provide insights on the achievable
efficiency of network slicing architectures, their dimensioning, and their interplay
with resource management algorithms.},
  doi       = {10.1145/3241539.3241567},
  isbn      = {9781450359030},
  keywords  = {resource management, network efficiency, network slicing},
  location  = {New Delhi, India},
  numpages  = {16},
  url       = {https://doi.org/10.1145/3241539.3241567},
}

@InProceedings{Wei2018,
  author    = {Wei, Tianshu and Chen, Xiaoming and Li, Xin and Zhu, Qi},
  booktitle = {Proceedings of the International Conference on Computer-Aided Design},
  title     = {Model-Based and Data-Driven Approaches for Building Automation and Control},
  year      = {2018},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICCAD '18},
  abstract  = {Smart buildings in the future are complex cyber-physical-human systems that involve
close interactions among embedded platform (for sensing, computation, communication
and control), mechanical components, physical environment, building architecture,
and occupant activities. The design and operation of such buildings require a new
set of methodologies and tools that can address these heterogeneous domains in a holistic,
quantitative and automated fashion. In this paper, we will present our design automation
methods for improving building energy efficiency and offering comfortable services
to occupants at low cost. In particular, we will highlight our work in developing
both model-based and data-driven approaches for building automation and control, including
methods for co-scheduling heterogeneous energy demands and supplies, for integrating
intelligent building energy management with grid optimization through a proactive
demand response framework, for optimizing HVAC control with deep reinforcement learning,
and for accurately measuring in-building temperature by combining prior modeling information
with few sensor measurements based upon Bayesian inference.},
  articleno = {26},
  doi       = {10.1145/3240765.3243485},
  isbn      = {9781450359504},
  keywords  = {smart buildings, deep reinforcement learning, model-based design, Bayesian inference, data-driven, model predictive control},
  location  = {San Diego, California},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3240765.3243485},
}

@InProceedings{Antonescu2014,
  author    = {Antonescu, Alexandru-Florian and Braun, Torsten},
  booktitle = {Proceedings of the 2014 ACM SIGCOMM Workshop on Distributed Cloud Computing},
  title     = {Modeling and Simulation of Concurrent Workload Processing in Cloud-Distributed Enterprise Information Systems},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {11–16},
  publisher = {Association for Computing Machinery},
  series    = {DCC '14},
  abstract  = {Cloud Computing enables provisioning and distribution of highly scalable services
in a reliable, on-demand and sustainable manner. Distributed Enterprise Information
Systems (dEIS) are a class of applications with important economic value and with
strong requirements in terms of performance and reliability. In order to validate
dEIS architectures, stability, scaling and SLA compliance, large testing deployments
are necessary, adding complexity to the design and testing of such systems. To fill
this gap, we present and validate a methodology for modeling and simulating such complex
distributed systems using the CloudSim cloud computing simulator, based on measurement
data from an actual distributed system. We present an approach for creating a performance-based
model of a distributed cloud application using recorded service performance traces.
We then show how to integrate the created model into CloudSim. We validate the CloudSim
simulation model by comparing performance traces gathered during distributed concurrent
experiments with simulation results using different VM configurations. We demonstrate
the usefulness of using a cloud simulator for modeling properties of real cloud-distributed
applications.},
  doi       = {10.1145/2627566.2627575},
  isbn      = {9781450329927},
  keywords  = {cloud computing, performance profiling, distributed applications, modelling and simulation},
  location  = {Chicago, Illinois, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2627566.2627575},
}

@InProceedings{Walter2017,
  author    = {Walter, J\"{u}rgen and Stier, Christian and Koziolek, Heiko and Kounev, Samuel},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
  title     = {An Expandable Extraction Framework for Architectural Performance Models},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {165–170},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '17 Companion},
  abstract  = {Providing users with Quality of Service (QoS) guarantees and the prevention of performance
problems are challenging tasks for software systems. Architectural performance models
can be applied to explore performance properties of a software system at design time
and run time. At design time, architectural performance models support reasoning on
effects of design decisions. At run time, they enable automatic reconfigurations by
reasoning on the effects of changing user behavior. In this paper, we present a framework
for the extraction of architectural performance models based on monitoring log files
generalizing over the targeted architectural modeling language. Using the presented
framework, the creation of a performance model extraction tool for a specific modeling
formalism requires only the implementation of a key set of object creation routines
specific to the formalism. Our framework integrates them with extraction techniques
that apply to many architectural performance models, e.g., resource demand estimation
techniques. This lowers the effort to implement performance model extraction tools
tremendously through a high level of reuse. We evaluate our framework presenting builders
for the Descartes Modeling Language (DML) and the Palladio Component Model(PCM). For
the extracted models we compare simulation results with measurements receiving accurate
results.},
  doi       = {10.1145/3053600.3053634},
  isbn      = {9781450348997},
  keywords  = {descartes modeling language, palladio component model, automated performance model extraction, builder pattern},
  location  = {L'Aquila, Italy},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3053600.3053634},
}

@InProceedings{Falkner2016,
  author    = {Falkner, Katrina and Szabo, Claudia and Chiprianov, Vanea},
  booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
  title     = {Model-Driven Performance Prediction of Systems of Systems},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {44},
  publisher = {Association for Computing Machinery},
  series    = {MODELS '16},
  abstract  = {Systems of Systems exhibit characteristics that pose difficulty in modelling and predicting
their overall performance capabilities, including the presence of operational independence,
emergent behaviour, and evolutionary development. When considering Systems of Systems
within the autonomous defence systems context, these aspects become increasingly critical,
as performance constraints are typically driven by hard constraints on space, weight
and power.System execution modelling languages and tools permit early prediction of
the performance of model-driven systems, however the focus to date has been on understanding
the performance of a model rather than determining if it meets performance requirements,
and only subsequently carrying out analysis to reveal the causes of any requirement
violations. Such an analysis is even more difficult when applied to several systems
cooperating to achieve a common goal - a System of Systems (SoS).The successful integration
of systems within a SoS context has been identified as one of the most substantial
challenges facing military systems development [2]. Accordingly, there is a critical
need to understand the non-functional aspects of the SoS (such as quality of service,
power, size, cost and scalable management of communications), and to explore how these
non-functional aspects evolve under new conditions and deployment scenarios. It is
crucial that we develop methodologies for modelling and understanding non-functional
properties early in the development and integration cycle to better inform our understanding
of the impact of emergent behaviour and evolution within the SoS.We propose an integrated
approach to performance prediction of model-driven real time embedded defence systems
and systems of systems [1]. Our architectural prototyping system supports a scenario-driven
experimental platform for evaluating model suitability within a set of deployment
and real-time performance constraints. We present an overview of our performance prediction
system, demonstrating the integration of modelling, execution and performance analysis,
and discuss a case study to illustrate our approach. Our work employs state-of-the-art
model-driven engineering techniques to facilitate SoS performance prediction and analysis
at design time, either before the SoS is built and deployed, or during its lifetime
when required to evolve.Our model-driven performance prediction platform supports
a scenario-driven experimental environment for evaluating a SoS within the context
of a specific deployment (modelling geographical distribution) and integration constraints.
The main contributions of our work are: (a) a modeling methodology that captures diverse
perspectives of the performance modeling of Systems of Systems; (b) a performance
analysis engine that captures metrics associated with these perspectives and (c) a
case study showing the performance evaluaton of a system of systems and its evolution
as a result of the performance analysis. We discuss how our approach to modelling
supports the specific characteristics of an SoS, and illustrate this through a case
study, based on a "Blue Ocean" scenario, demonstrating how we may obtain performance
predictions within a SoS with emergent and evolutionary properties. Within the context
of our environment, we define models for the individual systems within our System
of Systems, defined for representative workload to predict execution costs, i.e. CPU,
memory usage and network usage, within a generic situation. Our modelling environment
supports the generation of executable forms of these models, which may then be executed
above realistic deployment scenarios in order to obtain predictions of System of System
performance.},
  doi       = {10.1145/2976767.2987689},
  isbn      = {9781450343213},
  location  = {Saint-malo, France},
  numpages  = {1},
  url       = {https://doi.org/10.1145/2976767.2987689},
}

@InProceedings{Harsh2019,
  author    = {Harsh, Piyush and Ribera Laszkowski, Juan Francisco and Edmonds, Andy and Quang Thanh, Tran and Pauls, Michael and Vlaskovski, Radoslav and Avila-Garc\'{\i}a, Orlando and Pages, Enric and Gort\'{a}zar Bellas, Francisco and Gallego Carrillo, Micael},
  booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
  title     = {Cloud Enablers For Testing Large-Scale Distributed Applications},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {35–42},
  publisher = {Association for Computing Machinery},
  series    = {UCC '19 Companion},
  abstract  = {Testing large-scale distributed systems (also known as testing in the large) is a
challenge that spreads across different technical domains and areas of expertise.
Current methods and tools provide some minimal guarantees in relation to the correctness
of their functional properties and have serious limitations when evaluating their
extra-functional properties in realistic conditions, such as scalability, availability
and performance efficiency. Cloud Testing and more specifically "testing in the cloud''
has arisen to tackle those challenges. In this new paradigm, cloud-based environment
and infrastructure are used to run realistic end-to-end and/or system-level tests,
collect test data and analyse them. In this paper we present a set of cloud-native
services to take from the tester the responsibility of managing the resources and
complementary services required to simulate realistic operational conditions and production
environments. Specifically, they provide cloud testing capabilities such as logs and
measurements collection from both testing jobs and system under test; test data analytics
and visualization; provisioning and operation of additional services and processes
to replicate realistic production ecosystems; support to scalability and diversity
of underlying testing infrastructure; and replication of the operational conditions
of the software under test through its instrumentation. We present the architecture
of the cloud testing solution and the detailed design of each of the services; we
also evaluate their relative contribution to satisfy different needs in the context
of test execution.},
  doi       = {10.1145/3368235.3368838},
  isbn      = {9781450370448},
  keywords  = {reliability, cloud testing, continuous testing, testing, continuous integration, large-scale distributed systems, scalability},
  location  = {Auckland, New Zealand},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3368235.3368838},
}

@InProceedings{VasanthaAzhagu2016,
  author    = {VasanthaAzhagu, A. Kannaki and Gnanasekar, J. M.},
  booktitle = {Proceedings of the International Conference on Informatics and Analytics},
  title     = {Cloud Computing Overview, Security Threats and Solutions-A Survey},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICIA-16},
  abstract  = {Cloud Computing aims to provide computing everywhere. It delivers computing resources
on demand over internet in terms of anything anywhere anytime concept. It provides
everything as a service to its users like infrastructure platform software hardware
workplace data and security. Cloud computing has made revolutionary transformations
in the government and business. Cloud Computing transforms the databases and application
software to the huge data centers, where the management of the services and data may
not be trustworthy. To verify the correctness, integrity, confidentially and availability
of data in the cloud, in this paper, we focus on various cloud computing security
threats and solution that have been used since security is an important measure for
quality of service.},
  articleno = {109},
  doi       = {10.1145/2980258.2982046},
  isbn      = {9781450347563},
  keywords  = {Availability, Cloud Computing, Deployment Security threats, Integrity, Quality of Service (QoS)},
  location  = {Pondicherry, India},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2980258.2982046},
}

@InProceedings{Becker2015,
  author    = {Becker, Matthias and Lehrig, Sebastian and Becker, Steffen},
  booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
  title     = {Systematically Deriving Quality Metrics for Cloud Computing Systems},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {169–174},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '15},
  abstract  = {In cloud computing, software architects develop systems for virtually unlimited resources
that cloud providers account on a pay-per-use basis. Elasticity management systems
provision these resources autonomously to deal with changing workload. Such changing
workloads call for new objective metrics allowing architects to quantify quality properties
like scalability, elasticity, and efficiency, e.g., for requirements/SLO engineering
and software design analysis. In literature, initial metrics for these properties
have been proposed. However, current metrics lack a systematic derivation and assume
knowledge of implementation details like resource handling. Therefore, these metrics
are inapplicable where such knowledge is unavailable.To cope with these lacks, this
short paper derives metrics for scalability, elasticity, and efficiency properties
of cloud computing systems using the goal question metric (GQM) method. Our derivation
uses a running example that outlines characteristics of cloud computing systems. Eventually,
this example allows us to set up a systematic GQM plan and to derive an initial set
of six new metrics. We particularly show that our GQM plan allows to classify existing
metrics.},
  doi       = {10.1145/2668930.2688043},
  isbn      = {9781450332484},
  keywords  = {metric, elasticity, cloud computing, gqm, efficiency, slo, analysis, scalability},
  location  = {Austin, Texas, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2668930.2688043},
}

@InProceedings{Lehrig2015,
  author    = {Lehrig, Sebastian and Eikerling, Hendrik and Becker, Steffen},
  booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
  title     = {Scalability, Elasticity, and Efficiency in Cloud Computing: A Systematic Literature Review of Definitions and Metrics},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {83–92},
  publisher = {Association for Computing Machinery},
  series    = {QoSA '15},
  abstract  = {Context: In cloud computing, there is a multitude of definitions and metrics for scalability,
elasticity, and efficiency. However, stakeholders have little guidance for choosing
fitting definitions and metrics for these quality properties, thus leading to potential
misunderstandings. For example, cloud consumers and providers cannot negotiate reliable
and quantitative service level objectives directly understood by each stakeholder.
Objectives: Therefore, we examine existing definitions and metrics for these quality
properties from the viewpoint of cloud consumers, cloud providers, and software architects
with regard to commonly used concepts. Methods: We execute a systematic literature
review (SLR), reproducibly collecting common concepts in definitions and metrics for
scalability, elasticity, and efficiency. As quality selection criteria, we assess
whether existing literature differentiates the three properties, exemplifies metrics,
and considers typical cloud characteristics and cloud roles. Results: Our SLR yields
418 initial results from which we select 20 for in-depth evaluation based on our quality
selection criteria. In our evaluation, we recommend concepts, definitions, and metrics
for each property. Conclusions: Software architects can use our recommendations to
analyze the quality of cloud computing applications. Cloud providers and cloud consumers
can specify service level objectives based on our metric suggestions.},
  doi       = {10.1145/2737182.2737185},
  isbn      = {9781450334709},
  keywords  = {systematic literature review, cloud computing, cloud, scalability, metrics, elasticity, efficiency, definitions},
  location  = {Montr\'{e}al, QC, Canada},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2737182.2737185},
}

@InProceedings{Kirsal2015,
  author    = {Kirsal, Yonal and Ever, Yoney Kirsal and Mostarda, Leonardo and Gemikonakli, Orhan},
  booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
  title     = {Analytical Modelling and Performability Analysis for Cloud Computing Using Queuing System},
  year      = {2015},
  pages     = {643–647},
  publisher = {IEEE Press},
  series    = {UCC '15},
  abstract  = {In recent years, cloud computing becomes a new computing model emerged from the rapid
development of the internet. Users can reach their resources with high flexibility
using the cloud computing systems all over the world. However, such systems are prone
to failures. In order to obtain realistic quality of service (QoS) measurements, failure
and recovery behaviours of the system should be considered. System's failures and
repairs are associated with availability context in QoS measurements. In this paper,
performance issues are considered with the availability of the system. Markov Reward
Model (MRM) method is used to get QoS measurements. The mean queue length (MQL) results
are calculated using the MRM. The results explicitly show that failures and repairs
affect the system performance significantly.},
  isbn      = {9780769556970},
  keywords  = {quality of service, cloud computing, queuing system, analytical modelling, performability analysis},
  location  = {Limassol, Cyprus},
  numpages  = {5},
}

@InProceedings{Silva2020,
  author    = {Silva, Jorge Luiz Machado da and de Fran\c{c}a, Breno B. Nicolau and Rubira, Cec\'{\i}lia Mary Fischer},
  booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
  title     = {Generating Trustworthiness Adaptation Plans Based on Quality Models for Cloud Platforms},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {141–150},
  publisher = {Association for Computing Machinery},
  series    = {SBCARS '20},
  abstract  = {Cloud computing platforms can offer many benefits related to the provision of service
processing and storage for hosting client applications. Trustworthiness can be defined
as the trust of a customer in a cloud service and its provider; however, the assurance
of this property is not trivial. First, trustworthiness in general is not composed
by a single quality attribute, but by the combination of multiple attributes, such
as data privacy, performance, reliability, etc. Second, during runtime clients can
experience a change of the trustworthiness level required by their application due
to the degradation of the cloud service. This article presents a solution that monitors
during runtime the set of quality attributes of a specific application and generates
adaptation plans in order to certify that an adequate resource amount be provided
by the cloud in order to keep its trustworthiness level. Our solution is based on
quality models to compute the metric associated to each non-functional requirement
and their combination them into different types of trustworthiness levels. The main
contribution of the solution is to provide an approach which deals with multiple requirements
at the same time (or simultaneously) during runtime in order to adapt the cloud resources
to keep the trustworthiness level required by the application. The solution was evaluated
by an experiment considering a scenario where the application trustworthiness level
was composed by three quality attributes: data privacy, performance and reliability.
Initial results have shown that the approach is feasible in terms of the execution
of the adaptation plans during runtime to certify the trustworthiness level required
by the application.},
  doi       = {10.1145/3425269.3425272},
  isbn      = {9781450387545},
  keywords  = {Cloud Computing, Trustworthiness, Adaptation Planning, Self-adaptive Systems},
  location  = {Natal, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3425269.3425272},
}

@InProceedings{Lehrig2015a,
  author    = {Lehrig, Sebastian and Becker, Steffen},
  booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
  title     = {The CloudScale Method for Software Scalability, Elasticity, and Efficiency Engineering: A Tutorial},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {329–331},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '15},
  abstract  = {In cloud computing, software engineers design systems for virtually unlimited resources
that cloud providers account on a pay-per-use basis. Elasticity management systems
provision these resource autonomously to deal with changing workloads. Such workloads
call for new objective metrics allowing engineers to quantify quality properties like
scalability, elasticity, and efficiency. However, software engineers currently lack
engineering methods that aid them in engineering their software regarding such properties.
Therefore, the CloudScale project developed tools for such engineering tasks. These
tools cover reverse engineering of architectural models from source code, editors
for manual design/adaption of such models, as well as tools for the analysis of modeled
and operating software regarding scalability, elasticity, and efficiency. All tools
are interconnected via ScaleDL, a common architectural language, and the CloudScale
Method that leads through the engineering process. In this tutorial, we execute our
method step-by-step such that every tool and ScaleDL are briefly introduced.},
  doi       = {10.1145/2668930.2688818},
  isbn      = {9781450332484},
  keywords  = {engineering, cloudscale, tutorial, metrics, efficiency, scalability, elasticity, cloud computing, software analysis, method},
  location  = {Austin, Texas, USA},
  numpages  = {3},
  url       = {https://doi.org/10.1145/2668930.2688818},
}

@Article{Avgeris2019,
  author     = {Avgeris, Marios and Dechouniotis, Dimitrios and Athanasopoulos, Nikolaos and Papavassiliou, Symeon},
  journal    = {ACM Trans. Internet Technol.},
  title      = {Adaptive Resource Allocation for Computation Offloading: A Control-Theoretic Approach},
  year       = {2019},
  issn       = {1533-5399},
  month      = apr,
  number     = {2},
  volume     = {19},
  abstract   = {Although mobile devices today have powerful hardware and networking capabilities,
they fall short when it comes to executing compute-intensive applications. Computation
offloading (i.e., delegating resource-consuming tasks to servers located at the edge
of the network) contributes toward moving to a mobile cloud computing paradigm. In
this work, a two-level resource allocation and admission control mechanism for a cluster
of edge servers offers an alternative choice to mobile users for executing their tasks.
At the lower level, the behavior of edge servers is modeled by a set of linear systems,
and linear controllers are designed to meet the system’s constraints and quality of
service metrics, whereas at the upper level, an optimizer tackles the problems of
load balancing and application placement toward the maximization of the number the
offloaded requests. The evaluation illustrates the effectiveness of the proposed offloading
mechanism regarding the performance indicators, such as application average response
time, and the optimal utilization of the computational resources of edge servers.},
  address    = {New York, NY, USA},
  articleno  = {23},
  doi        = {10.1145/3284553},
  issue_date = {April 2019},
  keywords   = {feedback control, Edge computing, linear modeling},
  numpages   = {20},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3284553},
}

@InProceedings{Ilyushkin2017,
  author    = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Papadopoulos, Alessandro V. and Ghit, Bogdan and Epema, Dick and Iosup, Alexandru},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
  title     = {An Experimental Performance Evaluation of Autoscaling Policies for Complex Workflows},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {75–86},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '17},
  abstract  = {Simplifying the task of resource management and scheduling for customers, while still
delivering complex Quality-of-Service (QoS), is key to cloud computing. Many autoscaling
policies have been proposed in the past decade to decide on behalf of cloud customers
when and how to provision resources to a cloud application utilizing cloud elasticity
features. However, in prior work, when a new policy is proposed, it is seldom compared
to the state-of-the-art, and is often compared only to static provisioning using a
predefined QoS target. This reduces the ability of cloud customers and of cloud operators
to choose and deploy an autoscaling policy. In our work, we conduct an experimental
performance evaluation of autoscaling policies, using as application model workflows,
a commonly used formalism for automating resource management for applications with
well-defined yet complex structure. We present a detailed comparative study of general
state-of-the-art autoscaling policies, along with two new workflow-specific policies.
To understand the performance differences between the 7 policies, we conduct various
forms of pairwise and group comparisons. We report both individual and aggregated
metrics. Our results highlight the trade-offs between the suggested policies, and
thus enable a better understanding of the current state-of-the-art.},
  doi       = {10.1145/3030207.3030214},
  isbn      = {9781450344043},
  keywords  = {dag, cloud computing, metrics, supply, workflows, auto-scaling, scheduling, elasticity, clouds, opennebula, spec, demand, autoscaling, level of parallelism, performance, directed acyclic graph, workloads},
  location  = {L'Aquila, Italy},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3030207.3030214},
}

@Article{Ilyushkin2018,
  author     = {Ilyushkin, Alexey and Ali-Eldin, Ahmed and Herbst, Nikolas and Bauer, Andr\'{e} and Papadopoulos, Alessandro V. and Epema, Dick and Iosup, Alexandru},
  journal    = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
  title      = {An Experimental Performance Evaluation of Autoscalers for Complex Workflows},
  year       = {2018},
  issn       = {2376-3639},
  month      = apr,
  number     = {2},
  volume     = {3},
  abstract   = {Elasticity is one of the main features of cloud computing allowing customers to scale
their resources based on the workload. Many autoscalers have been proposed in the
past decade to decide on behalf of cloud customers when and how to provision resources
to a cloud application based on the workload utilizing cloud elasticity features.
However, in prior work, when a new policy is proposed, it is seldom compared to the
state-of-the-art, and is often compared only to static provisioning using a predefined
quality of service target. This reduces the ability of cloud customers and of cloud
operators to choose and deploy an autoscaling policy, as there is seldom enough analysis
on the performance of the autoscalers in different operating conditions and with different
applications. In our work, we conduct an experimental performance evaluation of autoscaling
policies, using as application model workflows, a popular formalism for automating
resource management for applications with well-defined yet complex structures. We
present a detailed comparative study of general state-of-the-art autoscaling policies,
along with two new workflow-specific policies. To understand the performance differences
between the seven policies, we conduct various experiments and compare their performance
in both pairwise and group comparisons. We report both individual and aggregated metrics.
As many workflows have deadline requirements on the tasks, we study the effect of
autoscaling on workflow deadlines. Additionally, we look into the effect of autoscaling
on the accounted and hourly based charged costs, and we evaluate performance variability
caused by the autoscaler selection for each group of workflow sizes. Our results highlight
the trade-offs between the suggested policies, how they can impact meeting the deadlines,
and how they perform in different operating conditions, thus enabling a better understanding
of the current state-of-the-art.},
  address    = {New York, NY, USA},
  articleno  = {8},
  doi        = {10.1145/3164537},
  issue_date = {April 2018},
  keywords   = {elasticity, benchmarking, metrics, Autoscaling, scientific workflows},
  numpages   = {32},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3164537},
}

@InProceedings{Michael2017,
  author    = {Michael, Nicolas and Ramannavar, Nitin and Shen, Yixiao and Patil, Sheetal and Sung, Jan-Lung},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
  title     = {CloudPerf: A Performance Test Framework for Distributed and Dynamic Multi-Tenant Environments},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {189–200},
  publisher = {Association for Computing Machinery},
  series    = {ICPE '17},
  abstract  = {The evolution of cloud-computing imposes many challenges on performance testing and
requires not only a different approach and methodology of performance evaluation and
analysis, but also specialized tools and frameworks to support such work. In traditional
performance testing, typically a single workload was run against a static test configuration.
The main metrics derived from such experiments included throughput, response times,
and system utilization at steady-state. While this may have been sufficient in the
past, where in many cases a single application was run on dedicated hardware, this
approach is no longer suitable for cloud-based deployments. Whether private or public
cloud, such environments typically host a variety of applications on distributed shared
hardware resources, simultaneously accessed by a large number of tenants running heterogeneous
workloads. The number of tenants as well as their activity and resource needs dynamically
change over time, and the cloud infrastructure reacts to this by reallocating existing
or provisioning new resources. Besides metrics such as the number of tenants and overall
resource utilization, performance testing in the cloud must be able to answer many
more questions: How is the quality of service of a tenant impacted by the constantly
changing activity of other tenants? How long does it take the cloud infrastructure
to react to changes in demand, and what is the effect on tenants while it does so?
How well are service level agreements met? What is the resource consumption of individual
tenants? How can global performance metrics on application- and system-level in a
distributed system be correlated to an individual tenant's perceived performance?In
this paper we present CloudPerf, a performance test framework specifically designed
for distributed and dynamic multi-tenant environments, capable of answering all of
the above questions, and more. CloudPerf consists of a distributed harness, a protocol-independent
load generator and workload modeling framework, an extensible statistics framework
with live-monitoring and post-analysis tools, interfaces for cloud deployment operations,
and a rich set of both low-level as well as high-level workloads from different domains.},
  doi       = {10.1145/3030207.3044530},
  isbn      = {9781450344043},
  keywords  = {cloud, multi-tenancy, statistics collection, load generation, performance testing, workload modeling},
  location  = {L'Aquila, Italy},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3030207.3044530},
}

@Article{GarciaDorado2017,
  author     = {Garc\'{\i}a-Dorado, Jos\'{e} Luis},
  journal    = {ACM Trans. Internet Technol.},
  title      = {Bandwidth Measurements within the Cloud: Characterizing Regular Behaviors and Correlating Downtimes},
  year       = {2017},
  issn       = {1533-5399},
  month      = aug,
  number     = {4},
  volume     = {17},
  abstract   = {The search for availability, reliability, and quality of service has led cloud infrastructure
customers to disseminate their services, contents, and data over multiple cloud data
centers, often involving several Cloud service providers (CSPs). The consequence of
this is that a large amount of data must be transmitted across the public Cloud. However,
little is known about the bandwidth dynamics involved. To address this, we have conducted
a measurement campaign for bandwidth between 18 data centers of four major CSPs. This
extensive campaign allowed us to characterize the resulting time series of bandwidth
as the addition of a stationary component and some infrequent excursions (typically
downtimes). While the former provides a description of the bandwidth users can expect
in the Cloud, the latter is closely related to the robustness of the Cloud (i.e.,
the occurrence of downtimes is correlated). Both components have been studied further
by applying factor analysis, specifically analysis of variance, as a mechanism to
formally compare data centers’ behaviors and extract generalities. The results show
that the stationary process is closely related to the data center locations and CSPs
involved in transfers that, fortunately, make the Cloud more predictable and allow
the set of reported measurements to be extrapolated. On the other hand, although correlation
in the Cloud is low, that is, only 10% of the measured pair of paths showed some correlation,
we found evidence that such correlation depends on the particular relationships between
pairs of data centers with little connection to more general factors. Positively,
this implies that data centers either in the same area or within the same CSP do not
show qualitatively more correlation than other data centers, which eases the deployment
of robust infrastructures. On the downside, this metric is scarcely generalizable
and, consequently, calls for exhaustive monitoring.},
  address    = {New York, NY, USA},
  articleno  = {39},
  doi        = {10.1145/3093893},
  issue_date = {September 2017},
  keywords   = {traffic correlation, inter-cloud, TCP bandwidth, ANOVA, Public cloud},
  numpages   = {25},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3093893},
}

@InProceedings{Ibrahim2016,
  author    = {Ibrahim, Abdallah Ali Zainelabden A. and Kliazovich, Dzmitry and Bouvry, Pascal},
  booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Service Level Agreement Assurance between Cloud Services Providers and Cloud Customers},
  year      = {2016},
  pages     = {588–591},
  publisher = {IEEE Press},
  series    = {CCGRID '16},
  abstract  = {Cloud services providers deliver cloud services to cloud customers on pay-per-use
model while the quality of the provided services are defined using service level agreements
also known as SLAs. Unfortunately, there is no standard mechanism which exists to
verify and assure that delivered services satisfy the signed SLA agreement in an automatic
way. There is no guarantee in terms of quality. Those applications have many performance
metrics. In this doctoral thesis, we propose a framework for SLA assurance, which
can be used by both cloud providers and cloud users. Inside the proposed framework,
we will define the performance metrics for the different applications. We will assess
the applications performance in different testing environment to assure good services
quality as mentioned in SLA. The proposed framework will be evaluated through simulations
and using testbed experiments. After testing the applications performance by measuring
the performance metrics, we will review the time correlations between those metrics.},
  doi       = {10.1109/CCGrid.2016.56},
  isbn      = {9781509024520},
  keywords  = {simulation, data centers, performance, metrics, quality of experience, applications, quality of services, cloud computing, service level agreement},
  location  = {Cartagena, Columbia},
  numpages  = {4},
  url       = {https://doi.org/10.1109/CCGrid.2016.56},
}

@InProceedings{Haq2017,
  author    = {Haq, Osama and Raja, Mamoon and Dogar, Fahad R.},
  booktitle = {Proceedings of the 26th International Conference on World Wide Web},
  title     = {Measuring and Improving the Reliability of Wide-Area Cloud Paths},
  year      = {2017},
  address   = {Republic and Canton of Geneva, CHE},
  pages     = {253–262},
  publisher = {International World Wide Web Conferences Steering Committee},
  series    = {WWW '17},
  abstract  = {Many popular cloud applications use inter-data center paths; yet, little is known
about the characteristics of these ``cloud paths''. Over an eighteen month period,
we measure the inter-continental cloud paths of three providers (Amazon, Google, and
Microsoft) using client side (VM-to-VM) measurements. We find that cloud paths are
more predictable compared to public Internet paths, with an order of magnitude lower
loss rate and jitter at the tail (95th percentile and beyond) compared to public Internet
paths. We also investigate the nature of packet losses on these paths (e.g., random
vs. bursty) and potential reasons why these paths may be better in quality. Based
on our insights, we consider how we can further improve the quality of these paths
with the help of existing loss mitigation techniques. We demonstrate that using the
cloud path in conjunction with a detour path can mask most of the cloud losses, resulting
in up to five 9's of network availability for applications.},
  doi       = {10.1145/3038912.3052560},
  isbn      = {9781450349130},
  keywords  = {inter-data center networks, loss rate, detour routing, cloud paths reliability, cloud availability, latency, bandwidth},
  location  = {Perth, Australia},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3038912.3052560},
}

@InProceedings{MartinezOrtiz2019,
  author    = {Martinez-Ortiz, Andres-Leonardo and Lizcano, David and Ortega, Miguel},
  booktitle = {Proceedings of the 14th International Workshop on Automation of Software Test},
  title     = {Software Metrics Artifacts Making Web Quality Measurable: AST 2019 Invited Paper},
  year      = {2019},
  pages     = {1–6},
  publisher = {IEEE Press},
  series    = {AST '19},
  abstract  = {Mining open source repositories introduces an effective approach to put in practice
empirical software engineering in a variety of technologies. Kernel development (Linux)
first and then Internet (Chromium) and more recently cloud orchestration (Kubernetes)
and machine learning (TensorFlow) are fundamental pieces not just for open source
ecosystem but also for the industry leading software innovation. Empirical software
engineering sustains a better understanding of these projects, reducing even more
the barriers for adoption. In this work we focus on empirical quality assessment developing
software metrics artifacts to make web components quality measurable. After reviewing
the state of the art and main frameworks for software measurement, we will present
our proposal for the empirical evaluation of quality metrics for web components, data
collection, measurement and prediction, discussing main benefits and some drawback
of the selected approach, which will be aimed at future works.},
  doi       = {10.1109/AST.2019.000-2},
  keywords  = {web technologies, open source, quality metrics, software engineering},
  location  = {Montreal, Quebec, Canada},
  numpages  = {6},
  url       = {https://doi.org/10.1109/AST.2019.000-2},
}

@InProceedings{Silva2018,
  author    = {Silva, Gabriel Costa and R\'{e}, Reginaldo and Silva, Marco Aur\'{e}lio Graciotto},
  booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
  title     = {Evaluating Efficiency, Effectiveness and Satisfaction of AWS and Azure from the Perspective of Cloud Beginners},
  year      = {2018},
  address   = {USA},
  pages     = {114–125},
  publisher = {IBM Corp.},
  series    = {CASCON '18},
  abstract  = {Quality has long been regarded as an important driver of cloud adoption. In particular,
quality in use (QiU) of cloud platforms may drive cloud beginners to the cloud platform
that offers the best cloud experience. Cloud beginners are critical to the cloud market
because they currently represent nearly a third of cloud users. We carried out three
experiments to measure the QiU (dependent variable) of public cloud platforms (independent
variable) regarding efficiency, effectiveness and satisfaction. AWS EC2 and Azure
Virtual Machines are the two cloud services used as representative proxies to evaluate
cloud platforms (treatments). Eleven undergraduate students with limited cloud knowledge
(participants) manually created 152 VMs (task) using the web interface of cloud platforms
(instrument) following seven different configurations (trials) for each cloud platform.
Whereas AWS performed significantly better than Azure for efficiency (p-value not
exceeding 0.001, A-statistic = 0.68), we could not find a significant difference between
platforms for effectiveness (p-value exceeding 0.05) - although the effect size was
found relevant (odds ratio = 0.41). Regarding satisfaction, most of our participants
perceived the AWS as (i) having the best GUI to benefiting user interaction, (ii)
the easiest platform to use, and (iii) the preferred cloud platform for creating VMs.
Once confirmed by independent replications, our results suggest that AWS outperforms
Azure regarding QiU. Therefore, cloud beginners might have a better cloud experience
starting off their cloud projects by using AWS rather than Azure. In addition, our
results may help to explain the AWS's cloud leadership.},
  keywords  = {experimentation, cloud platforms, quality in use},
  location  = {Markham, Ontario, Canada},
  numpages  = {12},
}

@InProceedings{Munoz2019,
  author    = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
  booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
  title     = {HADAS: Analysing Quality Attributes of Software Configurations},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {13–16},
  publisher = {Association for Computing Machinery},
  series    = {SPLC '19},
  abstract  = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses
of SPLs rely on solvers to navigate complex dependencies among features and find legal
solutions. Variability analysis tools are complex due to the diversity of products
and domain-specific knowledge. On that, while there are experimental studies that
analyse quality attributes, the knowledge is not easily accessible for developers,
and its appliance is not trivial. Aiming to allow the industry to quality-explore
SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects
quality attributes metrics in a cloud repository, and (2) reasons about it helping
developers with quality attributes requirements.},
  doi       = {10.1145/3307630.3342385},
  isbn      = {9781450366687},
  keywords  = {variability, numerical, attribute, software product line, model, NFQA},
  location  = {Paris, France},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3307630.3342385},
}

@InProceedings{Weber2014,
  author    = {Weber, Andreas and Herbst, Nikolas and Groenda, Henning and Kounev, Samuel},
  booktitle = {Proceedings of the 2nd International Workshop on Hot Topics in Cloud Service Scalability},
  title     = {Towards a Resource Elasticity Benchmark for Cloud Environments},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {HotTopiCS '14},
  abstract  = {Auto-scaling features offered by today's cloud infrastructures provide increased flexibility
especially for customers that experience high variations in the load intensity over
time. However, auto-scaling features introduce new system quality attributes when
considering their accuracy, timing, and boundaries. Therefore, distinguishing between
different offerings has become a complex task, as it is not yet supported by reliable
metrics and measurement approaches. In this paper, we discuss shortcomings of existing
approaches for measuring and evaluating elastic behavior and propose a novel benchmark
methodology specifically designed for evaluating the elasticity aspects of modern
cloud platforms. The benchmark is based on open workloads with realistic load variation
profiles that are calibrated to induce identical resource demand variations independent
of the underlying hardware performance. Furthermore, we propose new metrics that capture
the accuracy of resource allocations and de-allocations, as well as the timing aspects
of an auto-scaling mechanism explicitly.},
  articleno = {5},
  doi       = {10.1145/2649563.2649571},
  isbn      = {9781450330596},
  keywords  = {Resource, Elasticity, Supply, Demand, Load Profile},
  location  = {Dublin, Ireland},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2649563.2649571},
}

@InProceedings{AliEldin2014,
  author    = {Ali-Eldin, Ahmed and Seleznjev, Oleg and Sj\"{o}stedt-de Luna, Sara and Tordsson, Johan and Elmroth, Erik},
  booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
  title     = {Measuring Cloud Workload Burstiness},
  year      = {2014},
  address   = {USA},
  pages     = {566–572},
  publisher = {IEEE Computer Society},
  series    = {UCC '14},
  abstract  = {Workload burstiness and spikes are among the main reasons for service disruptions
and decrease in the Quality-of-Service (QoS) of online services. They are hurdles
that complicate autonomic resource management of data enters. In this paper, we review
the state-of-the-art in online identification of workload spikes and quantifying burstiness.
The applicability of some of the proposed techniques is examined for Cloud systems
where various workloads are co-hosted on the same platform. We discuss Sample Entropy
(Samp En), a measure used in biomedical signal analysis, as a potential measure for
burstiness. A modification to the original measure is introduced to make it more suitable
for Cloud workloads.},
  doi       = {10.1109/UCC.2014.87},
  isbn      = {9781479978816},
  numpages  = {7},
  url       = {https://doi.org/10.1109/UCC.2014.87},
}

@InProceedings{Chhetri2016,
  author    = {Chhetri, Mohan Baruwal and Vo, Quoc Bao and Kowalczyk, Ryszard},
  booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
  title     = {CL-SLAM: Cross-Layer SLA Monitoring Framework for Cloud Service-Based Applications},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {30–36},
  publisher = {Association for Computing Machinery},
  series    = {UCC '16},
  abstract  = {Modern applications are increasingly being composed from multiple components that
require and consume services at different layers of the cloud stack. The diverse,
dynamic and unpredictable nature of both cloud services and application workloads
makes quality-assured provision of such cloud service-based applications (CSBAs) a
major challenge. While elasticity and autoscaling gives CSBA providers the ability
to scale cloud resources on-demand, they require a comprehensive, system-wide view
of the application performance in order to make timely, cost-effective and performance-efficient
scaling decisions. In this paper, we propose, develop and validate CL-SLAM - a Cross-Layer
SLA Monitoring Framework for CSBAs. Its main features include (a) realtime, fine-grained
visibility into CSBA performance, (b) visual descriptive analytics to identify correlations
and inter-dependencies between cross-layer performance metrics, (c) temporal profiling
of CSBA performance, (d) proactive monitoring, detection and root-cause analysis of
SLA violation, and (e) support for both reactive and proactive adaptation in support
of quality-assured CSBA provision. We validate our approach through a prototype implementation.},
  doi       = {10.1145/2996890.2996906},
  isbn      = {9781450346160},
  keywords  = {cross-layer SLA monitoring, cloud service-based application},
  location  = {Shanghai, China},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2996890.2996906},
}

@InProceedings{Oprescu2015,
  author    = {Oprescu, Ana-Maria and (Vintila) Filip, Alexandra and Kielmann, Thilo},
  booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
  title     = {Fast Pareto Front Approximation for Cloud Instance Pool Optimization},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {1443–1444},
  publisher = {Association for Computing Machinery},
  series    = {GECCO Companion '15},
  abstract  = {Computing the Pareto Set (PS) of optimal cloud schedules in terms of cost and makespan
for a given application and set of cloud instance types is NP-complete. Moreover,
cloud instances' volatility requires fast PS recomputations. While genetic algorithms
(GA) are a promising approach, little knowledge of an approximated PS's quality leads
to GAs running for overly many generations, contradicting the goal of quickly computing
an approximate solution. We address this with MOO-GA, our GA enhanced with a domain-tailored
termination criteria delivering fast, well-approximated Pareto sets. We compare to
NSGAIII using PS convergence and diversity, and computational effort metrics. Results
show MOO-GA consistently computing better quality Pareto sets within one second on
average (df=98, p-value&lt;10-3).},
  doi       = {10.1145/2739482.2764720},
  isbn      = {9781450334884},
  keywords  = {pareto frontier, genetic algorithms, infrastructure-as-a-service},
  location  = {Madrid, Spain},
  numpages  = {2},
  url       = {https://doi.org/10.1145/2739482.2764720},
}

@InProceedings{Shatnawi2018,
  author    = {Shatnawi, Anas and Orr\`{u}, Matteo and Mobilio, Marco and Riganelli, Oliviero and Mariani, Leonardo},
  booktitle = {Proceedings of the 1st International Workshop on Software Health},
  title     = {Cloudhealth: A Model-Driven Approach to Watch the Health of Cloud Services},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {40–47},
  publisher = {Association for Computing Machinery},
  series    = {SoHeal '18},
  abstract  = {Cloud systems are complex and large systems where services provided by different operators
must coexist and eventually cooperate. In such a complex environment, controlling
the health of both the whole environment and the individual services is extremely
important to timely and effectively react to misbehaviours, unexpected events, and
failures. Although there are solutions to monitor cloud systems at different granularity
levels, how to relate the many KPIs that can be collected about the health of the
system and how health information can be properly reported to operators are open questions.This
paper reports the early results we achieved in the challenge of monitoring the health
of cloud systems. In particular we present CloudHealth, a model-based health monitoring
approach that can be used by operators to watch specific quality attributes. The Cloud-Health
Monitoring Model describes how to operationalize high level monitoring goals by dividing
them into subgoals, deriving metrics for the subgoals, and using probes to collect
the metrics. We use the CloudHealth Monitoring Model to control the probes that must
be deployed on the target system, the KPIs that are dynamically collected, and the
visualization of the data in dashboards.},
  doi       = {10.1145/3194124.3194130},
  isbn      = {9781450357302},
  keywords  = {quality model, software health, monitoring, cloud service, monitoring model, metrics, KPI},
  location  = {Gothenburg, Sweden},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3194124.3194130},
}

@InProceedings{Adegboyega2015,
  author    = {Adegboyega, Abiola},
  booktitle = {Proceedings of the International Workshop on Virtualization Technologies},
  title     = {An Adaptive Resource Provisioning Scheme for Effective QoS Maintenance in the IaaS Cloud},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {VT15},
  abstract  = {Effective bandwidth provisioning is of vital importance in the virtualized cloud where
tenants with unique SLAs share a finite network. Different tenants collocated on the
same physical server deployed with increasing VM density necessitates Quality of Service
(QoS) provisioning beginning at the hypervisor. Recent efforts at provisioning the
cloud network through various reservation methodologies have achieved some measure
of success. However most of them do not account for the entire path over which application
components communicate and cannot provide the necessary Service Level Agreement (SLA).
Cloud applications components often communicate across multiple network devices aggregated
into layers connected over finite bandwidth links that affect application response.
Furthermore, traffic to and from tenant applications display volatility. In view of
this, we design a virtual network reservation framework that is mindful of application
performance across multiple network devices &amp; traffic volatility. Our network reservation
framework is based on a forecasting engine motivated by the volatility existent in
traffic to and from virtualized cloud environments. This forecasting engine is able
to maintain SLAs by employing dynamic time-series models to develop novel bandwidth
provisioning thresholds that adapt to the time-variation in tenant workloads. We test
the effectiveness of our methods in the OpenStack cloud environment focusing on traffic
directionality in the datacenter network, VM density and QoS across multiple flows
competing for finite bandwidth. Our forecasting method offers a 25% improvement in
prediction accuracy over existing methods while the reservation framework maintains
SLAs at 95%.},
  articleno = {2},
  doi       = {10.1145/2835075.2835078},
  isbn      = {9781450337328},
  keywords  = {Virtualization, QoS, Forecasting, SDN, Volatility},
  location  = {Vancouver, BC, Canada},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2835075.2835078},
}

@InProceedings{Zhao2017a,
  author    = {Zhao, Yang and Xia, Nai and Tian, Chen and Li, Bo and Tang, Yizhou and Wang, Yi and Zhang, Gong and Li, Rui and Liu, Alex X.},
  booktitle = {Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems},
  title     = {Performance of Container Networking Technologies},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {1–6},
  publisher = {Association for Computing Machinery},
  series    = {HotConNet '17},
  abstract  = {Container networking is now an important part of cloud virtualization architectures.
It provides network access for containers by connecting both virtual and physical
network interfaces. The performance of container networking has multiple dependencies,
and each factor may significantly affect the performance. In this paper, we perform
systematic experiments to study the performance of container networking technologies.
For every measurement result, we try our best to qualify influencing factors.},
  doi       = {10.1145/3094405.3094406},
  isbn      = {9781450350587},
  keywords  = {Container, Measurement, Networking},
  location  = {Los Angeles, CA, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3094405.3094406},
}

@InProceedings{Herbst2015,
  author    = {Herbst, Nikolas Roman and Kounev, Samuel and Weber, Andreas and Groenda, Henning},
  booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
  title     = {BUNGEE: An Elasticity Benchmark for Self-Adaptive IaaS Cloud Environments},
  year      = {2015},
  pages     = {46–56},
  publisher = {IEEE Press},
  series    = {SEAMS '15},
  abstract  = {Today's infrastructure clouds provide resource elasticity (i.e. auto-scaling) mechanisms
enabling self-adaptive resource provisioning to reflect variations in the load intensity
over time. These mechanisms impact on the application performance, however, their
effect in specific situations is hard to quantify and compare. To evaluate the quality
of elasticity mechanisms provided by different platforms and configurations, respective
metrics and benchmarks are required. Existing metrics for elasticity only consider
the time required to provision and deprovision resources or the costs impact of adaptations.
Existing benchmarks lack the capability to handle open workloads with realistic load
intensity profiles and do not explicitly distinguish between the performance exhibited
by the provisioned underlying resources, on the one hand, and the quality of the elasticity
mechanisms themselves, on the other hand.In this paper, we propose reliable metrics
for quantifying the timing aspects and accuracy of elasticity. Based on these metrics,
we propose a novel approach for benchmarking the elasticity of Infrastructure-as-a-Service
(IaaS) cloud platforms independent of the performance exhibited by the provisioned
underlying resources. We show that the proposed metrics provide consistent ranking
of elastic platforms on an ordinal scale. Finally, we present an extensive case study
of real-world complexity demonstrating that the proposed approach is applicable in
realistic scenarios and can cope with different levels of resource efficiency.},
  location  = {Florence, Italy},
  numpages  = {11},
}

@Article{Moghaddam2019,
  author     = {Moghaddam, Sara Kardani and Buyya, Rajkumar and Ramamohanarao, Kotagiri},
  journal    = {ACM Comput. Surv.},
  title      = {Performance-Aware Management of Cloud Resources: A Taxonomy and Future Directions},
  year       = {2019},
  issn       = {0360-0300},
  month      = aug,
  number     = {4},
  volume     = {52},
  abstract   = {The dynamic nature of the cloud environment has made the distributed resource management
process a challenge for cloud service providers. The importance of maintaining quality
of service in accordance with customer expectations and the highly dynamic nature
of cloud-hosted applications add new levels of complexity to the process. Advances
in big-data learning approaches have shifted conventional static capacity planning
solutions to complex performance-aware resource management methods. It is shown that
the process of decision-making for resource adjustment is closely related to the behavior
of the system, including the utilization of resources and application components.
Therefore, a continuous monitoring of system attributes and performance metrics provides
the raw data for the analysis of problems affecting the performance of the application.
Data analytic methods, such as statistical and machine-learning approaches, offer
the required concepts, models, and tools to dig into the data and find general rules,
patterns, and characteristics that define the functionality of the system. Obtained
knowledge from the data analysis process helps to determine the changes in the workloads,
faulty components, or problems that can cause system performance to degrade. A timely
reaction to performance degradation can avoid violations of service level agreements,
including performing proper corrective actions such as auto-scaling or other resource
adjustment solutions. In this article, we investigate the main requirements and limitations
of cloud resource management, including a study of the approaches to workload and
anomaly analysis in the context of performance management in the cloud. A taxonomy
of the works on this problem is presented that identifies main approaches in existing
research from the data analysis side to resource adjustment techniques. Finally, considering
the observed gaps in the general direction of the reviewed works, a list of these
gaps is proposed for future researchers to pursue.},
  address    = {New York, NY, USA},
  articleno  = {84},
  doi        = {10.1145/3337956},
  issue_date = {September 2019},
  keywords   = {Anomaly detection, resource management, big-data analytics, performance management},
  numpages   = {37},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3337956},
}

@InProceedings{Zertal2017,
  author    = {Zertal, Soumia and Batouche, Mohamed Chawki},
  booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
  title     = {A Hybrid Approach for Optimized Composition of Cloud Services},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {BDCA'17},
  abstract  = {The increasing use of Cloud services as well as the increasing demands of complex
cloud services creates the need for a dynamic and adaptive composition of services,
in a decentralized and large scale environment, where the quality of services may
increase or decrease. Early attempts for dynamic composition of services have been
proposed. But they are limited by their ability to adapt when deploying in highly
dynamic and open environments. For better performance measurements, we use, in this
paper, the Particle Swarm Optimization (PSO) algorithm to find and provide the services
that meets the user's query. To assess the utility of each service, we take into consideration
its values of service quality provided in the past. The latter is represented by the
mechanism of stigmergy which uses the pheromone as a means of communication between
services.},
  articleno = {12},
  doi       = {10.1145/3090354.3090366},
  isbn      = {9781450348522},
  keywords  = {Stigmegy, Optimization, Particle Swarm Optimization, Service Composition, Cloud Computing},
  location  = {Tetouan, Morocco},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3090354.3090366},
}

@InProceedings{Shekhar2017,
  author    = {Shekhar, Shashank and Gokhale, Aniruddha},
  booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
  title     = {Dynamic Resource Management Across Cloud-Edge Resources for Performance-Sensitive Applications},
  year      = {2017},
  pages     = {707–710},
  publisher = {IEEE Press},
  series    = {CCGrid '17},
  abstract  = {A large number of modern applications and systems are cloud-hosted, however, limitations
in performance assurances from the cloud, and the longer and often unpredictable end-to-end
network latencies between the end user and the cloud can be detrimental to the response
time requirements of the applications, specifically those that have stringent Quality
of Service (QoS) requirements. Although edge resources, such as cloudlets, may alleviate
some of the latency concerns, there is a general lack of mechanisms that can dynamically
manage resources across the cloud-edge spectrum. To address these gaps, this research
proposes Dynamic Data Driven Cloud and Edge Systems (D3CES). It uses measurement data
collected from adaptively instrumenting the cloud and edge resources to learn and
enhance models of the distributed resource pool. In turn, the framework uses the learned
models in a feedback loop to make effective resource management decisions to host
applications and deliver their QoS properties. D3CES is being evaluated in the context
of a variety of cyber physical systems, such as smart city, online games, and augmented
reality applications.},
  doi       = {10.1109/CCGRID.2017.120},
  isbn      = {9781509066100},
  keywords  = {CPS, Edge Computing, IoT, DDDAS, Cloud Computing, Fog Computing, Resource Management},
  location  = {Madrid, Spain},
  numpages  = {4},
  url       = {https://doi.org/10.1109/CCGRID.2017.120},
}

@InProceedings{Imai2018,
  author    = {Imai, Shigeru and Patterson, Stacy and Varela, Carlos A.},
  booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
  title     = {Uncertainty-Aware Elastic Virtual Machine Scheduling for Stream Processing Systems},
  year      = {2018},
  pages     = {62–71},
  publisher = {IEEE Press},
  series    = {CCGrid '18},
  abstract  = {Stream processing systems deployed on the cloud need to be elastic to effectively
accommodate workload variations over time. Performance models can predict maximum
sustainable throughput (MST) as a function of the number of VMs allocated. We present
a scheduling framework that incorporates three statistical techniques to improve Quality
of Service (QoS) of cloud stream processing systems: (i) uncertainty quantification
to consider variance in the MST model; (ii) online learning to update MST model as
new performance metrics are gathered; and (iii) workload models to predict input data
stream rates assuming regular patterns occur over time. Our framework can be parameterized
by a QoS satisfaction target that statistically finds the best performance/cost tradeoff.
Our results illustrate that each of the three techniques alone significantly improves
QoS, from 52% to 73-81% QoS satisfaction rates on average for eight benchmark applications.
Furthermore, applying all three techniques allows us to reach 98.62% QoS satisfaction
rate with a cost less than twice the cost of the optimal (in hindsight) VM allocations,
and half of the cost of allocating VMs for the peak demand in the workload.},
  doi       = {10.1109/CCGRID.2018.00021},
  isbn      = {9781538658154},
  location  = {Washington, District of Columbia},
  numpages  = {10},
  url       = {https://doi.org/10.1109/CCGRID.2018.00021},
}

@InProceedings{Kuhlenkamp2019,
  author    = {Kuhlenkamp, J\"{o}rn and Werner, Sebastian and Borges, Maria C. and El Tal, Karim and Tai, Stefan},
  booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
  title     = {An Evaluation of FaaS Platforms as a Foundation for Serverless Big Data Processing},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {1–9},
  publisher = {Association for Computing Machinery},
  series    = {UCC'19},
  abstract  = {Function-as-a-Service (FaaS), offers a new alternative to operate cloud-based applications.
FaaS platforms enable developers to define their application only through a set of
service functions, relieving them of infrastructure management tasks, which are executed
automatically by the platform. Since its introduction, FaaS has grown to support workloads
beyond the lightweight use-cases it was originally intended for, and now serves as
a viable paradigm for big data processing. However, several questions regarding FaaS
platform quality are still unanswered. Specifically, the impact of automatic infrastructure
management on serverless big data applications remains unexplored.In this paper, we
propose a novel evaluation method (SIEM) to understand the impact of these tasks.
For this purpose, we introduce new metrics to quantify quality in different big data
application scenarios. We show an application of SIEM by evaluating the four major
FaaS providers, and contribute results and new insights for FaaS-based big data processing.},
  doi       = {10.1145/3344341.3368796},
  isbn      = {9781450368940},
  keywords  = {serverless, benchmarking, big data processing, cloud computing},
  location  = {Auckland, New Zealand},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3344341.3368796},
}

@InProceedings{Byholm2014,
  author    = {Byholm, Benjamin and Porres, Iv\'{a}n},
  booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  title     = {Cost-Efficient, Reliable, Utility-Based Session Management in the Cloud},
  year      = {2014},
  pages     = {102–111},
  publisher = {IEEE Press},
  series    = {CCGRID '14},
  abstract  = {We present a model and system for cost-efficient and reliable management of sessions
in a Cloud, based on the von Neumann-Morgenstern utility theorem. Our model enables
a web application provider to maximize profit while maintaining a desired quality
of service. The objective is to determine whether, when, where, and how long to store
a session, given multiple storage options with various properties, e.g. cost, capacity,
and reliability. Reliability is affected by three factors: how often session state
is stored, how many stores are used, and how reliable those stores are. To account
for these factors, we use a Markovian reliability model and treat the valid storage
options for each session as a von Neumann-Morgenstern lottery. We proceed by representing
the resulting problem as a knapsack problem, which can be heuristically solved for
a good compromise between efficiency and effectiveness. We analyze the results from
a discrete-event simulation involving multiple session management policies, including
two utility-based policies: a greedy heuristic policy intended to give real-time performance
and a reference policy based on solving the linear programming relaxation of the knapsack
problem, giving a theoretical upper bound on achievable utility. As the focus of this
work is exploratory, rather than performance-based, we do not directly measure the
time required for solving the model. Instead, we give the computational complexity
of the algorithms. Our results indicate that otherwise unprofitable services become
profitable through utility-based session management in a cloud setting. However, if
the costs are much lower than the expected revenues, all policies manage to turn a
profit. Different policies performed the best under different circumstances.},
  doi       = {10.1109/CCGrid.2014.22},
  isbn      = {9781479927838},
  keywords  = {analytical models, and serviceability, availability, utility theory, simulation, web-based services, distributed systems, markov processes, reliability},
  location  = {Chicago, Illinois},
  numpages  = {10},
  url       = {https://doi.org/10.1109/CCGrid.2014.22},
}

@InProceedings{Cano2016,
  author    = {Cano, Ignacio and Aiyar, Srinivas and Krishnamurthy, Arvind},
  booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
  title     = {Characterizing Private Clouds: A Large-Scale Empirical Analysis of Enterprise Clusters},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {29–41},
  publisher = {Association for Computing Machinery},
  series    = {SoCC '16},
  abstract  = {There is an increasing trend in the use of on-premise clusters within companies. Security,
regulatory constraints, and enhanced service quality push organizations to work in
these so called private cloud environments. On the other hand, the deployment of private
enterprise clusters requires careful consideration of what will be necessary or may
happen in the future, both in terms of compute demands and failures, as they lack
the public cloud's flexibility to immediately provision new nodes in case of demand
spikes or node failures.In order to better understand the challenges and tradeoffs
of operating in private settings, we perform, to the best of our knowledge, the first
extensive characterization of on-premise clusters. Specifically, we analyze data ranging
from hardware failures to typical compute/storage requirements and workload profiles,
from a large number of Nutanix clusters deployed at various companies.We show that
private cloud hardware failure rates are lower, and that load/demand needs are more
predictable than in other settings. Finally, we demonstrate the value of the measurements
by using them to provide an analytical model for computing durability in private clouds,
as well as a machine learning-driven approach for characterizing private clouds' growth.},
  doi       = {10.1145/2987550.2987584},
  isbn      = {9781450345255},
  keywords  = {Private clouds, Measurements, Reliability, Performance},
  location  = {Santa Clara, CA, USA},
  numpages  = {13},
  url       = {https://doi.org/10.1145/2987550.2987584},
}

@InProceedings{Santos2020,
  author    = {Santos, Guilherme and Paulino, Herv\'{e} and Vardasca, Tom\'{e}},
  booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
  title     = {QoE-Aware Auto-Scaling of Heterogeneous Containerized Services (and Its Application to Health Services)},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {242–249},
  publisher = {Association for Computing Machinery},
  series    = {SAC '20},
  abstract  = {Containerized service is currently a widely adopted solution to deploy services in
the cloud. However, many companies offer a very diverse set of Web accessible services
that are subjected to very distinctive workloads. Consequently, to correctly provision
the right amount of resources for each of these services is a challenge. In this paper
we propose the Autonomic ConTainerized Service Scaler (ACTS), an autonomic system
able to horizontally and vertically scale a set of heterogeneous containerized services
subjected to different workloads. The adaptation decisions depended on a set of high-level
Quality of Experience (QoE) metrics centered on the services' end-user. We have applied
ACTS to some of the digital services of the Shared Services of the Ministry of Health
(SPMS) public company. The experimental results show that our solution is able to
adequately adapt the configuration of each service, as a direct response to alterations
on its workload.},
  doi       = {10.1145/3341105.3373915},
  isbn      = {9781450368667},
  keywords  = {quality of experience, health care, auto-scaling, containers},
  location  = {Brno, Czech Republic},
  numpages  = {8},
  url       = {https://doi.org/10.1145/3341105.3373915},
}

@InProceedings{Moreno2017,
  author    = {Moreno, Gabriel A. and Papadopoulos, Alessandro V. and Angelopoulos, Konstantinos and C\'{a}mara, Javier and Schmerl, Bradley},
  booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
  title     = {Comparing Model-Based Predictive Approaches to Self-Adaptation: CobRA and PLA},
  year      = {2017},
  pages     = {42–53},
  publisher = {IEEE Press},
  series    = {SEAMS '17},
  abstract  = {Modern software-intensive systems must often guarantee certain quality requirements
under changing run-time conditions and high levels of uncertainty. Self-adaptation
has proven to be an effective way to engineer systems that can address such challenges,
but many of these approaches are purely reactive and adapt only after a failure has
taken place. To overcome some of the limitations of reactive approaches (e.g., lagging
behind environment changes and favoring short-term improvements), recent proactive
self-adaptation mechanisms apply ideas from control theory, such as model predictive
control (MPC), to improve adaptation. When selecting which MPC approach to apply,
the improvement that can be obtained with each approach is scenario-dependent, and
so guidance is needed to better understand how to choose an approach for a given situation.
In this paper, we compare CobRA and PLA, two approaches that are inspired by MPC.
CobRA is a requirements-based approach that applies control theory, whereas PLA is
architecture-based and applies stochastic analysis. We compare the two approaches
applied to RUBiS, a benchmark system for web and cloud application performance, discussing
the required expertise needed to use both approaches and comparing their run-time
performance with respect to different metrics.},
  doi       = {10.1109/SEAMS.2017.2},
  isbn      = {9781538615508},
  keywords  = {self-adaptation, adaptive system, model predictive control, CobRA, latency, PLA},
  location  = {Buenos Aires, Argentina},
  numpages  = {12},
  url       = {https://doi.org/10.1109/SEAMS.2017.2},
}

@InProceedings{Widder2018,
  author    = {Widder, David Gray and Hilton, Michael and K\"{a}stner, Christian and Vasilescu, Bogdan},
  booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
  title     = {I'm Leaving You, Travis: A Continuous Integration Breakup Story},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {165–169},
  publisher = {Association for Computing Machinery},
  series    = {MSR '18},
  abstract  = {Continuous Integration (CI) services, which can automatically build, test, and deploy
software projects, are an invaluable asset in distributed teams, increasing productivity
and helping to maintain code quality. Prior work has shown that CI pipelines can be
sophisticated, and choosing and configuring a CI system involves tradeoffs. As CI
technology matures, new CI tool offerings arise to meet the distinct wants and needs
of software teams, as they negotiate a path through these tradeoffs, depending on
their context. In this paper, we begin to uncover these nuances, and tell the story
of open-source projects falling out of love with Travis, the earliest and most popular
cloud-based CI system. Using logistic regression, we quantify the effects that open-source
community factors and project technical factors have on the rate of Travis abandonment.
We find that increased build complexity reduces the chances of abandonment, that larger
projects abandon at higher rates, and that a project's dominant language has significant
but varying effects. Finally, we find the surprising result that metrics of configuration
attempts and knowledge dispersion in the project do not affect the rate of abandonment.},
  doi       = {10.1145/3196398.3196422},
  isbn      = {9781450357166},
  location  = {Gothenburg, Sweden},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3196398.3196422},
}

@InProceedings{PerezPalacin2015,
  author    = {Perez-Palacin, Diego and Mirandola, Raffaela and Monterisi, Federico and Montoli, Andrea},
  booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
  title     = {QoS-Driven Probabilistic Runtime Evaluations of Virtual Machine Placement on Hosts},
  year      = {2015},
  pages     = {90–94},
  publisher = {IEEE Press},
  series    = {UCC '15},
  abstract  = {We tackle the cloud providers challenge of virtual machine placement when the client
experienced Quality of Service (QoS) is of paramount importance and resource demand
of virtual machines varies over time. To this end, this work investigates approaches
that leverage measured dynamic data for placement decisions. Relying on dynamic data
to guide decisions has, on the one hand, the potential to optimize hardware utilization,
while, on the other hand, increases the risk on the provided QoS. In this context,
we present three probabilistic methods for evaluation of host suitability to allocate
new virtual machines. We also present experiments results that illustrate the differences
in the outcomes of presented approaches.},
  isbn      = {9780769556970},
  location  = {Limassol, Cyprus},
  numpages  = {5},
}

@Article{Papadopoulos2016,
  author     = {Papadopoulos, Alessandro Vittorio and Ali-Eldin, Ahmed and \r{A}rz\'{e}n, Karl-Erik and Tordsson, Johan and Elmroth, Erik},
  journal    = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
  title      = {PEAS: A Performance Evaluation Framework for Auto-Scaling Strategies in Cloud Applications},
  year       = {2016},
  issn       = {2376-3639},
  month      = aug,
  number     = {4},
  volume     = {1},
  abstract   = {Numerous auto-scaling strategies have been proposed in the past few years for improving
various Quality of Service (QoS) indicators of cloud applications, for example, response
time and throughput, by adapting the amount of resources assigned to the application
to meet the workload demand. However, the evaluation of a proposed auto-scaler is
usually achieved through experiments under specific conditions and seldom includes
extensive testing to account for uncertainties in the workloads and unexpected behaviors
of the system. These tests by no means can provide guarantees about the behavior of
the system in general conditions. In this article, we present a Performance Evaluation
framework for Auto-Scaling (PEAS) strategies in the presence of uncertainties. The
evaluation is formulated as a chance constrained optimization problem, which is solved
using scenario theory. The adoption of such a technique allows one to give probabilistic
guarantees of the obtainable performance. Six different auto-scaling strategies have
been selected from the literature for extensive test evaluation and compared using
the proposed framework. We build a discrete event simulator and parameterize it based
on real experiments. Using the simulator, each auto-scaler’s performance is evaluated
using 796 distinct real workload traces from projects hosted on the Wikimedia foundations’
servers, and their performance is compared using PEAS. The evaluation is carried out
using different performance metrics, highlighting the flexibility of the framework,
while providing probabilistic bounds on the evaluation and the performance of the
algorithms. Our results highlight the problem of generalizing the conclusions of the
original published studies and show that based on the evaluation criteria, a controller
can be shown to be better than other controllers.},
  address    = {New York, NY, USA},
  articleno  = {15},
  doi        = {10.1145/2930659},
  issue_date = {September 2016},
  keywords   = {Performance evaluation, elasticity, cloud computing, randomized optimization, auto-scaling},
  numpages   = {31},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2930659},
}

@InProceedings{Klein2015a,
  author    = {Klein, John and Gorton, Ian},
  booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
  title     = {Runtime Performance Challenges in Big Data Systems},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {17–22},
  publisher = {Association for Computing Machinery},
  series    = {WOSP '15},
  abstract  = {Big data systems are becoming pervasive. They are distributed systems that include
redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient
to assure runtime performance in production. This is due to the scale of the deployed
system, the continually evolving workloads, and the unpredictable quality of service
of the shared infrastructure. Consequently, a solution for addressing performance
requirements needs sophisticated runtime observability and measurement. Observability
gives real-time insights into a system's health and status, both at the system and
application level, and provides historical data repositories for forensic analysis,
capacity planning, and predictive analytics. Due to the scale and heterogeneity of
big data systems, significant challenges exist in the design, customization and operations
of observability capabilities. These challenges include economical creation and insertion
of monitors into hundreds or thousands of computation and data nodes, efficient, low
overhead collection and storage of measurements (which is itself a big data problem),
and application-aware aggregation and visualization. In this paper we propose a reference
architecture to address these challenges, which uses a model-driven engineering toolkit
to generate architecture-aware monitors and application-specific visualizations.},
  doi       = {10.1145/2693561.2693563},
  isbn      = {9781450333405},
  keywords  = {observability, big data, model-driven engineering},
  location  = {Austin, Texas, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2693561.2693563},
}

@InProceedings{Raj2018,
  author    = {Raj, Vinay and Ravichandra, S.},
  booktitle = {2018 3rd IEEE International Conference on Recent Trends in Electronics, Information Communication Technology (RTEICT)},
  title     = {Microservices: A perfect SOA based solution for Enterprise Applications compared to Web Services},
  year      = {2018},
  month     = {May},
  pages     = {1531-1536},
  abstract  = {The Software Engineering community has defined different types of architectures to build applications. One among them is Service Oriented Architecture(SOA) which has created significant impact the way software applications are built. There are many implementations of SOA like Web Services, REST services etc. But Web Services and REST services do not fully follow all the principles of SOA. Microservices as an architectural style recently emerged from SOA by which we can develop business requirements with loosely coupled, self deploying and scalable services. Microservices have gained more popularity in application development as they are easy to understand, scale and deploy. In this paper we discuss principles of SOA, major drawbacks of web services and benefits of Microservices over SOA based web services. We have highlighted the importance of Microservices in software development. This paper gives information for architects as to why choose Microservices architecture over web services. We have also discussed metrics used for calculating Coupling between services and we evaluated by considering a smart payment application for ecommerce which is built using both the styles. We observed that Microservices architectural style has less coupling between services compared to Web Service style based on the metric values of the application.},
  doi       = {10.1109/RTEICT42901.2018.9012140},
  keywords  = {Service-oriented architecture;Couplings;Measurement;Computer architecture;Business;Service Oriented Architecture(SOA);Web Services;Microservices;Coupling;Metrics},
}

@InProceedings{Rosa2020,
  author    = {Rosa, Thatiane de Oliveira and Goldman, Alfredo and Guerra, Eduardo Martins},
  booktitle = {2020 IEEE International Conference on Software Architecture Companion (ICSA-C)},
  title     = {How ‘micro’ are your services?},
  year      = {2020},
  month     = {March},
  pages     = {75-78},
  abstract  = {Microservice is an architectural style that proposes that a complex system should be developed from small and independent services that work together. There is not a welldefined boundary about when a software architecture can be considered based on microservices or not. Because of that, defining microservices context and infrastructure is challenging, especially to characterize aspects related to microservice size, data consistency, and microservices coupling. Thus, it is crucial to understand the microservices-based software characteristics, to comprehend the impact of some evolutions on architecture, and evaluate how much a particular architecture fits the microservices architectural style. Therefore, based on bibliographic research and case studies conducted in academical and industrial environments, we aim to propose a model to characterize the architecture structure based on the main guidelines of the microservice architectural style. This model introduces dimensions that measure characteristics based on modules size, coupling to data sources, and service collaboration. This study should facilitate the mapping, measurement, and monitoring of different impacts generated in the software architecture from increments and refactoring performed. This work is on the initial development stage and as a result, we expected that the model supports architectural decisions that consider different quality attributes to achieve the right balance between service independence and collaboration for a given system.},
  doi       = {10.1109/ICSA-C50368.2020.00023},
  keywords  = {Measurement;Databases;Couplings;Complexity theory;Software;Computer architecture;Software architecture;software architecture;microservices;characterization model},
}

@InProceedings{Santos2020a,
  author    = {Santos, Nuno and Rito Silva, António},
  booktitle = {2020 IEEE International Conference on Software Architecture (ICSA)},
  title     = {A Complexity Metric for Microservices Architecture Migration},
  year      = {2020},
  month     = {March},
  pages     = {169-178},
  abstract  = {Monolith applications tend to be difficult to deploy, upgrade, maintain, and understand. Microservices, on the other hand, have the advantages of being independently developed, tested, deployed, scaled and, more importantly, easier to change and maintain. This paper addresses the problem of migrating a monolith to a microservices architecture. Therefore, we address two research questions: (1) Can we define the cost of decomposition in terms of the effort to redesign a functionality, which is implemented in the monolith as an ACID transaction, into several distributed transactions? (2) Considering several similarity measures between domain entities, which provide a better decomposition when they are compared using the proposed complexity metric? To answer the first research question, we propose a complexity metric, for each functionality of the monolith application, that measures the impact of relaxing the functionality consistency on the architecture redesign and implementation. Regarding the second research question, we experiment with four similarity measures, each based on a different type of information collected from monolith functionality implementation. We evaluated our approach with three monolith systems and compared our complexity metric against industry metrics of cohesion and coupling. We also evaluated the different similarity measures in terms of the complexity of the decomposition they produce. We were able to correctly correlate the complexity metric with other metrics of cohesion and coupling defined in other research and we conclude that no single combination of similarity measures outperforms the other, which is confirmed by the existing research. Additionally, we conclude that the approach can help on an incremental migration to microservices, which, actually, is the strategy proposed by the industry experts.},
  doi       = {10.1109/ICSA47634.2020.00024},
  keywords  = {Measurement;Complexity theory;Business;Computer architecture;Tools;Industries;Couplings;Monolith applications, Microservices, Complexity metrics, Architecture migration, Architecture evolution},
}

@InProceedings{Avritzer2020,
  author    = {Avritzer, Alberto},
  booktitle = {2020 IEEE International Conference on Software Architecture Companion (ICSA-C)},
  title     = {Challenges and Approaches for the Assessment of Micro-Service Architecture Deployment Alternatives in DevOps : A tutorial presented at ICSA 2020},
  year      = {2020},
  month     = {March},
  pages     = {1-2},
  abstract  = {The goal of this tutorial is to provide an overview of challenges and approaches for architecture/dependability assessment in the context of DevOps and microservices. Specifically, we present approaches that employ operational data obtained from production-level application performance management (APM) tools, giving access to operational workload profiles, architectural information, failure models, and security intrusions. We use this data to automatically create and conFigure architecture assessments based on models, load tests, and resilience benchmarks. The focus of this tutorial is on approaches that employ production usage, because these approaches provide more accurate recommendations for microservice architecture dependability assessment than approaches that do not consider production usage. We present an overview of (1) the state-of-the-art approaches for obtaining operational data from production systems using APM tools, (2) the challenges of dependability for DevOps and microservices, (3) selected approaches based on operational data to assess dependability. The architecture assessment focus of this tutorial is on scalability, resilience, survivability, and security. Particularly, we present a demo of the automated approach for the evaluation of a domain-based scalability and security metric assessment that is based on the microservice architecture ability to satisfy the performance requirement under load and/or intrusions. We illustrate the approach by presenting experimental results using a benchmark microservice architecture.},
  doi       = {10.1109/ICSA-C50368.2020.00007},
  keywords  = {Tutorials;Computer architecture;Scalability;Security;Tools;Data models;Production;micro-service architectures;operational profile;security intrusions},
}

@InProceedings{Honamore2016,
  author    = {Honamore, Suhas and Kumar, Lov and Rath, Santanu Ku.},
  booktitle = {2016 International Conference on Internet of Things and Applications (IOTA)},
  title     = {Analysis of control flow complexity metrics for web service composition},
  year      = {2016},
  month     = {Jan},
  pages     = {389-394},
  abstract  = {In service oriented computing, web services are combined to meet the interoperability demands in different heterogeneous and distributed applications. However, incisively measuring the control flow complexity of Web Service Composition (WSC) is not an easy task due to characteristics of distributed, loose-coupling, and heterogeneity. In Service Oriented Architecture (SOA), Business Process Execution Language (BPEL) is used to describe the combination of web services. This paper mainly focuses on the complexity measurement of web service composition from BPEL. Petri-net is one of the models to represent the work flow. The BPEL of WSC is converted into Petri-net based model and by extracting the information of places, transitions, and their interrelationship; the complexity is measured for that Petri-net model. Two metric sets are considered for analysis of the WSC's complexity, which are identified by studying the workflow's execution dependency relations. The first metric set describes the static features, and second metric set describes about the dynamic complexity of business process.},
  doi       = {10.1109/IOTA.2016.7562758},
  keywords  = {Complexity theory;Business;Service-oriented architecture;Atmospheric modeling;Weight measurement;Service oriented architecture;BPEL;WSC;Petri-net;Complexity metrics},
}

@InProceedings{Parekh2018,
  author    = {Parekh, Nikunj and Kurunji, Swathi and Beck, Alan},
  booktitle = {2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)},
  title     = {Monitoring Resources of Machine Learning Engine In Microservices Architecture},
  year      = {2018},
  month     = {Nov},
  pages     = {486-492},
  abstract  = {Microservices architecture facilitates building distributed scalable software products, usually deployed in a cloud environment. Monitoring microservices deployed in a Kubernetes orchestrated distributed advanced analytics machine learning engines is at the heart of many cloud resource management solutions. In addition, measuring resource utilization at more granular level such as per query or sub-query basis in an MPP Machine Learning Engine (MLE) is key to resource planning and is also the focus of our work. In this paper we propose two mechanisms to measure resource utilization in Teradata Machine Learning Engine (MLE). First mechanism is the Cluster Resource Monitoring (CRM). CRM is a high-level resource measuring mechanism for IT administrators and analytics users to visualize, plot, generates alerts and perform live and historical-analytics on overall cluster usage statistics. Second mechanism is the Query Resource Monitoring (QRM). QRM enables IT administrators and MLE users to measure compute resource utilization per individual query and its sub-queries. When query takes long time, QRM provides insights. This is useful to identify expensive phases within a query that tax certain resources more and skew the work distribution. We show the results of proposed mechanisms and highlight use-cases.},
  doi       = {10.1109/IEMCON.2018.8614791},
  keywords  = {Monitoring;Resource management;Engines;Containers;Machine learning;Maximum likelihood estimation;Customer relationship management;Big Data;Data Analytics;Machine Learning;Docker;Linux Containers (LXC);Massively Parallel Processing (MPP);MapReduce;Kubernetes;Monitoring;and Workload Skew},
}

@InProceedings{FernandesMiotodeOliveiradosSantos2019,
  author    = {Fernandes Mioto de Oliveira dos Santos, Eduardo and Lima Werner, Claudia Maria},
  booktitle = {2019 International Conference on Information Systems and Software Technologies (ICI2ST)},
  title     = {A Survey on Microservices Criticality Attributes on Established Architectures},
  year      = {2019},
  month     = {Nov},
  pages     = {149-155},
  abstract  = {The microservice oriented software architecture considers the delegation of responsibilities by separate components, thus creating a set of interconnected but independent services. Information about the most critical microservices is relevant to software architects and other decision-makers, thus guiding the maintenance and evolution of architecture in a more assertive and guided way. This paper aims to observe the need for a method to measure criticality in a microservice oriented architecture, motivated by this purpose, during August 2019, a survey with twenty experienced participants from the industry and academia was conducted, where the lack of a grounded method to measure the criticality on established architectures was observed.},
  doi       = {10.1109/ICI2ST.2019.00028},
  keywords  = {Computer architecture;ISO Standards;Atmospheric measurements;Particle measurements;Service-oriented architecture;Computational modeling;Microservices;Criticality;Attributes;Established Architectures;Survey},
}

@InProceedings{Chaudhari2016,
  author    = {Chaudhari, Nikhil and Bhadoria, Robin Singh and Prasad, Siddharth},
  booktitle = {2016 8th International Conference on Computational Intelligence and Communication Networks (CICN)},
  title     = {Information Handling and Processing Using Enterprise Service Bus in Service-Oriented Architecture System},
  year      = {2016},
  month     = {Dec},
  pages     = {418-421},
  abstract  = {Information is key factor in delivering service across networks. Messaging is important aspect in handing information using Enterprise Service Bus (ESB) in Service Oriented Architecture (SOA). Such information is generally passes and used as interaction parameters upon communication between two parties that could be carried out amongst multiple services. Integration between multiple application services could be strengthened by adopting this methodology which is important to handle web services over networks. ESB is messaging middleware framework that helps in designing and developing web services through which software intermediary could be possible. It is a kind of depletion layer that efficiently handles various overheads during communication and interaction between multiple application services. This paper details about issues related to application services with message handling and control. Testing and simulation has been carried out on - AdroitLogic UltraESB, WSO2 ESB and Red Hat JBoss Fuse ESBs. Several parameters like total and average message counts, overall bytes measure, overall message received and sent, processing time of messages, and memory allocation.},
  doi       = {10.1109/CICN.2016.88},
  issn      = {2472-7555},
  keywords  = {Service-oriented architecture;Time factors;Message systems;Business;Fuses;Concurrent computing;Service-Oriented Architecture (SOA);Enterprise Service Bus (ESB);Message Handling;Middleware},
}

@InProceedings{Pulparambil2016,
  author    = {Pulparambil, Supriya and Baghdadi, Youcef},
  booktitle = {2016 IEEE Students' Conference on Electrical, Electronics and Computer Science (SCEECS)},
  title     = {SOA maturity model a frame of reference},
  year      = {2016},
  month     = {March},
  pages     = {1-6},
  abstract  = {Service Oriented Architecture (SOA) is an architectural style that supports service orientation. In reality, SOA is much more than architecture. SOA adoption is prerequisite for organization to excel their service deliveries, as the delivery platforms are shifting to mobile, cloud and social media. A maturity model is a tool to accelerate enterprise SOA adoption, however it depends on how it should be applied. This paper presents a literature review of existing maturity models and proposes 5 major aspects that a maturity model has to address to improve SOA practices of an enterprise. A maturity model can be used as: (i) a roadmap for SOA adoption, (ii) a reference guide for SOA adoption, (iii) a tool to gauge maturity of process execution, (iv) a tool to measure the effectiveness of SOA motivations, and (v) a review tool for governance framework. This paper also sheds light on how SOA maturity assessment can be modeled. A model for SOA process execution maturity and perspective maturity assessment has been proposed along with a framework to include SOA scope of adoption.},
  doi       = {10.1109/SCEECS.2016.7509323},
  keywords  = {Service-oriented architecture;Semiconductor optical amplifiers;Capability maturity model;Organizations;Standards organizations;Industries;SOA maturity model;assessment;execution maturity;perspective maturity;scope;framework},
}

@InProceedings{Camilli2020,
  author    = {Camilli, Matteo and Colarusso, Carmine and Russo, Barbara and Zimeo, Eugenio},
  booktitle = {2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  title     = {Domain Metric Driven Decomposition of Data-Intensive Applications},
  year      = {2020},
  month     = {Oct},
  pages     = {189-196},
  abstract  = {The microservices architectural style is picking up more and more momentum in IT industry for the development of systems as loosely coupled, collaborating services. Companies that undergo the migration of their own applications have aspirations such as increasing maintainability and the scale of operation. Such a process is worthwhile but not easy, since it should ensure atomic improvements to the overall architecture for each migration step. Furthermore, the systematic evaluation of migration steps becomes cumbersome without sensible optimization metrics that take into account performance and scalability under expected operational conditions. Recent lines of research recognize this task as challenging, especially in data-intensive applications where known approaches based, for instance, on Domain Driven Design may not be adequate. In this paper, we introduce an approach to evaluate a migration in an iterative way and recognize whether it represents an improvement in terms of performance and scalability. The approach leverages a Domain Metric-based analysis to quantitatively evaluate alternative architectures. We exemplified the envisioned approach on a data-intensive application case study in the domain of smart mobility. Preliminary results from our controlled experiments show the effectiveness of our approach to support systematic and automated evaluation of migration processes.},
  doi       = {10.1109/ISSREW51248.2020.00071},
  keywords  = {Scalability;Testing;Measurement;Computer architecture;Roads;Business;Servers;Microservices;Decomposition;Performance Analysis;Scalability Analysis;Domain Metric},
}

@InProceedings{NikDaud2014,
  author    = {Nik Daud, Nik Marsyahariani and Wan Kadir, Wan M. N.},
  booktitle = {2014 8th. Malaysian Software Engineering Conference (MySEC)},
  title     = {Static and dynamic classifications for SOA structural attributes metrics},
  year      = {2014},
  month     = {Sep.},
  pages     = {130-135},
  abstract  = {Evaluating qualities of software based on software structural attributes such as coupling and cohesion are frequently done in practice as these attributes directly have impacts on value of higher level quality. Concerning oneself with structural attributes values early on helps developers to predict quality attributes level in the software. Service-Oriented Architecture (SOA) is an architectural concept where services are used as building blocks in developing new software. Lots of structural attributes metrics related to SOA had been proposed these recent years, which triggered an investigation to classify these metrics based on specific criteria. In this paper, we introduce classifications for SOA based structural attributes metrics, where the metrics are restricted to coupling, cohesion and complexity metrics. These metrics are classified based on software static and dynamic aspects with some brief introduction for each metric. By classifying these SOA based structural attributes metrics, it will allow user to avoid redundancy in proposing similar metrics thus increases the reusability of existing metrics.},
  doi       = {10.1109/MySec.2014.6986002},
  keywords  = {Couplings;Service-oriented architecture;Complexity theory;Software measurement;Semiconductor optical amplifiers;Structural attributes metric;Service Oriented Architecture;metrics classification},
}

@InProceedings{Tummalapalli2020a,
  author    = {Tummalapalli, Sahithi and Kumar, Lov and Neti, Lalita Bhanu Murthy and Krishna, Aneesh},
  booktitle = {2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
  title     = {An Empirical Analysis on the Role of WSDL Metrics in Web Service Anti-Pattern Prediction},
  year      = {2020},
  month     = {Dec},
  pages     = {559-564},
  abstract  = {Service-Oriented Architecture (SOA) is one of the most well-known models for designing web systems. SOA system evolution and maintenance is challenging because of its distributive nature and secondly due to the demand of designing high-quality, stable interfaces. This evolution leads to a problem called Anti-patterns in web services. It is observed that these anti-patterns negatively impact the evolution and maintenance of software systems, making the early detection and correction of them a primary concern for the software developers. The primary motivation of this work is to investigate the relationship between the Web Service Description Language(WSDL) metrics and anti-patterns in web services. This research aims to develop an automatic method for the detection of web service anti-patterns. The core idea of the methodology defined is to identify the most crucial WSDL metrics with the association of various feature selection techniques for the prediction of anti-patterns. Experimental results show that the model developed by using all the WSDL quantity metrics(AM) shows a bit high performance compared to the models developed with the other metric sets. Experimental results also showed that the performance of the models generated using Decision Tree(DT) and Major Voting Ensemble(MVE) is high compared to the models generated using other classifier techniques.},
  doi       = {10.1109/HPCC-SmartCity-DSS50907.2020.00070},
  keywords  = {Measurement;Computational modeling;High performance computing;Conferences;Maintenance engineering;Software systems;Feature extraction;Anti-patterns;Web Services;WSDL metrics;Neural Networks;Feature Selection;Data Sampling;Machine Learning},
}

@InProceedings{Nuraini2014,
  author    = {Nuraini, Aminah and Widyani, Yani},
  booktitle = {2014 International Conference on Data and Software Engineering (ICODSE)},
  title     = {Software with service oriented architecture quality assessment},
  year      = {2014},
  month     = {Nov},
  pages     = {1-6},
  abstract  = {Service Oriented Architecture (SOA) is becoming popular since its flexibility fulfill the need of rapidly changing enterprise requirement. Therefore, expectation of a good quality software with SOA is getting higher. To address this need, this paper presents a guideline to conduct quality assessment using an existing tool. The quality assessment model is designed by selecting the relevant quality factors, choosing an appropriate quality to metric mapping method, identifying the relevant metrics, and mapping each quality factor to the metrics. Using the model, the quality assessment process is prepared by identifying data and selecting the appropriate tools. The chosen tool may require some modification. The proposed quality assessment guideline can help the software quality assurance team to assess quality of their software with SOA. The proposed guideline has been used to assess the quality of an existing sofware with SOA (Bonita BPM). The result is considered as promising, although several improvement are still needed.},
  doi       = {10.1109/ICODSE.2014.7062707},
  keywords  = {Measurement;Quality assessment;Service-oriented architecture;Q-factor;Guidelines;Semiconductor optical amplifiers;SOA;software quality assessment;quality factor;quality metrics},
}

@InProceedings{Wijayanto2014,
  author    = {Wijayanto, Arie Wahyu and Suhardi},
  booktitle = {2014 International Conference on ICT For Smart Society (ICISS)},
  title     = {Service oriented architecture design using SOMA for optimizing public satisfaction in government agency: Case study: BPN - National Land Authority of Indonesia},
  year      = {2014},
  month     = {Sep.},
  pages     = {49-55},
  abstract  = {Service oriented architecture (SOA) enables organizations to easily integrate systems, data, and business processes. Implementation of SOA solution in private sector is widely used and successfully proven to increase their profit. But there are different challenge in public sector which is not profit oriented and has different business model. In public sector, user satisfaction on government agencies is one of common indicator to measure quality of public service. This paper presents SOA solution for public sector using SOMA to conduct a service integration for optimizing public satisfaction. We also combined SWOT and Porter's Value Chain to support business modelling analysis. The result shows that there is a simplicity and feasibility for users to access the service after SOA integration, which improves user satisfaction.},
  doi       = {10.1109/ICTSS.2014.7013150},
  keywords  = {Service-oriented architecture;Unified modeling language;Government;Analytical models;Computer architecture;Service Engineering;SOMA;Service Design;Service Oriented Architecture;Public Sector},
}

@InProceedings{Marmsoler2018,
  author    = {Marmsoler, Diego},
  booktitle = {2018 International Symposium on Theoretical Aspects of Software Engineering (TASE)},
  title     = {On Syntactic and Semantic Dependencies in Service-Oriented Architectures},
  year      = {2018},
  month     = {Aug},
  pages     = {132-137},
  abstract  = {In service oriented architectures, components provide services on their output ports and consume services from other components on their input ports. Thereby, a component is said to depend on another component if the former consumes a service provided by the latter. This notion of dependency (which we call syntactic dependency) is used by many architecture analysis tools as a measure for system maintainability. With this paper, we introduce a weaker notion of dependency, still sufficient, however, to guarantee semantic independence between components. Thereby, we discover the concepts of weak and strong semantic dependency and prove that strong semantic dependency indeed implies syntactic dependency. Our alternative notion of dependency paves the way to more precise dependency analysis tools. Moreover, our results about the different types of dependencies can be used for the verification of semantic independence.},
  doi       = {10.1109/TASE.2018.00025},
  keywords  = {Syntactic Dependency;Semantic Dependency;Service Oriented Architectures},
}

@Article{Sun2019,
  author   = {Sun, Chang-Ai and Dai, Hepeng and Wang, Guan and Towey, Dave and Chen, Tsong Yueh and Cai, Kai-Yuan},
  journal  = {IEEE Transactions on Services Computing},
  title    = {Dynamic Random Testing of Web Services: A Methodology and Evaluation},
  year     = {2019},
  issn     = {1939-1374},
  pages    = {1-1},
  abstract = {In recent years, Service Oriented Architecture (SOA) has been increasingly adopted to develop distributed applications in the context of the Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of web services. In this paper, we propose a dynamic random testing (DRT) technique for web services, which is an improvement over the widely-practiced random testing (RT) and partition testing (PT). We examine key issues when adapting DRT to the context of SOA, including a framework, guidelines for parameter settings, and a prototype for such an adaptation. Empirical studies are reported where DRT is used to test three real-life web services, and mutation analysis is employed to measure the effectiveness. Our experimental results show that, compared with the three baseline techniques, RT, Adaptive Testing (AT) and Random Partition Testing (RPT), DRT demonstrates higher fault-detection effectiveness with a lower test case selection overhead. Furthermore, the theoretical guidelines of parameter setting for DRT are confirmed to be effective. The proposed DRT and the prototype provide an effective and efficient approach for testing web services.},
  doi      = {10.1109/TSC.2019.2960496},
  keywords = {Testing;Service-oriented architecture;Guidelines;Reliability;Prototypes;Software Testing;Random Testing;Dynamic Random Testing;Web Service;Service Oriented Architecture},
}

@InProceedings{Barnwal2019,
  author    = {Barnwal, Anil and Jangade, Rajesh and Pugla, Satyakam},
  booktitle = {2019 9th International Conference on Cloud Computing, Data Science Engineering (Confluence)},
  title     = {Analyzing and Predicting the Allocation and Utilization of Resources in Cloud Computing System},
  year      = {2019},
  month     = {Jan},
  pages     = {56-62},
  abstract  = {The increasing use of cloud computing, constructed on good research in utility computing, networking, virtualization and web services provides some important benefits such as flexibility, cost reduction and easy availability for people using the system. These advantages are expected to increase the demand for more cloud services which further increase the installation of more clouds and its customer base. These demands lead to many technical issues such as applications of internet services, service oriented architecture including high scalability and availability, fault tolerance. So the core issue is to develop techniques for balancing of load effectively. It is clear from the fact that the measure and complexity makes these systems infeasible for assignment of centralized jobs to specific servers. So there is need of productive distributed solutions. In the current paper three proposed load balancing solutions for distributed environment is investigated. They are biased Random Sampling, Honeybee Foraging and Active Clustering.},
  doi       = {10.1109/CONFLUENCE.2019.8776974},
  keywords  = {Cloud computing;Servers;Load management;Workstations;Task analysis;Resource management;Complexity theory;Cloud Computing; Cloud Services;SaaS;PaaS;IaaS;Active Clustering;Random Sampling;Honeybee Foraging},
}

@Article{Zhao2017b,
  author   = {Zhao, Feng and Nian, Guodong and Jin, Hai and Yang, Laurence T. and Zhu, Yajun},
  journal  = {IEEE Systems Journal},
  title    = {A Hybrid eBusiness Software Metrics Framework for Decision Making in Cloud Computing Environment},
  year     = {2017},
  issn     = {1937-9234},
  month    = {June},
  number   = {2},
  pages    = {1049-1059},
  volume   = {11},
  abstract = {Developing high-quality software is essential for eBusiness organizations to cope with drastic market competition. With the development of cloud computing technologies, eBusiness systems and applications pay more attention to open endedness. In a cloud computing environment, eBusiness systems have the ability to provide information technology resources on demand. Traditional software metric methods in distributed systems and applications are technical and project driven, making the market demand and internal practical operation not perfectly balanced within a cloud-computing-based eBusiness corporation. To address this issue, this paper presents a hybrid framework based on the goal/question/metric paradigm to evaluate the quality and efficiency of previous software products, projects, and development organizations in a cloud computing environment. In our approach, to support decision making at the project and organization levels, three angular metrics are used, i.e., project metrics, product metrics, and organization metrics. Furthermore, an improved radial-basis-function-based model is also provided to manage existing projects and design new projects. Experimental results on a well-known eBusiness organization show that the proposed framework is effective, efficient, and operational. Moreover, using the described decision-making algorithm, the predicted data are very close to actual results on the software cost, the fault rate, the development workload, etc., which are greatly helpful in achieving high-quality software.},
  doi      = {10.1109/JSYST.2015.2443049},
  keywords = {Computational modeling;Organizations;Software metrics;Cloud computing;Data models;Cloud computing;decision making;eBusiness;prediction;radial basis function (RBF);software metrics},
}

@InProceedings{Gustamas2017,
  author    = {Gustamas, R. Gargista and Shidik, Guruh Fajar},
  booktitle = {2017 International Seminar on Application for Technology of Information and Communication (iSemantic)},
  title     = {Analysis of network infrastructure performance on cloud computing},
  year      = {2017},
  month     = {Oct},
  pages     = {169-174},
  abstract  = {Cloud Computing offers more convenience than conventional that provide custom Virtual Machine (VM) for any computation requirements. Network connectivity is closely related to the quality of cloud infrastructure itself. This paper focus in preliminary study to test the performance of cloud infrastructure with two type test. First test to measure Network performance and the second to measure cloud computation performance. OpenStack was used as cloud computing software infrastructure. We perform simple cloud infrastructure topology which is divided into three zones, there are Internal Zone, External Zone and Outside Cloud Infrastructure Zone. The parameter tested in this research are quality of bandwidth, latency, jitter and also Processing time during rendering process. The results show VM from simple topology cloud computing which is used to render video, able to perform processing time that slightly longer than using personal computer (PC) with same specification. The network side has been considering as a key of degradation render performance in cloud computing.},
  doi       = {10.1109/ISEMANTIC.2017.8251864},
  keywords  = {Cloud computing;Rendering (computer graphics);Servers;Topology;Network topology;Bandwidth;Jitter;Cloud Computing;Network Performance;IAAS;Rendering},
}

@InProceedings{Muralitharan2017,
  author    = {Muralitharan, D. Boobala and Reebha, S. Arockia Babi and Saravanan, D.},
  booktitle = {2017 International Conference on IoT and Application (ICIOT)},
  title     = {Optimization of performance and scheduling of HPC applications in cloud using cloudsim and scheduling approach},
  year      = {2017},
  month     = {May},
  pages     = {1-6},
  abstract  = {Cloud computing is emerging as a promising alternative to supercomputers for some High-Performance Computing (HPC) applications. Cloud computing is an essential component of the back bone of the Internet of Things (IoT). Clouds are needed to support huge numbers of interactions with varying quality requirements. Hence, Service quality will be a vital differentiator among cloud providers. In order to differentiate themselves from their competitors, cloud providers should offer best services that meet customers' expectations. A quality model can be used to represent, measure and compare the quality of the providers, such that a mutual understanding can be established among clouds take holders. With cloud as an additional deployment option, HPC users and providers faces the challenges of dealing with highly heterogeneous resources, where the variability spans across a wide range of processor configurations, interconnects, virtualization environments, and pricing models. HPC applications are increasingly being used in academia and laboratories for scientific research and in industries for business and analytics. Cloud computing offers the benefits of virtualization, elasticity of resources and elimination of cluster setup cost and time to HPC applications users. Effort was taken for holistic viewpoint to answer the questions - why and who should choose cloud for HPC, for what applications and how the cloud can be used for HPC? Comprehensive performance and cost evaluation and analysis of running a set of HPC applications on a range of platforms, varying from supercomputers to clouds was carried out. Further, performance of HPC applications is improved in cloud by optimizing HPC applications' characteristics for cloud and cloud virtualization mechanisms for HPC. In this paper, a novel heuristics for online application-aware job scheduling in multi-platform environments is presented. Experimental results and Simulations using CloudSim show that current clouds cannot substitute supercomputers but can effectively complement them.},
  doi       = {10.1109/ICIOTA.2017.8073634},
  keywords  = {Cloud computing;Virtualization;Supercomputers;Computational modeling;Servers;Processor scheduling;Resource management;Cloud computing;High-Performance Computing (HPC);Job scheduling;CloudSim},
}

@Article{Li2020c,
  author   = {Li, Keqin},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Quantitative Modeling and Analytical Calculation of Elasticity in Cloud Computing},
  year     = {2020},
  issn     = {2168-7161},
  month    = {Oct},
  number   = {4},
  pages    = {1135-1148},
  volume   = {8},
  abstract = {Elasticity is a fundamental feature of cloud computing and can be considered as a great advantage and a key benefit of cloud computing. One key challenge in cloud elasticity is lack of consensus on a quantifiable, measurable, observable, and calculable definition of elasticity and systematic approaches to modeling, quantifying, analyzing, and predicting elasticity. Another key challenge in cloud computing is lack of effective ways for prediction and optimization of performance and cost in an elastic cloud platform. The present paper makes the following significant contributions. First, we present a new, quantitative, and formal definition of elasticity in cloud computing, i.e., the probability that the computing resources provided by a cloud platform match the current workload. Our definition is applicable to any cloud platform and can be easily measured and monitored. Furthermore, we develop an analytical model to study elasticity by treating a cloud platform as a queueing system, and use a continuous-time Markov chain (CTMC) model to precisely calculate the elasticity value of a cloud platform by using an analytical and numerical method based on just a few parameters, namely, the task arrival rate, the service rate, the virtual machine start-up and shut-down rates. In addition, we formally define auto-scaling schemes and point out that our model and method can be easily extended to handle arbitrarily sophisticated scaling schemes. Second, we apply our model and method to predict many other important properties of an elastic cloud computing system, such as average task response time, throughput, quality of service, average number of VMs, average number of busy VMs, utilization, cost, cost-performance ratio, productivity, and scalability. In fact, from a cloud consumer's point of view, these performance and cost metrics are even more important than the elasticity metric. Our study in this paper has two significance. On one hand, a cloud service provider can predict its performance and cost guarantee using the results developed in this paper. On the other hand, a cloud service provider can optimize its elastic scaling scheme to deliver the best cost-performance ratio. To the best of our knowledge, this is the first paper that analytically and comprehensively studies elasticity, performance, and cost in cloud computing. Our model and method significantly contribute to the understanding of cloud elasticity and management of elastic cloud computing systems.},
  doi      = {10.1109/TCC.2017.2665549},
  keywords = {Cloud computing;Markov processes;Computational modeling;Analytical models;Pricing;Quality of service;Optimization;Queueing analysis;Cloud computing;continuous-time Markov chain;cost-performance ratio;elasticity;queueing model},
}

@Article{Guerron2020,
  author   = {Guerron, Ximena and Abrahão, Silvia and Insfran, Emilio and Fernández-Diego, Marta and González-Ladrón-De-Guevara, Fernando},
  journal  = {IEEE Access},
  title    = {A Taxonomy of Quality Metrics for Cloud Services},
  year     = {2020},
  issn     = {2169-3536},
  pages    = {131461-131498},
  volume   = {8},
  abstract = {A large number of metrics with which to assess the quality of cloud services have been proposed over the last years. However, this knowledge is still dispersed, and stakeholders have little or no guidance when choosing metrics that will be suitable to evaluate their cloud services. The objective of this paper is, therefore, to systematically identify, taxonomically classify, and compare existing quality of service (QoS) metrics in the cloud computing domain. We conducted a systematic literature review of 84 studies selected from a set of 4333 studies that were published from 2006 to November 2018. We specifically identified 470 metric operationalizations that were then classified using a taxonomy, which is also introduced in this paper. The data extracted from the metrics were subsequently analyzed using thematic analysis. The findings indicated that most metrics evaluate quality attributes related to performance efficiency (64%) and that there is a need for metrics that evaluate other characteristics, such as security and compatibility. The majority of the metrics are used during the Operation phase of the cloud services and are applied to the running service. Our results also revealed that metrics for cloud services are still in the early stages of maturity - only 10% of the metrics had been empirically validated. The proposed taxonomy can be used by practitioners as a guideline when specifying service level objectives or deciding which metric is best suited to the evaluation of their cloud services, and by researchers as a comprehensive quality framework in which to evaluate their approaches.},
  doi      = {10.1109/ACCESS.2020.3009079},
  keywords = {Measurement;Cloud computing;Taxonomy;Quality of service;Systematics;NIST;Elasticity;Software quality;metrics;cloud services;systematic literature review},
}

@Article{Zheng2014a,
  author   = {Zheng, Xianrong and Martin, Patrick and Brohman, Kathryn and Xu, Li Da},
  journal  = {IEEE Transactions on Industrial Informatics},
  title    = {CLOUDQUAL: A Quality Model for Cloud Services},
  year     = {2014},
  issn     = {1941-0050},
  month    = {May},
  number   = {2},
  pages    = {1527-1536},
  volume   = {10},
  abstract = {Cloud computing is an important component of the backbone of the Internet of Things (IoT). Clouds will be required to support large numbers of interactions with varying quality requirements. Service quality will therefore be an important differentiator among cloud providers. In order to distinguish themselves from their competitors, cloud providers should offer superior services that meet customers' expectations. A quality model can be used to represent, measure, and compare the quality of the providers, such that a mutual understanding can be established among cloud stakeholders. In this paper, we take a service perspective and initiate a quality model named CLOUDQUAL for cloud services. It is a model with quality dimensions and metrics that targets general cloud services. CLOUDQUAL contains six quality dimensions, i.e., usability, availability, reliability, responsiveness, security, and elasticity, of which usability is subjective, whereas the others are objective. To demonstrate the effectiveness of CLOUDQUAL, we conduct empirical case studies on three storage clouds. Results show that CLOUDQUAL can evaluate their quality. To demonstrate its soundness, we validate CLOUDQUAL with standard criteria and show that it can differentiate service quality.},
  doi      = {10.1109/TII.2014.2306329},
  keywords = {Cloud computing;Security;Availability;Quality of service;Measurement;Cloud computing;Internet of Things (IoT);quality model;validity criteria},
}

@InProceedings{Cedillo2015,
  author    = {Cedillo, Priscila and Jimenez-Gomez, Javier and Abrahao, Silvia and Insfran, Emilio},
  booktitle = {2015 IEEE International Conference on Services Computing},
  title     = {Towards a Monitoring Middleware for Cloud Services},
  year      = {2015},
  month     = {June},
  pages     = {451-458},
  abstract  = {Cloud Computing represents a new trend in the development and use of software. Many organizations are currently adopting the use of services that are hosted in the cloud by employing the Software as a Service (SaaS) model. Services are typically accompanied by a Service Level Agreement (SLA), which defines the quality terms that a provider offers to its customers. Many monitoring tools have been proposed to report compliance with the SLA. However, they have some limitations when changes to monitoring requirements must be made and because of the complexity involved in capturing low-level raw data from services at runtime. In this paper, we propose the design of a platform-independent monitoring middleware for cloud services, which supports the monitoring of SLA compliance and provides a report containing SLA violations that may help stakeholders to make decisions regarding how to improve the quality of cloud services. Moreover, our middleware definition is based on the use of models@run.time, which allows the dynamic change of quality requirements and/or the dynamic selection of different metric operationalizations (i.e., Calculation formulas) with which to measure the quality of services. In order to demonstrate the feasibility of our approach, we show the instantiation of the proposed middleware that can be used to monitor services when deployed on the Microsoft Azure© platform.},
  doi       = {10.1109/SCC.2015.68},
  keywords  = {Monitoring;Middleware;Measurement;Radiation detectors;Engines;Runtime;Software as a service;Cloud Computing;Software as a Service;Monitoring;Middleware;Quality of Service;Models@run.time},
}

@InProceedings{Atan2016,
  author    = {Rodziah binti Atan},
  booktitle = {2016 2nd International Conference on Science in Information Technology (ICSITech)},
  title     = {Enhancing service quality through Service Level Agreement (SLA) full implementation},
  year      = {2016},
  month     = {Oct},
  pages     = {1-1},
  abstract  = {Various SLA monitoring systems are proposed by different features and abilities to evaluate the agreed SLA. The current SLA monitoring systems in cloud computing for its structural, behavioral characteristics and situation are also in place. The systematic reviews of a well-known methods and approaches shows a significant numbers of researches been done in this area. Based on the number of effort and researches, the quality of services should proportionately increase alongside them. We look this matter from the perspectives of enforcement, that evident the stand of quality of services. Service Level Agreement (SLA) enforcement impact measures is a potential research area to be explored. Assumptions that this study is making are, SLA management will become better by a firm enforcement, where every customers are responsible to launch report of bugs or mischief of services such as unsatisfactory quality or service unavailability to a collection pool, and the provider will react immediately to the complaints so that the total downtime not exceeding the SLA value, with efficient enforcement. This study establishes fundamental theory to measure enforcement impact to SLA monitoring and management. We proposed eight activity phases from formulating until analyzing and decision formation. Descriptive statistics is utilized to analyze the extracted data. The SLA validation detection is the most frequent purpose of SLA monitoring systems in cloud by 58% and throughput is checked as an attribute target by 28%. The self-monitoring SLA, self-healing system, hierarchical structure are recognized points of SLA monitoring systems which need improvement before the enforcement could be based upon.},
  doi       = {10.1109/ICSITech.2016.7852595},
  keywords  = {Service Level Agreement;enforcement;monitoring;cloud computing;quality of service},
}

@InProceedings{Brataas2017,
  author    = {Brataas, Grunnar and Herbst, Nikolas and Ivansek, Simon and Polutnik, Jure},
  booktitle = {2017 IEEE International Conference on Autonomic Computing (ICAC)},
  title     = {Scalability Analysis of Cloud Software Services},
  year      = {2017},
  month     = {July},
  pages     = {285-292},
  abstract  = {Cloud computing theoretically offers its customers unlimited cloud resources. However, the scalability of software services is often limited by their underlying architecture. In contrast to current scalability analysis approaches, we make work parameters, quality thresholds, as well as the resource space explicit in a conceptually consistent set of equations. We propose two scalability metric functions based on these equations. The resource scalability metric function describes the relation between the capacity of the multi-tier cloud software service and its use of cloud resources, whereas the cost scalability metric function replaces cloud resources with cost. We validate using the Cloud-Store application. CloudStore follows the TPC-W specification, representing an online book store. We have experimented with 21 different public Amazon Web Service configurations and two private OpenStack configurations.},
  doi       = {10.1109/ICAC.2017.34},
  issn      = {2474-0756},
  keywords  = {Measurement;Scalability;Cloud computing;Computer architecture;Aerospace electronics;cloud;service;scalability;metric;measurement;cost},
}

@InProceedings{Lim2014,
  author    = {Lim, Erbin and Thiran, Philippe},
  booktitle = {2014 IEEE International Conference on Cloud Engineering},
  title     = {Communication of Technical QoS among Cloud Brokers},
  year      = {2014},
  month     = {March},
  pages     = {403-409},
  abstract  = {Service brokers are commonly used in the cloud computing paradigm to represent service requesters to select a service provider. They act as an intermediary between the two parties. One model of the cloud computing paradigm involves 3 layers, the user, the SaaS provider and the Cloud provider. The selection of service requesters is challenging due to the different levels of Quality of Service that each service provider can provide. In this paper we propose a unique mechanism that allows communication between service brokers in different layers in order to further improve this selection. In addition, we introduce a metric, efficiency, which service brokers can use to deterministically compare service providers with each other.},
  doi       = {10.1109/IC2E.2014.92},
  keywords  = {Quality of service;Monitoring;Measurement;Cloud computing;Availability;Computer architecture},
}

@Article{Singh2020,
  author   = {Singh, Sukhpal and Chana, Inderveer and Buyya, Rajkumar},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {STAR: SLA-aware Autonomic Management of Cloud Resources},
  year     = {2020},
  issn     = {2168-7161},
  month    = {Oct},
  number   = {4},
  pages    = {1040-1053},
  volume   = {8},
  abstract = {Cloud computing has recently emerged as an important service to manage applications efficiently over the Internet. Various cloud providers offer pay per use cloud services that requires Quality of Service (QoS) management to efficiently monitor and measure the delivered services through Internet of Things (IoT) and thus needs to follow Service Level Agreements (SLAs). However, providing dedicated cloud services that ensure user's dynamic QoS requirements by avoiding SLA violations is a big challenge in cloud computing. As dynamism, heterogeneity and complexity of cloud environment is increasing rapidly, it makes cloud systems insecure and unmanageable. To overcome these problems, cloud systems require self-management of services. Therefore, there is a need to develop a resource management technique that automatically manages QoS requirements of cloud users thus helping the cloud providers in achieving the SLAs and avoiding SLA violations. In this paper, we present SLA-aware autonomic resource management technique called STAR which mainly focuses on reducing SLA violation rate for the efficient delivery of cloud services. The performance of the proposed technique has been evaluated through cloud environment. The experimental results demonstrate that STAR is efficient in reducing SLA violation rate and in optimizing other QoS parameters which effect efficient cloud service delivery.},
  doi      = {10.1109/TCC.2017.2648788},
  keywords = {Cloud computing;Quality of service;Resource management;Monitoring;Reliability;Service level agreements;Software as a service;Autonomic cloud;resource provisioning;cloud computing;resource scheduling;quality of service;service level agreement},
}

@InProceedings{Ma2020a,
  author    = {Ma, Kun and Bagula, Antoine and Ajayi, Olasupo and Nyirenda, Clement},
  booktitle = {ICC 2020 - 2020 IEEE International Conference on Communications (ICC)},
  title     = {Aiming at QoS: A Modified DE Algorithm for Task Allocation in Cloud Computing},
  year      = {2020},
  month     = {June},
  pages     = {1-7},
  abstract  = {The Cloud computing system is characterized by large scale servers being utilized by an even larger number of users. It is a system where there is the need to frequently and efficiently schedule and manage different application tasks, with varied service requirements. One of the challenges of Cloud computing is managing the quality of service (QoS) rendered to users, specifically scheduling tasks between users and Cloud resources in a timely manner. Cloud users usually have widely diverse QoS requirements and meeting these simultaneously is also a challenge. In this paper, in order to improve on Cloud resource allocation and specifically to tailor it towards meeting varied QoS requirements of users, we proposed a new algorithm which combines Differential Evolution with the Shapley Value economic mode. This combination allows us measure the contribution of each virtual machine (VM), so as to improve the probability of obtaining a better tasks-to-resource allocation thereby improving user satisfaction. From results of conducted experiments, when compared with the traditional DE (Differential Evolution) algorithm and the conventional task-VM binding policy in CloudSim, both for allocations where special QoS requirements are required and in instances of multiple QoS requirements; the modified Shapley value based DE algorithm (SVBDA) shows significant improvement.},
  doi       = {10.1109/ICC40277.2020.9148980},
  issn      = {1938-1883},
  keywords  = {Quality of service;Task analysis;Cloud computing;Resource management;Games;Bandwidth;Processor scheduling;Cloud computing;Differential evolution algorithm;Execution time/cost minimizing;NP hard problem;Quality of service (QoS);Shapley value;Tasks scheduling;Virtual machine},
}

@InProceedings{Zhou2015,
  author    = {Zhou, Nianjun and Mohindra, Ajay},
  booktitle = {2015 IEEE International Conference on Services Computing},
  title     = {Causality-Driven Performance Monitoring and Scaling Automation for Managed Solutions},
  year      = {2015},
  month     = {June},
  pages     = {467-474},
  abstract  = {A key feature of Cloud computing is its agility and flexibility to support the scalability needs of business solutions. Currently, the agility is only limited to the scalability of the compute, memory and storage. To improve an application's agility, we need to monitor & measure solution level metrics and associate the performance of the metrics to the business agility needs of the solution by making real-time scalability or change decisions. In this paper, we illustrate a scaling decision mechanism utilizing the monitoring data from infrastructure, middleware, and business level metrics. We use these performance metrics as input to a causality analysis model to make architecture changes or scalability decisions. Mathematically, we define the causality as a graph to link the changes in the measured metric values to the action of the solution change. The causality analysis follows scalability principles as best practices. They are a) the principle of performance scalability b) principle of contribution margin for scalability, and c) principle of the least cost of SLA compliance. We define these scalability principles as the rules to ensure that the business stakeholder of the solution can maintain or improve their business quality or profit margins as the computing capability scales up or down. To implement those principles, we need to establish the linkages of the business metrics to the decision of changes. To make such linkage, we first utilize causality analysis to identify feasible scaling actions, and then associate those actions with the system, application, and business performance metrics. With the help of causality analysis, we implement a performance monitoring and scaling automation framework for managed solutions using an Open Source Monitoring system.},
  doi       = {10.1109/SCC.2015.70},
  keywords  = {Scalability;Measurement;Monitoring;Business;Computer architecture;Media;Servers;Scalability;Business Monitoring;Performance Metrics;Service Level Agreements;Decision;Trade-Off},
}

@Article{Cedillo2021,
  author   = {Cedillo, Priscila and Insfran, Emilio and Abrahão, Silvia and Vanderdonckt, Jean},
  journal  = {IEEE Access},
  title    = {Empirical Evaluation of a Method for Monitoring Cloud Services Based on Models at Runtime},
  year     = {2021},
  issn     = {2169-3536},
  pages    = {55898-55919},
  volume   = {9},
  abstract = {Cloud computing is being adopted by commercial and governmental organizations driven by the need to reduce the operational cost of their information technology resources and search for a scalable and flexible way to provide and release their software services. In this computing model, the Quality of Services (QoS) is agreed between service providers and their customers through Service Level Agreements (SLA). There is thus a need for systematic approaches with which to assess the quality of cloud services and their compliance with the SLA. In previous work, we introduced a generic method for Monitoring cloud Services using models at RunTime (MoS@RT), which allows the monitoring requirements or the metric operationalizations of these requirements to be changed at runtime without the modification of the underlying infrastructure. In this paper, we present the design of a monitoring infrastructure that supports the proposed method with its instantiation to a specific platform and reports the results of an experiment carried out to evaluate the perceived efficacy of 58 undergraduate students when using the infrastructure to configure the monitoring of cloud services deployed on the Microsoft Azure platform. The results show that the participants perceived MoS@RT to be easy to use, useful, and they also expressed their intention to use the method in the future. Although further experiments must be carried out to strengthen these results, MoS@RT has proved to be a promising monitoring method for cloud services.},
  doi      = {10.1109/ACCESS.2021.3071417},
  keywords = {Monitoring;Cloud computing;Tools;Runtime;Quality of service;Measurement;Service level agreements;Cloud computing;models@runtime;quality of service (QoS);services monitoring;software as a service (SaaS)},
}

@InProceedings{Pendlebury2016,
  author    = {Pendlebury, John and Emeakaroha, Vincent C. and O'Shea, David and Cafferkey, Neil and Morrison, John P. and Lynn, Theo},
  booktitle = {2016 2nd International Conference on Cloud Computing Technologies and Applications (CloudTech)},
  title     = {SOMBA - automated anomaly detection for Cloud quality of service},
  year      = {2016},
  month     = {May},
  pages     = {71-79},
  abstract  = {Cloud computing has transformed the standard model of service provisioning, allowing the delivery of on-demand services over the Internet. With its inherent requirements for elastic scalability and a pay-as-you-go pricing model, an additional level of complexity is added to its Quality of Service (QoS) management. This has made service provisioning more prone to performance anomalies due to the large-scale and evolving nature of Clouds. Existing methods for anomaly detection based on QoS monitoring in the Cloud rely on probabilistic methods, which are not computationally easy and are often valid for very short times before system dynamics change. We posit that more minimalistic approaches including automated techniques are needed for effective anomaly detection to support QoS enforcement in Clouds. In this paper, we present an automated anomaly detection scheme that recognises and adapts to changes in Clouds for efficient multi-metric performance anomaly detection to guarantee service quality. It includes a monitoring tool for collating performance data in real time for analysis and an anomaly detection technique based on an unsupervised machine learning strategy. Based on a Cloud service provisioning use case scenario, we evaluate our anomaly detection technique and compare it against two statistical anomaly detection approaches to demonstrate its efficiency.},
  doi       = {10.1109/CloudTech.2016.7847681},
  keywords  = {Monitoring;Quality of service;Cloud computing;Visualization;Resource management},
}

@Article{Liu2019a,
  author   = {Liu, Ying and Wang, Ke and Ge, Liang and Ye, Lei and Cheng, Jingde},
  journal  = {IEEE Access},
  title    = {Adaptive Evaluation of Virtual Machine Placement and Migration Scheduling Algorithms Using Stochastic Petri Nets},
  year     = {2019},
  issn     = {2169-3536},
  pages    = {79810-79824},
  volume   = {7},
  abstract = {More and more mobile applications rely on the combination of both mobile and cloud computing technology to bring out their full potential. The cloud is usually used for providing additional computing resources that cannot be handled efficiently by the mobile devices. Cloud usage, however, results in several challenges related to the management of virtualized resources. A large number of scheduling algorithms are proposed to balance between performance and cost of data center. Due to huge cost and time consuming of measure-based and simulation method, this paper proposes an adaptive method to evaluate scheduling algorithms. In this method, the virtual machine placement and migration process are modeled by using Stochastic Reward Nets. Different scheduling methods are described as reward functions to perform the adaptive evaluation. Two types of performance metrics are also discussed: one is about quality of service, such as system availability, mean waiting time, and mean service time, and the other is the cost of runtime, such as energy consumption and cost of migration. Compared to a simulation method, the analysis model in this paper only modifies the reward function for different scheduling algorithms and does not need to reconstruct the process. The numeric results suggest that it also has a good accuracy and can quantify the influence of scheduling algorithms on both quality of service and cost of runtime.},
  doi      = {10.1109/ACCESS.2019.2923592},
  keywords = {Scheduling algorithms;Cloud computing;Servers;Adaptation models;Stochastic processes;Virtual machining;Quality of service;Adaptive evaluation;virtual machine placement and migration;scheduling algorithms;stochastic reward net},
}

@InProceedings{Tchernykh2014,
  author    = {Tchernykh, Andrei and Lozano, Luz and Schwiegelshohn, Uwe and Bouvry, Pascal and Pecero, Johnatan E. and Nesmachnow, Sergio},
  booktitle = {2014 IEEE 3rd International Conference on Cloud Networking (CloudNet)},
  title     = {Bi-objective online scheduling with quality of service for IaaS clouds},
  year      = {2014},
  month     = {Oct},
  pages     = {307-312},
  abstract  = {This paper focuses on the bi-objective experimental analysis of online scheduling in the Infrastructure as a Service model of Cloud computing. In this model, customer have the choice between different service levels. Each service level is associated with a price per unit of job execution time and a slack factor that determines the maximal time span to deliver the requested amount of computing resources. It is responsibility of the system and its scheduling algorithm to guarantee the corresponding quality of service for all accepted jobs. We do not consider any optimistic scheduling approach, that is, a job cannot be accepted if its service guarantee will not be observed assuming that all accepted jobs receive the requested resources. We analyze several scheduling algorithms with different cloud configurations and workloads and use the maximization of the provider income and minimization of the total power consumption of a schedule as additional objectives. Therefore, we cannot expect finding a unique solution to a given problem but a set of nondominated solutions also known as Pareto optima. Then we assess the performance of different scheduling algorithms by using a set coverage metric to compare them in terms of Pareto dominance. Based on the presented case study, we claim that a simple algorithm can provide the best energy and income trade-offs. This scheduling algorithm performs well in different scenarios with a variety of workloads and cloud configurations.},
  doi       = {10.1109/CloudNet.2014.6969013},
  keywords  = {Power demand;Degradation;Measurement;Processor scheduling;Educational institutions;Energy efficiency;Resource management;Cloud computing;Service Level Agreement;Energy Efficiency;Scheduling},
}

@InProceedings{AlSaidAhmad2018,
  author    = {Al-Said Ahmad, Amro and Andras, Peter},
  booktitle = {2018 Fifth International Symposium on Innovation in Information and Communication Technology (ISIICT)},
  title     = {Measuring and Testing the Scalability of Cloud-based Software Services},
  year      = {2018},
  month     = {Oct},
  pages     = {1-8},
  abstract  = {Performance and scalability testing and measurements of cloud-based software services are critically important in the context of rapid growth of cloud computing and supporting the delivery of these services. Cloud-based software services performance aspects are interrelated, both elasticity and efficiency are depending on the delivery of a sufficient level of scalability performance. In this work, we focused on testing and measuring the scalability of cloud-based software services in technical terms. This paper uses technical scalability metrics that address both volume and quality scaling, that inspired by earlier technical metrics of elasticity. We show how our technical scalability metrics can be integrated into an earlier utility oriented metric of scalability. We demonstrate the application of the metrics using a practical example and discuss the importance of them.},
  doi       = {10.1109/ISIICT.2018.8613297},
  keywords  = {Scalability;Software;Elasticity;Software measurement;Testing;Cloud computing;Measurement;Performance;Testing;Scalability;Software-as-a-Service (SaaS);Metrics},
}

@Article{Candeia2015,
  author   = {Candeia, David and Santos, Ricardo Araújo and Lopes, Raquel},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Business-Driven Long-Term Capacity Planning for SaaS Applications},
  year     = {2015},
  issn     = {2168-7161},
  month    = {July},
  number   = {3},
  pages    = {290-303},
  volume   = {3},
  abstract = {Capacity Planning is one of the activities developed by Information Technology departments over the years, it aims at estimating the amount of resources needed to offer a computing service. This activity contributes to achieving high Quality of Service levels and also to pursuing better economic results for companies. In the Cloud Computing context, one plausible scenario is to have Software-as-a-Service (SaaS) providers that build their IT infrastructure acquiring resources from Infrastructure-as-a-Service (IaaS) providers. SaaS providers can reduce operational costs and complexity by buying instances from a reservation market, but then need to predict the number of instances needed in the long-term. This work investigates how important is the capacity planning in this context and how simple business-driven heuristics for long-term capacity planning impact on the profit achieved by SaaS providers. Simulation experiments were performed using synthetic e-commerce workloads. Our analysis show that proposed heuristics increase SaaS provider profit, on average, at 9.6501 percent per year. Analysing such results we demonstrate that capacity planning is still an important activity, contributing to the increase of SaaS providers profit. Besides, a good capacity planning may also avoid bad reputation due to unacceptable performance, which is a gain very hard to measure.},
  doi      = {10.1109/TCC.2015.2424877},
  keywords = {Capacity planning;Cloud computing;Measurement;Contracts;Quality of service;Planning;Capacity Planning;Cloud Computing;Software-as-a-Service;Capacity planning;cloud computing;software-as-a-service},
}

@Article{AbdelBaky2019,
  author   = {AbdelBaky, Moustafa and Parashar, Manish},
  journal  = {IEEE Transactions on Services Computing},
  title    = {A General Performance and QoS Model for Distributed Software-Defined Environments},
  year     = {2019},
  issn     = {1939-1374},
  pages    = {1-1},
  abstract = {The landscape for cloud services and cyberinfrastructure offerings has increased drastically over the past few years. Initially, users moved their applications to the cloud to take advantage of a pay-per-usage model and on-demand access. However, as more cloud providers joined the market, users shifted their goals for using cloud computing from cost reduction to resilience, agility, and optimization. These goals can be achieved by dynamically combining services from multiple providers, for example, to avoid data center or cloud zone outages or to take advantage of extensive offerings with different price points. However, to efficiently support application deployment in this dynamic environment, new models and tools that can measure the application performance and the Quality of Service (QoS) of different configurations are required. The goal of this work is to evaluate the application performance and the QoS of a distributed Software-Defined Environment as well as calculate the QoS of alternative configurations from the underlying pool of services. In particular, we present a mathematical model and a tool for evaluating the performance and QoS of batch application workflows in a distributed environment. We experimentally evaluate the proposed model using a bioinformatics workflow running on infrastructure services from multiple cloud providers.},
  doi      = {10.1109/TSC.2019.2928300},
  keywords = {Cloud computing;Quality of service;Computational modeling;Data models;Optimization;Mathematical model;Tools;QoS modeling;performance modeling;multi-cloud;software-define environments},
}

@Article{Raj2021,
  author    = {Vinay Raj and Ravichandra Sadam},
  journal   = {{SN} Computer Science},
  title     = {Evaluation of {SOA}-Based Web Services and Microservices Architecture Using Complexity Metrics},
  year      = {2021},
  month     = {jul},
  number    = {5},
  volume    = {2},
  abstract  = {Distributed systems have evolved rapidly as the demand for independent design and deployment of software applications has increased. Web services and microservices are two styles of designing distributed applications based on the principles of Service-Oriented Architecture (SOA). After the evolution of microservices, software architects are in chaos, whether to adopt microservices over web services. To the best of our knowledge, there has been no empirical work done in the literature for comparing both web services and microservices architecture in terms of the coupling principle of SOA. In this paper, a service graph-based approach is proposed to analyze and evaluate the effectiveness of microservices architecture when compared with web services. Loose coupling is used as a perspective for evaluation, and we have chosen two case study applications to evaluate them in terms of coupling. From the results, it is observed that microservices has lesser coupling values than web services.},
  doi       = {10.1007/s42979-021-00767-6},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs42979-021-00767-6},
}

@Article{Slimani2020,
  author    = {Sarra Slimani and Tarek Hamrouni and Faouzi Ben Charrada},
  journal   = {Cluster Computing},
  title     = {Service-oriented replication strategies for improving quality-of-service in cloud computing: a survey},
  year      = {2020},
  month     = {may},
  number    = {1},
  pages     = {361--392},
  volume    = {24},
  abstract  = {The recent years have witnessed significant interest in migrating different applications into the cloud platforms. In this context, one of the main challenges for cloud applications providers is how to ensure high availability of the delivered applications while meeting users’ QoS. In this respect, replication techniques are commonly applied to efficiently handle this issue. From the literature, according to the used granularity for replication there are two major approaches to achieve replication: either through replicating the service or the underlying data. The latter one is also known as Data-oriented Replication (DoR), while the former one is referred to as Service-oriented Replication (SoR). DoR is discussed extensively in the available literature and several surveys are already published. However, SoR is still at its infancy and there is a lack of research studies. Hence, in this paper we present a comprehensive survey of SoR strategies in cloud computing. We propose a classification of existing works based on the research methods they use. Then, we carried out an in-depth study and analysis of these works. In addition, a tabular representation of all relevant features is presented to facilitate the comparison of SoR techniques and the proposal of new enhanced strategies.},
  doi       = {10.1007/s10586-020-03108-z},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10586-020-03108-z},
}

@Article{Bogner2021,
  author    = {Justus Bogner and Jonas Fritzsch and Stefan Wagner and Alfred Zimmermann},
  journal   = {Empirical Software Engineering},
  title     = {Industry practices and challenges for the evolvability assurance of microservices},
  year      = {2021},
  month     = {jul},
  number    = {5},
  volume    = {26},
  abstract  = {Context
Microservices as a lightweight and decentralized architectural style with fine-grained services promise several beneficial characteristics for sustainable long-term software evolution. Success stories from early adopters like Netflix, Amazon, or Spotify have demonstrated that it is possible to achieve a high degree of flexibility and evolvability with these systems. However, the described advantageous characteristics offer no concrete guidance and little is known about evolvability assurance processes for microservices in industry as well as challenges in this area. Insights into the current state of practice are a very important prerequisite for relevant research in this field.

Objective
We therefore wanted to explore how practitioners structure the evolvability assurance processes for microservices, what tools, metrics, and patterns they use, and what challenges they perceive for the evolvability of their systems.

Method
We first conducted 17 semi-structured interviews and discussed 14 different microservice-based systems and their assurance processes with software professionals from 10 companies. Afterwards, we performed a systematic grey literature review (GLR) and used the created interview coding system to analyze 295 practitioner online resources.

Results
The combined analysis revealed the importance of finding a sensible balance between decentralization and standardization. Guidelines like architectural principles were seen as valuable to ensure a base consistency for evolvability and specialized test automation was a prevalent theme. Source code quality was the primary target for the usage of tools and metrics for our interview participants, while testing tools and productivity metrics were the focus of our GLR resources. In both studies, practitioners did not mention architectural or service-oriented tools and metrics, even though the most crucial challenges like Service Cutting or Microservices Integration were of an architectural nature.

Conclusions
Practitioners relied on guidelines, standardization, or patterns like Event-Driven Messaging to partially address some reported evolvability challenges. However, specialized techniques, tools, and metrics are needed to support industry with the continuous evaluation of service granularity and dependencies. Future microservices research in the areas of maintenance, evolution, and technical debt should take our findings and the reported industry sentiments into account.},
  doi       = {10.1007/s10664-021-09999-9},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10664-021-09999-9},
}

@InCollection{Engel2018,
  author    = {Thomas Engel and Melanie Langermeier and Bernhard Bauer and Alexander Hofmann},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {Evaluation of Microservice Architectures: A Metric and Tool-Based Approach},
  year      = {2018},
  pages     = {74--89},
  abstract  = {Microservices are an architectural style that decomposes the functionality of an application system into several small functional units. The services are implemented and managed independently from each other. Breaking up monolithic structures into a microservice architecture increases the number of single components massively. Thus, effective management of the dependencies between them is required. This task can be supported with the creation and evaluation of architectural models. In this work, we propose an evaluation approach for microservice architectures based on identified architecture principles from research and practice like a small size of the services, a domain-driven design or loose coupling. Based on a study showing the challenges in current microservice architectures, we derived principles and metrics for the evaluation of the architectural design. The required architecture data is captured with a reverse engineering approach from traces of communication data. The developed tool is finally evaluated within a case study.},
  doi       = {10.1007/978-3-319-92901-9_8},
  url       = {https://doi.org/10.1007%2F978-3-319-92901-9_8},
}

@Article{Monteiro2020,
  author    = {Davi Monteiro and Paulo Henrique M. Maia and Lincoln S. Rocha and Nabor C. Mendon{\c{c}}a},
  journal   = {Service Oriented Computing and Applications},
  title     = {Building orchestrated microservice systems using declarative business processes},
  year      = {2020},
  month     = {aug},
  number    = {4},
  pages     = {243--268},
  volume    = {14},
  abstract  = {The microservices architecture style proposes a solution for efficiently scaling computational resources and solving other problems presented in the traditional monolithic architecture. Despite providing benefits, microservices bring challenges when there is a need to manage business processes that expand across the boundaries of an individual microservice. Traditional approaches such as workflows are not suitable to manage business processes in microservice systems, and existing solutions have limitations in dealing with the dynamic location of microservices. To fill that gap, this work proposes the use of declarative business processes to facilitate the modeling and execution of microservice orchestration from the perspective of data flow processes. To this end, we use Beethoven, a platform composed of a reference architecture, and a domain-specific language for expressing microservice communication flows. Finally, we demonstrate the feasibility of Beethoven in orchestrating an existing microservice application and assess its impact on the performance of the orchestrated application.},
  doi       = {10.1007/s11761-020-00300-2},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11761-020-00300-2},
}

@InCollection{Khan2017,
  author    = {Gitosree Khan and Sabnam Sengupta and Anirban Sarkar},
  booktitle = {Requirements Engineering for Service and Cloud Computing},
  publisher = {Springer International Publishing},
  title     = {Formal Modeling of Enterprise Cloud Bus System: A High Level Petri-Net Based Approach},
  year      = {2017},
  pages     = {121--149},
  abstract  = {The chapter focuses on an abstraction layer of SaaS architecture for multi-agent-based inter-cloud environment, called Enterprise Cloud Bus System (ECBS) to conceptualize the different behavioral facets of such system in service and cloud computing paradigm. The model is formalized using a set of high level Petri-net-based formal constructs called High Level Enterprise Cloud Bus Petri-net (HECBP) with varieties of relationship types among participation cloud bus components. It is accompanied with a rich set of Petri-net graphical notations and those are used to specify the effective toward modeling interactions among the heterogeneous agent present within the cloud bus of ECBS at conceptual level design of multi cloud system. The approach facilitates to analyze the behavioral features of inter-cloud architecture and modeled its dynamics at the conceptual level. The HECBP is also able to ensure correctness and performance of the system at design time by focusing on meeting the increasing demands for distributed software as a service and making the system functionality more scalable, configurable, and shareable. This chapter includes modeling of several behavioral facets like, fairness, boundedness , liveliness , safeness , etc., in a dead lock-free way. Moreover, this chapter provides a discussion on state-space analysis study, which further validates the theoretical analysis of HECBP model and future research scope in this area.},
  doi       = {10.1007/978-3-319-51310-2_6},
  url       = {https://doi.org/10.1007%2F978-3-319-51310-2_6},
}

@InCollection{Loucopoulos2016,
  author    = {Pericles Loucopoulos and Evangelia Kavakli},
  booktitle = {Domain-Specific Conceptual Modeling},
  publisher = {Springer International Publishing},
  title     = {Capability Oriented Enterprise Knowledge Modeling: The {CODEK} Approach},
  year      = {2016},
  pages     = {197--215},
  abstract  = {Enterprise modeling has been defined as the ‘art of externalizing enterprise knowledge’. Traditional approaches to enterprise modeling rely on ‘blueprint thinking’ that focuses on the formal structure and organization of the enterprise, with business processes being the fundamental components of the enterprise operation. Such approaches generally assume enterprises as deterministic, top-down managed entities, with a well-defined group of processes that develop and maintain products or services for their customers. However, the prevalence and volatility of digital enterprises shifts enterprise modeling towards a more dynamic enterprise configuration, to embrace the idea of dynamic adaptation according to the internal and external influences that constantly (re-)shape the business environment. To this end, enterprise modeling research has adopted model-driven development methods and service-oriented architectures originating from the software development domain, as a means to achieve flexible service delivery and the notion of dynamic capability from the strategic management domain in order to address adaptation to the dynamic business context. This chapter will outline emergent trends in the field, introduce a conceptual framework for the capability-driven development of enterprise knowledge and discuss how this can be used to enable the design of capabilities and services using examples from an eGovernment case study.},
  doi       = {10.1007/978-3-319-39417-6_9},
  url       = {https://doi.org/10.1007%2F978-3-319-39417-6_9},
}

@Article{Moens2013,
  author    = {Hendrik Moens and Eddy Truyen and Stefan Walraven and Wouter Joosen and Bart Dhoedt and Filip De Turck},
  journal   = {Journal of Network and Systems Management},
  title     = {Cost-Effective Feature Placement of Customizable Multi-Tenant Applications in the Cloud},
  year      = {2013},
  month     = {feb},
  number    = {4},
  pages     = {517--558},
  volume    = {22},
  abstract  = {Cloud computing technologies can be used to more flexibly provision application resources. By exploiting multi-tenancy, instances can be shared between users, lowering the cost of providing applications. A weakness of current cloud offerings however, is the difficulty of creating customizable applications that retain these advantages. In this article, we define a feature-based cloud resource management model, making use of Software Product Line Engineering techniques, where applications are composed of feature instances using a service-oriented architecture. We focus on how resources can be allocated in a cost-effective way within this model, a problem which we refer to as the feature placement problem. A formal description of this problem, that can be used to allocate resources in a cost-effective way, is provided. We take both the cost of failure to place features, and the cost of using servers into account, making it possible to take energy costs or the cost of public cloud infrastructure into consideration during the placement calculation. Four algorithms that can be used to solve the feature placement problem are defined. We evaluate the algorithm solutions, comparing them with the optimal solution determined using an integer linear problem solver, and evaluating the execution times of the algorithms, making use of both generated inputs and a use case based on three applications. We show that, using our approach a higher degree of multi-tenancy can be achieved, and that for the considered scenarios, taking the relationships between features into account and using application-oriented placement performs 25–40 % better than a purely feature-oriented placement.},
  doi       = {10.1007/s10922-013-9265-5},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10922-013-9265-5},
}

@Article{Siavvas2021,
  author    = {Miltiadis Siavvas and Dionysios Kehagias and Dimitrios Tzovaras and Erol Gelenbe},
  journal   = {Software Quality Journal},
  title     = {A hierarchical model for quantifying software security based on static analysis alerts and software metrics},
  year      = {2021},
  month     = {may},
  number    = {2},
  pages     = {431--507},
  volume    = {29},
  abstract  = {Despite the acknowledged importance of quantitative security assessment in secure software development, current literature still lacks an efficient model for measuring internal software security risk. To this end, in this paper, we introduce a hierarchical security assessment model (SAM), able to assess the internal security level of software products based on low-level indicators, i.e., security-relevant static analysis alerts and software metrics. The model, following the guidelines of ISO/IEC 25010, and based on a set of thresholds and weights, systematically aggregates these low-level indicators in order to produce a high-level security score that reflects the internal security level of the analyzed software. The proposed model is practical, since it is fully automated and operationalized in the form of a standalone tool and as part of a broader Computer-Aided Software Engineering (CASE) platform. In order to enhance its reliability, the thresholds of the model were calibrated based on a repository of 100 popular software applications retrieved from Maven Repository. Furthermore, its weights were elicited in a way to chiefly reflect the knowledge expressed by the Common Weakness Enumeration (CWE), through a novel weights elicitation approach grounded on popular decision-making techniques. The proposed model was evaluated on a large repository of 150 open-source software applications retrieved from GitHub and 1200 classes retrieved from the OWASP Benchmark. The results of the experiments revealed the capacity of the proposed model to reliably assess internal security at both product level and class level of granularity, with sufficient discretion power. They also provide preliminary evidence for the ability of the model to be used as the basis for vulnerability prediction. To the best of our knowledge, this is the first fully automated, operationalized and sufficiently evaluated security assessment model in the modern literature.},
  doi       = {10.1007/s11219-021-09555-0},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11219-021-09555-0},
}

@InCollection{Chauhan2016,
  author    = {Muhammad Aufeef Chauhan and Muhammad Ali Babar and Christian W. Probst},
  booktitle = {Product-Focused Software Process Improvement},
  publisher = {Springer International Publishing},
  title     = {A Process Framework for Designing Software Reference Architectures for Providing Tools as a Service},
  year      = {2016},
  pages     = {111--126},
  abstract  = {Software Reference Architecture (SRA), which is a generic architecture solution for a specific type of software systems, provides foundation for the design of concrete architectures in terms of architecture design guidelines and architecture elements. The complexity and size of certain types of software systems need customized and systematic SRA design and evaluation methods. In this paper, we present a software Reference Architecture Design process Framework (RADeF) that can be used for analysis, design and evaluation of the SRA for provisioning of Tools as a Service as part of a cloud-enabled workSPACE (TSPACE). The framework is based on the state of the art results from literature and our experiences with designing software architectures for cloud-based systems. We have applied RADeF SRA design two types of TSPACE: software architecting TSPACE and software implementation TSPACE. The presented framework emphasizes on keeping the conceptual meta-model of the domain under investigation at the core of SRA design strategy and use it as a guiding tool for design, evaluation, implementation and evolution of the SRA. The framework also emphasizes to consider the nature of the tools to be provisioned and underlying cloud platforms to be used while designing SRA. The framework recommends adoption of the multi-faceted approach for evaluation of SRA and quantifiable measurement scheme to evaluate quality of the SRA. We foresee that RADeF can facilitate software architects and researchers during design, application and evaluation of a SRA and its instantiations into concrete software systems.},
  doi       = {10.1007/978-3-319-49094-6_8},
  url       = {https://doi.org/10.1007%2F978-3-319-49094-6_8},
}

@InCollection{Shahin2015,
  author    = {Mojtaba Shahin and Muhammad Ali Babar},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Improving the Quality of Architecture Design Through Peer-Reviews and Recombination},
  year      = {2015},
  pages     = {70--86},
  abstract  = {Software architecture reviews help improve the quality of architecture design decisions. Traditional reviews are considered expensive and time-consuming. We assert that organizations can consider leveraging peer-reviews and recombination (i.e., promoting design improvement through sharing design ideas) activities to improve the quality of architectures and getting staff trained. This paper reports a case study aimed at exploring the potential impact of combining peer-review and recombination on the quality of architecture design and design decisions made by novice architects, who usually have limited practical experience of architecture design. The findings show that the use of peer-review and recombination can improve both the quality of architecture design and documented decisions. From the decision-making perspective, this study also identifies the main types of challenges that the participants faced during architectural decision making and reasoning. These findings can be leveraged to focus on the types of training novice architects may need to effectively and efficiently address the types of challenges identified in this study.},
  doi       = {10.1007/978-3-319-23727-5_6},
  url       = {https://doi.org/10.1007%2F978-3-319-23727-5_6},
}

@InCollection{AdjeponYamoah2016,
  author    = {David Ebo Adjepon-Yamoah},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {cloud-{ATAM}: Method for Analysing Resilient Attributes of Cloud-Based Architectures},
  year      = {2016},
  pages     = {105--114},
  abstract  = {In this work, we argue that the existing architecture evaluation methods have limitations when assessing architectures interfacing with unpredictable environments such as the Cloud. The unpredictability of this environment is attributed to the dynamic elasticity, scale, and continuous evolution of the cloud topology. As a result, architectures interfacing such unpredictable environments are expected to encounter many uncertainties. It is however, important to focus on, and present holistic approaches combining aspects of both dynamic and static analysis of architecture resilience attributes. This paper introduces an ATAM derived methodology - cloud-ATAM - for evaluating the trade-off between multiple resilience quality attributes (i.e. availability and performance) of a cloud-based Reactive Architecture for Global Software Development.},
  doi       = {10.1007/978-3-319-45892-2_8},
  url       = {https://doi.org/10.1007%2F978-3-319-45892-2_8},
}

@InCollection{Buchgeher2015,
  author    = {Georg Buchgeher and Rainer Weinreich and Thomas Kriechbaum},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {Making the Case for Centralized Software Architecture Management},
  year      = {2015},
  month     = {dec},
  pages     = {109--121},
  abstract  = {Architectural knowledge is an important artifact for many software development and quality control activities. Examples for quality control activities based on architectural information are architecture reviews, dependency analyses, and conformance analyses. Architecture is also an important artifact for understanding, reuse, evolution, and maintenance. Unfortunately, in many projects architectural knowledge often remains implicit and is not available for a particular system stakeholder or outdated when it is actually needed. To address this problem, we propose to manage semi-formal software architecture knowledge in a central repository, where it is accessible to all stakeholders and where it can be automatically and continuously updated and analyzed by different tools. In this paper we discuss important elements and use cases of such an approach, and present an example for an architecture knowledge and information repository in the context of an enterprise service-oriented architecture (SOA).},
  doi       = {10.1007/978-3-319-27033-3_8},
  url       = {https://doi.org/10.1007%2F978-3-319-27033-3_8},
}

@Article{Nogueira2016,
  author    = {Elias Nogueira and Ana Moreira and Daniel Lucr{\'{e}}dio and Vin{\'{\i}}cius Garcia and Renata Fortes},
  journal   = {Journal of Software Engineering Research and Development},
  title     = {Issues on developing interoperable cloud applications: definitions, concepts, approaches, requirements, characteristics and evaluation models},
  year      = {2016},
  month     = {dec},
  number    = {1},
  volume    = {4},
  abstract  = {Among research opportunities in software engineering for cloud computing model, interoperability stands out. We found that the dynamic nature of cloud technologies and the battle for market domination make cloud applications locked-id, i.e, proprietary, non-portable and non-interoperable. In general context of cloud computing, interoperability goes beyond communication between systems like in other fields, it goes in direction of more dynamic, heterogeneous, complex and composed applications that take advantage of best features from different providers and services simultaneously. Interoperability in cloud constitutes a great challenge that must be overcome for that, in the future, software be more dynamic and improved.

Objective: This paper aims at identifying how interoperability in cloud computing has been addressed in the existing literature, offering an up-to-date view of concepts relate to how to develop interoperable software that takes advantage of different cloud models. Thus, providing a basis for further research in the field and consolidating e better exploring existing concepts.

Method: To fulfill this objective, we surveyed literature. We defined six research questions and conducted the study according to a protocol that included planning, and execution.

Results: A first result of the review is that there is no well established definition for cloud interoperability. This study also identified cloud interoperability concepts (e.g., cloud brokers, multi-cloud and cloud federation), requirements for interoperable applications and existing cloud interoperability solutions, showing that these are either too specific for particular situations. Finally, the survey found no evaluation models for cloud interoperability solutions. We also present a discussion on the findings of this study.

Conclusion: Since the study observed that there are no well-established cloud interoperability solutions yet, we conclude that the issues raised by lack of interoperability persist. Selecting one interoperable solution or even a cloud standard can free the system from the underlying providers, but it would still be locked into the selected particular solution.},
  doi       = {10.1186/s40411-016-0033-6},
  publisher = {Sociedade Brasileira de Computacao - {SB}},
  url       = {https://doi.org/10.1186%2Fs40411-016-0033-6},
}

@InCollection{Ardagna2014,
  author    = {Danilo Ardagna and Giovanni Paolo Gibilisco and Michele Ciavotta and Alexander Lavrentev},
  booktitle = {Search-Based Software Engineering},
  publisher = {Springer International Publishing},
  title     = {A Multi-model Optimization Framework for the Model Driven Design of Cloud Applications},
  year      = {2014},
  pages     = {61--76},
  abstract  = {The rise and adoption of the Cloud computing paradigm had a strong impact on the ICT world in the last few years; this technology has now reached maturity and Cloud providers offer a variety of solutions and services to their customers. However, beside the advantages, Cloud computing introduced new issues and challenges. In particular, the heterogeneity of the Cloud services offered and their relative pricing models makes the identification of a deployment solution that minimizes costs and guarantees QoS very complex. Performance assessment of Cloud based application needs for new models and tools to take into consideration the dynamism and multi-tenancy intrinsic of the Cloud environment. The aim of this work is to provide a novel mixed integer linear program (MILP) approach to find a minimum cost feasible cloud configuration for a given cloud based application. The feasibility of the solution is considered with respect to some non-functional requirements that are analyzed through multiple performance models with different levels of accuracy. The initial solution is further improved by a local search based procedure. The quality of the initial feasible solution is compared against first principle heuristics currently adopted by practitioners and Cloud providers.},
  doi       = {10.1007/978-3-319-09940-8_5},
  url       = {https://doi.org/10.1007%2F978-3-319-09940-8_5},
}

@InCollection{Schermann2020,
  author    = {Gerald Schermann and F{\'{a}}bio Oliveira and Erik Wittern and Philipp Leitner},
  booktitle = {Service-Oriented Computing},
  publisher = {Springer International Publishing},
  title     = {Topology-Aware Continuous Experimentation in Microservice-Based Applications},
  year      = {2020},
  pages     = {19--35},
  abstract  = {Continuous experiments, including practices such as canary releases or A/B testing, test new functionality on a small fraction of the user base in production environments. Monitoring data collected on different versions of a service is essential for decision-making on whether to continue or abort experiments. Existing approaches for decision-making rely on service-level metrics in isolation, ignoring that new functionality might introduce changes affecting other services or the overall application’s health state. Keeping track of these changes in applications comprising dozens or hundreds of services is challenging. We propose a holistic approach implemented as a research prototype to identify, visualize, and rank topological changes from distributed tracing data. We devise three ranking heuristics assessing how the changes impact the experiment’s outcome and the application’s health state. An evaluation on two case study scenarios shows that a hybrid heuristic based on structural analysis and a simple root-cause examination outperforms other heuristics in terms of ranking quality.},
  doi       = {10.1007/978-3-030-65310-1_2},
  url       = {https://doi.org/10.1007%2F978-3-030-65310-1_2},
}

@Article{Ciancone2013,
  author    = {Andrea Ciancone and Mauro Luigi Drago and Antonio Filieri and Vincenzo Grassi and Heiko Koziolek and Raffaela Mirandola},
  journal   = {Software {\&} Systems Modeling},
  title     = {The {KlaperSuite} framework for model-driven reliability analysis of component-based systems},
  year      = {2013},
  month     = {mar},
  number    = {4},
  pages     = {1269--1290},
  volume    = {13},
  abstract  = {Automatic prediction tools play a key role in enabling the application of non-functional requirements analysis, to simplify the selection and the assembly of components for component-based software systems, and in reducing the need for strong mathematical skills for software designers. By exploiting the paradigm of Model-Driven Engineering (MDE), it is possible to automatically transform design models into analytical models, thus enabling formal property verification. MDE is the core paradigm of the KlaperSuite framework presented in this paper, which exploits the KLAPER pivot language to fill the gap between design and analysis of component-based systems for reliability properties. KlaperSuite is a family of tools empowering designers with the ability to capture and analyze quality of service views of their systems, by building a one-click bridge towards a number of established verification instruments. In this article, we concentrate on the reliability-prediction capabilities of KlaperSuite and we evaluate them with respect to several case studies from literature and industry.},
  doi       = {10.1007/s10270-013-0334-8},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10270-013-0334-8},
}

@InCollection{Kopp2021,
  author    = {Andrii Kopp and Dmytro Orlovskyi},
  booktitle = {Information and Communication Technologies in Education, Research, and Industrial Applications},
  publisher = {Springer International Publishing},
  title     = {Towards the Method and Information Technology for Evaluation of Business Process Model Quality},
  year      = {2021},
  pages     = {93--118},
  abstract  = {Business process management has become the most widely-used and reliable approach to organizational management over the last decades. It is also considered as a part of quality management system in an organization. Business process modeling is the core of business process management, which is used for visualization, analysis, and improvement of organizational activities. Moreover, business process modeling plays an important role in the context of business process management maturity of an overall enterprise. Therefore, this paper is focused on the problem of business process model quality evaluation. Existing approaches based on business process modeling guidelines, measures, and their thresholds are considered. Refined business process modeling rules, measures, quality criteria of numerical and linguistic values, and a method for evaluation of business process model quality are proposed. The corresponding information technology is designed and implemented, and results of its usage are outlined.},
  doi       = {10.1007/978-3-030-77592-6_5},
  url       = {https://doi.org/10.1007%2F978-3-030-77592-6_5},
}

@InCollection{Dahanayake2015,
  author    = {Ajantha Dahanayake and Bernhard Thalheim},
  booktitle = {Correct Software in Web Applications and Web Services},
  publisher = {Springer International Publishing},
  title     = {W$\ast$H: The Conceptual Model for Services},
  year      = {2015},
  pages     = {145--176},
  abstract  = {Services as an emerging paradigm in modern information technology (IT) infrastructures underwent the first hype for service-oriented computing caused by Web services and the second hype by IT market pressures on large corporations (e.g. SAP), leading to standardisations incorporating logical-level specifications leaving much of the low-level details unaccounted for.The conception of a service needs a conceptual reflection. However, the service notation lacks a conceptual model. This gap is caused by the variety of aspects that must be reflected, such as the handling of the services as a collection of offerings, a proper annotation facility beyond ontologies, a tool to describe the service concept and the specification of the added value of a business user. Those requirements must be handled at the same time. Therefore, this chapter contributes to the development of a conceptual model of a service through a specification framework W∗H and through an embedding framework to the concept-content-annotation triptych and Hermagoras of Temnos inquiry frames.},
  doi       = {10.1007/978-3-319-17112-8_5},
  url       = {https://doi.org/10.1007%2F978-3-319-17112-8_5},
}

@Article{Gerpheide2015,
  author    = {Christine M. Gerpheide and Ramon R. H. Schiffelers and Alexander Serebrenik},
  journal   = {Software Quality Journal},
  title     = {Assessing and improving quality of {QVTo} model transformations},
  year      = {2015},
  month     = {jun},
  number    = {3},
  pages     = {797--834},
  volume    = {24},
  abstract  = {We investigate quality improvement in QVT operational mappings (QVTo) model transformations, one of the languages defined in the OMG standard on model-to-model transformations. Two research questions are addressed. First, how can we assess quality of QVTo model transformations? Second, how can we develop higher-quality QVTo transformations? To address the first question, we utilize a bottom–up approach, starting with a broad exploratory study including QVTo expert interviews, a review of existing material, and introspection. We then formalize QVTo transformation quality into a QVTo quality model. The quality model is validated through a survey of a broader group of QVTo developers. We find that although many quality properties recognized as important for QVTo do have counterparts in general purpose languages, a number of them are specific to QVTo or model transformation languages. To address the second research question, we leverage the quality model to identify developer support tooling for QVTo. We then implemented and evaluated one of the tools, namely a code test coverage tool. In designing the tool, code coverage criteria for QVTo model transformations are also identified. The primary contributions of this paper are a QVTo quality model relevant to QVTo practitioners and an open-source code coverage tool already usable by QVTo transformation developers. Secondary contributions are a bottom–up approach to building a quality model, a validation approach leveraging developer perceptions to evaluate quality properties, code test coverage criteria for QVTo, and numerous directions for future research and tooling related to QVTo quality.},
  doi       = {10.1007/s11219-015-9280-8},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11219-015-9280-8},
}

@Article{Bento2021,
  author    = {Andre Bento and Jaime Correia and Ricardo Filipe and Filipe Araujo and Jorge Cardoso},
  journal   = {Journal of Grid Computing},
  title     = {Automated Analysis of Distributed Tracing: Challenges and Research Directions},
  year      = {2021},
  month     = {feb},
  number    = {1},
  volume    = {19},
  abstract  = {Microservice-based architectures are gaining popularity for their benefits in software development. Distributed tracing can be used to help operators maintain observability in this highly distributed context, and find problems such as latency, and analyse their context and root cause. However, exploring and working with distributed tracing data is sometimes difficult due to its complexity and application specificity, volume of information and lack of tools. The most common and general tools available for this kind of data, focus on trace-level human-readable data visualisation. Unfortunately, these tools do not provide good ways to abstract, navigate, filter and analyse tracing data. Additionally, they do not automate or aid with trace analysis, relying on administrators to do it themselves. In this paper we propose using tracing data to extract service metrics, dependency graphs and work-flows with the objective of detecting anomalous services and operation patterns. We implemented and published open source prototype tools to process tracing data, conforming to the OpenTracing standard, and developed anomaly detection methods. We validated our tools and methods against real data provided by a major cloud provider. Results show that there is an underused wealth of actionable information that can be extracted from both metric and morphological aspects derived from tracing. In particular, our tools were able to detect anomalous behaviour and situate it both in terms of involved services, work-flows and time-frame. Furthermore, we identified some limitations of the OpenTracing format—as well as the industry accepted tracing abstractions—, and provide suggestions to test trace quality and enhance the standard.},
  doi       = {10.1007/s10723-021-09551-5},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10723-021-09551-5},
}

@InCollection{Elberzhager2017,
  author    = {Frank Elberzhager and Matthias Naab},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {High Quality at Short Time-to-Market: Challenges Towards This Goal and Guidelines for the Realization},
  year      = {2017},
  month     = {nov},
  pages     = {121--132},
  abstract  = {High quality and short time-to-market are business goals that are relevant for almost every company since decades. However, the duration between new releases heavily decreased, and the level of quality that customers expect increased drastically during the last years. Achieving both business goals imply investments that have to be considered. In this article, we sketch 22 best practices that help companies to strive towards a shorter time-to-market while providing high quality software products. We share furthermore experiences from a practical environment where a selected set of guidelines was applied. Especially DevOps including an automated deployment pipeline was an essential step towards high quality at short time-to-market.},
  doi       = {10.1007/978-3-319-71440-0_7},
  url       = {https://doi.org/10.1007%2F978-3-319-71440-0_7},
}

@Article{Kosinska2020,
  author    = {Joanna Kosi{\'{n}}ska and Krzysztof Zieli{\'{n}}ski},
  journal   = {Journal of Grid Computing},
  title     = {Autonomic Management Framework for Cloud-Native Applications},
  year      = {2020},
  month     = {sep},
  number    = {4},
  pages     = {779--796},
  volume    = {18},
  abstract  = {In order to meet the rapidly changing requirements of the Cloud-native dynamic execution environment, without human support and without the need to continually improve one’s skills, autonomic features need to be added. Embracing automation at every layer of performance management enables us to reduce costs while improving outcomes. The main contribution of this paper is the definition of autonomic management requirements of Cloud-native applications. We propose that the automation is achieved via high-level policies. In turn autonomy features are accomplished via the rule engine support. First, the paper presents the engineering perspective of building a framework for Autonomic Management of Cloud-Native Applications, namely AMoCNA, in accordance with Model Driven Architecture (MDA) concepts. AMoCNA has many desirable features whose main goal is to reduce the complexity of managing Cloud-native applications. The presented models are, in fact, meta-models, being technology agnostic. Secondly, the paper demonstrates one possibility of implementing the aforementioned design procedures. The presented AMoCNA implementation is also evaluated to identify the potential overhead introduced by the framework.},
  doi       = {10.1007/s10723-020-09532-0},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10723-020-09532-0},
}

@InCollection{Spillner2018,
  author    = {Josef Spillner and Giovanni Toffetti and Manuel Ram{\'{\i}}rez L{\'{o}}pez},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Cloud-Native Databases: An Application Perspective},
  year      = {2018},
  pages     = {102--116},
  abstract  = {As cloud computing technologies evolve to better support hosted software applications, software development businesses are faced with a multitude of options to migrate to the cloud. A key concern is the management of data. Research on cloud-native applications has guided the construction of highly elastically scalable and resilient stateless applications, while there is no corresponding concept for cloud-native databases yet. In particular, it is not clear what the trade-offs between using self-managed database services as part of the application and provider-managed database services are. We contribute an overview about the available options, a testbed to compare the options in a systematic way, and an analysis of selected benchmark results produced during the cloud migration of a commercial document management application.},
  doi       = {10.1007/978-3-319-79090-9_7},
  url       = {https://doi.org/10.1007%2F978-3-319-79090-9_7},
}

@InCollection{Bryzgalov2021,
  author    = {Anton Bryzgalov and Sergey Stupnikov},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {A Cloud-Native Serverless Approach for Implementation of Batch Extract-Load Processes in Data Lakes},
  year      = {2021},
  pages     = {27--42},
  abstract  = {The paper presents an approach to deal with batch extract-load processes for cloud data lakes. The approach combines multiple data ingestion techniques, provides advanced failover strategies and adopts cloud-native implementation. The suggested approach. The prototype implementation utilizes Amazon Web Services platform and is based on its serverless features. The approach can be implemented also using other cloud platforms like Google Cloud Platform or Microsoft Azure.},
  doi       = {10.1007/978-3-030-81200-3_3},
  url       = {https://doi.org/10.1007%2F978-3-030-81200-3_3},
}

@Article{Kehrer2019,
  author    = {Stefan Kehrer and Wolfgang Blochinger},
  journal   = {{SICS} Software-Intensive Cyber-Physical Systems},
  title     = {Migrating parallel applications to the cloud: assessing cloud readiness based on parallel design decisions},
  year      = {2019},
  month     = {feb},
  number    = {2-3},
  pages     = {73--84},
  volume    = {34},
  abstract  = {Parallel applications are the computational backbone of major industry trends and grand challenges in science. Whereas these applications are typically constructed for dedicated High Performance Computing clusters and supercomputers, the cloud emerges as attractive execution environment, which provides on-demand resource provisioning and a pay-per-use model. However, cloud environments require specific application properties that may restrict parallel application design. As a result, design trade-offs are required to simultaneously maximize parallel performance and benefit from cloud-specific characteristics. In this paper, we present a novel approach to assess the cloud readiness of parallel applications based on the design decisions made. By discovering and understanding the implications of these parallel design decisions on an application’s cloud readiness, our approach supports the migration of parallel applications to the cloud. We introduce an assessment procedure, its underlying meta model, and a corresponding instantiation to structure this multi-dimensional design space. For evaluation purposes, we present an extensive case study comprising three parallel applications and discuss their cloud readiness based on our approach.},
  doi       = {10.1007/s00450-019-00396-8},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00450-019-00396-8},
}

@Article{Pecorelli2021,
  author    = {Fabiano Pecorelli and Fabio Palomba and Andrea De Lucia},
  journal   = {Empirical Software Engineering},
  title     = {The Relation of Test-Related Factors to Software Quality: A Case Study on Apache Systems},
  year      = {2021},
  month     = {feb},
  number    = {2},
  volume    = {26},
  abstract  = {Testing represents a crucial activity to ensure software quality. Recent studies have shown that test-related factors (e.g., code coverage) can be reliable predictors of software code quality, as measured by post-release defects. While these studies provided initial compelling evidence on the relation between tests and post-release defects, they considered different test-related factors separately: as a consequence, there is still a lack of knowledge of whether these factors are still good predictors when considering all together. In this paper, we propose a comprehensive case study on how test-related factors relate to production code quality in Apache systems. We first investigated how the presence of tests relates to post-release defects; then, we analyzed the role played by the test-related factors previously shown as significantly related to post-release defects. The key findings of the study show that, when controlling for other metrics (e.g., size of the production class), test-related factors have a limited connection to post-release defects.},
  doi       = {10.1007/s10664-020-09891-y},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10664-020-09891-y},
}

@Article{Wei2021,
  author    = {Hao Wei and Joaquin Salvachua Rodriguez and Octavio Nieto-Taladriz Garcia},
  journal   = {Journal of Grid Computing},
  title     = {Deployment Management and Topology Discovery of Microservice Applications in the Multicloud Environment},
  year      = {2021},
  month     = {jan},
  number    = {1},
  volume    = {19},
  abstract  = {Cloud computing enables the evolution of modern software application design. Applications based on microservice architecture are an example. Meanwhile, multiclouds are widely accepted by enterprise as an infrastructure strategy; however, challenges remain. The autonomous and distributable nature of modern applications, as well as the complexity of multicloud infrastructure, often make universal application deployment management impractical. This phenomenon may further hinder application quality and efficiency. Therefore, deployment resource control and topology discovery in the multicloud infrastructure environment is an intriguing area of cloud computing research. This paper proposes a framework to manage application deployment in the multicloud environment. The framework uses a policy-based deployment control to automatically select and provide deployment resources from the multicloud infrastructure, and it subsequently uses topology discovery to visualize and verify the actual deployment. The proposed framework design is introduced in the paper, and a proof-of-concept prototype is implemented. Experiments in empirical scenarios are conducted. The experimental results indicate that the proposed framework is effective in controlling deployment resources and presenting actual deployment across clouds.},
  doi       = {10.1007/s10723-021-09539-1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10723-021-09539-1},
}

@InCollection{Lin2018,
  author    = {Jinjin Lin and Pengfei Chen and Zibin Zheng},
  booktitle = {Service-Oriented Computing},
  publisher = {Springer International Publishing},
  title     = {Microscope: Pinpoint Performance Issues with Causal Graphs in Micro-service Environments},
  year      = {2018},
  pages     = {3--20},
  abstract  = {Driven by the emerging business models (e.g., digital sales) and IT technologies (e.g., DevOps and Cloud computing), the architecture of software is shifting from monolithic to microservice rapidly. Benefit from microservice, software development, and delivery processes are accelerated significantly. However, along with many micro services running in the dynamic cloud environment with complex interactions, identifying and locating the abnormal services are extraordinarily difficult. This paper presents a novel system named “Microscope” to identify and locate the abnormal services with a ranked list of possible root causes in Micro-service environments. Without instrumenting the source code of micro services, Microscope can efficiently construct a service causal graph and infer the causes of performance problems in real time. Experimental evaluations in a micro-service benchmark environment show that Microscope achieves a good diagnosis result, i.e., 88% in precision and 80% in recall, which is higher than several state-of-the-art methods. Meanwhile, it has a good scalability to adapt to large-scale micro-service systems.},
  doi       = {10.1007/978-3-030-03596-9_1},
  url       = {https://doi.org/10.1007%2F978-3-030-03596-9_1},
}

@Article{Toka2021,
  author    = {L{\'{a}}szl{\'{o}} Toka},
  journal   = {Journal of Grid Computing},
  title     = {Ultra-Reliable and Low-Latency Computing in the Edge with Kubernetes},
  year      = {2021},
  month     = {jul},
  number    = {3},
  volume    = {19},
  abstract  = {Novel applications will require extending traditional cloud computing infrastructure with compute resources deployed close to the end user. Edge and fog computing tightly integrated with carrier networks can fulfill this demand. The emphasis is on integration: the rigorous delay constraints, ensuring reliability on the distributed, remote compute nodes, and the sheer scale of the system altogether call for a powerful resource provisioning platform that offers the applications the best of the underlying infrastructure. We therefore propose Kubernetes-edge-scheduler that provides high reliability for applications in the edge, while provisioning less than 10% of resources for this purpose, and at the same time, it guarantees compliance with the latency requirements that end users expect. We present a novel topology clustering method that considers application latency requirements, and enables scheduling applications even on a worldwide scale of edge clusters. We demonstrate that in a potential use case, a distributed stream analytics application, our orchestration system can reduce the job completion time to 40% of the baseline provided by the default Kubernetes scheduler.},
  doi       = {10.1007/s10723-021-09573-z},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10723-021-09573-z},
}

@InCollection{Jindal2021,
  author    = {Anshul Jindal and Michael Gerndt},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {From {DevOps} to {NoOps}: Is It Worth It?},
  year      = {2021},
  pages     = {178--202},
  abstract  = {With the rise of the adoption of microservice architecture due to its agility, scalability, and resiliency for building the cloud-based applications and their deployment using containerization, DevOps were in demand for handling the development and operations together. However, nowadays serverless computing offers a new way of developing and deploying cloud-native applications. Serverless computing also called NoOps, offloads management and server configuration (operations work) from the user to the cloud provider and lets the user focus only on the product developments. Hence, there are debates regarding which deployment strategy to use.

This research provides a performance comparison of a cloud-native web application along with three different function benchmarks in terms of scalability, reliability, and latency when deployed using DevOps and NoOps deployment strategy. NoOps deployment in this work is achieved using Google Cloud Function and OpenWhisk, while DevOps is achieved using the Kubernetes engine. This research shows that neither of the deployment strategies fits all the scenarios. The experimental results demonstrate that each type of deployment strategy has its advantages under different scenarios. The DevOps deployment strategy has a huge performance advantage (almost 72% lesser 90 percentile response time) for simple web-based requests and requests accessing databases while compute-intensive applications perform better with NoOps deployment. Additionally, NoOps deployment provides better scaling-agility as compared to DevOps.},
  doi       = {10.1007/978-3-030-72369-9_8},
  url       = {https://doi.org/10.1007%2F978-3-030-72369-9_8},
}

@Article{Zimmermann2016,
  author    = {Olaf Zimmermann},
  journal   = {Computing},
  title     = {Architectural refactoring for the cloud: a decision-centric view on cloud migration},
  year      = {2016},
  month     = {oct},
  number    = {2},
  pages     = {129--145},
  volume    = {99},
  abstract  = {Unlike code refactoring of programs, architectural refactoring of systems is not commonly practiced yet. However, legacy systems typically have to be refactored when migrating them to the cloud; otherwise, these systems may run in the cloud, but cannot fully benefit from cloud properties such as elasticity. One reason for the lack of adoption of architectural refactoring is that many of the involved artefacts are intangible—architectural refactoring therefore is harder to grasp than code refactoring. To overcome this inhibitor, we take a task-centric view on the subject and introduce an architectural refactoring template that highlights the architectural decisions to be revisited when refactoring application architectures for the cloud; in this approach, architectural smells are derived from quality stories. We also present a number of common architectural refactorings and evaluate existing patterns regarding their cloud affinity. The final contribution of this paper is the identification of an initial catalog of architectural refactorings for cloud application design. This refactoring catalog was compiled from the cloud patterns literature as well as project experiences. Cloud knowledge and supporting templates have been validated via action research and implementation in cooperation with practitioners.},
  doi       = {10.1007/s00607-016-0520-y},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00607-016-0520-y},
}

@InCollection{Raj2018a,
  author    = {Pethuru Raj and Anupama Raman},
  booktitle = {Software-Defined Cloud Centers},
  publisher = {Springer International Publishing},
  title     = {Automated Multi-cloud Operations and Container Orchestration},
  year      = {2018},
  pages     = {185--218},
  abstract  = {As inscribed a couple of times in this book, the Cloud phenomenon is really rocking and rewarding. The Cloud idea has penetrated deeply and decisively across. The worldwide enterprises are so keen on embracing the bevy of game-changing Cloud technologies, tools, and tips in order to bring in a variety of rationalization and optimization in their IT environments. The IT product vendors, system integrators, academicians, and research laboratories have been working overtime to bring forth newer products, optimized processes, best practices, architecture, design, deployment, operation and integration patterns, performance-enhancement techniques, state-of-the-art infrastructures, etc., in order to make the Cloud idea viable, venerable, and visible. The emergence of multi-cloud environments is one such innovation in the Cloud landscape. Establishing and managing multi-cloud environments is not without challenges and concerns. Cloud brokerage solutions and services come handy in realizing multi-cloud facilities. Cloud migration solutions and services are being rolled out in order to speed up the process of Cloud adoption. Cloud operations, as enunciated in earlier chapters, are automated through toolsets. Most of the Cloud operations are integrated together in order to facilitate the orchestration. This chapter is specially prepared and presented in order to explain the growing significance of the orchestration capability for efficiently and elegantly operating multi-cloud environments.},
  doi       = {10.1007/978-3-319-78637-7_9},
  url       = {https://doi.org/10.1007%2F978-3-319-78637-7_9},
}

@Article{Casale2019,
  author    = {G. Casale and M. Arta{\v{c}} and W.-J. van den Heuvel and A. van Hoorn and P. Jakovits and F. Leymann and M. Long and V. Papanikolaou and D. Presenza and A. Russo and S. N. Srirama and D. A. Tamburri and M. Wurster and L. Zhu},
  journal   = {{SICS} Software-Intensive Cyber-Physical Systems},
  title     = {{RADON}: rational decomposition and orchestration for serverless computing},
  year      = {2019},
  month     = {aug},
  number    = {1-2},
  pages     = {77--87},
  volume    = {35},
  abstract  = {Emerging serverless computing technologies, such as function as a service (FaaS), enable developers to virtualize the internal logic of an application, simplifying the management of cloud-native services and allowing cost savings through billing and scaling at the level of individual functions. Serverless computing is therefore rapidly shifting the attention of software vendors to the challenge of developing cloud applications deployable on FaaS platforms. In this vision paper, we present the research agenda of the RADON project (http://radon-h2020.eu), which aims to develop a model-driven DevOps framework for creating and managing applications based on serverless computing. RADON applications will consist of fine-grained and independent microservices that can efficiently and optimally exploit FaaS and container technologies. Our methodology strives to tackle complexity in designing such applications, including the solution of optimal decomposition, the reuse of serverless functions as well as the abstraction and actuation of event processing chains, while avoiding cloud vendor lock-in through models.},
  doi       = {10.1007/s00450-019-00413-w},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs00450-019-00413-w},
}

@Article{Mondal2021,
  author    = {Subrota Kumar Mondal and Rui Pan and H M Dipu Kabir and Tan Tian and Hong-Ning Dai},
  journal   = {The Journal of Supercomputing},
  title     = {Kubernetes in {IT} administration and serverless computing: An empirical study and research challenges},
  year      = {2021},
  month     = {jul},
  abstract  = {Today’s industry has gradually realized the importance of lifting efficiency and saving costs during the life-cycle of an application. In particular, we see that most of the cloud-based applications and services often consist of hundreds of micro-services; however, the traditional monolithic pattern is no longer suitable for today’s development life-cycle. This is due to the difficulties of maintenance, scale, load balance, and many other factors associated with it. Consequently, people switch their focus on containerization—a lightweight virtualization technology. The saving grace is that it can use machine resources more efficiently than the virtual machine (VM). In VM, a guest OS is required to simulate on the host machine, whereas containerization enables applications to share a common OS. Furthermore, containerization facilitates users to create, delete, or deploy containers effortlessly. In order to manipulate and manage the multiple containers, the leading Cloud providers introduced the container orchestration platforms, such as Kubernetes, Docker Swarm, Nomad, and many others. In this paper, a rigorous study on Kubernetes from an administrator’s perspective is conducted. In a later stage, serverless computing paradigm was redefined and integrated with Kubernetes to accelerate the development of software applications. Theoretical knowledge and experimental evaluation show that this novel approach can be accommodated by the developers to design software architecture and development more efficiently and effectively by minimizing the cost charged by public cloud providers (such as AWS, GCP, Azure). However, serverless functions are attached with several issues, such as security threats, cold start problem, inadequacy of function debugging, and many other. Consequently, the challenge is to find ways to address these issues. However, there are difficulties and hardships in addressing all the issues altogether. Respectively, in this paper, we simply narrow down our analysis toward the security aspects of serverless. In particular, we quantitatively measure the success probability of attack in serverless (using Attack Tree and Attack–Defense Tree) with the possible attack scenarios and the related countermeasures. Thereafter, we show how the quantification can reflect toward the end-to-end security enhancement. In fine, this study concludes with research challenges such as the burdensome and error-prone steps of setting the platform, and investigating the existing security vulnerabilities of serverless computing, and possible future directions.},
  doi       = {10.1007/s11227-021-03982-3},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11227-021-03982-3},
}

@InCollection{Raj2017,
  author    = {Pethuru Raj and Parvathy Arulmozhi and Nithya Chidambaram},
  booktitle = {Requirements Engineering for Service and Cloud Computing},
  publisher = {Springer International Publishing},
  title     = {The Requirements Elicitation Approaches for Software-Defined Cloud Environments},
  year      = {2017},
  pages     = {89--117},
  abstract  = {Without an iota of doubt, the overwhelming domain of requirements elicitation and engineering has been continuously and consistently evolving in order to catch up and match with the tricky and terrific expectations of producing and sustaining next-generation systems, solutions, and services. Especially in the hot and happening IT space and considering the growing complications of systems of engagements (SoE)-like applications, the aspect of requirements engineering is garnering a lot of attention and attraction. IT industry professionals and academic professors are working in unison in charting out easy-to-implement and use methods toward simplified requirements-gathering platforms, procedures, and practices. In this chapter, we would like to dig deeper and deal with the concept of software-defined clouds. Further on, the readers can read how requirements are being solicited and subjected to a variety of investigations before getting selected as sound and rewarding requirements for the right formation of software-defined clouds. We will also register the proven and promising approaches and articulations to speed up the process of simplifying and streamlining up the tasks associated with the requirements engineering activity.},
  doi       = {10.1007/978-3-319-51310-2_5},
  url       = {https://doi.org/10.1007%2F978-3-319-51310-2_5},
}

@Article{Wang2021,
  author    = {Yingying Wang and Harshavardhan Kadiyala and Julia Rubin},
  journal   = {Empirical Software Engineering},
  title     = {Promises and challenges of microservices: an exploratory study},
  year      = {2021},
  month     = {may},
  number    = {4},
  volume    = {26},
  abstract  = {Microservice-based architecture is a SOA-inspired principle of building complex systems as a composition of small, loosely coupled components that communicate with each other using language-agnostic APIs. This architectural principle is now becoming increasingly popular in industry due to its advantages, such as greater software development agility and improved scalability of deployed applications. In this work, we aim at collecting and categorizing best practices, challenges, and some existing solutions for these challenges employed by practitioners successfully developing microservice-based applications for commercial use. Specifically, we focus our study on “mature” teams developing microservice-based applications for at least two years, explicitly excluding “newcomers” to the field. We conduct a broad, mixed-method study that includes in-depth interviews with 21 practitioners and a follow-up online survey with 37 respondents, covering 37 companies in total. Our study shows that, in several cases, practitioners opt to deviate from the “standard” advice, e.g., instead of splitting microservices by business capabilities, they focus on resource consumption and intended deployment infrastructure. Some also choose to refrain from using multiple programming languages for implementing their microservices, as that practice hinders reuse opportunities. In fact, our study participants identified robust and shared infrastructural support established early on in the development process as one of the main factors contributing to their success. They also identified several pressing challenges related to the efficient managing of common code across services and the support of product variants. The results of our study can benefit practitioners who are interested to learn from each other, borrow successful ideas, and avoid common mistakes. It can also inform researchers and inspire novel solutions to some of the identified challenges.},
  doi       = {10.1007/s10664-020-09910-y},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10664-020-09910-y},
}

@InCollection{Andreou2021,
  author    = {Andreas S. Andreou and Andreas Christoforou},
  booktitle = {Next-Gen Digital Services. A Retrospective and Roadmap for Service Computing of the Future},
  publisher = {Springer International Publishing},
  title     = {On the Migration to and Synthesis of (Micro-)services: The Use of Intelligent Techniques},
  year      = {2021},
  pages     = {48--66},
  abstract  = {This chapter investigates the use of Computational Intelligence (CI) to tackle two challenges in the area of services. The first is involved with providing efficient decision support for migrating from monolithic to service-oriented software, while the latter addresses automatic service composition, which is a special form of service migration. Migration to service-oriented architecture (SOA) is influenced by a number of different and intertwined factors. These factors are identified through literature review and expert consultation. Different CI models, such as Fuzzy Influence Diagrams and Fuzzy Cognitive Maps, are employed to organize the factors and study their behavior. Various simulations are conducted that enable decision makers to execute what-if scenarios and take informed decisions as to whether to migrate or not to SOA, as well as to study the decisive factors contributing in favor or against this migration. Service synthesis is a tedious task considering on one hand the plethora of available services and on the other their different, often conflicting characteristics. Automation of this task is therefore a critical issue which deserves attention. In this context, the challenge of automatic service synthesis is addressed through specific methods and techniques based on Evolutionary Computation to achieve such automation to the best possible extent.},
  doi       = {10.1007/978-3-030-73203-5_4},
  url       = {https://doi.org/10.1007%2F978-3-030-73203-5_4},
}

@InCollection{Kousalya2017,
  author    = {G. Kousalya and P. Balakrishnan and C. Pethuru Raj},
  booktitle = {Computer Communications and Networks},
  publisher = {Springer International Publishing},
  title     = {Demystifying the Traits of Software-Defined Cloud Environments ({SDCEs})},
  year      = {2017},
  pages     = {23--53},
  abstract  = {Definitely the cloud journey is on the fast track. The cloud idea got originated and started to thrive from the days of server virtualization. Server machines are being virtualized in order to have multiple virtual machines, which are provisioned dynamically and kept in ready and steady state to deliver sufficient resources (compute, storage, and network) for optimally running any software application. That is, a physical machine can be empowered to run multiple and different applications through the aspect of virtualization. Resultantly, the utilization of expensive compute machines is steadily going up.

This chapter details and describes the nitty-gritty of next-generation cloud centers. The motivations, the key advantages, and the enabling tools and engines along with other relevant details are being neatly illustrated there. An SDCE is an additional abstraction layer that ultimately defines a complete data center. This software layer presents the resources of the data center as pools of virtual and physical resources to host and deliver software applications. A modern SDCE is nimble and supple as per the vagaries of business movements. SECE is, therefore, a collection of virtualized IT resources that can be scaled up or down as required and can be deployed as needed in a number of distinct ways. There are three key components making up SDCEs:

    1. Software-defined computing
    2. Software-defined networking
    3. Software-defined storage
     
The trait of software enablement of different hardware systems has pervaded into other domains so that we hear and read about software-defined protection, security, etc. There are several useful links in the portal (Sang-Woo et al. “Scalable multi-access flash store for big data analytics” FPGA’14, Monterey, CA, USA, February 26–28, 2014) pointing to a number of resources on the software-defined cloud environments.},
  doi       = {10.1007/978-3-319-56982-6_2},
  url       = {https://doi.org/10.1007%2F978-3-319-56982-6_2},
}

@Article{OparaMartins2016,
  author    = {Justice Opara-Martins and Reza Sahandi and Feng Tian},
  journal   = {Journal of Cloud Computing},
  title     = {Critical analysis of vendor lock-in and its impact on cloud computing migration: a business perspective},
  year      = {2016},
  month     = {apr},
  number    = {1},
  volume    = {5},
  abstract  = {Vendor lock-in is a major barrier to the adoption of cloud computing, due to the lack of standardization. Current solutions and efforts tackling the vendor lock-in problem are predominantly technology-oriented. Limited studies exist to analyse and highlight the complexity of vendor lock-in problem in the cloud environment. Consequently, most customers are unaware of proprietary standards which inhibit interoperability and portability of applications when taking services from vendors. This paper provides a critical analysis of the vendor lock-in problem, from a business perspective. A survey based on qualitative and quantitative approaches conducted in this study has identified the main risk factors that give rise to lock-in situations. The analysis of our survey of 114 participants shows that, as computing resources migrate from on-premise to the cloud, the vendor lock-in problem is exacerbated. Furthermore, the findings exemplify the importance of interoperability, portability and standards in cloud computing. A number of strategies are proposed on how to avoid and mitigate lock-in risks when migrating to cloud computing. The strategies relate to contracts, selection of vendors that support standardised formats and protocols regarding standard data structures and APIs, developing awareness of commonalities and dependencies among cloud-based solutions. We strongly believe that the implementation of these strategies has a great potential to reduce the risks of vendor lock-in.},
  doi       = {10.1186/s13677-016-0054-z},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1186%2Fs13677-016-0054-z},
}

@InCollection{Andrikopoulos2018,
  author    = {Vasilios Andrikopoulos},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Engineering Cloud-Based Applications: Towards an Application Lifecycle},
  year      = {2018},
  pages     = {57--72},
  abstract  = {The adoption of cloud computing by organizations of all sizes and types in the recent years has created multiple opportunities and challenges for the development of software to be used in this environment. In this work-in-progress paper, the focus is on the latter part, providing a view on the main research challenges that are created for software engineering by cloud computing. These challenges stem from the inherent characteristics of the cloud computing paradigm, and require a multi-dimensional approach to address them. Towards this goal, a lifecycle for cloud-based applications is presented, as the foundation for further work in the area.},
  doi       = {10.1007/978-3-319-79090-9_4},
  url       = {https://doi.org/10.1007%2F978-3-319-79090-9_4},
}

@Article{ParviziMosaed2014,
  author    = {Alireza Parvizi-Mosaed and Shahrouz Moaven and Jafar Habibi and Ghazaleh Beigi and Mahdieh Naser-Shariat},
  journal   = {Frontiers of Information Technology {\&} Electronic Engineering},
  title     = {Towards a self-adaptive service-oriented methodology based on extended {SOMA}},
  year      = {2014},
  month     = {dec},
  number    = {1},
  pages     = {43--69},
  volume    = {16},
  abstract  = {We propose a self-adaptive process (SAP) that maintains the software architecture quality using the MAPE-K standard model. The proposed process can be plugged into various software development processes and service-oriented methodologies due to its explicitly defined inputs and outputs. To this aim, the proposed SAP is integrated with the service-oriented modeling and application (SOMA) methodology in a two-layered structure to create a novel methodology, named self-adaptive service-oriented architecture methodology (SASOAM), which provides a semi-automatic self-aware method by the composition of architectural tactics. Moreover, the maintenance activity of SOMA is improved using architectural and adaptive patterns, which results in controlling the software architecture quality. The improvement in the maintainability of SOMA is demonstrated by an analytic hierarchy process (AHP) based evaluation method. Furthermore, the proposed method is applied to a case study to represent the feasibility and practicality of SASOAM.},
  doi       = {10.1631/fitee.1400040},
  publisher = {Zhejiang University Press},
  url       = {https://doi.org/10.1631%2Ffitee.1400040},
}

@InCollection{Apel2019,
  author    = {Sebastian Apel and Florian Hertrampf and Steffen Späthe},
  booktitle = {Innovations for Community Services},
  publisher = {Springer International Publishing},
  title     = {Towards a Metrics-Based Software Quality Rating for a Microservice Architecture},
  year      = {2019},
  pages     = {205--220},
  abstract  = {Microservice architectures should be based on isolated, independent and resilient services. In practice, however, that means that different concepts must be taken into account when designing, developing, and operating services. The WINNER research project is developing an application, based on such a microservice architecture in the context of Smart Home, Smart Grid and electromobility in tenant households, as a measurement and processing infrastructure. About this WINNER software, system metrics are calculated and collected, and the potential for rating software quality in the sense of ISO 25010 is examined. For analysis, a microservice architecture describing model will be designed witches describes correlations and links in the service network. Its instance in the context of WINNER, as well as source code and process analyses, are used to perform the final quality considerations.},
  doi       = {10.1007/978-3-030-22482-0_15},
  url       = {https://doi.org/10.1007%2F978-3-030-22482-0_15},
}

@InCollection{Avritzer2018,
  author    = {Alberto Avritzer and Vincenzo Ferme and Andrea Janes and Barbara Russo and Henning Schulz and Andr{\'{e}} van Hoorn},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {A Quantitative Approach for the Assessment of Microservice Architecture Deployment Alternatives by Automated Performance Testing},
  year      = {2018},
  pages     = {159--174},
  abstract  = {Microservices have emerged as an architectural style for developing distributed applications. Assessing the performance of architectural deployment alternatives is challenging and must be aligned with the system usage in the production environment. In this paper, we introduce an approach for using operational profiles to generate load tests to automatically assess scalability pass/fail criteria of several microservices deployment alternatives. We have evaluated our approach with different architecture deployment alternatives using extensive lab studies in a large bare metal host environment and a virtualized environment. The data presented in this paper supports the need to carefully evaluate the impact of increasing the level of computing resources on performance. Specifically, for the case study presented in this paper, we observed that the evaluated performance metric is a non-increasing function of the number of CPU resources for one of the environments under study.},
  doi       = {10.1007/978-3-030-00761-4_11},
  url       = {https://doi.org/10.1007%2F978-3-030-00761-4_11},
}

@InCollection{Athanasopoulos2021,
  author    = {Dionysis Athanasopoulos and Daniel Keenan},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {Stability Metrics for Continuous Integration of Service-Oriented Systems},
  year      = {2021},
  pages     = {139--147},
  abstract  = {One of the key principles of the service orientation is the standardised service contract. However, the assumption that the service contract is kept unmodified during the whole life-cycle of a system is not always held. Evolution changes on the service APIs have an impact on the maintainability of their programming clients within the system making difficult the continuous integration of the services. The metrics that have currently been applied for the service maintainability assess the service coupling, cohesion, complexity, and granularity. Software stability can further contribute in assessing the maintainability of systems. However, it is challenging to measure the stability of service APIs without having evolved their programming clients, because it should be measured by considering the types of the evolution changes in APIs that have direct impact on the programming clients. To address this challenge, we define a set of mappings between evolved service APIs based on which the stability changes can be determined. We further specify a generic algorithm that recognises the evolution changes required on the programming clients of the evolved APIs. We finally define an initial version of a suite of metrics that estimate the stability of a service system without assuming the existence of the evolved programming clients.},
  doi       = {10.1007/978-3-030-74296-6_11},
  url       = {https://doi.org/10.1007%2F978-3-030-74296-6_11},
}

@Article{Mohsin2018,
  author    = {Ahmad Mohsin and Naeem Khalid Janjua},
  journal   = {Service Oriented Computing and Applications},
  title     = {A review and future directions of {SOA}-based software architecture modeling approaches for System of Systems},
  year      = {2018},
  month     = {oct},
  number    = {3-4},
  pages     = {183--200},
  volume    = {12},
  abstract  = {Software architecture is a software system’s earliest set of design decisions that are critical for the quality of the system desired by the stakeholders. The architecture makes it easier to reason about and manage change during different phases of complex software life cycle. The modeling of software architecture for System of Systems (SoS) is a challenging task because of a system’s complexity arising from an integration of heterogeneous, distributed, managerially and operationally independent systems collaborating to achieve global missions. SoS is essentially dynamic and evolutionary by design requiring suitable architectural patterns to deal with runtime volatility. Service-oriented architecture offers several architectural features to these complex systems; these include, interoperability, loose coupling, abstraction and the provision of dynamic services based on standard interfaces and protocols. There is some research work available that provides critical analysis of current software architecture modeling approaches for SoS. However, none of them outlines the important characteristics of SoS or provides detailed analysis of current service-oriented architecture modeling approaches to model those characteristics. This article addresses this research gap and provides a taxonomy of software architecture modeling approaches, comparing and contrasting them using criteria critical for realization of SoS. Additionally, research gaps are identified, and future directions are outlined for building software architecture for SoS to model and reason about architecture quality in a more efficient way in service-oriented paradigm.},
  doi       = {10.1007/s11761-018-0245-1},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11761-018-0245-1},
}

@InCollection{Ntentos2020a,
  author    = {Evangelos Ntentos and Uwe Zdun and Konstantinos Plakidas and Sebastian Meixner and Sebastian Geiger},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Assessing Architecture Conformance to Coupling-Related Patterns and Practices in Microservices},
  year      = {2020},
  pages     = {3--20},
  abstract  = {Microservices are the go-to architectural style for building applications that are polyglot, support high scalability, independent development and deployment, and are rapidly adaptable to changes. Among the core tenets for a successful microservice architecture is high independence of the individual microservices, i.e. loose coupling. A number of patterns and best practices are well-established in the literature, but most actual microservice-based systems do not, as a whole or in part, conform to them. Assessing this conformance manually is not realistically possible for large-scale systems. This study aims to provide the foundations for an automated approach for assessing conformance to coupling-related patterns and practices specific for microservice architectures. We propose a model-based assessment based on generic, technology-independent metrics, connected to typical design decisions encountered in microservice architectures. We demonstrate and assess the validity and appropriateness of these metrics by performing an assessment of the conformance of real-world systems to patterns through statistical methods.},
  doi       = {10.1007/978-3-030-58923-3_1},
  url       = {https://doi.org/10.1007%2F978-3-030-58923-3_1},
}

@InCollection{Salgado2016,
  author    = {Carlos E. Salgado and Ricardo J. Machado and Rita S. P. Maciel},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {A Three-Dimensional Approach for a Quality-Based Alignment Between Requirements and Architecture},
  year      = {2016},
  pages     = {112--125},
  abstract  = {The relation between requirements and architecture is a crucial part of an information system, standing as one of the main challenges for its successful development, with traditional projects focused on the connection of functional requirements with architecture components having a tendency to ignore quality concerns. As the quality attributes of a system support its architecture high level structure and behavior, also being highly related to its early nonfunctional requirements, there is a pressing need to align these two realities. Following our solution for aligning business requirements with services quality characteristics by derivation of a logical architecture, we now propose the specification of a metamodel and method supporting a three-dimensional approach for handling the alignment of quality issues between requirements and architecture. Taking advantage of a cube structure and method definition within a SPEM approach, which is adaptable to model variations, our proposal contributes to an improved aligned and traceable solution.},
  doi       = {10.1007/978-3-319-32689-4_9},
  url       = {https://doi.org/10.1007%2F978-3-319-32689-4_9},
}

@InCollection{Staron2020,
  author    = {Miroslaw Staron},
  booktitle = {Automotive Software Architectures},
  publisher = {Springer International Publishing},
  title     = {Evaluation of Automotive Software Architectures},
  year      = {2020},
  month     = {dec},
  pages     = {189--213},
  abstract  = {In this chapter we introduce methods for assessing the quality of software architectures and we discuss one of the techniques—ATAM. We discuss the non-functional properties of automotive software and we review the methods used to assess such properties as dependability, robustness and reliability. We follow the ISO/IEC 25000 series of standards when discussing these properties. In this chapter we also address the challenges related to the integration of hardware and software and the impact of this integration. We review differences with stand-alone desktop applications and discuss examples of these differences. Towards the end of the chapter we discuss the need to measure these properties and introduce the need for software measurement.},
  doi       = {10.1007/978-3-030-65939-4_8},
  url       = {https://doi.org/10.1007%2F978-3-030-65939-4_8},
}

@InCollection{Bogner2020,
  author    = {Justus Bogner and Stefan Wagner and Alfred Zimmermann},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Collecting Service-Based Maintainability Metrics from {RESTful} {API} Descriptions: Static Analysis and Threshold Derivation},
  year      = {2020},
  pages     = {215--227},
  abstract  = {While many maintainability metrics have been explicitly designed for service-based systems, tool-supported approaches to automatically collect these metrics are lacking. Especially in the context of microservices, decentralization and technological heterogeneity may pose challenges for static analysis. We therefore propose the modular and extensible RAMA approach (RESTful API Metric Analyzer) to calculate such metrics from machine-readable interface descriptions of RESTful services. We also provide prototypical tool support, the RAMA CLI, which currently parses the formats OpenAPI, RAML, and WADL and calculates 10 structural service-based metrics proposed in scientific literature. To make RAMA measurement results more actionable, we additionally designed a repeatable benchmark for quartile-based threshold ranges (green, yellow, orange, red). In an exemplary run, we derived thresholds for all RAMA CLI metrics from the interface descriptions of 1,737 publicly available RESTful APIs. Researchers and practitioners can use RAMA to evaluate the maintainability of RESTful services or to support the empirical evaluation of new service interface metrics.},
  doi       = {10.1007/978-3-030-59155-7_16},
  url       = {https://doi.org/10.1007%2F978-3-030-59155-7_16},
}

@InCollection{Wu2018a,
  author    = {Wensheng Wu and Yuanfang Cai and Rick Kazman and Ran Mo and Zhipeng Liu and Rongbiao Chen and Yingan Ge and Weicai Liu and Junhui Zhang},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Software Architecture Measurement{\textemdash}Experiences from a Multinational Company},
  year      = {2018},
  pages     = {303--319},
  abstract  = {In this paper, we present our 4-year experience of creating, evolving, and validating an automated software architecture measurement system within Huawei. This system is centered around a comprehensive scale called the Standard Architecture Index (SAI), which is composed of a number of measures, each reflecting a recurring architecture problem. Development teams use this as a guide to figure out how to achieve a better score by addressing the underlying problems. The measurement practice thus motivates desired behaviors and outcomes. In this paper, we present our experience of creating and validating SAI 1.0 and 2.0, which has been adopted as the enterprise-wide standard, and our directions towards SAI 3.0. We will describe how we got the development teams to accept and apply SAI through pilot studies, constantly adjusting the formula based on feedback, and correlating SAI scores with productivity measures. Our experience shows that it is critical to guide development teams to focus on the underlying problems behind each measure within SAI, rather than on the score itself. It is also critical to introduce state-of-the-art technologies to the development teams. In doing so they can leverage these technologies to pinpoint and quantify architecture problems so that better SAI scores can be achieved, along with better quality and productivity.},
  doi       = {10.1007/978-3-030-00761-4_20},
  url       = {https://doi.org/10.1007%2F978-3-030-00761-4_20},
}

@InCollection{BaniIsmail2018,
  author    = {Basel Bani-Ismail and Youcef Baghdadi},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {A Survey of Existing Evaluation Frameworks for Service Identification Methods: Towards a Comprehensive Evaluation Framework},
  year      = {2018},
  pages     = {191--202},
  abstract  = {Service identification is one of the main challenges in developing services for Service-Oriented Architecture (SOA). A large number of Service Identification Methods (SIMs) have been proposed to simplify service identification. Therefore, many evaluation frameworks are available in the literature for comparing the existing SIMs. This paper aims to identify and analyze the existing evaluation frameworks for SIMs. Moreover, it aims to propose comprehensive evaluation criteria that address most aspects of the existing SIMs. A review of 23 evaluation frameworks for SIMs built the foundation for deriving a comprehensive set of 16 criteria, namely SOA lifecycle coverage, approach, input artifact, technique, types of services, service description, service quality attributes, service granularity, comprehensive, systematic, availability, tool support, adoption of existing practices, validation, configurability, and domain. The proposed criteria set can be used as a first step towards a comprehensive evaluation framework for SIMs.},
  doi       = {10.1007/978-3-319-95204-8_17},
  url       = {https://doi.org/10.1007%2F978-3-319-95204-8_17},
}

@InCollection{Ntentos2019,
  author    = {Evangelos Ntentos and Uwe Zdun and Konstantinos Plakidas and Daniel Schall and Fei Li and Sebastian Meixner},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Supporting Architectural Decision Making on Data Management in Microservice Architectures},
  year      = {2019},
  pages     = {20--36},
  abstract  = {Today many service-based systems follow the microservice architecture style. As microservices are used to build distributed systems and promote architecture properties such as independent service development, polyglot technology stacks including polyglot persistence, and loosely coupled dependencies, architecting data management is crucial in most microservice architectures. Many patterns and practices for microservice data management architectures have been proposed, but are today mainly informally discussed in the so-called “grey literature”: practitioner blogs, experience reports, and system documentations. As a result, the architectural knowledge is scattered across many knowledge sources that are usually based on personal experiences, inconsistent, and, when studied on their own, incomplete. In this paper we report on a qualitative, in-depth study of 35 practitioner descriptions of best practices and patterns on microservice data management architectures. Following a model-based qualitative research method, we derived a formal architecture decision model containing 325 elements and relations. Comparing the completeness of our model with an existing pattern catalog, we conclude that our architectural decision model substantially reduces the effort needed to sufficiently understand microservice data management decisions, as well as the uncertainty in the design process.},
  doi       = {10.1007/978-3-030-29983-5_2},
  url       = {https://doi.org/10.1007%2F978-3-030-29983-5_2},
}

@InCollection{Zdun2017,
  author    = {Uwe Zdun and Elena Navarro and Frank Leymann},
  booktitle = {Service-Oriented Computing},
  publisher = {Springer International Publishing},
  title     = {Ensuring and Assessing Architecture Conformance to Microservice Decomposition Patterns},
  year      = {2017},
  pages     = {411--429},
  abstract  = {Microservice-based software architecture design has been widely discussed, and best practices have been published as architecture design patterns. However, conformance to those patterns is hard to ensure and assess automatically, leading to problems such as architectural drift and erosion, especially in the context of continued software evolution or large-scale microservice systems. In addition, not much in the component and connector architecture models is specific (only) to the microservices approach, whereas other aspects really specific to that approach, such as independent deployment of microservices, are usually modeled in other views or not at all. We suggest a set of constraints to check and metrics to assess architecture conformance to microservice patterns. In comparison to expert judgment derived from the patterns, a subset of these constraints and metrics shows a good relative performance and potential for automation.},
  doi       = {10.1007/978-3-319-69035-3_29},
  url       = {https://doi.org/10.1007%2F978-3-319-69035-3_29},
}

@Article{Detten2013,
  author    = {Markus von Detten and Marie Christin Platenius and Steffen Becker},
  journal   = {Software {\&} Systems Modeling},
  title     = {Reengineering component-based software systems with Archimetrix},
  year      = {2013},
  month     = {apr},
  number    = {4},
  pages     = {1239--1268},
  volume    = {13},
  abstract  = {Many software development, planning, or analysis tasks require an up-to-date software architecture documentation. However, this documentation is often outdated, unavailable, or at least not available as a formal model which analysis tools could use. Reverse engineering methods try to fill this gap. However, as they process the system’s source code, they are easily misled by design deficiencies (e.g., violations of component encapsulation) which leaked into the code during the system’s evolution. Despite the high impact of design deficiencies on the quality of the resulting software architecture models, none of the surveyed related works is able to cope with them during the reverse engineering process. Therefore, we have developed the Archimetrix approach which semiautomatically recovers the system’s concrete architecture in a formal model while simultaneously detecting and removing design deficiencies. We have validated Archimetrix on a case study system and two implementation variants of the CoCoME benchmark system. Results show that the removal of relevant design deficiencies leads to an architecture model which more closely matches the system’s conceptual architecture.},
  doi       = {10.1007/s10270-013-0341-9},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs10270-013-0341-9},
}

@InCollection{Salgado2015,
  author    = {Carlos E. Salgado and Ricardo J. Machado and Rita S. P. Maciel},
  booktitle = {New Contributions in Information Systems and Technologies},
  publisher = {Springer International Publishing},
  title     = {Aligning Business Requirements with Services Quality Characteristics by Using Logical Architectures},
  year      = {2015},
  pages     = {593--602},
  abstract  = {Derivation of logical architectures has been largely focused on the elicitation of functional requirements, disregarding the non-functional ones. Consequently, relevant business requirements content is not reflected in the information system architectural solution, so lowering its quality. Although research has recently been approaching this issue, much is left to do, especially regarding the alignment of business requirements with the logical architecture components. Following our proposed metamodel for relating processes, goals and rules (PGR), elicited from business requirements, and the 4SRS-SoaML method for the derivation of a logical architecture in SOA environments, we now aim to extend our work by generating the quality information associated to architectural services from business requirements. By extending our PGR metamodel to include the architectural services and associated quality characteristics, we hope to contribute to the improved alignment and traceability between the use cases problem-set and the logical architecture’s components solution.},
  doi       = {10.1007/978-3-319-16486-1_58},
  url       = {https://doi.org/10.1007%2F978-3-319-16486-1_58},
}

@InCollection{Schnoor2019,
  author    = {Henning Schnoor and Wilhelm Hasselbring},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Comparing Static and Dynamic Weighted Software Coupling Metrics},
  year      = {2019},
  pages     = {285--298},
  abstract  = {Coupling metrics are an established way to measure software architecture quality with respect to modularity. Static coupling metrics are obtained from the source or compiled code of a program, while dynamic metrics use runtime data gathered e.g., by monitoring a system in production. We study weighted dynamic coupling that takes into account how often a connection is executed during a system’s run. We investigate the correlation between dynamic weighted metrics and their static counterparts. We use data collected from four different experiments, each monitoring production use of a commercial software system over a period of four weeks. We observe an unexpected level of correlation between the static and the weighted dynamic case as well as revealing differences between class- and package-level analyses.},
  doi       = {10.1007/978-3-030-30275-7_22},
  url       = {https://doi.org/10.1007%2F978-3-030-30275-7_22},
}

@InCollection{Abrahao2016,
  author    = {Silvia Abrahao and Maria Teresa Baldassarre and Danilo Caivano and Yvonne Dittrich and Rosa Lanzilotti and Antonio Piccinno},
  booktitle = {Product-Focused Software Process Improvement},
  publisher = {Springer International Publishing},
  title     = {Human Factors in Software Development Processes: Measuring System Quality},
  year      = {2016},
  pages     = {691--696},
  abstract  = {Software Engineering and Human-Computer Interaction look at the development process from different perspectives. They apparently use very different approaches, are inspired by different principles and address different needs. But, they definitively have the same goal: develop high quality software in the most effective way. The second edition of the workshop puts particular attention on efforts of the two communities in enhancing system quality. The research question discussed is: who, what, where, when, why, and how should we evaluate?},
  doi       = {10.1007/978-3-319-49094-6_57},
  url       = {https://doi.org/10.1007%2F978-3-319-49094-6_57},
}

@InCollection{Tsoumas2020,
  author    = {Ilias Tsoumas and Chrysostomos Symvoulidis and Dimosthenis Kyriazis and Panagiotis Gouvas and Anastasios Zafeiropoulos and Javier Melian and Janez Sterle},
  booktitle = {Information Systems},
  publisher = {Springer International Publishing},
  title     = {Modelling 5G Cloud-Native Applications by Exploiting the Service Mesh Paradigm},
  year      = {2020},
  pages     = {151--162},
  abstract  = {The new-coming 5G network is considered to be one of the most significant innovations today. This is due to the opportunities that is going to provide to the vertical industries. 5G infrastructures will introduce a new way for low-delay, reliable deployment of services. In fact, such infrastructures can be used for the placement of application services in the form of application graphs. An application graph consists of several application components (i.e. micro-services) that may be hosted in the same infrastructure or in different ones. Conflicting requirements that arise when deploying in such infrastructures are now handled through network slicing, which regards a way for partitioning conventional network and computing resources into virtual elements. In this paper, we define a universal application metamodel of a 5G compatible application in order to guarantee the annotation of each application descriptor with its proper requirements for their fulfillment at the instantiation time. In terms of application architecture, we consider each application graph as a service mesh topology in order to adopt this novel service architecture as a dominant methodology that is well fitting in the promising 5G capabilities},
  doi       = {10.1007/978-3-030-44322-1_12},
  url       = {https://doi.org/10.1007%2F978-3-030-44322-1_12},
}

@InCollection{Kratzke2017,
  author    = {Nane Kratzke and Peter-Christian Quint},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Investigation of Impacts on Network Performance in the Advance of a Microservice Design},
  year      = {2017},
  pages     = {187--208},
  abstract  = {Due to REST-based protocols, microservice architectures are inherently horizontally scalable. That might be why the microservice architectural style is getting more and more attention for cloud-native application engineering. Corresponding microservice architectures often rely on a complex technology stack which includes containers, elastic platforms and software defined networks. Astonishingly, there are almost no specialized tools to figure out performance impacts (coming along with this microservice architectural style) in the upfront of a microservice design. Therefore, we propose a benchmarking solution intentionally designed for this upfront design phase. Furthermore, we evaluate our benchmark and present some performance data to reflect some often heard cloud-native application performance rules (or myths).},
  doi       = {10.1007/978-3-319-62594-2_10},
  url       = {https://doi.org/10.1007%2F978-3-319-62594-2_10},
}

@InCollection{Poth2018,
  author    = {Alexander Poth and Mark Werner and Xinyan Lei},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {How to Deliver Faster with {CI}/{CD} Integrated Testing Services?},
  year      = {2018},
  pages     = {401--409},
  abstract  = {After Volkswagen AG has setup its hybrid cloud products and services are expected to have a significant shorter time-to-market and a high quality level. To satisfy these expectations, the existing testing procedures should have to speed up, so that it won’tbe the bottle neck for new features that have to be shipped fast. In order to realize a faster delivery of products and services Testing as a Service is developed with lean/agile methods [1, 2] and integrated into the CI/CD pipeline of the Volkwagen Group IT Cloud.},
  doi       = {10.1007/978-3-319-97925-0_33},
  url       = {https://doi.org/10.1007%2F978-3-319-97925-0_33},
}

@InCollection{Rosati2019,
  author    = {Pierangelo Rosati and Frank Fowley and Claus Pahl and Davide Taibi and Theo Lynn},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Right Scaling for Right Pricing: A Case Study on Total Cost of Ownership Measurement for Cloud Migration},
  year      = {2019},
  pages     = {190--214},
  abstract  = {Cloud computing promises traditional enterprises and independent software vendors a myriad of advantages over on-premise installations including cost, operational and organizational efficiencies. The decision to migrate software configured for on-premise delivery to the cloud requires careful technical consideration and planning. In this chapter, we discuss the impact of right-scaling on the cost modelling for migration decision making and price setting of software for commercial resale. An integrated process is presented for measuring total cost of ownership, taking in to account IaaS/PaaS resource consumption based on forecast SaaS usage levels. The process is illustrated with a real world case study.},
  doi       = {10.1007/978-3-030-29193-8_10},
  url       = {https://doi.org/10.1007%2F978-3-030-29193-8_10},
}

@InCollection{Palma2021,
  author    = {Stefano Dalla Palma and Martin Garriga and Dario Di Nucci and Damian Andrew Tamburri and Willem-Jan Van Den Heuvel},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {{DevOps} and Quality Management in Serverless Computing: The {RADON} Approach},
  year      = {2021},
  pages     = {155--160},
  abstract  = {The onset of microservices and serverless computer solutions has forced an ever-increasing demand for tools and techniques to establish and maintain the quality of infrastructure code, the blueprint that drives the operationalization of large-scale software systems. In the EU H2020 project RADON, we propose a machine-learning approach to elaborate and evolve Infrastructure-as-Code as part of a full-fledged industrial-strength DevOps pipeline. This paper illustrates RADON and shows our research roadmap.},
  doi       = {10.1007/978-3-030-71906-7_13},
  url       = {https://doi.org/10.1007%2F978-3-030-71906-7_13},
}

@InCollection{Banijamali2020,
  author    = {Ahmad Banijamali and Pasi Kuvaja and Markku Oivo and Pooyan Jamshidi},
  booktitle = {Product-Focused Software Process Improvement},
  publisher = {Springer International Publishing},
  title     = {Kuksa{\textdollar}{\textdollar}{\^{}}$\lbrace${\ast}$\rbrace${\textdollar}{\textdollar}: Self-adaptive Microservices in Automotive Systems},
  year      = {2020},
  pages     = {367--384},
  abstract  = {In pervasive dynamic environments, vehicles connect to other objects to send operational data and receive updates so that vehicular applications can provide services to users on demand. Automotive systems should be self-adaptive, thereby they can make real-time decisions based on changing operating conditions. Emerging modern solutions, such as microservices could improve self-adaptation capabilities and ensure higher levels of quality performance in many domains. We employed a real-world automotive platform called Eclipse Kuksa to propose a framework based on microservices architecture to enhance the self-adaptation capabilities of automotive systems for runtime data analysis. To evaluate the designed solution, we conducted an experiment in an automotive laboratory setting where our solution was implemented as a microservice-based adaptation engine and integrated with other Eclipse Kuksa components. The results of our study indicate the importance of design trade-offs for quality requirements’ satisfaction levels of each microservices and the whole system for the optimal performance of an adaptive system at runtime.},
  doi       = {10.1007/978-3-030-64148-1_23},
  url       = {https://doi.org/10.1007%2F978-3-030-64148-1_23},
}

@InCollection{Fowley2018,
  author    = {Frank Fowley and Claus Pahl},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Cloud Migration Architecture and Pricing {\textendash} Mapping a Licensing Business Model for Software Vendors to a {SaaS} Business Model},
  year      = {2018},
  pages     = {91--103},
  abstract  = {Cloud migration is about moving an on-premise software system into the cloud. Many approaches exist that describe the technical migration analysis and the architectural migration. Equally, cloud cost models have been investigated. What we aim to investigate here is to link architecture and software utilisation to costing and business models. We specifically look at software vendors that use the cloud to provide their solutions to customers. They might face the challenges to migrate an in-house developed and provided product onto a cloud IaaS or PaaS platform, while also mapping a licensing model onto a cloud monetisation model. We provide here an experience report. This is based on experience with five migration case studies. We discuss the migration process under consideration of cost aspects, covering both income and expenses in the cloud in relation to the cloud delivery model chosen. We focus on one of the case studies to illustrate the concepts and observations.},
  doi       = {10.1007/978-3-319-72125-5_7},
  url       = {https://doi.org/10.1007%2F978-3-319-72125-5_7},
}

@InCollection{Shabelnyk2021,
  author    = {Oleksandr Shabelnyk and Pantelis A. Frangoudis and Schahram Dustdar and Christos Tsigkanos},
  booktitle = {Software Architecture},
  publisher = {Springer International Publishing},
  title     = {Updating Service-Based Software Systems in Air-Gapped Environments},
  year      = {2021},
  pages     = {147--163},
  abstract  = {Contemporary component-based systems often manifest themselves as service-based architectures, where a central activity is management of their software updates. However, stringent security constraints in mission-critical settings often impose compulsory network isolation among systems, also known as air-gap; a prevalent choice in different sectors including private, public or governmental organizations. This raises several issues involving updates, stemming from the fact that controlling the update procedure of a distributed service-based system centrally and remotely is precluded by network isolation policies. A dedicated software architecture is thus required, where key themes are dependability of the update process, interoperability with respect to the software supported and auditability regarding update actions previously performed. We adopt an architectural viewpoint and present a technical framework for updating service-based systems in air-gapped environments. We describe the particularities of the domain characterized by network isolation and provide suitable notations for service versions, whereupon satisfiability is leveraged for dependency resolution; those are situated within an overall architectural design. Finally, we evaluate the proposed framework over a realistic case study of an international organization, and assess the performance of the dependency resolution procedures for practical problem sizes.},
  doi       = {10.1007/978-3-030-86044-8_10},
  url       = {https://doi.org/10.1007%2F978-3-030-86044-8_10},
}

@InCollection{Floerecke2018,
  author    = {Sebastian Floerecke},
  booktitle = {Exploring Service Science},
  publisher = {Springer International Publishing},
  title     = {Success Factors of {SaaS} Providers' Business Models {\textendash} An Exploratory Multiple-Case Study},
  year      = {2018},
  pages     = {193--207},
  abstract  = {Market studies have revealed major differences in the level of performance among providers of Software as a Service (SaaS). The literature’s understanding of the underlying success factors and thus, the reasons for this performance discrepancy is, however, still limited. The goal of this research paper is therefore to investigate the success factors of SaaS providers’ business models by conducting an exploratory multiple-case study. 21 expert interviews with representatives from 17 cloud providers serve as central method of data collection. The study’s result is a catalogue of 27 success factors. In particular, a SaaS service should be developed as a system comprising modular microservices in order to meet the desired requirements in terms of cost advantages, performance and scalability. Overall, established SaaS providers obtain a reference framework to compare, rethink and innovate their present business models. Companies that are planning to offer SaaS in future gain valuable insights which should directly feed into their business model design process.},
  doi       = {10.1007/978-3-030-00713-3_15},
  url       = {https://doi.org/10.1007%2F978-3-030-00713-3_15},
}

@inproceedings{10.1145/3143434.3143443,
author = {Bogner, Justus and Wagner, Stefan and Zimmermann, Alfred},
title = {Automatically Measuring the Maintainability of Service- and Microservice-Based Systems: A Literature Review},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143443},
doi = {10.1145/3143434.3143443},
abstract = {In a time of digital transformation, the ability to quickly and efficiently adapt
software systems to changed business requirements becomes more important than ever.
Measuring the maintainability of software is therefore crucial for the long-term management
of such products. With Service-based Systems (SBSs) being a very important form of
enterprise software, we present a holistic overview of such metrics specifically designed
for this type of system, since traditional metrics - e.g. object-oriented ones - are
not fully applicable in this case. The selected metric candidates from the literature
review were mapped to 4 dominant design properties: size, complexity, coupling, and
cohesion. Microservice-based Systems (μSBSs) emerge as an agile and fine-grained variant
of SBSs. While the majority of identified metrics are also applicable to this specialization
(with some limitations), the large number of services in combination with technological
heterogeneity and decentralization of control significantly impacts automatic metric
collection in such a system. Our research therefore suggest that specialized tool
support is required to guarantee the practical applicability of the presented metrics
to μSBSs.},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {107–115},
numpages = {9},
keywords = {maintainability, metrics, service-based systems, SOA, microservices},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@InProceedings{Ntentos2020,
  author       = {Ntentos, Evangelos and Zdun, Uwe and Plakidas, Konstantinos and Meixner, Sebastian and Geiger, Sebastian},
  booktitle    = {International Conference on Service-Oriented Computing},
  title        = {Metrics for Assessing Architecture Conformance to Microservice Architecture Patterns and Practices},
  year         = {2020},
  organization = {Springer},
  pages        = {580--596},
  doi          = {10.1007/978-3-030-65310-1_42},
  url          = {https://doi.org/10.1007/978-3-030-65310-1_42},
}

@Comment{jabref-meta: databaseType:bibtex;}
